{
    "title": "ByS1VpgRZ",
    "content": "We propose a novel approach to incorporating conditional information into GAN discriminators, improving class conditional image generation on the ImageNet dataset. This method differs from current conditional GAN frameworks by respecting the role of conditional information in the probabilistic model. Additionally, we achieved superior results with a single discriminator-generator pair and successfully applied the technique to super-resolution, producing high-quality images. The new structure allows high-quality category transformation through parametric functional transformation of conditional batch normalization layers in the generator. Generative Adversarial Networks (GANs) are a framework for creating generative models that mimic the target distribution, leading to state-of-the-art algorithms in image generation. GANs train the generator by iteratively training the discriminator, with conditional GANs using conditional information for improved performance. Conditional Generative Adversarial Networks (cGANs) are gaining attention for class conditional image generation, image-to-image translation, and generating images from text. Unlike standard GANs, cGANs have a discriminator that discriminates between the generator distribution and the target distribution based on pairs of generated samples and their intended conditional variables. Most cGAN frameworks feed conditional information into the discriminator by concatenating it to the input or feature vector. However, there is a need to consider the structure of conditional probabilistic models in the discriminator function. In this paper, a specific form of the discriminator is proposed for Conditional Generative Adversarial Networks (cGANs), motivated by a probabilistic model with discrete or uni-modal continuous distributions for the conditional variable y given x. This model assumption is common in real-world applications like class-conditional image generation and super-resolution. Adhering to this assumption leads to a discriminator structure that involves taking an inner product between the embedded condition vector y and the feature vector, resulting in significant quality improvements. The paper proposes a specific discriminator for cGANs, improving class conditional image generation and super-resolution tasks. The discriminator structure involves an inner product between the condition vector y and the feature vector, leading to quality enhancements. The standard adversarial loss for the discriminator in cGANs is defined with the sigmoid function. The 'critic' D significantly impacts the generator G's performance. An alternative approach is proposed to feeding y to D by decomposing the optimal solution for the loss function into two log likelihood ratios. This involves modeling the log likelihood ratios with parametric functions f1 and f2 for y|x and x, assuming simple distributions like Gaussian or discrete log linear. The discriminator in cGANs is optimized using an adversarial loss with a sigmoid function. A new approach involves decomposing the optimal loss solution into log likelihood ratios modeled by parametric functions for y|x and x, assuming Gaussian or discrete log linear distributions. The model parameters are trained to optimize the adversarial loss, with the discriminator referred to as projection. The optimal solution of the discriminator objective function is derived for categorical variables, where y takes values in {1, . . . , C}. The log linear model for p(y|x) involves a partition function and \u03c6 as input. The target distribution q can also be parametrized in this form. The log likelihood ratio can be expressed as a combination of normalization constants and r(x). The formulation introduces label information via an inner product, rather than concatenation. The log linear model for p(y|x) involves a partition function and \u03c6 as input. The target distribution q can also be parametrized in this form. The log likelihood ratio can be expressed as a combination of normalization constants and r(x). The formulation introduces label information via an inner product, as opposed to concatenation. The regularization effect on the generator G during training is unclear in its formulation. The text discusses the relationship between the generator distribution p and the target distribution q, emphasizing the importance of a regularity condition on the divergence measure for stability in training. It highlights the need for theoretical work to assess the impact of the function space for the discriminator on the training process of the generator. The text also mentions the arbitrary nature of incorporating conditional information through concatenation, which may introduce candidate functions without a logical basis. The text discusses the importance of using a discriminator that aligns with the probabilistic model and incorporating conditional information into the training process. AC-GANs and PPGNs are two approaches that manipulate the loss function and use auxiliary classifier functions for generative models. The generative model uses an auxiliary classifier function to generate high-quality images, but this method may lead to the generator producing images that are easy for the classifier to classify, deviating from the true distribution. This issue worsens with more labels, as seen in experiments with a dataset of 1000 class categories. The newly proposed architecture for the discriminator was evaluated through class conditional image generation and super-resolution experiments on the ILSVRC2012 dataset. The ResNet-based discriminator and generator with spectral normalization were used, along with the hinge version of the adversarial loss function. The experiments were optimized using the Adam optimizer. The experiments used the Adam optimizer with specific hyper-parameters. The ImageNet dataset was utilized for class conditional image generation. A ResNet-based generator and discriminator were employed, along with conditional batch normalization. The proposed projection model discriminator includes a 'projection layer' for inner product calculations. The experiments utilized the ImageNet dataset for class conditional image generation with a ResNet-based generator and discriminator. The proposed projection model discriminator includes a 'projection layer' for inner product calculations. The comparative experiments involved training the model for 450K iterations, with AC-GANs collapsing prematurely before completion. The training of concat to stabilize. AC-GANs collapsed prematurely before 450K iterations, so results were reported at 80K iterations. Our method continued to improve even after 450K, with results at 850K iterations exclusively. Inception score and FID were used for evaluation, with FID being a measure of diversity and visual quality. FID BID9 is a quantitative measure of diversity and visual appearance, computed between generated and dataset images within each class. It measures the 2-Wasserstein distance between two distributions and is calculated using mean and covariance of final feature vectors. Intra FID is observed to be effective in detecting collapsed modes in generated examples. In experiments, intra FID effectively measures diversity and visual quality. Comparing inner-product approach to ACGANs and concatenation, hidden concatenation shows better inception scores. Projection outperforms concat in training curves. The comparison between projection and concat in training shows that projection outperforms concat in terms of the inception score. The images generated by projection have lower intra FID scores, indicating a smaller Wasserstein distance to the target distribution. The model also performed better on CIFAR10 and CIFAR 100 datasets. The comparison between projection and concat in training shows that projection outperforms concat in terms of the inception score. When the concat outperforms projection, it only wins by a slight margin, whereas projection outperforms concat by a large margin in the opposite case. The FID score suggests that the visual quality is being measured, with mode-collapse observed in the results generated by AC-GANs. This tendency is reported by the inventors themselves. ACGANs can generate easily recognizable images but at the cost of losing diversity, resulting in a generative distribution significantly different from the target distribution. The intra FID scores indicate that the projection model reproduces diversity better than the concat discriminator. Images generated by projection show less mode-collapse compared to cGAN. Classes with high FID scores feature complex objects like humans, leading to some degree of mode-collapse even in high-quality images. The study prioritized training completion over using the most complex neural network. Increasing model complexity could enhance image quality and diversity. The new architecture allowed successful category morphism, demonstrated in FIG6 with an interpolated generator mixing parameters of conditional batch normalization layers. The interpolated generator output shows meaningful images even with significantly different y values. Fine-tuning with a pretrained model on the ILSVRC2012 classification task improved visual appearance by augmenting the cost function with a label classifier. However, this modification may sacrifice diversity for visual performance. The super-resolution task involves inferring high-resolution RGB images using a specific discriminator function. The model construction assumes unimodal probability distribution for p(y|x) even if p(x|y) is multimodal. The concat model was used for experiments, removing certain modules for evaluation. Refer to FIG3 in the appendix for network architectures used. The study focused on improving super-resolution by modifying the projection model, achieving clearer and smoother images compared to bicubic and concat methods. High and low resolution images were used, with generators updated 150K times and learning rate decayed linearly. The results showed significant improvements in image fidelity. The projection model achieved high inception accuracy and MS-SSIM compared to bicubic and concat methods. Multiple batches of images were generated with different random values to improve prediction accuracy. The research proposed a model for the discriminator of cGANs that significantly improved the performance of the generator on image generation and super-resolution tasks. The success with ensemble learning suggests a new way to enhance classification accuracy on low-resolution images. The choice of the discriminator's form is crucial for the generator and target distribution. The importance of the discriminator's form in conditional image generation on CIFAR-10 and CIFAR-100 was highlighted. The projection model outperformed other methods, utilizing concatenation at the hidden layer for improved performance. The projection architecture excelled over other methods by utilizing concatenation at the hidden layer for improved performance in conditional image generation on CIFAR-10 and CIFAR-100. Hyper-parameter search on CIFAR-100 was conducted to explore the impact of Adam hyper-parameters on the architecture's performance. The projection architecture outperformed other architectures with an inception score of 9.53, while concat architectures achieved a maximum score of 8.82. The best concat model for CIFAR-100 was the hidden concat with specific parameter values. The experiment followed the PPGNs model and incorporated an additional auxiliary classifier loss. The generator was trained with the original adversarial loss initially and then with the augmented loss. The learning rate hyperparameter values were consistent with previous experiments. The ResNet50 model from BID7 was used as the pretrained classifier. Results from the augmented objective function outperformed PPGNs in terms of inception score, but the method with auxiliary classifier loss did not improve the generative model. Spectral normalization was applied to each conv layer in the discriminator, and BID4 BID1 replaced the standard batch normalization layer. The ResBlock architecture for the generator implemented upsampling for super resolution tasks. The model implemented upsampling by concatenating the random vector z to the low-resolution image vector y before the first convolution layer. Downsampling and upsampling procedures followed BID6's implementation. Downsampling was done for the discriminator after the second conv of the ResBlock, while upsampling was done for the generator before the first conv of the ResBlock. ResBlocks performing downsampling and upsampling replaced identity mapping with 1x1 conv layers to balance dimensions. Upsampling was applied before the 1x1 conv for the ResBlock performing upsampling."
}