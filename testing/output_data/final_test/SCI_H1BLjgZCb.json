{
    "title": "H1BLjgZCb",
    "content": "Recent work on adversarial examples has helped evaluate the robustness of machine learning models by exposing scenarios where they fail. A proposed framework aims to generate natural and legible adversarial examples using generative adversarial networks, applicable to various applications like image classification and textual entailment. Recent work on adversarial examples has shown that manipulations of input data can lead to incorrect predictions from classifiers, raising concerns about the security of machine learning algorithms. Adversarial attacks have been successful even against state-of-the-art models like deep neural networks. These adversaries are generated effectively with access to the gradients of target models. Adversarial examples, generated with access to target model gradients, result in higher attack success rates than random noise perturbations. Training models with these adversaries can offer regularization benefits. However, these examples reveal \"blind spots\" in models but are unnatural and not likely to occur in real-world scenarios. Understanding the decision behavior of black-box classifiers in adversarial situations remains challenging, highlighting a gap between input and semantic spaces. There is often a mismatch between the input space and the semantic space we understand, leading to significant differences in input instances due to minor changes. Existing approaches for generating adversarial examples in text often result in ungrammatical sentences or require manual intervention. This paper introduces a framework to create natural adversarial examples that are meaningfully similar, valid, and helpful for interpretation. The proposed framework aims to generate natural adversarial examples that are meaningful and valid for interpretation. It utilizes generative adversarial networks (GANs) to map fixed-length vectors to data instances in a dense and continuous representation. Adversaries are searched for in the latent space neighborhood of the input instance, with the search range recursively tightened. Our approach generates natural adversarial examples that are more similar to the original input compared to existing methods. These adversaries provide insight into the classifier's behavior and are applicable to both image and text domains, aiding in the interpretation of black-box models. Our approach generates natural adversarial examples close to the input, aiding in interpreting black-box models for image classification, textual entailment, and machine translation. The goal is to generate adversarial examples for a given data instance that result in a different prediction from the black-box classifier, even without labeled training data. Our approach focuses on finding adversarial instances close to the input by searching in a dense representation of z space, rather than the original data representation. This method aims to generate valid and semantically close adversaries using generative models. Generative models like GANs learn to map noise to synthetic data close to training data. GANs involve a minimax game between a generator and a critic trained on unlabeled instances. The original GAN objective is difficult to optimize, so it is refined to improve performance. The Wasserstein GAN improves learning stability and provides useful learning curves. Further enhancements to the GAN framework, including WGAN structure and improvements, are discussed. Training a WGAN on corpus X generates natural examples close to the data distribution, using a generator G\u03b8 and inverter I\u03b3. The text discusses minimizing reconstruction error and divergence to encourage a normally distributed latent space. It introduces the concept of natural adversarial examples and perturbing dense representations to fool classifiers. Two search algorithms are proposed to identify adversaries using an inverter and generator. The text introduces a method to generate natural adversarial examples by perturbing inverted z and decoding perturbations via G \u03b8. A hybrid shrinking search strategy is proposed to efficiently find adversaries by tightening the search range with denser sampling in bisections. The text introduces a method for generating natural adversaries by perturbing z and using a hybrid shrinking search strategy to efficiently find adversaries without needing access to classifier gradients. This approach is demonstrated on various classifiers for visual and textual domains, showing promising results on datasets like MNIST and LSUN. The text introduces a method for generating natural adversaries by perturbing z and using a hybrid shrinking search strategy to efficiently find adversaries without needing access to classifier gradients. The approach is demonstrated on various classifiers for visual and textual domains, showing promising results on datasets like MNIST and LSUN. The WGAN is trained on 60,000 MNIST images with a generator and critic architecture, and adversaries are generated against Random Forests and LeNet classifiers treated as black-boxes. The text introduces a method for generating natural adversaries by perturbing z and using a hybrid shrinking search strategy to efficiently find adversaries without needing access to classifier gradients. Adversaries generated by FGSM look like the original digits eroded by uninterpretable noise. Natural adversaries against both classifiers are similar to the original inputs in style and shape, providing insights into classifiers' decision behavior. Adversaries against RF often resemble the original images more than those against LeNet, implying that RF is less robust in classification. We apply our approach to outdoor, color images of higher resolution, specifically focusing on the \"Church Outdoor\" category in the LSUN dataset BID30. By resizing images to 64\u00d764, we generate adversaries for three classifiers by perturbing the hypothesis. The training procedure is similar to MNIST, with observations on the robustness of classifiers like RF compared to LeNet. The training procedure for generating adversarial examples in WGAN involves deep residual networks and an MLP classifier with a test accuracy of 71.3%. Adversaries make natural changes to images to alter classifier predictions, such as sharpening roofs or converting objects into different categories. Generating coherent adversarial text is challenging due to the discrete nature of language. Our approach for generating textual adversaries involves perturbations in the continuous space z using an adversarially regularized autoencoder (ARAE) model. The ARAE model encodes sentences into continuous codes and performs adversarial training to capture the data distribution. An inverter maps these continuous codes into a Gaussian space of z \u2208 IR 100. Our approach involves using a 4-layer strided CNN for the encoder and training two MLP models for the generator and inverter to learn mappings between noise and continuous codes. We use the Stanford Natural Language Inference (SNLI) BID5 data for training, and focus on Textual Entailment (TE) task for evaluating common-sense reasoning in language. The approach involves using a 4-layer strided CNN for the encoder and training two MLP models for the generator and inverter to learn mappings between noise and continuous codes. The hypothesis is entailed by the premise, contradicts it, or is neutral. Adversaries are generated by perturbing the hypothesis to deceive classifiers while keeping the premise unchanged. Three classifiers are trained: an embedding classifier, an LSTM model, and TreeLSTM BID6. As classifiers become more accurate, they require more complexity. The text discusses the generation of adversaries in machine translation systems by testing translations for specific properties using a probing function. Adversaries are found using API access to the Google Translate model, with the goal of generating adversarial English sentences. The approach involves using a generator and inverter to deceive classifiers while keeping the premise unchanged. The text discusses generating adversarial English sentences by introducing specific German words into translations. A probing function tests for the presence of the word, such as \"stehen\" (\"stand\" in English). These adversaries do not expose vulnerabilities but can aid in language learning. More complex probing functions can target specific translation system weaknesses. The text discusses generating adversarial English sentences to test Google's English to German translation system. Adversaries are created by introducing specific German words into translations to expose vulnerabilities. These adversaries suggest a vulnerability in the system where a word acting as a gerund in English is dropped from the translation. The text demonstrates a vulnerability in Google's English to German translation system where gerunds are often dropped from the translation. The approach presented allows for evaluating black-box models without labeled data through experimental results on images and text data. The framework is applied to various classifiers for images and text, showing its usefulness in interpreting these models. The analysis suggests that more accurate classifiers require substantial changes to alter their predictions. The more efficient hybrid shrinking search algorithm is used in the experiments to quantify the extent of change for an adversary. The study quantifies the extent of change for adversaries by measuring the distance in the latent space (\u2206z = z * \u2212 z). Adversaries generated for each instance are compared against a group of classifiers to determine the impact on classifier predictions. Results show that more accurate classifiers require larger changes to inputs, as observed in experiments on MNIST and Textual Entailment datasets. The study evaluates the impact of generating adversaries on the accuracy of black-box classifiers by analyzing changes in inputs. By training neural networks with varying structures and hyperparameters, it is observed that more complex classifiers require larger changes to inputs. This trend is confirmed by experiments on MNIST dataset and by varying dropout rates. The study confirms a strong correlation between the average magnitude of change in adversaries and test set accuracy of classifiers. A pilot study with human subjects evaluates the naturalness of generated adversaries and their impact on classifier accuracy. In a pilot study, human subjects were presented with a questionnaire to evaluate the naturalness and closeness to the original image of adversaries generated against RF and LeNet classifiers for hand-written digits from MNIST. Results show that subjects found RF adversaries to be closer to the original image than LeNet adversaries. Additionally, comparisons were made between adversaries generated by FGSM and a different approach, with subjects agreeing that the latter made more natural changes to the original images. The fast gradient sign method (FGSM) is used to generate adversarial examples quickly by shifting the input in the direction of minimizing the cost function. An extension of FGSM proposed in BID16 applies it multiple times to increase the attack rate. The method has been validated in a pilot study for textual entailment tasks, showing that classifiers requiring more substantial changes to the hypothesis tend to be more accurate. Detailed user studies are left for future work. The Jacobian-based saliency map attack (JSMA) modifies input features greedily based on a computed saliency map to create adversarial examples. These examples can transfer between models, making it practical to evaluate machine learning systems in realistic scenarios. BID20 introduces a method to generate adversaries against black-box classifiers, including those without gradients like Random Forests. The noise added by these methods is uninterpretable, while natural adversaries provide insights into classifiers' decision behavior. Adversaries for text have received less attention, with BID14 creating adversarial examples for reading comprehension systems and BID19 introducing a framework to understand neural networks through representation erasure. Our approach perturbs the latent coding of sentences using generative models, resulting in legible generated sentences suitable for text applications like sentiment analysis and machine translation. The framework builds upon GANs, with the quality of generated examples directly influenced by GAN capabilities. In visual domains, GANs have shown promising results but are known for their training instability. Recent approaches aim to improve training stability and the objective function of GANs. For text generation, incorporating a discrete structure autoencoder with continuous code space regularized by WGAN has been proposed. In text generation, GANs are preferred for higher quality images over VAEs, which tend to produce blurrier ones. Future work will explore incorporating VAEs and GAN variants like \u03b1-GAN and Wasserstein Auto-Encoders. The iterative stochastic search algorithm for identifying adversaries is computationally expensive due to naive sampling and local-search methods. In text generation, GANs are preferred for higher quality images over VAEs. The efficiency of hybrid shrinking search is improved by using a coarse-to-fine strategy, resulting in a 4\u00d7 speedup with similar results as iterative search. Fine-tuning the latent vector with a fixed GAN refines generated adversarial examples. The assumption is that generated samples are within the same class if perturbations are small enough. In this paper, a framework is proposed for generating natural adversaries against black-box classifiers in both visual and textual domains. The adversaries are legible, grammatical, and meaningfully similar to the input, aiding in interpreting decision behavior and evaluating classifiers. The approach allows for evaluating black-box classifiers with substantial input changes. The framework proposed in this paper generates natural adversaries for black-box classifiers in visual and textual domains. These adversaries help interpret decision behavior and evaluate classifier accuracy without labeled training data. The approach utilizes recent advancements in GANs to create adversaries for various applications like image classification and machine translation. The code for generating natural adversaries is available at https://github.com/zhengliz/natural-adversary. Our approach generates natural adversaries for black-box classifiers by searching in latent space to find the closest adversary in semantic space. The iterative search process is outlined in Algorithm 1, with Algorithm 2 improving efficiency through a coarse-to-fine strategy. The search range is iteratively expanded in latent space until samples are generated that alter the classifier's prediction. Our approach generates natural adversaries for black-box classifiers by searching in latent space to find the closest adversary in semantic space. The hybrid shrinking search approach, detailed in Algorithm 2, tightens the upper bound of the optimal \u2206z four times faster than iterative search. The framework for continuous images adopts WGAN with gradient penalty and trains an inverter on top of the generator. Different architectures are used for MNIST digits and LSUN images datasets. The approach generates natural adversaries for black-box classifiers by searching in latent space to find the closest adversary in semantic space. Using a hybrid shrinking search approach, the framework adopts WGAN with gradient penalty and trains an inverter on top of the generator for MNIST digits and LSUN images datasets. The generator and critic are residual networks with specific architectures for different datasets. The approach generates natural adversaries for black-box classifiers by searching in latent space to find the closest adversary in semantic space. It uses a hybrid shrinking search approach and adopts WGAN with gradient penalty. The model includes a generator, an inverter, and two MLPs for mapping noise to continuous codes. The ARAE model is trained with autoencoder reconstruction loss and WGAN loss functions. The ARAE model is trained using a WGAN strategy and includes components like encoder, decoder, and generator. The inverter is trained on top of these components to minimize Jensen-Shannon divergence. The framework is trained on sentences up to length 10 from the SNLI dataset, generating grammatical and semantically close perturbations. Additional examples of generated adversarial hypotheses and probing functions are provided in tables. The ARAE model, trained using a WGAN strategy, includes encoder, decoder, and generator components. It is trained on sentences up to length 10 from the SNLI dataset to generate grammatical and semantically close perturbations. Examples of generated adversarial hypotheses and probing functions are provided in tables, including perturbations in semantic space. The ARAE model, trained using a WGAN strategy, includes encoder, decoder, and generator components. Trained on sentences up to length 10 from the SNLI dataset to generate grammatical and semantically close perturbations. Examples of generated adversarial hypotheses and probing functions are provided in tables, including perturbations in semantic space. Large trees. The man is lost in the woods. Contradiction: The man is crying in the woods. TreeLSTM: The man is lost in a bed."
}