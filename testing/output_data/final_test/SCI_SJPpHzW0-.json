{
    "title": "SJPpHzW0-",
    "content": "Influence-directed explanations aim to explain behavioral properties of deep neural networks by identifying influential neurons and interpreting the concepts they represent. The approach was evaluated on various datasets, showing its ability to localize features, distinguish instances, extract class knowledge, and aid in debugging misclassifications. This paper introduces influence-directed explanations for deep neural networks, focusing on convolutional neural networks. It involves identifying influential neurons to interpret concepts learned by the network, enabling a deeper understanding of the network's inner workings and the properties it seeks to explain. The paper introduces influence-directed explanations for deep neural networks, focusing on convolutional neural networks. It identifies influential neurons in higher layers to interpret general concepts learned by the network. A key contribution is distributional influence, a measure for internal neurons that can explain different properties of interest. The influence measure introduced in the paper aims to explain different properties of interest in a network, focusing on causality, distributional faithfulness, and flexibility. It identifies influential parts of the network that have the most impact on outcomes and ensures evaluation within the input distribution. The approach is evaluated on various datasets using convolutional neural networks, demonstrating its effectiveness in providing explanations. The evaluation of influence-directed explanations on different datasets shows their ability to characterize why inputs were classified a certain way, explain classification choices, localize reasons for classification, extract network learning, and aid in debugging misclassifications. Previous work on interpreting CNNs has focused on identifying relevant parts of an input image for a neuron and maximizing neuron activation. One approach to interpreting predictions for convolutional networks is to map activations of neurons back to relevant regions in the input image. This can be achieved by visualizing gradients or propagating activations back using gradients. Another approach is to visualize features learned by networks by identifying input instances that maximally activate a neuron. This differs from prior work by examining the causal influence of neurons rather than just their activations. Examining causal influence of neurons in a network for classification is more effective than analyzing their activations. The explanations provided are parametric in a distribution of interest, allowing for granularity in understanding network behavior. Identifying \"expert\" neurons for certain distributions is possible, unlike the generalization issues seen with input influences. The explanations also focus on a quantity of interest, providing insights into different system behaviors. In this section, distributional influence is proposed as a family of measures of influence parameterized by a quantity of interest and a distribution of interest. It is the average partial derivative over the distribution of interest and is the only measure that satisfies certain criteria. The choice of influence measures is guided by axiomatic choices different from prior work, with a focus on distributional faithfulness criteria. This approach imposes a weaker distribution marginality principle compared to Integrated Gradients, leading to constraints on acceptable baseline images. In Section 2.2, the distributional influence measure is introduced as the only measure satisfying specific properties. Section 2.3 extends this measure to internal neurons. Quantities of interest in networks are represented as functions f from X \u2192 R, with a distributional influence measure denoted by \u03c7 i (f, P ) for input i, quantity of interest f, and distribution P over X. The measures are parameterized by f and P, with examples including outcomes for 'cat' class or 'cat' versus 'dog'. The curr_chunk discusses different distributions of interest in influence measures for machine learning models, such as single instances, class essence, and overall population influence. It also mentions a measure called Integrated Gradients. The space of influence measures is narrowed down using three axioms. The curr_chunk introduces three axioms for measuring influence in machine learning models: linear agreement, distributional marginality, and distribution linearity. These axioms uniquely define the measure of interest and ensure that influence is determined by the behavior of the model within the input distribution manifold. The third axiom, distribution linearity, states that influence measures are properly weighted over the input space, giving lesser weight to infrequent regions compared to more frequent regions. The only measure that satisfies linear agreement, distributional marginality, and distribution linearity is the weighted gradient of the input probability distribution. This measure can also be used to assess the influence of an internal neuron in a network. The influence of an internal neuron in a network can be measured by defining a slice of the network, which is a partitioning that exposes its internals. The influence of an element in the internal representation is determined by certain axiomatic properties on the network structure. The weighted gradient of the input probability distribution is the measure that satisfies these properties and can be used to assess neuron influence. The influence of internal neurons in a network is measured by defining slices that expose its internals. Axiomatic properties on the network structure determine this influence, with the weighted gradient of the input distribution being the measure that satisfies these properties. Two slices are considered equivalent if they represent the same function for a given outcome, and the only measure that satisfies these properties is presented above. The explanation framework discussed in this section generalizes other influence-based frameworks like saliency maps and integrated gradients. It offers richer explanations by exploring axes left unexplored by previous works, allowing for explanations for slices, various quantities, and distributions of interest. The framework provides flexibility to answer specific queries about a model's behavior. The framework discussed offers flexibility to confirm or refute hypotheses about a model's behavior by computing influences on a network slice. This helps determine the relevance of neurons in intermediate layers to specific network behaviors. Influence measurements on a slice can reveal which features in an image were relevant to the network's prediction. The visualization of units in the model's softmax output corresponding to the correct label was obtained by measuring the influence of input pixels along each color channel. The resulting interpretation shows distinct regions in the original image, such as the left eye and mouth. This method was compared with integrated gradients to explain classes \"sports car\" and \"convertible\" using the top-three most influential units at the conv4_1 layer of VGG16. The influence-directed explanation of the network's internal units at the conv4_1 layer of VGG16 is better at localizing features used in prediction compared to integrated gradients. Influence-directed explanations are parameterized by a quantity of interest, allowing flexibility in interpretation and answering counterfactual questions. The comparative quantity of interest between classes in a softmax classification model is calculated as f Li \u2212 f Lj. This captures the model's tendency to classify instances as L i over L j. An example using a VGG16 model trained on ImageNet dataset is shown in Figure 2(a), where influence is measured against the leaf class \"convertible\" at the conv4_1 layer. The interpretation is computed on the top-three most influential units at this layer. The most influential unit at the top of the vehicle corresponds to the hard top, a distinctive feature of the sports car class. The explanation isolates the distinguishing features between \"sports car\" and \"convertible\". Influence measurements were taken on the DR model at the bottom-most fully connected layer, conditioned on different classes. In figure (a), an instance from class 5 is correctly classified, while in figure (b), an instance from class 5 is incorrectly classified as class 1. The distribution of interest in influence-directed explanations can vary, with some using a point mass defined over a single instance. Defining the distribution over a larger set of instances can provide more generalized explanations. Support over a larger set of instances can yield explanations capturing common factors in network behaviors. These explanations show the essence of what the network learned about a class and can help understand problematic behaviors when debugging. Visualizations in Figure 3 illustrate this concept, generated by measuring influence on a layer of the Inception Diabetic Retinopathy model. The units in the layer are sorted by influence, with the top-left corner having the largest positive influence and the bottom-right the largest negative influence. The visualizations in Figure 3 demonstrate how influences on a layer of the Inception Diabetic Retinopathy model are measured. The size and shape of the boxes represent the magnitude and sign of the corresponding unit in the class-wide ordering. The influences align closely with the order determined by the distributional influence measurements for correctly classified instances, while they are more random for incorrectly classified cases. This highlights that relatively few units are highly influential towards a particular class, referred to as the \"essence\" of the network's learning. The essence of a class is determined by units highly influential towards it. These units can be isolated to create expert models that excel at classifying instances. Influence measurements are used to compress models and reveal class-specific essence. The slice compression method involves discarding non-selected units to create class-specific experts for better classification performance. Influence measurements help identify key units for extracting experts from large networks efficiently. The slice compression method involves selecting key units based on influence measurements to create class-specific experts for improved classification performance. Concrete values for selecting units are determined through parameter sweep, resulting in experts with high precision and recall. The slice compression method selects key units based on influence measurements to create class-specific experts with improved classification performance. The experts show better recall without a difference in precision, indicating that top and bottom influential neurons capture the essence of a class. This suggests that other neurons may capture spurious correlations that hurt performance, while the essence of the class is a better indicator of class membership. Influence-directed explanations can aid in debugging models by interpreting influence measurements to understand reasons for incorrect behavior. Demonstrated in the context of Diabetic Retinopathy classification, the method replicates prior work to build a model and target questionable explanations. To predict the severity of Diabetic Retinopathy, a convolutional network was built using a Kaggle dataset. The dataset is skewed towards Class 1, with fewer images from other classes. Results were not reproducible with the initial architecture, but using the Google Inception network and specific data preprocessing methods, performance matched prior work. The model's performance matched prior work by Pratt et al. with a similar pattern of misclassification. The bug of no images being predicted as Severity-2 DR is being investigated to determine if the network learned distinguishing features for class 2. The bias in the distribution may have led to a preference for class 1 and 3 predictions despite the presence of class 2 concepts. The \"drowning experts\" hypothesis suggests that in the model, experts for class 2 are outweighed by experts for other classes, leading to misclassifications. Techniques can be used to identify experts for class 2, even if they sacrifice some precision. After identifying experts for every class except class 2, it was observed that the model may not have learned class 2 correctly. Features distinguishing classes 2-5 are lesions and defects of increasing size, intensity, and frequency. Class 2 is differentiated from class 1 by small features, but preprocessing steps may have blurred these distinctions. Removing the blur and retraining the model confirmed that data labeled as class 2 was often indistinguishable from class 1. After identifying experts for every class except class 2, it was observed that the model may not have learned class 2 correctly due to preprocessing steps blurring distinctions. Retraining the model without the blur confirmed that data labeled as class 2 was often indistinguishable from class 1. Using influence-directed explanations, the absence of units responsible for predicting class 2 pointed to a step in the data preprocessing phase as the cause of the issue. This diagnosis and repair process highlighted the importance of being able to define quantities of interest and understand the model's decisions. The limited depth of network architecture was observed, and the distributional influence measure is expected to be applicable to various deep neural networks. Future work includes coupling the measure with interpretation methods for producing influence-directed explanations for different types of deep networks. Comparison to prior work on explaining CNN decisions is presented in Table 1, highlighting the framework properties and flexibility of different explanation methods. Decomposition-based explanations can be computed for internal neurons in higher layers and bottom-most input layer. Frameworks like Integrated Gradients, Sensitivity Analysis, and Simple Taylor Decomposition attribute relevance solely to input features. Comparison of influence-directed explanations to prior work is shown in Table 1. The curr_chunk discusses different explanation frameworks and their capabilities, such as flexibility in quantity and distribution, and the ability to characterize internal neurons. It also mentions properties of underlying influence measures like faithfulness, sensitivity, and completeness. The explanation frameworks are compared to prior work in Table 1. The curr_chunk provides a detailed discussion on Sensitivity and Completeness in explanation frameworks. It highlights the importance of using influence measures that satisfy sensitivity to avoid blind spots or focusing on irrelevant features. The influence-directed explanation framework presented in this paper supports features in Table 1, with distributional faithfulness and completeness contingent on parameter selection. Ensuring distributional faithfulness is achievable by using the source distribution or its variants. This feature allows for relevant explanations in counterfactual scenarios. The experiments in this paper focus on using distributions corresponding to a single point from the training data or the marginal distribution of instances in a given class. Completeness is not insisted upon in all cases to allow for the derivation of various useful classes of explanation, such as filtering out irrelevant information in explanations. The explanations shown in FIG0 and 2 were computed by identifying the most influential filters in an intermediate layer on a relevant quantity of interest. The experiments focus on using distributions from training data or marginal instances in a class. The explanations highlight influential filters in an intermediate layer on a relevant quantity of interest. Integrated gradients offer flexibility in computing attributions but are limited to linear combinations of baseline and input instances. The integrated gradients method provides faithful attributions by combining baseline and input instances, with the choice of baseline affecting the distributional faithfulness. The simple Taylor decomposition is a special case that aggregates attributions for the baseline and input instance, supporting flexibility in the distribution of interest through baseline calibration. The distributional faithfulness in relevance propagation depends on the appropriate selection of a baseline instance. Layer-wise Relevance Propagation generalizes the propagation rule used in back-propagation methods, ensuring completeness but not always sensitivity. The only measure satisfying linear agreement, distributional marginality, and distribution linearity is given by Equation 1. The distributional marginality and distribution linearity are defined by Equation 1. By distributional marginality, it is shown that any distribution can be written as a combination of its marginal distributions. The slice invariance axiom is also discussed in relation to j-equivalent slices. Preprocessing involves proving the slice invariance property for j-equivalent slices by taking partial derivatives and aggregating them over the input space. This property shows that the influence on a neuron only depends on the relationship between input, neuron, and outcomes. The text chunk discusses precision and recall results for model compression on a randomly-selected class from ImageNet using the VGG16 network. The results show that strong results can be achieved by selecting relatively few units for compression. Influence measures in cooperative game theory focus on attributing outcomes to participants. Two key properties are the marginality principle and efficiency. The marginality principle states that an agent's attribution depends only on its own contribution. The axiom of distributional marginality is a weaker form of this principle. The distributional marginality principle requires equality of attribution when partial derivatives are the same in the distribution. Efficiency ensures that attributions add up to the total value generated, defining the Aumann-Shapley Value BID0. The Aumann-Shapley Value is used for attributions in BID12 with efficiency as a justification. The Aumann-Shapley value can be recovered by choosing the distribution of interest as the uniform distribution on the line segment joining an instance x and a baseline image b. Certain choices of baselines can be problematic for distributional faithfulness. The distributional marginality principle requires equality of attribution when partial derivatives are the same in the distribution. Efficiency ensures that attributions add up to the total value generated, defining the Aumann-Shapley Value BID0. In BID12, the baseline chosen is the zero vector, representing scaled images within distribution. Certain baselines can pose challenges for distributional faithfulness."
}