{
    "title": "rJl2E3AcF7",
    "content": "Computations for the softmax function in neural network models can be expensive with a large number of output classes, impacting training and inference efficiency. The Doubly Sparse Softmax (DS-Softmax) method presented in this paper improves softmax inference efficiency by learning a two-level class hierarchy. Each expert is responsible for a subset of the output class space, reducing computation without loss in performance. Deep learning models often use the softmax function for classification tasks, but its computation complexity can be a bottleneck, especially in models with large output dimensions like language modeling and face recognition. Various methods have been proposed to reduce softmax complexity for training and inference, such as sampling-based and hierarchical-based approaches. These methods aim to decrease training time and improve efficiency, particularly on limited computational resources like mobile devices. Our work aims to improve the inference efficiency of the softmax layer by proposing a novel Doubly Sparse softmax (DS-Softmax) layer. This method learns a two-level overlapping hierarchy using sparse mixture, aiming to accurately and efficiently search for top-K classes during inference. The DS-Softmax method reduces softmax computation significantly by using a sparse mixture of experts to select the most related expert for input, resulting in a dramatic speedup in tasks like language modeling and translation without sacrificing prediction performance. The DS-Softmax method utilizes a sparse mixture of experts to reduce softmax computation, achieving significant speedups in language modeling and translation tasks while maintaining performance. The approach focuses on learning a two-level overlapping hierarchy, differentiating from prior methods that rely on predefined hierarchies. BID29 designed a sparsely gated mixture of experts model to improve performance in language modeling and translation by using a few experts selected by a sparsely gating network for computation on each example. They utilized group lasso to sparsify the experts, reducing the number of nodes in the neural network. Our method extends Goodman's two-level hierarchy for language modeling by allowing overlapping hierarchies, making it easier to predict words based on their properties. This approach increases efficiency by narrowing down the search for the correct answer within specific word clusters. Doubly Sparse softmax (DS-Softmax) is designed to capture a two-level overlapping hierarchy among output classes, where a sparse mixture selects the right expert/cluster and sparse experts divide the output space into small class clusters. Inspired by BID29, DS-Softmax significantly speeds up inference by searching a small subset of the output space. The DS-Softmax model uses a sparse gating mechanism to select experts, with only the top expert chosen for faster inference. The gating values are calculated and normalized before selection, allowing back-propagation of gradients. The DS-Softmax model utilizes a sparse gating mechanism to select experts, with only the top expert chosen for faster inference. The gating values are calculated and normalized before selection, allowing back-propagation of gradients. Eq. 1 enables gradient back-propagation to the entire weight vector despite only selecting the top-1 expert. The probability of class c under context h is computed using Eq. 2, where the gating values influence the final categorical distribution produced by the chosen expert. Smaller gating values result in a more uniform distribution, while larger values lead to a sharper one. The DS-Softmax model uses sparse experts with group lasso for sparsification. Pruning is done during training to reduce memory requirements. Sparsity percentage and utilization are key factors for speedup in training. The method called mitosis training is used to reduce memory requirements by gradually increasing the number of experts during training. This approach ensures sparsity is maintained while improving utilization for speedup. Group lasso loss is incorporated to promote exclusiveness of classes among the experts. The method of mitosis training reduces memory requirements by increasing the number of experts gradually. This maintains sparsity and improves speedup. The final training algorithm combines various contributions and is evaluated on synthetic and real tasks, including natural language modeling and neural machine translation. Other layers are pretrained except for the DS-Softmax layer for faster convergence. The DS-Softmax layer was not pretrained for faster convergence. Hyper-parameters like \u03bb load and threshold remain constant. \u03bb lasso and \u03bb expert share the same value but need tuning based on validation set performance. A two-level hierarchy synthetic dataset was used with sizes of 10x10 and 100x100. DS-Softmax effectively captured the hierarchy. Group lasso and balancing were found to be important for the model. Language modeling typically involves a large output dimension, with PennTree Bank (PTB) being one of the standard datasets used. For word level language modeling, standard datasets like PennTree Bank and WikiText-2 are used with output dimensions of 10,000 and 33,278 respectively. A two-layer LSTM model with 200 hidden size is utilized, and accuracy metrics like Top 1, Top 5, and Top 10 are preferred over perplexity. In neural machine translation tasks, IWSLT English to Vietnamese dataset is employed with evaluation based on BLEU score. The vanilla softmax model seq2seq is implemented using tensorflow with dataset preprocessing similar to the original implementation. For Chinese handwriting character recognition, the offline and filtered CASIA dataset BID17 is used. The dataset contains over four thousand characters, with some special characters removed for better model performance. Two-thirds of the data is used for training and the rest for testing. Real device experiments were conducted on a machine with Two Intel(R) Xeon(R) CPU @ 2.20GHz and 16G memory. Different configurations of SVD-Softmax BID30, SVD-5, and SVD-10 were evaluated, showing significant speedup in testing accuracy. In the evaluation of SVD-Softmax BID30, two configurations, SVD-5 and SVD-10, were assessed using top 5% and 10% dimensions for final evaluation. Indexing and sorting are computationally heavy for SVD-softmax with Numpy implementation. Latency results are reported without sorting and indexing for SVD-softmax, while full latency is reported for full softmax and DS-Softmax. DS-Softmax shows better speedup and lower latency compared to other models. The mitosis training scheme on PTB language modeling task involves initializing with 2 experts and cloning up to 64 experts sequentially, requiring significantly less memory for training the DS-64 model. The final model has significantly smaller memory and less than 1.5 redundancy, indicating small memory requirement for inference. Sparsity increases with longer training on language modeling task with PTB dataset. High gating values indicate higher certainty. Words chosen for experts are semantically related, with high frequency words appearing in more experts. This phenomenon is similar to topic models in BID4 BID35. In many use cases of softmax inference, only top-K most probable classes are needed. Existing methods for speeding up softmax inference focus on finding top-K classes post-training. Methods like SVD-Softmax decompose the softmax embedding matrix using singular value decomposition. Approximation methods like Locality Sensitive Hashing and small word graph are also popular. These methods depend on a well-learned softmax and can be costly for high precision requirements. Our method falls under the category of learning-based methods that aim to speed up inference by partially activating parameters. It learns a two-level overlapping class hierarchy, where each class can belong to more than one expert. The proposed method, doubly sparse, is a sparse mixture of sparse experts for efficient softmax inference, trained end-to-end. Our method is trained end-to-end to learn a two-level overlapping class hierarchy with experts responsible for specific output classes. In experiments, we use a data generation process with d = 10. The model is initialized with 2 experts and gradually cloned to have 4, 8, 16, 32, and 64 experts, reducing memory usage significantly. The DS-64 model requires significantly less memory for training compared to traditional methods. A redundancy/overlapping pattern is demonstrated, with key words classified into money, time, and comparison related groups. Money related words include \"million\", \"billion\", and \"trillion\", time related words include weekday names, and comparison related words include terms like \"up\", \"down\", and \"growth\"."
}