{
    "title": "Sktm4zWRb",
    "content": "Value iteration networks are used in robot motion planning, specifically for planetary rovers. The soft value iteration network (SVIN) is introduced to improve training gradients by incorporating a soft policy model. This model represents the policy as a probability distribution over all possible actions, rather than a deterministic policy. The effectiveness of SVIN in robot motion planning is demonstrated. In robot motion planning scenarios, the proposed method of using Value Iteration Networks (VIN) is demonstrated, with a focus on planetary rover navigation challenges. The architecture includes the VIN and reward network components for supervised learning in robot path planning. The approach incorporates a differentiable planning component into a reinforcement learning framework. The approach involves using Value Iteration Networks (VIN) for robot path planning, with a focus on 2D grid path planning problems. The learned component exists within the reward network, which must produce a well-behaved reward function for planning. Calculating a reward function for desired planning results can be challenging due to balancing costs and rewards of different terrain types. Planetary rovers, like those on Mars, face navigation challenges due to bandwidth constraints and communication delays with Earth. Current rover driving techniques rely on surface imagery for planning, limited by terrain variations and camera capabilities. Orbital imagery offers unlimited distance views but at lower resolution. Orbital imagery can provide longer range rover plans to reduce reliance on Earth for navigation. Autonomous driving techniques lack high-level planning abilities for advantageous path selection. The goal is to use orbital imagery to formulate algorithms for future rover navigation. In this work, an imitation learning architecture based on value iteration networks is presented for solving navigation planning problems using historical rover driving data. The architecture relies on orbital data to produce plans beyond visible ranges and integrates with existing local planners for safety critical systems. Deep reinforcement learning is also utilized in this approach. The text discusses the use of deep reinforcement learning techniques in modeling action policies for robot navigation. Value iteration networks are utilized tightly bound to terrain maps and state transitions. Other researchers have explored using neural networks for dynamic programming and planning. Imiation learning for navigation, such as the LEARCH technique, has also been studied. The text discusses imitation learning with DNN policies for robot navigation in a Markov Decision Process framework. It defines policies, trajectories, value functions, and Q functions conditioned on a policy \u03c0. The text discusses the value iteration algorithm for finding the optimal value function in a planning problem. It adapts elements from the Value Iteration Network architecture and emphasizes the use of expert-designed local control algorithms. The text discusses the value iteration algorithm for operating planetary rovers, incorporating expert-designed local control algorithms. The data flow involves stacking visual map data, a one-hot goal map, and relevant data layers, passing through network f R to produce a reward map for the value iteration module. The output from the VI module is used for training or deployment, where it can be fed to an expert-designed local planner for planning decisions during deployment. The architecture involves training parameters in the network f R using supervised learning techniques. The network receives orbital imagery, stereo elevation data, surface roughness estimates, and a goal position as input to calculate the value for planning decisions. The rover uses expert algorithms for navigation within its sight radius, combining path costs and value estimates to choose plans. Replanning occurs as resources allow, with the value iteration module approximating value iteration for a fixed number of iterations. The architecture involves training parameters in the network using supervised learning techniques and receiving input such as orbital imagery, elevation data, roughness estimates, and a goal position for planning decisions. The value iteration module in the network uses reward map and convolutional kernel inputs to produce Q values, which are then collapsed into the next value map. While the traditional algorithm requires max pooling, an alternative approach is proposed. Updates are typically done using the optimal action policy of choosing the highest Q value, but in value iteration networks, an effective gradient is also considered through a reward function parameterized by \u03b8. The value iteration algorithm in the network uses reward map and convolutional kernel inputs to generate Q values, which are then condensed into the next value map. A modification to the value iteration algorithm is proposed to enhance network training by incorporating more effective gradient values, particularly in the early training stages. This modification involves performing value iteration under a probabilistic action policy assumption rather than an optimal action policy. The proposed imitation learning framework uses a probabilistic action policy for training the network, leading to higher accuracy results. Experiments show that the 'soft' action model produces better performance. The framework is tested on two datasets, with the main dataset being synthetic. The algorithm's performance is compared with other methods. The proposed algorithm is compared with other methods on two datasets, including Curiosity Mars rover data. Performance is tracked using loss and accuracy metrics, with a focus on accuracy during training. The Cumulative fractional area (CFA) model for rock distribution on Mars surface serves as inspiration. The rock distribution model on Mars surface, inspired by the CFA model, creates a set of rock world maps. These maps are discretized into 32x32 binary obstacle/free traversibility maps for pathfinding. The dataset consists of 1000 maps with approximately 50 paths per map. The performance of the SVIN algorithm is evaluated on this dataset. The rock-world data is split into 90% training and 10% test sets. The reward network used for the rock-world consists of 2 convolutional layers. The SVIN algorithm achieves high performance (90%) on the rock-world dataset, outperforming the hard action model which plateaus at 70-75% accuracy. The optimal upper performance bound is less than 100%, with the SVIN algorithm reaching over 90% of the optimal solution. Vector fields display optimal policy planning results, with some deviations from the learned policy actions. The rock-world network is trained with k = 64, longer than most paths but shorter than some in the dataset. The MSL \"Curiosity\" rover has been on Mars for over 5 years, driven over 17km. Supervision behavior is imitated by generating a dataset of rover driving data from overhead imagery. The dataset consists of segments of rover driving data generated from overhead imagery taken from the MSL 'basemap'. The imagery has a resolution of 25cm per pixel and is used to create 64x64 tiles of the rover driving path. A digital elevation map with a resolution of 1 meter per pixel is also created from stereo interpolation of the imagery. A test set is separated from the data, representing a little more than 20% of the samples, to account for systematic variations in terrain appearance as the rover drives into new types of terrain. The dataset consists of rover driving data segments from overhead imagery taken from the MSL 'basemap', with a resolution of 25cm per pixel. A test set accounts for systematic variations in terrain appearance as the rover drives into new types of terrain. The dataset includes about 4700 tiles, with path selection complicated by the rover's behavior of diverting to examine geological features. Noise in the dataset can be reduced by identifying the rover's sub-goals as break points in dividing up the path, which is a subject of future work. A more complex reward network with three convolutional layers is used for this dataset. The SVIN approach, utilizing three convolutional layers, aims to solve motion planning problems through imitation learning by transforming map data into a calibrated reward function. The network achieved 45% accuracy on the noisy Mars navigation dataset, with potential for further improvement through extended training. In future work, the focus will be on improving performance on the Mars dataset by augmenting it with synthetic simulation data, using a pre-trained reward network, and selecting correct sub-goals on rover paths to reduce noise and enhance learning results."
}