{
    "title": "B1lXGnRctX",
    "content": "Combining information from different sensory modalities to execute goal-directed actions is crucial for human intelligence. This study focuses on the problem of retrieving a target object from a drawer using only tactile sensing after being shown an image of the object. Success in this task requires integration of visual and tactile sensing, demonstrated through a method using an anthropomorphic hand in a simulated environment. Future research in this area aims to further explore the combination of sensory modalities. The ability to integrate and translate information between sensory modalities is a core aspect of human intelligence. For example, we can easily retrieve objects from a drawer using touch sensation, even without visual feedback. This translation of information between sensory modalities is not limited to tactile and vision. Humans can easily translate information between sensory modalities, such as hearing a sound and visualizing someone walking down stairs. This ability allows for learning useful representations of sensory inputs, as seen in self-supervised learning where visual features are learned by predicting corresponding sounds, ego-motion, depth, or color values from grayscale images. Performing goal-directed actions in partially observable settings is another critical use of sensing from multiple modalities. For example, in the scenario of retrieving objects from a drawer, tactile sensing becomes crucial when there is no light source available. This integration of vision and touch is highlighted in a study where participants struggled to pick up and light a matchstick when their hands were anesthetized. Other examples include pedestrians being alerted by the sound of a car approaching from behind and animals in the jungle detecting a tiger's presence by the sound of movement. In this study, the experimental setup involves retrieving objects from a drawer using tactile sensing and vision. The agent must explore spatially to locate the objects and translate visual information into tactile representation to retrieve them by touching. The agent explores spatially to locate objects in a drawer and uses tactile sensing to determine if the object needs to be retrieved. It learns a mapping from visual to tactile signals through unsupervised exploration, enabling it to compare the expected tactile response of each object by touching them. The agent explores objects in a drawer by touching them to compare tactile responses with expectations. Leveraging results from image classification, the agent classifies objects from tactile signals, showing that the learned representation can generalize to retrieve novel objects. This work builds on the idea of using haptics as a sensory modality to explore the world. Haptics as a sensory modality for exploring the world is crucial for object recognition. Lederman and colleagues discuss exploratory procedures for understanding object properties. Multi-modal learning is essential for biological agents to build object models. Previous work in haptic exploration focused on hand-engineered features for object recognition. Challenges include robust sensors and effective object exploration control. The curr_chunk discusses various approaches to measuring physical properties of objects using bio-tac sensors and different exploration procedures. Different studies employ hand-engineered features and dynamic feature models for object classification and tactile sensing for grasping. Results show the importance of leveraging tactile sensing in object manipulation. The curr_chunk discusses the use of tactile sensing for object manipulation and classification. Various studies have shown the effectiveness of combining visual and haptic information for better classification of haptic properties. Additionally, employing tactile inputs in learned models can improve predictions of graspability. Some research suggests that tactile features may not be necessary for certain constrained in-hand manipulation tasks. The authors employ a setup with rich 3D data and a powerful learning method to navigate around tactile sensing requirements. The agent explores objects using pre-determined routines combining palpation and grasping-like movements to identify and grasp the target object. Movement and grasping are done using fixed routines, with the hand being translationally fixed but able to rotate. The fingers can explore movements with restrictions imposed by joints. Haptic forces and images are collected for each episode of 500 time steps. The dataset includes 500 samples per object, each with 19 dimensions. A simulated model of an anthropomorphic hand is used for evaluation, based on the SHAP test suite. This hand was built for prosthetic evaluation and inspired a similar model for the DARPA Haptix challenge. A simulated model of an anthropomorphic hand with five fingers and 22 joints, many of which are tendon coupled, was built using the Mujoco physics engine. Thirteen joints are actuated, with ten controlling finger motion and three controlling hand rotation. The hand has 16 degrees of actuation along the (x, y, z) axis, controlled by setting the position of 16 actuators. Additionally, the hand is equipped with 19 contact sensors to measure normal contact forces. In our setup, we have two sets of networks. Network f1 predicts haptic responses for objects based on images at time t. Another network, f2, predicts object identities using the haptic responses. This allows for effective discrimination of objects. During testing, an image is presented to the model, and predicted haptic responses are used to determine the object category. To train the haptics predictor network, grayscale 64x64 images with the focus object centered were used. The network had three convolutional layers with filters 32, 32, 32 and kernel size 5x5. The output was fed into fully connected layers to predict 19 dimensional haptic forces. The network was trained using ADAM with a learning rate of 1e-4. For the object discriminator network, average haptic forces with 19 dimensions were used as inputs. The network for haptics prediction used 19-dimensional inputs passed through fully connected layers. Cross entropy loss was minimized using ADAM with a learning rate of 1e-4. Normalization of input images and haptic forces was crucial for training. A scale term was introduced for haptic forces to align target data distribution with the network output range. Objects used in experiments are shown in Figure 3. In the experiments, a set of 25 objects from the ShapeNet dataset was used, each presented in various poses. Three sets of experiments were conducted to study object identification using tactile sensing, exploration length, and identifying objects in the dark. Training involved 400 samples per object category with inputs being average haptic forces. During training, 400 samples per object category were used with random rotations about the z-axis. A total of 4400 samples were used. Evaluation was done with 50 samples per object, and during testing, 50 samples per object with unseen poses were provided. The model achieved near 100% accuracy in identifying objects using haptic forces on known object samples. The model achieved near 100% accuracy in identifying objects using haptic forces on known object samples. A separate tactile classification network was trained with different numbers of samples obtained by exploration, showing that about 100 samples are sufficient for accurate classification. Performance only marginally improves when 500 samples are collected. The study showed that object identification accuracy using haptic forces improved marginally with 500 samples collected. Results indicated that classification accuracy increased significantly with more time steps and saturated quickly. However, it remains unclear if object retrieval is possible based on tactile sensing alone when only an image of the object is presented. The study presented an experimental setup where an input image of an object is used to predict haptic responses and classify objects using a nearest-neighbor classifier in the latent space of the network. The performance of the classification was compared using raw haptic predictions and embedded haptic predictions. The model was trained on 11 objects and tested on 14 novel objects, with the difficulty of the task increasing with the number of objects presented. The study compared classification accuracy using raw haptic predictions and embedded haptic predictions for identifying novel objects. Results showed an increase in accuracy with haptic embedding. The model was tested on 14 objects, with task difficulty increasing with the number of objects. The study compared classification accuracy using raw haptic predictions and embedded haptic predictions for identifying novel objects. Results showed an increase in accuracy with haptic embedding. The model can identify novel objects using tactile sensing only, with task difficulty increasing as the number of objects presented in a trial increases. The model suggests training a RL agent to explore objects for identification, and optimizing networks jointly to maximize object identification and smooth prediction errors. The MuJoCo simulator provides fast physics solvers for computing contact forces, but only normal forces are computed. Research suggests exploring tactile sensing on robot grippers to measure a variety of forces. Training networks in a supervised manner for object identification is impractical for real robots. It would be more interesting to pose the problem as self-supervised and learn to identify novel objects using tactile sensing."
}