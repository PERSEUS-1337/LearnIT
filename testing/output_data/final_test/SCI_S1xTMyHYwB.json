{
    "title": "S1xTMyHYwB",
    "content": "To understand deep neural networks, we study untrained random weight CNN-DCN architecture. Training DCN for random-weight CNN converges quickly and yields high-quality image reconstruction. Investigating network width, depth, random channels, and kernel size impact on reconstruction quality. Deep neural networks have achieved impressive performance on various machine learning tasks. However, our understanding of how these models operate remains limited. Existing works propose mathematical models for learning architectures, but fail to capture state-of-the-art architectures. Other studies leverage compressive sensing or ordinary differential equations to facilitate understanding of CNNs. In this paper, the authors aim to bridge the gap between empirical observations and theoretical explanations of convolutional neural networks (CNNs), specifically focusing on the invertibility of the overall random CNN-DCN architecture. They propose applying randomization on deconvolutional networks (DCNs) to systematically investigate deep representations and provide insights into the intrinsic properties of deep convolutional networks. Various visualization techniques are utilized to understand the deep representations of intermediate layers in CNNs. The authors aim to bridge the gap between empirical observations and theoretical explanations of convolutional neural networks by studying the invertibility of the random CNN-DCN architecture. Training the DCN for reconstruction with random CNN yields richer information and faster convergence compared to pre-trained CNN. This leads to curiosity about the performance of a CNN-DCN architecture with random weights. Random weights in feature learning architectures have shown satisfactory validity in object recognition tasks. Certain convolutional pooling architectures, even with random weights, exhibit frequency selectivity and translation invariance, potentially enabling fast network architecture search. Studying complex systems with random weights can provide a better understanding, such as breakthroughs in compressed sensing with random sampling. Investigations on random deep neural networks have also been conducted for highly complicated systems with nonlinear operations. Our work focuses on studying random deconvolutional networks to analyze the feature representation of deep convolutional nets. We investigate the randomness in deconvolutional networks without any training, and surprisingly achieve satisfactory quality in inverting images while preserving geometric and photometric features. This research provides new insights and potential applications for untrained deep neural models. The CNN-DCN architecture with random weights can effectively preserve geometric and photometric features in image reconstruction without the need for pre-training on deep models. Extensive empirical studies have shown its effectiveness in texture synthesis, style transfer, image segmentation, and image inpainting. This approach saves time and energy and offers flexibility in trying different neural networks. Deconvolution involves a CNN-DCN architecture for inverting intermediate features to original images. Randomization involves stochastic weight assignment in deep neural networks. Deconvolutional networks are commonly used for deep feature visualization and feature projection back to input pixel space. Zeiler et al. propose a multi-layered deconvolutional network for this purpose. Randomization in neural networks dates back to the 1960s with random binary connections in shallow networks. Recent studies have shown that randomization is computationally cheaper than optimization, leading to its resurgence in research. Dosovitskiy et al. (2016) used a deconvolution variant to invert image representations learned from a pre-trained CNN, finding that features in higher layers preserve colors and contours relevant to the classification task. This method is quicker than gradient descent-based representation inverting. Randomization has been resurfacing in machine learning literature as it is computationally cheaper than optimization. It is used to stochastically assign weights in a feedforward network for regression or classification problems. The technique involves computing a weighted sum of inputs passed through randomized nonlinearities to simplify the optimization task. Empirical comparisons and theoretical guarantees are provided for this approximation method. Other related works include random kernel approximation and reservoir computing on random recurrent networks. Several works have explored the use of random weights in convolutional neural networks (CNNs). Jarrett et al. (2009) found that random weights perform similarly to pre-trained weights in certain architectures. Saxe et al. (2011) demonstrated that random weights can lead to frequency selectivity and translation invariance in CNNs. He et al. (2016) achieved various visualization tasks using random weight CNNs. Daniely et al. (2016) extended the concept to different network types, showing that random networks approximate the kernel space. Gilbert et al. (2017) combined compressive sensing with random-weight CNNs. Gilbert et al. (2017) combine compressive sensing with random-weight CNNs to investigate CNN architectures. Ulyanov et al. (2017a) use randomly-initialized neural nets for denoising and inpainting tasks. We explore convolution followed by deconvolution architecture, focusing on VGG16 for deconvolution. The network architecture involves a convolutional layer followed by a pooling layer, except for the last convolutional layer. We analyze the \"feature representation\" after the convolutional layer but before the pooling layer. In a CNN-DCN architecture, the feature representation between the convolutional and pooling layers is studied. The deconvolutional layer in DCN uses the same convolution operator as CNN, with an upsampling operator to invert the pooling operator. Random CNNs are explored for their reconstruction ability by assigning Gaussian random weights and training the corresponding DCN to minimize pixel-wise loss. Each intermediate layer is trained to minimize the L2-norm loss between inputs and outputs. The DCN is trained to minimize loss by obtaining desired weights. Different network architectures and random weight distributions are explored. Weight decay and maximum iterations are set empirically. The CNN-DCN architecture studies feature representation and uses deconvolutional layers to invert pooling operators. The VGG CD2 architecture shows quicker convergence and lower loss for reconstruction compared to AlexNet. Pre-trained CNN encodes features favorably for classification but harder for reconstruction. VGG has lower reconstruction loss than AlexNet when trained on ImageNet samples. The pre-trained CNN discards non-essential information for classification, making reconstruction harder, while the random CNN encodes richer image details. Random VGG shows accurate reconstruction even on deeper layers, unlike pre-trained VGG. The random VGG structure yields accurate reconstruction on deeper layers, outperforming pre-trained VGG. The reconstructions on VGG based deconvolution are better than on AlexNet. Exploring purely random VGG CNN-DCN architecture shows satisfactory image reconstruction, highlighting the contribution to geometric and photometric invariance. The reconstruction ability of the rrVGG architecture with ReLU nonlinearity was systematically explored. Network depth had a bigger impact than network width, with deeper layers leading to decay in reconstruction quality. Increasing random channels and smaller kernel sizes promoted better reconstruction quality. Evaluation was done using the structural similarity index, with inputs and outputs transformed to grayscale for consistency. Higher SSIM values indicated higher image similarity. The impact of network depth and width on random reconstruction quality was explored using a cat image as an example. Despite no training, DCN could perceive geometric positions and contours up to CD3. Deeper random representations resulted in coarser images, with even a 10-layer deep CD4 architecture showing rough contours. Theoretical analysis revealed depth affects results by influencing receptive field size. Varying dimensions (width) of a Conv1-DeConv1 (CD1) architecture showed smaller widths led to differences in reconstruction quality. The reconstruction quality of rrVGG Conv1-DeConv1 networks was investigated by varying the number of random channels from 4 to 2048. Input images were taken from the ImageNet validation set, transformed to grey-scale, and SSIM values were calculated. The trends in SSIM with increasing channels were analyzed, showing differences between the original rrVGG network and a modified version. The variant of rrVGG network, with an average layer replacing the last convolutional layer, shows improved reconstruction quality as the number of random channels increases. This method is similar to random forest, where different channels independently extract features. With a sufficient number of random channels, all information can be encoded and transformed to the next layer. In the next section, it will be proven that as the width of the random neural network approaches infinity, the output will converge to a fixed image close to the original image. In Figure 8, input images are transformed to grey-scale for comparing structural similarity. The reconstruction quality improves with more channels. Experiments on rrVGG CD2 show quality decay with deeper layers. Larger kernel sizes lead to lower quality due to lack of local visual features. rrVGG Conv1_1 DeConv1_1 architecture is used for these experiments. The rrVGG Conv1_1 DeConv1_1 architecture uses random weights in N (0, 0.1) distribution for reconstruction. Results confirm assumptions. Potential application includes random CNN-DCN-Style Transfer with rrVGG for high-quality reconstructions with slight differences in background color and texture. Multiple rrVGG models can be obtained efficiently without training. Theoretical analysis is provided to explain empirical results. The random CNN architecture can reconstruct input images with slight variations. The depth of the network affects reconstruction difficulty, with larger depths making it harder. Increasing the number of channels in each layer improves reconstruction ability, while increasing network depth decreases it. Theoretical analysis supports these findings. The network increases in width, with the DCN being a type of CNN with up-sampling layers. The convergence of the output image is shown as the network width approaches infinity. Different from previous works, this study focuses on random convolutional neural networks without training. An upper bound on the reconstruction error is provided by analyzing the difference between the real output and the convergence value. Random CNN architecture is defined with different filters in the same layer being i.i.d. random vectors and filters in different layers being independent. The probability density function of each filter is isotropic. The last layer of a random CNN is the arithmetic mean of the channels of the previous layer. Each layer's convolutional feature maps are normalized by a factor related to the number of channels. Increasing the number of channels improves the output image quality, and in the limit of infinite channels, the output converges. The convergence value is a constant times the weighted norm of the receptive field. Theorem 1 states that when the number of filters in each layer goes to infinity, the output for a fixed input will converge to a fixed value. In a random CNN, the output converges to a fixed image with probability 1 as the number of filters goes to infinity. The pooling layer uses l2-norm pooling instead of average pooling. The focus is on the difference between real and finite number of channels in each layer. The random CNN architecture converges to a fixed image with probability 1 as the number of filters increases. The angle between the real output and convergence value decreases as the number of channels in each layer increases. The multilayer variance theorem states that with l2-norm pooling, the angle between output and convergence value decreases with high probability. The random CNN architecture converges to a fixed image with high probability. The angle between the output of a random CNN and the convergence value is upper-bounded. The performance of reconstruction is evaluated by comparing the convergence value to the input image. For a two-layer random CNN, the angle between the input image and the output's convergence value will be small if the pixels are similar. The multi-layer random CNN has the same insight as the two-layer one but requires more complex techniques. The kernel size and stride of a random CNN are defined, with a focus on the difference between pixels and the angle between input and output images. The study introduces a novel investigation on deep random representations using convolution-deconvolution architecture. The study explores the potential of randomness for image reconstruction on deep neural networks, showing that images can be reconstructed with satisfactory quality with a sufficient number of channels. The random CNN architecture can reconstruct input images, with the output converging to the input image as the network width goes to infinity. The reconstruction error is bounded as a function of network width and depth. The study focuses on exploring feature representations in a CNN-DCN architecture, specifically looking at convolutional layers. Dosovitskiy & Brox (2016) found that DCNs for each layer of pre-trained AlexNet produced vague images from fully connected layers. The study explores feature representations in a CNN-DCN architecture, focusing on convolutional layers. Leaky ReLU nonlinearity with slope 0.2 is applied in the activation operator. A final Crop layer is added at the end of the DCN to match the output shape with the original images. Deconvolutional networks are built on VGG16 and AlexNet, emphasizing random features of the CNN structure during training. Different Gaussian distributions are tested for random weights, with a small variance around 0.015 showing minimal inverting loss. Other random distributions like Uniform, Logistic, and Laplace are also explored. The study explores feature representations in a CNN-DCN architecture, focusing on convolutional layers. Different random distributions like Uniform, Logistic, and Laplace are tested for their impact on training the DCN. The Laplace distribution encourages sparsity by putting more probability density at 0. Training involves minimizing the L2-norm loss between inputs and outputs to obtain desired weights that minimize loss. The study focuses on training a DCN to minimize loss by obtaining desired weights. Different random distributions are tested for their impact on training. The Laplace distribution encourages sparsity. Training involves minimizing the L2-norm loss between inputs and outputs. The weight decay is set to 0.0004 to avoid overfitting, with a maximum of 200,000 iterations. Results show quicker convergence on random CNN for reconstruction compared to pre-trained weights, especially on VGG. This suggests that pre-training for classification encodes features favorably for classification but harder for reconstruction. The study compares the impact of different random distributions on training a DCN for reconstruction. Results show that VGG architecture is good for generalization in reconstruction, with random VGG yielding less loss than pre-trained VGG. The study compares the impact of different random distributions on training a DCN for reconstruction. Results show that VGG architecture is good for generalization in reconstruction, with random VGG yielding less loss than pre-trained VGG. In deeper layers, pre-trained VGG discards more non-crucial information, leading to a better classifier but a harder reconstruction task. Random VGG shows better reconstruction quality compared to pre-trained VGG on the CD3 architecture. Reconstructions from various layers of random VGG and random AlexNet show that reconstruction quality decays for deeper layers, with rwVGG yielding more accurate reconstruction even on Conv5. The study explores the impact of different random distributions on training a DCN for reconstruction. Random CNN speeds up training, improves reconstruction quality, and generalizes well for various inputs. Random encoding of CNN is easier to decode, indicating that image features of different categories are separated into distinct manifolds during classification training. The study examines the effect of random distributions on training a DCN for reconstruction. Random CNN accelerates training, enhances reconstruction quality, and generalizes effectively for different inputs. It simplifies decoding, indicating that image features of various categories are segregated into separate manifolds during classification training. Random CNN architecture differs from classic CNN in three main points: filters are i.i.d. random vectors, filters in different layers are independent, and the last layer computes the arithmetic mean of previous layer channels. This structure accelerates training, improves reconstruction quality, and enhances generalization for various inputs. The last layer in a random CNN computes the arithmetic mean of the previous layer channels. Convergence is guaranteed when using l2-norm pooling in all pooling layers, with the output converging to a fixed image. This pooling method calculates the norm of values in a patch instead of the arithmetic mean. Theorem A.7 shows the result for average pooling. Lemma 5 proves the theorem by considering random row vectors and isotropic probability density functions. The element-wise operations and rotation of vectors are key in the proof. Lemma A.2 states that for convolutional feature maps X(i+1), the channels are i.i.d. random variables. By the strong law of large numbers, it is shown that with probability 1, the recurrence relation between z*(i+1) and z*(i) in a convolutional layer holds. In a convolutional layer, the recurrence relation between z*(i+1) and z*(i) is obtained through a multi-layer sliding window scheme. By using the same sliding window scheme as convolutional, pooling, or upsampling operations, z*(i+1) is calculated based on z*(i). The inner product of a filter and vector is replaced by calculating the l2-norm of the vector in the sliding window and multiplying it by k(i)2. This process is repeated layer by layer to obtain z*(L-2). In a multi-layer sliding window scheme similar to CNN structure, z* is calculated based on input image and scheme. The weighted sum of input pixel values within the receptive field of the output pixel is obtained, where the weight is the number of routes from input to output pixel. Theorem 6 discusses the variance for a two-layer random CNN with N filters in the first convolutional layer. The convergence value f* is related to the output f with probability 1-\u03b4. The extension to a multi-layer result is discussed with a lemma. In a multi-layer sliding window scheme similar to CNN structure, z* is calculated based on input image and scheme. The weighted sum of input pixel values within the receptive field of the output pixel is obtained, where the weight is the number of routes from input to output pixel. Theorem 8 discusses the multilayer variance for a random CNN with L layers and N i filters in the i th layer, bounding the angle between the output f and the convergence value f* with probability 1-\u03b4. In a multi-layer sliding window scheme similar to CNN structure, z* is calculated based on input image and scheme. Theorem 8 discusses the multilayer variance for a random CNN with L layers and N i filters in the i th layer, bounding the angle between the output f and the convergence value f* with probability 1-\u03b4. The angle between feature maps is recursively bounded, and the bound of \u0398 is obtained with probability 1-\u03b4. The convergence value for average pooling in a random CNN with infinite filters is discussed. The output f converges to a fixed image f* with probability 1. The recurrence relation between C*(i+1) and convolutional feature maps is given. The recurrence relation for an average pooling layer and up-sampling layer is discussed, along with the convergence value f* in a random CNN with infinite filters. The rrVGG Conv1-DeConv1 architecture can achieve high-quality reconstruction with slight differences in background color and texture, suitable for style transfer results. The framework utilizes a linear combination of content loss and style loss, with content loss being the euclidean distance between content and stylized feature vectors, and style loss calculated from the Gram matrix of feature vectors. Inspired by previous work, mean value and standard deviation of feature vectors are also used to calculate style loss. FVT (iterative optimization) includes Conv2 to Conv5 layers, while rrVGG includes Conv1 and DeConv1 layers, with the option to add more layers to rrVGG and fewer layers to FVT for optimization. Our framework utilizes rrVGG and FVT models with different layers for optimization, speeding up the style transfer process. In experiments, our approach is faster than traditional methods and can transfer arbitrary styles effectively. Comparison with previous approaches shows competitive results in stylization. Our stylized results focus on extracting style features while slightly weakening content representation. Loss of some content information is possible during transformation between feature and image space using rrVGG CNN and DCN. Our approach focuses on generating high-quality stylized images by adjusting the balance between style and content in FVT. The transition from content to style is smooth with increasing ratio, extracting more features from the style image. Different rrVGG models can reconstruct images with diverse styles. Our framework utilizes different rrVGG models to generate stylized images with diverse textures and colors, without the need for additional training. The stylized images vary based on the rrVGG model weights, as shown in Fig. 17. The proposed style transfer framework utilizes various rrVGG models to create stylized images that preserve style structures and exhibit novel combinations of structure, shade, and hue. This framework allows for diversity and variation within a single style image and can be implemented quickly and flexibly through optimization or feed-forward convolutional layers."
}