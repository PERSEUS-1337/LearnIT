{
    "title": "B1xu6yStPH",
    "content": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on detecting attacks using model explainability to identify images inconsistent with the predicted class. The proposed approach, EXAID, utilizes SHAP to highlight input features contributing to a class decision without requiring modification. The proposed approach, EXAID, uses SHAP to highlight input features contributing to a class decision without modifying the attacked model. It can successfully detect unfamiliar attacks and improves detection accuracy significantly, outperforming state-of-the-art methods across various noise levels. Adversarial attacks can trick machine learning systems into making incorrect decisions, but EXAID can help identify such attacks without prior knowledge of the model. In this paper, the proposed approach, EXAID, uses SHAP to highlight input features contributing to a class decision without modifying the attacked model. It aims to detect adversarial attacks in machine learning systems, which can trick models into making incorrect decisions. The approach can improve detection accuracy significantly and outperform state-of-the-art methods, even in the presence of noise levels. The importance of explaining decisions to reduce misclassification is emphasized, drawing an analogy to the process of paper acceptance in conferences. The curr_chunk discusses the importance of using explanations to detect incorrect decisions, particularly in the case of adversarial samples designed to confuse classifiers. By probing explanations, one can identify classification decisions inconsistent with the explanation provided. The focus is on detecting adversarial samples rather than developing a system for robust classifications under attacks. The goal is to detect attacks even if automatic correction is not possible. The curr_chunk discusses detecting adversarial attacks by identifying differences in network behavior when presented with tainted inputs. Building on explainability algorithms, the focus is on pointing out input features that influence decision-making, even though these methods are based on correlations rather than causal features. Using explainable features for adversarial detection holds promise as they are designed to explain classification decisions independently of specific adversarial attacks, making them effective against adversarial perturbations that lead to nonsensical classification decisions. Previous methods based on learning statistical abnormalities of perturbations are sensitive to attack characteristics, while explainability is not. Explanation-based detection is less sensitive to perturbation magnitude, focusing on input features that explain decisions for a given class. EXAID (EXplAIn-then-Detect) is an explanation-based method designed to detect low-noise perturbations from unknown attacks by building per-class explanation models. The novel approach described in the curr_chunk focuses on detecting adversarial attacks using explainability techniques, studying the effect of negative sampling techniques, and demonstrating robustness against low-noise perturbations. The detection method provides state-of-the-art defense against leading attacks, including FGSM, PGD, and CW, for both known and unfamiliar attacks. Various methods like LIME and DeepLIFT have been proposed to address black box decisions of AI systems. The curr_chunk discusses the use of DeepLIFT and SHAP techniques to compute feature contributions in deep neural networks. It also introduces EXAID, a method for generating explanations based on network activations and outputs, followed by a detector to check consistency with predicted labels. The curr_chunk explains the use of game theory in distributing gains to players and the application of DeepLIFT as an approximation for Shapley values in deep neural networks. It focuses on three high-performing adversarial attacks, including one by Goodfellow et al. (2014) that creates perturbations based on the gradient direction. The parameter controls the magnitude of the perturbation, and Projected Gradient Descent (PGD) is suggested to improve the attack method. The curr_chunk discusses the improvement of adversarial attacks through multi-step variants like Carlini and Wagner's optimization algorithm. Different factors such as attack success probability, image appearance changes, and attack runtime are considered in designing attacks, leading to different tradeoff points. The curr_chunk discusses various techniques proposed to detect adversarial examples, including measuring the effect of quantization and smoothing on network classification, reducing input space degrees of freedom, analyzing the region surrounding a reference example, modeling activation distribution at hidden layers, and modeling changes in k-nn labels. Various techniques have been proposed to detect adversarial examples, such as modeling changes in k-nn labels, using a new loss in training to distinguish adversarial examples, and explaining detections through per-class explanations. The EXAID system involves creating per-class explanations for correct and incorrect predictions and training a binary classifier to determine if an explanation aligns with the class decision. The explanation model extracts explanations for each sample classified by a pretrained attacked classifier, mapping input features to explanation space. To create explanations for correct and incorrect predictions, positive explanations are straightforward by applying the model to correct predictions. Negative explanations involve wrong negatives, adversarial negatives, and other-class negatives. Wrong negatives are samples where the model made incorrect decisions, while adversarial negatives and other-class negatives consider confusion between classes. The process of creating explanations for correct and incorrect predictions involves considering wrong negatives, adversarial negatives, and other-class negatives. Adversarial negatives are obtained by employing adversarial attacks on training data to collect explanations for cases where the model made incorrect decisions due to being fooled by the input. These explanations may reflect typical patterns of adversarial examples but training against incorrect attacks may lead to overfitting. Additionally, explanations are produced for all incorrect classes for every labeled sample. As an explainable AI approach, SHAP deep explainer is used to provide explanations for correct and incorrect predictions. SHAP is considered a leading explainer with strong agreement with human explanations. It is the only explainer with both local accuracy and consistency. A deep binary classifier is trained per class to detect inconsistent explanations with model predictions. In the context of explainable AI, a binary multiclass multi-label detector is trained to provide explanations for predictions. Two learning setups are considered to defend against unknown or familiar attacks. The focus is on controlling the data used for training the detector, with two variants of EXAID - EXAID familiar involves training with adversarial negatives to learn explanations from specific attacks. The EXAID binary detector is trained using high-noise FGSM and does not encounter adversarial negatives during training. It is evaluated on CIFAR10 and SVHN datasets for attack detection. MNIST is not suitable for evaluating adversarial defenses due to its low dimensionality. Carlini & Wagner's results on CIFAR-10 are referenced. The code for EXAID is available on GitHub. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 classes, with 50,000 training images and 10,000 test images. The SVHN dataset has over 600,000 32x32 color images in 10 classes, with 73,257 digits for training and 26,032 digits for testing. A pretrained Resnet34 model was used for both datasets. The EXAID detector was trained using positive, wrong negative, and other-class negative explanations from natural images. The EXAID-unknown model was trained on these explanations. The EXAID detector was trained using positive, wrong negative, and other-class negative explanations from natural images. To enable running SHAP on a large number of examples, the implementation was modified to run on GPU. EXAID was compared with three adversarial detection baselines and two new variants. The authors used the implementation provided by the authors to tune hyperparameters using hyperopt. They also utilized Mahanalobis approach to model the distribution of activations in hidden layers and trained a classifier on adversarial examples. Additionally, they modified the Mahanalobis method to create an attack-agnostic baseline. The authors modified the Mahalanobis method to create an attack-agnostic baseline by estimating the likelihood of network activations as the product of all layers' likelihoods. They used the LID measure to assess the characteristics of the region surrounding a reference example and trained a classifier with adversarial examples crafted by FGSM for detection against oblivious adversaries. In the context of defending against black box attacks, the transferability of adversarial examples between models is a key factor. While attacking a black box model is not significantly harder than a white box, defended neural networks show reduced transferability of adversarial examples. The magnitude of perturbation used in an attack plays a crucial role in the success of adversarial detection methods. Comparing attacks and determining a clear protocol for evaluation remains an open challenge in the literature. EXAID outperforms other defense methods in detecting small perturbations caused by various attack methods, including FGSM, PGD, and C&W. The experiments were conducted across a wide range of noise levels, showing significant performance improvement with low noise levels. EXAID, a novel attack-detection approach, outperforms other defense methods in detecting small perturbations caused by various attack methods. The AUC is increased from 70% to over 90%, with LID and Mahalanobis performing well in high noise scenarios. However, LID and Mahalanobis suffer in low noise levels, while EXAID's performance remains high. The unsupervised variant of LID performs as well or better than the original LID, highlighting the importance of benchmarking defense models against different noise levels. Our method surpasses previous state-of-the-art methods for three attack methods and various noise levels, showing the significant impact of attack noise level on defense methods. This highlights the need for future defense methods to be evaluated across a wide range of noise levels."
}