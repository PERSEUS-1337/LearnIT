{
    "title": "BJgmnmn5Lr",
    "content": "Generative priors are effective in solving inverse problems like denoising and inpainting. They can represent images with lower dimensional latent codes. Recent studies show untrained deep neural networks can also work as priors for image recovery. This paper optimizes network weights and latent codes to use untrained generative networks as priors for video compressive sensing, providing concise representations of frames. By optimizing over latent code, we can obtain a concise representation of video frames while maintaining structural similarity. Low-rank constraint is applied to further reduce the dimensionality of the latent space. Empirical results demonstrate that our approach offers improved accuracy and lower computational complexity compared to existing methods in compressive sensing for recovering video sequences from limited measurements. The recovery problem is challenging with few measurements compared to unknowns. Generative priors map low-dimensional vectors to high-dimensional images. Using generative models for compressive sensing has shown promise in image reconstruction. Deep image prior (DIP) and deep decoder are used for reconstruction from compressive measurements. DIP utilizes an untrained convolutional generative model as a prior for solving inverse problems like inpainting and denoising. Both methods update model weights to reconstruct images, with DIP being highly overparameterized and deep decoder being underparameterized. This paper aims to address limitations in video recovery using these approaches. In this paper, the limitations of DIP and deep decoder-based video recovery are addressed. The methods currently used have issues with fixed latent codes and computational constraints when applied to video sequences. To overcome these challenges, the proposal involves joint optimization over network weights and latent codes to reconstruct video sequences efficiently. By optimizing over latent codes alongside network weights, temporal similarities in video frames can be reflected in the latent code representation. The paper addresses limitations in video recovery methods by proposing joint optimization over network weights and latent codes to efficiently reconstruct video sequences. Temporal similarities in video frames are reflected in the latent code representation, with low-rank constraints imposed for a more compact representation. The key contribution is demonstrating that joint optimization allows learning a single generator network for an entire video sequence and corresponding latent codes simultaneously, reducing computational complexity. The approach proposed in the paper addresses limitations in video recovery methods by introducing low-rank constraints on generator latent codes to encode similarities among reconstructed frames. This enables a more compact representation of video sequences with fewer parameters in the latent space. The joint optimization over network weights and latent codes allows for learning a single generator network for the entire video sequence, reducing computational complexity. The optimization process involves initializing z randomly and optimizing latent codes and generator parameters jointly. The reconstructed video sequence is generated using estimated latent codes and generator weights. Latent codes are initialized with samples from a Gaussian distribution and normalized, while the generator is initialized with random weights. Testing showed little difference in performance between different initialization methods, so random initialization is used in this study. Each iteration of joint optimization includes steps for latent code optimization. In this paper, joint optimization involves two steps: latent code optimization and network parameter optimization. The model parameters are updated with stochastic gradient descent after each gradient descent update of the latent codes. Different sets of network weights can be learned for small segments of longer video sequences. The end result includes trained weights, reconstructed frames, and optimal latent codes capturing temporal similarity. The assumption is made that variations in image sequences are localized to exploit redundancies. The latent codes sequence in a video sequence can be represented in a low-dimensional space by imposing a low-rank constraint using principal component analysis or singular value decomposition. This allows for expressing each latent code in terms of orthogonal basis vectors, enabling a low-rank representation of the latent codes. The low-rank representation of latent codes in a video sequence offers compression in code representation. The study includes experiments on various real video datasets and reports results for specific video sequences. The DCGAN architecture was used for the experiments. The study utilized DCGAN architecture for generators without batch-normalization layer. Latent code dimensions varied for different video resolutions. Adam optimizer was used for generator weights, and SGD for latent code optimization. Rank=4 constraint was applied for video sequences with 32 frames. Comparison was made with TVAL3D algorithm and deep decoder on denoising, inpainting, and compressive sensing tasks. Two deep decoder settings were used: underparameterized and overparameterized. In addition to denoising and inpainting tasks, compressive random projection experiments were conducted using separable measurements. Results for denoising at 20 dB SNR noise, inpainting with 80% missing pixels, and compressive sensing with 20% available measurements were reported. Joint optimization without low-rank constraint outperformed TVAL3D algorithm and UP deep decoder, performing similarly to OP deep decoder. Reconstruction performance for denoising, inpainting, and compressive sensing tasks was shown in Figure 2. In Figure 2, reconstruction performance for denoising, inpainting, and compressive sensing tasks on the 'Handwaving' video sequence is shown. Joint optimization performs comparably to other algorithms, especially excelling in reconstructing details from masked frames. The computational complexity of the proposed methods varies based on the generator structure chosen, with DCGAN generator structure used in the experiments. Memory requirements for gradient descent were calculated using the torchsummary package. Memory requirements for different deep decoder methods vary based on image size and video sequence length. UP deep decoder, OP deep decoder, and joint optimization have different memory requirements for various image sizes and video sequences. OP deep decoder has a high memory requirement, making it unsuitable for optimizing entire video sequences, while UP deep decoder lacks the capacity to generate entire video sequences. Joint optimization with DCGAN generator requires a large number of network parameters for representing video sequences. In an experiment, the similarity structure in latent codes obtained by joint optimization was investigated by concatenating 16 frames from each video sequence. The cosine similarity matrices for video frames, compressive measurements, and latent codes were compared with fixed random codes, joint optimization codes, and low-rank joint optimization codes. The low-rank constraint improved the similarity matrix, allowing for representation of video sequences in a lower dimensional space. Using a continuous generator function, the latent space representation of video sequences retains their sequential structure, as demonstrated with an example using rank=2. In a demonstration, a rank=2 constraint is applied to latent codes to reconstruct 'Rotating MNIST' sequence from a masked version with 80% missing pixels. The latent codes are represented in a 2D plane using orthogonal basis vectors, showing a sequential pattern in their low-dimensional representation. For complex motions, higher dimensional representation may be needed to observe sequential patterns."
}