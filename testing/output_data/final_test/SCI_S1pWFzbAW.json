{
    "title": "S1pWFzbAW",
    "content": "The paper introduces a novel scheme called Weightless for lossy weight encoding in deep neural networks. This technique can compress DNN weights by up to 496x while maintaining model accuracy, outperforming existing methods by up to 1.51x. The correlation between model size and accuracy in deep neural networks (DNNs) poses a challenge for resource-constrained platforms like mobile phones and wireless sensors. While new hardware enables DNN inferences on these devices, distributing large models remains a practical issue, especially in regions with low-bandwidth networks. Limited network bandwidth and storage capacity on these devices further complicate the distribution of DNNs. Model compression is essential for supporting deep learning on edge devices. Weightless, a novel lossy encoding method based on Bloomier filters, is proposed in this paper to reduce the size of DNN models without sacrificing accuracy. Existing compression algorithms focus on simplifying weight values, while Weightless offers a new approach by adjusting filter parameters to use less storage space. Weightless is a novel lossy encoding method using Bloomier filters to compress DNN models, reducing storage space without compromising accuracy. It achieves compression rates up to 496\u00d7 and scales well with increasing sparsity, outperforming existing techniques by up to 1.51\u00d7. This approach demonstrates the effectiveness of compressing DNNs with lossy encoding, even after aggressive simplification steps like weight pruning and clustering. Section 3 reviews Bloomier filters and Weightless, a novel encoding method for compressing DNN models. Section 4 presents state-of-the-art compression results using Weightless, showing its scalability with increasing sparsity. The goal is to minimize the memory footprint of neural networks without compromising accuracy by exploiting redundancy in deep neural network weights. The first class of methods focuses on training a network with a small memory footprint by introducing specialized structure or loss. Examples include low-rank matrices and randomly-tied weights. Another approach is to compress an existing trained model by removing unnecessary information to save memory. Prior work has explored simplifying weight matrices to save memory by pruning and quantizing weight values. This can reduce the information needed to represent them, enabling more compact encoding schemes for improved compression. Weightless is a lossy encoding scheme based on Bloomier filters, which generalize Bloom filters to answer set membership queries. It encodes neural network weights effectively for compression, allowing false positives to reduce size and improve encoding strength. Weightless is a lossy encoding scheme based on Bloomier filters, which generalize Bloom filters to answer set membership queries. Bloomier filters encode a function for each value in a domain, returning a value when present and a null value otherwise, with occasional false positives. A Bloomier filter allows for inexact reconstruction of compressed data by hashing locations and combining entries with a mask. The red path in the illustration shows a false positive due to collisions. A Bloomier filter uses a small number of hash functions and a hash table to store values. Querying a Bloomier filter runs in constant time. False positives can occur with a certain probability. The Bloomier filter involves finding values for X such that the relationship holds for all values in S. Published approaches use randomized algorithms to search for configurations efficiently. BID5 introduced a greedy algorithm that produces a table of size cn t bits in O(n log n) time. BID4 offer slightly better constructions, including a method that runs in O(n) time with identical space requirements. They also present an algorithm for producing a smaller table in O(n log n) time with c closer to 1. Using a more advanced algorithm could lead to a more compact table and improve compression rates. Construction may be costly but is a one-time expense with a small runtime compared to the overall time. The Bloomier filter is proposed for compactly storing weights in a neural network. The function f encodes the mapping between indices of nonzero weights to their corresponding values. The range is remapped to encode cluster indices, with a null response indicating a weight value of zero. This method offers savings compared to fully training a network and involves a one-time cost with a small runtime. The Bloomier filter is used to store weights in a neural network efficiently. The function f encodes nonzero weight indices to values, with zero weights represented by null responses. An approximation of the original weight matrix is reconstructed using the filter. Pairing approximate encoding with weight transformations can improve compression. To improve compression, approximate encoding is paired with weight transformations. Two pruning techniques are considered: magnitude threshold plus retraining and dynamic network surgery (DNS). DNS prunes the network during training, while magnitude pruning with retraining offers good results. Improving sparsity reduces encoding size linearly with the number of non-zeros, with no effect on the false positive rate. Using two methods demonstrates the benefits of Weightless networks. Using k-means clustering to discretize weights, the centroids are saved into an auxiliary table and indices into this table replace elements of W. Bloomier filters are well-suited for this indirect encoding, allowing for a small, contiguous set of integers without the need for k to be a power of 2. Decoding Bloomier filters involves checking XOR results with an inequality, reducing false positives by a factor of 1 \u2212 k 2 r. Tuning the hyperparameter t, the number of bits per cell, balances filter size and false positive rate for model accuracy. Bloomier filters simplify encoding by freezing layers after construction and retraining subsequent layers. Tuning t is easier than other DNN hyperparameters due to encoding k clusters. Experimental findings suggest that the hyperparameter t typically falls within the range of 6 to 9 for models utilizing Bloomier filters. Retraining deeper network layers sequentially can help mitigate the effects of false positives, as the deterministic nature of the filter allows for compensation for errors without sacrificing benefits. When retraining deeper network layers sequentially to mitigate errors in Bloomier filters, it is more effective to retrain all subsequent layers instead of just the encoded layer. This process typically reduces the hyperparameter t by one or two bits and converges in tens of epochs. Additionally, compressing weight matrices using arithmetic coding for transmission can further optimize the process. Weightless was evaluated on three deep neural networks: LeNet-300-100, LeNet5, and VGG-16. The networks were trained and tested in Keras. The Bloomier filter used a Mersenne Twister pseudorandom number generator for hash functions. To reduce construction time for VGG-16, non-zero weights were sharded into ten filters built in parallel. Sharding did not significantly affect compression or false positive rate. Weightless was applied to the largest layers in each model. Weightless was applied to the largest layers in LeNet-300-100, LeNet5, and VGG-16, which account for the majority of the weights in each model. The results show the compression ratio and improvements compared to Deep Compression, the current state-of-the-art method. Deep Compression implements lossless optimization using compressed sparse row encoding and Huffman coding for weight compression. The compression achieved by Deep Compression is notably better than the original publication. Weightless, on the other hand, performs lossy compression with a bound on the error impact. Test errors from Weightless and Deep Compression are the same after simplification steps. Weightless and Deep Compression are compared in terms of compression performance, with Weightless sometimes slightly better due to training fluctuations. The strongest case for Weightless is to compare it against lossless techniques with iso-accuracy. Bloomier filters are evaluated for encoding sparse weights, showing exceptional performance with a 445\u00d7 compression of a large fully connected layer in LeNet5. Bloomier encoding demonstrates a 1.99\u00d7 improvement over CSR, an alternative encoding strategy used in Deep Compression. The encoding strategy used in Deep Compression relies on sparsity and clustering to determine the compactness of Bloomier filters. Results show that sparsity is more crucial than the number of clusters in reducing filter size. Comparing LeNet models' magnitude pruning to dynamic network surgery, increased sparsity leads to significant size reduction. DNNs can tolerate a high false positive rate, making it challenging to reduce the t value even if k is reduced. In VGG-16 FC-0, there are more false positives in the weight matrix than reconstructed. After simplification, VGG-16 FC-0 has 6.2 million false positives with t = 6, reduced to 3.07 million weights. Bloomier filter encoding increased top-1 error by 2.0 percentage points before retraining. Weightless outperforms Deep Compression in compression effectiveness. Weightless exploits sparsity better than Deep Compression. Weightless exploits sparsity more effectively than Deep Compression, delivering better compression ratios as sparsity increases. Different encoding techniques scale with increasing sparsity, with Weightless outperforming Deep Compression without loss of accuracy. This method could potentially be applied to Bloomier filters in future work. Weightless is a novel lossy encoding scheme for compressing sparse weights in deep neural networks, utilizing the Bloomier filter for maximum compression. It achieves up to 496\u00d7 compression, surpassing the previous state-of-the-art by 1.51\u00d7. Weightless achieves up to 496\u00d7 compression, surpassing the previous state-of-the-art by 1.51\u00d7. There are avenues for further research in improving weight pruning mechanisms and exploring more advanced construction algorithms for Bloomier filters. Additionally, the use of lossy encoding schemes for model compression opens up opportunities for research in encoding algorithms and probabilistic data structures. The encoding process involves generating neighborhoods of hash digests for keys, identifying unique neighbors, and saving their iota values in an ordered list. This process is repeated until all keys have a unique location for encoding. Values are then encoded into the filter in reverse order of unique locations found during the search. The Bloomier filter encoding process involves generating unique neighbor values for keys by XORing neighbor filter entries, a random mask, and the key's value. The correct value is retrieved by XORing the neighbor values in reverse order. An implementation of the Bloomier filter will be released with publication. After encoding FC-0 with a Bloomier filter, the validation accuracy increases from 30% to 33%. The error introduced by lossy encoding is recovered after a few epochs. Test accuracy for FC-0 before retraining FC-1 is 38.0% and decreases to 36.0%. Sensitivity analysis shows that a lower number of weight clusters (k=8) performs better for encoding. Comparison with previous DNN compression techniques shows competitive accuracy for Weightless in LeNet5. In comparison to TAB3, Weightless' compression ratio scales better than Deep Compression for FC-0 in LeNet5. The plot was generated by sweeping the pruning threshold from 6 to 9 for Weightless and Deep Compression."
}