{
    "title": "SkGT6sRcFX",
    "content": "Infinite-width neural networks have been used to study the theoretical properties of standard neural networks. The main challenge lies in defining appropriate weight sampling distributions. A principled weight initialization approach is proposed to enable the construction of deep infinite-width networks by reparametrizing hidden-layer activations into reproducing kernel Hilbert spaces. Infinite-width neural networks are reparametrized into reproducing kernel Hilbert spaces to construct deep networks with principled weight distributions. A novel weight initialization scheme is derived for standard networks, improving performance on various datasets. Despite their empirical success, deep neural networks remain hard to interpret black boxes, prompting research efforts to better understand them. Significant research has focused on the connections between infinite-width networks and kernel methods, with a correspondence established between single-layer infinite-width networks and Gaussian processes. Recent interest in deep infinite-width networks has led to a novel approach proposed to enable their construction beyond two hidden layers. To construct networks with infinitely wide hidden layers, a novel approach is proposed to ensure inner products between hidden layer representations and weights are well-defined. This involves connecting layers with weights in the same function space as the activations of the previous layer. Infinite-width networks with more than two hidden layers can be constructed by defining a probability distribution for infinitely many weights connecting the layers. The proposed approach ensures that weights connecting layers are in the appropriate function space by defining probability distributions over reproducing kernel Hilbert spaces (RKHS). This allows for the composition of infinitely many layers of infinite width. The paper introduces a novel approach to constructing probability distributions over RKHS for defining weight initialisation in neural networks. This method allows for the incorporation of data and task-specific information into the network's structure. The paper presents a novel approach to constructing infinite-width networks with many hidden layers, complex kernels, and a unique weight initialization scheme. It discusses related work, introduces the proposed approach, showcases practical implications for finite-width networks, presents experimental results, and concludes with insights from the correspondence between single-layer neural networks and GPs. Recent research has focused on the connection between neural networks and kernel methods, exploring the construction of kernels that mimic computations in neural networks. Various studies have developed kernels that correspond to different types of neural networks, such as those with Sign and ReLU nonlinearities, convolutional networks, and networks represented by directed acyclic graphs. This work builds on the correspondence between single-layer neural networks and GPs. Various studies have explored the connection between neural networks and kernels, constructing kernels that mimic computations in neural networks. Examples include using GPs with covariance kernels defined on the output of fully-connected and recurrent neural networks, learning the kernel of an infinite-width neural network, and analyzing neural networks using kernels. The traditional GP-neural network correspondence has also been a topic of recent interest. BID13 constructs kernels for infinite-width neural networks with up to two hidden layers and arbitrary nonlinearities. A GP with a general covariance kernel is used for sampling the weights between the hidden layers, but the resulting network may be ill-defined due to mismatched sampling distributions. Recent papers BID24 and BID27 explore the connections between Gaussian processes and deep neural networks, focusing on increasing hidden layer width and performing fully Bayesian inference for regression. Our work focuses on enabling the construction of deep infinite-width neural networks, studying the initialization requirements, and addressing the changing nature of weights as the width of hidden layers goes to infinity. This differs from previous contributions that examine the behavior of deep neural networks as the width of hidden layers increases. In this section, a novel method for constructing infinite-width networks is presented, enabling the creation of deep infinite-width networks. Previous work on infinite-width networks was limited to two layers or examined limits of deep networks. The weights cannot be sampled from a normal distribution to avoid an ill-defined inner product with the infinite-width hidden layer. Different goals and approaches lead to varying definitions and utilizations of kernels, with a key distinction being the kernel defined on the space of activations rather than pre-activations. In this subsection, the focus is on the initialization requirements of infinite-width networks. The main challenge lies in specifying appropriate sampling distributions for the weights to ensure the network is well-defined. Activation of neurons in the first layer is computed with a non-linear function applied elementwise to the weights. The text discusses the initialization requirements for infinite-width networks, focusing on sampling distributions for weights to ensure network well-definition. Neuron activation in the first layer is computed with a non-linear function applied to the weights. The inner products of weights and input need to be well-defined for every neuron in the first layer. To construct a well-defined first layer, infinitely many weights need to be sampled from the same space as the input. The text discusses the need for well-defined second layer activations in infinite-width networks by sampling weights from the same space as the input functions in the first layer. In constructing infinite-width networks, the challenge lies in defining appropriate weight sampling distributions to ensure well-defined activations in each layer. The weights need to be sampled from the same space as the input functions to maintain network coherence. The main challenge in constructing infinite-width networks is defining appropriate weight distributions to ensure well-defined activations in each layer. To address this difficulty, a principled approach to weight initialization is proposed to automatically draw weights from the appropriate function space. The proposed weight initialization scheme allows for the construction of deep infinite-width networks with arbitrarily many hidden layers by defining probability distributions over RKHSs for sampling weights. This approach reparametrizes hidden layer activations into RKHSs induced by associated kernels, creating a hierarchy of complex kernels capturing network geometry and biases. The kernel defined in the previous layer captures the geometry and biases, inducing an RKHS. This RKHS allows for the construction of a second layer of infinite-width networks. The text discusses constructing a second layer of infinite-width networks by defining a distribution over weights using the induced RKHS and kernel. Sampling weights from a Gaussian process with zero mean function and covariance function is proposed for extending the network with an additional layer. The text outlines a construction approach for infinite-width networks, involving isometric isomorphism between layer activations and induced RKHS. The process involves iteratively constructing layers, defining kernels, reparametrizing activations, and sampling weights from a canonical distribution over the RKHS. Infinite-width networks are constructed using isometric isomorphism between layer activations and induced RKHS. A weight initialization method for finite-width networks is derived based on this construction, treating them as Monte Carlo approximations. The output of each layer in the finite-width case is computed recursively from its infinite-width counterpart. In the finite-width case, weight vectors connect layers and are initialized using a data-and task-dependent approach called Weight Initialisation with Infinite Networks (Win-Win). This method involves Monte Carlo approximations for weight initialisation in infinite-width networks, using sampling to compute kernels and GP covariance. The number of samples used corresponds to the dimensionality of the feature expansion. In the finite-width case, weight vectors connect layers and are initialized using a data-and task-dependent approach called Weight Initialisation with Infinite Networks (Win-Win). This method involves Monte Carlo approximations for weight initialisation in infinite-width networks, using sampling to compute kernels and GP covariance. The number of samples used corresponds to the dimensionality of the feature expansion. Additionally, the representations in finite-width networks can be viewed as random feature expansions of a particular kernel, establishing a connection to BID31. The coefficients and selected training points play a role in the interaction between weights and hidden layer representations, with weight matrix rows being initialised as weighted linear combinations of activations from the layer. This facilitates an intuitive interpretation of neurons measuring data alignment with specific subspaces. Three different approaches are proposed for selecting these subspaces, including random selection from training data. In the context of weight initialization in neural networks, structured approaches to subspace selection are proposed. These approaches aim to disentangle factors of variation in the data by making similar objects more similar and dissimilar objects more dissimilar across layers. Two methods are suggested for uncovering disentangled directions: using k-means clustering and class information. This approach incorporates data structure and task information into weight initialization, moving beyond random selection from training data. The proposed methods for weight initialization in neural networks involve subspace selection using k-means clustering and class information to modify learning dynamics towards the data manifold. This approach aims to disentangle factors of variation in the data and align with the end goal of classification. The new weight initialization scheme Win-Win is compared to commonly used schemes using various datasets. Points from a cluster should activate the corresponding neuron more than points from other clusters, making them more similar to each other. The approach aims to align with the data manifold for better classification. The Adam optimizer BID18 was used with default hyperparameters and a fixed learning rate for training. ReLU non-linearity was utilized, and error bars represent one standard deviation of the mean. No advanced techniques were used to isolate the effect of initialization on network performance. Testing was done on the MNIST dataset with a 2-layer fully-connected architecture. The study tested three different methods for subspace selection in Win-Win - random, class, and kmeans. Random and kmeans-based Win-Win showed competitiveness with other methods, while class-based Win-Win performed worse. The activation patterns for different initialization methods were also studied, showing sparse activation across neurons and classes at initialization. In the study, different methods for subspace selection in Win-Win were tested, including random, class, and kmeans. Random and kmeans-based Win-Win showed competitiveness, while class-based Win-Win performed worse. The activation patterns for different initialization methods were studied, showing sparse activation across neurons and classes at initialization. In the context of CIFAR-10 dataset, a deep convolutional neural network was used with a fully-connected linear layer. The study tested different methods for subspace selection in Win-Win, including random, class, and kmeans. The deep convolutional neural network used a fully-connected linear layer as the output layer and was trained for 400 epochs. The Win-Win initialization outperformed other methods significantly, as shown in TAB1. Additionally, the new initialization scheme was assessed on a regression task using the Year Prediction MSD dataset, showing promising results. In a regression task using the Year Prediction MSD dataset, the data was split into training, validation, and test sets. The features and labels were pre-processed to have a standard normal distribution. Two fully-connected architectures were trained with the objective of minimizing mean squared error. The number of units in each hidden layer was set to 89. The experimental results show that Win-Win initialization outperforms other schemes for a single hidden layer and is comparable to the best scheme (SntDefault) for two hidden layers. The study focuses on the initialization requirements of infinite-width networks and proposes a novel method for constructing deep infinite-width networks with many hidden layers. The study proposes a novel weight initialization method for constructing deep infinite-width networks with many hidden layers using Gaussian processes with specific covariance kernels. Monte Carlo approximations were used to examine the practical implications of this method for standard, finite-width networks. The study introduces a novel weight initialization method for deep infinite-width networks using Gaussian processes with specific covariance kernels. It demonstrates competitive performance on diverse datasets and provides proofs for the proposition and lemma. The study presents a weight initialization method for deep infinite-width networks using Gaussian processes with specific covariance kernels. It utilizes an isometric isomorphism to simplify the construction of distributions for the weights, particularly focusing on H k1. This approach ensures a Gaussian process with a special covariance structure, leading to competitive performance on various datasets. The construction of a second hidden layer in infinite-width networks involves sampling weights from a Gaussian process with a specific covariance function. This ensures well-defined representations between layers and leads to the definition of a corresponding kernel for the second layer. The isometric isomorphism simplifies the relationship between the spaces of functions, ultimately contributing to competitive performance on datasets. In constructing an infinite-width network with multiple hidden layers, we define kernels for each layer and reparametrize them into induced Reproducing Kernel Hilbert Spaces (RKHS). The connecting weights between layers are sampled from a Gaussian process with a specific covariance function, ensuring well-defined representations. This process simplifies the relationship between function spaces and contributes to competitive performance on datasets. In constructing an infinite-width network with multiple hidden layers, kernels are defined for each layer and reparametrized into induced Reproducing Kernel Hilbert Spaces (RKHS). The weights between layers are sampled from a Gaussian process with a specific covariance function. This simplifies the relationship between function spaces and contributes to competitive performance on datasets. Lemma 1 discusses the isometric isomorphism between the representational space of a layer and the corresponding RKHS. The mapping U connects the two spaces, ensuring a bijective map that satisfies specific conditions. In constructing an infinite-width network with multiple hidden layers, kernels are reparametrized into induced Reproducing Kernel Hilbert Spaces (RKHS). The mapping U ensures a bijective map between the representational space of a layer and the corresponding RKHS, guaranteeing unchanged geometry. Activation patterns of randomly sampled points from each class are shown at initialisation and during training epochs. During training, activation patterns of a convolutional architecture for CIFAR10 experiment are visualized. The left column shows activations of the first hidden layer, while the right column shows the output layer after applying the softmax function. Neurons are grouped into buckets and averaged for layers with more than 10 neurons. The architecture includes conv2D layers with varying parameters and batch normalization followed by ReLU activation. The architecture for the CIFAR10 experiment includes conv2D layers with varying parameters, batch normalization, ReLU activation, average pooling, and a linear layer with 512 units followed by softmax."
}