{
    "title": "SyeKf30cFQ",
    "content": "In this paper, a novel theoretical framework is proposed for deep convolutional neural networks (DCNN) with ReLU nonlinearity. The framework connects data distribution with gradient descent rules, promotes disentangled representations, and is compatible with Batch Norm. It is based on a teacher-student setting, projecting the student's computations onto the teacher's graph without unrealistic assumptions. This framework could aid in analyzing practical issues like disentangled representations in deep networks. Convolutional Neural Network (DCNN) has achieved empirical success in various disciplines, but its theoretical properties remain an open research topic. Deep models are often seen as non-convex optimization in high-dimensional spaces, with analyses on loss function landscapes, saddle points, and gradient descent trajectories. However, these analyses often overlook the importance of specific network structures and input data distribution, which are critical in practice. Deep models have shown empirical success for certain data types like images, but theoretical methods like gradient descent may fail for specific data distributions. Previous theoretical works focus on shallow networks with specific data distributions, but extending these approaches to deep networks with strong empirical performance is challenging. A new theoretical framework is proposed for deep ReLU networks applicable to general data distributions, using a teacher-student setting to compute classification labels. The teacher-student setting involves the teacher computing classification labels using a computational graph with local structures like CNN. The student network updates weights to fit the teacher's labels without knowledge of the summarization variables. The goal is to show that each node in the student network becomes highly selective with respect to the teacher's summarization variable. This sheds light on how effective methods like CNN are trained. The forward/backward pass in gradient descent is reformulated by marginalizing out input data conditioned on the teacher's graph variables at each layer, relating data distribution with gradient update rules. The reformulation relates data distribution with gradient update rules and is compatible with existing receptive fields hierarchy. It involves a locally connected neural network trained with pairs of input and class labels generated by the teacher. The network utilizes state-of-the-art regularization techniques and favors disentangled representation in data distributions. Our work introduces a theoretical framework for deep and locally connected nonlinear networks that promotes disentangled representation in data distributions. Compared to previous works, our framework imposes mild assumptions, explicitly deals with back-propagation, and considers the spatial locality of neurons in practical deep models. In a multi-layer locally connected network with ReLU nonlinearity, we analyze the hierarchical structure of label generation. Using a teacher-student setting, the student network learns the teacher's label via gradient descent without knowledge of the teacher's internal representations. Neurons in the lower layer cover a small region while those in the upper layer cover a larger region, each represented by Greek letters for receptive fields. Receptive fields in a multi-layer locally connected network are represented by Greek letters {\u03b1, \u03b2, . . . , \u03c9}. The hierarchy of receptive fields is defined, with each field having a parent and children. Neurons in the network are denoted by j \u2208 \u03b1, covering the same region with n \u03b1 nodes. The image content is represented as x \u03b1(j), abbreviated as x j. The label generation process is analyzed in a teacher-student setting, where the student network learns the teacher's label via gradient descent without knowledge of the teacher's internal representations. The parent node's receptive field covers its children's. The label y of input x is computed bottom-up by a teacher using summarization variables. Multiple nodes share the same receptive field, grouped together. The top-level summarization z \u03c9 is computed to determine the class label y.\u03c6 = {\u03c6 \u03b1 } represents all summarization functions, with z \u03b1 as discrete variables taking m \u03b1 possible values. The summarization variables Z = {z \u03b1 } are used by a teacher network to compute the class label y of input x. The relationship between input x and summarization variable z at each layer is deterministic. The teacher network, like a locally connected network, encodes z \u03b1 values from different channels at specific spatial locations. Probabilistic quantities are not involved in this process. The relationship between input x and summarization variable z at each layer is deterministic. Probabilistic quantities appear in the formulation due to marginalization over z. This implicitly establishes a relationship between conditional probabilities and the input data distribution. By specifying P(x) and a summarization function, we can compute P(z \u03b1 |z \u03b2) through sampling x and accumulating frequency statistics. Overlapping receptive fields may indicate a relationship among P(z \u03b1 |z \u03b2). The curr_chunk discusses the flexibility of data distribution and compares it with a top-down generative model. It highlights the challenges of top-down modeling when latent variables overlap, leading to a loopy graphical model. This complexity makes dealing with the population loss function difficult. The text discusses the challenges of top-down modeling in establishing a concise relationship between parameters and optimization techniques in neural networks. It introduces a natural relationship between gradient descent rules and conditional probability in summarization variables. The text discusses the relationship between activation and gradient in locally connected networks, highlighting the dependence on specific regions rather than the entire image. The gradient is determined by the entire image and its label, leading to the concept of marginalized gradient with structured properties. The text explains the recursive structure of marginalized gradients in locally connected networks, showing how gradients are computed and projected between nodes based on specific regions. The text discusses computing expected gradients in neural nodes based on specific factors, emphasizing the importance of learning hidden events. It also touches on correlations in multi-class classification and questions whether gradient descent can automatically adjust gradients. The text explores the relationship between factors and gradients in neural networks, questioning if gradient descent can automatically align gradients with specific factors. It introduces a theoretical framework for analyzing the correlation between neighboring layers and the iterative equations that result. Theorem 3 states that if P(x j |z \u03b1 ) is a delta function for all \u03b1, then all conditions in Thm. 2 hold. The main idea is that image content x \u03b1 is most related to summarization variable z \u03b1 at the same receptive field, and less related to others. Empirical observations show that low-level features in DCNN are generic, while high-level features are more class-specific. This formulation relates conditional information loss to classification. The formulation relates conditional probabilities and input data distribution into gradient descent rules, allowing for a better understanding of backpropagation. Stochastic gradient descent is modeled as using an imperfect estimate of true probability due to batch sampling. This unites analysis of gradient descent and stochastic gradient descent. The text discusses treating input pixels as events, representing nodes with class labels, and using matrix forms for dynamics. It also mentions the top level gradient and input factors for regions. The reformulation includes Batch Normalization technique. The text discusses Batch Normalization as a regularization technique, highlighting a novel finding that the back-propagated gradient through a Batch Norm layer is a projection onto the orthogonal complementary subspace. The gradient after passing through the Batch Norm layer is zero-mean and perpendicular to the input activation. The text discusses Batch Normalization and its analysis without imposing assumptions, focusing on the expectation of input x and its application in reformulation. The projection matrix P and its properties are explored, leading to the analysis of interesting structures. Sec. 5.2 discusses the analysis of gradient descent in deep models under specific data distribution conditions. It includes examples of the role of nonlinearity and conditions for achieving disentangled representation. The text also provides general comments on deep learning issues like overfitting and GD versus SGD. The analysis shows that a linear model struggles with exponential events in a region, while a nonlinear model with ReLU can handle it. Definitions for Convex Hull of a Set and k-vert matrix are also given. Theorem 6 discusses the expressibility of ReLU nonlinearity in deep models. It shows that nonlinearity guarantees full rank output even with low-rank matrices. The theorem also states that for intermediate layers, if all matrices are all-vert and the input is full-rank, the output can be made identity. This implies that randomly sampling W will result in full-rank outputs with high probability. The analysis in Sec. 5.1 assumes that with random sampling of W, all F \u03b2 are full-rank, ensuring zero generalization error. In practice, when n \u03b1 m \u03b1 = O(exp(sz(\u03b1))), proper representation of information is needed for conveying to the top level. Ideally, binary factors can be used to concisely represent events with nodes. The text discusses constructing factorizable P \u03b1\u03b2 for disentangled representation in deep nonlinear networks. It defines disentangled representation and the gradientG \u03b1 as a 2-by-1 vector. The text discusses the disentangled properties of \u03b1 in deep nonlinear networks. It explores whether these properties carry over layers in the forward pass and how gradient descent affects this structure. The text discusses the disentangled properties of \u03b1 in deep nonlinear networks, exploring how gradient descent affects this structure. It is noted that the separable structure is conserved over gradient descent if certain conditions are met, such as disentangled F \u03b2 and G \u03b1. The issue of whether G \u03b2 remains disentangled during backpropagation is highlighted as a non-trivial problem, requiring further investigation. The proposed formulation integrates the input x into the probabilistic distribution P(z \u03b1 , z \u03b2 ) and their marginals, allowing for analysis of practical factors in DL training. Overfitting occurs due to errors in estimated distributions, leading to drastic changes in optimal weights for prediction. An example is given where events at different reception fields are unrelated to the class label. The text discusses the challenges of spurious correlations in deep learning training, particularly when dealing with finite samples. It highlights the difficulty in separating detailed structures in P(x \u03b1 |z \u03b1 ) compared to P(x \u03b3 |z \u03b3 ), and how this impacts the weight connections in the neural network. The text also mentions the weak signal from z \u03b3 in relation to the label, emphasizing the importance of meaningful receptive fields in the training process. The text discusses the impact of spurious correlations in deep learning training, particularly with finite samples. It explains how weights connecting to meaningful receptive fields receive weak gradients, leading to overfitting. More data helps alleviate overfitting by improving accuracy and distinctiveness. The appearance difference between different factors may be purely noise, affecting generalization error. Different induction biases can yield drastically different outcomes. Gradient Descent: Stochastic or not? Previous research shows that stochastic gradient descent (SGD) with small batch sizes converges to \"flat\" minima, offering better generalization than larger batches. Using a perturbed version of P(z \u03b1 , z \u03b2 ) at each iteration reduces overfitting issues caused by data distribution sensitivity. This approach helps neural networks fit random-labeled data and generalize well for real data. The paper proposes a novel theoretical framework for deep nonlinear networks with ReLU activation and local receptive fields, addressing issues related to data distribution sensitivity. It reveals more network structures compared to non-convex modeling and can model deep networks without assuming idealistic data distributions. The framework also analyzes regularization techniques like Batch Norm, showing its compatibility with the proposed framework. The paper introduces a novel theoretical framework for deep nonlinear networks with ReLU activation and local receptive fields, addressing data distribution sensitivity. It explores various practical issues in deep models and offers a new perspective on overfitting, generalization, and disentangled representation. Future work aims to delve deeper into these core issues using the theoretical framework presented. The activation function and gradient for a locally connected network are defined, along with weight updates for gradient descent. The marginalized gradient is introduced as the average of input images excluding one, and the back-propagated gradient to a node is discussed in relation to the activation function. The marginalized gradient in a locally connected network is computed independently by marginalizing with respect to regions outside the receptive field of a node. The relationship between these gradients respects the locality structure, as shown in Theorem 1 and Theorem 2, which outline conditions for the focus and broadness of knowledge, and decorrelation. The proof involves equations 22a, 22b, 25, 26, 36, and 40, showing the relationship between different entries and the use of knowledge focus and broadness. The reformulation is exact if z \u03b1 contains all information of the region, as stated in Theorem 3. The reformulation becomes exact when P(x j |z \u03b1 ) is a delta function, indicating z \u03b1 contains all information of x j. This implies broadness and focus of knowledge, as well as decorrelation between different functions. The matrix representation involves weight matrices, probability matrices, and diagonal matrices encoding prior probabilities. The text chunk discusses the use of group \u03b1 and \u03b2 with receptive fields x \u03b1 and x \u03b2. It simplifies equations and discusses gradient update rules, assuming decorrelation. The focus is on backpropagation of Batch Norm and gradient updates. The context before it involves reformulation, broadness, focus of knowledge, and decorrelation in matrix representations. The text discusses the vector form of forward pass of batch normalization and the compact update equations with clear geometric meaning. It also covers the computation of the Jacobian of the batch normalization layer using vector notation. The text discusses the vector form of forward pass of batch normalization and the compact update equations with clear geometric meaning. It also covers the computation of the Jacobian of the batch normalization layer using vector notation. In vector f, P \u22a5 f projects a vector into the orthogonal complementary space of f. The symmetric projection matrix projects the input gradient to the orthogonal complement space spanned by x and 1. An interesting property is that any gradient back-propagated to the input of BN layer will be orthogonal to that activation. The Batch Normalization layer helps stabilize training by conserving the row energy of the weight matrix over time, preventing energy \"leakage\" between layers. This is achieved by placing the Batch Normalization layer after ReLU activation and linear layer, ensuring that the row energy remains constant. Theoretical framework analyzes gradient descent in deep models under specific data distribution conditions. Examples include the role of nonlinearity and conditions for achieving disentangled representation. General comments on deep learning issues like overfitting and GD versus SGD are also provided. Linear models struggle with exponential events, while nonlinear models with ReLU can handle them. Definitions for convex hull and k-vert matrices are given. The matrix P is defined as k-vert if its k rows are vertices of the convex hull. Theorem 6 states that under certain conditions, there exists a weight such that the activation F \u03b1 = I for all \u03b1. This is proven by induction, with the base case being trivial. If each P \u03b1\u03b2 is all-vert, then all diagonal elements of F raw \u03b1 are 1. After ReLU activation, the matrix F \u03b1 becomes an identity matrix. Through induction, it is shown that F \u03c9 = I and G \u03c9 = 0. In the linear case, the rank of F \u03b1 is bounded by the size of \u03b1's receptive field. The information in \u03b1 grows exponentially with the receptive field size. The loss for a linear network is at least proportional to the size of the receptive field. This holds even with a Batch Normalization layer in between due to its linear transformation. This highlights the importance of nonlinearity in neural networks. Theorem 7 states that if F is full row rank, then the property of being all-vert is preserved. This means that if all P \u03b1\u03b2 are all-vert and its input F \u03b2 is full-rank, then F \u03b1 can be made identity. Sampling W randomly ensures that all F \u03b2 are full-rank, including the top-level input F 1. The analysis assumes full-rank F \u03b2, particularly F 1, and using top-level W 1 alone can yield zero generalization error. In practice, when n \u03b1 m \u03b1 = O(exp(sz(\u03b1))), the network needs to represent information properly for classification. A theory for disentangled representation in deep nonlinear networks is complex and not covered here. In this paper, the focus is on constructing factorizable P \u03b1\u03b2 to enable disentangled representation in the forward pass. The definition of disentangled representation is discussed, along with the question of whether gradient descent preserves this structure. Theorems and lemmas are presented to address these issues, with proofs based on properties of tensor products. Theorem 8 states that if certain conditions are met regarding tensor products and separability, then the disentangled nature of F \u03b1 can be inferred from F \u03b2. The disentangled nature of F \u03b1 can be inferred from F \u03b2 by computing the quantity P \u03b1\u03b2 F \u03b2. If each \u03b1 \u2208 pa(\u03b2) is informative in a diverse way, then the forward information sent from \u03b2 to \u03b1 can lead to a better structure. Gating D \u03b2, which is disentangled, will also play a role in regularizing G \u03b2."
}