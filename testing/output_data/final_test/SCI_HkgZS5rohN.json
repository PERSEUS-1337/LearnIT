{
    "title": "HkgZS5rohN",
    "content": "Structured tabular data is commonly used in industry, with Gradient Boosting Trees, Support Vector Machine, Random Forest, and Logistic Regression being popular for classification tasks. The Super Characters method using two-dimensional word embeddings has shown promise in text classification. A new approach called SuperTML is proposed, which utilizes two-dimensional embeddings and ImageNet CNN models for classification on tabular data, achieving state-of-the-art results on both large and small datasets. In data science, structured data is categorized as tabular data. DNN models are used for unstructured data like images, speech, and text. CNNs and RNNs excel with unstructured data. CNN models have surpassed human performance in image classification tasks. Machine learning models like SVM, GBT, Random Forest, and Logistic Regression are used for structured data processing. Recent research has focused on using one-dimensional embedding, RNNs, and CNNs for structured data processing tasks. XGBoost is currently dominating structured data competitions, with at least 65% of data scientists working daily with relational data. This reliance on one-dimensional embeddings may soon change based on recent NLP research findings. The Super Characters method BID9, known for achieving state-of-the-art results on large datasets, involves drawing input text characters onto an image and using two-dimensional CNN models for classification. A new method called SuperTML applies this concept to tabular data, projecting features onto a two-dimensional embedding for classification while handling categorical types and missing values. The SuperTML method automates handling categorical type and missing values in tabular data by treating features as stringified tokens, allowing for application of existing CNN models from the Super Characters method. This approach combines two-dimensional embedding with pre-trained CNN models to achieve state-of-the-art results on text classification tasks. Unlike text classification, tabular data features are in separate dimensions, requiring generated images for analysis. SuperTML automates handling categorical type and missing values in tabular data by using two-dimensional embedding and pretrained CNN models. The first step involves projecting tabular features onto SuperTML images, while the second step fine-tunes CNN models on these images. For inference, testing data undergoes the same preprocessing to generate SuperTML images. The SuperTML method automates handling categorical type and missing values in tabular data by using two-dimensional embedding and pretrained CNN models. It involves projecting tabular features onto SuperTML images and fine-tuning CNN models on these images. The SuperTML EF method is introduced to make the process more autonomous and efficient. The Iris dataset from the UCI Machine Learning Repository is widely used in machine learning courses and tutorials. The Iris dataset is commonly used in machine learning. SuperTML images are generated using Iris data, with features displayed in different sizes based on importance score. Experimental results show SuperTML slightly outperformed XGBoost. The Adult dataset aims to predict income levels based on surveyed data. The SuperTML method outperforms the fine-tuned XGBoost model by 0.32% accuracy on the Adult dataset, which aims to predict income levels. The Higgs Boson Machine Learning Challenge involved classifying quantum events as signal or background. The Learning Challenge on Kaggle involved binary classification of quantum events as signal or background. The challenge data has 25,000 training samples and 55,000 testing samples with 30 features each. The AMS score was used as the performance metric, with the SuperTML EF method achieving the best score of 3.979. The method uses two-dimensional embeddings, outperforming other algorithms like DNN and XGBoost. The SuperTML method, based on two-dimensional embeddings from Super Characters, achieves state-of-the-art results on tabular datasets without calculating importance scores."
}