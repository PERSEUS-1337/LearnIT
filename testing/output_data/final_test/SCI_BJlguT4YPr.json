{
    "title": "BJlguT4YPr",
    "content": "The sparse-matrix reified KB representation enables scalable, fast, and expressive neural modules for multi-hop inferences. It can be distributed across multiple GPUs, scale to large KBs, and achieve competitive performance on various tasks. Previous work focused on using neural networks to generalize KB contents through low-dimensional embeddings of entities and relations. Incorporating a symbolic KB into a neural system involves designing fully differentiable, accurate, expressive, and scalable neural KB inference modules. This allows for injecting knowledge from a KB directly into a neural model, enabling non-trivial inferences and large KB incorporation. This approach is motivated by tasks like learning neural semantic parsers from denotations. Learning neural semantic parsers from denotations involves using KBQA methods to translate natural-language questions into structured queries against a KB, which are then executed with a symbolic KB query engine. Researchers have also explored learning semantic parsers from denotations as an alternative to pairing natural-language questions with structured queries for training data. Learning semantic parsers from denotations is challenging due to the non-differentiable operation of reasoning with symbolic KBs. Prior systems have used heuristic search or gradient approaches to infer structured queries from denotations, but noise and ambiguity can arise. At the time of writing, entities and relations in a knowledge base are encoded as vectors and matrices. Some systems use reinforcement learning for KB reasoning, while others have \"neuralized\" KB reasoning over small KBs. This approach requires coupling a learner with a mechanism to retrieve relevant information. In this paper, a novel scheme is introduced for incorporating reasoning on a large question-independent KB into a neural network using a sparse-matrix reified KB. This compact encoding allows for faster processing compared to alternative implementations, even for KBs with many relations. The new architectural component simplifies neural semantic parsing from denotations, leading to more efficient architectures. Neural reasoning architectures based on end-to-end processes are competitive with state-of-the-art for benchmark tasks. These models are the first fully end-to-end neural parsers from denotations applied to these tasks. The architectures scale to long chains of reasoning on synthetic tasks and KB completion. A relation in a knowledge base (KB) is a set of entity pairs representing relationships between entities. Weighted sets are used to encode entities in the KB, with each element associated with a non-negative real number. The weight of an element in a weighted set indicates the confidence level or multiplicity of that element. Hard sets have all elements with a weight of 1, while weighted sets can be encoded as entity-set vectors. In a knowledge base, relations are represented as sets of entity pairs. Entities are encoded using weighted sets, with each element having a non-negative real number as its weight. Relations in the KB are associated with integer indices, and can be encoded as relation matrices. Relations in a knowledge base are represented as sets of entity pairs, encoded using sparse matrices to efficiently store large amounts of data. Using a sparse coordinate pair (COO) encoding, each KB fact only requires storing two integers and one float, making it a practical solution for handling vast amounts of information in a knowledge base. Our implementations are based on Tensorflow, which has limited support for sparse matrices. Tensorflow only supports matrix multiplication between a sparse matrix COO and a dense matrix, not between two sparse matrices or between sparse higher-rank tensors and dense tensors. Grouping entities into disjoint sets by type can lead to useful optimizations in knowledge bases. The full formal extension of definitions to typed entities and relations is given in Appendix A. Relations can be viewed as labeled edges in a knowledge graph, with entities as vertices. R-neighbors of an entity xi are entities xj connected by an edge labeled r. Computing R-neighbors is a single-step reasoning operation. Multi-hop reasoning involves nested R-neighborhoods. For example, R-neighbors(R-neighbors(X)) for R = {actor_of} is the set of actors in movies produced. The text discusses approximating the computation of R-neighbors using differentiable operations on vectors encoding sets of entities and relations. It introduces a relation-set following operation for logical reasoning, closely related to the R-neighborhood operation. The support of this operation is the set of R-neighbors. The implications of this are further discussed in the appendix. The text discusses the space complexity of different matrix operations in Tensorflow for relation-set following. Two implementations, naive mixing and late mixing, are compared in terms of their ability to handle sparse matrix computations and extend to minibatches. Late mixing is more suitable for minibatches compared to naive mixing. Late mixing can be extended easily to minibatches, leading to N R matrices XM k of size O(bN E). The space complexity becomes O(bN E + bN R + N T) and an additional cost is summing up N R dense matrices. Realistic wide-coverage knowledge bases with many relations make late mixing expensive when many relations are mixed. Late mixing in knowledge bases can be costly when many relations are mixed. An alternative approach is to represent each KB assertion as a tuple (i, j, k) where i, j, k are the indices of entities and relations. Sparse matrices M subj, M obj, and M rel map the triples to subject, object, and relation respectively. The relation-set following can be implemented using these matrices, with operations extending to minibatches. The reified KB has a size of O(N T). The reified KB representation is compact, using only six integers and three floats for each KB triple. It is preferable for knowledge bases with many relations. The analysis shows no dependence on N R, with a final size of O(bN T + bN E). Speedups of reified KBs over baseline implementations are shown in Figure 1. It is important to be able to distribute a KB across multiple GPUs due to limited GPU memory. We implemented a distributed sparse-matrix representation of reified KBs to distribute matrices across multiple GPUs. The KB used for scalability studies is based on an n-by-n grid with O(n^2) entities and O(n) triples. Inference time was measured for 2-hop inference on minibatches of b=128 vectors. The study implemented 2-hop inference on minibatches of b=128 vectors, comparing late mixing and reified KB methods. Results show late mixing is faster with fewer relations, but reified KB is faster with more than 20 relations. Reified KB outperforms late mixing significantly with 1000 relations. The key-value network is faster than the naive approach, but slower for larger datasets. Reified KB scales better and can handle more entities and relations efficiently. It is closely related to key-value memory networks and offers a more efficient implementation for reasoning with symbolic KBs. Including an entire KB in a model can simplify complexity by eliminating the need for retrieval machinery. Using a reified KB for reasoning simplifies complexity by eliminating the need for retrieval machinery. Tasks such as learning semantic parsers and completing a KB can benefit from this approach. MetaQA dataset includes one-hop, two-hop, and three-hop questions with corresponding entity ids for reasoning chains. The neural model uses a reified KB for reasoning, predicting relations and chaining them together. The model computes relation sets based on the question and predicts answers using softmax. Each step involves a linear projection of a common encoding for the question. The full KB is loaded into a single GPU for processing. The neural model utilizes a reified KB for reasoning and predicting relations. The model simplifies KB reasoning by encapsulating it in a single scalable differentiable neural module, reducing the complexity of learning structured KB queries. The neural model simplifies KB reasoning by using differentiable functions for each reasoning \"hop\". These functions are interpretable as mixtures of relation identifiers, making optimization simple by back-propagating loss to relation-prediction functions. KBQA on FreeBase involves answering natural language questions using entities from FreeBase, which includes real-world entities and compound value types. The dataset used for KBQA involves non-binary relationships or events, with questions answerable within 1 or 2-hop chains passing through a CVT entity. The model derives relation sets and mixes potential inferences using softmax and cross entropy loss. Freebase subset with 43.7 million facts and 12.9 million entities was used, split across GPUs for processing. The KB was split across three 12-Gb GPUs, with a fourth GPU used for the model. Storing all KB triples in memory for relation-set following is impractical due to size constraints. Key-value memory networks typically use a subset of the KB for memory population. Knowledge base completion is treated as an inference task in the model. KB completion is approached as an inference task, similar to KBQA, where queries are relation names and head entities used to predict tail entities through multiple inference chains. An encoder-decoder architecture is employed for complex reasoning tasks, generating artificial natural-language sentences to describe longer chains of relationships on a grid. The final output is a softmax mix of all entities, with the question encoded using the final hidden state of an LSTM. The model encodes the question with the final hidden state of an LSTM and generates a reasoning chain using a decoder LSTM. The final predicted location is a mixture of all steps weighted by the probability of stopping at each iteration. Experimental results show the model's performance relative to strong baselines. The study presents models for various tasks, not aiming to challenge current benchmarks but to introduce a scheme for incorporating symbolic KB reasoning. The focus is on simple models using word2vec for question encodings and linear classifiers for relation predictions. The emphasis is on confirming the effectiveness of the reified KB models. The study focuses on confirming the effectiveness of reified KB models by comparing their performance on KBQA tasks to a Key-Value Memory Network baseline. ReifKB consistently outperforms the baseline, especially for longer reasoning chains, showing little degradation as chain length increases. The synthetic grid task demonstrates the ability to predict entities and relations in a KB. The ReifKB model is compared to other complex architectures for end-to-end question answering, showing competitive performance, especially on difficult 3-hop settings. A small extension involves masking seed entities in answers, resulting in slightly better performance than GRAFT-Net on 2-hop and 3-hop questions. The model is evaluated on the NELL-995 dataset for KB completion, outperforming popular embedding approaches. The ReifKB model, compared to other architectures for question answering, shows competitive performance, outperforming popular embedding approaches on the NELL-995 dataset for KB completion. The model uses inference on the existing symbolic KB, requiring fewer parameters and learning logical inference chains in the incomplete KB. The ReifKB model demonstrates competitive performance in question answering, surpassing popular embedding methods on the NELL-995 dataset for KB completion. It utilizes logical inference on the symbolic KB, requiring fewer parameters and learning inference chains within the incomplete KB. In contrast, the MINERVA model, which uses reinforcement learning, slightly outperforms simple KB completion models but is trained to find a single answer rather than inferring a set of answers. The ReifKB model outperforms MINERVA in question answering by inferring sets of answers from KB relations. MINERVA's performance drops significantly when multiple locations are involved. Additionally, the modified MetaQA also shows poor performance. The training time of ReifKB model is compared with minibatch size of 10 on various datasets, taking less than 10 minutes for one epoch. The ReifKB model outperforms MINERVA in question answering by inferring sets of answers from KB relations. It takes less than 10 minutes to run one epoch over WebQuestionsSP on four P100 GPUs. The tradeoffs between accuracy and training time for our model and three baselines on the MetaQA 3-hop task are summarized. The state-of-the-art PullNet system is about 15 times slower than the reified KB system. GRAFT-Net is slightly less accurate but only slightly faster. A key-value memory baseline including the full KB is nearly three times as slow as our system. The relation-set following operation using reified KBs is implemented in an open-source package called NQL, which offers a broader range of operations for manipulating KBs. TensorLog, a probabilistic logic system, can be compiled to Tensorflow but does not support relation sets like NQL. The differentiable theorem prover (DTP) is another approach to neuralizing a KB but lacks support for efficient reified KB representation. The differentiable theorem prover (DTP) is a less scalable approach compared to reified KB representation. Neural ILP system focuses on rule-learning and does not scale well to large tasks like NELL995. Our proposed approach offers a reusable neural component for KB embedding methods. Embedded KBs have disadvantages compared to reified KBs, as they are larger and have more parameters. Models that allow multi-step inference often produce errors on long reasoning chains. Our focus is on accurate reasoning in symbolic KBs, considering scalability with sparse matrix representations. The definition of relation-set following is similar to the bilinear model for path following, but it generalizes to path queries with weighted sets of relations. This allows for learning relations in paths. Different from previous work, the vector representation used for weighted sets in a reified KB makes intersection and union operations trivial to implement. Neural architectures like memory networks or those using attention can be used for soft versions of relation-set following, but they do not scale well to large KBs. Relation-set following and graph CNNs have been used for reasoning tasks, but they do not scale well to large knowledge bases. Existing implementations have not been scaled to handle millions of triples/edges or entities/graph nodes. While graph CNNs have been used for reasoning tasks, the formal connection between them and logical reasoning is unclear, unlike relation-set following which has a precise connection to inference. Reinforcement learning methods have been applied to KB completion tasks and learning mappings from natural-language questions to non-differentiable logical representations. The text discusses a novel method for representing a symbolic knowledge base (KB) called a sparsematrix reified KB. This representation enables neural modules that are fully differentiable, expressive enough to model multi-hop inferences, and scalable for use with large KBs. The method is compared to MINERVA and gradient-based approaches are preferred for their ease of implementation and combination with other architectural elements. The text introduces a new architectural component that significantly speeds up relations in neural semantic parsing and KB completion. It simplifies architectures for neural KBQA models, enabling end-to-end learning from text to KB entity sets for large KBs with millions of triples and entities. Each entity in the KB has a unique index and type, facilitating efficient representation and processing. The text introduces typed weighted sets for relations in a knowledge base (KB). Relations have subject and object types, and are encoded as subsets of entity indices. Differentiable operations are based on typed weighted sets, with each element associated with a non-negative real number. Sets of relations must be type-compatible, and can be used for tasks like KBQA models. The text discusses encoding relations in a knowledge base as typed weighted sets, with relations represented by matrices and entity indices. Sparse matrices are used for encoding, with a COO encoding format. The paper assumes equal weights of 1.0 for non-zero values in relation matrices, but suggests extending this for confidence values in a knowledge base. The text discusses extending the knowledge base to encode weights for triples, redefining M rel to hold the weight. It explains how follow(x, r) can approximate the R neighborhood by adjusting components of x and r. The smooth, differentiable path to this approximation is detailed using matrices and vectors. The text discusses distributing matrices for a reified KB horizontally, storing different triple ids on different GPUs. The challenge lies in adapting to mini-batches due to the absence of general sparse tensor contractions. The text discusses late mixing as an alternative strategy for mini-batches in sparse tensor contractions. It involves mixing the output of single-relation following steps instead of the KB itself. The implementation of relation-set following for the reified KB can be extended to a minibatch in grid experiments. The text discusses the number of relations in key-value networks and the runtime for queries on untrained networks. It mentions the efficiency of the model using CVT and entity nodes, as well as three types of relations. The model uses CVT and entity nodes with three types of relations. MetaQA experiments involve 2-hop questions like \"Who co-starred with Robert Downey Jr. in their movies?\" Triples in the knowledge base are represented as (subject, relation, object) triples. In MetaQA questions, seed entities are often excluded after the second hop. The extended model outperforms GRAFT-Net on 2-hop and 3-hop questions. The model has access to outputs of all chains of length less than t, expanding to a mixture of paths applying different subsets of functions to the initial input. The model's weights can be controlled by varying relation vectors, increasing expressive power without new parameters. The final mixture biases towards shorter paths, proving useful in practice."
}