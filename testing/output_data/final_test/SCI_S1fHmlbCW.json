{
    "title": "S1fHmlbCW",
    "content": "Designing neural networks for continuous-time stochastic processes is challenging, especially with irregular observations. The article analyzes neural networks from a frame theoretic perspective to identify conditions for recoverable signal representations in L^2(R). It shows that certain networks can effectively compute a Discrete Wavelet Transform, dividing signals into orthogonal sub-spaces of different temporal scale and localization. The resulting neural network is evaluated on tasks like auto-encoding, video classification, and financial forecasting. The article discusses designing neural networks for continuous-time stochastic processes with irregular observations. It highlights the challenges of working with irregularly observed time series and the limitations of interpolation schemes. It mentions the use of Gaussian processes, kernel learning, and deep learning for analyzing irregularly observed time series. In this article, the focus is on investigating the representation of time series data processed by neural networks to ensure information preservation for irregularly observed signals. The analysis involves studying neural networks from a frame theoretic perspective, considering the impact of discrete sampling on continuous-time signal representations. Recent work has linked frames with non-linear operators in Banach space, leading to the concept of non-linear frames, which is extended to characterize various neural network families. This research explores the composition of non-linear neural layers for information preservation in signal processing. Frame theory is applied to analyze neural networks, specifically non-linear layers like convolutions and fully-connected layers forming non-linear frames in L 2 (R). This theory can also be used to study randomly-observed time series, such as Hawkes processes, showing that stable processes almost surely yield non-linear frames on band-limited functions. Convolutional neural networks can efficiently divide the time-axis of a time series into orthogonal sub-spaces of different temporal scale and localization under certain conditions. Neural networks can divide time series into orthogonal sub-spaces of different temporal scales and localization by optimizing convolution filters to compute a Discrete Wavelet Transform. This allows for learning sparse representations of signals and improving accuracy and efficiency in tasks like video classification and financial forecasting. The article introduces theoretical analysis on neural networks for recoverable signal representations in L 2 (R) and shows that enforcing constraints on convolutional filters leads to coordinate-dependent representations in an orthonormal basis. Numerical experiments evaluate the network on tasks like auto-encoding, video classification, and financial forecasting. The inner product spaces L 2 (R), l 2 (Z), and l 2 d (Z) are defined with their respective norms induced by inner products. The inner products of these spaces form Hilbert spaces on equivalent functions for the Lebesgue measure. The Fourier transform F T [\u22c5] and complex conjugate z are introduced, along with the Fourier transform of a sequence. The notation for a function and a set A is also explained. The text discusses the concept of frames in signal representation, focusing on conditions for composite functions to produce discretized representations of continuous-time signals that can be smoothly reconstructed. Frame theory is introduced as a tool to characterize suitable properties for linear representations of irregularly observed signals, defining a frame as an operator from L 2 (R) to l 2 (Z) characterized by a family of functions. Frames in signal representation are essential for producing discretized representations of continuous-time signals that can be smoothly reconstructed. A frame in L 2 (R) consists of atoms that depend smoothly on their inputs and can be inverted in a smooth manner on its image. Examples include the Haar function and orthonormal families of functions. These frames are crucial for non-linear representations of L 2 (R) to be homeomorphic. A non-linear frame is an operator from L 2 (R) to l 2 (Z) characterized by families of functions and real-valued functions. It is invertible on its image of L 2 (R) and ensures smooth signal recovery. Smooth signal recovery is crucial for non-linear signal approximations. Smooth signal recovery is essential for stable non-linear signal approximations. Sufficient conditions on non-linear operators to create non-linear frames are explored, introducing definitions on multivariate real-valued functions. BLI operators and framing constants are defined, leading to a theorem on BLI operators and linear frames. The proof highlights the importance of carefully defining non-linear frames and the preservation of injectivity by composition. In this study, we establish guarantees about operator pipelines using simple conditions that are easy to verify. We then apply this theory to analyze the representational properties of CNN and RNN architectures, specifically those utilizing depth-wise separable convolutions. By imposing constraints on temporal filters, we create a non-linear frame network, sacrificing expressiveness for representational guarantees. In depth-wise separated convolution stacks, a temporal convolution is applied before a depth-wise linear transform and a leaky ReLU layer. The assumption is that the depth-wise linear operators are full rank, inspired by multi-scale parsing with discrete wavelet transforms. Time domain convolutions are used as conjugate mirror filters, forming a decomposition filter bank. This bank guarantees injectivity with a dual reconstruction filter bank. Element-wise Leaky ReLU (LReLU) applies a piecewise linear function element-wise. The text discusses the properties of LReLU and depth-wise fully connected layers in convolutional neural networks. It mentions the invertibility of LReLU and the representational properties of time domain convolution layers with filters constrained in the Frequency domain. Reconstructible convolution layer (RConv) involves convolution filters h, g \u2208 l 2 (Z) with conditions for existence. Temporal convolutions with Fourier transforms satisfying specific conditions are invertible operators. The text discusses injectivity and properties of convolution filters in convolutional neural networks. The text discusses the stability of injectivity in composite operators and the framing properties of RNNs, highlighting the issue of vanishing gradients in popular recurrent architectures. It also presents a proposition stating that saturating RNNs do not provide non-linear frames. The text explores representations of irregularly sampled data using non-linear frames, showing that neural networks can still produce a homeomorphic representation even with irregular signals. Sampling by Hawkes processes is common in finance, seismology, and social media analytics. The text also introduces the canonical filtration associated with stochastic processes and recommends further reading on the topic. Sampling density of stable Hawkes processes: If the Hawkes process is stable, then almost surely a complete proof of the ergodic behavior is provided. This process can model random observation times in a continuous-time series where information is observed asynchronously across multiple channels. The lemma explains that exact recovery is only possible for band-limited functions under a random sampling scheme. Framing is preserved almost surely under a stable Hawkes process random sampling. Theorem 1.4 states that under a stable Hawkes process random sampling, recovery of band-limited signals is possible with a non-linear frame operator that is invertible on its image by a Lipschitz inverse. This result is a direct consequence of previous propositions and theorems. Theorems and propositions have shown the recovery of band-limited signals is possible with a non-linear frame operator that is invertible by a Lipschitz inverse. However, limitations exist when observations are on compact intervals or with finite signal approximations. Theorems and propositions demonstrate the recovery of band-limited signals using a non-linear frame operator invertible by a Lipschitz inverse. The impact of approximation errors on functions observed on compact intervals and finite signal approximations is discussed. Wavelet decomposition can efficiently approximate non-periodic functions that are smooth and not band-limited, with a certain number of scales of decomposition and scalars representing the approximation of functions in the space of Lipschitz functions. The text discusses the importance of Lipschitz-ness of frames and BLI functions in ensuring accurate representation of continuous-time signals. Additional conditions on time-domain convolutional filter banks are shown to guarantee minimal representation. The text introduces multi-resolution approximations for obtaining different representations of a time series while avoiding redundancy. It discusses the existence of a Riesz basis with a scaling function, such as the Haar wavelets, and provides general conditions for a function to be a scaling function. The Conjugate mirror temporal convolution layer (CMConv) is defined with convolution filters and a scaling function for a multi-resolution approximation. The Wavelet function is also discussed as an orthonormal basis. The text discusses depth-wise separable convolutions with scaling and wavelet filters, ensuring orthogonality. It presents a non-linear frame dependent on the coordinates of f in an orthonormal basis of L2(R). The proof involves showing the computation of a Discrete Wavelet Transform using cascading convolutions. The convolutions compute a Discrete Wavelet Transform, yielding coordinates of f in an orthonormal basis. The orthogonal CNN provides novel information about the input signal's properties on a specific temporal scale. 1x1 convolutions operate in depth, preserving the temporal scaling properties of the Discrete Wavelet Transform. The implementation approximates constraints by computing the Fast Fourier Transform of the filter. In solving the minimization problem for neural networks, an adapted solver can find an optimal solution rapidly by exploiting the problem's structure. The experiments explore the properties of neural networks compared to baselines, generating irregular signals in the first numerical experiment. The objective is to train conjugate mirror filters with stochastic optimization methods to improve representational power. A 16 parameter filter is trained for compression, selecting 64 scalar values with higher magnitude from 128 observations. An inverse Discrete Wavelet Transform is used for reconstruction, with quality measured by squared L2 loss. The reconstruction quality of the input signal is measured using squared L2 loss. The model is trained using RMSProp optimization algorithm for 2,500 iterations with a learning rate of 10^-3. A constraint enforcing program is used every 100 iterations. The procedure improves filters and out-performs an LSTM-based auto-encoder model. Additionally, a wavelet representation can be combined with recurrent architecture to handle noisy data, especially useful for LSTM networks. The YouTube-8M dataset contains millions of videos. The YouTube-8M dataset contains millions of videos with pre-featurized frames. Models for this dataset must leverage the temporal structure as the raw video feed is not available. Authors achieved state-of-the-art results in video classification using a 2-layer LSTM model. In an experiment, a similar model was trained on a multi-scale wavelet representation of data, reducing the total number of parameters in the recurrent network. The curr_chunk discusses a model with fewer parameters in recurrent layers, improving performance of neural networks. It also mentions the importance of forecasting trade volume in financial markets. The curr_chunk discusses the challenges of forecasting trade volume in financial markets using neural networks. It compares the performance of a wavelet transform network and an LSTM model, showing that the wavelet transform network is more robust and improves prediction performance by half a percent. This improvement represents 50 thousand USD of exchanged volume over a 15-minute period. The article analyzes neural networks from a frame theoretic perspective. The curr_chunk discusses analyzing neural networks from a frame theoretic perspective, leveraging recent contributions to frame theory to devise robust convolutional neural networks for time series. It proves properties about non-linear frames for convolutional neural networks, ensuring injective and bi-Lipschitz representations of continuous time signals. The article also highlights that bounded-output recurrent neural networks do not meet the conditions for non-linear frames. The curr_chunk discusses building a convolutional neural network that computes a Discrete Wavelet Transform with dynamically learned filters. The network preserves orthogonality and properties of non-linear frames, benefiting real-world prediction tasks by producing compact representations for efficient learning on continuous-time stochastic processes. Wavelet approximations are utilized to show orthonormality, with Wavelet functions defined as high frequency mirrors of low frequency Scale functions. The curr_chunk discusses functions with bounded support and their approximation by polynomial splines on the interval [0, 1]. The proposition from BID22 examines how this approximation affects the representations used. The architecture proposed for Youtube video classification leverages a multi-resolution approximation computed by a wavelet convolution stack."
}