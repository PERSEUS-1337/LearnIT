{
    "title": "rkMk9j0qYm",
    "content": "Explainable Adversarial Learning (ExL) is a method for training neural networks to be robust against adversarial attacks by incorporating generative modeling of random noise. The approach improves the model's understanding of data manifold, enhancing adversarial robustness. ExL combined with adversarial training shows remarkable performance against various attacks, surpassing human capabilities. Machine Learning models vulnerable to adversarial attacks across various platforms like computer vision, malware detection, and gaming environments. Adversarial inputs can transfer across models, enabling simple Black-Box attacks. Some defenses against adversarial attacks have shown vulnerabilities to stronger attacks. The state-of-the-art defense is ensemble adversarial training. The state-of-the-art defense against adversarial attacks involves ensemble adversarial training, which augments the training dataset with adversarial examples from other models. Despite advancements in robustness to White-Box attacks, errors still occur for perturbations beyond the model's training. Hypotheses suggest that the linear behavior of deep neural models in high-dimensional spaces and adversarial examples being off the data manifold contribute to model susceptibility. In this paper, Explainable Adversarial Learning (ExL) is proposed to improve model generalization by introducing multiplicative noise into training inputs. The noise is optimized with Stochastic Gradient Descent (SGD) to approximate the input distribution and maximize the likelihood of class labels. Input noise is gradually learned during training, as shown in FIG0 with a ConvNet architecture learning handwritten digits from the MNIST dataset. The noise learned with Explainable Adversarial Learning (ExL) on colored CIFAR10 images reveals prominent color blobs on a greyish-black background, de-emphasizing background pixels. This noise gradually transforms to highlight dominant features in the training data, suggesting the model not only predicts accurately but also provides the right explanation. Explainable Adversarial Learning (ExL) improves model generalization by incorporating out-of-sample data without direct data augmentation. Empirical evaluation using PCA shows models with adversarial robustness have lower distance between adversarial and clean inputs in the Principal Component subspace. This noise modeling enhances the understanding of input/output distribution characteristics. Explainable Adversarial Learning (ExL) injects random noise during training to improve model generalization and reduce dimensionality of adversarial examples. ExL enhances adversarial robustness on tasks like MNIST, CIFAR10, and CIFAR100, especially when combined with ensemble/PGD adversarial training. The noise dimensionality matches the input size, and mini-batch SGD optimization is used in all experiments. The noise dimensionality matches the input size, and mini-batch SGD optimization is used in all experiments. The total number of noisy templates equals the total number of inputs in each minibatch. Noise learning follows the overall loss minimization to enforce maximum likelihood of the posterior. Experiments show that using multiplicative noise (X \u00d7 N) for ExL training yields improved noise characteristics compared to additive noise (X + N). Multiplicative noise maintains accuracy levels similar to standard SGD, while additive noise results in a significant accuracy loss. Utilizing only negative gradients during backpropagation for ExL leads to the best accuracy. Visualizing the noise learned after training, X + N severely distorts the image, while X \u00d7 N has a subtle effect, aligning with the accuracy results. The study compared the effects of multiplicative noise (X \u00d7 N) and additive noise (X + N) on model optimization and adversarial robustness. It was found that noise templates learned with X \u00d7 N and X + N were very different, with X \u00d7 N showing slightly better resistance to adversarial attacks. The effectiveness of multiplicative noise training in improving a model's intrinsic adversarial resistance was established through empirical studies. In experiments, noise N is initialized from a random uniform distribution. The noise is used during evaluation by averaging it across templates and applying it to test images for predictions. An optimization perspective using maximum likelihood criterion for classification tasks is discussed, mentioning the use of techniques like regularization and momentum for improved optimization. An adversarially robust model should include off-manifold data points when modeling the conditional probability p(Y |X, A; \u03b8). Using Bayes rule, predictions can be derived from posterior modeling from a generative standpoint. Additional studies comparing X +N vs. X \u00d7N with different gradient update conditions are available in the appendix. Adversarial training methods like BID30, BID15, and BID21 augment training data with adversarial samples to enhance robustness against specific adversaries. However, these methods are still vulnerable to stronger attack scenarios. Achieving robustness requires modeling the adversarial distribution and joint input/output distribution characteristics, which is a challenging engineering task. Some recent works use generative models to address this challenge. Explainable Adversarial Learning addresses difficulties in implementing generative models for robustness by modeling noise based on prediction loss. The noise learning in ExL aims to maximize the likelihood of the posterior distribution while implicitly generating potential adversarial examples. During training, ExL models focus on maximizing the posterior distribution to enhance robustness against adversarial attacks. Empirical evidence using PCA shows that noise modeling captures off-manifold data points, allowing for visualization of the relationship between clean and adversarial inputs in a reduced feature space. During training, ExL models aim to enhance robustness against adversarial attacks by maximizing the posterior distribution. Using PCA, the learned features of a trained model are centered around zero, factorized using SVD, and projected onto a new subspace. Visualizing the representations of a ResNet18 model's Conv1 layer on CIFAR-10, it is observed that adversarial images emphasize low-variance PCs, while clean images do not. The images emphasize low-variance PCs for adversarial examples compared to clean inputs. The cosine distance between adversarial and clean projections in the PC subspace increases for latter PCs, as shown in FIG1. The ExL noise enables the model to incorporate more adversarial data into its representation, with a lower cosine distance between adversarial and clean features. The variance in the Conv1 layer's activations of the ResNet18 model in response to clean inputs was analyzed by different principal components. The ExL noise increases the variance along high-rank principal components (PCs) in the ResNet18 model, indicating the inclusion of more data points during learning. This inclusion of off-manifold adversarial points affects the model's behavior. The authors conducted PCA whitening of raw image data for clean and adversarial inputs, showing greater variance in adversarial image coefficients for later PCs. Results from PC subspace analysis align with their experiments, providing insights into the model's behavior for adversarial attacks using the Fast Gradient Sign Method (FGSM) with a step size of 8/255. In this work, the authors study adversarial attacks on images where a perturbation \u2206 is added to create misclassifications by a classifier. They focus on \u221e bounded adversaries and assess robustness against both black-box and white-box attacks. The Fast Gradient Sign Method (FGSM) is used as a simple attack method, with a modification called R-FGSM that includes a small random step to improve effectiveness. The study applies iterative FGSM attacks with bounded perturbations and evaluates the ExL model on MNIST, CIFAR10, and CIFAR100 datasets against BB/WB attacks. Different training scenarios are tested, including ExL Noise, Ensemble Adversarial Training, and PGD Adversarial Training. Our study compares ExL Noise with PGDAdv Training to standard adversarial training methods (SGD ens and SGD P GD). We analyze how noise modeling improves adversarial susceptibility and benefits state-of-the-art PGD/Ensemble adversarial training techniques. EnsAdv training enhances robustness against BB attacks, while PGD training makes a model robust to both BB/WB attacks. The experiments report WB/BB accuracy against strong adversaries. In experiments, WB/BB accuracy against strong adversaries created with PGD attack is reported. For BB, worst-case error over small-step attacks FGSM, I-FGSM, R-FGSM is also reported. Networks were trained with mini-batch SGD using batch size of 64 and momentum of 0.9 (0.5) for CIFAR (MNIST). Additional weight decay regularization was used for CIFAR10, CIFAR100. ExL requires more epochs to converge due to noise modeling. Proper learning rate tuning is essential to prevent overfitting in ExL models. Learning rate for noise (\u03b7 noise) was kept 1-2 orders of magnitude lower than overall network learning rate (\u03b7). ExL noise significantly enhances model robustness against BB attacks. ExL ens/SGD ens models were trained with BB adversaries using R-FGSM with = 0.1. PGDAdv models were trained with WB adversaries using PGD with = 0.3, step-size = 0.01 over 40 steps. For perturbation size = 0.1, ExL ens/SGD ens models have similar accuracy around 98%. However, for larger perturbation sizes (0.2, 0.3), ExL noise trained models show higher prediction capability (> 5%) against PGD attacks. ExL P GD yields better accuracy than SGD P GD even beyond the adversarial training level. In the study, higher accuracies were observed in BB attacks when the source model was trained with ExL noise. The paper focused on evaluating adversarial robustness using BB attacks from models trained without noise modeling. Different architectures were used for CIFAR10 and CIFAR100 datasets, with separate training for target models in each scenario. Adversarial training was conducted using R-FGSM and PGD methods with specific parameters. The study found that ExL noise significantly improved model robustness in BB attacks compared to SGD. The accuracy with ExL noise alone in BB attacks was comparable to models trained with EnsAdv without noise. ExL noise also improved accuracy for perturbations greater than what the network was trained for. The study shows that ExL noise enhances model robustness in BB attacks compared to SGD. ExL noise improves accuracy for perturbations beyond the network's training limits. ExL models exhibit higher resistance to WB attacks, with ExL ens outperforming SGD ens. Adversarial data inclusion in training improves model accuracy against stronger attacks. Variance across leading PCs decreases in the order of ExL P GD > SGD P GD > ExL ens > ExL > SGD ens > SGD. The inclusion of adversarial data points in training improves model explainability and accuracy. The cosine distance between adversarial and clean inputs in the PC subspace increases in the order of ExL P GD < SGD P GD < ExL ens < ExL < SGD ens < SGD, reflecting decreasing variance and justifying accuracy results. The decreasing distance with ExL compared to SGD indicates improved realization of on-/off-manifold data. The integration of noise modeling with adversarial training enhances adversarial robustness, as shown by lower distances for ExL compared to SGD ens. Noise modeling allows for implicit inclusion of adversarial data without explicit data augmentation, leading to comparable accuracy between ExL and SGD ens. Additionally, ExL noise implicitly embraces adversarial points, as evidenced by the evaluation of adversarial subspace dimension using the GAAS method. The dimensionality estimation involves finding orthogonal perturbations near data points causing misclassification. Results show a lower space of adversarial samples for ExL noise compared to standard SGD. Adversarial directions are reduced with EnsAdv and PGDAdv training, especially with ExL P GD for both BB/WB instances. The ExL P GD training significantly reduces adversarial dimensions for both BB/WB attacks. Increasing perturbation size results in fewer misclassified points and adversarial dimensions for models trained with noise modeling. Loss surface smoothening shows ExL can defend against BB attacks well but remains vulnerable to WB attacks. Combining ExL noise modeling with adversarial training improves defense against both BB and WB attacks. The loss surface in SGD is highly curved with steep slopes near data points for both BB and WB attacks. EnsAdv training smoothens out the slope in the BB direction, making models robust against BB attacks. ExL models, even without data augmentation, have a softer loss surface, resulting in lower transfer rates of BB attacks. PGDAdv smoothens out the loss surface in both directions, improving defense against attacks. ExL models, through Explainable Adversarial Learning, improve adversarial robustness by including noise modeling during training. This approach results in a softer loss surface compared to traditional methods, enhancing the model's ability to generalize to out-of-sample adversarial data. Additionally, PCA analysis shows consistent results across datasets, supporting the effectiveness of ExL in boosting robustness. ExL, in combination with other defense techniques like EnsAdv & PGDAdv, shows promising results in improving adversarial robustness. While ExL alone may not provide strong defense, the addition of noise modeling decreases the impact of gradient masking. However, for WB perturbations exceeding training values, ExL+PGDAdv may still be vulnerable. Overall, ExL offers scalability and integration with other defense methods for enhanced performance against adversarial attacks. ExL+PGDAdv breaks for adaptive BB adversaries BID30 or those querying full prediction confidence. ExL is independent of attack/defense methods and can be combined with stronger attacks for stronger defenses. PCA provides intuition on generalization capability of image models. PC results show superiority of adversarial training methods and can gauge susceptibility in future proposals. Improved noise modeling techniques are indicated by likelihood theory for better gradient handling. Improved noise modeling techniques, as indicated by likelihood theory, can enhance robustness by using better gradient penalties. Future work directions include exploring noise modeling at intermediate layers to improve variance/explainability. For the MNIST dataset, using multiplicative noise enhances relevant pixels while additive noise disrupts images. Training with noise-integrated images shows improved accuracy compared to standard SGD training. In a SGD training scenario without noise, a simple convolutional architecture is trained with 2 Convolutional layers and a Fully-Connected layer. Mini-batch SGD with momentum and learning rate decay is used. Three ConvNet models are trained independently for 30 epochs. For ExL scenarios, noise modeling with negative loss gradients is conducted. The noise learned by the ConvNet is showcased under different gradient update conditions. The noise learned by the ConvNet enhances the region of interest while minimizing background pixels, with RGB components creating color blobs in the noise template. Using only negative gradients during backpropagation for noise modeling results in minimal loss in accuracy compared to standard SGD training. Mini-batch SGD with momentum, weight decay, and learning rate decay is used to train 4 ConvNet models independently for 30 epochs, with noise modeling conducted by backpropagating the corresponding gradient throughout the training process. The noise learned by the ConvNet enhances the region of interest while minimizing background pixels. ExL noise increases explainability along high rank PCs, with a transition from generic to specific features in the network hierarchy. ExL model widens explainability in intermediate layers compared to SGD, except for the final Block4. Linear PC subspace analysis is more applicable in earlier layers. Mini-batch SGD with momentum, weight decay, and learning rate decay is used for training. The study utilized mini-batch SGD with momentum, weight decay, and learning rate decay for training two independent ResNet-18 models for 60 epochs. Noise modeling was done with a decayed noise parameter. Principal Component analysis was conducted on a sample set of 700 test images, showing variance in Conv1 and Block1 layers for clean and adversarial inputs. Results indicated similar variance for clean and adversarial inputs in Conv1 with ExL/SGD, slightly lower variance for adversarial inputs in Block1 with SGD, and consistent variance with ExL in Block1. The study analyzed the variance in Conv1 and Block1 layers for clean and adversarial inputs. PC variance statistics cannot differentiate between a model's knowledge of on-/off-manifold data, only indicating if a model has acquired more knowledge about the data manifold. To assess a model's understanding of adversarial data, the relationship between clean and adversarial projection onto the PC subspace was examined using the cosine distance. Adversaries were created using the Fast Gradient Sign Method (FGSM) with a step size of 8/255 from another ResNet-18 model trained for 40 epochs. Noise templates were learned with noise modeling in different training scenarios. The study analyzed noise modeling in different training scenarios for MNIST and CIFAR10 data using ExL, ExL PGD, and ExL ens techniques. Different noise templates were observed after training, indicating noise influences optimization. Pytorch implementation of ResNet-18 and ResNext-29 architectures were used for CIFAR10 and CIFAR100 datasets. Training details included mini-batch SGD with momentum, weight decay, and batch size. The hyperparameters for attacking target models using black-box (BB) attacks are shown in TAB2 and TAB5. BB attacks are conducted using models trained with different techniques such as SGD and PGD adversarial training. Ensemble Adversarial Training is also utilized in the experiments. Ensemble Adversarial Training (EnsAdv) uses a different learning rate approach compared to BID15, adjusting \u03b7 adv /\u03b7 for adversarial/clean inputs. PGD Adversarial Training (PGD) follows techniques from BID12, suggesting training on a mix of clean and adversarial examples for better performance. The problem described by BID21 improves performance by maintaining accuracy on clean and adversarial examples. Different learning rates are used for training with adversarial/clean inputs, including noise modeling learning rates. Adversarial inputs for EnsAdv training are created using BB adversaries, while PGDAdv training uses WB adversaries. Test accuracy for each model is shown for reference. Learning rates decay by a factor of 0.1. The models in TAB2, A2 use different architectures for source/target models with specific hyperparameters for training. The source model is trained with PGDAdv training for crafting BB attacks, while the target model is trained with mini-batch SGD. Adversarial robustness is justified by integrating noise during training. Integrating noise during training enhances model robustness by exploring multiple directions near data points, aligning noise modeling with loss gradients for improved generalization in adversarial spaces. To achieve guaranteed adversarial robustness, the input/output distribution must be realized, and noise modeling should cover the entire space of adversarial data."
}