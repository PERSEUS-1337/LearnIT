{
    "title": "r1iuQjxCZ",
    "content": "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. The differences between networks that generalize and those that do not remain unclear. Single directions' tuning properties have been highlighted but not evaluated for importance. A network's reliance on single directions predicts its generalization performance across various scenarios. Dropout regularizes this reliance up to a point, while batch normalization discourages it. Recent work has shown that deep neural networks can memorize large datasets like ImageNet, but still achieve low generalization error in tasks like image classification and language translation. The question of why some networks generalize well while others do not has been explored in various studies. Batch normalization reduces the selectivity of individual units, which may contribute to networks minimizing their reliance on specific units for strong performance. In recent studies, the relationship between generalization performance and the flatness of minima, PAC-Bayes bounds, and sharp minima has been explored. Some research has focused on the information content stored in network weights, while others have shown that stochastic gradient descent promotes generalization. Ablation analyses have revealed that networks relying heavily on single directions tend to memorize the training set, with this dependence being more pronounced in networks with lower generalization performance. In this study, it was found that networks trained with batch normalization show better generalization performance and are more robust to cumulative ablations. The class selectivity of individual feature maps is decreased with batch normalization, indicating a potential mechanism for improved generalization. Additionally, the reliance on single directions in overfitting networks could serve as a signal for early stopping. The importance of single units' class selectivity to the network's output was found to be a poor predictor. The study analyzed the relationship between network generalization performance and reliance on single directions in activation space using three different models. ReLU nonlinearities were applied to all layers, except the output, and batch normalization was used for all convolutional networks. Different fractions of randomized labels were used to create datasets for varying degrees of memorization. The study analyzed network generalization performance by examining reliance on single directions in activation space. Ablations were conducted to measure the impact of removing specific directions on network performance. This was done by clamping the activity of the direction to a fixed value. Ablations were performed on single units in MLPs or entire feature maps in convolutional networks. The study evaluated how the network's performance degraded as the influence of increasing subsets of single directions was removed. The study analyzed network generalization performance by examining reliance on single directions in activation space. Ablations were conducted to measure the impact of removing specific directions on network performance. Clamping the activation of a unit to zero was found to be more damaging than clamping it to the mean activation. Gaussian noise was added to test networks' reliance on random single directions. To test networks' reliance on random single directions, Gaussian noise was added with increasing variance normalized by the unit's activations. Class selectivity of units was quantified using a metric inspired by neuroscience. Memorizing networks showed sensitivity to cumulative ablations across different datasets. The selectivity index was calculated based on the class-conditional mean activity across the test set. This metric measures the discriminability of classes in a neural network, with values ranging from 0 to 1. It identifies units with similar class tuning properties and highlights the analysis of deep neural networks. In addition to class selectivity, the study also used mutual information to identify units with information about multiple classes. The results were qualitatively similar, suggesting a network's reliance on single directions may impact generalization performance. The comparison of two networks trained on labeled data highlights the importance of learning underlying structure for better generalization. The study compared two types of networks: one that memorizes data poorly and one that learns data structure well. The memorizing network uses more capacity and single directions, making it more susceptible to perturbations. Testing showed that memorization leads to greater reliance on single directions in neural networks. The study compared two types of networks: one that memorizes data poorly and one that learns data structure well. The networks were ablated to test their sensitivity to perturbations, with results showing that networks trained on corrupted labels were more sensitive to ablations. The study tested networks trained on corrupted labels and found they were more sensitive to noise compared to networks trained on true labels. It is unclear if these results apply to networks trained on uncorrupted data. To investigate, 200 networks were trained on CIFAR-10 with the same topology and dataset but different generalization performance, showing varying reliance on single directions. The study tested networks trained on corrupted labels and found they were more sensitive to noise compared to networks trained on true labels. To investigate further, 200 networks were trained on CIFAR-10 with varying reliance on single directions, showing a relationship between generalization performance and single directions. The study explored the relationship between generalization performance and single direction reliance in networks trained on corrupted labels. It raised the question of using single direction reliance to estimate generalization performance without a test set and for early stopping or hyperparameter selection. An experiment on an MLP trained on MNIST showed that the point where the area under the cumulative ablation curve began to drop coincided with the divergence of train and test loss. Additionally, AUC and test loss were found to be negatively correlated. The study found a negative correlation between AUC and test loss. In a hyperparameter selection experiment, AUC and test accuracy were highly correlated. AUC selected top hyperparameter settings with high accuracy, suggesting it could be used for hyperparameter selection and early stopping. Dropout encourages networks to be robust to cumulative ablations but should not discourage reliance on single directions past the dropout fraction used in training. Networks may guard against dropout by copying information to other directions, making only the minimum number of copies necessary. This approach makes the network robust to dropout as long as redundant directions are not simultaneously removed. Dropout encourages network robustness to cumulative ablations but may hinder reliance on single directions beyond the dropout fraction used in training. Networks trained on randomized labels require more epochs to converge and converge to worse solutions at higher dropout probabilities. Networks trained on corrupted and unmodified labels show minimal loss in training accuracy up to the dropout fraction, but randomized labels are more sensitive to cumulative ablations. Networks with different dropout fractions on unmodified labels exhibit similar robustness to ablations. Batch normalization, unlike dropout, discourages reliance on single directions in convolutional networks trained on CIFAR-10. Networks with batch normalization are more robust to cumulative ablations of single directions compared to those without batch normalization. This indicates that batch normalization decreases class selectivity and increases mutual information in the trained networks. Batch normalization reduces covariate shift and discourages reliance on single directions in trained networks. This leads to better generalization performance, despite past research emphasizing the importance of selective features. The class selectivity of single directions is tested to see its impact on network output. Batch normalization discourages reliance on single directions and influences the distribution of class information across networks. Networks trained without batch normalization show high class selectivity in feature maps, while batch normalization lowers class selectivity but increases mutual information in feature maps. This suggests that batch normalization promotes feature maps with information about multiple classes rather than concentrated class information. The study analyzed the impact of ablating highly selective feature maps in neural networks trained on MNIST, CIFAR-10, and ImageNet. Results showed that there was only a slight correlation between a unit's class selectivity and the network's loss when ablating the unit. In fact, highly selective feature maps had minimal impact when ablated, suggesting that class selectivity may not always be beneficial for network performance. In CIFAR-10 networks, a negative correlation was found between class selectivity and feature map importance, driven mainly by early layers. Ablations in early layers had a more significant impact than in later layers, consistent with theoretical observations. Similar results were obtained when using mutual information instead of class selectivity. Comparisons were also made between class selectivity and the L1-norm of filter weights. The study found that class selectivity was not a good predictor of importance in network performance, and may even be detrimental. Further research is needed to determine the impact of class and feature selectivity. The results were inspired by previous work and replicated using partially corrupted labels on CIFAR-10 and ImageNet datasets. The study also showed that memorizing networks rely more on single directions. Our work addresses the question of whether there is an empirical difference between networks that memorize and those that generalize. Studies have shown that flat minima generalize better than sharp minima, and networks trained on randomized labels store more information in their weights. This is related to the idea that networks rely more on single directions for memorization. Our work is consistent with studies showing that networks with more information stored in their weights rely more on single directions for memorization. While dropout can prevent memorization, it does not discourage reliance on single directions. Class selectivity is not a reliable predictor of unit importance, aligning with recent neuroscience studies. In neuroscience studies, neural systems robust to noise have been explored through perturbation analyses. Model pruning research focuses on generating smaller models with similar performance by removing units. Recent work has investigated methods for discovering important directions. Various deep learning studies have identified selective units for features or classes, analyzing the minimum number of feature maps needed for accuracy. However, the relationship between a unit's class selectivity or information has not been thoroughly tested. Recent studies have explored neural systems' robustness to noise through perturbation analyses and model pruning to generate smaller models with similar performance. Deep learning research has identified selective units for features or classes, but the relationship between a unit's class selectivity and its necessity to the network's output has not been thoroughly tested. BID7 found that units become more concept-selective with depth, and there is a correlation between the number of concept-selective units and network performance. In this study, the focus is on understanding the factors that differentiate neural networks that generalize well from those that do not. The experiments show that generalization capability is linked to a network's reliance on single directions, regardless of data corruption or training stage. Batch normalization is found to discourage reliance on single directions. One potential extension is to develop a regularizer that penalizes reliance on single directions, with dropout being a promising candidate for this purpose. The study focuses on neural networks' reliance on single directions for generalization. Dropout is suggested as a potential regularizer for this purpose, as it does not appear to regulate single direction reliance beyond the dropout fraction used in training. This observation could be utilized to predict a network's generalization performance without needing a validation or test set. Additionally, measuring networks' reliance on single directions could be used for early-stopping or hyperparameter selection in situations with limited labeled training data. The study explores neural networks' reliance on single directions for generalization, suggesting dropout as a potential regularizer. Further research is needed to evaluate its viability in complex datasets and its relationship with generalization performance across different regimes. The observation about the role of individually selective units in DNNs is highlighted as a potentially surprising finding. Batch normalization reduces class selectivity of individual units in DNNs, indicating that highly class selective units may hinder network performance. Methods focusing on analyzing single units may be misleading, and further research is needed to clarify these points. Comparing two methods for ablating directions in convolutional networks trained on CIFAR-10: ablating to zero vs. ablating to the empirical mean over the training set. Ablations to zero were found to be significantly less damaging than ablating to the feature map's mean. This aligns with ablation strategies commonly used in model pruning literature. For class selectivity, generalization, early stopping, and dropout experiments, MLP layers had varying units (128, 512, 2048, 2048) and were trained for 640 epochs. CIFAR-10 ConvNets had layers with sizes (64, 64, 128, 128, 128, 256, 256, 256, 512, 512, 512) and were trained for 100 epochs. ImageNet ResNet 50-layer networks were trained with 32 workers and a batch size of 32 for 200,000 steps. The distribution of class selectivity increased with depth in networks trained on CIFAR-10 and ImageNet. This aligns with previous studies showing that concept-selectivity and linear decodability of class information also increase with depth. Our results show that class selectivity is not a good predictor of importance in neural networks. Comparing class selectivity to the L1-norm of filter weights revealed no correlation in the ImageNet network and a negative correlation in the CIFAR-10 network. Mutual information was tested as a predictor of unit importance in neural networks, but it was found to be generally a poor predictor compared to class selectivity."
}