{
    "title": "rke_YiRct7",
    "content": "We investigate the loss surface of neural networks and prove that even one-hidden-layer networks with slight nonlinearity have spurious local minima in most cases. This challenges the belief that spurious local minima are limited to deep linear networks. For ReLU networks, we show that there are infinitely many local minima on practical datasets. Our results extend to other activations like sigmoid and tanh, where bad local minima exist. We provide a comprehensive characterization of global optimality for deep linear networks, unifying existing results on this topic. Neural network training involves solving nonconvex empirical risk minimization problems, which is generally intractable. Deep learning success stories suggest that local minima could be close to global minima. BID5 uses spherical spin-glass models to explain how the size of neural networks may lead to close local minima. However, understanding optimality in deep neural networks remains challenging due to nonlinearity. Initial progress has been made in understanding optimality in deep linear networks, where some shallow networks have no spurious local minima. The theory of nonlinear neural networks is still in its early stages, with recent results suggesting that the property of \"local minima are global\" does not always hold. Rigorously proving such results has proven to be challenging, leading some authors to make unrealistic assumptions on data. In contrast, the existence of spurious local minima is proven under less restrictive assumptions. The existence of spurious local minima is proven under less restrictive assumptions for optimization of neural networks, with recent results suggesting that the property of \"local minima are global\" does not always hold. Studies have focused on the loss surface and whether this property holds for nonlinear networks, with negative results obtained. In the context of optimizing neural networks, recent research has shown that the assumption of \"local minima are global\" does not always hold. For piecewise linear activation functions, it is proven that spurious local minima exist when linear models cannot perfectly fit the data. Additionally, for more general nonlinear activation functions, a simple architecture and dataset are provided where a local minimum is inferior to the global minimum. Our analysis covers various activation functions, showing a local minimum inferior to the global minimum for a realizable dataset. The surprising counterexample suggests the situation may be worse for non-realizable data. Additionally, a positive result on linear networks is presented, relating the type of stationarity to the behavior of the loss function. The analysis covers different activation functions, revealing a local minimum that is not as good as the global minimum for a feasible dataset. A positive result on linear networks is also discussed, linking the stationarity type to the loss function behavior. Nonlinear neural networks can have spurious local minima, even for simple networks like ReLU and ReLU-like networks. By using nonnegative homogeneity, parameters can be scaled to create infinitely many local minima. The analysis includes a 1-hidden-layer neural network with squared loss and piecewise linear nonnegative homogeneous functions. The activation function h s+,s\u2212 creates spurious local minima with the same risk as linear least squares models due to nonnegative homogeneity. This leads to bad local minima for most real-world datasets when using this activation function in neural networks. Rigorous proof of this phenomenon is provided, contrasting with experimental results. Theorem 1 holds for general datasets, even with slight nonlinearities. Existing results on squared error loss provide limited insight, while our result applies to realistic situations. Several results prove conditions for global optimality of nonlinear neural networks. Theorem 1 addresses the case where linear models cannot fit Y, providing a more realistic scenario compared to existing results on global optimality of nonlinear neural networks. The authors prove that Leaky-ReLU networks do not have spurious local minima, even when d1 \u2265 m. Previous research showed that one-hidden-layer neural networks with d1 \u2265 m do not have strict spurious local minima, but only non-strict ones due to nonnegative homogeneity. Despite claims that wide hidden layers can fit any Y with random parameters, this is not true if linear models cannot fit Y. There exists a non-trivial region in the parameter space where the neural network output is still a linear combination of X rows, limiting its performance to that of linear models. The authors demonstrate that Leaky-ReLU networks do not have spurious local minima, even with d1 \u2265 m. They show that one-hidden-layer neural networks with d1 \u2265 m do not have strict spurious local minima due to nonnegative homogeneity. The performance of neural networks is limited to that of linear models in a non-trivial region of the parameter space. In the current chunk, it is explained how local minima can be as good as the linear solution by tuning parameters to make hidden nodes locally linear. The current chunk explains how tuning parameters to make hidden nodes locally linear can result in local minima that are as good as the linear solution. This is achieved by ensuring all hidden node inputs are positive, leading to locally optimal solutions. Step 4 - Summarized text chunk:\nThe perturbed parameters are considered to ensure larger risk, leading to locally optimal solutions with infinitely many local minima. By manipulating hidden node inputs and utilizing local linearity, a point better than the local minimum can be achieved. The construction involves defining sets of data points in increasing order and presenting a proof for a specific case, with the perturbation argument left for further explanation in the appendix. Parameters are selected to push network outputs in a certain direction based on the sign of the difference between data points and a constant. The empirical risk for a one-hidden-layer nonlinear neural network is minimized by selecting parameters to push network outputs in a specific direction. The proof of this concept relies on the piecewise linearity of activation functions, showing that spurious local minima are not specific to certain nonlinearities. Various popular activation functions can result in local minima inferior to the global minimum with zero empirical risk. The nonlinear neural network with fixed dimensions and derivatives has specific conditions for local minimums. Activations meeting certain criteria introduce spurious local minimums, with the empirical risk being zero at the global minimum. The network can \"generate\" the data, satisfying the realizability assumption. The conditions for local minimums in a nonlinear neural network with fixed dimensions and derivatives are specific. The proof of Theorem 2 can be found in Appendix A3.Discussion. The class of functions that satisfy these conditions is quite large and includes the nonlinear activation functions used in practice. The class of functions satisfying conditions for local minimums in a nonlinear neural network is large, including popular activation functions like sigmoid, tanh, arctan, quadratic, ELU, and SELU. The example in Theorem 2 and Corollary 3 shows that the \"local minimum is global\" property may not hold for practical nonlinear activations, even for realizable datasets. ReLU-like activation functions do not meet all conditions, so Theorem 2 does not directly apply to them. In deep linear neural networks, critical points of the loss with a multilinear parameterization inherit the type of critical points of the loss with a linear parameterization. The \"local minimum is global\" property may not hold for practical nonlinear activations, even for realizable datasets. ReLU-like activation functions do not meet all conditions for Theorem 2 to apply. Parameterization in deep linear neural networks determines the type of critical points of the loss. For differentiable losses with globally optimal critical points, these networks have only global minima or saddle points. An efficiently checkable condition for global minimality is provided for networks with hidden layers and weight matrices. The empirical risk for linear networks is considered, assuming a suitable differentiable loss function. Bias terms are also noted. Theorem 4 states conditions for critical points of the loss in deep linear neural networks. It mentions saddle points and local minima/maxima based on certain criteria. If specific rank conditions are met, then certain properties hold for the critical points. Theorem 4 discusses conditions for critical points in deep linear neural networks, focusing on saddle points and local minima/maxima based on rank criteria. It states that critical points are governed by the behavior of the loss function at specific product values, with different cases handled based on the gradient of the loss function. If the product achieves full rank, certain properties hold for the critical points. Theorem 4 discusses conditions for critical points in deep linear neural networks, focusing on saddle points and local minima/maxima based on rank criteria. If a point is not critical for the loss function, it must be a saddle point. The types of critical points match exactly in terms of minima, maxima, and saddle points. Corollary 5 extends this to show that if all critical points are global minima or maxima, then certain properties hold for the critical points. The text discusses conditions for critical points in deep linear neural networks, focusing on global minima and saddle points. Corollary 5 shows that for certain loss functions, global minima and saddle points are the only critical points, with a distinguishable condition between them. This result generalizes previous works on linear networks and provides a simple necessary and sufficient condition for global optimality. Laurent & Brecht (2018b) proved that local minima are global minima for linear neural networks. They also showed the existence of spurious local minima on nonlinear networks, applicable to most datasets and activation functions. Their results provide a simple test condition for global optimality. Theorem 4 presents a general result on critical points in multilinearly parametrized functions, unifying existing results on linear neural networks. Future research directions include studying the gap between local and global minima, exploring the impact of network size, and investigating the loss surface in restricted parameter spaces. The paper aims to pave the way for further research in this area. The lemma discusses implications of J = \u2205, stating conditions for duplicate and non-duplicate \u0233 j values. It proves that if these conditions are not met, it leads to a contradiction or J = \u2205. The lemma discusses conditions for duplicate and non-duplicate \u0233 j values, showing that violating these conditions leads to a contradiction or J = \u2205. The proof strategy involves choosing a duplicate \u0233 j and perturbing the linear least squares solution to break ties between i's satisfying \u0233 i = \u0233 j and \u0233 i = y i. The lemma discusses conditions for duplicate and non-duplicate \u0233 j values, showing that violating these conditions leads to a contradiction or J = \u2205. To break ties between i's satisfying \u0233 i = \u0233 j and \u0233 i = y i, a perturbation is applied to the linear least squares solution. The lemma discusses conditions for duplicate and non-duplicate \u0233 j values, showing that violating these conditions leads to a contradiction or J = \u2205. A perturbation is applied to the linear least squares solution to break ties between i's satisfying \u0233 i = \u0233 j and \u0233 i = y i. The constants g and M are defined, with g being one fourth times the minimum gap between all distinct values of \u0233 i. If \u0233 i and \u0233 j are distinct and there is an order \u0233 i < \u0233 j, perturbation of [W] by \u2212\u03b1v T does not change the order. The lemma discusses conditions for duplicate and non-duplicate \u0233 j values, proving statements and defining parameters. It also addresses perturbation to break ties and the definition of constants g and M. The lemma concludes by discussing the differences between distinct x values. The lemma discusses conditions for duplicate and non-duplicate \u0233 j values, proving statements and defining parameters. It also addresses perturbation to break ties and the definition of constants g and M. The lemma concludes by discussing the differences between distinct x values, where at least one must be a strict inequality. Parameters (W j ,b j ) with smaller empirical risk than (\u0174 j ,b j ) can be assigned with specific values, leading to squared error analysis. The lemma discusses conditions for duplicate and non-duplicate \u0233 j values, proving statements and defining parameters. It also addresses perturbation to break ties and the definition of constants g and M. The lemma concludes by discussing the differences between distinct x values, where at least one must be a strict inequality. Parameters (W j ,b j ) with smaller empirical risk than (\u0174 j ,b j ) can be assigned with specific values, leading to squared error analysis. For i > j 1, the squared error loss of this point is proportional to \u03b1. The magnitude of \u03b3 is proportional to \u03b1, and with specific values, the output of the network matches the linear least squares model. The lemma discusses conditions for duplicate and non-duplicate \u0233 j values, proving statements and defining parameters. It also addresses perturbation to break ties and the definition of constants g and M. The lemma concludes by discussing the differences between distinct x values, where at least one must be a strict inequality. Parameters (W j ,b j ) with smaller empirical risk than (\u0174 j ,b j ) can be assigned with specific values, leading to squared error analysis. For i > j 1, the squared error loss of this point is proportional to \u03b1. The magnitude of \u03b3 is proportional to \u03b1, and with specific values, the output of the network matches the linear least squares model. To show local minimality, perturbations are applied to the parameters to check if the risk after perturbation is greater than or equal to certain values. Lemma A.3 rearranges terms to help prove local minimality, with conditions on perturbed parameters to ensure non-negativity of certain terms. The lemma discusses conditions for local minimality by proving statements and defining parameters related to perturbations and empirical risk. It concludes by rearranging terms to ensure non-negativity and deriving equations using Taylor series expansion. The lemma presents useful lemmas related to perturbations and empirical risk, proving conditions for local minimality. It includes equations with polynomials in variables a, b, c, and d, showcasing the degree of terms in the polynomials. The lemma provides formulas for polynomials q and r in variables a, b, c, and d, demonstrating the degree of terms. The proof involves expanding equations and simplifying terms using Lemmas A.4 and A.5. Convergence of summations is shown for small perturbations. The lemma provides formulas for polynomials q and r in variables a, b, c, and d, demonstrating the degree of terms. The proof involves expanding equations and simplifying terms using Lemmas A.4 and A.5. Convergence of summations is shown for small perturbations. In conclusion, for small enough perturbations, the summation converges, and the summands converge to zero as n increases. The terms in the summation dominate for small enough perturbations. The squared terms are expressed and split into three parts, showing convergence when perturbations are small enough. The lemma provides formulas for polynomials q and r in variables a, b, c, and d. The proof involves expanding equations and simplifying terms using Lemmas A.4 and A.5. Convergence of summations is shown for small perturbations. The terms in the summation dominate for small enough perturbations. The proof of the corollary involves presenting values of real numbers satisfying specific assumptions for various activation functions. The lemma provides formulas for polynomials q and r in variables a, b, c, and d. The proof involves expanding equations and simplifying terms using Lemmas A.4 and A.5. Convergence of summations is shown for small perturbations. The terms in the summation dominate for small enough perturbations. The proof of the corollary involves presenting values of real numbers satisfying specific assumptions for various activation functions, such as sigmoid and quadratic functions. The lemma provides formulas for polynomials q and r in variables a, b, c, and d. The proof involves expanding equations and simplifying terms using Lemmas A.4 and A.5. Convergence of summations is shown for small perturbations. The terms in the summation dominate for small enough perturbations. The proof of the corollary involves presenting values of real numbers satisfying specific assumptions for various activation functions, such as sigmoid and quadratic functions. In Appendix A3, parameters are defined for ReLU, and perturbations are applied to check if the risk after perturbation is greater than or equal to a certain value. The partial derivatives for Part 1 are computed using matrix calculus to show that (\u0174 j ) H+1 j=1 is neither a local minimum nor a local maximum. By perturbing the tuple (\u0174 j ) H+1 j=1, we aim to make the directional derivative of 0 along P H+1:1 \u2212 W H+1:1 positive. Constructing these perturbations is a key challenge outlined in the approach. In constructing perturbations, the approach involves ensuring the directional derivative is positive along P H+1:1 \u2212 W H+1:1. The construction process is detailed, assuming d x \u2265 d y for simplicity. The proof is split into cases regarding the null spaces of the matrices involved. In constructing perturbations, the approach involves ensuring the directional derivative is positive along P H+1:1 \u2212 W H+1:1. The construction process is detailed, assuming d x \u2265 d y for simplicity. The proof is split into cases regarding the null spaces of the matrices involved. By constructing matrices with specific entries, it is shown that R = W H+1:1 for a given R. To prove the local minimum of \u0174 H+1:1, a lemma is stated involving full row rank and full column rank conditions. The lemma demonstrates that for any R = V H+1:1 satisfying R \u2212\u0174 H+1:1 F \u2264 \u03c3 min (A)\u03c3 min (B), we have 0 (R) = 0 (V H+1:1) = ((V j) H+1 j=1) \u2265 ((\u0174 j) H+1 j=1) = 0 (\u0174 H+1:1). The local maximum part can be proven similarly."
}