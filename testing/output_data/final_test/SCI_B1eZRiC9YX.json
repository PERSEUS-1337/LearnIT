{
    "title": "B1eZRiC9YX",
    "content": "Idealised models, such as idealised Bayesian neural networks (BNNs), can have no adversarial examples under certain conditions. Near-idealised BNNs using HMC inference demonstrate this concept in practice. Experiments on synthetic data show that adversarial images lie off the image manifold, while near-perfect epistemic uncertainty correlates with density under the manifold. MC dropout, as a form of approximate inference, is effective in defending against adversarial attacks. Failure cases of non-idealised BNNs relying on dropout highlight a new attack and defense strategy. This defense is demonstrated on a cats-vs-dogs image classification task. Adversarial examples are a major concern in machine learning, with various hypotheses trying to explain their existence. Examples can lie near or far from decision boundaries, in low or high density regions. Different approaches have been used to detect adversarial examples, such as thresholding low input density. The study focused on detecting adversarial examples in discriminative classification models using tools like Bayesian neural network classifiers and stochastic regularization techniques. The research proved that an idealized model cannot have adversarial examples in low or high density regions of the input space. The pragmatic Bayesian perspective sheds light on adversarial examples and discusses models that meet the conditions. MC dropout-based techniques are effective against attacks, supported mathematically and experimentally. A new attack for MC dropout-based models is developed without requiring gradient information. The text discusses adversarial examples and introduces a mitigation technique that does not require gradient information. It mentions using MNIST and real-world image classification tasks with a VGG13 variant. Adversarial examples were initially crafted using gradient techniques, leading to targeted and non-targeted attacks. The crafting technique of perturbing images to wrong classes led to the discovery of \"garbage\" images as adversarial examples. Research showed that deep neural networks are vulnerable to adversarial examples due to their linearity in high-dimensional spaces. Linear classifiers that avoid this vulnerability have been developed. Adversarial examples occur when the classification boundary is close to the data manifold. BID22 developed crafting techniques using genetic algorithms to generate adversarial examples with low probability under the data distribution. BID17 extended these ideas to non-garbage adversarial examples and showed that a deep naive Bayes classifier can detect targeted adversarial examples by thresholding low input density. BID9 proposed a \"manifold attack\" that generates adversarial examples in high input density regions, contradicting the belief that they only exist in low density areas. The hypothesis that adversarial examples exist in low density regions of the input space is challenged by recent research. Bounds on perturbation magnitude for adversarial images have been studied, with varying assumptions and definitions of robustness. Recent work extends on these ideas and proposes a lower bound on robustness to perturbations. The text discusses different approaches to determining the robustness of neural networks to perturbations. One approach assumes the existence of an oracle to assign correct labels, while another relies on local Lipschitz continuity. The work focuses on discriminative Bayesian models and the continuity of the classifier. The text discusses idealized models for classification, including deep neural networks, RBF networks, and nearest neighbor models. The focus is on formalizing arguments in Bayesian Neural Network (BNN) terminology to provide precise language and empirical support for the developed ideas. The text introduces the concept of Bayesian neural networks, where weights are treated as random variables with a prior distribution. Bayesian inference is conducted by marginalizing parameters to obtain the posterior distribution. This approach aims to integrate uncertainty in predictions over the posterior distribution. Bayesian neural networks integrate uncertainty in predictions by marginalizing parameters over the posterior distribution. Approximate inference techniques like Hamiltonian Monte Carlo and dropout approximation are used for deep Bayesian neural networks. Bayesian neural networks and Gaussian processes are connected, with Gaussian processes being an infinite limit of single hidden layer Bayesian neural networks. Both quantify epistemic uncertainty, which is uncertainty due to lack of knowledge. This is different from aleatoric uncertainty, which is due to genuine stochasticity in the data. The information gain between model parameters and data helps distinguish epistemic from aleatoric uncertainty. Mutual information measures the model's epistemic uncertainty, while predictive entropy indicates high uncertainty. The mutual information distinguishes epistemic from aleatoric uncertainty in Bayesian Neural Networks. By analyzing BNN realizations in function space with a toy dataset, we can assess the discrepancy of functions on a given input to determine proximity to the training data. This helps identify if a tested point is near the training data or not, aiding in classification tasks. The discrepancy in pre-softmax function values for a fixed input results in lower output probability when averaged over postsoftmax values. Idealized discriminative Bayesian neural networks, capturing perfect uncertainty and data invariances, cannot have adversarial examples. An adversarial example is defined as an input far from training data but classified with high probability or an input with a small perturbation that also classifies with high probability. In a binary classification setting with continuous models, a small perturbation can lead to a high output probability on a prediction. The training data is assumed to have no ambiguity, with each input labeled as either class 0 or class 1. The threshold for a high output probability prediction is defined as p > 1 - . In a binary classification setting, high output probability predictions are defined as p > 1 - for class 1 and p < for class 0. The data distribution is invariant under transformations T, ensuring no ambiguity in the training set. The augmented training set includes all possible transformations on X and Y. In a binary classification setting, high output probability predictions are defined as p > 1 for class 1 and p < 0 for class 0. The data distribution is invariant under transformations T, ensuring no ambiguity in the training set. The augmented training set includes all possible transformations on X and Y. To guarantee full coverage, every point in the input space must belong to some trajectory generated by a point from the train set. An 'idealised NN' outputs probability 1 for each training set point with label 1 and probability 0 for points with label 0. A 'Bayesian idealised NN' is a Bayesian model average of idealised NNs. A Bayesian idealised NN outputs predictive probabilities using Bayesian model averaging. Infinitely wide BNNs converge to Gaussian processes, while even finite width BNNs share properties with GPs, including epistemic uncertainty. The text discusses the formal definition of 'epistemic uncertainty' in Bayesian Neural Networks (BNNs) and introduces a supporting lemma for the definition of an 'idealised BNN'. The text introduces the concept of an 'idealised BNN' as a Bayesian idealised NN with a GP-like distribution over the function space, accounting for invariances and increasing uncertainty fast enough. An idealised BNN is defined as a Bayesian idealised NN with high epistemic uncertainty outside the training set. This condition ensures robustness to adversarial examples. The main result states that an idealised Bayesian neural network cannot have adversarial examples. The entropy of the prediction in a Bayesian neural network is such that the output probability is low for both class 0 and class 1. This implies that perturbed inputs within a delta ball do not change the predicted class, making them non-adversarial. The assumption of lack of dataset ambiguity can be relaxed in the proof. The proof easily generalizes to datasets with more than two classes. Real-world conditions can replace the idealized model structure condition, suggesting that existing models capture sensible invariances. The simplicity of the tools used in the proof has profound implications, linking input-space density to epistemic uncertainty. The idealized Bayesian Neural Networks (BNNs) do not have adversarial examples, linking this observation to real-world conditions on BNNs. This insight guides research towards developing tools that better approximate these conditions for robust machine learning. The theoretical support provided for a hypothesis in BID22, along with empirical evidence from BID17, resolves inconsistencies with BID9. BID9 demonstrated the possibility of constructing adversarial examples by perturbing data within high input density regions. The idealized Bayesian Neural Networks (BNNs) do not have adversarial examples in high input density regions, contrary to previous hypotheses. Adversarial examples exist in low density regions when the model captures data invariances. Idealized BNNs increase uncertainty for off-manifold adversarial examples and reject points off the spheres dataset. An idealized Bayesian Neural Network (BNN) is robust to adversarial examples in high input density regions, rejecting points off the spheres dataset. The results reported by BID17 can be explained by the observation that low test error corresponds to high coverage. The proof generalizes to other idealized models satisfying specific conditions, with empirical evidence supporting these arguments using near-perfect epistemic uncertainty obtained from HMC. The curr_chunk discusses experimental evidence supporting the hypothesis that ground truth image density decreases as images become adversarial. It also explores the correlation between uncertainty and input image density, as well as the failure of adversarial crafting techniques with HMC. Additionally, it tests the transferability of these ideas to non-idealized data and models, highlighting failures of dropout uncertainty on MNIST and proposing a new attack and mitigation strategy. The curr_chunk discusses the creation of a new image dataset called Manifold MNIST (MMNIST) derived from MNIST BID15. A variational auto-encoder (VAE) was trained on MNIST with a 2D latent space, focusing on classes 0, 1, and 4. An analytical density was defined in the latent space based on these classes, and 5,000 samples were generated for the training set. The training set for the Manifold MNIST dataset was obtained by decoding samples using a fixed VAE decoder. The density decreases as images become adversarial, with trajectories from targeted attacks showing low probability under the dataset. The deterministic neural network accuracy decreases on adversarial images, indicating successful fooling of the model. Epistemic uncertainty is correlated to density in the image manifold, visualized in the VAE latent space. Adversarial crafting fails for HMC, as shown in experiments sampling from the predictive distribution. In experiments, a non-targeted attack was used to fool deterministic ensembles and HMC BNNs, showing similar success rates. HMC BNN's entropy quickly increases, indicating many possible output values for perturbed images. In experiments, a non-targeted attack successfully fooled deterministic ensembles and HMC BNNs. HMC BNN's entropy quickly increases, indicating many possible output values for perturbed images. Comparing real-world inference methods, dropout shows uncertainty 'holes' compared to HMC on real noisy data (MNIST). A new attack is suggested to generate 'garbage' images with high output probability by querying the model for its confidence without requiring gradient information. Using intuition from experiments, a mitigation strategy using an ensemble of randomly initialised dropout models is proposed to fix uncertainty 'holes' in low MI points. The ensemble correlation with HMC MI shows promising results in comparison to deterministic model ensembles. This sheds light on the effectiveness of dropout in identifying adversarial examples and its relation to Bayesian modelling. Several idealised models satisfying robustness conditions are presented, indicating the potential of dropout as a defense mechanism against attacks. The text discusses the challenges with modern Bayesian Neural Networks (BNNs) in terms of uncertainty and the need to improve practical inference techniques to enhance robustness. It also touches on the conditions for robustness to adversarial examples in idealized models and provides simple examples to illustrate these conditions. The text formalizes conditions for models to be robust against adversarial examples, requiring idealized architecture and the ability to identify inputs far from valid points. These properties ensure high coverage and the ability to detect points not seen before. The proof focuses on defining an idealized model that can correctly classify valid inputs and reject others by ensuring that continuous classification model outputs do not change significantly within small neighborhoods of training set points. The challenge lies in guaranteeing that such models generalize well and have high coverage, especially against adversarial examples. This is achieved by implicitly augmenting the training set with transformations. The proof involves augmenting the training set with transformations extracted from the model to ensure generalization and avoid model degeneracy. The augmented training set is not used for model training but to demonstrate the model's ability to generalize. The transformations that the data distribution is invariant to are built into the model to prevent overfitting. The spheres dataset from BID9 consists of two concentric spheres labeled with different classes. To prevent adversarial examples, a rotation invariance is built into the model, allowing it to generalize well with just two training points. The rotation invariant model trained with only two data points generalizes well, equivalent to an idealized model trained with an infinite number of points on the sphere. The proof relies on a set of rotations and avoids ambiguity in the dataset to prevent issues with low predictive probability near the training data. The proof relies on rotations and avoids ambiguity in the dataset to prevent issues with low predictive probability near the training data. In practice, it is challenging to define all transformations the model should be invariant to, which can lead to limited coverage and unjustifiable rejection of test points. In practice, defining transformations data should be invariant to is difficult, leading to limited coverage. Empirical estimation of model coverage shows CNNs have low uncertainty on test images from the same distribution as train images, indicating good generalization. The connection between a model's generalization error and its invariance to data transformations is discussed further. Existing models capture sensible invariances from real data, enabling good generalization empirically. The argument presented does not claim the existence of an idealised BNN but rather proves that under a specific definition, such a BNN cannot have adversarial examples. The question arises whether real-world BNNs can approximate this definition, which is explored empirically. It is noted that not all BNN architectures can meet this idealised definition, as they need to increase uncertainty rapidly. Empirically, many practical BNN architectures do exhibit increased uncertainty away from the data. The uncertainty property of BNNs increases with the number of units. The choice of kernel and hyper-parameters affects the robustness of GPs to adversarial examples. There is a potential relationship between Lipschitz continuity and model uncertainty. The idealised Bayesian equivalents of practical NN architectures may not exhibit the same properties. In experiments, realistic BNN architectures approximate perfect uncertainty, while dropout inference approximates some properties but fails for others. Low test error implies high coverage, with a model being invariant to a transformation almost everywhere. The data distribution has no ambiguity. The data distribution has no ambiguity, and if a model has zero expected error, it is invariant to transformations almost everywhere. Empirical evidence shows that HMC LeNet BNN and dropout LeNet variants have low test error and high coverage, suggesting good generalization. The idealized models discussed in this section aim to generalize well and have high coverage, including zero coverage for invalid inputs. The class of idealized models that meet these criteria is diverse, and the focus is on determining which models are most suitable for specific tasks and which approximate the ideal properties effectively. The comparison of idealized models on the spheres dataset BID9 helps assess their ability to satisfy these conditions. In this section, idealized models are compared on the spheres dataset BID9 to determine their ability to meet specific conditions. A NN with rotation invariances will not have adversarial examples on the spheres, but may have 'garbage' adversarial examples that classify incorrectly with high confidence. An idealized Bayesian Neural Network (BNN) can predict with high output probability far from the training set, avoiding adversarial examples on the sphere by outputting 'don't know' for points not in the training set. This BNN with rotation invariances eliminates adversarial examples on the sphere. An idealized Bayesian Neural Network (BNN) with rotation invariances will have no adversarial examples on the sphere and full coverage for all sphere points. This is equivalent to having an infinite training set. Additionally, an idealized RBF network that collapses to uniform prediction quickly follows the same intuition as the BNN. Nearest neighbor models with thresholding to declare 'don't know' and a finite dataset will have no adversarial examples but low coverage. Nearest neighbor models with thresholding to declare 'don't know' and a finite dataset will have low coverage. To fix this issue, one can perform nearest neighbor in feature space, building invariances into the distances nearest neighbors use to achieve full coverage with a finite training set. The Bayesian approach provides tools for thresholding in the output probability space, allowing for defining a tolerance to false positives. It is not feasible to simply define a third class for 'don't know' even in rotation invariant nearest neighbor models due to the infinite quotient group of distinct points in the dataset. The augmented train set trick induces finite train sets for defining invariant model loss, but with a 'don't know' class, the train set becomes infinite, making it infeasible to define a loss. Therefore, the study focuses on idealised and near-idealised BNNs, with an analytical expression for density in the latent space for each class in the MMNIST dataset. In practice, importance sampling is used for density calculations over the latent space. The ground truth latent space density correlates strongly with image density from the estimator on test MMNIST images. A comparison between a new latent space attack and the untargeted FGS attack on a dropout NN with MNIST shows a higher success rate for the latent space attack. The robustness of dropout ensemble versus dropout with a MIM attack on MNIST is also demonstrated, with the ensemble showing improved robustness. The improved robustness of dropout ensemble versus dropout with a MIM attack on MNIST is demonstrated. Success rates for changing image labels with the MIM attack on both dropout and dropout ensemble are compared, showing lower rates for the ensemble. Further figures on trajectories from targeted attacks on Manifold MNIST with a deterministic model are also provided. The study demonstrates the improved robustness of an ensemble of dropout models compared to a single dropout model using a VGG13 BID28 variant on the ASIRRA BID3 cats and dogs classification dataset. Adversarial images were generated using the FGM attack, showing that the VGG13 model is vulnerable to powerful attacks. Dropout Resnet-50 based models appear to be more resilient. The study shows that the VGG13 model is vulnerable to powerful attacks, while dropout Resnet-50 models are more robust. Future research will explore the effect of model architecture on uncertainty and robustness."
}