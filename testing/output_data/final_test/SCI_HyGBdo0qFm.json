{
    "title": "HyGBdo0qFm",
    "content": "Alternatives to recurrent neural networks, such as architectures based on attention or convolutions like the Transformer and Neural GPU, have been found to be Turing complete without needing external memory. This study highlights the computational power of these models in accessing and computing internal dense representations of data, showcasing their ability to learn algorithms from examples. Architectures like recurrent neural networks (RNNs) can learn algorithms from examples and are Turing complete even with limited resources. Siegelmann & Sontag's work shows that RNNs can compute internal dense representations of data, making them capable of simulating any Turing machine. The view proposed by Siegelmann & Sontag demonstrates that RNNs can achieve full computational power without increasing model complexity. Recent trends show the benefits of designing networks that manipulate sequences without direct recurrence. This paper explores Turing completeness in models like the Transformer and Neural GPU. The paper demonstrates that the Transformer and Neural GPU are Turing complete without needing external memory. It simulates a Turing machine for the Transformer and uses sequence-to-sequence RNNs for the Neural GPU. Minimal sets of elements are identified for achieving completeness. Comparisons between the computational power of Transformers and Neural GPUs have been informally discussed in current literature. Our paper formalizes the comparison between the computational power of Transformers and Neural GPUs, which has been informally discussed in current literature. Background work traces the study of neural networks' computational power back to analogies with first-order logic sentences and finite automata. While being Turing complete does not guarantee practical algorithm learning ability, there is interest in enhancing RNNs with mechanisms to support this task. The addition of inductive biases in architectures like Neural Turing Machines (NTMs) and Transformer has improved their performance on language-processing tasks. While NTMs use soft attention mechanisms for memory access, Transformers rely heavily on attention mechanisms. Enriching Transformer's architecture with new features has been suggested to improve its ability to learn general procedures. The original Transformer architecture struggles with generalizing to input lengths not seen during training. However, it is shown to be Turing complete, highlighting differences between theory and practice. Different considerations, such as fixed vs. arbitrary precision, can complement each other in understanding the architecture's intricacies. The use of hard attention in theoretical proofs contrasts with the soft attention often used in training Transformers. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors. It can learn decimal multiplication end-to-end and is compared to cellular automata for Turing completeness. The number of cells used in computation is proportional to the input sequence size, with the option to pad with dummy symbols for more cells if needed. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors to achieve Turing completeness. It leverages dense representations of Neural GPU cells without requiring additional cells beyond those used for input storage. The use of rational numbers and rational functions with rational coefficients is assumed, with a focus on sequence-to-sequence neural network architectures. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors to achieve Turing completeness. It leverages dense representations of Neural GPU cells without requiring additional cells beyond those used for input storage. A seq-to-seq network takes a sequence of input vectors and produces an output sequence, with a fixed number of output vectors. This network can be viewed as a language recognizer for strings. The language recognizer A accepts strings from a finite alphabet \u03a3, using an embedding function f: \u03a3 \u2192 Qd and a seq-to-seq network N. A seed vector s \u2208 Qd and a set of final vectors F \u2286 Qd are used to determine acceptance. The language accepted by A, denoted L(A), consists of all accepted strings. Two restrictions are imposed: the embedding function f should be computed in linear time, and the set F should be recognizable in linear time as well. This ensures efficient computation of input embeddings and membership checks in the set F. The language recognizer A accepts strings from a finite alphabet \u03a3 using an embedding function f: \u03a3 \u2192 Qd and a seq-to-seq network N. A seed vector s \u2208 Qd and a set of final vectors F \u2286 Qd determine acceptance. The language accepted by A, denoted L(A), consists of all accepted strings. Restrictions ensure efficient computation of input embeddings and membership checks in set F. The class N of seq-to-seq neural network architectures defines the class L N of languages accepted by recognizers using networks in N. Turing completeness of class N follows naturally from these notions. An encoder-decoder RNN is defined by recursions involving matrices, vectors, output and activation functions. The Transformer architecture, based on the attention mechanism, is formalized in this section. It abstracts specific choices of functions and parameters to establish its mathematical properties in a formal way. The class of encoder-decoder RNNs is proven to be Turing complete, even with specific restrictions on matrices, vectors, and activation functions. The scoring function in the Transformer architecture is defined as a combination of keys and values, with options for different scoring and normalization functions such as additive or multiplicative attention.softmax is a common choice for normalization. The Transformer architecture uses hardmax as a normalization function in its proofs, with hard attention being the term used when hardmax is applied. The single-layer encoder of the Transformer is defined by a parametric function Enc(X; \u03b8) that receives a sequence of vectors and returns a sequence of vectors of the same length. The parameters in the encoder are functions from Q d to Q d. The Transformer encoder is defined as the repeated application of single-layer encoders, with two final transformation functions applied to every vector in the output sequence. The L-layer Transformer encoder is defined by a recursion. A single-layer decoder is similar to a single-layer encoder but with additional attention to external key-value vectors. The Transformer decoder consists of single-layer decoders that utilize key-value vectors (K e , V e) and a sequence Y = (y 1 , . . . , y k ). The output is a sequence Z = (z 1 , . . . , z k ). The decoder is parameterized by functions Q(\u00b7), K(\u00b7), V(\u00b7), and O(\u00b7). The L-layer Transformer decoder is a repeated application of single-layer decoders with a final transformation function F applied to the decoded sequence. The Transformer decoder utilizes key-value vectors and a sequence Y = (y 1 , . . . , y k ). The output sequence Z = (z 1 , . . . , z k ) is generated by a single-layer Transformer decoder. The Transformer network is order-invariant due to the attention function's property, which treats permutations of input sequences the same. The Transformer network is order-invariant, achieved through positional encodings. Regular languages recognized by finite automata may not all be recognized by a Transformer network due to order-invariance. The Transformer satisfies a stronger invariance property called proportion invariance, showing that not all regular languages recognized by Transformers are order-invariant. The Transformer network satisfies proportion invariance, recognizing languages that are not necessarily regular. It can recognize non-regular languages like those with more 'a' than 'b' symbols, showing its computational power. The computational power of Transformer networks without positional encoding is weak, unable to recognize order-invariant regular languages or express counting properties beyond regularity. The inclusion of positional encodings changes this significantly, providing a new input format for the Transformer encoder and decoder. Positional encoding must be computable by a Turing machine in linear time. The main result of this section is the completeness of Transformers with positional encodings. It shows that for every Turing machine, there exists a transformer that simulates its execution. The transformer takes input X and produces a sequence y containing the next state and symbol under M's head. The completeness of Transformers with positional encodings is demonstrated by showing that a transformer can simulate the execution of a Turing machine. The transformer takes input X and produces a sequence y containing information about the machine's state and symbol under its head, encoded as one-hot vectors. The construction involves implementing the machine's transition function, storing relevant information using residual connections, and determining the symbol at a specific index based on previous symbols written by the machine. The transformer can simulate a Turing machine by taking input X and producing a sequence y with machine state and symbol information. The decoder uses self-attention to copy symbols written by the machine and construct the output. The construction involves attending to the encoder, copying input symbols, and handling unseen cells with a blank symbol. This process is implemented using feed-forward networks and attention mechanisms. The transformer architecture closely follows Vaswani et al. (2017) but with some key differences. They use hard attention instead of softmax, and do not use sin-cos positional encodings. Future work could explore using arbitrary functions with restrictions like finite precision. The Transformer architecture, inspired by Vaswani et al. (2017), utilizes hard attention and does not employ sin-cos positional encodings. Weiss et al. (2018) emphasize the importance of arbitrary precision for internal representations in neural networks, although practical implementations often use fixed precision hardware. When fixed precision is used, positional encodings can be viewed as functions that increase the input alphabet size. The Neural GPU BID11 architecture combines convolutions and gated recurrences over tridimensional tensors, parameterized by three functions U, R, and F. These functions produce a sequence of tensors given an input tensor S and a value r. Neural GPUs resemble gated recurrent units, with U as the update gate and R as the reset gate. The Neural GPU architecture combines convolutions and gated recurrences over tensors, parameterized by functions U, R, and F. The convolution operation involves a 4-dimensional kernel bank with scalar multiplication replaced by vector-matrix multiplication. This approach is similar to cellular automata, where each cell is updated based on its neighbors according to a fixed rule. Zero-padding is assumed when convolving outside the grid. The Neural GPU architecture combines convolutions and gated recurrences over tensors, parameterized by functions U, R, and F. The computational power of Neural GPUs is studied by casting them as a standard seq-to-seq architecture. Input sequences are transformed into tensors, with the output being a sequence of vectors. The Neural GPUs can be viewed as language recognizers. The number of parameters in a Neural GPU grows with the size of the bias tensor. The Neural GPU architecture combines convolutions and gated recurrences over tensors, parameterized by functions U, R, and F. The computational power of Neural GPUs is studied by casting them as a standard seq-to-seq architecture. The number of parameters in a Neural GPU grows with the size of the input. To address this, the notion of uniform Neural GPU is introduced, where each bias has a corresponding matrix, making it finitely specified. The class of uniform Neural GPUs is proven to be Turing complete by simulating a seq-to-seq RNN. The Neural GPU architecture combines convolutions and gated recurrences over tensors using E for the encoder and D for the decoder. Kernel banks are used with uniform bias tensors for computation. The gating mechanism ensures sequential updates of cells, allowing for arbitrary long computations. The Neural GPU architecture combines convolutions and gated recurrences over tensors using E for the encoder and D for the decoder. At iteration t, E t = h t and at iteration n + t, D n = g t. The construction requires 3d + 3 components for implementing update and reset gates. Kernels of shape (2, 1, d, d) are used for Turing completeness. Zero padding is necessary for convolution in the proof of Theorem 4.1. The Neural GPU architecture combines convolutions and gated recurrences over tensors using E for the encoder and D for the decoder. Zero padding is necessary for convolution in the proof of Theorem 4.1. Circular convolutions in Neural GPUs lead to loss of Turing-completeness and inability to differentiate periodic sequences of different lengths. Uniform Neural GPUs with circular convolutions are not Turing complete. The BID16 model struggles with highly symmetric inputs, such as 11111111 \u00d7 11111111, failing for inputs with eight or more 1s. BID5 simplified Neural GPUs by using piecewise linear activations and bidimensional input tensors, achieving better results in training time and generalization. The Turing completeness proof of Neural GPUs relies on bidimensional tensors and piecewise linear activations, retaining full expressiveness while simplifying practical applicability. The analysis compares the Turing completeness of the Transformer and Neural GPU architectures for sequence-processing tasks. The proof for Transformer requires residual connections, while Neural GPUs heavily rely on the gating mechanism. The study aims to determine if these features are essential for completeness. General abstract versions of both architectures were presented for theoretical results, with some variations in functions and parameters from practical choices. The study explores the impact of piecewise linear activation functions on the Transformer and Neural GPU architectures. While Neural GPUs benefit from these activations, further experimentation is needed for Transformers. The research aims to investigate the undecidability of language modeling problems in both architectures, building on previous work that established the Turing completeness of RNNs. This analysis is part of future research endeavors. Undecidability results for Transformers and Neural GPUs imply undecidability for language modeling problems based on these architectures. The study plans to explore the impact of finite precision on these architectures, following previous work on RNNs. Siegelmann & Sontag's proof shows how a single RNN can simulate a Turing machine, providing insights into the internal representations of these models. The internal representation of strings in a network encodes every string as a rational number between 0 and 1 using base 4. This allows for simulating stack operations easily. Another network is constructed to simulate a two-stacks machine by using one neuron value to represent each stack. The input for the simulated machine is given as an initial state to this network. The network encodes strings as rational numbers between 0 and 1 using base 4 for stack operations. A network simulates a two-stacks machine with one neuron representing each stack, taking an initial state as input. Combining two networks, N 1 and N 2, constructs a network N resembling an encoder-decoder RNN. N expects an input of the form 0 r for r steps. This construction allows for an RNN encoder-decoder N and a language recognizer A to simulate the two-stacks machine M. Key details include N being defined by specific formulas and R in Equation FORMULA1 being the null matrix. A defines its own embedding function for every vector used. The text discusses how a network can encode strings as rational numbers using base 4 for stack operations. It explains the construction of a network simulating a two-stacks machine and combining two networks to create an encoder-decoder RNN. The network is defined by specific formulas, with A defining its own embedding function for vectors. Additionally, it mentions modifying the construction to ensure a neuron is activated only at specific times. PropInv is extended with a new definition, ensuring preservation of properties under embedding functions. The proof of Proposition 3.1 involves showing a specific property holds true. The notation pXv is introduced to represent the occurrence of a vector v in X. This sets the stage for further simplification in the proof process. The proof of Proposition 3.1 involves demonstrating a property related to the preservation of values under certain conditions. It is shown that for every pair of indices where two vectors are equal, a specific equation holds true. This equation is then further simplified using normalization functions. Finally, the proof is completed by showing the equality of values and the existence of a constant factor. The proof involves demonstrating the preservation of values under certain conditions. It is shown that if x i = x j, then k i = k j and v i = v j. This leads to the existence of a mapping for every pair of indices. The proof is completed by showing the equality of values and the existence of a constant factor in the transformation process. The proof involves using an inductive argument to show the equality of values in a transformation process. A contradiction is then derived by considering two strings in a language recognizer, leading to the completion of the proof of the corollary. The curr_chunk describes a simple Transformer network with dimension d = 2, using one layer each for encoder and decoder. The encoder implements the identity function, preserving original values through residual connections. The decoder follows a similar approach with self attention and external attention mechanisms. The curr_chunk discusses the proof of a Transformer network's computation process, introducing notation and showing how the network computes a sequence based on input values. The proof of the Transformer network's computation process involves external attention and residual connections, showing how the network computes a sequence based on input values. The key details include the use of key vectors, score values, and the final output calculation. The proof involves a Turing machine with an infinite tape using a special symbol # to mark blank positions. The machine moves its head left or right, starts at state q init, and transitions to state q read to read the input. It moves right until # is read. The construction of a transformer network Trans M is able to simulate a general Turing machine on every possible input string. The process involves dividing the construction and proof into three parts, starting with a high-level view of the strategy used. The encoder part of Trans M receives the input string and uses an embedding function to represent each symbol. The encoder in Trans M uses an embedding function to represent input symbols as one-hot vectors with positional encoding. The decoder simulates the execution of a Turing machine on the input string by computing values for each time step using self-attention and attention over the encoder. This allows for the reconstruction of the computation history and effective simulation of the Turing machine. The construction of the output vectors in Trans M involves encoding information about q and s as one-hot vectors. The process uses a two-layer feed-forward network to mimic the transition function \u03b4, allowing for the computation of values for each time step. The construction of output vectors in Trans M involves encoding q and s as one-hot vectors using a two-layer feed-forward network. This allows for the computation of values for each time step, including q (i+1), v (i), and m (i) in the decoder. The symbol s (r+1) can be computed with two additional decoder layers based on the position of the head of machine M at time step r + 1. The construction of output vectors in Trans M involves encoding q and s as one-hot vectors using a two-layer feed-forward network. This allows for the computation of values for each time step, including q (i+1), v (i), and m (i) in the decoder. The symbol s (r+1) can be computed with two additional decoder layers based on the position of the head of machine M at time step r + 1. In cases where i > n, s (i) is the last symbol written by M at index k, and the computation of the index that M is pointing to at time r + 1 is crucial. The computation of c (i) and c (i+1) can be done using one layer of self attention. The helping value (i) is defined as the last time in which M was pointing to position c (i). If c (i) is visited for the first time at time step i, (i) is ill-defined and set to i - 1. The computation of c (i) and c (i+1) is done using one layer of self attention. If c (i) is visited for the first time at time step i, (i) is set to i - 1. To compute s (r+1), we use an additional self-attention decoder layer. This decoder computes q (r+1) and s (r+1) to produce y r+1. The architecture of the encoder and decoder for implementing the strategy is detailed. A non-linear function \u03d5(x) = \u2212|x| is used, implemented as \u03d5(x) = \u2212 relu(x) \u2212 relu(\u2212x). The scoring function score \u03d5 is defined using \u03d5(\u00b7). The computation of hard attention with score \u03d5 (\u00b7) involves selecting the element v j from tuple V = (v 1 , . . . , v n ) that maximizes score \u03d5 (q, k j ). The architecture of the encoder and decoder for implementing the strategy is detailed, using a non-linear function \u03d5(x) = \u2212|x|. When computing hard attention with the function score \u03d5 (\u00b7), the goal is to select the vector v j such that the dot product q, k j is as close to 0 as possible. If multiple indexes minimize the dot product, attention behaves like an average of all value vectors. The vectors used in the Trans M layers are of dimension d = 2|Q|+4|\u03a3|+11, arranged in four groups of values. The encoder and decoder architecture utilizes a non-linear function for hard attention selection. Vectors in the Trans M layers are grouped and represented as one-hot vectors. Positional encoding is introduced using an embedding function and positional encoding function. The encoder and decoder architecture in Trans M utilizes positional encoding with values 1/i and 1/i^2. The encoder uses a single-layer encoder with self-attention and linear transformations for V(\u00b7) and K(\u00b7). The decoder construction is not detailed in this section. The decoder in Trans M utilizes linear transformations for V(\u00b7) and K(\u00b7) in Equation (8). A lemma is used to construct the decoder, producing a sequence of outputs y1, y2, ... containing information about the state of M at each time step. The starting vector for the decoder is described, with the need to include m(i-1) explained. The construction resembles a proof by induction, detailing the architecture piece by piece. The architecture of the decoder in Trans M is described piece by piece, constructing y r+1 from previous vectors. Using positional encodings, the input for the first layer is y 0 + pos(1), y 1 + pos(2), ..., y r + pos(r + 1). The first self-attention produces the identity, followed by attending over the encoder to obtain a vector a. The first decoder layer also includes a function O 1 (\u00b7) satisfying certain properties. The first decoder layer in Trans M uses a function O 1 (\u00b7) and produces the sequence (z). It contains info about q (r+1) and m (r) needed for constructing vector y r+1. The symbol s (r+1) can be computed with two additional decoder layers, taking advantage of the cell property c (i). The first decoder layer in Trans M uses function O 1 (\u00b7) to produce sequence (z) containing info about q (r+1) and m (r) for constructing vector y r+1. The second decoder layer utilizes the cell property c (i) to compute symbol s (r+1). The third decoder layer employs feed-forward networks to generate desired s (r+1) value. The proof of Lemma B.4 shows that by attending to position (i+1) and copying values, one can prove Lemma B.1 and Lemma B.2 using defined functions and properties. Lemma B.2 is proven by constructing one-hot vectors for pairs in Q \u00d7 \u03a3 using enumerations \u03c0 1 and \u03c0 2. A new enumeration \u03c0 is defined for Q \u00d7 \u03a3 \u00d7 {\u22121, 1} to simplify notation. Three helping properties are then proven for q \u2208 Q, s \u2208 \u03a3, and m \u2208 {\u22121, 1}. The text discusses proving three helping properties for q \u2208 Q, s \u2208 \u03a3, and m \u2208 {\u22121, 1} using vectors and enumerations. It involves constructing one-hot vectors for pairs in Q \u00d7 \u03a3 and defining a new enumeration for simplification. The text discusses constructing vectors and matrices to define three functions for q \u2208 Q, s \u2208 \u03a3, and m \u2208 {\u22121, 1}. It involves applying affine transformations, piecewise-linear sigmoidal activation, and constructing matrices based on \u03b4(q, s). The text discusses constructing functions for q \u2208 Q, s \u2208 \u03a3, and m \u2208 {\u22121, 1} using affine transformations and piecewise-linear sigmoidal activation. It involves defining vectors and matrices based on \u03b4(q, s) to compute attention scores and averages. The text discusses constructing linear transformation functions Q3(\u00b7), K3(\u00b7), and V3(\u00b7) for vectors in z \u2208 i. These functions are defined using feedforward networks to compute attention scores and obtain the desired vector. The text discusses constructing linear transformation functions Q3(\u00b7), K3(\u00b7), and V3(\u00b7) for vectors in z \u2208 i. These functions are defined using feedforward networks to compute attention scores and obtain the desired vector. The proof involves showing properties related to specific indices j1 and j2 in relation to the values of c(i+1). The argument concludes with the determination that arg min j |\u03c7 i j | = (i + 1). The text discusses constructing linear transformation functions for vectors in z \u2208 i using feedforward networks to compute attention scores. The argument concludes with proving that arg min j |\u03c7 i j | = (i + 1) based on specific indices j1 and j2 in relation to the values of c(i+1). The text discusses constructing a Neural GPU network NGPU to simulate an RNN encoder-decoder N. Input tensor S is described as a 2D grid for simulating an RNN efficiently. The text describes constructing kernel banks for simulating an RNN using a bi-dimensional tensor. Matrices in the kernel banks are defined as block matrices, with blank spaces considered as 0. The proof shows that the construction effectively simulates N by using the first d components for the encoder part and the next d components for data communication. The text describes simulating an RNN using kernel banks constructed with a bi-dimensional tensor. Matrices in the kernel banks are defined as block matrices, with blank spaces considered as 0. The construction effectively simulates N by using components for the encoder, data communication, and decoder parts. The proof involves induction in t, showing the computation of U t and updating hidden states properly. The text discusses the periodicity property of a uniform Neural GPU processing tensor sequences. It proves that if the input is periodic, the output will also be periodic. The proof involves induction and the existence of specific matrices in the Neural GPU. The text discusses the inability of uniform Neural GPUs to recognize the length of periodic inputs. It introduces a language recognizer A defined by a uniform neural GPU N containing all strings of even length. An example with strings of odd length is used to illustrate this limitation. The text discusses the limitation of uniform Neural GPUs in recognizing the length of periodic inputs. It introduces a language recognizer A defined by a uniform neural GPU N containing all strings of even length. The outputs of N for both inputs X and X are the same, leading to a contradiction if A accepts w."
}