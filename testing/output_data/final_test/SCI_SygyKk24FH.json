{
    "title": "SygyKk24FH",
    "content": "In Bayesian statistics, inconsistent estimates can arise in misspecified settings. To address this issue, a pseudo-likelihood based on Maximum Mean Discrepancy has been proposed, providing a consistent and robust MMD-Bayes posterior. Variational approximations of this posterior are also shown to be consistent and robust. Numerical simulations suggest that this estimator is more robust to model misspecification compared to likelihood-based estimators. Bayesian methods are popular in statistics and machine learning for modeling uncertainty. The classical Bayesian methodology is not robust to model misspecification, leading to inconsistent posterior estimates. To address this, replacing the log-likelihood with a relevant risk measure is proposed in PAC-Bayes theory and Gibbs posteriors. Knoblauch et al. (2019) extends this idea to approximate Bayesian inference. In the context of Bayesian methodology, robust divergence-based Bayes inference is explored, with a focus on using Maximum Mean Discrepancy (MMD) as an alternative loss in Bayes' formula. This approach, termed MMD-Bayes, utilizes an embedding of distributions into a reproducing kernel Hilbert space (RKHS). The MMD-Bayes method utilizes an embedding of distributions into a RKHS, allowing for the application of kernel methods in parametric estimation. It is consistent and robust to model misspecification, data contamination, and outliers, even with a tractable approximation via variational inference. In a measurable space X, a statistical model {P \u03b8 /\u03b8 \u2208 \u0398} is considered with a parameter space \u0398. A kernel mean embedding in a Hilbert space is defined using a strictly positive definite kernel. The MMD-Bayes method utilizes this embedding for parametric estimation, with a tractable approximation via variational inference. The kernel mean embedding maps probability distributions into a Hilbert space. The MMD between two distributions is the distance in this space between their embeddings. MMD-Bayes is consistent and robust when the true distribution is within the model. The MMD-Bayes is consistent and robust to misspecification when the true distribution is within the model. A robust version of the prior mass condition is proposed based on a neighborhood of an approximation of the true parameter. The prior mass condition in MMD-Bayes is robust to misspecification by considering a neighborhood of an approximation of the true parameter, ensuring it is satisfied by choosing a large enough temperature parameter \u03b2. The MMD-Bayes posterior distribution is robust to misspecification under the prior mass condition. The concentration of the MMD-Bayes posterior in the well-specified case ensures convergence to the true distribution at a minimax rate. In this section, an efficient implementation of the MMD-Bayes based on VI is provided, ensuring n \u22121/2 -consistency as the MMD-Bayes. The variational approximation of the posterior distribution is guaranteed to be consistent under certain conditions. In this section, an efficient implementation of the MMD-Bayes based on VI is provided, ensuring n \u22121/2 -consistency as the MMD-Bayes. The variational set F must contain probability distributions concentrated around the best approximation P \u03b8 * for the robust extended prior mass condition. The variational approximation is robust in practice when estimating a Gaussian mean and a uniform distribution in the presence of outliers. A stochastic gradient descent algorithm is designed based on a U-statistic approximation of the MMD criterion. Short simulations provide empirical support to the theoretical results. The experiment involved Gaussian mean estimation with outliers. Observations were sampled from Gaussian distribution with some corrupted by a Cauchy distribution. Results showed method performed comparably to componentwise median and outperformed maximum likelihood estimator as outliers increased. Simulations were also conducted for multidimensional Gaussians and robust estimation of a uniform distribution's location parameter. Refer to Appendix H for more details. The MMD-Bayes posterior concentrates at the minimax rate and is robust to model misspecification. Reasonable variational approximations of this posterior also exhibit these properties. A stochastic gradient algorithm is proposed for computing such approximations, supported by numerical simulations. Future research could explore relaxing the i.i.d assumption and investigating the robustness of the MMD-based estimator to data dependency. The MMD-Bayes posterior concentrates at the minimax rate and is robust to model misspecification. Variational approximations of this posterior also exhibit these properties. A stochastic gradient algorithm is proposed for computing such approximations, supported by numerical simulations. Future research could explore relaxing the i.i.d assumption and investigating the robustness of the MMD-based estimator to data dependency. Lemma 1 in Briol et al. (2019) holds with high probability, with the rate n \u22121/2 known to be minimax. Lemma 6, a triangle-like inequality, is used extensively in the proofs. MMDBayes can be defined using an argmin over the set of all probability distributions absolutely continuous with respect to \u03c0 and the Kullback-Leibler divergence. The MMD-Bayes posterior concentration is guaranteed by Jensen's inequality, leading to the convergence in probability of MMD-Bayes to the true distribution at a rate of n^-1/2. This is proven using Markov's inequality, ensuring the concentration of MMD-Bayes to P0. The proof of Theorem 1 involves the variational approximation of MMD-Bayes using an argmin over set F:\u03c0, applying Donsker and Varadhan's lemma. The extended prior mass condition leads to Formula (4.3) following the lines of Theorem 2's proof. In Appendix D, an example illustrates the robustness of MMD distance compared to KL divergence in the Bayesian posterior. The Huber's contamination model showcases when MMD-Bayes is more suitable than traditional Bayesian methods. In this appendix, the focus is on recovering the true parameter \u03b8 0 using the MMD distance to the true distribution P 0, rather than the KL divergence. The model consists of Gaussian distributions and a contamination rate \u2208 (0, 1/2). The goal is to show the superiority of MMD distance over KL divergence in this context. The text discusses the computation of a robust prior mass in Gaussian mean estimation using the MMD distance to the true distribution P 0. It compares the minimizer of D k (P 0 , P \u03b8 ) w.r.t \u03b8 with the minimizer of KL (P 0 , P \u03b8 ) w.r.t \u03b8, showing the superiority of MMD distance over KL divergence in situations where the corrupted mean \u03b8 c is far from the true parameter \u03b8 0. In this appendix, the computation of a prior mass in Gaussian mean estimation is addressed, leading to various values of \u03b2 satisfying the prior mass condition C(\u03c0, \u03b2) for a standard normal prior \u03c0. The prior mass condition is met when \u03b2 \u2265 \u2212 log \u03c0(B n )n, with lower bounds involving the parameter \u03b8 * for computing the prior mass. Choosing a temperature parameter \u03b2 \u2265 f (\u03b8 * )\u2212log L n ensures the prior mass condition, even in a misspecified case. The model considers a mixture of observations sampled from a Gaussian distribution and corrupted observations. The Gaussian distribution model is more general than Huber's contamination model. When the model is well-specified, the true distribution is also the distribution of interest. The prior mass of a ball in Gaussian mean estimation is computed using a Gaussian kernel and a standard normal prior. The inequality defining parameters belonging to a ball is derived, leading to the prior mass condition being met with a suitable temperature parameter. The prior mass of a ball in Gaussian mean estimation is computed using a standard normal prior. The larger the value of \u03b8*, the smaller the prior mass can be. Values of \u03b2 leading to consistency of the MMDBayes are determined, with a lower bound on valid values of \u03b2. Stochastic gradients can be computed directly for non-differentiable log-densities. The model uses a uniform distribution and Gaussian kernels. Results show that the MMD estimator performs well in various problems, outperforming other methods in the presence of outliers. The MMD estimator performs well in various problems, outperforming other methods in the presence of outliers. The method of moments and MLE give inconsistent estimates with outliers."
}