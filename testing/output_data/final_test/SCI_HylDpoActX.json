{
    "title": "HylDpoActX",
    "content": "Recent research has focused on optimizing Convolutional Neural Networks (CNNs) for deployment on resource-constrained systems. One technique, quantization, allows weights and activations to be represented with fewer bits while maintaining prediction performance. However, aggressive quantization methods still struggle to match full-precision performance on large-scale tasks. A proposed method for weight and activation quantization uses trainable scaling factors and a clustering strategy to maintain performance close to full-precision CNNs. The nested-means clustering strategy allows for efficient weight updates and convergence in CNNs. A linear quantization approach considers batch normalization statistics for activation compression. The method demonstrates effectiveness on ImageNet models, addressing the need for model compression and inference acceleration in resource-constrained devices. Inference acceleration can be achieved by lowering the precision of computations in neural networks through quantization techniques, transforming weights and activations into lower resolutions. Aggressive quantization approaches aim for binary or ternary representations, but there is still a gap between low bitwidth representations and state-of-the-art architectures for real-world tasks. In this work, a scalable non-uniform quantization method for weights is introduced to address accuracy degradation in neural networks. The method utilizes trainable scaling factors and nested-means clustering to split the weight distribution into quantization intervals, allowing for more quantization levels if needed. Nested-means clustering assigns weights within quantization intervals the same weight, improving classification performance by prioritizing larger weights. The approach is evaluated on CNN architectures for computational efficiency and prediction accuracy in the ImageNet classification task. The paper discusses related work, background on inference acceleration, weight and activation quantization, and presents experimental results for ImageNet. CNN quantization is a dynamic research area with various approaches and objectives. In the context of CNN quantization, various strategies are reviewed, including non-scalable weight-only quantization methods like Binary Connect (BC), Binary Weight Networks (BWNs), and Ternary Weight Networks (TWNs). Scalable weight-only quantization methods are also discussed, such as Trained Ternary Quantization (TTQ). Non-scalable weight-only quantization methods like Binary Connect (BC), Binary Weight Networks (BWNs), and Ternary Weight Networks (TWNs) are discussed in the context of CNN quantization. Scalable weight-only quantization methods include Trained Ternary Quantization (TTQ). Deep Compression leverages pruning, weight sharing, and Huffman Coding for model compression. Incremental Network Quantization (INQ) quantizes pre-trained full-precision DNNs to zeros and powers of two by iteratively partitioning weights. Binarized Neural Networks, XNOR-Net, Bi-Real Net, and ABCNet quantize both weights and activations to -1 or 1. Low-precision activations are proposed using Half-Wave Gaussian Quantization (HWGQ) with Symmetric Quantization (SYQ) introduced for symmetric binary and ternary weights. In this work, a novel Symmetric Quantization (SYQ) method is introduced for scalable quantization of weights and activations, aiming to maintain accuracy while reducing computational complexity. The approach differs from previous methods by being easy to compute, maintaining high prediction accuracy, and minimizing impact on training time. Our approach enables non-uniform n-ary weight representations with high sparsity, reducing training time. Quantization function maps filter weights and inputs to quantization levels. Input data requires more quantization levels than filter weights for single-precision accuracy. Few-bit integer scalar products can be implemented using the bitserial/bit-plane approach. The Efficient Inference Engine is a specialized hardware CNN inference accelerator that leverages non-uniform weights, uniform activations, and sparsity. Reduce-and-scale architectures efficiently utilize sparsity in weights by summing equally weighted inputs and multiplying the result with the respective weight. The approach efficiently leverages sparsity in weights, requiring only one multiplication per quantization level and per output feature. Weight quantization can be non-uniform, tailored for weights and activations. Full-precision weights are maintained for training and quantized during forward propagation. The gradient of full-precision weights is approximated using the straight-through gradient estimator for inference acceleration. Only quantized weights are used for model compression and inference. The quantization strategy used at forward propagation involves scaling factors for model compression and inference acceleration. For binary weights, a uniform scaling factor is proposed per layer and output feature map. Ternary weights utilize trainable non-uniform scaling factors per layer determined by gradient descent to increase model capacity. Trainable scaling factors are adopted in the method to adjust scaling factors and minimize loss function. Trainable scaling factors are used in the method to adjust weights during training. Interval thresholds are defined to partition real numbers, with a trainable scaling factor assigned to quantize weights. The scaling factors are updated using gradients, with a fixed scaling factor for zero weights. Finding optimal interval thresholds is crucial for prediction performance. Non-uniform weights allow for different weight representations, with good quantized weights exhibiting symmetry around zero. In this work, various n-ary weight representations are explored to achieve high compression levels while improving model capacity. Quaternary weights, for example, can be encoded with only two bits, introducing either an additional positive (quaternary+) aspect. Quinary weights extend ternary weights by one positive and one negative value, encoded with only two bits in a sparse format. Weight clustering is necessary for optimal approximation, achieved through static or dynamic implementation using thresholds to define clusters. Weight clustering is essential for optimal approximation in quinary weights, with clustering thresholds calculated during training based on the maximum absolute value of the full-precision. However, re-training is required to compensate for the loss of information due to quantization, leading to non-optimal solutions found by iterative algorithms. Lowering the learning rate for re-training can reduce changes in weight distribution but may result in longer convergence times and the risk of getting stuck at plateau regions. Applying iterative clustering approaches during training is impractical due to the significant increase in training time. Weight clustering is crucial for optimal approximation in quinary weights, with cluster thresholds determined during training based on the maximum absolute value of full-precision weights per layer. However, the presence of an additional hyperparameter for each scaling factor makes hyperparameter tuning impractical. To address this issue, a symmetric nested-means clustering algorithm is proposed to assign full-precision weights to quantization clusters, overcoming issues related to aggressive threshold changes caused by weight updates. The symmetric nested-means clustering algorithm divides weights into positive and negative clusters, further dividing them into inner and outer clusters. This process continues until the desired number of quantization intervals is achieved, with cluster intervals becoming smaller for larger weights. This approach is beneficial as larger weights are shown to be more important, despite most weights being close to zero. The nested-means clustering algorithm divides weights into clusters based on their values, with larger weights playing a more important role. This approach is beneficial for convergence and computational efficiency, especially when reducing input bit width for better parallelism. Linear quantization is used for activations, enabling low bit-width additions on commodity devices. The ReLU activation function is commonly used for its computational efficiency and ability to address the vanishing gradient problem. To handle unbounded outputs, a common solution is to clip the outputs in the interval (0, \u03b3]. Selecting the clipping parameter \u03b3 involves a trade-off between gradient mismatches and quantization errors. Defining an appropriate clipping interval is crucial for efficient quantization. Batch normalization has become a standard tool to accelerate convergence in CNNs by transforming pre-activations to have zero mean and unit variance. To define a clipping interval, an empirical rule approximates the ReLU function by setting \u03b3 = \u00b5 + 3\u03c3, filtering out outliers. However, this approach requires \u00b5 and \u03c3 to be repeatedly calculated during training. The proposed quantization approach uses a fixed clipping parameter \u03b3 = 3 to maintain small quantization intervals and limit the number of clipped activations. It was applied on ResNet and Inception networks trained on ImageNet using TensorFlow and Tensorpack library. Training details include learning rate schedules, weight decay, momentum, eight GPUs for training, and a batch size of 64 per GPU. The quantized networks use eight GPUs for training with a batch size of 64 per GPU. Convolutional and fully-connected layers are quantized, except for input and output layers, to maintain accuracy. Evaluation is done on ternary, quaternary, and quinary representations for compression potential. Training time increases with quantization levels, more so for ResNet model than Inception-BN. The ResNet model shows up to a 2.0x increase in training time due to better GPU utilization. Activation quantization using ResNet-18 on ImageNet is evaluated, with negligible influence on training time. Ternary weights and 32-bit and 4-bit activations have similar learning curves, highlighting the robustness of the quantization approach. The proposed quantization approach shows stable learning behavior with 2-bit activations, despite a slight accuracy degradation. The effectiveness of nested-mean clustering is demonstrated by comparing it to various quantile clustering approaches, emphasizing the importance of threshold robustness for convergence and prediction performance. The proposed quantization approach defines thresholds based on weight distribution for aggressive changes during updates. Configurability allows for n-ary weight representations, with quaternary achieving higher accuracy than ternary. Nested-mean clustering is validated against quantile clustering for Gaussian-distributed weights. The thresholds for cluster probabilities are computed to summarize accuracy using quinary weights for nested-means and quantile clustering. Increasing smaller cluster sizes while decreasing larger ones improves accuracy. Quantile-clustering assumes Gaussian weight distribution, unlike nested-means clustering. Parameter requirements and compression ratios for ResNet18 model layers are summarized in TAB5, while computational efficiency is shown for a typical ResNet layer in TAB6. The focus is on reduce-and-scale inference. Our approach focuses on compressing CNNs through quantization and connection pruning, reducing the resolution of weights and activations. This results in a significant reduction in computational complexity, with improvements in addition workload by a factor of 4.5x to 9.1x. The approach targets reduce-and-scale inference, utilizing reduced-precision additions and one full-precision multiplication per quantization level and output feature. Additional bits are added to activations for addition operations to prevent overflows. The approach focuses on compressing CNNs through weight quantization, reducing computational complexity and memory requirements. A nested-means clustering algorithm is used to assign weights to trainable scaling factors, offering low complexity and robustness to updates. The method allows for various quantization levels, enabling high compression rates while maintaining prediction accuracies close to single-precision floating-point weights. Additional quantization levels, like ternary to quaternary weights, improve prediction accuracy without increasing the bit requirements. The approach focuses on compressing CNNs through weight quantization, using a nested-means clustering algorithm for assigning weights to scaling factors. Experiments with DNN architectures like ResNet-18 and ImageNet show improved prediction accuracy with activation quantization based on statistical attributes."
}