{
    "title": "S1680_1Rb",
    "content": "The rise of graph-structured data in various domains has sparked interest in extending deep learning models to non-Euclidean domains. In this paper, a new spectral domain convolutional architecture for deep learning on graphs is introduced. The model utilizes parametric rational complex functions to efficiently compute spectral filters specialized on frequency bands of interest. The model's filters are localized in space, scale linearly with input data size for sparsely-connected graphs, and can handle different Laplacian operator constructions. Experimental results demonstrate superior performance in spectral image classification, community detection, vertex classification, and matrix completion tasks on large-scale non-Euclidean data domains such as social networks, genetic regulatory networks, brain functional networks, and 3D shapes represented as discrete manifolds. Geometric deep learning techniques, including convolutional neural networks (CNNs), have gained interest in extending models to data on graphs and manifolds. These approaches have been applied successfully in various fields such as computer graphics, brain imaging, and drug design. The earliest neural network formulation on graphs combined random walks with recurrent neural networks, while the first CNN-type architecture on graphs was proposed to address the challenge of extending CNNs to graphs. Several approaches have been proposed to extend CNNs to graphs, addressing challenges such as the lack of vector-space structure and shift-invariance. These include formulating convolution-like operations in the spectral domain, using smooth parametric spectral filters for spatial localization, and employing efficient filtering schemes with recurrent Chebyshev polynomials. Other methods involve operating on 1-hop neighborhoods of the graph, utilizing random walks, and implementing local patch operators represented as Gaussian mixture models. In this paper, graph CNNs are constructed using an efficient spectral filtering scheme based on Cayley polynomials, offering advantages over Chebyshev filters by detecting narrow frequency bands during training and specializing on them. Experimental results show that this method provides greater flexibility and performs better on various graph learning problems. In spectral graph theory, matrices and operations are defined to analyze undirected weighted graphs. The unnormalized graph Laplacian is a key concept in this field. The unnormalized graph Laplacian is a symmetric positive-semidefinite matrix used in spectral graph theory. It can be decomposed into eigenvectors and eigenvalues, which play a role in Fourier analysis. The graph Fourier transform of a signal on a graph is given by \u03a6f. Spectral convolution of two signals f and g on the graph is defined as the element-wise product of their Fourier transforms. Fourier transforms are used in spectral CNNs to generalize CNNs on graphs. Spectral convolutional layers involve spectral multipliers and nonlinearity functions applied to vertex-wise values. Graph coarsening is used for pooling, transferring signals from fine to coarse graphs. However, this framework has drawbacks, such as basis dependence in spectral filter coefficients. The spectral CNN framework has drawbacks including basis dependence in filter coefficients, expensive computation of Fourier transforms, and lack of spatial localization in filters. Smooth spectral filter coefficients aim to address these issues by ensuring spatially-localized filters. Smooth spectral filters are represented as g(\u03bb i ) = g(\u03bb), where g(\u03bb) is a transfer function of frequency \u03bb. BID13 used parametric functions with fixed interpolation kernels and optimization variables during network training. The filter is expressed as Gf = \u03a6diag(B\u03b1)\u03a6 f, resulting in filters with r = O(1) parameters. ChebNet. BID8 utilizes polynomial filters in the Chebyshev basis applied to rescaled frequency \u03bb \u2208 [\u22121, 1]. The filter is parametrized by a vector of polynomial coefficients \u03b1 optimized during training. This approach offers advantages such as avoiding expensive computation of Laplacian eigenvectors and having a low number of parameters independent of graph size. The filter computation involves applying the Laplacian r times, resulting in efficient operations for sparse matrices. ChebNet utilizes polynomial filters in the Chebyshev basis for efficient operations on sparse matrices. However, a key disadvantage is the difficulty in producing narrow-band filters due to the high order required, especially in graphs with community structures. To overcome this drawback, a new class of filters is needed that are both spatially localized and specialized in narrow frequency bands. The paper introduces Cayley filters as a family of complex filters that offer advantages over Chebyshev filters. These filters are spatially localized and specialized in narrow frequency bands, optimized through training. Cayley filters involve basic matrix operations and can be applied without expensive eigendecomposition. They are shown to be analytically well behaved. Cayley filters are analytically well behaved and can be represented as a Cayley polynomial. The Cayley transform is a smooth bijection between real numbers and the unit complex circle. The complex matrix obtained through the Cayley transform is unitary. Cayley filters can be written as a conjugate-even Laurent polynomial with respect to the scaled Laplacian. A Cayley filter can be seen as a multiplication by a finite Fourier expansion in the frequency domain. Spectral filters can be formulated as Cayley filters, specified by a finite sequence of values that can be interpolated by a trigonometric polynomial. Cayley filters with real coefficients result in even cosine polynomials, while those with imaginary coefficients result in odd sine polynomials. The spectrum of the Laplacian is mapped to the positive real numbers. The Cayley filter involves using complex coefficients to represent spectral filters, allowing for a larger variety without increasing complexity. The parameter h in the filter plays a crucial role in dilating the spectrum and spacing eigenvalues. A larger h spreads the spectrum apart in the positive real numbers, while a smaller h keeps high frequencies closer to zero. The Cayley filter uses complex coefficients for spectral filters, with parameter h dilating the spectrum and spacing eigenvalues. Tuning h allows for specialization in different frequency bands. The numerical core involves computing C j (h\u2206)f sequentially, with stable approximation using the Jacobi method. Equations can be solved exactly with matrix inversion but at a high cost. The Jacobi iteration matrix for equation (6) is denoted as J = (hD + iI) \u22121 hW. The approximate Cayley filter application is given by Gf = \u2211cj\u1ef9j, with O(rKn) operations for a sparse Laplacian. Error bounds are provided for both unnormalized and normalized Laplacians. Proposition 1 discusses the behavior of the method under certain assumptions. The spectral zoom parameter h accelerates convergence, with smaller values leading to faster convergence. In a CayleyNet, the number of Jacobi iterations is fixed for a graph, but may vary depending on the graph's properties. The trade-off between the spectral zoom amount h and inversion accuracy is balanced through training. The computational complexity of the method is studied in relation to the number of edges. The computational complexity of the method is analyzed as the number of edges of the graph increases. The Jacobi error is controlled by fixing the number of Jacobi iterations and the filter order independently of the graph size. Cayley filters, unlike Chebyshev filters, are rational functions with support across the entire graph, yet they remain well localized. Cayley filters are rational functions well localized on the graph. G\u03b4 m decays fast, with exponential decay on graphs defined. The Jacobi method for solving Ax = b involves decomposing A = Diag(A) + Off(A) iteratively. Theorem 4 states that a Cayley filter of order r has exponential decay in L2 around m, with constants c = 2M1G\u03b4m2 and \u03b3 = \u03ba1/r. Chebyshev filters are a special case of Cayley filters, providing stable approximations optimal for smooth functions. However, Chebyshev filters have the drawback of mapping the spectrum of \u2206 linearly to [-1, 1]. Cayley filters use a spectral zoom parameter to focus on specific frequency bands, unlike Chebyshev filters which linearly map the spectrum of \u2206. By choosing the right parameter h, Cayley filters can approximate ideal filters for extracting community information from graphs with strong communities. Projections onto trigonometric polynomials of order r provide a good approximation, preserving the interesting part of the filter concentrated on a small frequency band. Cayley filters are more stable than Chebyshev filters when approximating filters on small bands. They can model both concentrated impulse-like filters and wider Gabor-like filters with fewer coefficients. Both Cayley and Chebyshev filters have linear complexity and are simple to implement. The proposed CayleyNets have linear complexity O(n) and are simple to implement like Chebyshev filters. Experimental settings include using TensorFlow, Intel Core i7 CPU, NVIDIA Titan X GPU, and optimization methods like SGD+Momentum and Adam. Community detection experiments were conducted on a synthetic graph with 15 communities. The study involved analyzing connectivity within and across communities using a dataset with noisy step signals. A neural network with spectral convolutional layers was used for classification, showing that CayleyNet outperformed ChebNet by up to 80% for smaller filter orders. CayleyNet outperforms ChebNet by up to 80% for smaller filter orders, generating band-pass filters in the low-frequency band that discriminate well between communities. Experimental validation of computational complexity shows that approximate inversion guarantees O(n) complexity, with very few Jacobi iterations required for optimal performance. For a toy example, MNIST digits classification is approached as a learning problem on graphs, with each pixel of an image as a vertex on a graph. A graph CNN architecture with spectral convolutional layers based on Chebyshev and Cayley filters is used, achieving near perfect accuracy. ChebNet and CayleyNet filters are compared, with CayleyNet outperforming ChebNet for smaller filter orders. ChebNet and CayleyNet filters are compared for vertex classification on graphs using the CORA citation graph. Each vertex represents a scientific paper with a 1433-dimensional feature vector. The task is to classify vertices into 7 classes. The graph is split into training, validation, and test sets. ChebNet and CayleyNet are trained for this task, with CayleyNet requiring less time for analysis. ChebNet and CayleyNet are compared for vertex classification on graphs using the CORA citation graph. ChebNet requires Laplacians with spectra bounded in [\u22121, 1], while CayleyNet uses scaled unnormalized Laplacians. The comparison includes different settings, fixed filter orders, and network parameter numbers. Cayley filters are restricted to even cosine polynomials with real coefficients. In a comparison between ChebNet and CayleyNet for vertex classification on graphs, CayleyNet outperforms competitors consistently. CayleyNet is applied to a recommendation system for matrix completion, using a Recurrent Graph CNN architecture to extract spatial features from the score matrix. CayleyNet consistently outperforms ChebNet in vertex classification on graphs, requiring fewer parameters. In a recommendation system for matrix completion, Cayley filters are used in a Recurrent Graph CNN architecture with two CayleyNets of order r = 4. The study introduced a new spectral graph CNN architecture based on complex rational Cayley filters, outperforming previous methods in efficiency and performance. The architecture scales linearly with input data dimension and requires fewer parameters than ChebNet. The study introduced a new spectral graph CNN architecture based on complex rational Cayley filters that are localized in space and highly regular. The model specializes in narrow frequency bands with few filter parameters while preserving locality. Experimental validation showed superior performance in graph learning problems. The spectral radius of \u2206 is bounded by 2d in the non-normalized Laplacian case. Eigenvalues of \u2206 are less than \u03bb if (\u2206 - \u03bbI) is invertible, with (\u2206 - \u03bbI) being strictly dominant diagonal for any |\u03bb| > 2d. Levy-Desplanques theorem states that any strictly dominant diagonal matrix is invertible. The spectral radius of J is realized on the smallest eigenvalue of \u2206, which is |d - 0| = d. This leads to J having a spectral radius of \u03ba. By continuing from equation (9), we can derive that e_j \u2264 j\u03baK+1. In the case of the normalized Laplacian of a regular graph, J has a spectral radius of h\u221a(h^2 + 1), leading to e_j \u2264 j\u03baK+1. The text chunk discusses the inequality in test and training times based on filter order and graph size in a community detection dataset."
}