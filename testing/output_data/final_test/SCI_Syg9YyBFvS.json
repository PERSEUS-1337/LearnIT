{
    "title": "Syg9YyBFvS",
    "content": "Deep neural models like convolutional and recurrent networks excel with spatial data like images and text, while gradient boosting of decision trees (GBDT) is preferred for tabular data. To bridge this gap, we introduce deep neural forests (DNF), a novel architecture combining elements from decision trees and dense residual connections. Results from an empirical study comparing GBDTs, DNFs, and fully-connected networks are presented. The results show that Decision Neural Forests (DNFs) perform similarly to Gradient Boosting Decision Trees (GBDTs) on tabular data, offering potential for end-to-end neural modeling of multi-modal data. DNFs were successfully applied in a hybrid architecture for a multi-modal driving scene classification task. Decision forests have advantages in handling different feature types, insensitivity to feature scaling, and automatic feature engineering through decision stumps conjunctions. In contrast to deep neural models like CNNs and RNNs, GBDTs excel with tabular data due to their use of decision stumps conjunctions. While neural models are favored for spatial proximity structures in data like video and images, they struggle to match GBDTs' performance on tabular data. Default fully connected networks often fall short on tabular data, lacking specific biases towards this type of data. Our objective is to create a neural architecture that can be trained end-to-end using gradient based optimization and achieve comparable or better performance to GBDTs on tabular data. This architecture will allow the treatment of multi-modal data involving both tabular and spatial data while enjoying the best of both GBDTs and deep models. The objective is to create a neural architecture that combines the strengths of GBDTs and deep models for handling tabular data. The architecture aims to be scalable beyond the limitations of GBDTs and incorporate inductive bias relevant to the application domain, borrowing properties of decision trees and forests into the network structure. The Deep Neural Forest (DNF) architecture combines elements from decision forests and neural networks to achieve performance similar to GBDTs on tabular data. It consists of neural branches connected via dense residual links, forming an ensemble of stacks. Empirical studies show DNFs outperform FCNs and GBDTs on challenging classification tasks. The study compares DNFs, FCNs, and GBDTs on various tasks, showing DNFs outperform FCNs and match GBDTs' performance. DNFs are applied to multi-modal data in a hybrid architecture, leading to significant performance improvements. Previous works have explored neural networks for tabular data, with decision trees or forests as competitors. The reliance on conventional decision tree or forest methods in the proposed solution hinders end-to-end neural optimization, making it challenging to use on large datasets and in combination with other neural modules. Various techniques have been developed to address tabular data using pure neural networks. Recent techniques for coping with tabular data using neural optimization include approximating a single node of a decision tree with a soft binning function and a regularization technique for producing sparse networks suitable for large feature spaces in microbiome datasets. The proposed architecture utilizes Neural Branches (NB) to emulate the inductive bias of decision trees, with depth created by stacking NBs using dense residual links. A Neural Tree (NT) approximates a disjunctive normal form formula, representing decision trees as a set of conjunctions over decision stumps. The Neural Tree (NT) is constructed using soft binary OR and AND gates, with affine models similar to oblique decision trees. It is a three-layer network with the first hidden layer representing internal decision nodes. The NT approximates a disjunctive normal form formula and uses Neural Branches (NB) to emulate decision trees' inductive bias. The Neural Tree (NT) is a three-layer network where only the first hidden layer is trainable, representing internal decision nodes. Each decision node belongs to a single branch, with decision boundaries orthogonal to others in the same branch. Orthonormality is encouraged through the loss function to prevent redundancy. The Neural Tree (NT) is a three-layer network with orthonormality encouraged in the loss function to prevent redundancy. DenseNet is a convolutional architecture with a hierarchical structure and maximum information flow between layers. A Deep Neural Tree (DNT) is a stack of layers of NBs interconnected using dense residual links, with an OR gate applied only on the last NBs layer. It retains the desirable properties of deep neural models by introducing depth through dense residual links, allowing for improved flow of information and gradients throughout the network. In a Deep Neural Tree (DNT), layers of NBs are interconnected with dense residual links, with an OR gate applied only on the last NBs layer. Decision trees have greedy feature selection at any split, allowing them to exclude irrelevant features. Li et al. (2016) introduced a neural component for feature selection using heavily regularized masks. In our study, we add an independent mask for each DNT to multiply the input vector elementwise. In a Deep Neural Tree (DNT), a heavily regularized mask is added for each DNT to multiply the input vector elementwise. The mask weights undergo heavy elastic net regularization, with a binary threshold used to avoid pitfalls. The feature selection component involves a smooth approximation of the sign function for calculating gradients in the backward pass. The power of decision trees can be enhanced through ensemble methods like bagging or boosting. The final Deep Neural Forest (DNF) architecture is a weighted ensemble of DNTs, implemented by concatenating the DNTs outputs and applying one fully-connected layer. To enhance ensemble diversity, localization and random feature sampling techniques are used for each base learner (DNT). The Deep Neural Forest (DNF) utilizes a trainable mean vector \u00b5 and a constant covariance matrix \u03a3 = \u03c3 2 I. Each DNT specializes in a local sub-space using a learnable projection matrix W p. Feature sampling increases diversity by randomly selecting a subset of features for each DNT. In this section, the performance of Deep Neural Forests (DNFs), Gradient Boosted Decision Trees (GBDTs), and Fully Connected Networks (FCNs) is compared on simple synthetic classification tasks. FCNs, known as universal approximators, can approximate any function but are challenging to train using gradient methods. Training FCNs on tasks like learning parity is notoriously difficult, even with manually constructed networks. In simpler checkerboard problems, training Fully Connected Networks (FCNs) is challenging, even compared to learning parity. The checkerboard classification problem involves a two-dimensional square feature space with alternating labels in uniform squares. While a 2-hidden layers FCN can solve the checkerboard perfectly, achieving this through SGD training is not feasible. In an experiment testing FCNs, GBDTs, and DNFs on checkerboard instances, 10,000 labeled samples were generated for evaluation. Hyperparameters were optimized for FCNs and GBDTs, with FCNs tested on 1000 configurations including depth and width variations. The optimization process involved finding the dropout rate and L1 regularization coefficient for training FCNs and DNFs using SGD with the Adam optimizer. A grid search was conducted for decision tree algorithms, and the checkerboard experiment showed a decrease in performance as the checkerboard size increased. The average number of training points in each checkerboard cell is decreasing, with FCNs failing to outperform random guessing for n \u2265 14 board sizes. XGBoost consistently outperforms FCNs for n > 2, while DNFs achieve the best results for n > 9. FCNs perform best with millions of parameters, while DNFs excel with only 4K trainable parameters. Batch size significantly affects results, with FCNs not showing an advantage for n \u2265 14 with mini-batches larger than 512. Checkerboard-like phenomena are common in tabular datasets. In tabular datasets, Checkerboard-like phenomena are common. Using the Titanic dataset as an example, patterns in male age versus survival probability are observed. DNFs demonstrate the ability to handle irrelevant features, as shown in a XOR-problem scenario with additional irrelevant features. Performance remains excellent with increasing numbers of irrelevant features. The top-performing FCNs had one hidden layer and strong L1 regularization, showing representation efficiency. DNFs outperformed FCNs with less than 200 neurons compared to 10K neurons. XGBoost struggled with increasing irrelevant features due to high symmetry. A comparison between DNFs and basic neural trees (NT) with dense residual connectivity was conducted to examine the effect of depth. The data was generated using a checkerboard pattern with an additional binary feature. The data was generated using a checkerboard pattern with an additional binary feature. The label of each instance is a XOR between the binary feature and the label defined by the checkerboard. DNFs outperformed FCNs on checkerboards with n \u2265 11, while NTs excelled on checkerboards with n \u2208 {4, . . . , 8}. FCNs were not included in this study as their performance on the checkerboard alone was inferior. The performance of DNFs and baselines (FCNs and XGBoost) on tabular datasets from Kaggle and OpenML was examined. The study utilized datasets from Kaggle and OpenML, training models without feature engineering. Hyper-parameters were optimized using grid search. DNFs were trained with SGD and Adam optimizer, while FCNs were trained with SGD and Adam as well. Results are summarized in Table 1, with the best result highlighted. The study compared the performance of DNFs, XGBoost, and FCNs in tabular data experiments. GBDTs have dominated the tabular data domain, while deep models like CNNs and RNNs have dominated the visual and textual domains. Integrating GBDTs with CNNs can be problematic due to GBDTs not being differentiable. Typically, FCNs are used instead, but their utilization can degrade performance. The study examines the utilization of DNFs as an alternative to FCNs in a hybrid approach. In this section, the utilization of DNFs in a hybrid model is examined for handling multi-modal data from the Honda Research Institute Driving Dataset. The dataset includes video frames and sensor measurements from real human driving. Four classification tasks related to driver behavior are defined, with the first task focusing on goal-oriented actions such as 'right turn' and 'left turn'. Ramanishka et al. (2018) presented baseline results for this task. In their study, Ramanishka et al. (2018) presented baseline results for handling multi-modal data using a CNN for images and an FCN for sensor data, fused and fed into an LSTM. The authors replicated this structure with the only change being the replacement of the FCN with a DNF. They conducted comparative experiments on two tasks: predicting navigation labels using only sensors and utilizing both video and sensors. In a study by Ramanishka et al. (2018), baseline results for handling multi-modal data were presented using a CNN for images and an FCN for sensor data, fused and fed into an LSTM. The authors replaced the FCN with a DNF and conducted experiments on predicting navigation labels using only sensors and both video and sensors. The results showed that DNFs significantly outperformed FCNs over tabular data tasks. The study suggests that DNFs outperform FCNs in tabular data tasks and perform comparably to GBDTs. Further research is needed to fully understand DNFs and optimize them sequentially like GBDTs. Theoretical understanding of tabular data characteristics can lead to performance gains in multi-modal settings. Hyperparameter details can be found in Table. To demonstrate the checkerboard phenomena in tabular data, probability estimates for the Titanic dataset are plotted. The task is to predict passenger survival using age and ticket fare as features. Gender greatly influences the results, so plots are focused on males. The first plot shows a sharp transition from missing data to babies, and softer transitions at ages 14, 25, and 42, indicating a checkerboard behavior. The second plot is bivariate, showing age on the x-axis. The second plot is bivariate, with age on the x-axis and ticket fare on the y-axis, showing a checkerboard-like pattern in survival probability. Gradient boosting models like XGBoost, LightGBM, and CatBoost have a disadvantage of needing to store the entire dataset in memory, but optimizations like quantizing features and selecting random subsets can help mitigate this issue. When dealing with large datasets like the Honda Research Institute Driving Dataset (HRI-DD) of Section 6, traditional Gradient Boosting Decision Tree (GBDT) techniques become less effective due to memory constraints. The HRI-DD set contains approximately 1.2M samples, requiring around 440GB of RAM to hold all the data. To achieve reasonable performance on tasks like the HRI-DD, data should be modeled as time-series, leading to memory requirements that can exceed available RAM. A Disjunctive Normal Form (DNF) is implemented by concatenating Decision Tree outputs and applying a fully connected layer."
}