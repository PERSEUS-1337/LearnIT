{
    "title": "SJIA6ZWC-",
    "content": "Machine learning models are typically tuned by optimizing model weights and hyperparameters separately. A new method is proposed to combine this nested optimization into joint stochastic optimization. This method trains a neural network to output optimal weights based on hyperparameters, leading to locally optimal results. The effectiveness of this approach is demonstrated for tuning thousands of hyperparameters. Hyperparameter optimization is a bottleneck in designing predictive models. Gradient-free optimization methods like grid search, random search, and Bayesian optimization are commonly used. However, methods like Hyperband and freezethaw Bayesian optimization resume model training to avoid wasteful reinitialization of weights. In 2014, model training resumes without wasting effort. Gradient-free optimization struggles beyond 10 or 20 dimensions. To avoid re-training from scratch, a neural network is trained with hyperparameters as inputs to find optimal weights. This approach allows for convergence using stochastic gradient descent and optimization of hyperparameters with gradient-based methods. A hypernet is used to output approximately optimal weights for another neural network by updating hypernet weights \u03c6 instead of updating weights w using the loss gradient. This allows for convergence using stochastic gradient descent and optimization of hyperparameters with gradient-based methods. Hyper-training updates hypernet weights \u03c6 using the chain rule, aiming to closely match the best-response function w * (\u03bb) for optimal weights. This method contrasts with Bayesian optimization, which builds a model of validation loss but requires optimizing models for each set of hyperparameters. Hyper-training avoids this by never fully optimizing any one model. Hyper-training optimizes hypernet weights using the chain rule to match the best-response function for optimal weights. It differs from Bayesian optimization by not fully optimizing any one model and utilizing the known, differentiable validation loss function. Hyperparameters optimized include optimization parameters like learning rates and regularization/model architecture parameters. Inner optimization hyperparameters are not present in hyper-training due to the absence of an inner training loop. The method involves choosing optimization parameters for the fused optimization loop in unconstrained bi-level optimization problems. It updates hypernet weights using the chain rule and gradient-based hyperparameter optimization, unlike standard methods. Hyperopt is a generic hyperparameter optimization method that can handle discrete hyperparameters but not continuous ones. The optimized hyperparameters must impact training loss, excluding optimization hyperparameters like learning rate. The approach focuses on making local changes to hyperparameters and does not incorporate uncertainty-based exploration. Training a hypernet typically requires training several times as many parameters as training a single model. Training a hypernet requires training multiple parameters compared to a single model. Algorithm 2 uses SGD to train the hypernet, converging to a local best-response. Sufficiently powerful hypernets can learn continuous best-response functions, minimizing expected loss for any hyperparameter distribution. The hypernet training algorithm aims to minimize expected loss for any hyperparameter distribution by finding optimal hypernet parameters. The algorithm involves stochastic optimization of hypernet and hyperparameters, ensuring continuous best-response functions are learned. The hypernet training algorithm aims to minimize expected loss by finding optimal hypernet parameters. It learns best-response in the support of the hyperparameter distribution, using linear regression locally. Joint optimization of \u03c6 and p(\u03bb) is motivated to put mass on promising hyperparameter values. Introducing a \"current\" hyperparameter\u03bb allows for the use of a limited-capacity hypernet. In practice, the optimal hypernet depends on the hyperparameter distribution p(\u03bb), not just its support. Algorithm 3 focuses on learning a best response locally by updating a \"current\" hyperparameter\u03bb and using a conditional hyperparameter distribution p(\u03bb|\u03bb) that places mass near\u03bb. This approach combines the two phases of Algorithm 2 into one. Algorithm 3 simplifies the process by sampling hyperparameters near the current best guess and training a locally-trained hypernet to estimate weights for a small set of hyperparameters. The hypernet, which can be a linear function, provides gradients to update hyperparameters based on validation set performance. This approach combines the two phases of Algorithm 2 into one. Our work involves restricting the capacity of a linear hypernet by factorizing its weights, adding a bottleneck layer with a linear activation. We focus on exploring continuous spaces of models, while other approaches like grid search and random search are model-free and only use trial-and-error for hyperparameter optimization. Model-based approaches aim to build a surrogate function for optimization. Model-based approaches aim to build a surrogate function for optimization, such as Bayesian optimization. Freeze-thaw Bayesian optimization can condition on partially-optimized model performance. Optimization-based approaches approximate gradients of the validation loss with respect to hyperparameters using methods like BID4 and BID16. Other techniques like DrMAD and HOAG find hyperparameter gradients through implicit differentiation. The BID6 study forward and reverse-mode differentiation for hyperparameter gradients. BID5 establish conditions for smooth validation loss, enabling gradient-based hyperparameter training. T 1 \u2212 T 2 method of BID15 also offers stochastic gradient-based optimization of hyperparameters. Our method's convergence relies on a powerful hypernet, while BID15's method approximates the Hessian with the identity matrix for convergence. Game theory explores best-response functions in multi-agent games, including adversarial training. In adversarial training, a discriminator is trained by observing the generator's parameters. The experiment focuses on stochastic gradient-based optimization of neural networks with weight regularization. Gradient-based methods use the loss gradient, while gradient-free methods do not but can use a surrogate loss gradient. Our algorithm contrasts with gradient-free techniques like Bayesian optimization, as it handles hyperparameters affecting the training loss and optimization parameters. Comparing to gradient-based methods, our approach specifically deals with parameters affecting the training loss. In experiments optimizing neural networks with weight regularization, Algorithms 2 or 3 are used to optimize weights with mean squared error on MNIST. Autograd is used for computing derivatives, Adam for training, and ReLU activation for hidden units. Each experiment involves minibatch samples, training data points, and a MacBook pro for running. The first experiment demonstrates learning a global approximation to a best response function using Algorithm 2. The hypernetwork was trained using Algorithm 2 with 10 data points to visualize regularization loss. We compared the performance of weights output by the hypernet to standard cross-validation. Hyperparameters were randomly initialized and optimized using Adam BID12. The hypernet had 50 hidden units and 400-450 parameters. The results showed that the hypernet can approximate a global best-response function in small problems. Using fused updates of Algorithm 3 also led to finding a best-response approximation. The experiment showed that a locally-trained linear best-response function can provide enough gradient information to optimize hyperparameters efficiently. Algorithm 3 was found to be less computationally expensive compared to Algorithms 1 and 2. The hypernetwork used a linear model with 15,700 weights and a conditional hyperparameter distribution of p(\u03bb|\u03bb) = N(\u03bb, 0.00001). The hypernet and hyperparameters were updated using the same optimizer as the global best-response. This approach resulted in finding the true optimum on the validation loss. The experiment demonstrated that Algorithm 3 efficiently optimizes hyperparameters by using a linear hypernet with 10 hidden units. Hyper-training with deeper networks reaches sub-optimal solutions due to limitations on sampling hyperparameters, but overfits less than unrolling. This method effectively optimizes thousands of hyperparameters, showcasing its scalability compared to standard Bayesian optimization. The experiment showed that Algorithm 3 efficiently optimizes hyperparameters using a linear hypernet with 10 hidden units. It can scale to networks with multiple hidden layers and outperform hand-tuned settings. Adding more layers reduces the gap between validation and testing loss, leading to better performance on the validation set. Future work should explore other architectures like recurrent or convolutional networks. In a final experiment, the study aims to determine the reason for the superior performance of their method compared to Bayesian optimization. They constructed a hyper-training set by optimizing 25 sets of weights with randomly-sampled hyperparameters to investigate if the performance improvement is due to a better inductive bias or the ability to explore more hyperparameter settings during optimization. The study chose 25 samples to leverage Gaussian process-based approaches. A validation set of 10,215 (optimized weight, hyperparameter) tuples was created. A Gaussian process regression model and a hypernet were fitted to the hyper-training data. Three approaches were compared for predicting validation loss: Gaussian process, hypernet, and a proposed method using stochastically sampled hyperparameters. The proposed method showed a distribution of predicted and true losses, with the diagonal line representing equality. The distribution of differences between predicted and true losses was analyzed. The Gaussian process tended to under-predict true loss, while the hypernet over-predicted it. Two hypernets were trained with the same optimizer parameters, showing different prediction errors. The hypernet produced bad weights in regions with insufficient training data, leading to overestimation of loss. The hypernet trained with Algorithm 2 produces errors tightly centered around 0, learning more accurate surrogate functions than a GP for equal compute budgets. Hypernets provide a better inductive bias for hyperparameter optimization than Gaussian processes. The code for all experiments will be available upon publication. In a study on stochastic hyperparameter optimization, the best-response for all hyperparameters is learned. The experiment involves optimizing a model with 10 hyperparameters using a linear hypernet, resulting in faster convergence to a better optimum compared to random search. This demonstrates that medium-sized hyperparameter optimization problems can be effectively solved. The study demonstrates that medium-sized hyperparameter optimization problems can be effectively solved using Algorithm 3. Hypernetwork-based optimization converges faster than random search or Bayesian optimization. Overfitting of hyperparameters on the validation set can be reduced by introducing hyperhyperparameters. The runtime comparison includes the inner optimization for gradient-free approaches. The study shows that hypernetwork-based optimization is more efficient than random search or Bayesian optimization for medium-sized hyperparameter problems. Introducing hyperhyperparameters can help reduce overfitting on the validation set."
}