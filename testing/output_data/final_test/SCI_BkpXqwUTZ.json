{
    "title": "BkpXqwUTZ",
    "content": "In vanilla backpropagation, the choice of activation function is crucial for non-linearity and differentiability. This study introduces a new approach using iterative temporal differencing with fixed random feedback weight alignment to replace the derivative of the activation function. This method makes error backpropagation more biologically plausible for learning deep neural network architectures. The study introduces iterative temporal differencing with fixed random feedback weight alignment to enhance error backpropagation for deep neural network architectures. This approach aims to integrate spike-time dependent plasticity (STDP) principles into deep learning, mimicking biological processes for more effective learning. The discovery of fixed random synaptic feedback weights alignments in error backpropagation for deep learning led to the quest for a biological version of VBP. Spiketime dependent plasticity and deep learning using segregated dendrites, inspired by Hinton's recirculation idea, aim to model STDP into backprop successfully. Visual demonstration of ITD using FBA in VBP shows VBP, VBP with FBA, and ITD using FBA in one figure, with Tanh function as the activation function. The ITD was applied to the MNIST standard dataset and compared using maximum cross entropy. In this study, iterative temporal differencing (ITD) was compared to fixed random synaptic feedback alignment (FBA) and visualized using maximum cross entropy (MCE) as the loss function. The experiments had equal hyperparameters with 5000 iterations, 0.01 learning rate, and 100 minibatch size. The focus was on creating a biologically plausible backpropagation method for deep learning, integrating STDP learning processes in the brain. Future steps involve exploring STDP processes in learning and dopamine-based unsupervised learning. The study focused on integrating STDP learning processes in the brain for creating a biologically plausible backpropagation method for deep learning. Future steps involve investigating STDP processes in learning, dopamine-based unsupervised learning, and generating Poisson-based spikes."
}