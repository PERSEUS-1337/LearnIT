{
    "title": "H1lBj2VFPS",
    "content": "The paper proposes a learned linear symmetric quantizer for integer neural network processors, optimizing performance by quantizing neural parameters and activations to low-bit integers. It also accelerates hardware inference through batch normalization fusion and low-precision accumulators and multipliers. The approach outperforms previous methods in efficiency and hardware coordination. The proposed method uses a unified approach to quantize weights and activations, achieving superior results in various networks like AlexNet, ResNet, and MobileNet. It is also applied to object detection models with high performance in YOLO-v2. The quantized models are deployed on a specialized DNN accelerator, showing effectiveness with linear symmetric quantization even outperforming asymmetric or non-linear methods in 4-bit networks. The quantizer induces less than 0.4% accuracy drop in ResNet18, ResNet34, and AlexNet when quantizing the entire network for integer processors."
}