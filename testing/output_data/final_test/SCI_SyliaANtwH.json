{
    "title": "SyliaANtwH",
    "content": "A new class of data poisoning attacks on neural networks involves the attacker making small perturbations to a subset of training data to take control of the model. This meta-poisoning strategy can poison networks trained from scratch with unknown initialization and transfer across hyperparameters, causing misclassification of target images. Results show over 50% attack success rate by poisoning just 3-10% of the training dataset. Deep neural networks are used in high stakes applications like automated vehicles, medical diagnosis, and copyright detection. Neural networks are vulnerable to security vulnerabilities, with data poisoning emerging as a threat model where attackers manipulate training data to control classifier behavior. Unlike evasion attacks, data poisoning can occur without access to test-time data, posing a significant threat to the reliability of machine learning systems. Data poisoning is a threat to neural networks where attackers manipulate training data to control classifier behavior. Current methods for targeted attacks are limited to transfer learning or backdoor attacks. A new method called meta-poisoning, inspired by meta-learning techniques, aims to learn image perturbations that interfere with training dynamics to produce targeted behaviors. Meta-poisoning manipulates the training pipeline to create images that affect training outcomes. It can reliably change the label of a test image using a small poison budget, even in from-scratch training. The process can use augmentation strategies to create poisons that work across different models and training environments. Targeted poisoning attacks, such as backdoor attacks, insert patterns into training images to manipulate the network into associating these patterns with a specific class, leading to misclassification at test time. These attacks assume a strong threat model where the attacker can manipulate both train and test inputs. In feature collision attacks, the attacker manipulates a training image to alter the class label of a target image at test time without the need for a trigger. The network over-fits on the data during training, leading to misclassification of the target image with a similar feature representation to the poison image. This attack has been extended to convex polytope attacks. Clean-label attacks on networks trained from scratch have not been demonstrated yet. Existing attacks like feature collision and convex polytope attacks rely on hand-crafted formulations and heuristics, assuming that the decision boundary will wrap around the poison if it is close enough to the target in representation space. Backdoor attacks also rely on a Trojan feature to assign class labels. Meta-learning is a field that exploits training dynamics to create networks that learn quickly when presented with few-shot learning tasks. It involves an inner loop that uses an optimizer to train on new data and an outer loop to find optimal network parameters that adapt quickly to new tasks. While primarily used for few-shot learning, meta-learning has other applications as well. The proposed methods in meta-learning craft poison images to poison networks trained from scratch without manipulating test-time inputs. The poisons can transfer across different training pipelines and be crafted for different types of attacks by changing the crafting loss. The attacker can perturb the training set by adding small values to a limited number of examples. The attacker can perturb a small number of examples in the training set to craft a transferable poison attack that is robust to changes in initialization and training processes. This involves optimizing a loss function to find the network parameters for the perturbed training data. The attacker can perturb training examples to create a transferable poison attack that is robust to changes. To optimize the loss function, methods from meta-learning are adopted to find model parameters that minimize poisoning loss after applying an SGD update with a randomly chosen model. MetaPoison is a method for crafting poisoned images via Meta-Learning, where a subset of images from the training set are selected to be poisoned by unrolling and back-propagating through a few steps in the training process. This approach aims to minimize poisoning loss after applying an SGD update with a randomly chosen model. MetaPoison crafts poisoned images by selecting training set images to be poisoned through unrolling and back-propagation. It updates the poison perturbation to cause severe impact when added to training images for a single SGD step. The method aims to minimize poisoning loss by updating models and evaluating adversarial loss on target images. The method of crafting poisoned images involves selecting training set images to be poisoned through unrolling and back-propagation. The process includes refining the perturbation by applying a gradient to the source image, and evaluating the loss on the target image. This is done by unrolling the computation graph over multiple SGD steps to compute a meta-gradient, which is then applied as a perturbation using the Adam optimizer. To evaluate the crafted poisons, poisoned images are inserted into a \"victim\" model with unknown initialization and training strategy. The training follows a new optimization trajectory to minimize training error and target adversarial error. This approach involves repeating gradient computation over multiple surrogate models pre-trained to varying epochs before the poisoning process begins. The algorithm for poisoning models involves resetting models to epoch 0 with new random initialization after reaching a certain number of epochs. The task involves poisoning a standard Resnet-20 trained on CIFAR-10 using cross entropy loss and the Adam optimizer. Target images are chosen from the test set, and source images are selected from a random poison class. The Adam optimizer Carlini & Wagner (2017) is used to project onto a constraint after each gradient update. Poisoned images successfully change the label of the targeted image without affecting validation accuracy, making the attack imperceptible. The crafting process shows a gradual decrease in target loss over training epochs. The poisoning attack successfully changes the label of the targeted image without affecting validation accuracy, making it nearly undetectable. The effectiveness of the poison increases as more information is accumulated during the crafting process. Training on the poisoned dataset leads to misclassification of the target example, with minimal impact on other images. The adversarial target loss decreases and attack success rate increases as the model's generalization ability improves. Different source-target pairs show consistent results over multiple training runs. The poison budget, limiting access to poison class images during training, is explored, with findings showing a reduction in the number of images needed for successful attacks. The effectiveness of poisoning attacks varies depending on the poison-target pairings rather than the control over the image set. The method used in the attack shows precision in modifying only the target image, making it hard to detect without prior knowledge. The poisoning attacks are effective due to precision in modifying target images, making detection difficult without prior knowledge. Crafted poisons are robust against hyperparameter changes, as shown in Figure 5b. The poisoning attacks are effective due to precision in modifying target images, making detection difficult without prior knowledge. Crafted poisons are robust against hyperparameter changes. MetaPoison can flexibly attack in other ways, including a Third-Party Attack scenario where the attacker aims to transfer a target image to a different label. The poisoning attacks are effective in modifying target images precisely, making detection challenging without prior knowledge. Multi-target attacks are difficult due to inter-class variability between different targets, reducing the reliability of poisoning. Loss landscape visualization using PCA helps infer information about the actual loss landscape. The main components differentiate final clean and target weight vectors, accounting for 96% of the variance. The plot shows the agreement between schematics and experimental results, with MetaPoison having substantial distance between target and poison, suggesting a different mechanism. The optimization-based approach generates the mechanism implicitly. By extending learning-to-learn techniques, a novel fast algorithm was devised to solve the bi-level optimization problem in data poisoning attacks. The results demonstrate the effectiveness of this method in creating clean-label poisoning attacks on networks trained from scratch. These attacks are versatile and can even perform third-party attacks, showcasing their potential to target industrially-relevant architectures. This work establishes a strong attack baseline for future research on clean-label data poisoning. The novel fast algorithm can create clean-label poisoning attacks on networks, showcasing their potential to target industrially-relevant architectures."
}