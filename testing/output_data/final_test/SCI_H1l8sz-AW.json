{
    "title": "H1l8sz-AW",
    "content": "Learning rules for neural networks often involve regularization, which can be done in the space of parameters or functions. A proposed approach involves measuring networks in an $L^2$ Hilbert space and using a learning rule called Hilbert-constrained gradient descent (HCGD) to regulate the distance a network can travel through $L^2$-space each update. This method is inspired by gradient descent and the natural gradient, leading to better generalization in experiments with large neural networks. The text discusses how learning rules for neural networks can encourage better generalization through regularization techniques, such as weight decay and constraints on parameter movement. A new learning rule is introduced that limits how much the output function can change during learning, similar to the natural gradient approach. The text introduces a new learning rule called Hilbert-constrained gradient descent (HCGD) that uses the expected L2 norm over function space. It discusses the natural gradient approach, which involves computing the covariance of gradients and multiplying the current gradients by its inverse. This approach is seen as a regularizer of functional change. The natural gradient is a learning rule that scales updates based on the informativeness of each dimension in parameter space. It is developed in the context of information geometry and efficiency, aiming to optimize neural networks by considering the covariance of gradients and the benefits of whitened gradients. The natural gradient learning rule aims to optimize neural networks by considering the informativeness of each parameter dimension. It uses the Fisher information matrix to scale updates based on parameters' informativeness, leading to a more efficient learning process. The Adagrad paper demonstrated that using the F\u22121 J update reduces regret compared to gradient descent, with F representing the Malahanobis norm computed over the same examples as J. The Adagrad paper introduced the concept of using a diagonal approximation of the empirical Fisher for neural network optimization. Related methods like Adadelta and RMSprop also employ similar diagonal approximations. Whitening gradients through techniques like F\u22121 J can make SGD more similar to the natural gradient and speed up convergence. Activation whitening methods, including Batch Normalization, normalize and whiten gradients or activations to improve parameter space as a proxy for function space. K-FAC is a scalable approximation of the natural gradient that converges faster than SGD with momentum. Pascanu and Bengio explain the natural gradient as taking constant steps in output distribution space, measured by KL divergence. The natural gradient is the optimal update when penalizing the change in a network's output distribution, measured by KL divergence. To regulate the output distribution throughout parameter optimization, similarity between distributions is measured using KL divergence. To regulate the output distribution throughout parameter optimization, a new cost function is defined with a regularization term controlled by a hyperparameter \u03bb. The KL divergence from P \u03b8t to P \u03b8t+1 is used to measure the change in entropy of the output distribution during learning, leading to a solution with higher entropy. The KL divergence is used to measure entropy changes between optimization steps, leading to a solution with higher entropy. The Fisher information metric is defined as the covariance matrix of gradients. The regularized cost function is optimized via gradient descent by minimizing Equation 2. The natural gradient emerges as the optimal update when explicitly regularizing the change in the output distribution during learning. Using the L2 space with a specific norm, the distance between two functions can be defined. This notion of distance is used to regulate the change of a network's output function. The cost function in Equation 6 imposes a penalty on the output difference between the current network at time t and the proposed network at t + 1. This leads to a learning rule called Hilbert-constrained gradient descent (HCGD), which is a modification of gradient descent. The implementation, shown in Algorithm 1, incorporates lessons from the natural gradient approach. The Hilbert-constrained gradient descent (HCGD) algorithm, inspired by the natural gradient, aims to converge to an optimal solution at each update step by iteratively correcting the proposed \u2206\u03b8 using gradient descent. Empirical findings suggest that a single correction is often sufficient, but multiple iterations are also possible for higher precision. The algorithm tightens the analogy between HCGD and the natural gradient, discussing how the natural gradient can be approximated with an inner first-order method. The text discusses approximating the natural gradient with an inner first-order optimization loop to avoid poor behavior caused by evaluating the Fisher and gradient on the same batch of data. It emphasizes the importance of using a different batch for the inner loop to prevent overfitting and provides steps for obtaining a proposed update via SGD with momentum. The text discusses implementing momentum for HCGD to improve optimization speed and reduce generalization error. HCGD uses a velocity term updated with the final Hilbert-constrained update instead of the instantaneous gradient. This approach is computationally cheaper than exact natural gradient methods. When the validation batch X V is drawn anew for each corrective iteration in HCGD, it requires additional passes for each correction i < n. This can be reduced by 1 pass for i \u2265 1 if X V is drawn anew just for i = 0. The method is demonstrated on MNIST digit classification and CIFAR10 image classification tasks using tuned learning rates and specific values for \u03bb and \u03b7. The batch size for the \"validation\" batch is set to 256, and all models are implemented in PyTorch BID14. The dense multilayer perceptron models were implemented in PyTorch BID14 without dropout or batch normalization. HCGD notably improved performance on the test set for both algorithms with and without momentum. The gradient correction towards the ideal Hilbert-constrained update was tested with n = 1 and n = 10, showing similar behavior. The n = 1 version was used in future tests, showing improved accuracy on MNIST digit classification. HCGD converges faster than SGD and generalizes better for gradient corrections. It requires more passes through the computational graph but outperforms standard SGD and ADAM in training a 3-layer CNN with Batch Normalization on MNIST digit classification. Batch normalization normalizes and whitens activations and gradients. HCGD outperforms SGD in training a Squeezenet v1.1 model on CIFAR10 image classification. It performs better with a high initial learning rate but worse once the learning rate is decreased. This could be due to HCGD reducing the effective learning rate, impacting the annealing effects. When comparing HCGD and SGD on training a Squeezenet v1.1 model on CIFAR10, HCGD generally decreases test error at a given learning rate but requires a higher learning rate to achieve a similar level of gradient noise. The study emphasizes that regularization and optimization should occur in the space of functions, not just parameters. In this section, we investigate the difference between SGD and HCGD in terms of their movement through parameter and function space. SGD is a local update rule that discourages large jumps, leading to solutions close to initialization and limited exploration. This may explain its unexpected generalization abilities. The movement through parameter space may not directly translate to function space due to qualitative differences in distances. In examining network movement through parameter and function space, the cumulative squared distance traveled during optimization is plotted. SGD moves slowly through parameter space but linearly through L2 function space. Even after test error saturation, the network continues to drift through L2 space. SGD with momentum reduces the total distance traveled significantly. The effect of momentum on network drift in parameter space and L2-space is shown in Figure 5 for an MLP trained on MNIST. SGD continues to drift in L2-space during overfitting, while HCGD plateaus. Momentum decreases the scale of distance traveled. HCGD algorithm is designed to reduce motion through L2-space. The HCGD algorithm greatly reduces motion through L2-space, converging to a single function location. Unlike SGD, HCGD allows parameters to drift even after convergence. Regularizing the change in L2 space between updates limits function movement, similar to how gradient descent limits parameter movement. The Hilbert-constrained gradient descent (HCGD) learning rule improves test performance on image classification architectures by penalizing sensitive controls of outputs in L2-space. HCGD limits changes in L2-space and encourages learning from current examples without affecting previous learning. The algorithm aims to prove better generalization bounds by stabilizing analysis. The stability analysis framework has been used to establish bounds on the generalization error of SGD. By analyzing the stability of SGD in parameter space and moving to function space using a Lipschitz condition, the error stability can be bounded. It is suggested that bounding the movement through L2-space may lead to increased error stability compared to bounding movement through parameter space. The idea of learning rules that diminish the size of changes in neuroscience is also discussed. The nervous system may not follow natural gradient descent, but evidence suggests a similar mechanism is at play. Behavioral learning rates in motor tasks are influenced by error direction, not error magnitude. Regularization on behavior change, rather than synaptic change, predicts slow learning in neurons central to many actions. The nervous system may not follow natural gradient descent, but evidence suggests a similar mechanism is at play. In order to better compare the natural gradient to the Hilbert-constrained gradient, a natural gradient algorithm of a similar style is proposed. Previous work on the natural gradient has aimed to approximate F \u22121 as best and as cheaply as possible, minimizing Equation 2 with a single iteration of a second-order optimizer. For very large neural networks, it is much cheaper to calculate matrix-vector products than to approximately invert a large matrix. The natural gradient may be more accessible via an inner gradient descent, which would be performed during each update step as an inner loop. The algorithm described in Algorithm 2 corrects update steps towards the natural gradient by iteratively adjusting proposed updates. It is recommended to use a fast diagonal approximation of the natural gradient as the main optimizer for a good initial proposed update. Each correction requires just one matrix-vector product after calculating gradients, leading to a small number of iterations that can improve the update. This algorithm can be paired with any optimizer to increase its similarity to the natural gradient. The Fisher matrix F can be calculated from the covariance of gradients without needing to be fully stored. Gradients must be calculated on a per-example basis to avoid a rank-1 Fisher matrix. Deep learning frameworks need to implement forward-mode differentiation for efficient computation. The array of per-example gradients on the minibatch is known as the 'empirical Fisher'. Sampling from the output distribution of the network is the proper method to calculate G. To calculate G, one can sample from the output distribution and re-run backpropagation on fictitious targets, using activations from the minibatch. Alternatively, unlabeled or validation data can be used to calculate G on each batch."
}