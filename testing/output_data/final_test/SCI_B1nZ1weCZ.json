{
    "title": "B1nZ1weCZ",
    "content": "Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning, where a student network learns from multiple task-specific expert networks by mimicking their policies. However, these approaches require supervision from large expert networks, leading to extensive data and computation time for training. In this work, an efficient multi-task learning framework is proposed for solving goal-directed tasks online without expert supervision. Active learning principles are used to sample harder tasks more frequently. Three models are introduced: an adaptive method with competitive performance, a UCB-based meta-learner treating task selection as a bandit problem, and a meta-learning method using actor-critic for optimizing multi-task performance. Results are demonstrated in the Atari 2600 domain across various multi-task instances. Deep Reinforcement Learning (DRL) combines Deep Learning (DL) with Reinforcement Learning (RL) to solve complex visual control tasks directly from raw pixels. However, DRL agents trained using such algorithms tend to be task-specific, motivating the field of multi-task learning to find a single agent that can perform well on all tasks. Training a neural network with a multi-task learning algorithm leads to a multi-tasking agent capable of generalizing across tasks. A multi-tasking agent (MTA) is instantiated through Multi-Task Integration (MTI), enabling it to learn task-agnostic representations and generalize learning across different tasks. Deep Reinforcement Learning (DRL) approaches to multi-task learning fall into two categories: extracting expertise from multiple task-specific networks into a single student network, and training individual expert networks before distilling their policies into the student network. Expert networks need to be trained, which is computationally intensive. DRL approaches to multi-tasking include transfer learning methods like Progressive Networks, which create task-specific column networks for each new task. However, scalability is limited as the number of parameters grows with each task, and the approach requires a predetermined training order. The proposed fully on-line multi-task DRL approach addresses the challenge of training networks on multiple tasks in a specific order. It introduces a novel framework for multi-task learning with visually diverse high-dimensional state spaces. The approach includes three different instantiations: an adaptive method, a UCB-based meta-learning method, and an A3C based meta-learning method. Additionally, the framework introduces robust evaluation metrics for assessing multi-tasking algorithms and analyzes the abstract features learned by the methods. The proposed framework for multi-task learning in deep reinforcement learning introduces novel approaches for training networks on multiple tasks. It includes three different instantiations and robust evaluation metrics for assessing multi-tasking algorithms. The framework analyzes abstract features learned by the methods and demonstrates generalization across tasks. The UCB algorithms are efficient for solving the bandit problem by selecting the arm with the maximum expected reward. They track uncertainty in estimates and provide exploration bonuses for lesser explored arms. Discounted UCB style algorithms are used for non-stationary bandit problems. In UCB-based meta-learner experiments, the Discounted UCB1-Tuned+ algorithm is utilized. Actor critic algorithms are also used for learning optimal control in reinforcement learning. Actor critic algorithms in reinforcement learning involve an actor and a critic. The actor's parametric function maps states to actions, while the critic estimates action values and a state-dependent baseline. The critic is trained using temporal difference learning algorithms like TD(0). The Asynchronous Advantage Actor-Critic Algorithm (A3C) is used as the base RL algorithm in experiments involving Multi-Task Learning. The goal is to have a single agent perform well on all tasks in a given fixed MTI. The agent has access to all tasks in the multi-task instance and acts in a combined action space. The effectiveness of the Multi-Task Agent (MTA) is demonstrated on games from the Arcade Learning Environment. Our Multi-Task Agent (MTA) shows effectiveness across visually distinct games in the Arcade Learning Environment. Unlike other methods, our MTA does not require the identity of the task during training, instead inferring it from input features and task dynamics. Previous works define MTA performance using the arithmetic mean of normalized game-play scores, which we argue is not a robust evaluation metric. The Multi-Task Agent (MTA) performance is evaluated using different metrics such as p am, q am, q gm, and q hm. The q am metric is considered better as the MTA needs to perform well in all tasks to achieve a high score. The evaluation on q am is reported, while evaluations on other metrics are in Appendix E. The framework for Multi-Task Learning (MTL) is introduced, starting with a naive framework for on-line MTL and then presenting an extension to this framework. The Multi-Task Agent (MTA) performance is evaluated using metrics such as p am, q am, q gm, and q hm. The q am metric is crucial for overall performance. An on-line MTL algorithm is described, with the BA3C being a simple example. The algorithm involves training a single A3C network by interleaving observations from multiple tasks. Task decision steps are made periodically to determine the next task for training. The BA3C algorithm trains a single A3C network by interleaving observations from multiple tasks. The task decision steps occur at the end of each training episode, with the next task chosen randomly. The uniform probability distribution for task selection may impact the multi-tasking abilities of the trained MTA. The framework, A4C-Active sampling A3C, is inspired by active learning principles and aims to improve performance. The A4C-Active sampling A3C framework aims to improve multi-task learning by allowing the agent to actively sample tasks based on its current performance. Two approaches for the meta-learner are explored - treating the problem as a multi-arm bandit or a full RL problem. The Active Sampling multi-tasking agent 7:meta_decider selects tasks based on current performance estimates and target performance, improving task selection in multi-task learning without the need for single-task expert networks. Target performance can be based on published scores or human performance. The A5C framework, an extension of the A4C framework, involves task decision steps at the end of each training episode of the MTN. It does not learn the sampling distribution and computes an estimate instead. The UA4C method, an extension of the A5C framework, involves task decision steps at the end of every training episode for the MTN. It estimates the performance of the MTN on each task and uses a softmax probability distribution to make smarter decisions on where to allocate training resources next. The UA4C method extends the A5C framework by using a multi-armed bandit problem to select tasks for training the MTN. The reward for the meta-learner is defined based on the latest task trained on, allowing optimization for tasks the MTN struggles with. The Discounted-UCB algorithm BID8 is used, with a tunable hyperparameter \u03b2 controlling the importance of the bonus term. The EA4C method uses an LSTM-A3C-based controller to select tasks for training the MTN based on optimizing future rewards. It treats task selection as a full RL problem to learn a curriculum for training. The EA4C method utilizes an LSTM-A3C-based controller to optimize future rewards by selecting tasks for training the MTN. The reward function is defined based on task performance and a hyper-parameter, aiming to improve multitasking performance. The meta-learner's state is represented by a 3k length vector to learn the optimal task training policy. The meta-learner's state in the EA4C method is represented by a 3k length vector, including counts of task sampling, previous task identity, and sampling distribution. This state helps the meta-learner learn policies conditioned on task sampling counts for optimizing future rewards in multitasking performance. The A3C MTNs used in this study have the same size and architecture as single-task networks, with shared output layers for all tasks in a multitask setting. Experimental details are provided in Appendix B, including an argument against using different-output head MTAs. Previous works only focused on a single multitask setting, while this study presents results on seven different multitask settings. Hyperparameters were tuned on one multitask setting, and all MTNs were trained for a specific number of time steps based on the number of tasks in the setting. In this study, MTNs were trained on MTIs with varying numbers of tasks, ranging from 6 to 21. Hyperparameters were tuned on an MTI with 6 tasks, and experiments were conducted on seven different MTIs. The robustness of the framework was demonstrated by testing algorithms on a 21-task MTI, which is more than double the number of tasks in any previous work. Target scores were taken from a previous study, and it was shown that single-task expert networks are not necessary for achieving these scores. The study demonstrated the robustness of the proposed models in game-play experiments. A5C outperformed UA4C and EA4C, attributed to fewer hyperparameters to tune. Tuning all important hyperparameters for UA4C and EA4C resulted in slightly lower performance compared to A5C. The UA4C and EA4C agents were tuned for hyperparameters, but the tuning granularity may have affected performance. UA4C generalizes better than A5C on larger MTIs, while EA4C performs similarly to A5C and UA4C in multitasking instances. EA4C excels in the 21-task instance (MT7), showcasing superior generalization capabilities compared to other methods. The proposed methods demonstrate strong performance on large scale multitask instances like MT7, showcasing a second level of generalization in hyper-parameter settings. Target scores for different tasks are a key component of the framework, addressing concerns about access to trained single-task agents and training active-sampling based agents on new tasks. The proposed framework addresses concerns about access to trained single-task agents by using published resources for target scores. In cases where tasks have not been solved, a doubling of targets paradigm is suggested to estimate target scores, showing impressive performances. The doubling target paradigm maintains estimates for target scores in MTA tasks. When performance surpasses the estimated target, it is doubled. Hyper-parameters from UA4C on MT1 were retained, creating an unfavorable setup for DUA4C. Despite this, DUA4C's performance remains impressive, as shown in Figure 4 and TAB2. Additional metrics and training curves are detailed in Appendix K. The performance of DUA4C is impressive in an unfavorable setup. Tuning hyper-parameters could further enhance performance. The MTL framework A4C outperforms the baseline BA3C due to task-agnostic abstract features. The deep neural network's representational power allows for task-specific feature learning. Activation patterns of the LSTM controller are analyzed to identify task-agnostic neurons. The A4C agents overcome catastrophic forgetting by sampling tasks based on their probabilities, leading to improved performance on specific tasks and preventing degradation through network changes. The A4C agents prevent degradation through network changes by sampling tasks based on their probabilities, ensuring that good performance is maintained without the need for constant updates. Neurons fire for tasks based on a low threshold of 0.01 to detect rare events. A large fraction of neurons fire for multiple tasks, while some are task-specific. Neuron index versus firing frequency for each task is plotted in FIG4, showing task-specificity. The analysis for all MTIs is in Appendix H. The turnoff-analysis method is introduced to analyze multitasking agents without thresholds. By forcing the activations of neurons in LSTM output to 0, the change in performance on individual tasks is observed. The variance of column i in the tasks versus neuron matrix A gives a score for neuron i's task-specificity. Sorting the columns of A by variance and plotting a heatmap shows that A4C agents learn non task-specific abstract features aiding performance. The A4C agents learn task-agnostic abstract features, outperforming BA3C agents. A framework for training MTNs through active learning for online multi-task learning is proposed. The method allows MTA to focus on tasks where it performs poorly. While not a definitive solution, it is a crucial first step in online multi-task reinforcement learning. The method complements the turnoff-analysis method for analyzing multitasking agents. The EA4C method introduces task-agnostic abstract features for A4C agents, outperforming BA3C agents. Future work could explore regularizations to enforce task-agnostic representations. Training on multi-task instances involved 300 million steps across tasks MT1, MT2, and MT3. The FA4C method introduces Fine-grained meta-learner Activesampling A3C to address limitations in EA4C. Task decision steps occur every N steps during training, allowing for larger neural networks to be used. With 300 million training steps for the multi-tasking network, the meta-learner now requires 15 million training steps. The meta-learner now requires 15 million training steps, allowing for larger neural networks. LSTM cells store state for each task after 20 training steps. The reward function for FA4C evaluates the meta-learner's 20-step task selection policy based on performance. Target scores are inversely proportional to the desired performance during those steps. The target scores in FA4C are computed based on the performance over twenty time steps, different from other methods in the paper. The scores are obtained by summing the score of a trained single-task agent over twenty time steps and averaging it over the episode length. This design has potential flaws related to task rewards in specific state spaces. The target scores in FA4C are computed based on the performance over twenty time steps, different from other methods in the paper. Access to fine-grained target scores is challenging and requires training single-task networks. The overall reward function remains the same as EA4C, with a change in the definition of mi. The state function for the fine-grained meta-learner remains the same as the episodic meta-learner. Experimental results show FA4C outperforms random on some multi-tasking instances but not on others, highlighting the need for better design of meta controllers. The paper describes seven multi-tasking instances and common hyper-parameters for all methods (BA3C, A5C, UA4C, EA4C, FA4C) experimented with. Subsequent subsections detail hyper-parameter choices for A5C, UA4C, EA4C, FA4C. The paper describes seven multi-tasking instances, including six-task, eight-task, and twelve-task instances, to demonstrate the generalization capabilities of the methods. These instances allow for comparisons with other reported results and highlight the need for better meta controller design. The experiments involved using a neural network four times the size of a single-task network for demonstrating results on multiple task instances. The largest task instance, with 21 visually different tasks, used a network only twice the size of a single-task network. This instance is more than twice the size of any previously published work in multi-tasking. The network used was an LSTM version trained with the async-rms-prop algorithm. The neural network used in the experiments was trained with the async-rms-prop algorithm. Key hyperparameters included an initial learning rate of 10^-3, n-step returns set to 20, discount factor \u03b3 = 0.99, and entropy-regularization with \u03b2 = 0.02. Different \u03b2 values were tested for various active sampling methods, with the best value for BA3C found to be 0.01. Task instances (MT1, MT2, MT3) were trained for 300 million steps. The neural network was trained with async-rms-prop algorithm using key hyperparameters. Task instances were trained for different durations with varying numbers of parallel threads. Active sampling principles were used during training to improve multi-tasking performance. The neural network was trained using async-rms-prop algorithm with key hyperparameters. The multi-tasking network executed on each task for a set number of episodes, with evaluation done after specific training steps. The evaluation scheme used was similar to BID26, and the training progress was shown for MT1 in FIG3. The neural network architecture used in the study is similar to BID26, with convolutional and fully connected layers followed by an LSTM layer. The policy and value functions are derived from the LSTM outputs, with different output heads. The Actor and Critic share all layers except the final one. The final output layers for the policy and value functions have different non-linearities to model the multinomial distribution. Hyperparameters for the meta-task-decider are described for each method. The hyper-parameters for the meta-task-decider in the proposed methods include tuning the temperature parameter \u03c4 to 0.05, setting hyper-parameters n to 10 and l to 4 million. The discounted UCB1-tuned + algorithm from BID8 was used for implementation. For UA4C agents, hyper-parameter tuning was done for the discount factor \u03b3 (0.99) and scaling factor for the bonus \u03b2 (0.25). The meta-learner network was based on an A3C network with one meta-learner thread per multi-task learner thread. The meta-learner in the A3C network used 1-step returns and was trained with asyncrms-prop. Hyper-parameter tuning was done for entropy regularization (\u03b2 meta = 0) and discounting factor (\u03b3 meta = 0.8). The initial learning rate was tuned over a set of values. The meta-learner in the A3C network was optimized with an initial learning rate of 10^-3. LSTM versions outperformed feedforward versions in multi-tasking performance. Increasing network depth beyond 3 layers hurt performance, while wide networks did not perform as well as narrower ones. The number of neurons per layer was tuned between 50 to 500. The optimal architecture for the meta-learner network in the A3C network includes two fully-connected layers with 100 neurons each, followed by an LSTM layer with 100 LSTM cells, and one linear layer each for the policy and value function. Dropout layers were experimented with but did not improve performance. Hyper-parameters for FA4C were tuned the same way as EA4C, with task decisions in FA4C being time-driven. The training of the multi-tasking network in the FA4C method involved time-driven task decision steps at regular intervals, with n set to 20 for n-step returns. This choice allowed for easier implementation and meaningful estimates of cumulative returns without high variance. The FA4C meta-learner was trained for roughly 15 million steps, while the multi-tasking networks were trained for 300 million steps. Algorithm 1 provides pseudo-code for the active sampling method proposed in this work. The appendix contains specific instantiations of algorithms for all methods proposed in this work, including training the baseline MTA. Algorithm FORMULA7 is used for training steps, while Algorithm A5C is for multitasking across a set of tasks T. It involves target scores, episodes, training steps, and tracking the agent's performance on each task. The Active Sampling multi-tasking agent (amta) trains on multiple tasks using a non-parametric policy with a temperature hyper-parameter. It compares the performance of various agents across multiple tasks. The Active Sampling multi-tasking agent (amta) trains on multiple tasks using a non-parametric policy with a temperature hyper-parameter. It compares the performance of various agents across multiple tasks, including BA3C, A5C, UA4C, EA4C, and FA4C agents, along with task-specific A3C agents for MT2, MT3, and MT4. These agents were trained for 300-400 million time steps, requiring half the data and computation compared to training task-specific agents for all tasks. The experiments compared the performance of various agents for multi-tasking instances with different numbers of tasks (MT4, MT5, MT6) trained for 400-600 million time steps, requiring half the data and computation compared to training task-specific agents for all tasks. In a large-scale experiment with 21 tasks, a single network had to learn the prowess of visually different Atari tasks, doubling the number of parameters compared to previous multi-tasking instances. The performance of various agents, including task-specific A3C agents, was compared for MT7, trained for 1.05 billion time steps, requiring half the data and computation of training task-specific agents for all tasks. In a large-scale experiment with 21 tasks, a single network had to learn the prowess of visually different Atari tasks, doubling the number of parameters compared to previous multi-tasking instances. The performance of various agents, including task-specific A3C agents, was compared for MT7, trained for 1.05 billion time steps, requiring half the data and computation of training task-specific agents for all tasks. The performance of different methods on four performance metrics is documented in the appendix. The adaptive method is difficult to beat, while the UCB-based meta-learner struggles with larger instances but EA4C generalizes well to the largest multi-tasking instance MT7. In a large-scale experiment with 21 tasks, a single network had to learn visually different Atari tasks, doubling the number of parameters compared to previous instances. The performance of various agents, including task-specific A3C agents, was compared for MT7. The adaptive method is difficult to beat, while EA4C generalizes well to the largest multi-tasking instance MT7. The need for evaluation metrics is demonstrated in TAB6, showing that BA3C performs best in non-clipped average performance for MT4 but is not a good MTL algorithm due to training bias on a single task. Qhm metric is suggested as the best choice to understand multi-tasking performance. A5C performance was slightly better than EA4C for MT4. In a targeted experiment, human scores were used as target scores for the HUA4C agent in MT1, resulting in improved performance. The training curve for HUA4C with human scores as targets is shown in FIG0. The performance of HUA4C on all metrics is detailed in TAB9. The paper proposed a setup for UA4C on MT1 with hyper-parameters retained but not re-tuned, which may not be favorable. UA4C shows impressive performance but struggles with learning in two tasks and has mediocre performance in three others. Tuning hyper-parameters specific to the paradigm/framework could potentially improve performance. Experiments were conducted with twice the single-task performance as target scores to show the method's robustness. All experiments in this section were performed with A5C. In this sub-section, experiments were conducted with A5C using multi-tasking instances MT2 and MT3 to demonstrate the effect of using twice the target scores. The hyper-parameters were not tuned specifically for these experiments, but using twice-the-single-task-performance as target scores improved the performance of the agents, with significant improvements seen in some cases like MT3. Firing Analyses results for all MTIs were presented in this section. Neuron-Firing Analysis results for all MTIs show that active sampling methods have a high fraction of neurons firing across tasks, indicating successful feature learning. Training curves for DUA4C agent on MT1 demonstrate improved performance with half the data and computation compared to task-specific agents. Training curves for the DUA4C agent on MT2 and MT5 show improved performance with half the data and computation compared to task-specific agents (STA3C). Agents were trained for 300 million and 400 million time steps on MT2, and 600 million time steps on MT5."
}