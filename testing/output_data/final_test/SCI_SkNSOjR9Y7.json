{
    "title": "SkNSOjR9Y7",
    "content": "The paper proposes a method to train VAEs with binary or categorically valued latent representations using a differentiable estimator based on importance sampling. This approach addresses the challenge of training VAEs with discrete valued latent variables, for which conventional methods like reparametrized sampling are not feasible. The paper introduces a method to train VAEs with Bernoulli and Categorically distributed latent representations on benchmark datasets. The VAE is a generative model trained to approximate the data generating distribution of a random vector from a training set. It is used for tasks like density estimation, data generation, outlier detection, and clustering. The model involves latent variables and maximizes the probability of observing the data during training. The VAE is a generative model that approximates the data generating distribution of a random vector from a training set. It involves latent variables and maximizes the probability of observing the data during training. The model assigns a true but intractable posterior distribution to z, with a corresponding tractable variational approximation. The goal is to minimize the Kullback-Leibler divergence between the true posterior and the approximation, achieved through Stochastic Gradient Variational Bayes (SGVB) using parametric distributions parametrized by encoder and decoder networks. The VAE uses an encoder-decoder structure to model data likelihood with mean x. The variational posterior q(z|x) is an exponential family distribution with natural parameters \u03b7. The encoder network outputs \u03b7, while the decoder network outputs x. This allows for training with different q(z|x) within the same framework. The VAE proposed in BID5 learns continuous latent representations z \u2208 Rc using i.i.d. Gaussian distributed z. Learning discrete representations like binary or categorical z can be advantageous for tasks like hashing and clustering. However, training VAEs with discrete latent representations is challenging due to optimization issues with standard SGVB. Training VAEs with discrete latent representations is challenging due to optimization issues with standard SGVB. SGVB requires a good estimator for the expected log likelihood term that is unbiased, differentiable, and has low variance. The method places restrictions on the form of q(z|x) and fails if z is discrete. In this paper, a simple and differentiable estimator based on importance sampling is proposed for training VAEs with binary or categorical latent representations. The estimator is unbiased and has low variance, making it advantageous compared to existing methods like VQ-VAE. It can be used with SGVB under certain conditions, providing a solution to the challenges of optimizing VAEs with discrete latent representations. The derivative of Eq. 5 must exist for reparametrization in VAE training. A new estimator based on importance sampling is proposed for training VAEs with binary or categorical latent representations, allowing direct backpropagation of gradients. This method overcomes limitations of reparametrization with discrete latent representations. The estimator for training VAEs with binary or categorical latent representations allows direct backpropagation of gradients, overcoming limitations of reparametrization with discrete latent representations. The estimator computes a weighted sum of the log likelihood ln p(x|z m ) with the weighting q(z m |x)/q I (z), where z m \u223c q I (z) are samples from the distribution. The log likelihood depends on the decoder parameters \u03b8 D only, while the weighting depends on \u03b8 D and not on \u03b8 E. The estimator for training VAEs with binary or categorical latent representations allows direct backpropagation of gradients. The variance of the estimator heavily depends on the choice of natural parameters. Choosing \u03be = \u03b7 leads to a simple gradient form. The estimator is unbiased and the variance reduces to zero as q approaches 0 or 1. This is desirable for maximizing the log likelihood during training. The only candidate points that maximize the log likelihood are q = 0 or q = 1 near q(z = 1|x) = 0/1. Training longer improves gradient accuracy. For Categorically distributed z, variational posterior distribution q(z|x) and q I (z) have the form DISPLAYFORM0 with z \u2208 {e 1 , ..., e c }. The expected gradient of the log likelihood is now in an easy form with q = softmax(\u03b7). Preliminary experiments on MNIST and Fashion MNIST datasets have been conducted with two types of VAEs. The BVAE and CVAE architectures are evaluated using different distributions for z. Training is done using the estimator LIL(\u03b8) and two different architectures. The models are trained on the MNIST dataset for 300 epochs using SGVB and ADAM optimizer. The convergence of loss, log likelihood, and variance of the estimator is shown in FIG6. The VAEs assign ln p(x|z) to training data with decreasing variance of the estimatorL I L (\u03b8) as training progresses. Results from simulations with CNN BVAE and CNN CVAE are shown in Fig. 9. FC CVAE performs worse than FC BVAE due to lower information content in latent variables. FC CVAE can generate a maximum of 100 different handwritten digits. The FC CVAE can generate a maximum of 100 different handwritten digits, which is a small number compared to the 250 different images that the model can learn to generate. The generated digits are blurry and different from the input, with some even having the class flipped due to limited model capacity. Similar results are seen with the CNN BVAE and CNN CVAE. The FC CVAE can generate up to 100 different handwritten digits, with some images resembling mixtures of different digits. The decoder learns to generate template images that fit well to all training images, but some latent representations decode to meaningless patterns. The categorical latent representation can be interpreted as cluster affiliation. The FC CVAE's latent space can only encode limited information, limiting its generative capabilities. A hybrid latent space with continuous variables could enhance nonlinear clustering. An easy ELBO estimator was derived in this paper, allowing for differentiable estimates without reparametrized sampling. Theoretical and experimental results support the effectiveness of this approach. The FC BVAE can accurately reconstruct clothing shapes but loses texture details due to limited model capacity. The estimator's variance approaches zero near optimal parameters, making it ideal for training."
}