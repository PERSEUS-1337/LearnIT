{
    "title": "BkeC_J-R-",
    "content": "Reinforcement learning methods have shown success in control problems but require extensive training data for complex inputs. Supervised learning can learn from few samples but lacks reward-based control policies. A model-free control method combining reinforcement and supervised learning is proposed for autonomous control in real-world environments. The proposed approach combines reinforcement and supervised learning for autonomous control in real-world environments, demonstrating superior performance with fewer samples compared to state-of-the-art techniques. It is applicable to MuJoCo control problems and can handle dynamic scenarios involving speed or acceleration. The proposed approach combines reinforcement and supervised learning for autonomous control in real-world environments, demonstrating superior performance with fewer samples. It can handle dynamic scenarios involving speed or acceleration by providing parallel signals of the autopilot to restrict reinforcement learning solutions towards sensible control actions. This method can be applied to robotic control using analytical or machine learning models trained on alternative sensors. The problem statement involves injecting supervised data into reinforcement learning using regularisation of Q-functions, which is different from previous works. The approach presented in this paper combines reinforcement and supervised learning for autonomous control, reducing the number of samples needed for training from millions to hundreds of thousands. It includes a novel greedy actor-critic reinforcement learning algorithm and shares the replay buffer between the two types of data. The method aims to improve the quality of training against supervised and reinforcement learning, with a focus on dynamic scenarios involving speed or acceleration in real-world environments. The method combines supervised and reinforcement learning for autonomous control, aiming to reduce training samples needed. It involves initial supervised pre-training using annotated dataset examples to mimic existing control methods. Fine-tuning is done with both explicit labels and rewards using a pretrained model to avoid nonsense control behavior. Access to labels helps divert the reinforcement learning model from exploring non-meaningful input-control signal combinations. The problem statement involves defining an operator for control signals based on subsequences of input signals. The goal is to generate meaningful control signals given a set of input sequences and corresponding signal sequences produced by a reference method. The reinforcement learning method, inspired by DDPG algorithm, is reworked for a combination of supervised and reinforcement learning in real time. The model, referred to as an agent, generates control signals as actions based on observations. The initial state x1 is received, followed by action c1. The agent generates control signals based on observations, transitioning between states x1 and x2 through actions c1. Rewards are received for each action, determined by a policy mapping states to actions. The Bellman equation is used to calculate discounted future rewards, approximated by a critic neural network in an actor-critic approach. The proposed method uses a neural network in line with the actor-critic approach and utilizes a replay buffer for training. Unlike many state-of-the-art methods, the optimization is carried out in a greedy way, interlacing steps of testing the current policy, updating the critic, and the actor. The method involves updating the critic and actor to minimize the difference between testing and training scenarios, ensuring reasonable actor performance with few epochs. Regularization is used to prevent performance deterioration in the pretrained model, bringing parameters closer to a reference policy. The control signal is bounded between values (t 1 , t 2 ). The training sample for the critic is augmented with values Q(x, t 1 ) = Q(x, t 2 ) = 0 for every value x. The optimisation problem is based on the regularised Q-function f (x, \u03c0(x|\u0398 \u03c0 )) where \u03b1 \u2265 0 is a coefficient, and the weight Q(x,\u03c0(x)) encourages the actor to follow the reference policy if the expected reward is high. The update of the critic involves solving an optimization problem using values from the replay buffer. The training procedure follows Algorithm 1 with a maximum number of epochs. The proposed model separates filling the replay buffer, training critic, and actor into three steps, making the algorithm greedy. The 0-th epoch's testing episodes assess the model's performance with supervised pretraining. The study assessed model performance during pretraining using TORCS/SpeedDreams driving simulator with Simplix bot as a reference actor. The bot's steering angles were limited, and a recovery procedure was provided if the car got stuck. Rewards were based on car speed, and assessment was done on a single computer with NVIDIA GeForce GTX 980 Ti GPU. The study was conducted on a single computer with a NVIDIA GeForce GTX 980 Ti GPU running on Ubuntu Linux. Model parameters are detailed in Appendix A. The actor's parameters were initialized and trained on a supervised dataset. The critic's parameters were also initialized. Testing episodes were performed with the current parameters, and gradient descent updates were made for both the critic and actor. The network architecture for the actor-critic approach includes fine-tuning the InceptionV1 network on a dataset from the SpeedDreams simulator. The last layer is replaced with a fully connected layer for feature extraction, avoiding the need to store images in the replay buffer. The actor-critic approach fine-tunes the InceptionV1 network on a dataset from the SpeedDreams simulator. The network architecture includes ReLU nonlinearity in most layers, with the actor's last layer using tanh to restrict output between -1 and 1. A loss function is used during supervised finetuning and pretraining stages to address control signal variations. The actor-critic approach fine-tunes the InceptionV1 network on a dataset from the SpeedDreams simulator. A distance-like function \u03c1 is defined with a small constant. A siamese network architecture is used to calculate features for the current and previous frame, taking into account the dynamic environment. The previous control signal is input for the critic. Results comparison is shown for different values of parameter \u03b1. The scatter points in Figure 3 represent total rewards during testing episodes, with curves showing mean total rewards per epoch. The performance of the pretrained model is compared to different values of parameter \u03b1, with smaller \u03b1 values leaning towards supervised active learning. The shaded area in the graph indicates the standard deviation of testing episode performance. The coefficient \u03b1 values affect performance, with smaller \u03b1 leading to longer convergence time and potential overfitting. Using \u03b1 = 0 for supervised data during pre-training results in lower performance. Total rewards for different \u03b1 values are shown in TAB1. The maximum mean total reward (max R) is calculated for each epoch over all testing episodes for a given parameter \u03b1. The rewards of the proposed algorithm are compared to the Simplix bot in the same conditions, with the mean reward of the bot being R bot = 81214.07 and the maximum reward achieved by the bot being max R bot = 81931.15. The percentage of the proposed algorithm's rewards compared to the bot's is also provided. The proposed algorithm requires fewer measurements compared to standard reinforcement learning techniques. The proposed method shows significant improvement in performance compared to traditional reinforcement learning techniques, achieving up to 92.63% of the bot's performance. It also demonstrates a dramatic reduction in the number of samples needed for video data, down to just several hundred thousand. The proposed method combines reinforcement and supervised learning to improve performance in complex spaces, such as for autonomous cars or robots. However, it still requires label data throughout training, but future work aims to reduce the need for labeled episodes. The parameterization for the experiments is provided in TAB2. The proposed algorithm combines reinforcement and supervised learning for pretraining the actor network using Momentum and Adam algorithms. It is implemented in Python with TensorFlow. Experiments in MuJoCo environment show the effectiveness of regularizing Q-values with supervised learning. The Q-function regularization is added to the standard DDPG algorithm from OpenAI Baselines. The experiments compared three cases: original DDPG algorithm, DDPG with fixed regularization coefficient, and DDPG with exponential decay. Results showed that the model with fixed regularization coefficient reached the pretrained value easily but then lagged behind. The experiments compared three cases: original DDPG algorithm, DDPG with fixed regularization coefficient, and DDPG with exponential decay. The fixed regularization coefficient can easily reach the pretrained value but then lags behind the algorithm with exponential decay. The exponential decay algorithm takes advantage of the reference actor performance and gradual decay enables exploration further from the reference actor. In certain cases, regularisation may prevent further exploration beyond the reference actor performance. The performance of the original DDPG algorithm peaks beyond the reference actor baseline near step 270000 in the Hopper scenario, suggesting instability for this task. The model with fixed regularization coefficient can reach performance beyond the reference actor. The experiments compared three cases: original DDPG algorithm, DDPG with fixed regularization coefficient, and DDPG with exponential decay. Convergence in the InvertedDoublePendulum task depends on the initial value of parameter \u03b1. The version with exponential decay and larger initial \u03b1 value shows better results due to heavier reliance on the supervised part. All versions maintain stable performance after initial training. In the Swimmer scenario, the exponential \u03b1 setting allows performance beyond the reference actor baseline, while \u03b1 = 0.1 shows smaller variance and better average performance than the original DDPG method."
}