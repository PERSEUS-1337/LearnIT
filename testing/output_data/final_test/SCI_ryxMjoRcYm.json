{
    "title": "ryxMjoRcYm",
    "content": "This paper presents a method for training Q-function efficiently for continuous-state Markov Decision Processes (MDP) to ensure resulting policies satisfy a Linear Temporal Logic (LTL) property. LTL can express various time-dependent logical properties like safety and liveness. The LTL property is converted into a deterministic Buchi automaton, and a synchronized product MDP is created. The control policy is synthesized using reinforcement learning without prior MDP knowledge. The method is evaluated numerically and compared to traditional policy synthesis methods like MDP abstraction and approximate dynamic programming. Reinforcement Learning (RL) is used to train an agent in sequential decision-making problems with unknown stochastic behavior. RL is typically focused on finite state and action problems, but real-world tasks often require actions based on high-dimensional inputs. One solution is to use an approximation function through regression to estimate values at different states. In continuous-state RL, neural networks are used to approximate the expected reward, replacing conventional state-action-reward tables. Various methods like CMACs, kernel-based modeling, and tree-based regression are available for reward approximation. Feedforward networks, such as multi-layer perceptrons, are proposed in this paper to synthesize a control policy for infinite-state MDPs. The paper proposes using LTL to specify complex control objectives for infinite-state MDPs in RL algorithms, enabling the generation of policies that satisfy these objectives efficiently. No previous research has focused on enabling RL to generate policies for continuous-state MDPs. No research has been done to enable RL to generate policies according to full LTL properties. Control synthesis in finite-state MDPs for temporal logic has been explored in various works. Different approaches involve converting LTL properties to Deterministic Rabin Automaton (DRA) and using Dynamic Programming algorithms to maximize the probability of satisfying the specification. Some methods assume unknown transition probabilities and aim to calculate the value function for each state within an error bound of the actual state value. The PAC MDP is generated via an RL-like algorithm and scLTL is proposed for mission specification. BID0 employs the concept of shielding to synthesize a reactive system ensuring agent safety. Unlike BID0, our focus is on full LTL expressivity in safe RL approaches. The tuple M = (S, A, s 0 , P, AP, L) defines a continuous-state space MDP in safe RL, where S is a set of states, A is a set of actions, s 0 is the initial state, P is a transition kernel, AP is a set of atomic propositions, and L is a labelling function. A finite-state MDP is a special case with a finite number of states and a transition probability function. The transition function P induces a transition probability matrix in MDP. An MDP is considered solved when an optimal policy is discovered to maximize expected reward. The agent must take actions leading to the highest expected reward, with the objective of finding an optimal policy on its own. In Section A.2, this paper introduces approaches for solving infinite-state MDPs using Linear Temporal Logic (LTL) to specify desirable constraints on agent policies. LTL formulas express properties like safety and persistence, extending propositional logic with temporal modalities. Linear Temporal Logic (LTL) extends propositional logic with temporal modalities until, eventually, and always. These modalities can be combined to create complex task specifications, such as in robot control problems. LTL formulas can be expressed using automata, such as Limit Deterministic B\u00fcchi Automatons (LDBA), which are succinct and simple for this purpose. A Generalized B\u00fcchi Automaton (GBA) is a structure with states, initial states, alphabet, accepting conditions, and transition relation. An LDBA is a GBA with initial and accepting partitions. An algorithm based on Neural Fitted Q-iteration (NFQ) is proposed for LDBAs. LogicallyConstrained NFQ (LCNFQ) algorithm is proposed to synthesize a policy satisfying a temporal logic property by synchronizing MDP and automaton structures. The transition kernel P \u2297 defines new states based on current state-action pairs, adding an extra dimension to the state space to track automaton states and evaluate LTL property satisfaction. The absorbing set in the automaton evaluates the satisfaction of the LTL property. It is accepting if it includes F \u2297 and once a trace enters it, it cannot escape. The product MDP combines characteristics of the original MDP and B\u00fcchi automaton, allowing for the synthesis of a policy that respects both the MDP and LTL property. In this paper, a random variable reward function is proposed for the original MDP and LTL property. The function assigns scalar values based on the current state, action, and subsequent state. A switch parameter is used to control the effect of randomness in the reward function, specifically designed for LCNFQ neural nets. The LTL formula is converted to an LDBA N to create a product MDP M N for use in experience replay. In this paper, a random variable reward function is proposed for the original MDP and LTL property. A switch parameter controls the randomness effect in the reward function for LCNFQ neural nets. The LTL formula is converted to an LDBA N to form a product MDP M N. The agent explores the MDP using experience replay, reinitializing it upon receiving positive rewards or after a set number of iterations. All experiences are stored as episode traces in a sample set E. Once exploration is complete, the learning phase begins with n separate multi-layer perceptrons associated with states in the automaton N. The neural nets associated with states in the LDBA approximate the Q-function in the product MDP. Each automaton state q i has an associated neural net B qi for local Q-function approximation. The set of neural nets acts as a global hybrid Q-function approximator. The weights of B qi are updated to minimize error when transitioning between states. Experiences associated with a state q i are stored in E qi. The Algorithm 1: LCNFQ approximates the Q-function in the product MDP using neural nets associated with states in the LDBA. Each automaton state q i has a neural net B qi for local Q-function approximation, with experiences stored in E qi. The weights of B qi are updated using Rprop to minimize error during state transitions. Training starts from accepting states and goes backward until reaching the networks associated with the. LCNFQ approximates the Q-function in the product MDP using neural nets associated with states in the LDBA. The algorithm stops when the policy satisfies the LTL property and no longer improves. Different embeddings were tried but poor performance was observed, leading to the use of n separate neural nets to approximate the global Q-function. The reward function only returns a positive value when the agent transitions to an accepting state in the product MDP. The agent can derive a policy that leads to accepting states by following a reward function. The optimal policy has the highest expected probability of satisfying the LTL property. A Voronoi quantizer is proposed for state space discretization in the product MDP. The agent's view is initialized with just one state. The agent initializes its view with just one state and explores the state space using a Voronoi quantizer. The algorithm involves resets where the agent returns to its initial state, known as episodic VQ. The size of the state space representation grows as the agent explores and appends new states to its collection. The algorithm proposed in this section, called episodic VQ, is a modified version of FVI that can handle the product MDP. The global value function consists of sub-value functions for each state, and the Bellman update for each sub-value function is defined. The update does not have a running reward and the reward is embedded via value function initialization. The algorithm episodic VQ is a modified version of FVI for product MDPs. The value function is initialized with a rule, and the Bellman operator is executed using an approximation method due to the lack of analytical representation of the value function in continuous state MDPs. The operator L constructs an approximation of the value function and sub-value functions using a set of points called centers distributed uniformly over S. A kernel-based approximator is employed for the FVI algorithm, with the kernel averager method representing the approximate value function using a radial basis function. The operator L approximates the value function using a set of points called centers distributed uniformly over S. A Monte Carlo sampling technique is used to approximate the integral in Bellman update. The approximate value function Lv is initialized and updated in each cycle of FVI. The automaton uses LCNFQ to plan missions for an autonomous Mars-rover, starting with image analysis and adding labels for safety. Assumptions are made about potential disturbances, with a focus on conservative estimates. The next step involves expressing the desired mission in LTL format and running LCNFQ on the labeled image before sending the rover to Mars. LCNFQ is trained to guide the rover on the Mars surface, outperforming other methods like Voronoi quantizer and FVI. The area of interest on Mars is the Coprates quadrangle, known for signs of water and ancient river valleys. Valles Marineris, a canyon system in Coprates quadrangle, is considered for the mission. The blue dots in Valles Marineris indicate recurring slope lineae (RSL), seasonal dark streaks suggesting liquid water on Mars. Melas Chasma and Coprates Chasma have the highest RSL density. The rover's actions in the MDP state space S include left, right, up, down movements. The rover's movements in the MDP state space S are randomly drawn within a range unless it hits the boundary, where it stays. The labelling function divides the area into neutral, unsafe, and target regions based on elevation, with blue dots indicating the target label (RSL) and red for unsafe areas. The rover's mission involves exploring areas with varying elevations, divided into neutral, unsafe, and target regions. The target regions are labeled as RSL, with blue dots representing lower risk areas and red dots indicating higher risk areas. The rover must visit target 1 and then target 2, following a specific path outlined in the LTL formula. The rover's mission involves exploring target regions while avoiding unsafe areas. The LTL formulas guide the rover's path, ensuring it reaches the targets and stays there, while also highlighting the risk of entering unsafe areas. B\u00fcchi automata are used to represent the formulas for navigation. The simulation results of using B\u00fcchi automata for LTL formulas in a rover's path planning are presented. LCNFQ is run on product MDPs with neural networks associated with automaton states. Results show learning for specific LTL formulas and the generation of a control policy for the rover. The episodic VQ algorithm is explored as an alternative to LCNFQ for rover path planning. Different resolutions are tested, with coarser resolutions leading to inefficient reward propagation and potential agent looping. Training time and travel distance are key metrics, with a generated policy shown for a resolution of 1.2 km. The VQ algorithm is used for rover path planning, with different resolutions tested. The Voronoi discretisation is shown after implementing VQ, accurately partitioning relevant parts of the state space. Results of the FVI method for LTL formulas are presented, with empirically adjusted parameters for generating satisfying policies efficiently. In the context of rover path planning using the VQ algorithm and FVI method for LTL formulas, LCNFQ is highlighted as a method that efficiently generates a better policy compared to FVI and VQ. It does not require external intervention and does not need careful hyper-parameter tuning like FVI and VQ. LCNFQ outperforms FVI and VQ in generating efficient policies with less sample complexity, higher reliability, and better expected rewards. It trains Q-function in a continuous-state MDP to satisfy a logical property using hybrid modes. Tested successfully in a numerical example, LCNFQ switches between neural nets automatically for improved performance. In an MDP, a policy maps states to actions with probabilities. A deterministic policy outputs a single action per state. The immediate reward function R(s, a) gives the agent a reward for taking action a in state s. The expected discounted reward under a policy is calculated using a discount factor \u03b3. An optimal policy is one that maximizes the expected discounted reward over all possible policies. The set D consists of stationary deterministic policies over state space S. To solve an infinite-state MDP with RL, discretizing the state space is a common approach. However, this method may not capture the full dynamics of the original MDP accurately due to the trade-off between accuracy and the curse of dimensionality. Q-learning (QL) is a widely used RL algorithm for finding the optimal policy in a finite-state MDP by assigning quantitative values to state-action pairs. QL assigns quantitative values to state-action pairs in a finite-state MDP. The Q-function is updated based on rewards received, converging to a unique limit. The optimal policy is generated by selecting the action with the highest Q-value. Neural Fitted Q-iteration (NFQ) uses neural networks to approximate Q-values in continuous state spaces, as storing Q-values for every state-action pair is impractical. NFQ is a core component of Google's Deep algorithm. Neural Fitted Q-iteration (NFQ) is the core of Google's Deep Reinforcement Learning algorithm BID19. The update rule involves introducing a loss function to measure the error between current and new Q-values, allowing for adjustment of neural network weights to minimize error. In continuous state-space cases, the neural net weights are updated when a new state-action pair is visited, requiring numerous trainings to find an optimal policy due to changes in the Q-function approximation. The core idea of Neural Fitted Q-iteration (NFQ) involves using experience replay to prevent uncontrollable changes in the Q-function approximation caused by adjusting network weights for specific state-action pairs. This technique ensures that previous samples are reintroduced when updating neural network weights to minimize errors and accurately approximate the Q-function. Neural Fitted Q-iteration (NFQ) stores previous experiences for reuse in updating the neural Q-function, making it an offline algorithm. NFQ benefits from generalization in approximation, requiring less experience and being data efficient. Traditional RL algorithms like QL are limited to finite state spaces and not directly applicable to continuous state-space MDPs. Nearest neighbor vector quantization is a method for discretizing the state space into a set of disjoint regions. The Voronoi Quantizer maps the state space onto a finite set of disjoint regions called Voronoi cells. Designing a nearest neighbor quantizer allows for autonomy in solving infinite-state MDPs without manual discretization. The set of centroids C is crucial for designing a nearest neighbor vector quantizer to approximate the optimal policy for a continuous-state space MDP. Fitted Value Iteration (FVI) is introduced for continuous-state numerical dynamic programming using a function approximator. In continuous state spaces, an approximation of the value function can be obtained numerically through approximate value iteration. Approximate value iteration can be used to obtain an approximation numerically through the Bellman operator T on an initial value function. FVI is discussed in the paper and has been proven to be stable and converging with a non-expansive approximation operator."
}