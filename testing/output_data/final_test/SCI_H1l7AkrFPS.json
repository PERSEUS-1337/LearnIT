{
    "title": "H1l7AkrFPS",
    "content": "In this paper, the authors investigate the reliance on spatial information in image classification. They propose three methods to destroy spatial information during training and testing phases and evaluate them on various object recognition datasets with different CNN architectures. The results show that spatial information can be deleted from many layers with minimal performance drops. Despite the success of CNNs in computer vision tasks, their inner workings remain largely unknown. Despite the success of CNNs in computer vision tasks, their inner workings remain mostly unknown. Modern CNNs for image classification learn spatial information across all convolutional layers, based on the assumption that spatial information is important at every layer. Recent work has shown that it is possible to achieve competitive performance with networks that have shallow depth. Recent research has questioned the necessity of spatial information in standard CNNs, proposing methods to eliminate it at multiple layers. Surprisingly, modified CNNs without access to spatial information can still achieve competitive results on object recognition datasets, suggesting that spatial information may be overrated for standard CNNs. Our proposed simplifications for CNNs, such as GAP+FC or 1x1Conv layers, can replace last layers of standard CNNs to create smaller models with fewer parameters while maintaining high performance on image classification datasets. Additionally, shuffle conv randomly shuffles feature maps before convolution, enhancing spatial information aggregation in decision-making. Recent research has focused on generalizing the spatial sampling of convolutional kernels in CNNs to allow for globally spread out sampling. While improvements have been made in classification metrics, there is debate on the importance of spatial information for common CNNs. For example, studies have shown that certain architectures like VGG-16 trained on ImageNet are largely invariant to scrambled images. This raises questions about the extent to which architectural changes alone contribute to performance improvements. The curr_chunk discusses the construction of a modified ResNet architecture with a limited receptive field and its competitive results on ImageNet. It also mentions studies indicating that models trained solely on ImageNet do not learn shape sensitive representations, emphasizing the investigation of the necessity of spatial information in CNNs. The curr_chunk focuses on removing spatial information in intermediate layers of CNNs to understand how they process spatial information. Three approaches - shuffle convolution, GAP+FC, and 1x1Conv - are proposed to evaluate the importance of spatial information in well-established architectures. The evaluation compares classification accuracy for models with varying levels of spatial information propagation. Section 3.1 details the approaches, while section 3.2 discusses the experimental setup. Shuffle Convolution extends ordinary convolution by randomly shuffling input feature maps to prevent spatial information encoding. This non-differentiable process is repeated for each feature map before feeding into a regular convolutional layer. Shuffle Convolution extends ordinary convolution by randomly shuffling input feature maps to prevent spatial information encoding. Gradients can still be propagated through the shuffled output in the same way as Max Pooling, allowing for end-to-end training. Images within the same batch are shuffled in the same way for simplicity, and all layers from the last to a specific depth are shuffled to prevent memorization of encountered permutations by the model. Global Average Pooling and Fully Connected Layers, known as GAP+FC, offer a deterministic alternative to Shuffle Convolution for preserving spatial information in CNNs. By deploying Global Average Pooling after an intermediate layer and replacing subsequent layers with fully connected layers, GAP+FC efficiently avoids learning spatial information at intermediate layers by shrinking feature maps to a size of 1. This approach collapses spatial information to a single point, as demonstrated in Fig. 1. The use of 1x1 convolutions in CNNs, known as GAP+FC, collapses spatial information to a size of 1, potentially affecting model expressiveness. This method retains spatial information while being invariant to spatial relationships, replacing 3x3 convolutions in the last layers of a network. In modifying the last 2 layers of a toy CNN, ResNets' stride-two convolution for downsampling bottleneck feature maps is replaced with max or average pooling using 2x2 windows. Different architectures like shuffle conv, GAP+FC, and 1x1Conv are tested on CIFAR100, Small-ImageNet-32x32, and ImageNet datasets for accuracy and model parameters. The modification starts from the last layer and extends consecutively to the first layer, denoted as K for the number of modified convolutional layers. The modification of convolutional layers in VGG-16 and ResNet-50 architectures involves changing the 3x3 convolutions inside bottleneck sub-modules. K varies from 0 to 13 in VGG-16 and one bottleneck is considered as one layer in ResNet-50. The rest of the operations remain the same, with 2x2 average pooling used for down-sampling in all experiments. The ResNet-50 architecture is modified for CIFAR100 and Small-ImageNet-32x32 experiments by adjusting the first convolution to 3x3 with stride 1 and removing the first max pooling layer. Models are trained with the same setup and initialized with the same random seed. Different random seeds are used during testing. The study focuses on observations for VGG-16 and ResNet-50 on CIFAR100, then verifies them on other datasets and architectures. The influence of depth and receptive field size is discussed in detail. In this section, the study investigates the invariance of pre-trained models to the absence of spatial information at test time by applying Shuffle Conv to the last 3 convolutional layers of a pre-trained VGG-16 on CIFAR100. The final test accuracy is the average of 200 evaluations with the standard deviation presented. The study explores the impact of Shuffle Conv on model performance on CIFAR100. Results show that incorporating Shuffle Conv during training improves baseline performance regardless of test-time shuffling. Modifications are made to VGG-16 and ResNet-50 by removing spatial information in different ways. The study evaluates the impact of Shuffle Conv on model performance on CIFAR100 by training K on the training set and evaluating on the hold-out validation set. Results show that 1x1Conv gives higher test accuracy with fewer parameters. The results for VGG-16 and ResNet-50 on CIFAR100 are shown in Fig. 2, with the x-axis representing the number of modified layers K. The performance of ResNet-50 and VGG-16 models is evaluated with different methods of layer modification. Shuffle conv, 1x1Conv, and GAP+FC can tolerate modification of the last 5 layers without losing accuracy. This challenges the belief that spatial information is crucial for object recognition tasks. Additionally, 1x1Conv and GAP+FC can reduce model parameters without impacting performance. The study evaluates the impact of layer modification methods like 1x1Conv and GAP+FC on ResNet-50 and VGG-16 models. These methods reduce model parameters without affecting performance and can improve generalization when data augmentation is not used. While models with data augmentation show similar test accuracy, significant performance improvements are seen in models without data augmentation. For example, 1x1Conv outperforms the baseline by 8.01% on CIFAR100. However, this effect is not consistent across other architectures and datasets. The study evaluates the impact of layer modification methods like 1x1Conv and GAP+FC on ResNet-50 and VGG-16 models. Results show that modified models reach higher test accuracy when data augmentation is not applied, with significant performance improvements seen in models without data augmentation. For example, ResNet-50 with 1x1Conv trained without data augmentation shows a performance improvement from 65.64% to 73.65% on CIFAR100. The study suggests that spatial information can be neglected in last layers without affecting performance if invariance is imposed during training. It is beneficial to have different designs for different layers to reduce model complexity. Comparing methods, 1x1Conv is found to be more robust. Comparing three methods, 1x1Conv is more robust to the absence of spatial information. CNNs can benefit from larger activation maps even without spatial information. Experiments on Small-ImageNet show similar behavior to CIFAR100 dataset. The performance gap between GAP+FC and 1x1Conv increases on ResNet50. Spatial information at last layers of CNNs may not be necessary for good performance on complex datasets. Experiments on full ImageNet with different architectures show that 1x1Conv outperforms GAP+FC due to its computational efficiency. Models with high capacity can learn relevant representations in earlier layers, suggesting that spatial information at last layers can be ignored. In experiments on architectures designed for minimal complexity, MobileNetV2 and SqueezeNet with few parameters achieve competitive performance on ImageNet. MobileNetV2 uses an inverted residual bottleneck with a specific building block, while our modification removes a certain convolution layer for efficiency. In our modification, we remove the 3 \u00d7 3 depthwise convolution in SqueezeNet and replace all 3 \u00d7 3 convolutions in the expand phase with 1 \u00d7 1 convolutions. The experiments show that neglecting spatial information at the last layers of MobileNetV2 and SqueezeNet does not affect performance, indicating that spatial information at the last layers is not necessary for good performance. The study explores the generalizability of reducing model parameters across different architectures like MobileNetV2 and SqueezeNet. It investigates the impact of neglecting spatial information in layers and evaluates the importance of layer depth in models like ResNet-50. The study evaluates the importance of layer depth in ResNet-50 models by modifying sub-modules with 1x1Conv layers. Results show that even though spatial information at last layers is not necessary, they are essential for good performance. Additionally, the test accuracy saturates with the increase of receptive field size for a given image size on VGG-16 over CIFAR100. The spatial information is marginalized out at some depth in the network, possibly related to the receptive field size of a layer. Ablating VGG-16 by replacing 3x3 convolutional layers with 1x1 convolutional layers shows the importance of spatial information for good performance. The study replaced 3x3 convolutional layers in VGG-16 with 1x1 convolutional layers to vary the receptive field size, showing that test accuracy saturates with increasing receptive field size. The minimal required receptive field size exceeds the actual image size by a large margin, reaching around 50 for 32x32 input images and 120 for 64x64 input images to achieve similar accuracy as a vanilla VGG-16 model. The study showed that using 1x1 convolution or fully connected layers at the last layers of CNN architectures can reduce parameters without affecting performance. This approach achieved competitive results on various object recognition datasets, indicating that spatial information may not be crucial for object recognition tasks. Using 1x1 convolution or fully connected layers at the last layers reduces parameters without performance impact. Future research could explore generalizing these methods to other computer vision tasks like object detection and pose estimation. Convolution with stride 2 as a replacement for pooling layers may lead to a more rapid decrease in test accuracy compared to average pooling and max pooling on ResNet50 over Small-ImageNet. The failure of the stride 2 case in using 1x1 convolution for down-sampling may be due to the loss of spatial information. Unlike average pooling or max pooling, 1x1 convolution ignores a significant portion of activations, impacting performance. The skip connection branch also suffers from information loss with 1x1 convolution. Testing on VGG-16 and ResNet-50 shows that average pooling and max pooling perform similarly, highlighting the importance of spatial information. Experimental setup is the same as the CIFAR100 experiment. Results in Fig. 5 show that GAP+FC and 1x1Conv can replace the last layers in VGG-16 with minimal test accuracy difference. The test accuracy can be preserved until K = 3 and K = 6 for GAP+FC and 1x1Conv, confirming the better performance of 1x1Conv over GAP+FC. Random shuffle conv at different depths in VGG-16 on CIFAR100 is tested, showing the impact of shuffle conv at one layer at a time. The test accuracy of VGG-16 models with shuffle conv modifications at different depths is compared to the baseline performance. Random shuffle has a larger impact on the first layers compared to the last layers, with the baseline performance maintained for the last 4 layers. The test accuracy of specific models with modified last 3 layers is presented in Table 2. The test accuracy of VGG-16 models with shuffle conv modifications at different depths is compared to the baseline performance. Results show that consistent training and test schemes give higher accuracy compared to inconsistent ones. The reason for this difference is not fully understood. Results on ImageNet show the test accuracy of ResNet-152 and ResNet-50 modified by 1x1Conv. Spatial information not necessary at last layers generalizes well on ImageNet. Channel shuffle experiments on VGG-16 on CIFAR100 show a faster drop in test accuracy compared to random spatial shuffle, indicating weaker model robustness against channel shuffle. The ResNet-50 architecture was modified by shuffle conv, GAP+FC, and 1x1Conv in the last 3 bottlenecks, highlighting the importance of feature map order in encoding information. The modification was applied to the 3x3 convolution within each bottleneck due to its spatial extent."
}