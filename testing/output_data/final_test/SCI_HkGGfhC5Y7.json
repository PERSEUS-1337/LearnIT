{
    "title": "HkGGfhC5Y7",
    "content": "Deep neural networks with discrete latent variables aim to improve symbolic reasoning and learn useful abstractions for new tasks. Despite recent interest and advancements, training these models remains challenging and their performance lags behind continuous counterparts. A new training technique for VQ-VAE, inspired by the EM algorithm, combined with sequence-level knowledge distillation, enables the development of a non-autoregressive machine translation model with accuracy close to a strong autoregressive model. A new non-autoregressive machine translation model achieves high accuracy comparable to a strong autoregressive model, while being significantly faster at inference. Unsupervised learning of meaningful representations is crucial in machine learning due to the high cost of labeled data. Continuous representations are commonly used in deep learning models for images, audio, and video. However, some datasets are better represented as sequences of discrete symbols, such as language and speech. Improved discrete latent variable models, like the Vector Quantized Variational Autoencoder (VQ-VAE), can lead to more interpretable representations and novel data compression algorithms. The VQ-VAE method utilizes a learned code-book and nearest neighbor search to train a discrete latent variable model. It samples latent codes from a prior and uses them in the decoder to generate data. The model achieves impressive results in image, speech, and video generation tasks, performing almost as well as continuous VAEs on datasets like CIFAR-10. Additionally, an extension of this method outperforms continuous autoencoders on the WMT English-German translation task. The Latent Transformer introduced a new state-of-the-art in non-autoregressive Neural Machine Translation for the WMT English-German task. Tuning the code-book size and utilizing EM algorithm improved results, achieving a BLEU score of 22.4, outperforming by 2.6 BLEU. Knowledge distillation provided significant gains, reaching 26.7 BLEU, almost matching autoregressive transformer models but 3.3\u00d7 faster. The contributions of the study include outperforming previous state-of-the-art models without product quantization, introducing a new training algorithm for discrete variational autoencoders inspired by the EM algorithm, developing a non-autoregressive machine translation model that matches a strong autoregressive baseline Transformer but is 3.3 times faster, and showing significant improvement in English-French translation using denoising discrete autoencoders. The curr_chunk discusses the VQ-VAE discrete autoencoder and its connection to classical EM algorithms. It models the joint distribution of data points and discrete latent variables. The encoder output is passed through a discretization bottleneck to select the discrete latent code for each position independently. The encoder output z e (x i ) is discretized using a nearest-neighbor lookup on embedding vectors e \u2208 R K\u00d7D . The selected latent variable's embedding is then passed to the decoder. The model is trained to minimize the reconstruction loss l r using a gradient-based loss function or maintaining an exponential moving average of encoder hidden states for training the embeddings. The encoder output is discretized using a nearest-neighbor lookup on embedding vectors. An exponential moving average is maintained over the embeddings and counts of encoder hidden states for stable training. Stochastic gradient is applied to code-book embeddings and cluster assignments. These techniques have been successful in minibatch K-means and online EM. The generative process for the latent variable NMT model involves autoregressively sampling discrete latent codes from a model conditioned on the input. The decoder uses these latent variables to generate the target sequence, with an autoregressive learned prior on the latent variables. The goal is to learn a shorter sequence of latents compared to the targets, speeding up decoding without loss in accuracy. The architecture of the encoder, decoder, and latent predictor model is detailed in Section 5. The hard EM algorithm BID3 solves an optimization problem by performing coordinate descent on model parameters \u0398 and hidden variables z 1 , . . . , z N. A special case is K-means clustering where Gaussians are modeled with identity covariance matrix. Equation FORMULA12 is optimized using coordinate descent until convergence. The EM algorithm is used for training VQ-VAE, with the optimization objective solved using coordinate descent. The E step assigns data points to a mixture of clusters, while the M step updates the means of the clusters. This new training strategy connects VQ-VAE updates to K-means clustering. The EM algorithm is utilized for training VQ-VAE, with the E step assigning data points to clusters and the M step updating cluster means. The encoder output corresponds to data points, and latent variables represent clusters. A probability distribution is generated in the E step, modeled as a Gaussian with identity covariance matrix. Monte-Carlo Expectation Maximization is used in the M step for efficiency. The E step in training VQ-VAE assigns data points to clusters using a K-way multinomial distribution. The M step updates model parameters to maximize a Monte-Carlo estimate. Instead of sending the posterior mode to the decoder, the average of sampled latent embeddings is sent. This allows all latent code embeddings to be updated in the backward pass for a single training example. To train the latent predictor model, a similar approach to label smoothing is used. Variational autoencoders were initially introduced for continuous representations but face challenges in training for discrete latent variable models. Various gradient estimators, such as REINFORCE, REBAR, and RELAX, have been explored to improve training. Another approach is to use continuous representations for gradient estimators. An alternate approach to gradient estimators involves using continuous relaxations of categorical distributions, such as the Gumbel-Softmax reparametrization trick. Recent advancements in machine translation using deep neural networks have shown impressive results, with state-of-the-art models being auto-regressive. Efforts have been made to speed up machine translation decoding by utilizing the Transformer model together with the REINFORCE algorithm to model word fertilities. However, this approach requires extensive fine-tuning to make policy gradients work. BID18 propose a non-autoregressive model using iterative refinement to improve decoding speed of autoregressive translation models. Experiments were conducted with VQ-VAE and EM on English-German translation task. The model follows the architecture proposed in Figure 1, using Adam BID15 optimizer with hidden states dimension of 512. The decoder has a dimension of 512 and optimal hyperparameters are selected using WMT'13 English-German validation set. The VQ-VAE model is adapted to conditional supervised translation, with the encoder, decoder, and latent predictor conditioning on the source sentence x. The model architecture includes strided convolutional layers for the encoder and causal self-attention layers for the source sentence x. The encoder of the autoencoder converts the input to hidden states using causal self-attention layers and VQ-VAE for discretization. The decoder, after a bottleneck layer, utilizes transposed convolution layers and a transformer decoder with causal attention to generate the output. Tuning the code-book size to 2^12 discrete latents significantly improves accuracy. Sequence-level distillation further enhances model performance. Our VQ-VAE model, trained with soft EM and distillation, achieves a BLEU score of 26.7, 1.4 points lower than an autoregressive model but 4.1\u00d7 faster. The length of the latent variables is reduced by half at each compression step, with a compression factor denoted as n c. In experiments, the VQ-VAE model achieves a BLEU score of 26.7, 1.4 points lower than an autoregressive model but 4.1\u00d7 faster. The compression factor n c is set to 3, reducing the length by 8. Increasing n c to 4 decreases decoding time to 58 milliseconds with 25.4 BLEU. Attention to source sentence encoder is found unnecessary, resulting in stable training and improved performance of VQ-VAE without the use of Product. The decoder of the discrete autoencoder in our work does not attend to the source sentence. Training with EM involves a Monte-Carlo update with a small number of samples. Comparing Gumbel-Softmax, improved semantic hashing, and VQ-VAE models, the latter reached 26.4 BLEU on WMT'14 with sequence level knowledge distillation. Improved semantic hashing and VQ-VAE models achieved high BLEU scores on the WMT'14 English-German dataset. Different models were compared, including autoregressive Transformer, Non-Autoregressive Transformer, and Latent Transformer with semantic hashing. Distillation techniques were also used, with VQ-VAE utilizing a code-book of size 2^12 and a hidden dimension of 512. Decoding was done on a single CPU machine. Training a non-autoregressive machine translation model using VQ-VAE with EM algorithm and sequence distillation achieved accuracy close to autoregressive baseline, 3.3 times faster at inference. Results suggest using vector quantization for fast decoding of autoregressive sequence models. The VQ-VAE model is used for image reconstruction by selecting discrete latent variables and training a latent predictor model. Experiments were conducted on CIFAR-10 dataset using VQ-VAE and EM, with a field of 8x8x10 latents and a code-book size of 2^8. The encoder and decoder used are the same as in Machine Translation. The encoder and decoder architecture for image reconstruction using VQ-VAE and EM is similar to that of Machine Translation. The encoder consists of 4 convolutional layers followed by 2 residual layers, while the decoder has 2 residual layers and 4 deconvolutional layers. Training with EM shows more reliable reconstructions compared to VQ-VAE, with high-quality results. The paper proposed a gradient-based update rule for learning the code-book entries by minimizing a specific loss function. However, it was found that the Exponential Moving Average (EMA) update worked better than this gradient-based approach. The update rule for the embeddings is similar to EMA but also maintains an EMA over the counts. When using SGD with momentum or Adam, the update rule changes significantly. The effect of model size on BLEU score for models trained with EM and distillation is discussed. The paper discusses the effect of model size on BLEU score for models trained with EM and distillation. EM training shows a small performance improvement and leads to more robust training for machine translation. EM training is highlighted for its stability in image reconstruction experiments. Comparisons between VQ-VAE and EM on the WMT'14 English-German translation dataset are also presented. The teacher-forced BLEU score on the test set is used for evaluation during training. The VQ-VAE run collapsed, while the EM runs showed more stability. Latent sentences end with a fixed code, indicating the sequence's end. The model pads the latent sequence with this code to match the true sentence length. The Denoising autoencoder uses word dropout and permutation. Examples of latent codes from the WMT'14 English-German dataset highlight the emergence of the EOS/PAD latent. The Denoising autoencoder uses word dropout and permutation for regularization. It significantly improves performance on the WMT English-French dataset, with a gain of 1.0 BLEU on VQ-VAE and 0.9 BLEU over EM. Additional analysis on latents using Point-wise Mutual Information and tf-idf scores did not reveal any significant semantic patterns. The study reports preliminary results on the WMT English-French dataset without knowledge distillation from an autoregressive teacher. The study utilizes a Transformer base model for translation tasks and compares non-autoregressive and autoregressive models on the WMT'13 English-French dataset. The non-autoregressive model achieves a BLEU score of 30.0, while the autoregressive model scores 33.3 BLEU. Knowledge distillation is expected to bridge this performance gap. Various model configurations and decoding times are presented in Table 6, including compression factors and sampling techniques. Regularization techniques such as word dropout and permutation are employed, with a codebook hidden dimension of 512. Decoding is conducted on a single CPU. The codebook has a hidden dimension of 512, and decoding is done on a single CPU machine with an NVIDIA GeForce GTX 1080."
}