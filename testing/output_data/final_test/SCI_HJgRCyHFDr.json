{
    "title": "HJgRCyHFDr",
    "content": "Weight-sharing is a crucial aspect of neural architecture search, involving the optimization of multiple neural networks with the same parameters. It introduces new algorithmic and theoretical challenges, extending beyond just an optimization trick. The approach relaxes the hypothesis space structure, requiring careful consideration in gradient-based minimization methods. Additionally, weight-sharing methods solve bilevel optimization problems, with applications in kernel configuration and NLP feature selection. In this work, weight-sharing neural architecture search methods are explored, showing how they optimize bilevel objectives and extend the hypothesis class by treating architecture hyperparameters as learned parameters. A novel exponentiated gradient method for NAS is developed, aligning with optimization geometry and achieving state-of-the-art results on CIFAR-10. Weight-sharing in neural architecture search extends the hypothesis class by tuning hyperparameters of parameterized feature maps. This approach optimizes the objective of minimizing empirical risk in the joint space of model and architecture parameters efficiently using gradient-based methods. Weight-sharing in neural architecture search involves optimizing the empirical risk minimization objective efficiently using gradient-based methods in complex search spaces. A more principled, geometry-aware formulation of the optimization problem is developed, providing non-asymptotic convergence guarantees for weight-sharing algorithms that connect to the problem structure and handle the alternating-block nature of architecture search. The architecture search problem aims to find a configuration that achieves good generalization performance. Bilevel optimization is commonly used to optimize architecture weights, providing generalization guarantees over structured hypothesis spaces associated with a finite set of architectures. Weight-sharing in two settings: (1) shallow feature map selection and (2) CNN neural architecture search. Demonstrating efficient optimization of the bilevel objective and low generalization error. Developing EDARTS for better optimization of CNN architectures for CIFAR-10. Our work on optimization for weight-sharing benefits from literature on first-order stochastic optimization and mirror descent framework. We use successive convex approximation to show convergence of alternating minimization and derive geometry-dependent rates comparable to existing work on non-convex stochastic mirror descent. Our result generalizes to constrained, non-Euclidean, and multi-block settings. The approach by Agarwal et al. (2019) aims to achieve non-convex convergence from strongly convex minimization, which is a novel contribution. Previous optimization results for NAS have focused on regret bounds or monotonic improvement guarantees. The analysis of bilevel optimization properties is related to model selection work but does not consider configuration parameters as explicit controls on model complexity. The learning results are broadly related to hyperparameter optimization, with a focus on statistical rather than algorithmic questions. The weight-sharing learning problem is formalized in this section, extending the hypothesis space to be parameterized by a finite set of configurations. ERM can be applied with optional regularization to select a hypothesis from this extended space, as done by some NAS methods. The learning algorithm involves minimizing for block-specific regularizers in the presence of weight-sharing, eliminating the need to learn separate hypotheses for each subclass. Weight-sharing methods allow for more efficient gradient-based optimization approaches in selecting hypotheses from different subclasses. Feature map selection involves tuning hyperparameters for kernel ridge classification and NLP featurization pipelines. Neural Architecture Search uses micro cell-based search spaces for tractability and additional structure. Neural Architecture Search utilizes micro cell-based search spaces represented as directed acyclic graphs with nodes and edges. The search spaces are defined by the number of nodes, edges per node, and operations that can be applied. Weight-sharing methods train a single shared-weights network using both weights and architecture decisions as parameters. Weight-sharing methods train a single shared-weights network encompassing all possible functions within the search space, including edges between nodes and operations. Architecture weights are used as input, indicating the weight given to each operation on an edge. Gradient-based weight-sharing methods apply continuous relaxations to the search space to compute gradients, with some methods using a mixture of operations per edge before discretizing to a valid architecture. The text discusses the use of continuous relaxation in weight-sharing methods for architecture search, enabling the optimization of architecture weights using gradient-based methods. Despite the effectiveness of continuous relaxation, architecture search remains costly and noisy, with state-of-the-art methods requiring complex computations and suffering from high variance in policy gradients. The text discusses the challenges in optimizing architecture weights due to high variance policy gradients. Various optimization schemes have been proposed, but most focus on auxiliary objectives and do not address the problem's two-block nature. Mirror descent framework is suggested as a suitable tool for designing algorithms in block optimization. The text discusses using geometry-aware gradient algorithms for faster convergence in block optimization problems like architecture search. It presents two optimization algorithms for multi-block optimization, relaxing problems to minimize a function over a convex product space consisting of blocks with associated norms. The text introduces a distance-generating function for each block in network weights, emphasizing the importance of the dual norm in measuring the second moment of the gradient for stochastic MD steps in optimization algorithms. In this paper, the authors propose two methods for multi-block optimization problems: stochastic block mirror descent (SBMD) and alternating successive convex approximation (ASCA). These methods involve updating a random coordinate i using mirror descent with batched gradient for SBMD, and optimizing a strongly-convex surrogate function with a user-specified solver for ASCA. The focus is on dimension-free convergence guarantees in constrained geometries, with potential applications in architecture search. The methods proposed for multi-block optimization problems are stochastic block mirror descent (SBMD) and alternating successive convex approximation (ASCA). SBMD involves updating a random coordinate using mirror descent with batched gradient, while ASCA optimizes a strongly-convex surrogate function with a user-specified solver. Convergence guarantees in constrained geometries are discussed, with potential applications in architecture search. The operator prox\u2207 \u03bb (x) minimizes the function with a fixed point, yielding the projected gradient measure. In the multi-block case, it replaces an \u03b5-stationary point of f with respect to the projected gradient. The norm of the projected gradient measures the distance from satisfying a first-order optimality condition. Algorithm 1 guarantees reaching an \u03b5-stationary point x in X. ASCA provides a reduction to strongly-convex optimization algorithms. ASCA is a method that reduces strongly-convex optimization algorithms by constructing and solving a surrogate function at each iteration. It is particularly useful when efficient solvers are available for some or all blocks, such as in feature map selection problems. The projected stationarity measure is analyzed in ASCA, which provides a stronger notion of stationarity. The proof generalizes a result of Agarwal et al. (2019) and shows that if solvers return approximate optima of strongly-convex functions on each block, we can converge to a stationary point of the original function. The convergence rate depends on the solver used, with a specification given for stochastic mirror descent. Corollary 3.1 states that under certain conditions, ASCA using the Epoch-GD method requires a certain number of oracle calls to reach an \u03b5-stationary-point x \u2208 X. The Benefit of Geometry-Aware Optimization: Convergence of gradient-based architecture-search algorithms can benefit from this strategy. By optimizing shared weights with SGD and updating architecture parameters with exponentiated gradient, we can reach an \u03b5-stationary point. Theorem 3.1 shows that updating architecture parameters leads to reaching an \u03b5-stationary point in O stochastic gradient computations. This improvement is crucial due to high noise in architecture gradients, especially with policy gradient estimation. ASCA can provide similar guarantees with bounded probabilities and shared weights. The \u03c3 2 2 d term remains in both SBMD and ASCA, but can be improved with adaptive algorithms. The weight-sharing hypothesis class H(C, W) is described as a set of functions partitioned by configurations C sharing weights in W. The bilevel optimization problem involves selecting a hypothesis from H(C, W) with separate validation sets for architecture parameter updates. This setup is related to model selection and cross-validation, with a key difference being the regularization of training losses. The choice of configuration in NAS does not control the complexity of the hypothesis space like in model selection. Model selection often involves nested hypothesis classes with increasing complexity, while NAS configuration parameters behave more like regular model parameters. This difference raises questions about the role of NAS in controlling model complexity. The bilevel formulation in NAS raises questions about the partitioning of the hypothesis space induced by configurations. The optima of the weight-sharing problem restrict model weights, reducing complexity without affecting performance. The covering-number of functions at scale \u03b5 helps in constructing an \u03b5-cover of the set of functions. The version entropy quantifies the complexity of the hypothesis class and the worst-case complexity of global minimizers of the loss function. In feature selection, the version entropy is log |C| due to strongly-convex loss, while in nested model selection it reflects the complexity of the largest model. In practical problems like NAS, constraints on input edges impose an inductive bias. The excess risk is bounded by version entropy, with an assumption that a good configuration exists in C, ensuring at least one low-risk classifier in the optimization process. This prevents scenarios where optimal configurations do not yield good weights despite knowing the best setup. Theorem 4.1 provides a guarantee on solutions to the bilevel optimization under certain assumptions. It states that if the hypothesis is B-bounded, then the first difference is bounded by version entropy, the second by optimality of the hypothesis, the third by Hoeffding's inequality, and the last by Assumption 4.1. This theorem ensures that a bound on the version entropy can guarantee excess risk almost as good as the optimal configuration without assumptions about complexity or behavior. The feature map and kernel selection problem is addressed in the context of a bilevel optimization, where each feature map induces a unique weight leading to a singleton version space. For kernel selection using random Fourier approximation, generalization guarantees can compete with the optimal Reproducing Kernel Hilbert Space (RKHS). In feature map and kernel selection, a random Fourier feature approximation is used to approximate an RKHS. Risk bounds are achieved similar to knowing the optimal configuration beforehand, with an additional capacity term. This approach avoids the high complexity of the hypothesis space induced by the worst configuration. In Neural Architecture Search, there is no bound on the version entropy, which depends on all configurations. The version entropy bound in Neural Architecture Search depends on all configurations of the model. The complexity of deep networks compared to the number of samples remains uncertain, with evidence suggesting a small version entropy. Shared-weight optimization results in models with smaller 2-norm and distance from initialization. However, there is debate on whether implicit regularization can lead to a non-vacuous uniform convergence bound, potentially indicating a large NAS version entropy. Weight-sharing can speed up architecture search problems by applying it to feature map selection. A geometry-aware weight-sharing method is used to design CNN cells for CIFAR-10. The weight-sharing procedure involves a probability distribution over N and minimization of validation loss. This approach is equivalent to probabilistic NAS. The text discusses weight-sharing in architecture search, specifically focusing on probabilistic NAS for updating classifier parameters using random feature maps. Two update schemes are considered: exponentiated gradient and successive elimination. The first problem tackled is kernel ridge regression on CIFAR-10 with three configuration decisions: data preprocessing, kernel choice, and bandwidth parameter. In the study, weight-sharing is explored for feature map selection in logistic regression for IMDB sentiment analysis using Bag-of-n-Gram featurizations. Eight configuration decisions are considered, including tokenization method and feature dimension hashing. Performance is tested by sampling configurations for CIFAR-10 and IMDB datasets to determine optimal choices. The study compares different optimization schemes for feature map selection in logistic regression for IMDB sentiment analysis. Successive halving performs almost as well as random search in less time, while exponentiated gradient returns a configuration in the top 10%. Over-parameterization is beneficial for IMDB due to the large n-gram vocabulary size. Weight-sharing can be used as a fast way to obtain signal in regular learning algorithm configuration, not just in NAS. Experiments on NAS on CIFAR-10 show that Exponentiated-DARTS (EDARTS) modifies first-order DARTS by using standard normalization for architecture weights and exponentiated gradient for updates. Exponentiated-DARTS (EDARTS) modifies first-order DARTS by using exponentiated gradient for updating architecture weights. Compared to XNAS, it requires fewer modifications and aligns with the optimization geometry of ERM. Our XNAS implementation did not yield competitive results, so we evaluate EDARTS on designing a CNN cell for CIFAR-10 using the same search space and evaluation process as DARTS. Table 1 displays the performance of EDARTS compared to manually designed and NAS-discovered architectures. EDARTS achieves competitive results with fewer parameters than manual designs and outperforms first and second-order DARTS while requiring less compute time. It also matches the performance of state-of-the-art method XNAS when evaluated using the stage 3 training routine of DARTS. The extended evaluation of the best architecture found by EDARTS with AutoAugment, cosine power annealing, and cross-entropy with label smoothing achieved a test error of 2.18%. Comparatively, XNAS had a test error of 2.15% in the reproduced evaluation. EDARTS showed lower variance across experiments than random search with weight sharing, although there was still noticeable variance in the performance based on different random seed initializations. The section discusses proofs and generalizations of non-convex optimization results, including definitions from convex analysis. It introduces the concept of Bregman divergence induced by a strictly convex distance-generating function, highlighting its properties. The text discusses the properties of a 1-strongly-convex distance-generating function (DGF) \u03c9 in relation to the proximal operator and proximal gradient operator. It also presents Theorem A.1 regarding convexity conditions for the proximal operator and proximal gradient operator. The text discusses convergence measures for constrained non-convex optimization, focusing on the norm of the projected gradient and a stronger measure called the proximal-mapping-based stationarity measure. The text discusses convergence measures for constrained non-convex optimization, focusing on the norm of the projected gradient and a stronger measure called the proximal-mapping-based stationarity measure. The convergence results hold for a measure inspired by Bregman stationarity, using the prox grad operator instead of the prox operator. The text introduces the concept of (\u03b3, \u03c9)-RWC as a generalization of \u03b3-strong-smoothness for functions that are strongly-convex with respect to a given norm. It defines the prox operator for \u03bb > \u03b3 and discusses the Bregman gradient. The relationship between the Bregman stationarity measure and first-order optimality conditions is also explored. The text discusses guaranteeing improvement of a non-convex function by optimizing a strongly convex surrogate. It introduces a lemma regarding strongly convex DGFs and subdifferentiable functions. The multi-block setting and assumptions are formalized, with convex subsets and 1-strongly-convex DGFs. The text introduces a generic successive convex approximation algorithm for optimizing a non-convex function in a multi-block setting. It relies on a general assumption that can be derived from strong smoothness or weak convexity. The algorithm aims to reach a stationary point by iteratively updating points in the product space. The main result relies on a lemma guaranteeing non-convex convergence in Setting A.1. The output of Algorithm 2 satisfies a specific condition involving the function F and the expectation over sampling and randomness. Theorem A.2 provides a guarantee under certain assumptions in Setting A.1. The output of Algorithm 2 in Setting A.1 satisfies a specific condition involving the function F and the expectation over sampling and randomness. The algorithm returns a point x \u2208 X satisfying certain criteria with N T calls to the stochastic gradient oracle. Algorithm 2 in Setting A.1 returns a point x \u2208 X satisfying specific criteria with N T calls to the stochastic gradient oracle. The expected \u03b5-stationary-point is achieved by applying Theorem 5 of Hazan and Kale (2014) and considering the strong convexity and bounded stochastic gradient of f x. The results hold for the multi-block case with the projected stationarity measure defined by Theorem A.3. Under Assumption A.1, if f (\u00b7, x \u2212i ) is \u03b2-strongly-smooth, the projected stationarity measure \u2206 \u03bb is defined w.r.t. the Bregman divergence of the DGF \u03c9. Applying Lemma A.4 in the multiblock setting requires showing equality of projected stationarity measures on each block. For some \u03bb > 0 and any i \u2208 [b], it is shown that applying Lemma A.2 with \u03bb = 1 4\u03c1 yields the desired result. In Setting A.1, Algorithm 2 returns a point x \u2208 X satisfying specific criteria with N T calls to the stochastic gradient oracle. The expected \u03b5-stationary-point is achieved by applying Theorem 5 of Hazan and Kale (2014) for the multi-block case. For some \u03bb > 0 and any i \u2208 [b], applying Lemma A.2 with \u03bb = 1 4\u03c1 yields the desired result. The section contains proofs of generalization results in Section 4, focusing on a setting where a set of possible architecture/configurations is associated with parameterized hypothesis classes. Each configuration is linked to a hypothesis class for input space Z, with a fixed set of weights. The performance of a hypothesis on an input is measured using specific criteria. The performance of a hypothesis on an input is measured using specific criteria, considering solutions of optimization problems dependent on training data and architecture. The main assumption required is the existence of a good architecture that minimizes the optimization problem induced by the weights. The text discusses the importance of finding a good set of weights for a given optimal configuration in an optimization problem induced by training data. It introduces the concept of error functions that decrease with the number of training samples and increase poly-logarithmically in a certain parameter. The text also mentions how Assumption B.1 can be formally shown to hold in specific settings, with a focus on covering numbers of function classes in statistical learning theory. The text discusses a standard result in statistical learning theory related to the log covering number of version spaces induced by an optimization procedure over a training set. It introduces a theorem under a specific setting and provides bounds for terms in the optimization problem solution. The text also mentions the hypothesis space coverage and optimality of the pair obtained, along with the application of Hoeffding's inequality. The text discusses applying Hoeffding's inequality and Theorem B.2 to obtain a solution in Setting B.2, with a focus on kernel selection and generalization results for learning with random features. It also mentions the optimization problem for obtaining a solution in the case of neural architecture search. In neural architecture search, solving ERM without regularization can lead to a solution with low generalization error. Hoeffding's inequality is applied to obtain a solution with low generalization error in Setting B.1. Ridge regression and logistic regression solvers from scikit-learn were used for CIFAR-10 dataset. For CIFAR-10, the kernel configuration setting was based on Li et al. (2018) with a regularization parameter replaced by the Laplace kernel. The regularization was set to \u03bb = 1/2 and the dataset split was 40K/10K/10K. For IMDB, architectures were trained for 600 epochs with hyperparameters from DARTS. The search space included operations like separable and dilated convolutions, max pooling, average pooling, and identity operations for designing normal and reduction cells. In the experiments, cells have stride 1 operations for input nodes and 4 intermediate nodes, while reduction cells have stride 2 operations that halve the dimensions. EDARTS is used to train a smaller shared-weights network with 8 layers and 24 initial channels. An auxiliary head with weight 0.4 and scheduled path dropout of 0.2 is used. Learning rates are set at 0.2 for normal cells and 0.6 for reduction cells, with other hyperparameters the same as DARTS. The larger evaluation network has 20 layers and 36 initial channels, trained for 600 epochs using SGD with momentum, a batch size of 96, and a learning rate annealed from 0.025 to 0.001. Weight-sharing is investigated for implicit regularization effects on the hypothesis space. The study explores weight-sharing for implicit regularization effects in the EDARTS architecture. Results show that weight-sharing acts as a form of implicit regularization, leading to lower variance compared to random search with weight sharing. However, there is still variance in performance based on different random seed initializations, requiring multiple searches before selecting an architecture."
}