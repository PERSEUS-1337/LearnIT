{
    "title": "H1xJhJStPS",
    "content": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience by computing gradients similar to Backpropagation Through Time (BPTT) but with a spatially local learning rule. EP involves two phases: neurons evolve freely towards a steady state, then output neurons are adjusted towards the target until reaching another steady state. However, current implementations of EP lack temporal locality in the learning rule, hindering its biological plausibility and hardware efficiency. Continual Equilibrium Propagation (C-EP) is a version of EP where neuron and synapse dynamics occur simultaneously in the second phase, making weight updates local in time. Theoretical proof shows that with small learning rates, neurons and synapses follow loss gradients from BPTT at each time step in the second phase. Continual Equilibrium Propagation (C-EP) is demonstrated on MNIST and extended to neural networks with asymmetric connections. Results show that closely following BPTT gradients during network updates leads to better performance in training. This approach aims to bridge the gap between neuroscience and Artificial Intelligence research by integrating principles of animal intelligence into building intelligent machines. Despite the power of backpropagation in training artificial neural networks, its energy consumption on conventional hardware far exceeds that of the brain. Investigating alternative learning paradigms for energy-efficient hardware inspired by the brain's dynamics and topology is crucial for developing neuromorphic computing. Equilibrium Propagation offers advantages in estimating error gradients, bridging the gap between neuroscience and AI research. Equilibrium Propagation (EP) is a contrastive Hebbian learning (CHL) algorithm that benefits from neural dynamics and synaptic updates based on locally available information. It applies to convergent RNNs, adjusting weights to produce output values close to targets. CHL algorithms involve two phases: neurons evolve freely to a steady state, then settle to a second steady state influenced by target values. Weight updates in CHL algorithms are Hebbian in nature. Equilibrium Propagation (EP) is a contrastive Hebbian learning (CHL) algorithm that utilizes neural dynamics and synaptic updates based on local information. Unlike standard CHL algorithms, EP does not clamp output neurons in the second phase but elastically pulls them towards the target y. EP is closely linked to backpropagation, following gradients of recurrent backpropagation (RBP) and backpropagation through time (BPTT) for synaptic updates. Despite its attractiveness in bridging neural networks, EP still faces limitations in bioplausibility. The limitations of Equilibrium Propagation (EP) include non-locality in time and the requirement of symmetric weights in network dynamics. A new implementation called C-EP addresses these issues by enabling temporal locality and allowing for asymmetric synaptic connections. Continual Equilibrium Propagation (C-EP) introduces a new version of EP with continual weight updates, allowing for asymmetric synaptic connections and addressing issues of non-locality in time and symmetric weights in network dynamics. The Gradient Descending Dynamics (GDD) property is demonstrated in the training process with C-EP on MNIST, approaching the accuracy of standard EP. The C-EP algorithm is adapted to neural networks with asymmetric connections, named C-VF, inspired by the Vector Field method. Numerical results show that training performance is linked to the satisfaction of GDD. The Recurrent Backpropagation (RBP) algorithm is compared to C-EP, EP, and BPTT on a simple analytical model. The goal is to optimize the parameter \u03b8 to minimize loss for RNNs using algorithms like BPTT and Recurrent Backpropagation. The loss for RNNs is optimized using algorithms like Backpropagation Through Time (BPTT), Recurrent Backpropagation (RBP), and Equilibrium Propagation (EP). A new algorithm called C-EP is introduced for continual parameter updates to optimize the loss function. The C-EP algorithm offers continuous weight updates based on neuron states, unlike traditional methods like BPTT, RBP, and EP. It updates weights between consecutive states in the second phase, rather than at the end based on steady states. This continual update approach distinguishes C-EP from EP. The C-EP algorithm introduces continuous weight updates based on neuron states, unlike traditional methods like BPTT, RBP, and EP. It distinguishes itself by updating weights between consecutive states in the second phase, rather than at the end based on steady states. This continual update approach sets C-EP apart from EP, with parameter updates performed throughout the second phase. The C-EP algorithm introduces continuous weight updates based on neuron states, with dynamics of s and \u03b8 depending on hyperparameters \u03b2 and \u03b7. The parameter used to update s in C-EP is the current \u03b8, leading to similar dynamics as EP when \u03b7 is close to 0. The C-EP algorithm introduces continuous weight updates based on neuron states, with dynamics depending on hyperparameters \u03b2 and \u03b7. Theorem 1 states that under certain conditions, the normalized updates in the second phase of C-EP follow the gradients of BPTT as \u03b2 and \u03b7 approach 0. In the second phase of C-EP, neurons and synapses descend gradients of the loss obtained with BPTT, with \u03b2 and \u03b7 acting as learning rates. Validation on the MNIST dataset is done with two models: a vanilla RNN with tied weights and a Discrete-Time RNN with untied weights. The second model, a Discrete-Time RNN with untied and asymmetric weights, is trained using C-VF, inspired by the Vector-Field dynamics algorithm. Previous simulations have shown that when EP updates match BPTT gradients, the model performs well. Implementing C-EP and C-VF on vanilla RNNs accelerates simulations. The dynamics of the first phase are defined with an activation function and symmetric weights. The dynamics of the first phase in the model involve activation functions, symmetric weight matrices, and normalized updates. This model applies to any network topology with symmetric connections between layers. The Vanilla RNN with asymmetric weights is trained using C-VF, with dynamics similar to Eq. (10). The dynamics in the first phase of the model involve activation functions and symmetric weight matrices. The weight matrix W is no longer assumed to be symmetric in the C-VF trained Vanilla RNN with asymmetric weights. The weight dynamics in the second phase are replaced by a version for asymmetric weights, allowing for normalized updates. This model can be applied to deep networks with any number of layers and follows the gradients of BPTT. Training experiments on MNIST demonstrate the effectiveness of this approach. Training experiments on MNIST with EP, C-EP, and C-VF are conducted using multi-layered vanilla RNNs. Results show that while C-EP approaches the test error of EP, there is a degradation in accuracy. The initial angle between forward and backward weights in C-VF affects the test error rate on MNIST. In training experiments on MNIST with EP, C-EP, and C-VF using multi-layered vanilla RNNs, C-EP approaches the test error of EP but shows a degradation in accuracy due to the finite learning rate leading to divergence between update curves. The deeper the network, the more challenging it is for C-EP dynamics to follow BPTT gradients. In training experiments on MNIST with EP, C-EP, and C-VF using multi-layered vanilla RNNs, C-EP approaches the test error of EP but shows a degradation in accuracy due to the finite learning rate leading to divergence between update curves. By adjusting small learning rates and rescaling weight updates, standard EP results can be recovered. Different deviations from ideal conditions are depicted in Fig. 5 (b), showing how normalized updates and gradients may split apart with continual weight updates and untied weights. To create these deviations and study their impact on training, the initial angle between forward and backward weights is tuned in C-VF simulations. In training experiments on MNIST with EP, C-EP, and C-VF using multi-layered vanilla RNNs, the angle between forward and backward weights is tuned in C-VF simulations. The angle \u03a8(\u03b8 f , \u03b8 b ) is computed for weight initialization, showing high error rates with high angles between gradients of C-VF and BPTT. Test error increases monotonically with \u03a8 \u2206(tot) from standard EP to C-VF. Equilibrium Propagation (EP), Contrastive Equilibrium Propagation (C-EP), and Contrastive Variational Free Energy (C-VF) were compared in training experiments on MNIST with multi-layered vanilla RNNs. Proper weight initialization is crucial in C-VF, with test error increasing with weight untying. C-EP leverages neuron dynamics for weight gradients computation, eliminating the need for artificial memory units. The C-EP framework preserves equivalence with Backpropagation Through Time, satisfying Gradient Descending Dynamics in slow synaptic dynamics. Experimental results confirm this theorem, showing a modest reduction in MNIST accuracy compared to standard EP. This reduction can be eliminated by using smaller learning rates and rescaling the total weight update. Theorem 1 extends the theory of Ernoult et al. (2019) and provides a statistically robust tool for C-EP based learning. The C-EP and C-VF algorithms mimic biology by incorporating features like Spike Timing Dependent Plasticity (STDP) in RNNs with asymmetric weights. STDP modulates synaptic strength based on the timing of pre and post synaptic spikes. Each synapse corresponds to a color, and the learning rule is local in space and time. The C-EP and C-VF algorithms aim to implement Spike Timing Dependent Plasticity (STDP) in RNNs with asymmetric weights. While they do not model biological learning directly, they bring EP closer to biology. The focus is on proposing a local implementation of EP for hardware, reducing simulation times significantly with a Titan RTX GPU. Our work provides engineering guidance for mapping algorithms onto neuromorphic systems, focusing on tuning hyperparameters for efficient training. By bridging Equilibrium Propagation with neuromorphic computing, we aim to enable energy-efficient implementations of gradient-based learning algorithms. In this appendix, Theorem 1 is proven, stating that under certain conditions, the first K normalized updates in the second phase of C-EP are equal to the negatives of the first K gradients of BPTT. The equivalence of four algorithms for computing the gradient of the loss is also discussed: Backpropagation Through Time (BPTT), Recurrent Backpropagation (RBP), Equilibrium Propagation (EP), and another algorithm. Theorem 1 proves that in certain conditions, the first K normalized updates in the second phase of C-EP are equal to the negatives of the first K gradients of BPTT. Different algorithms for computing the gradient of the loss are discussed, including BPTT, RBP, EP, and C-EP. The transition function derives from a primitive function, F(x, s, \u03b8) = \u2202\u03a6 \u2202s (x, s, \u03b8). In the limit of small hyperparameter \u03b2, the normalized updates of EP are equal to the gradients of RBP. The network reaching steady state s* after T \u2212 K steps implies the first K gradients of BPTT are equal to the first K gradient of RBP. Proofs of Lemmas can be found in various sources. Proof of the equivalence of EP and BPTT was derived in Ernoult et al. (2019). In the limit of small learning rate, the updates of C-EP are equal to those of EP. By computing the limits of the updates as the learning rate approaches zero, it is shown that the two methods are equivalent. In the context of small learning rates, the updates of C-EP match those of EP, leading to the standard EP learning rule. Additionally, Backprop Through Time (BPTT) and Almeida-Pineda Recurrent Backprop (RBP) algorithms are discussed as methods to optimize loss in RNNs, with BPTT being the preferred choice in deep learning for sequential data tasks. Backpropagation Through Time (BPTT) and Recurrent Backprop (RBP) are methods to optimize loss in RNNs. RBP can be seen as a particular case of BPTT. The equivalence of BPTT and RBP is shown when the network reaches a steady state after T - K steps. The gradients of BPTT and RBP are not equal for t > K because they compute gradients of different loss functions. BPTT is the standard method to train RNNs and convergent RNNs. In this paper, the focus is on convergent RNNs and the cost of the state after T time steps. The loss at the steady state is approximated by the loss after T time steps. The gradients are computed using BPTT, which involves iteratively computing partial derivatives backward in time. The gradients of BPTT are computed iteratively backward in time using the chain rule of differentiation. In the specific setting with static input x, if the network reaches a steady state after T \u2212 K steps, only the steady state needs to be stored in memory to compute the gradients. The Almeida-Pineda algorithm (RBP) computes gradients using only the steady state s* without storing past hidden states. It is more memory efficient than BPTT. The RBP algorithm is more memory efficient than BPTT as it only requires the steady state s* to compute gradients, unlike BPTT which needs to store past states. The RBP algorithm is more memory efficient than BPTT as it only requires the steady state s* to compute gradients. Gradients of the projected cost function are computed by the RBP algorithm, where L* represents the loss at a given time step. The gradient of the loss function L* is computed using the chain rule of differentiation. The model studied involves a scalar variable s and parameter \u03b8, with a steady state solution s* = \u03b8. The dynamics and cost function are defined, and the equivalence of different algorithms (BPTT, RBP, EP, CEP) is illustrated in a simple model. The BPTT model is for pedagogical purposes only. It approximates the steady state and loss after T time steps. Gradients of BPTT converge to RBP gradients as T approaches infinity. The dynamics' steady state is s* = \u03b8. The second phase of EP involves a linear dynamical system that can be solved analytically. For small \u03b2 values, the trajectory is close to s* = \u03b8. Normalized updates of EP are computed using Eq. 19. The normalized updates of EP converge to the gradients of RBP as \u03b2 approaches 0. The system of equations governing the system is solved analytically, with the trajectory close to s* = \u03b8 for small \u03b2 values. The dynamics of EP and gradient computation in BPTT were shown to be equivalent in previous work. The GDU property of Ernoult et al. (2019) states that normalized updates of EP are equal to gradients of BPTT. The GDD property in this work shows that normalized updates of C-EP are also equal to BPTT gradients. The difference lies in the interpretation of 'updates' in EP and C-EP. In C-EP, updates dynamically occur throughout the second phase, while in EP, updates are performed at the end of the phase. The GDD property states that system dynamics from a primitive function follow a specific form. The normalized updates of C-EP match the gradients of BPTT, even for C-VF dynamics that do not derive from a primitive function. The transition function F is defined as \u03c3(W \u00b7 s), leading to specific dynamics in the first and second phases. This property is illustrated in figures 5, 12, and 13. In the second phase, the normalized updates of C-VF follow the gradients of BPTT. The Jacobian of F is symmetric if W is symmetric, meeting the conditions of Theorem 9. Theorem 1 is a special case of Theorem 9, with specific dynamics defined by the transition function F as \u03c3(W \u00b7 s). In the second phase, the normalized updates of C-VF follow the gradients of BPTT. The Jacobian of F is symmetric if W is symmetric, meeting the conditions of Theorem 9. Theorem 1 is a special case of Theorem 9, with specific dynamics defined by the transition function F as \u03c3(W \u00b7 s). Define the (normalized) neural and weight updates of C-VF in the limit \u03b7 \u2192 0 and \u03b2 \u2192 0. The proof of Lemma 10 is similar to the one provided in Ernoult et al. (2019). In this section, we describe the C-EP and C-VF algorithms when implemented on multi-layered models, with tied weights and untied weights respectively. In this experiment, the Gradient Descending Dynamics (GDD) property is demonstrated on a 784-512-...-512-10 network architecture using tanh activation function. The experiment involves taking a random MNIST sample and target, performing phases with time-discretization parameter, BPTT, and C-EP/C-VF over K steps for given values of \u03b2. In this experiment, the Gradient Descending Dynamics (GDD) property is demonstrated on a network architecture using tanh activation function. The experiment involves comparing gradients and normalized updates provided by BPTT and C-EP/C-VF algorithms over K steps for given values of \u03b2 and \u03b7. The dynamics of the first and second phases are defined, and the function is considered for computing updates. Comparing equations, it is observed that the feedback connections are constrained to be the transpose of the feedforward connections in the layered architecture. In this experiment, the Gradient Descending Dynamics (GDD) property is demonstrated on a network architecture using tanh activation function. The dynamics of the first and second phases are defined, and the function is considered for computing updates. The model is used for training with defined equations for different layers and phases. The C-EP model is introduced for training experiments in Section 4.2. The dynamics of the first phase are defined, with feedback connections constrained to be the transpose of feedforward connections. The equations for the C-EP model can be written in a vectorized block-wise fashion. This model has not been used in this work but is introduced for further exploration. The C-EP model is introduced for training experiments in Section 4.2, with feedback connections constrained to be the transpose of feedforward connections. The equations for the C-EP model can be written in a vectorized block-wise fashion. This model has not been used in this work but is introduced for completeness. The C-EP model introduces vectorized block-wise equations with feedback connections constrained to be the transpose of feedforward connections. Eq. (98) can be vectorized similarly to Eq. (91) with defined hyperparameters. The dynamics of the model are defined in two phases, with continual updates shown to affect learning processes in different models. The C-EP model introduces vectorized block-wise equations with feedback connections constrained to be the transpose of feedforward connections. Simulations have been carried out in Pytorch, with code attached in the supplementary materials. The paper includes a readme.txt with dependencies and commands for reproducing results. Training was done on the MNIST dataset with SGD and Glorot-initialized weights. No regularization was used, and a shifted sigmoid function was employed. The 'Random \u03b2' option was utilized in the experiments. The 'Random \u03b2' option in the experiments involves sampling the sign of \u03b2 from a Bernoulli distribution. Additionally, the study investigates the angle between forward and backward weights to improve model convergence for Continual Equilibrium Propagation. The study explores tuning the angle between forward and backward weights to enhance model convergence for Continual Equilibrium Propagation. This involves gradually adjusting the angle by flipping the sign of components of the backward weights, controlled by a binary mask. Hyperparameter search is conducted for recurrent parameters T and \u03b2, as well as learning rates. The study involves tuning the angle between forward and backward weights to improve model convergence for Continual Equilibrium Propagation. Hyperparameter search is conducted for recurrent parameters T and \u03b2, as well as learning rates. Initial guesses for T and \u03b2 are found by analyzing processes associated with synapses and neurons. The learning rates are initialized with increasing values from the output layer to upstream layers, with typical ranges specified for T, K, and \u03b2. Hyperparameters are adjusted to minimize train error. The study involved adjusting hyperparameters to minimize train error, with a focus on obtaining minimal recurrent hyperparameters. A table of hyperparameters used for training was provided, along with training results on MNIST with EP, C-EP, and C-VF. The number of hidden layers and iterations were specified, with mean and standard deviation values for test and train errors presented."
}