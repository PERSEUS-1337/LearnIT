{
    "title": "HkevCyrFDS",
    "content": "Existing unsupervised video-to-video translation methods struggle to create realistic, semantic-preserving, and consistent translated videos. A novel model is proposed in this work, utilizing style-content decomposition, specialized encoder-decoder structure, and bidirectional RNN units to propagate inter-frame information. The model can achieve long-term style-consistent translations and modality flexible results. Additionally, a video interpolation loss captures temporal information for self-supervised training, resulting in photo-realistic, spatio-temporal consistent translated videos. Our model produces photo-realistic, spatio-temporal consistent translated videos in a multimodal way, validated by subjective and objective experimental results. Recent image-to-image translation (I2I) methods using Generative Adversarial Networks (GANs) have shown impressive results. While most GAN-based I2I methods focus on paired data, the introduction of cycle-consistency loss in CycleGAN has led to promising performance in unsupervised image-to-image translation. Video-to-video translation (V2V) is more challenging than I2I, with less exploration in the field. Video-to-video translation (V2V) is more challenging than image-to-image (I2I) translation due to the need for temporal consistency. Wang et al. proposed a supervised solution using optical flow and video-specific constraints. Previous unsupervised V2V methods have focused on designing spatio-temporal translators or losses for better temporal results. Unsupervised V2V aims to achieve temporally consistent results while preserving semantic information. 3DCycleGAN method uses 3D convolution to capture temporal information but struggles with generating long-term consistent video sequences. Bansal et al. introduced a Recycle loss for better modeling in V2V translation. Bansal et al. (2018) introduced Recycle loss for spatio-temporal relationship modeling in video frame translation. The method achieved state-of-the-art unsupervised V2V results but faces challenges in translating videos with a large domain gap. UVIT is a novel method for unsupervised video-to-video translation that addresses limitations in existing methods like RecycleGAN and Motion-guided CycleGAN. It aims to achieve long-term style consistency and short-term content coherence in video sequences. The UVIT model focuses on achieving long-term style consistency and short-term content coherence in video sequences. It decomposes input video sequences into content and style, processes the content using TrajGRUs for translation and interpolation, resulting in visually realistic video sequences. The UVIT model utilizes an encoder-decoder architecture for video translation and interpolation, maintaining style and content consistency. It involves extracting content and style information, propagating inter-frame content, and utilizing conditional instance normalization for decoding. The model aims to achieve long-term style consistency and short-term content coherence in video sequences. The proposed architecture for video translation and interpolation involves using style information as a condition for the decoder, combining multi-frame content information for consistent outputs, and utilizing bidirectional RNNs for long-term and short-term consistency. The training strategy benefits from structured video data and self-supervised learning. The Encoder-RNN-Decoder translator benefits from structured video data and self-supervised training. It uses temporal information within the video data itself for video interpolation, unlike the RecycleGAN method which indirectly utilizes video predictors for spatio-temporal constraints. The paper introduces a novel Encoder-RNN-Decoder framework for unsupervised video-to-video translation and a video interpolation loss for temporal information capture. Extensive experiments demonstrate the model's superiority at both video and image levels. The paper presents an Encoder-RNN-Decoder framework for unsupervised video-to-video translation, aiming to align the conditional distribution of translated videos with target domain videos. Style latent variables are used to generate style-consistent video sequences, allowing for deterministic control over the output style. The Encoder-RNN-Decoder framework proposed for unsupervised video-to-video translation includes content encoders, style encoders, and Trajectory Gated Recurrent Units for inter-frame content propagation. The proposed Encoder-RNN-Decoder framework for unsupervised video-to-video translation includes Trajectory Gated Recurrent Units for inter-frame content propagation, a Merge Module for combining forward and backward content, and conditional content decoders for generating output frames based on spatio-temporal content information and style codes. The conditional generator in the video translation framework allows for modality flexible translation by controlling subset style through style code assignment. Video interpolation loss is used for self-supervised training, ensuring similarity between translated and real video frames in the target domain. Illustrations of translation and interpolation mappings are provided. The text discusses the use of image-level and video-level discriminators to ensure quality in different domains. It mentions the adoption of relativistic LSGAN loss and various loss terms such as video interpolation, cycle consistency, and style encoder loss to maintain semantic consistency and realism in video frames. The focus is on training the style encoder and ensuring the quality of video frames in the target domain. The text discusses training the style encoder with defined losses for the Generator and Discriminator modules. Implementation details include training with 6 frames per batch at 128 \u00d7 128 resolution on a single Titan Xp GPU. During testing, video clips with 30 frames are divided into 7 sequences of 6 frames each, maintaining style consistency. The model can be extended to process video sequences of any length. The model discussed can process video sequences of any length and is evaluated using the Viper dataset. The dataset includes semantic label videos and scene image videos with subsets like day, sunset, rain, snow, and night. The diversity of scenes makes it challenging for unsupervised V2V tasks. Translation performance is quantitatively evaluated for image-to-label and label-to-image mapping tasks, with further analysis on different scene subsets. Ablation study experiments are conducted before comparing the proposed UVIT with other approaches, emphasizing style-conditioned translation and video interpolation loss effectiveness. UVIT utilizes an Encoder-RNN-Decoder architecture with a conditional decoder for style consistency in video translation. A 21-dimensional vector is used as the style latent variable to control subset style deterministically and generate various video sequences stochastically. Different subset labels and stochastic parts create diverse video sequences. The proposed conditional video translation mechanism in UVIT utilizes subset labels for deterministic controllable translation, but can still generate style consistent results in a stochastic manner. Ablation experiments show the effectiveness of the video interpolation loss in both image-to-label and label-to-image tasks, comparing UVIT with and without the loss, as well as with image reconstruction loss for training encoder-decoder architectures. UVIT utilizes subset labels for deterministic controllable translation and can generate style consistent results in a stochastic manner. Ablation experiments show the effectiveness of video interpolation loss in training encoder-decoder architectures. Semantic segmentation metrics are used to evaluate image-to-label results quantitatively, while Fr\u00e9chet Inception Distance is used for label-to-image task evaluation. The FID is calculated using features extracted from videos with a pretrained I3D model. The proposed UVIT model utilizes semantic labels for video generation and achieves good translation results. Video interpolation loss is crucial for achieving these results, incorporating temporal information for better video-to-video translation. Ablation studies validate the advantage of UVIT over competing methods in both image-to-label and label-to-image tasks. The results in Table 3 confirm the superiority of our method in preserving semantic information over other approaches. In the label-to-image task, we compare different methods' quality in translating video sequences, including our improved RecycleGAN. The proposed UVIT model outperforms RecycleGAN and 3DCycleGAN in FID results on 5 sub-domains. Subjective evaluation on AMT platform shows UVIT's effectiveness compared to other models. Video-level comparison shows UVIT surpasses RecycleGAN and 3DCycleGAN, achieving comparable results with supervised vid2vid model. UVIT outperforms RecycleGAN and 3DCycleGAN in FID results on 5 sub-domains and achieves comparable results with supervised vid2vid model. It shows better HPS in image-level comparison and produces content consistent video sequences. UVIT translates video sequences between different image subsets and datasets, with visual examples provided in the Viper dataset. In this paper, UVIT is proposed as a novel method for unsupervised video-to-video translation. It utilizes an Encoder-RNN-Decoder architecture to decompose style and content in videos for consistent translation. A video interpolation loss is designed for self-supervised training, showing effectiveness in producing realistic and consistent video translations without paired training data. The UVIT model achieves excellent results in multimodal video translation, preserving semantic information and consistency at both image and video levels. The Trajectory Gated Recurrent Units (TrajGRUs) actively learn location-variant structures in video data by generating local neighborhood sets for each location at each time. Two TrajGRUs are used to propagate inter-frame information in both directions in a shared content space. The network is trained on a single Titan Xp GPU for 3 to 4 days due to GPU memory limitations, with a batch size of one and 6 frames per clip. The batch size is one, with 6 frames per clip. Increasing frames per clip can improve model content dependency capture but requires more GPU memory. Style inconsistency of RecyceGAN is shown in figure 7. UVIT model outputs consistent segmentation labels, with comparisons in figures 8, 9, and 10. Label sequences to image sequences with multimodality are shown in figure 11. Qualitative analysis is conducted on Cityscapes and Viper scene video translation. The qualitative analysis on the translation between scene videos of Cityscapes and Viper dataset is presented in figure 12."
}