{
    "title": "BJDH5M-AW",
    "content": "Neural networks achieve high accuracy in various tasks but are vulnerable to adversarial examples, which are perturbed inputs causing misbehavior. These examples do not consistently fool classifiers in the physical world due to natural transformations. Standard techniques for generating adversarial examples require complete control over input, which is often impossible in real-world systems. The first method for creating real-world 3D objects that deceive neural networks across different angles and viewpoints is introduced. An algorithm is presented to generate robust adversarial examples that can withstand various transformations. This approach is demonstrated in two dimensions with images resistant to noise, distortion, and affine transformation. Additionally, the algorithm is applied to create physical 3D-printed adversarial objects, proving its effectiveness in real-world scenarios and highlighting the practical implications of adversarial examples for real-world systems. The existence of adversarial examples for neural networks has been a theoretical concern, but new research shows that physical-world adversarial examples can be reliably produced. A new algorithm creates 3D objects that deceive neural networks across different viewpoints, as demonstrated with a 3D-printed turtle consistently classified as a rifle. This highlights the vulnerability of neural network-based classifiers to physical-world adversarial examples. In this paper, the efficacy of generating physical-world adversarial examples is demonstrated by consistently classifying a 3D-printed turtle as a rifle. Previous techniques for creating robust adversarial examples have had limited success, highlighting the vulnerability of real-world systems to adversarial attacks. Current efforts to synthesize robust adversarial examples for the physical world have shown limited success, focusing on two-dimensional cases with nonstandard classifiers. Generating 3D adversarial objects that remain robust against complex transformations like rotations and perspective projection is a challenge. This work demonstrates the real threat posed by adversarial examples in the physical world and proposes a general-purpose algorithm for constructing reliable 3D adversarial objects. The algorithm proposed constructs robust 3D adversarial objects that are resistant to various transformations. It introduces the Expectation Over Transformation (EOT) algorithm, which generates single adversarial examples that are effective across a distribution of transformations. The approach successfully synthesizes adversarial objects in the physical world. The Expectation Over Transformation (EOT) algorithm is used to create adversarial objects that remain adversarial in the physical world. This approach demonstrates the real concern of adversarial examples in practical deep learning systems. The Expectation Over Transformation (EOT) algorithm addresses the issue of adversarial examples becoming non-adversarial under minor perturbations. It models perturbations within the optimization process by using a distribution of transformation functions. This approach aims to create adversarial examples that remain effective in real-world scenarios. The Expectation Over Transformation (EOT) algorithm aims to constrain the effective distance between adversarial and original inputs by modeling perceptual distortions through a distribution of transformation functions. It generalizes beyond simple transformations and finds examples robust under any perception distribution. The objective function can be optimized by stochastic gradient descent. The EOT framework allows for generating robust adversarial examples in 2D, 3D, and physical-world objects by applying transformations independently at each gradient descent step. The framework offers flexibility in the generation process, including the choice of transformation function, distance metric, and optimization method. In the 2D case, realistic distortions are approximated through random transformations, while in the 3D case, transformations map textures to perceived images through nonlinear functions like rendering, rotation, translation, and perspective distortion. In the 3D case, adversarial examples are transferred to the physical world through nonlinear transformations like rendering, rotation, translation, and perspective distortion. EOT requires differentiation through the 3D renderer with respect to the texture, achieved by using a sparse matrix multiplication between the texture and a coordinate map. This map maps coordinates on the texture to coordinates in the classifier's field of view. To optimize the induced objective function in EOT, a Lagrangian-relaxed form of the problem is used. Imperceptibility of generated images is encouraged by setting the 2 norm in the LAB color space for distance calculation. The Lagrangian formulation allows for sampling and estimation of the objective function. The Lagrangian formulation in EOT allows for intricate constraint of the search space using LAB distance at low computational cost. Projected gradient descent is used for optimization to produce transformation-tolerant adversarial examples in 2D and 3D, including complex shapes. Adversarial objects created remain effective regardless of viewpoint or real-world factors. In evaluating adversarial examples, different viewpoints are considered such as \"adversarial\", \"correct\", and \"misclassified\". The effectiveness of the adversarial example is assessed by examining the proportion of these viewpoints. Evaluation is done in 2D and 3D cases, as well as physical-world 3D adversarial objects. The virtual and physical cases present fundamentally different challenges. In the physical world, we approximate the distribution of transformations to create robust adversarial objects. This method works well in practice for producing objects that remain adversarial under the true physical-world distribution. See Appendix A for specific distribution parameters used in different cases. In experiments, TensorFlow's InceptionV3 classifier with 78.0% accuracy on ImageNet is used to synthesize adversarial examples using EOT. The 2 distance per pixel is measured between original and adversarial examples, along with classification accuracy and adversariality. Evaluation is done over a large number of transformations in simulation and manually-captured images in the physical world. In experiments, TensorFlow's InceptionV3 classifier with 78.0% accuracy on ImageNet is used to synthesize adversarial examples using EOT. The adversarial examples are evaluated over a large number of manually-captured images of adversarial objects taken from different viewpoints. Various transformations are considered, including rescaling, rotation, lightening or darkening, adding Gaussian noise, and in-bounds translation of the image. The approach is highly effective in producing robust adversarial examples, with an average adversariality of 96.4%. The text discusses the generation of 3D adversarial examples using EOT, optimizing texture to create adversarial renderings from different viewpoints. The distribution includes various transformations like camera distances, object rotation, and background colors. Expectation over transformations is approximated by batch mean loss, reusing up to 80% of batches but ensuring each batch has at least 8 new images. Parameters of the distribution are detailed in Appendix A. The text discusses using EOT to generate 3D adversarial examples with high adversariality. Results are summarized in TAB3 and visualized in FIG2, showing the effectiveness of the method. Physical-world adversarial examples require modeling beyond the 3D rendering process. In addition to modeling 3D rendering, physical-world phenomena like lighting effects and camera noise need to be considered. The 3D printing process, using full-color technology, introduces color accuracy variations that are modeled as printing errors. Various transformations under EOT are used to approximate these phenomena, including camera noise, lighting effects, and color inaccuracies. Physical adversarial examples are evaluated on 3D-printed objects, showing high adversariality. We evaluate physical adversarial examples on 3D-printed objects, including a turtle and a baseball. The objects are correctly classified as their true classes with 100% accuracy. Adversarial examples are generated using EOT, and the objects are shown to be strongly adversarial across various transformations. The EOT algorithm demonstrates efficacy in creating physical adversarial examples across various transformations, ensuring robustness in diverse environments. This is shown through qualitative analysis of adversarial objects like a turtle in different settings. The EOT algorithm successfully creates physical adversarial objects, overcoming color accuracy issues in 3D printing. It can produce robust adversarial examples with smaller perturbations, leading to semantically relevant misclassifications. State of the art neural networks are vulnerable to adversarial examples, where objects are often misclassified as semantically similar items. Researchers have developed methods for creating adversarial examples, such as L-BFGS and the Fast Gradient Sign Method. Researchers have developed various methods for creating adversarial examples, such as L-BFGS, the Fast Gradient Sign Method, and a Lagrangian relaxation formulation for the single-viewpoint case. Projected Gradient Descent is considered a universal first-order adversary, while universal adversarial perturbations have been shown to induce misclassification across different images. Preliminary experiments suggest that these perturbations are not inherently robust to transformations. In the first work on 2D physical-world adversarial examples, BID4 demonstrate the transferability of FGSM-generated adversarial misclassification on a printed page. Their results show the existence of 2D physical-world adversarial examples for approximately axis-aligned views, proving robustness to camera noise, rescaling, and lighting effects. BID9 develop a real-world adversarial attack on a state-of-the-art face. BID9 developed a real-world adversarial attack on a face recognition algorithm using eyeglass frames to cause misclassification in portrait photos. The algorithm creates robust perturbations by optimizing over a fixed set of inputs, while also enhancing printability using a color map. This approach differs from others by producing universal adversarial perturbations for portrait photos and addressing color inaccuracy differently. Our approach focuses on creating adversarial examples that are robust to color inaccuracy, especially in 3D-printed objects. Unlike previous methods limited to 2D cases, our approach does not require a large number of photos for preprocessing, making it more practical for physical-world systems. Our work demonstrates the practical threat of adversarial examples to neural network-based image classifiers. By utilizing EOT, we successfully create physical adversarial objects that maintain their classification as a chosen target class across various viewpoints. This approach is particularly useful for 3D rendering and printing, allowing us to fabricate three-dimensional adversarial objects using low-cost 3D printing technology. The study demonstrates the practical threat of adversarial examples to image classifiers by creating physical adversarial objects that maintain their classification as a chosen target class across various angles, viewpoints, and lighting conditions. The optimization process under the EOT framework produces robust adversarial examples. Specific parameters for 2D, 3D, and physical-world cases are provided, along with a random sample of 2D adversarial examples. The study demonstrates the threat of adversarial examples to image classifiers by creating physical adversarial objects that maintain their classification as a chosen target class. Specific parameters for 2D, 3D, and physical-world cases are provided, along with a random sample of 2D adversarial examples."
}