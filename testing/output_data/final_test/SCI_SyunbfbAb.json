{
    "title": "SyunbfbAb",
    "content": "FigureQA is a visual reasoning corpus with over one million question-answer pairs based on 100,000 synthetic scientific-style images. Questions cover relationships between plot elements and characteristics like maximum, minimum, area-under-the-curve, smoothness, and intersection. The corpus includes side data for training machine learning systems, such as numerical data used to generate figures and bounding-box annotations for plot elements. The FigureQA visual reasoning task involves training models to recognize patterns in scientific figures, which summarize valuable information like trends and proportions. Machine understanding of plots could assist in extracting knowledge from scientific documentation. Machine understanding of plots is valuable in science and artificial intelligence. FigureQA is a dataset with over one million question-answer pairs grounded in figures, designed to study comprehension and reasoning in machines. FigureQA is a dataset with 15 question types that address properties like magnitude, maximum, minimum, median, area-under-the-curve, smoothness, and intersections in various types of plots. The dataset provides reliable ground-truth answers and allows for greater control over task complexity. The FigureQA dataset includes 15 question types related to properties in plots, with reliable ground-truth answers. The corpus can be extended iteratively to increase task complexity and model performance, similar to curriculum learning. Additional annotation allows for defining new tasks beyond those introduced in the manuscript. The dataset is built using a two-stage generation process, sampling numerical data and using the Bokeh plotting library. Releasing the data aims to gauge interest in the research community and accelerate research in this direction. The Bokeh plotting library is used to create natural-looking figures and provide access to quantitative data. Bounding boxes are added to plot elements for supplementary information, which can aid in tasks like reconstructing data from figures. Annotations for bounding box targets may help in training attention mechanisms. The dataset generation process aims to facilitate research in this area. In Section 3, the FigureQA dataset and visual reasoning task are described. Section 4 evaluates neural baseline models, including a text-only LSTM model, LSTM model with CNN image features, and a Relation Network model. The RN achieves accuracies of 72.40% and 76.52% on the FigureQA test set with alternated color scheme and without swapping colors. The VQA challenge involves models answering natural-language questions about visual scenes, requiring vision, language, and common-sense knowledge. Researchers can use generation scripts to create variations of the data for machine learning tasks. Models often rely on linguistic priors instead of understanding visual content. To address the reliance on linguistic priors in models for visual question answering, Goyal et al. (2016) introduced the balanced VQA task with additional image-question-answer triples. Real-world images in VQA challenges can complicate visual-linguistic reasoning with common-sense concepts, which synthetic datasets like CLEVR BID6 and NLVR BID20 avoid by focusing on geometric objects and their arrangements. Machines need spatial and relational reasoning for visual understanding tasks, leading to advancements in neural models. FigureQA, like CLEVR and NLVR, aims to improve figure-understanding algorithms. Previous studies like BID17 focused on linguistic questions about figure data, with datasets like FigureSeer containing annotated figure images. The FigureSeer dataset contains 140,000 figures with real data plots. It involves reasoning about figure content and various detection tasks. Models need good OCR performance for tasks like detecting legends and labels. Different studies focus on tasks like recovering visual encodings from chart images and data extraction from figures using convolutional networks. The FigureQA dataset focuses on reasoning about figure content and provides rich bounding-box annotations for each figure along with underlying numerical data. It allows for training visual-linguistic models from scratch and offers a setting for dense supervision. The FigureQA dataset emphasizes dense supervision for intuitive figure understanding without relying on visualization pipeline inversion. Recent methods for VQA tasks employ neural encoder-decoder frameworks with pretrained CNNs like VGG or ResNet for visual encoding and object detectors for additional information extraction from images. In FigureQA dataset, pretrained object detectors and language encoders are used to combine image representations with language encodings for answering questions. The dataset contains 100,000 training images with 1.3 million questions and 20,000 images each in validation and test sets with over 250,000 questions. The dataset represents numerical data in five common figure types. The dataset in FigureQA contains numerical data represented in five common figure types: horizontal and vertical bar graphs, continuous and discontinuous line charts, and pie charts. Each figure is generated with a white background and plot elements are colored from a set of 100 colors. Question-answer pairs are generated for each figure based on predefined templates, with 15 question types comparing quantitative attributes. Questions focus on properties like maximum, minimum, median, roughness, and greater than/less than relationships, posed as binary choices between yes and no. The dataset in FigureQA contains numerical data in various figure types with question-answer pairs based on predefined templates. Parameters are used to ensure consistent, realistic plots with variation, drawing data values from uniform random distributions. Data shapes are constrained using common functions with perturbations, and data points are visually identified by color. FigureQA dataset uses 100 unique colors from the X11 named color set to visually identify data points. The colors are divided into two subsets (A and B) for training, validation, and testing sets to ensure unseen color-plot combinations are used. The \"alternated color scheme\" is used for training, validation, and testing sets. Various aspects during data generation are randomized, including legend placement, figure width, font sizes, and grid lines. Synthetic data parameters are defined in Table 1, with questions and answers generated based on figure source data. The dataset is filtered to balance yes and no answers for each question type, removing bias. Additional questions can be synthesized from the provided source data, making the dataset extensible. A roughness metric is used to measure curve smoothness for specific question templates. Figures are generated using Bokeh for its ease of use and expressiveness. We modified the Bokeh library's web-based rendering component to extract bounding boxes for figure elements encoded in RGB channels and saved in PNG format. Images are resized to 256 pixels on the longer side, padded with zeros, and augmented by randomly cropping back to 256x256 size. Training models use the Adam optimizer on cross-entropy loss with a learning rate of 0.00025. The text describes the process of resizing images to 256 pixels, padding with zeros, and randomly cropping back to 256x256 size. It also outlines two baseline models: a text-only model using LSTM and an MLP classifier, and a CNN+LSTM model with visual representation from a CNN with convolutional layers. The baseline models include a text-only model using LSTM and an MLP classifier, and a CNN+LSTM model with visual features extracted from an ImageNet-pretrained VGG-16 network. The CNN has four convolutional layers with ReLU activation and batch normalization, followed by a fully connected layer of size 512. The models were trained with different batch sizes and parallel workers for gradient computation. The BID16 neural module for relational reasoning uses object representations to compute relations between objects. The module is fully differentiable and implemented as MLPs. In FigureQA experiments, the architecture is modified for higher resolution input images. Random rotations for data augmentation are not used to prevent distortions. Object representations are provided by a CNN with a similar architecture to the previous baseline, but without the fully-connected layer. The BID16 neural module for relational reasoning uses object representations to compute relations between objects. The module is implemented as MLPs and takes input from a CNN output. The CNN output consists of 64 feature maps of size 8 \u00d7 8, each representing an \"object\". The module encodes the location of objects by concatenating row and column coordinates to the object representations. Object pairs are processed to produce a feature representation of their relation, which is then used to predict outputs. The MLPs in the module have multiple layers with ReLU units for processing relational features and overall relational representation. The relational reasoning neural module has two hidden layers with 256 ReLU units each, using dropout in the second layer. The model is trained with four parallel workers, computing gradients on batches of size 160. Training includes early stopping based on validation accuracy. The best model is evaluated on the test set. Training and validation accuracy are shown in FIG4. The results from testing a subset of 16,876 questions with 1,275 figures show that the Relational Network (RN) outperforms the CNN+LSTM model due to the relational structure of the questions. The human baseline also performs well. Performance comparisons are detailed in TAB3 by figure and question type. The questions focus on plot characteristics like extrema, area under the curve, smoothness, and intersection. The curr_chunk discusses the training of neural models on data related to plot characteristics and visual reasoning tasks. It highlights the need for more powerful models to reach human-level performance and outlines plans for future work, including testing model transfer and dataset expansion. FigureQA is a dataset that presents sample figures of different plot types along with question-answer pairs and bounding boxes. Human accuracy on the test set was measured, with editors achieving 91.21% accuracy compared to a baseline of 72.18%. Further analysis of the human results is provided. In a study by Santoro et al. (2017), human accuracy on different figure types was analyzed. People performed well on bar graphs but struggled with line plots, dot-line plots, and pie charts. Mistakes were common with pie charts having similarly sized slices. Dot-line plots had lower accuracy due to elements obscuring each other. Human accuracy varied across question types, with better performance on minimum, maximum, and greater/less than queries. Categorical figures had higher accuracy compared to continuous figures, with lower accuracy on median and curve smoothness questions. Median questions were challenging with a larger number of unordered elements in the plots. In a study by Santoro et al. (2017), human accuracy on different figure types was analyzed. People performed well on bar graphs but struggled with line plots, dot-line plots, and pie charts. Mistakes were common with pie charts having similarly sized slices. Dot-line plots had lower accuracy due to elements obscuring each other. Human accuracy varied across question types, with better performance on minimum, maximum, and greater/less than queries. Categorical figures had higher accuracy compared to continuous figures, with lower accuracy on median and curve smoothness questions. Annotators faced challenges in determining smoothness due to the difficulty of considering deviations in curves and the precision required for ground truth answers. An unknown answer option was provided for ambiguous cases, with only 0.34% of test questions answered as unknown. In an experiment, the performance of a Relation Network was evaluated with and without alternated color schemes. Annotators often selected \"unknown\" when two colors were difficult to distinguish or when plot elements were obscured. The experiment involved training the RN baseline with and without swapped colors, and comparing the results on test sets. The experiment involved training a Relation Network with and without swapped colors, comparing performances on test sets with different color assignments."
}