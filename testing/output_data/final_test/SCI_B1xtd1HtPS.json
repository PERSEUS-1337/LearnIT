{
    "title": "B1xtd1HtPS",
    "content": "The 3D capsule architecture for processing point clouds is equivariant with respect to rotation, translation, and permutation of input sets. It uses a novel 3D quaternion group capsule layer for disentangling geometry from pose, enabling joint object classification and orientation estimation. The dynamic routing procedure is connected to the Weiszfeld algorithm for robust pose estimation. Our architecture enables joint object classification and orientation estimation using capsules, validated on benchmark datasets. Recent trends aim to extend the success of 2D convolutional neural networks to the 3D domain for applications like shape retrieval, manipulation, and object modeling. However, adapting CNN architectures to 3D point clouds is challenging due to their irregularity and the complexity of transformations involved in 3D data. The proposed QE-Network is designed to process 3D point clouds and is equivariant to SO(3) rotations parameterized by quaternions. It ensures translation and permutation equivariance by efficiently covering SO(3) using local reference frames and capsule layers. The proposed architecture utilizes local reference frames to disentangle pose from object existence, enabling simultaneous rotation estimation and object classification. The network is fully equivariant to SO(3) rotations and uses classification error as the training cue, without explicit pose annotations or augmented rotations. The network utilizes local reference frames to disentangle pose from object existence, achieving rotation equivariant capsules. It reduces the space of orientations considered by using LRFs on points. The equivariance properties of the 3D network regarding the quaternion group are theoretically proven. Additionally, a connection is established between dynamic routing and Generalized Weiszfeld iterations for convergence. In this paper, the authors discuss the equivariance of point clouds under quaternions and define equivariant maps and networks. They experimentally demonstrate the capabilities of their network on 3D shape classification and orientation estimation. An equivariant network consists of layers that are equivariant maps, resulting in globally equivariant networks when stacked. Quaternions are preferred over 3-vectors due to avoiding singularities and redundancies. Quaternion computations are more efficient for GPU implementation. Quaternion q is an element of Hamilton algebra H1 with scalar and vector parts. Unit quaternions provide a stable way to represent object orientation on the unit sphere. Unit quaternions form a double covering group of SO(3) and are closed under non-commutative multiplication. Definition 4 introduces the linear representation of quaternions using matrix notation. The injective homomorphism T maps quaternions to orthonormal matrices satisfying specific conditions. The Hamilton product and group composition are linearized by T. Definition 5 defines a 3D surface as a differentiable 2-manifold in Euclidean space, while a point cloud is a discrete subset sampled from it. Definition 6 defines a local reference frame for a smooth point cloud as an ordered basis of orthonormal vectors in the tangent space. The tangent frame at a point x on a 3D surface is defined by an ordered basis of orthonormal vectors. Recent trends acknowledge ambiguity in determining directions and propose equivariant designs. A quaternion equivariant network processes local patches by mapping point sets to transformations for computing hypotheses. The curr_chunk discusses a dynamic routing procedure in a capsule network that computes hypotheses from input poses. It achieves disentanglement of rotation parameters by decoupling local reference frames. The method relies on the network's robustness for cases where assumptions do not hold. The orientation and activations in the capsules are equivariant and invariant, respectively. The text also mentions visualizing LRFs of an airplane object and the importance of guaranteed equivariances and invariances in disentangling orientation from representations. The text discusses extending capsule networks to achieve equivariance to general groups by using a manifold-mean and special aggregation. It introduces novel capsule layers operating on local reference frames parameterized by quaternions. The goal is to construct an SO(3)-equivariant 3D capsule network that yields invariant representations and equivariant rotations. To achieve equivariant layers on the group of rotations, a left-equivariant averaging operator is defined. The text introduces a geodesic distance and quaternion mean for rotations, defining an averaging operator that is invariant under permutations. The average quaternion is calculated through a maximization procedure, with properties that ensure equivariance to general groups. Our capsule architecture involves hierarchically sending local patches to a Qnetwork for classification and pose estimation. The transformations preserve geodesic distance and can be implemented batchwise. Group dynamic routing is achieved through iterative clustering to assign primary capsules to latent capsules. The iterative clustering assigns weighted group mean to output capsules based on distance between vote quaternion and cluster center. The routing procedure is a variant of the affine Weiszfeld algorithm for computing the geometric median. Proof follows from Weiszfeld iteration and mean/distance operators defined earlier. The dynamic routing algorithm is equivalent to solving normal equations using the IRLS scheme. The weighting function can be chosen freely as long as it is inversely proportional to geodesic distance and concave. The algorithm can be viewed as a clustering procedure with KL divergence regularization, leading to better routing algorithms. The convergence behavior can be analyzed using the Weiszfeld algorithm within a theoretical framework. The QE-Network architecture, analyzed within the theoretical framework, almost surely converges to a critical point. The input includes local points, rotations, and activations, with Nc representing the number of input capsule channels per point. The architecture is illustrated in Fig. 2 with corresponding pseudocode in Alg. 3 of suppl. material. The QE-Network architecture involves computing quaternion averages for initial pose candidates using pre-computed LRFs. A point-to-transform network maps points to transformations for dynamic routing, utilizing fully-connected layers for regression. This approach provides a continuous alternative for optimizing pose transformations. The QE-Network architecture computes quaternion averages for initial pose candidates using pre-computed LRFs. It utilizes fully-connected layers for regression to map points to transformations for dynamic routing, providing a continuous alternative for optimizing pose transformations. The output capsules are refined iteratively by routing by agreement, stacking QE-networks to increase the receptive field gradually. The architecture consists of two QE-networks processing 64 patches, with pooling centers in the first layer linked to their vicinity to compute intermediary capsules. The second layer routes output capsules from the first layer to final capsules for each class. The final layer combines all pooling centers into a single patch for the QE-network to generate C \u00d7 4 capsules. The network architecture includes two QE-networks processing 64 patches, with pooling centers in the first layer linked to their vicinity to compute intermediary capsules. A single QE-network acts on a single patch to create the final C \u00d7 4 capsules and C activations. Implementation details include using PyTorch, the ADAM optimizer with a learning rate of 0.001, and a point-transformation mapping network with two FC-layers of 64 hidden units. Classification involves using spread loss and rotation loss, with surface normals computed by local plane fits and the second axis of the LRF computed by FLARE. The network architecture involves two QE-networks processing 64 patches, with pooling centers in the first layer linked to their vicinity to compute intermediary capsules. A single QE-network acts on a single patch to create the final C \u00d7 4 capsules and C activations. FLARE is used to compute the second axis of the local reference frame. Other LRFs such as SHOT or GFrames can also be used. The algorithm Var avoids canonicalization within the QE-network and can handle NR/NR cases but not random SO(3) variations (AR). PPF-FoldNet uses point-pair-feature encoding for invariant input representations. Our equivariant version outperforms other methods in the scenario of NR/AR, including equivariant spherical CNNs, by at least 5%. Object rotational symmetries in the dataset contribute significantly to errors. The use of LRFs helps reduce the number of parameters in our network, making it more efficient. Our network can estimate both canonical and relative object rotations in 3D point clouds. Our network can estimate canonical and relative object rotations in 3D point clouds without pose-supervision. Evaluation was done on well-classified shapes from the ModelNet10 dataset. Multiple instances per shape were generated by transforming with five arbitrary SO(3) rotations. The QE-architecture estimates pose using the output capsule with the highest activation or a siamese architecture computing relative quaternion between maximally activated capsules. Results show improvement over baselines without data augmentation. The study compares results of different 3D alignment methods, including PointNetLK and IT-Net, against baselines like Mean LRF and PCA. Mesh input methods like Spherical CNNs are excluded, as well as rotation-invariant methods like Tensorfield Networks. IT-Net and PointLK require extensive training. The study compares 3D alignment methods like PointNetLK and IT-Net against baselines like Mean LRF and PCA. Both IT-Net and PointLK require extensive training, while the study focuses on the challenges posed by resampling and random rotations. The network's robustness against point and LRF resampling, as well as density changes in local neighborhoods, are key factors affecting the results. The study investigates the impact of resampling and random rotations on the network's performance. Ablation experiments using random resamplings on the ModelNet10 dataset show the network's robustness to changes in local reference frames (LRFs) and point density. PointNet's ability to process raw, unordered point clouds within a neural network is highlighted as a key advancement in deep learning on point sets. PointNet has been extended to increase local receptive field size, making it suitable for processing point-clouds as sets. Common neural network operators in this category are equivariant to permutations and translations but not to other groups. Early attempts at achieving invariant data representations involved data augmentation techniques, but recent efforts focus on theoretically equivariant or invariant representations. In recent years, there has been significant progress in the theory and practice of equivariant neural networks, focusing on achieving equivariance with respect to finite symmetry groups and discrete symmetries. Some approaches include group convolution, Steerable CNNs, and frequency domain filters. However, filters in the frequency space are less interpretable and expressive compared to spatial filters. Generalizing these ideas to achieve equivariance in 3D is possible. Achieving equivariance in 3D involves generalizing ideas from 2D to voxelized 3D data. However, methods using dense grids suffer from increased storage costs. Various approaches generalize harmonic basis filters to SO(3) using spherical harmonic basis. These methods require input to be projected to the unit sphere, posing challenges for unstructured point clouds. Other methods define a regular structure on the sphere to propose equivariant convolution operators for learning rotation equivariant representations of 3D shapes. In 3D shape representation, achieving rotation equivariance can be done by acting on the input data or network. Different methods like presenting augmented data, ensuring rotation-invariance, or enforcing equivariance in the bottleneck have been proposed. Vector Field Networks and 3D Tensor Field Networks are examples of approaches that embed equivariance in the network. The 3D Tensor Field Networks (TFN) achieve localized filters equivariant to rotations, translations, and permutations. Capsule Networks, introduced by Hinton et al., utilize dynamic routing by agreement for orientation handling. In this work, a new framework is presented for achieving permutation invariant and SO(3) equivariant representations on 3D point clouds using a variant of capsule networks. The network operates on a sparse set of rotations specified by input LRFs, consuming a compact representation of 3D rotations - quaternions. The network also establishes convergence results for Weiszfeld dynamic routing and has an explicit group-valued latent space. Our network has an explicit group-valued latent space that estimates the orientation of input shapes without supervision. However, performance is affected by shape symmetries, and the length of the activation vector depends on the number of classes. Despite reported robustness, computation of LRFs is sensitive to point density changes. Future work aims to establish invariance to direction in the tangent plane and apply the network in the broader context of 3D objects. The network aims to establish invariance to direction in the tangent plane and apply it in the context of 3D object detection under arbitrary rotations. The proof of Proposition 1 shows the invariance under permutations and transformations that preserve geodesic distance. The text discusses the concept of affine subspaces and orthogonal projections onto them, as well as the distance to affine subspaces. These definitions are crucial for constructing the connection between dynamic routing and the Weiszfeld algorithm. Distance from a point x to a set of affine subspaces can be calculated using quaternion mathematics. The unit quaternion represents the unit normal of a four-dimensional hyperplane, with an added term qd to adjust for the shift. The quaternion qi forms an affine subspace with dimension 4, and the projection operator is determined by the matrix I\u2212qq, which is an orthogonal projection matrix onto the affine subspace spanned by q. The quaternion mean suggested in Markley et al. (2007) is equivalent to the Euclidean Weiszfeld mean on affine quaternion subspaces. The L q -Weiszfeld Algorithm on affine subspaces minimizes a q-norm variant of the cost. When q = 2, the algorithm computes a non-weighted mean. When q = 2, the algorithm simplifies to computing a non-weighted mean. A closed form solution exists for Eq (29) given by the normal equations. The solution lies in nullspace M i and can be obtained through eigendecomposition, resulting in the unweighted Quaternion mean. Once Lemma 1 is proven, direct convergence results can be applied. Consider a set of points Y = {y 1 . . . y K } where K > 2 and y i \u2208 H 1. A ball B(o, \u03c1) encapsulates all y i, and D = {x \u2208 H 1 | C q (x) < C q (o)}, the region. The algorithm simplifies to computing a non-weighted mean when q = 2. A closed form solution exists for Eq (29) given by the normal equations, resulting in the unweighted Quaternion mean. The sequence produced by Eq (29) will converge to a critical point unless x t = y i for any t and i. The critical point is on one of the subspaces specified in Eq (18) and is a geometric median. The assumption H2 prevents convergence from any given point, which can be a problem for randomly initialized networks. However, in experiments, no issues with convergence of dynamic routing were observed. This result is an important first step in the analysis of DR. In the analysis of DR, different weighting choices for q : 1 \u2264 q \u2264 2 lead to varying forms of weights. The IRLS algorithm converges for a broader range of weighting choices if certain conditions are met. A simple sigmoid function is commonly used in practice. For estimating relative pose with supervision, a Siamese network variation is utilized, where latent capsule representations of two point sets X and Y contribute to pose regression. The shared network processes both point clouds to obtain latent representations, from which the rotation is computed. This approach allows for rotations to be disentangled into capsules. The algorithm of the network involves quaternion calculations for rotations disentangled into capsules. Results include local reference frames and multi-channel capsules. Evaluation was done on Modelnet40 and Modelenet10 datasets with specific splits for training and testing. Training dataset did not include random augmentation. During training, the dataset was not augmented with random rotations, trained with single orientation (NR). Testing involved generating multiple arbitrary rotations for each shape (AR). The confusion matrix for classification on ModelNet10 objects is reported, showing that categories with less rotational symmetry have higher accuracy. Errors distribution is detailed, including a histogram of errors in Fig. 8. In Fig. 8, the histogram of errors within quantized orientation ranges is shown. Our Siamese architecture excels in estimating object rotation. Results from our ablation studies, Ours-2kLRF and Ours-1kLRF, demonstrate robustness to varying point densities. Modifications to IT-Net and PointNetLK focus solely on predicting rotation. Data augmentation is not used in training our networks, unlike PointNetLK and IT-Net."
}