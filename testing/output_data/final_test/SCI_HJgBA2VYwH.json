{
    "title": "HJgBA2VYwH",
    "content": "Traditional set prediction models can struggle with simple datasets due to the responsibility problem. A pooling method for sets of feature vectors based on sorting features across elements of the set can be used to construct a permutation-equivariant auto-encoder that avoids this issue. This approach produces better reconstructions and representations on datasets like polygons and MNIST. Replacing the pooling function in existing set encoders with FSPool improves accuracy and convergence speed on various datasets. The problem of encoding and decoding sets of points presents unique challenges, with recent progress focusing on learning functions that can efficiently represent relations between elements in a set. The encoder transforms points into a latent space, while the decoder reconstructs the set from the latent space, requiring a method to handle the arbitrary order of elements in the target set. In this paper, a set pooling method for neural networks is introduced to address the encoding bottleneck and decoding failure issues in set prediction models. The authors identify the responsibility problem and introduce FSPOOL, a differentiable sorting-based pooling method for variable-size sets. This method aims to improve the modeling of even simple datasets like polygon reconstruction. FSPOOL is a differentiable sorting-based pooling method for variable-size sets, used in set auto-encoders to train with MSE loss for reconstruction without assignment-based loss. It enables learning polygon reconstructions with close-to-zero error and improves reconstruction quality and representation learning in a set version of MNIST. Additionally, in classification experiments on CLEVR and graph datasets, FSPool in a set encoder outperforms non-trivial baselines. Combining FSPool with Relation Networks enhances performance significantly in models relying heavily on it. The text discusses the challenges of predicting sets due to arbitrary element order and introduces an assignment-based loss solution. It compares predicted and ground-truth sets using matrices with feature vectors, aiming to measure set differences. The text introduces methods for producing a predicted set \u0176 from a target set Y using a multi-layer perceptron. It discusses linear assignment and Chamfer loss as ways to minimize total loss in set prediction, with the latter ensuring all points in the target set are covered. These methods are permutation-invariant functions that compare predicted and ground-truth sets based on feature vectors. Standard neural networks struggle with modeling symmetries that arise from different list representations of the same set, as highlighted in an example of training an auto-encoder on a polygon dataset with a square. When the square is rotated, the elements within the set are simply permuted, resulting in the same latent representation. When rotating a set of points, there is a discontinuity in how the outputs are assigned, leading to sudden changes in output responsibility. This highlights the challenge neural networks face in modeling symmetries arising from different list representations of the same set. When rotating a set of points, there is a discontinuity in how the outputs are assigned, leading to sudden changes in output responsibility. Neural networks struggle to model functions with discontinuous jumps, making it difficult to learn situations where all outputs must change simultaneously. This challenge becomes more pronounced as the number of vertices in the polygon increases. Our experiment in subsection 6.1 confirms that whenever there are at least two set elements that can be smoothly interchanged, discontinuities arise. This issue is formally shown in Appendix A. For example, in object detection, the set of bounding boxes can be interchanged similarly to the points in our square. Traditional object detectors like Faster R-CNN do not face this problem due to their anchor-based approach. Our pooling method involves sorting each feature across the set elements and performing a weighted sum, ensuring permutation invariance. The difficulty in determining weights for a weighted sum in a way that works for variable-sized sets is addressed by storing the permutation in the encoder and applying the inverse in the decoder. This makes the auto-encoder permutation-equivariant, avoiding discontinuities in outputs during rotations. The model is described for encoding fixed-size sets and extended to variable-sized sets. The FSPOOL model for variable-sized sets involves encoding fixed-size sets in subsection 4.1, extending to variable-sized sets in subsection 4.2, and discussing usage in an auto-encoder in subsection 4.3. The goal is to produce a single feature vector invariant to column permutation in the matrix by sorting features numerically within the rows of X. The FSPOOL model aims to create a feature vector invariant to column permutation by sorting features within rows of X. SORT(\u00b7) sorts a vector in descending order to ensure independence of features. Deep Learning frameworks like PyTorch offer efficient parallel implementations of SORT using bitonic sort. Gradients can still be propagated pathwise despite the non-differentiability of the sorting permutation. A learnable weight matrix W \u2208 R d\u00d7n is then applied to X elementwise. The FSPOOL model creates a feature vector invariant to column permutation by sorting features within rows of X. A learnable weight matrix W \u2208 R d\u00d7n is applied to X by elementwise multiplying and summing over the columns. This weight vector allows different weightings of different ranks and is a generalization of both max and sum pooling methods. It is potentially more flexible in what it can represent and can handle varying sizes of sets. The weight vector in each row is used to parametrize a piecewise linear function for variable-sized sets. A weight vector w \u2208 Rk is used to interpolate between points based on a ratio r \u2208 [0, 1]. Each feature has a different weight vector placed in the rows of a weight matrix. The weight matrix W is used to parametrize a piecewise linear function for variable-sized sets. The pooled representation y has a potentially varying set size n as input. Setting k = 20 without tuning it for most experiments. The decoder in the auto-encoder reverses the operations of the encoder, unpooling and unsorting the features. The auto-encoder uses permutation-equivariant feature vectors to restore the original ordering of set elements. In the decoder, differentiable sorting networks are used to avoid discontinuities caused by changes in sorting order. The auto-encoder utilizes differentiable sorting networks based on the recently proposed method by Grover et al. (2019) to maintain permutation-equivariant feature vectors. This allows for a fully differentiable model, although it comes with increased computation costs. The temperature of the sorting can be decayed during training to improve efficiency, and the original order can be restored using the inverse permutation from the encoder. This approach makes the auto-encoder similar to a U-net architecture (Long et al., 2015). The proposed differentiable function maps feature vectors to a single vector, inspired by Deep Sets and PointNet models. This approach addresses issues with large latent spaces in the U-net architecture. The Deep Sets model is used to approximate any set function, but struggles with higher-order interactions. Relation networks (RN) excel at capturing interactions by summing over all pairs of elements. Our work proposes an alternative operator to relation networks (RN) for modeling relations between elements in sets. The use of sorting in set learning literature is considered for permutation-invariance, but our approach sorts each feature individually to avoid discontinuities. Our approach in set learning literature involves handling variable set sizes without discontinuities through featurewise sort and continuous weight space. Other methods include truncating to a fixed-size set by computing scores for each element and keeping elements with the top-k scores. Outside of set learning, rank-based pooling in CNNs has been used, where the rank is converted into a weight. In set learning literature, various methods are used to handle variable set sizes, such as featurewise sort and continuous weight space. Different approaches include truncating sets to a fixed size by computing scores for each element and keeping elements with the top-k scores. Additionally, rank-based pooling in CNNs converts rank into a weight for improved modeling functions and robustness to adversarial examples. The model uses the gradient of the set encoder, closely related to FSUnpool. Two auto-encoder experiments are conducted, followed by tasks replacing pooling with FSPool in an established model. Experimental details are in Appendix H, with code for reproducibility provided. The dataset involves auto-encoding regular polygons to test the responsibility problem, with set sizes of increasing powers of 2. The encoder includes a 2-layer MLP, FSPool, and a 2-layer. The encoder and decoder in the model consist of 2-layer MLPs and FSPool. The model is trained to minimize mean squared error. Results show that the latent space is necessary for training. The auto-encoder can reconstruct point sets accurately for set sizes up to 128, while the baseline model struggles with high reconstruction error for fewer points. The model struggles with learning datasets with 8 or fewer points, regardless of changes in latent size or loss function. The baseline fails completely at 16 points, showing discontinuous jump behavior. In contrast, the model can fit the dataset easily due to the responsibility problem. Moving on to auto-encoding MNIST images as sets of points, each pixel above the mean level is considered part of the set with x-y coordinates as features. The set size varies between examples and is 133 on average. Gaussian noise is added to the points in the set for training the denoising auto-encoder. The model architecture is the same as on the polygon dataset. Baseline models include sum/mean/max pooling encoders with MLP/LSTM decoders trained with the Chamfer loss. Results show comparison of the FSPool-FSUnpool model against the best baseline. Our FSPool-FSUnpool model outperforms the baseline in reconstructing digits, avoiding errors like turning 5s into 8s. Additionally, we can classify MNIST sets using a 2-layer MLP classifier, testing the informative nature of learned representations. Results show that the FSPool-FSUnpool auto-encoder model outperforms baseline models in classifying sets, even with fixed encoder weights. The model achieves better performance after just 1 epoch compared to baseline models after 10 epochs. Training the FSPool model takes 45 seconds per epoch on a GTX 1080 GPU. The model takes 45 seconds per epoch on a GTX 1080 GPU, slightly more than baselines with 37 seconds per epoch. CLEVR is a visual question answering dataset where the task is to classify an answer to a question about an image showing scenes of 3D objects. The dataset is used with ground-truth state description as input. Comparison is made against relation networks, Janossy pooling, and regular pooling functions. A tuned implementation shows 2.6% better accuracy than the original RN paper. In comparison to baselines, the FSPool model achieves the best accuracy with fewer epochs, reaching 99% accuracy in 5.3 hours. Despite tuned hyperparameters, relation networks do not offer any advantage. The learned functions of FSPool are more complex than simple sums or maximums. FSPool uses complex functions to capture more information about the set than other pooling functions. Experiments were conducted on various graph classification datasets from the TU repository, including bioinformatics and social network datasets. The task is to classify the whole graph into multiple classes using the GIN graph neural network as a baseline model. FSPool utilizes complex functions to gather more set information than other pooling methods. Results from experiments on graph classification datasets show FSPool outperforms the standard GIN model in test accuracy and convergence speed. Zhang et al. (2019a) further develop this concept for predicting sets from images. In this experiment, the benefit of using the RN + FSPool set encoder is quantified. By replacing FSPool with sum or max pooling, it was found that the RN encoder with FSPool outperforms the others in predicting bounding boxes or state descriptions in challenging tasks. This suggests that replacing sum with FSPool can improve standard Relation Networks for challenging tasks. The study introduced FSPool as a solution for predicting sets in auto-encoders, showing improved reconstructions on point cloud datasets. However, the limitation lies in the inability to create a generative set model due to the decoder's reliance on the encoder. Despite this, leveraging the auto-encoder for better representations and pre-trained weights can still be advantageous. Additionally, replacing the pooling function with FSPool in classification models yielded better results and faster convergence. The study introduced FSPool as a solution for predicting sets in auto-encoders, showing improved reconstructions on point cloud datasets. FSPool consistently learns better set representations at a small computational cost, leading to improved results in downstream tasks. Theorem 1 provides a formal treatment of the responsibility problem resulting in discontinuities in set functions. The theorem discusses discontinuities in set functions when a fixed permutation is chosen for a set representation, contrasting it with the permutation-equivariance in the model's approach. Results from various model and training loss combinations are presented in Table 3, Table 4, and Table 5. FSPool with direct MSE training loss outperforms the baseline with linear assignment or Chamfer loss on all evaluation metrics. For set sizes of 16 or greater, other combinations perform as well as the random baseline due to outputting a constant set regardless of input. In the default MNIST setting, the sum pooling baseline has a lower Chamfer reconstruction error than the model, despite visually worse outputs in Figure 3, highlighting a weakness of the Chamfer loss. The model's use of normal MSE loss helps avoid this weakness. The model trained with normal MSE loss avoids the weakness of the Chamfer loss, which struggles with sets containing duplicates or near-duplicates. This leads to higher Chamfer loss when minimizing MSE compared to directly minimizing Chamfer loss, despite potentially worse qualitative results. Our model, trained with normal MSE loss, outperforms baselines in predicting an additional \"mask feature\" for each set element, improving test Chamfer loss results. The model's structure allows it to distinguish padding elements efficiently during training. Our model's structure efficiently distinguishes padding elements during training, leading to superior classification accuracy on MNIST compared to baselines. Results show that our model, with 3820 parameters and without complex features like dropout or batch norm, achieves similar accuracy to larger models. The FSPool model achieves high accuracies of around 99% for \u03c3 = 0.00 on CLEVR. The learned piecewise linear functions show various shapes, including variants of max pooling with weights close to 0 for most ranks and a large non-zero weight on either the maximum or minimum value. These functions may have a stronger tendency towards 0 values due to weight decay. The experimental setup uses the same datasets and node features as GIN without cherry-picking. The node features used in the study are similar to those in GIN, without cherry-picking. Different graph pooling methods are compared based on their performance on various datasets, with hyperparameters selected based on best validation accuracy. In GIN, hyperparameters are selected based on best test accuracy, which can lead to variations in results. For example, selecting based on best test accuracy can improve the average result on the PROTEINS dataset from 73.8% to 77.1%. FSPool with k = 5 consistently outperformed k = 20 in an experiment. Results show that FSPool has slightly better accuracies than the strong baseline and reaches its highest validation accuracy in fewer epochs. The GIN-FSPool model shows significant improvement in RDT datasets, particularly with large numbers of nodes per graph. The comparison between GIN-Base and GIN-FSPool is crucial, as the pooling method is the only differing factor. Proper hyperparameter selection in GIN-Base implementation leads to better results compared to GINBase*. Results show that the FSPool-based RN encoder outperforms baselines, especially in harder datasets like state prediction. The representation of DSPN-RN-FSPool is strong, allowing for improved predictions with more iterations. Code to reproduce experiments is available. FSPool and its unpooling version with k = 20 were used for most experiments without major differences observed when changing k on CLEVR. The model is initialised with W as a matrix of all 1s to match the sum pooling baseline on CLEVR and graph classification datasets. Polygons are centred at 0 with a radius of 1, and points are randomly permuted. A batch size of 16 is used for training with 10240 steps. The Adam optimizer with a learning rate of 0.001 is employed, and weights are initialized as suggested in Glorot & Bengio (2010). The model's layers are initialized as suggested in Glorot & Bengio (2010), with hidden layer size set to 16 and latent space set to 1. Different hidden and latent space sizes were tested, with training on MNIST for 10 epochs. Results are from the test set, with pixels above a mean level of 0.1307 included. Linear assignment loss was not included due to convergence issues. Latent space increased to 16 and hidden layer size to 32, with other hyperparameters unchanged. The hidden layers are increased from 16 to 32, with all other hyperparameters remaining the same as for the Polygons dataset. The architecture and hyperparameters are based on a third-party open-source implementation. For the RN baseline, the set is expanded into pairs by concatenating feature vectors. Janossy Pooling baseline uses a model configuration with \u03c0-SGD and an LSTM with neighbourhood size |h|. The question representation from a 256-unit LSTM is concatenated to all elements in the set and processed by a 4-layer MLP with 512 neurons in each layer and ReLU activations. The GIN architecture utilizes a 4-layer MLP with 512 neurons in each layer and ReLU activations. Feature vectors are pooled using a sum pooling method and processed with a 3-layer MLP with hidden sizes 512, 1024, and the number of answer classes, along with a dropout rate of 0.05. Adam optimizer is used with a starting learning rate of 0.000005, doubling every 20 epochs until reaching 0.0005. Weight decay of 0.0001 is applied, and the model is trained for 350 epochs. The GIN architecture uses a 4-layer MLP with 512 neurons and ReLU activations. Feature vectors are pooled and processed with a 3-layer MLP. The model is trained with Adam optimizer, starting learning rate of 0.000005, weight decay of 0.0001, and trained for 350 epochs. In contrast, the model with Linear-BN-ReLU-Linear-BN-ReLU had slightly worse validation results. The outputs of 5 blocks are concatenated and pooled, followed by BNLinear-ReLU-Dropout-Linear classifier with softmax output. Torch-geometric library was used to implement the model with specific hyperparameters. The dataset has a hidden size of 64 and is trained for 250 epochs. A batch size of 100 was used for social network datasets due to GPU memory limitations. Best hyperparameters are selected based on average validation accuracy in 10-fold cross-validation. The architecture and hyperparameters are based on a third-party implementation, with the only change being the pooling in the RN. The input image is encoded with a ResNet-34 and additional convolutional layers to obtain a feature vector. The image feature vector is decoded into a set using the DSPN algorithm, which involves encoding an intermediate set with a set encoder and performing gradient descent on it. The set encoder creates pairs of sets, processes them with a 2-layer MLP, and updates the intermediate set with the gradient during training. The model is trained to minimize linear assignment loss using the Adam optimizer for 100 epochs with a learning rate of 0.0003."
}