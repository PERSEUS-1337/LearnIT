{
    "title": "SkOb1Fl0Z",
    "content": "Automated architecture search simplifies the process of designing neural architectures, but existing methods have limitations in flexibility and components for recurrent neural networks (RNNs). A domain-specific language (DSL) is proposed for automated architecture search, allowing the creation of novel RNNs with various depths and widths, including non-standard components. Two candidate generation techniques, random search with a ranking function and reinforcement learning, are used to explore the novel architectures produced by the RNN DSL for language modeling and machine translation. Automated architecture search simplifies designing neural architectures, with a focus on RNNs. The space of usable RNN architectures is larger than assumed, and developing novel architectures is crucial for AI advancements. Automated methods aim to find optimal model architectures for tasks, but ensuring results similar to human intuition is challenging. Various methods, including hyperparameter optimization and novel architecture generation, have been explored in automating machine learning processes. Automated methods for architecture search in RNNs face challenges due to the large search space and limited choice of operators. A meta-learning strategy is proposed to include novel operators in the search process, with three stages outlined. The candidate architecture generation function uses a flexible DSL to create potential RNN architectures without constraints on size or complexity. Automated methods for architecture search in RNNs face challenges due to the large search space and limited choice of operators. A meta-learning strategy proposes a three-stage process for generating candidate architectures using a flexible DSL. The ranking function processes each architecture's DSL with a recursive neural network to predict performance, while an evaluator compiles promising candidates to executable code for training on specific tasks. Architecture-performance pairs are used to improve the generator and ranking function. In this section, a domain specific language (DSL) is described for defining recurrent neural network architectures. The DSL allows for a broader modeling search space compared to previous methods. It includes core operators such as unary, binary, and ternary operators for defining standard RNN architectures like GRU and LSTM in a human and machine-readable manner. The DSL described allows for defining RNN architectures using core operators like MM, Mult, and Gate3. These operators are applied to source nodes including input vectors, output of the RNN, and optional long term memory. The Gate3 operator is necessary for architectures like GRU. The DSL enables defining standard RNN cells such as tanh RNN and more complex examples like the GRU. The DSL allows for defining RNN architectures using core operators like MM, Mult, and Gate3. It supports standard RNN cells and more complex examples like the GRU, which includes an additional hidden state for long term memory. The DSL can be extended to extract values from specific nodes by numbering them. The expanded DSL for defining RNN architectures includes various standard and non-standard architectures like GRU, LSTM, MGU, QRNN, NASCell, and simple RNNs. It also introduces new operators and trigonometric curves to enable the design of novel architectures. The DSL for defining RNN architectures includes standard and non-standard architectures like GRU, LSTM, MGU, QRNN, NASCell, and simple RNNs. It introduces new operators and trigonometric curves for novel designs. PosEnc adds positional encoding at each timestep, optimizations include LayerNorm and SeLU activations. The DSL can be compiled to code for initialization and runtime. Candidate architectures are generated, filtered, and evaluated to prevent multiple representations. Architecture growth starts from the output node ht. Representations for equivalent architectures are constructed incrementally, starting from the output node ht. Operators are selected and added to the computation graph, with limits on the tree height enforced by selecting source nodes when necessary. An intelligent agent is preferred for constructing architectures, as it can focus on promising directions in the search space. The proposed approach involves using a tree encoder and an RNN to represent and select nodes in the architecture. The tree encoder is an LSTM applied recursively to nodes, while the RNN predicts action scores for operations. Both components are trained jointly using the REINFORCE algorithm. A target token is introduced for architectures with empty nodes. The proposed approach involves using a tree encoder and an RNN to represent and select nodes in the architecture. A target token is introduced to indicate the next node to be selected. Training a ranking network through regression on architecture-performance pairs approximates the full training of a candidate architecture, allowing for a richer representation of transitions between hidden states. Human experts can inject previous best known architectures into the training dataset. The proposed approach involves using a tree encoder and an RNN to represent and select nodes in the architecture. A ranking function constructs a recursive neural network that reflects candidate RNN architecture nodes. Source nodes and operators are represented by learned vectors and functions, with the final output minimizing the difference between predicted and real performance. TreeLSTM nodes are used for operators in the tree. The vector representation of source nodes is crucial for accurately representing hidden states. The proposed approach involves using a tree encoder and an RNN to represent and select nodes in the architecture. To improve the representation of source nodes, unrolling the architecture for a single timestep can be done. This allows for a better understanding of the operations applied to produce hidden states. The evaluation of the architecture generation was done on language modeling (LM) and machine experiments. In two experiments, language modeling (LM) and machine translation (MT), our architecture generation was evaluated. Each experiment was limited to one combination of generator components due to computational requirements. For LM, random search was used with a learned ranking function, while for MT, the RL generator was used without a ranking function. Evaluation was done on the WikiText-2 dataset, constructing a two-layer c-RNN for proposed novel RNN cells with aggressive gradient clipping. The ranking network weights were trained using regression on architecture-perplexity pairs. The ranking network was trained using regression on architecture-perplexity pairs with specific restrictions on generated architectures, such as limiting nodes to 21 and preventing stacking of identical operations. Operations for Gate3 were also constrained to meet RNN requirements. The architecture filtering process involves preventing the stacking of identical operations like two sigmoid activations, two ReLU activations, or two matrix multiplications in succession. Valid subgraphs containing recurrent connections and three or more nodes are queried to generate new candidate architectures using random search guided by a learned ranking function. Up to 50,000 candidate architecture DSL definitions are considered. The architecture search process involves generating up to 50,000 candidate architecture definitions using a random generator. These architectures are then evaluated by a ranking network, with up to 32 selected for further training based on their perplexity scores. Additionally, new architectures were introduced during the search process after evaluating 750 valid architectures. The architecture search process involves generating candidate architectures using a random generator and evaluating them with ranking network. The top performing cell BC3 was an unexpected layering of two Gate3 operators, breaking with many human intuitions regarding RNN architectures. The BC3 architecture features a non-conventional design with multiple matrix multiplications, gates, and activations before producing the output. It does not have a masking output gate like LSTM, resembling GRU outputs. Despite its unconventional design, BC3 performs well and uses fewer parameters compared to other models. The highly tuned AWD-LSTM BID20 and skip connection LSTM BID19 outperformed the Recurrent Highway Network BID35 and NASCell on the Penn Treebank. The RL agent generated candidate architectures without a ranking function, focusing on local knowledge. The generator was pre-trained with intuitive priors before evaluating the constructed architectures. The study focused on enforcing well-formed RNNs with moderate depth restrictions and evaluating multiple architectures in parallel. The generator prioritized functioning architectures and considered different placement options for the memory gate. The study focused on evaluating various architectures in parallel, using the Multi30k English to German BID8 machine translation dataset. The baseline experiment utilized an attentional unidirectional encoder-decoder LSTM architecture with specific hyper-parameters. The generator in the study utilizes a learning rate of 0.2, input feeding, and stochastic gradient descent. The learning rate decreases by 50% when validation perplexity does not improve, and training stops when it drops below 0.03. The generator predominantly relies on core DSL operators in early batches, with limited usage of extended DSL operators due to instability. The generator in the study initially relies on core DSL operators but later successfully incorporates a wide variety of extended DSL operators. It takes substantial training time for the generator to understand how to build robust architectures. Despite the complex and unknown reward function, the generator finds 806 architectures outperforming the LSTM, with the best architecture achieving a test BLEU score of 36.53 compared to the LSTM's 34.05. The study found that the generator discovered architectures that outperformed the LSTM, with the best achieving a test BLEU score of 36.53. The architectures incorporated a variety of operators and showed potential for unexplored combinations in successful RNN architectures. Top five architectures were tested on the IWSLT 2016 dataset, consisting of 209,772 sentence pairs. The training set from the IWSLT 2016 dataset BID6 includes 209,772 sentence pairs from transcribed TED presentations. Architectures that outperform LSTM on the Multi30k dataset are analyzed for operator frequency. The study reveals that even less commonly used operators like sine curves and positional encoding contribute to successful architectures. Model loss and BLEU scores on Multi30k and IWSLT'16 MT datasets are compared in Table 3. The study analyzed architectures that outperformed LSTM on the Multi30k dataset, but did not transfer well to the larger IWSLT dataset. It suggests that architecture search should be conducted on larger datasets or evaluated across multiple datasets for general architectures. The correlation between loss and BLEU scores was found to be suboptimal. The correlation between loss and BLEU scores is not optimal, with architectures excelling in loss sometimes performing poorly on BLEU. It is important to use more accurate metrics for evaluating model quality. Previous work by Zoph and Le (2017) used a policy gradient approach to search for neural architectures. The slot filling approach for binary tree nodes is limited in flexibility compared to other techniques like DSL. Various reinforcement learning methods have been used for designing neural network architectures, including CNNs with Q-learning and neuroevolution techniques like NEAT and HyperNEAT. These methods have been extended to evolve weight parameters and structures of networks, such as producing non-shared weights for LSTMs and evolving network structures. The study introduces a flexible domain-specific language for defining recurrent neural network architectures, allowing for novel combinations using core and unstudied operators. The resulting architectures, not following human intuition, perform well on tasks, suggesting a larger space of usable RNN architectures. Component-based concept for architecture search is introduced, with two approaches: ranking function-driven search and Reinforcement Learning agent. The study introduces a domain-specific language for RNN architectures, optimizing matrix multiplications for faster running speed. Different DSL specifications can result in equivalent RNN cells. This optimization simplifies LSTM's matrix multiplications, improving GPU utilization and reducing CUDA kernel launch overhead. In optimizing RNN architectures, matrix multiplications are streamlined by applying specific equivalences and sorting arguments based on commutativity. Training models use stochastic gradient descent with a learning rate of 20, continuing for 40 epochs. During training, stochastic gradient descent (SGD) is used with an initial learning rate of 20, which is divided by 4 if validation perplexity does not improve. Dropout is applied at a rate of 0.2 to word embeddings and outputs. Aggressive gradient clipping (0.075) is performed to prevent exploding gradients. Failed architectures with perplexity over 500 are terminated, but still used as training examples for the ranking function. The TreeLSTM nodes have a hidden size of 128 for the ranking function. During training, SGD is used with an initial learning rate of 20, which is divided by 4 if validation perplexity does not improve. Dropout is applied at a rate of 0.2 to word embeddings and outputs. Failed architectures with perplexity over 500 are terminated but still used for the ranking function. The TreeLSTM nodes have a hidden size of 128 for the ranking function, with L2 regularization of 1 \u00d7 10 \u22124 and dropout on the final dense layer output of 0.2. Lower perplexity architectures are sampled more frequently. Proper unrolling replaces xt with xt\u22121 and xt\u22121 with xt\u22122, but the ranking network performed better without these substitutions. Baseline experiments dictate the models generated, as seen with BC3 not being discovered if standard regularization techniques were used. Variational dropout BID10's application to BC3 frames the importance of hyperparameters. Variational dropout BID10 is applied to LSTM cells, specifically on ht\u22121, not ct\u22121, to prevent loss of information in the long term hidden state ct. In BC3, applying variational dropout to ht\u22121 in the final gating operation would result in permanent information loss, so it is only applied to the ht\u22121 values in the gates f and o. This decision is justified by the observation that architectures using ht\u22121 directly would be disadvantaged otherwise. Hyperparameters for the Penn Treebank BC3 language modeling results were mostly kept the same as the baseline AWD-LSTM model. The model was trained for 200 epochs with specific hyperparameters such as a learning rate of 15, batch size of 20, and BPTT of 70. Variational dropout and embedding dropout were applied, along with specific dimensions for word vectors and hidden layers. BC3 used 3 layers with weight drop, activation regularization, and temporal activation regularization. Gradient clipping was set to 0.25. Finetuning was done for additional epochs. Parameters for WikiText-2 BC3 and Penn Treebank experiments were kept equal. Hyperparameters for Penn Treebank GRU were the same as BC3 PTB experiment. The GRU language modeling results included hyperparameters such as a hidden size of 1350, weight drop of 0.3, learning rate of 20, and gradient clipping of 0.15. The model ran for 280 epochs with 6 epochs of finetuning. For WikiText-2, the model ran for 125 epochs with 50 epochs of finetuning. The architecture involved tokenizing and embedding operations into vectors, applying LSTM to these vectors and result vectors of node's children. The same LSTM was used for every node but its hidden states were reset between nodes. The reward for architecture performance is computed based on validation loss and rescaled using a soft exponential function. A specific reward function is used to keep rewards between zero and 140. Prior to generator pre-training, a list of priors is established. Prior to generator pre-training, a list of priors is established for the generator. The hidden state progression over time for different generated architectures is explored, showing substantial differences in activations. Ensembles of heterogeneous architectures may be effective due to variations in capturing, storing, and processing input features. Visualization of the hidden state and generator progress over time is shown, highlighting switches between exploitation and exploration strategies. Only valid architectures are considered. The architecture search was conducted for 5 days on AWS using 28 GPUs. The best architecture was found after 40 hours, with the generator consistently creating well-performing architectures with more training."
}