{
    "title": "r1gOn7h9UH",
    "content": "Deep convolutional networks often use bias terms for richer functional mappings and training facilitation. However, bias terms in CNNs interfere with interpretability, hinder performance generalization, and prevent analysis with linear-algebraic tools. Bias-free CNNs are locally linear and offer interpretations of network functionality in terms of low-dimensional subspaces. Denoising is a key application in statistical estimation, with the Wiener filter being the classical solution assuming a Gaussian signal model. BF-CNNs generalize well and achieve high performance even beyond trained noise levels, connecting learning methods to traditional denoising techniques. The Wiener filter is the optimal estimator for denoising images by shrinking high-frequency components more aggressively than lower-frequency ones. More powerful solutions based on multi-scale transforms were developed in the 1990s, allowing for denoising through nonlinear thresholding operations. In the 2000's, algorithms were developed to project noisy input onto a lower-dimensional subspace containing signal content, eliminating noise. State-of-the-art models incorporated data-driven perspectives, learning sparsifying transforms or nonlinear shrinkage functions. Recently, convolutional neural networks have surpassed previous methods in performance by using cascades of filters and nonlinearities to represent powerful functions. Convolutional neural networks have achieved impressive results in blind denoising, but the denoising mechanisms they implement are not well understood. The benefits of specific architecture aspects like skip connections and batch normalization are unclear, making it challenging to isolate their contributions to denoising performance. This work focuses on the use of bias terms in CNNs. Bias terms in CNNs interfere with interpretability, do not improve performance, and hinder generalization to noise levels not seen during training. Bias-free CNNs are locally linear, allowing for direct analysis with linear-algebraic tools and robust generalization to noise levels beyond training range. The denoising problem involves finding a function that estimates the original image from noisy observations. The denoising problem involves finding a function that estimates the original image from noisy observations using feedforward neural networks with rectified linear units (ReLUs) that are piecewise affine. The denoising neural network implements a single affine transformation with bias terms depending on ReLU activation patterns. Removing bias terms results in a strictly linear network, revealing noise-removal mechanisms. This analysis is illustrated using a Denoising CNN architecture. The denoising neural network, including DnCNN, utilizes adaptive filters to estimate denoised pixels by averaging over noisy pixels without blurring across edges. The filters adapt to image structure and are diverse, tailored to local features. The linear structure of a BF-CNN allows analysis of its functional capabilities through singular value decomposition of the Jacobian matrix. The SVD of a BF-CNN on natural images shows most singular values close to zero, indicating the network discards most of the input image. The preserved subspace's effective dimensionality is measured by computing the total noise variance in the denoised image. Weighting functions used in computing pixels vary in shape and are adapted to image content. The denoised image from the BF-CNN has a symmetric Jacobian, projecting the noisy signal onto a low-dimensional subspace tailored to the input image. The subspace preserves most of the input image's norm, as visualized by the singular vectors capturing input features. The BF-CNN implements an approximate projection onto an adaptive signal subspace that preserves image structure and suppresses noise. The dimensionality of the signal subspace decreases as noise level increases, with nested subspaces containing richer image features at lower noise levels. The BF-CNN implements an adaptive signal subspace to preserve image structure and suppress noise. The denoising performance is explained by the signal subspace containing the clean image. The mean squared error scales proportionally to the noise variance, implying a linear relationship between input PSNR and denoised image PSNR. Generalization across noise levels is investigated by comparing networks with and without bias. BF-CNNs are constructed based on various Denoising CNNs, removing all sources of additive bias. The BF-CNN implements an adaptive signal subspace to preserve image structure and suppress noise. The denoising performance is explained by the signal subspace containing the clean image. The mean squared error scales proportionally to the noise variance, implying a linear relationship between input PSNR and denoised image PSNR. Generalization across noise levels is investigated by comparing networks with and without bias. BF-CNNs are constructed based on various Denoising CNNs, removing all sources of additive bias. The CNN performs poorly at high noise levels, while the BF-CNN generalizes robustly even beyond the training range. The CNN performs poorly at high noise levels, while BF-CNN performs at state-of-the-art levels even beyond the training range. Alternative architectures yield similar results."
}