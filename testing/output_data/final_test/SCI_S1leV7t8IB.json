{
    "title": "S1leV7t8IB",
    "content": "The text discusses the development of 'dynamic neural relational inference' (dNRI) to understand the functional organization of cortical microcircuits from large-scale recordings of neural activity. The method uncovers dynamic relations between neurons more reliably than existing baselines, helping to extract latent temporal dynamics in complex networks and predict their behavior. Various machine learning methods have been used to encode/decode behavior from recorded activity of large neuronal populations, but existing models often lack the temporal dynamics of firing activity and interactions between neurons. The text introduces 'dynamic Neural Relational Inference' (dNRI) to uncover dynamic interactions in neural networks for understanding brain computations. Various methods like principal components analysis, ConvNMF, SeqNMF, and LFADS have been proposed for low-dimensional latent representations of neural network activity. However, existing models often lack explicit modeling of temporal correlations between individual neurons, hindering functional connectivity reconstruction of neural circuits. The dNRI extension aims to address this limitation by explicitly modeling dynamic relations between neurons. The 'dynamic Neural Relational Inference' (dNRI) model aims to extract rapid dynamic changes in network activity by capturing functional relations between recorded neurons. It enables tracking of temporal evolution of functional connectivity, providing insights into hidden brain information flows and circuitry. The method is demonstrated on synthetic spiking data and live mouse cortex recordings to recover dynamic information flow between neurons. The 'dynamic Neural Relational Inference' (dNRI) model aims to estimate whether spiking of one neuron excites or suppresses another neuron's spiking at different time points. Neural spiking information is represented by matrices, and the goal is to predict connections between neurons at each time step. The model follows the NRI formulation and uses a variational auto-encoder to learn the relationships between neurons. The dNRI model uses a variational auto-encoder to learn dynamic connections between neurons based on neural spiking patterns. The encoder estimates the probability of connections between neurons at different time points using LSTM-based deep nets. The dNRI model utilizes LSTM-based deep nets to encode likely interaction patterns between connected neurons at a specific time. Sampling from a concrete distribution allows for backpropagation of gradients from the decoder to the encoder parameters. The decoder reconstructs input spike trains based on sampled edges from the approximate posterior using an LSTM-based deep net. Additionally, a separate RNN models each neuron's influence based on predicted edges. The RNNs in the model take masked spiking probabilities as input to represent predicted edges. A Bernoulli prior is used to encourage sparsity in edge modeling, with a probability of no edge set above 0.5 to reduce spurious predictions. Different priors were used for synthetic and real data experiments. Training involves optimizing parameters of the decoder and encoder. The decoder and encoder parameters are trained using spiking data. The encoder predicts the approximate posterior for each latent variable, samples from the distribution, and predicts spiking activity using the decoder. Ground-truth spikes are used for training, while predictions are used during testing. The efficacy of dNRI is demonstrated using synthetic datasets with simulated neurons and baseline spiking rates. The second type of data involves spiking activity of 24 neurons recorded from a mouse navigating in a tactile virtual reality. The proposed approach is compared to four baselines using metrics like F1 score and reconstruction error. The dataset is split into train, validation, and test sets for training and tuning hyperparameters. The train, validation, and test sets are used to tune hyperparameters for the model. Baselines like Tensor Component Analysis (TCA) and SeqNMF are compared based on their ability to predict edges in neural activity data. The study compares different models for predicting neural activity data. Baselines like TCA and SeqNMF are evaluated, but dNRI stands out for accurately recovering dynamic connections in the data. The study evaluates various models for predicting neural activity data, with dNRI standing out for accurately recovering dynamic connections. In real-world data analysis, the focus is on a choice period between 400ms and 950ms, showcasing the last stage of sensory information processing before motor action. Neurons are ordered by cortical depth and assigned to specific layers, with sparse correlations revealed by dNRI focusing on rapid, transient connections. Results show information flow from L2/3 to L5B neurons and within deep L5A and L5B. Other models like SeqNMF, GLM, and TCA fail to capture these fast transient features. The dNRI model can capture fast transient features in neural spiking data, including predicting active connections between neurons. This allows for time-dependent functional relations to be extracted from large-scale cortical network recordings, providing additional analysis capabilities compared to static techniques. The proposed approach accurately recovers implanted interactions in large-scale neural spiking data recordings of cortical networks, outperforming baselines that model relations implicitly."
}