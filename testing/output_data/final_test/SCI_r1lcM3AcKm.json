{
    "title": "r1lcM3AcKm",
    "content": "Training RNNs on long sequences with BPTT is challenging, but adding an unsupervised loss term can improve effectiveness. This paper introduces an architecture with private and shared hidden units to mitigate the negative impact of unsupervised loss on the main supervised task. The proposed framework divides the RNN hidden space into private and shared spaces for supervised and unsupervised tasks, respectively. Extensive experiments show performance gains in modeling long sequences, where BPTT struggles with transmitting gradients effectively due to exploding or vanishing gradients. Recent work suggests that architectures like LSTM and GRU RNNs with parameterized gates can improve gradient flow for long sequences. Evidence shows that learning supervised and unsupervised tasks simultaneously can enhance an RNN's ability to capture long-term dependencies. Injecting unsupervised tasks locally along the sequence can provide reliable gradients to optimize RNNs for long sequence learning tasks effectively. This strategy involves employing semi-supervised learning architectures with two types of RNNs for primary supervised and auxiliary unsupervised tasks. The challenge addressed is coordinating supervised and unsupervised tasks in semi-supervised learning. One common approach is to weight the loss functions with varying coefficients, but this method has limitations. The second approach in coordinating supervised and unsupervised learning involves specifying a training order and separating them into different learning phases. This method first pre-trains a model under unsupervised setting before using it for supervised learning. While this approach can provide valuable auxiliary knowledge, there is no guarantee of synchronous learning. The approach of coordinating supervised and unsupervised learning involves pre-training a model under unsupervised setting before using it for supervised learning. It is crucial to determine how auxiliary unsupervised tasks can best serve the main supervised learning task for capturing long-term dependencies. Clockwork RNNs divide the representational space into different groups to model long-term dependencies effectively. The proposal suggests enabling two RNNs to have a shared feature space for both supervised and unsupervised tasks to solve the long-term dependency problem. The proposed architecture addresses the long-term dependency problem by allowing two RNNs to share a feature space for both supervised and unsupervised tasks. This design enables the RNNs to exchange features efficiently and introduces an inductive bias for faster training and evaluation. The architecture includes a private space dedicated to the supervised task, as shown in FIG0-(c) for gradient flow visualization. The proposed architecture introduces a private-and-shared feature space for semisupervised sequential learning tasks, motivated by gradient flows. It has shown significant improvement in modeling long-term dependencies compared to other algorithms. Sections 3, 4, and 5 detail the method, experiments, and analysis. Previous studies have shown that RNNs with unsupervised loss outperform TCNs in accuracy on long sequences. RNNs with an auxiliary unsupervised loss outperform TCNs in accuracy on long sequences. Unsupervised learning is often used in pre-training, leading to significant improvements in natural language understanding tasks. BID14 propose RNN-AE as an auxiliary unsupervised task to aid RNNs in handling long sequences. Our method, unlike skim-RNN, is designed for long sequence learning with an unsupervised loss. It uses multiple RNNs instead of just one at each time-step, making it easier to train without relying on reinforcement learning algorithms. Our method introduces an inductive bias for different hidden sub-spaces in the RNN, making it easier to train. We discuss our version of RNN-AE and its differences with previous methods. Additionally, we add unsupervised tasks for local sequence reconstruction and prediction at anchor points within the input sequence. Our method introduces an inductive bias for different hidden sub-spaces in the RNN, making it easier to train. In our implementation, we evenly divide the input into sub-sequences for anchor points, ensuring reconstruction spans most of the input sequence. The r-RNN reconstructs the local sequence backward, and we include unsupervised tasks for local sequence reconstruction and prediction at anchor points. Our method introduces an inductive bias by dividing the hidden space of the main RNN into task-specific sub-spaces. This disentangles the feature space to prevent learned features from unsupervised tasks affecting the entire state vector used for the supervised task. By creating an uncontaminated region solely learned through the supervised task, specific features for the supervised tasks are captured more effectively. Our method divides the hidden space of the main RNN into task-specific sub-spaces to prevent unsupervised task features from affecting the supervised task. This allows for more effective capture of specific features for the supervised tasks. Experiments on image classification and sentiment analysis datasets show the effectiveness of our proposed method compared to traditional RNNs. Our method divides the hidden space of the main RNN into task-specific sub-spaces to prevent unsupervised task features from affecting the supervised task. Experiments on image classification and sentiment analysis datasets show the effectiveness of our proposed method compared to traditional RNNs. In our experiments, we grid search hyperparameters using a validation set and use SGDR BID9 as the optimizer. Early stopping with patience of 50 epochs is incorporated to avoid overfitting. Our proposed method outperforms previous competitive results on MNIST and CIFAR-10. In our experiments, hyperparameters for BID14 are grid searched for fair comparison. Our models perform well on various datasets with better accuracy and fewer parameters than BID14. The effect of key hyperparameters like private-vs-shared proportion is studied, leading to the question of optimal shared space. Next, we delve into this inquiry in section 5.1. In section 5.1, two analyses on the private-shared structure are presented. The impact of shared proportion on model outcomes is examined, along with visualization of private and shared hidden spaces. Experiments on shared proportions are conducted on CIFAR-10, showing varying share percentages from 0% to 100% and their effects on classification tasks. The model's accuracy improves as the shared percentage increases, peaking at 50% where half of the state is shared with the auxiliary RNN. However, performance starts to worsen after this point, possibly due to overfitting despite a fixed main RNN capacity. Increasing the hidden state for unsupervised tasks impacts model outcomes. The auxiliary RNN's capacity increases with more hidden state for unsupervised tasks, leading to a rise in parameter numbers. The auxiliary test loss decreases progressively, indicating no overfitting. Performance differences are attributed to the shared proportion, with a significant drop in classification rate at 100% shared. This highlights the importance of disentangling the hidden space for optimal model performance. The importance of disentangling the hidden space through a private-shared framework is further supported. Analyzing the hidden vectors of the main RNN with a 50% shared proportion and a hidden space size of 16 reveals how the framework influences representation learning. By visualizing the hidden space through collecting hidden state vectors after receiving an MNIST image as input, we can observe how the dimensions of the hidden state change over time. The hidden state vectors of the main RNN with a 50% shared proportion and a hidden space size of 16 are analyzed to understand representation learning. The n th dimension of the hidden state is observed over time by reshaping each row into a 28 \u00d7 28 image. Visualizations show how the hidden state changes in response to input as training progresses, with shared dimensions initially displaying similar responses to private dimensions. As training progresses, shared dimensions in the hidden state vectors initially show similar responses to private dimensions. However, over time, the shared representation evolves to contain more abstract features while the private representation retains the original response. This suggests that the original response may be more crucial for the supervised task than the unsupervised task. Additionally, a dimension of the hidden state with a large non-zero value when the input is non-zero helps propagate important information through time steps in the RNN. In this paper, a semi-supervised RNN architecture with private and shared hidden representations is presented. The architecture allows information transfer between supervised and unsupervised tasks in a novel way, leading to performance gains in experiments on benchmark datasets. The shared representation evolves to contain abstract features, while the private representation retains the original response, suggesting its importance for the supervised task. Our proposed architecture combines gradient and information flow in architectures with shared and private representations, leading to faster training and evaluation compared to alternatives. The architecture introduces an inductive bias for modules handling auxiliary tasks to have fewer parameters."
}