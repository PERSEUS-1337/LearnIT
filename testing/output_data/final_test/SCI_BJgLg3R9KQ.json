{
    "title": "BJgLg3R9KQ",
    "content": "Most recent advances in visual recognition have come from incorporating attention mechanisms in deep convolutional networks (DCNs). These networks learn where to focus using weak supervision from image labels. A study introduced a large-scale experiment (ClickMe) to add human-derived attention maps to ImageNet. Human psychophysics confirmed that these top-down features are more useful for image categorization than bottom-up saliency features. Enhancing an attention network with ClickMe supervision improved accuracy and generated more effective visual features. Attention in deep learning has been a focus of research, with ClickMe supervision improving accuracy and generating more interpretable visual features. While human attention is diverse and task-driven, computer vision attention networks are optimized for object recognition, relying on weak supervisory signals for learning. In investigating how human supervision impacts deep convolutional networks (DCNs), attention models in human vision are explored. Global and local features play a role in guiding visual processing to task-relevant image locations, with visual saliency driving a separate form of attention. This research aims to understand the effects of explicit human supervision on DCNs' performance and interpretability. Visual saliency is a task-independent encoding of feature conspicuity in a visual scene, aiding in filtering clutter from object detection processes. Research focuses on predicting human eye fixations and integrating attention modules in trainable DCNs, with feature-based and spatial attention being key components. The current study combines spatial and feature-based attention into a single mask to improve performance on visual tasks like question answering and captioning. The goal is to co-train an attention network with human supervision to enhance vision systems. The study combines spatial and feature-based attention to enhance vision systems by co-training an attention network with human supervision. Online games are used to collect high-quality human ground-truth data, with a new game introduced where a human player collaborates with a DCN to discover important information. The study introduces a new game where a human player collaborates with a DCN to discover important visual features for recognition at ImageNet scale. A gamification procedure was used to collect nearly half a million \"top-down\" attention maps over several months, reflecting human recognition strategies better than previous methods. The study introduces a new game to collect top-down attention maps reflecting human recognition strategies better than previous methods. It describes a large-scale online experiment ClickMe.ai used to supplement ImageNet with nearly half a million attention maps. Additionally, it introduces the global-and-local attention (GALA) module, which improves accuracy on ILSVRC12, and shows that incorporating ClickMe supervision into GALA training leads to even larger accuracy gains. By supplementing ImageNet with ClickMe attention maps, the study aims to develop network architectures that are more accurate and interpretable. ClickMe.ai is a large-scale effort to gather attention annotations for training neural network models, using a two-player game to collect attention maps for object images. This approach addresses the challenge of scaling up attention map collection beyond a few hundred images. ClickMe.ai is a single-player game developed to support large-scale data acquisition by having participants recognize images from the ILSVRC12 challenge. Players use a mouse cursor to \"paint\" image parts that are most informative for recognizing the category. This method prevents sparse salt-and-pepper types of maps and encourages careful monitoring of annotations. The game ClickMe.ai involves players identifying important image parts by painting them with translucent bubbles on the screen. A partner then tries to recognize the object based on these bubbled regions, with each round lasting until recognition or 7 seconds. The game aims to be entertaining and fast-paced, with larger bubbled regions shown to ensure quick recognition by the partner. Keeping a partner in the loop discourages players from randomly bubbling unrelated areas in the image. The game ClickMe.ai involves players identifying important image parts by painting them with translucent bubbles on the screen. Players aim to help a Deep Convolutional Network (DCN) recognize objects by bubbling relevant areas in the image. Data collection efforts on ClickMe.ai drew 1,235 participants who played an average of 380 images each. In total, over 35M bubbles were recorded from 1,235 participants playing 380 images each. ClickMe maps were created on 196,499 unique images from ILSVRC12 dataset. The maps highlighted local image features and specific object parts, showing biases towards facial components for animal categories and front-oriented bias for inanimate objects. The experiment focused on comparing top-down ClickMe features with bottom-up image saliency for object recognition. The design followed a rapid visual categorization paradigm, aiming to validate the effectiveness of ClickMe features in object recognition. Participants in the study were divided into two groups, one viewing images masked according to ClickMe maps and the other according to bottom-up saliency. They were tested on 40 animal and 40 non-animal images, presented either intact or with a phase-scrambled mask revealing important visual features. The study divided participants into two groups, one viewing images masked based on ClickMe maps and the other on bottom-up saliency. They were tested on animal and non-animal images with phase-scrambled masks revealing important visual features. Preprocessing of attention maps ensured spatial continuity for pixels covering important visual features, using a \"stochastic\" flood-fill algorithm. Different versions of each image revealed varying percentages of important pixels. The GALA module combined local saliency with global contextual information. The GALA module combines local saliency with global contextual signals to guide attention in object recognition. It uses separate local and global operators to derive attention masks integrated into the activity. Optionally, it can be supervised by ClickMe maps with an additional loss function. The experiment measured how increasing the proportion of important visual features from ClickMe or Salicon attention maps influenced human behavior. Human observers reached ceiling performance with 40% of ClickMe features visible, while images masked according to Salicon required up to 63% visibility for similar performance. These results validate that ClickMe features are distinct from bottom-up saliency and are sufficient for human object recognition. The GALA block is a circuit for learning complex combinations of local saliency and global contextual modulations, supervised by ClickMe maps. It modulates input layer activity with an attention mask capturing global and local forms of attention. Global attention in GALA is based on the SE module. The GALA block utilizes global attention based on the SE module to calculate summary statistics per channel and apply a multilayer perceptron for non-linear transformations, enabling complex dependencies to be learned between channels. This involves a two-step process of shrinking and expanding the dimensionality of the feature maps. The GALA block uses global attention with the SE module to compute channel summary statistics and apply a multilayer perceptron for non-linear transformations. It involves shrinking and expanding feature map dimensionality, along with a local saliency module for computing local feature attention maps. The outputs from both pathways are integrated to generate the attention volume. The GALA block integrates parameters for additive and multiplicative combinations of attention activities. It tiles attention activities to combine them, using a tanh activation function for the GALA module. This allows for dis-inhibition of bottom-up feature activations and flipping of individual unit signs. The GALA approach integrates parameters for additive and multiplicative combinations of attention activities, allowing for dis-inhibition of bottom-up feature activations and flipping of individual unit signs. GALA modules were applied to specific feature layers in ResNet-50, enhancing visual features similar to object-parts highlighted in ClickMe maps. The attention activity maps had a size of 14\u00d714, and were combined with residual layer activities to adjust the amount of attention applied. Incorporating the GALA module into ResNet-50 offers a small improvement over SE-ResNet-50. GALA is more beneficial on smaller datasets with attention supervision. Co-training GALA with ClickMe maps enhances model performance and interpretability by introducing an additional loss function. Incorporating the GALA module into ResNet-50 offers a small improvement over SE-ResNet-50, especially on smaller datasets with attention supervision. Co-training GALA with ClickMe maps enhances model performance and interpretability by introducing an additional loss function that optimizes object classification and predicting ClickMe maps from input images. The loss is scaled by a hyperparameter \u03bb, and ClickMe maps are resized to match GALA module activity tensors. We evaluated our approach for supervising GALA with ClickMe maps by partitioning the dataset into separate folds for training, validation, and testing. Approximately 5% of the dataset was set aside for validation and another 5% for testing, with the rest used for training. Each split contained exemplars from all 1,000 ILSVRC12 categories. Training and validation splits were used to optimize the GALA training routine, while the test split was reserved for evaluating model performance and interpretability. The trade-off between maximizing object categorization accuracy and predicting ClickMe maps was investigated. The study analyzed the hyperparameter \u03bb to improve object categorization and ClickMe map prediction. A value of \u03bb = 6 was found to be optimal for training GALA-ResNet-50. Model performance was compared on the test split of the ClickMe dataset, showing high classification accuracy and the ability to explain human ClickMe map variability. The GALA-ResNet-50 model outperformed ResNet-50 and SE-ResNet-50 in object classification accuracy. Models incorporating attention were better at predicting ClickMe maps than the baseline ResNet-50. Training GALA-ResNet-50 with ClickMe maps improved classification performance and explained human ClickMe map variability. ClickMe maps were effective for co-training GALA, emphasizing specific object parts. The study tested whether a GALA module is necessary for model improvement with ClickMe supervision. GALA-ResNet-50 trained with ClickMe maps outperformed controls, showing that ClickMe maps enhance GALA performance. However, applying ClickMe maps directly to model feature encoding did not improve performance. Additionally, testing on a subset of the training set showed that ClickMe maps could still enhance model performance. The GALA-ResNet-50 model trained with ClickMe supervision outperformed other models on the ILSVRC12 validation set. By including ClickMe supervision in training, the model highlighted features that were more consistent with human observers' identifications. The GALA-ResNet-50 model trained with ClickMe supervision highlighted object parts like facial features in animals and car details. Attention maps showed focus on important visual features or segmenting objects from the background, even in cluttered or occluded scenes. The GALA-ResNet-50 model trained with ClickMe supervision shows better interpretability of attention maps compared to the model trained without ClickMe. It selects more similar features to human observers and generalizes to object images not found in ILSVRC12, even localizing foreground object parts in Microsoft COCO 2014 BID21. The GALA-ResNet-50 model trained with ClickMe supervision demonstrated higher interpretability of attention maps compared to the model without ClickMe. This was measured by calculating the intersection-over-union (IOU) of attention maps with ground truth object segmentation masks from COCO, showing significantly better results (0.26 IOU vs. 0.03 IOU, p <0.001). The model was able to generalize to object categories not present in ILSVRC12, localizing foreground object parts in Microsoft COCO 2014 BID21. The ClickMe dataset supplements ImageNet with human-derived attention maps, validated for rapid visual categorization. GALA-ResNet-50 with ClickMe supervision shows interpretable attention maps, emphasizing animal parts and ignoring vehicle parts. Without ClickMe supervision, attention is distributed and less interpretable. Participants viewed masked images with ClickMe map locations, reaching high recognition accuracy with only 6% visibility. In contrast, images masked with saliency map locations required full visibility for high performance. ClickMe.ai offers insights into human vision beyond traditional saliency measures, with potential for analyzing feature selection mechanisms. The study also extended the squeeze-and-excitation module for improved architecture in ILSVRC17. In the ILSVRC17 challenge, an SE-ResNet-50 architecture was trained on reduced data and showed overfitting compared to a standard ResNet-50. A novel GALA module was introduced, leading to a GALA-ResNet-50 model that significantly increased accuracy and reduced top-5 error by 25%. Co-training GALA using ClickMe supervision improved object recognition by cueing the network to focus on diagnostic image regions. Trade-offs were found between learning visual representations similar to human observers and those optimal for ILSVRC, resulting in a balanced model. Recent advancements in DCNs have led to models with better classification accuracy and more interpretable visual representations. Qualitative differences in visual strategies employed by DCNs suggest the need for improved training regimens to encourage more human-like representations. DCNs lack mechanisms for perceptual grouping and figure-ground segmentation, but training with ClickMe map supervision shows promise in bridging this gap. The lack of explicit mechanisms for perceptual grouping and figure-ground segmentation in DCNs leads to more distributed representations compared to humans. Novel training paradigms using visual cues like depth and motion could substitute for human supervision. ClickMe.ai hosted 25 contests to drive traffic and reward top-scoring players with gift cards from February 1st, 2017, to September 24th, 2017. Participants in the ClickMe.ai contests were rewarded with gift cards for their performance. They were given usernames to track their progress and could play multiple game rounds. More than 90% of participants played more than one image. The ClickMe dataset recorded over 35M bubbles, creating 472,946 ClickMe maps on 196,499 unique images. Around 5% of images were excluded due to poor quality or incorrect labels. ClickMe maps highlighted local image features, with animal categories often focusing on facial components. The ClickMe dataset recorded over 35M bubbles, creating 472,946 ClickMe maps on 196,499 unique images. The maps for animal categories are oriented towards facial components, even for animals like snakes. Inanimate objects show a front-oriented bias, focusing on distinguishing parts like engines and wheels. The attention maps displayed strong regularity and consistency across participants, with an average inter-participant reliability of \u03c1 = 0.58 (p <0.001) for 10,000 different images. The study found a strong average inter-participant reliability of \u03c1 = 0.58 (p <0.001) in the ClickMe dataset, indicating that participants' bubbled features during gameplay were very similar. The \"Fraction of human ClickMe map variability\" was calculated based on the similarity between model's feature attention maps and humans. A null inter-participant reliability was also derived, with an average correlation of \u03c1 r = 0.18 across 10,000 randomly paired images, further confirming the observed reliability. ClickMe was inspired by the Clicktionary game BID22, where two human partners identify visual features. ClickMe validated its game mechanics by using the same set of 10 images as Clicktionary, showing high correlation (\u03c1 r = 0.59, p <0.001). Participants did not prioritize visual features important to their DCN partners over other humans. Participants in the study did not prioritize visual features important to their DCN partners over other humans. They played fewer rounds than the number of object categories in ClickMe, were not aware of how their clicked regions were revealed to their partners, and did not show learning effects over a shorter timescale. The study found that participants were equally accurate in their first and second sets of trials. A \"stochastic\" flood-fill algorithm was used to reveal important image regions based on attention scores, ensuring participants could see key parts without foveating. The similarity between human-selected visual features in ClickMe was greater than between humans and DCN features. Participants in the psychophysics experiment viewed images masked by either ClickMe or Salicon saliency maps, with each unique exemplar shown only once in a randomly selected masking configuration. The total number of pixels in the attention maps for each image was equalized between the two types of maps. Original images were sampled from 4 target and 4 distractor categories. In each trial, participants viewed a sequence of events and had 550ms to view the image and press a button to judge its category. Participants in the psychophysics experiment were given 550ms to view images and categorize them by pressing a button. Feedback was provided for response times outside the time limit. The experiment included a training phase and feedback after each of the five experimental blocks. The trials were implemented using the psiTurk framework and custom javascript functions. Each trial sequence was converted to an HTML5-compatible video format for fast presentation on the web. Videos were preloaded before each trial to optimize experiment timing in a web browser. Stimulus timing was verified to be accurate within 10ms. Images were sized at 256 \u00d7 256 pixels. Two participant groups viewed images with parts revealed based on ClickMe maps or Salicon-derived salience. Statistical testing compared group performance at different levels of image reveal. In experiments comparing ClickMe vs. Salicon groups, a null distribution was created by randomly switching participants' group memberships to assess differences in accuracy. ClickMe maps were blurred before training, and models were trained for 100 epochs with the best validation accuracy weights selected. In experiments, models were trained for 100 epochs with weights selected for best validation accuracy. Models were implemented in Tensorflow, trained \"from scratch\" with weights from a normal distribution. SGD with Nesterov momentum and a piece-wise constant learning rate schedule were used. Different devices were used for training, with varying batch sizes due to memory constraints. Bicubic interpolation on GPUs was replaced with bilinear interpolation on TPUs. Reference implementation can be found at https://github.com/serre-lab/gala_tpu. In experiments, models were trained for 100 epochs with weights selected for best validation accuracy. Bicubic interpolation on GPUs was replaced with bilinear interpolation on TPUs. Investigated trade-off between object categorization accuracy and predicting ClickMe maps. Systematic analysis over different values of hyperparameter \u03bb. Attention maps derived from networks as feature column-wise L2 norms of activity from final layer of GALA or SE attention. Model attention map similarity with ClickMe maps measured with rank-order correlation. Five models trained at each \u03bb value for 100 epochs. In experiments, models were trained with weights optimized for validation accuracy. Object categorization and ClickMe map prediction improved with \u03bb = 6. GALA-ResNet-50 trained with ClickMe maps enhanced object classification performance. Incorporating ClickMe maps in the loss significantly improved predicting ClickMe maps and classification accuracy. Model accuracy was measured after training on various \u03bb values, showing a large improvement in predicting ClickMe maps and classification accuracy. The GALA-ResNet-50 model outperforms vanilla ResNet-50 and ResNet-50 with SE attention on the ILSVRC12 dataset. It excels in predicting human attention maps and reduces error by 25% when co-trained with ClickMe maps. The GALA-ResNet-50 model, trained with ClickMe maps, outperformed other models in classification performance on the full ILSVRC12 dataset. Fine-grained annotations in ClickMe maps were found to be important for supervising GALA attention. The GALA-ResNet-50 model, trained with ClickMe maps, outperformed other models in classification performance on the full ILSVRC12 dataset by providing useful information about attention supervision resolution. ClickMe maps also directly supervised feature learning in residual networks, showing comparable performance to a normal ResNet-50 but less accurate than GALA-ResNet-50 with ClickMe maps. The GALA-ResNet-50 model, trained with ClickMe maps, showed better accuracy than the GALA-ResNet-50 without ClickMe maps in interpreting attention maps. This was tested on a subset of images from the Microsoft COCO 2017 object detection challenge, focusing on animal and vehicle categories. Each image was resized and passed through both models to extract attention activities for visualization. The GALA-ResNet-50 model, trained with ClickMe maps, displayed more accurate attention maps compared to the model without ClickMe maps. The attention maps highlighted important features like animal parts and vehicle components, while the model without ClickMe maps showed less interpretable results. The attention maps were visualized by setting attention columns to their L2 norm and resizing them to 480x640 pixels. The GALA-ResNet-50 model, trained with ClickMe maps, produced more accurate attention maps highlighting animal and vehicle parts. The interpretability of the attention maps was evaluated using an intersection-over-union (IOU) score with the image's COCO segmentation mask. The GALA-ResNet-50 model trained with ClickMe maps improved interpretability by selecting animal or vehicle parts over background locations. Evaluation on 2,055 COCO images showed a significant difference in interpretability compared to a model without ClickMe supervision. The GALA-ResNet-50 model trained with ClickMe supervision shows visual features more similar to human observers than a model without such supervision. ClickMe maps highlight important object parts, with a preference for local features in the former. The GALA modules of the ClickMe model exhibit more interpretable object and part-based attention compared to the vanilla model."
}