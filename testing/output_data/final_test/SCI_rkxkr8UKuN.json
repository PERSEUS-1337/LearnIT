{
    "title": "rkxkr8UKuN",
    "content": "Modern generative models aim to match target distributions in the data space, but the discrepancy in dimensionality can make training difficult. The proposed perceptual generative autoencoder (PGA) maps distributions to a latent space using an autoencoder's encoder, improving training by matching distributions in the latent space. PGA can be combined with maximum likelihood or variational autoencoder (VAE) objectives to train generative models, allowing for sharper sample generation compared to vanilla VAE. PGA can produce sharper samples than vanilla VAE. While GANs have been popular, they face challenges like unstable training and mode collapse. Efforts have been made to address these issues, with non-adversarial approaches like VAEs, reversible generative models, and WAEs gaining attention. However, non-adversarial methods have limitations such as blurry samples from VAEs and restricted neural network architectures for reversible generative models. Recent research suggests that the discrepancy between intrinsic and ambient dimensionalities of data contributes to training difficulties in GANs and VAEs. A novel framework for training autoencoder-based generative models with non-adversarial losses and unrestricted neural network architectures is presented. Instead of matching the target distribution in the data space, both generated and target distributions are mapped to the latent space using the encoder. The generator is then trained to match the distributions in the latent space. The framework presented is called Perceptual Generative Autoencoder (PGA), which aims to map target distributions to the latent space using the encoder and train the generator to minimize divergence between the mapped distributions. This approach allows for training generative autoencoders with maximum likelihood, without restrictions on architectures or latent dimensionalities. Additionally, combining PGA with VAE results in generating sharper samples compared to vanilla VAE. The paper introduces the Perceptual Generative Autoencoder (PGA) framework, which utilizes the encoder to map distributions to the latent space and trains the generator to minimize divergence between the mapped distributions. This method enables training generative autoencoders without architectural or dimensional restrictions, leading to sharper sample generation when combined with VAE. The encoder is used to map the input data onto the manifold of the decoder's output, serving as a surrogate target for training the generator. The generator is trained to match the mapped distribution in the latent space, with the encoder reutilized for this purpose. The paper introduces the Perceptual Generative Autoencoder (PGA) framework, utilizing the encoder to map distributions to the latent space and training the generator to minimize divergence between the mapped distributions. The framework allows training generative autoencoders without restrictions, leading to sharper sample generation when combined with VAE. The encoder maps input data to the decoder's output manifold, serving as a target for training the generator to match the mapped distribution in the latent space. The theorem states that different x's generated by the decoder are mapped to different z's by the encoder, with specific conditions on the distributions D and H. The paper introduces the Perceptual Generative Autoencoder (PGA) framework, utilizing the encoder to map distributions to the latent space and training the generator to minimize divergence between the mapped distributions. The framework allows training generative autoencoders without restrictions, leading to sharper sample generation when combined with VAE. The theorem states that different x's generated by the decoder are mapped to different z's by the encoder, with specific conditions on the distributions D and H. D andD approach continuous distributions, with a focus on training the generative model to map N (0, I) to H through a perceptual generative model. Different approaches, including a maximum likelihood approach and a VAE-based approach, are presented for training the perceptual generative model. The paper introduces the Perceptual Generative Autoencoder (PGA) framework for training generative autoencoders without restrictions. It focuses on maximizing Eq. (4) with respect to the generator parameters \u03b8. To avoid computing the Jacobian, an approximation is made using a loss function for the decoder. This loss function attracts latent representations of data samples to the origin and expands the volume occupied by each sample in the latent space. The paper introduces the Perceptual Generative Autoencoder (PGA) framework for training generative autoencoders without restrictions. It focuses on maximizing Eq. (4) with respect to the generator parameters \u03b8. The approach involves training the autoencoder to obtain a generative model by minimizing a loss function with hyperparameters \u03b1, \u03b2, and \u03b3. This modified VAE, referred to as LPGA, aims to optimize the model without the need for computing the Jacobian determinant. The paper introduces the Perceptual Generative Autoencoder (PGA) framework for training generative autoencoders without restrictions. It focuses on maximizing Eq. (4) with respect to the generator parameters \u03b8. The approach involves training the autoencoder to obtain a generative model by minimizing a loss function with hyperparameters \u03b1, \u03b2, and \u03b3. This modified VAE, referred to as LPGA, aims to optimize the model without the need for computing the Jacobian determinant. In our case, we modify Eq. (8) to maximize log p (\u1e91) by replacing p (x | z) with p (\u1e91 | z) and derive a lower bound on log p (\u1e91). The VAE variant, known as variational PGA (VPGA), is trained by minimizing a loss function. The performance of LPGA and VPGA is evaluated on three image datasets: MNIST, CIFAR-10, and CelebA. For CelebA, the discriminator and generator architecture of DCGAN is employed. The Perceptual Generative Autoencoder (PGA) framework utilizes the discriminator and generator architecture of DCGAN for CelebA. The number of filters is halved for faster experiments, and batch normalization is used cautiously to avoid stability issues. SGD with a momentum of 0.9 is employed for training all models, and the training process of PGA is generally stable. The PGA framework significantly improves sample quality over VAE, especially on CIFAR-10 and CelebA. FID scores show better performance compared to other non-adversarial generative models. PGA utilizes non-adversarial losses and unrestricted neural network architectures for training autoencoder-based generative models. PGA trained with maximum likelihood matches target distributions in the latent space, generalizing reversible generative models to various neural network architectures and latent dimensionalities. It enhances VAE performance and can be combined with any method for training perceptual generative models. Future work could involve combining PGA with an adversarial discriminator for latent representations and investigating compatibility issues with batch normalization."
}