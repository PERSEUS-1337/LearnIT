{
    "title": "SyzrLjA5FQ",
    "content": "Semi-supervised learning (SSL) efficiently utilizes unlabeled data to improve performance with limited labeled data. A new method called selective self-training (SST) selectively includes unlabeled samples in the training process, even when classes differ between labeled and unlabeled data. This approach addresses the limitations of conventional SSL methods. The proposed method in SSL can be combined with other algorithms and handles new SSL problems with separated data. It maintains performance even when classes of unlabeled data differ from labeled data. SSL helps in data collection efficiency by utilizing unlabeled data. The significance of Semi-Supervised Learning (SSL) lies in its ability to efficiently learn from fewer labeled data by using a large amount of unlabeled data. While SSL has been extensively studied in previous literature, recent research has highlighted limitations in conventional SSL methods, particularly in their application to real-world scenarios. These methods assume that all unlabeled data belong to one of the classes of the labeled data, which may not hold true in practice. In this paper, a deep neural network approach called selective self-training (SST) is proposed to address the limitations of traditional SSL methods. Unlike conventional self-training methods, SST selectively uses unlabeled data for training by incorporating a selection network based on deep neural networks. This approach does not rely on classification results for data selection and adopts an ensemble approach to improve performance. The proposed selective self-training (SST) approach utilizes a temporal ensemble method in the selection network to ensure clean training data. A simple heuristic is suggested to balance the number of samples per class for optimal network performance. Reliable samples are added to the training set while uncertain samples, including out-of-class data, are excluded. SST iteratively adopts newly annotated training data for improved performance. The proposed Selective Self-Training (SST) method utilizes a temporal ensemble approach to ensure clean training data. It is suitable for incremental and lifelong learning, adding only relevant in-class samples. The method performs comparably to other SSL algorithms and can be combined with them. The proposed Selective Self-Training (SST) method is effective for SSL problems, requires few hyper-parameters, and is suitable for lifelong learning. It outperforms other SSL algorithms in classification errors and is applicable to real-world situations. The method utilizes a temporal ensemble approach and can be combined with other algorithms. The proposed Selective Self-Training (SST) method is based on self-training and consistency regularization algorithms. It is evaluated using benchmark datasets like CIFAR-10, CIFAR-100, and SVHN. The method involves repeatedly labeling unlabeled samples based on confidence scores and retraining with pseudo-annotated data. The SSL system overview is shown in FIG0. The proposed algorithm is based on self-training and consistency regularization. It addresses the issue of misclassification of instances with low likelihood in real-world scenarios. Ensemble and balancing methods are used to select reliable samples. Popular SSL methods like \u03a0 model and temporal ensembling define new loss functions for unlabeled data. The proposed algorithm utilizes self-training and consistency regularization to address misclassification of instances with low likelihood in real-world scenarios. Popular SSL methods like \u03a0 model and temporal ensembling define new loss functions for unlabeled data, requiring consideration of various hyperparameters and customized settings for training. The proposed selective self-training (SST) method utilizes three networks - a backbone network, a classification network f cl (\u00b7; \u03b8 c), and a selection network f sel (\u00b7; \u03b8 s) with learnable parameters. The model is trained using labeled and unlabeled datasets with hyperparameters and epochs for retraining. The proposed SST method involves three networks - a backbone network, a classification network, and a selection network with learnable parameters. The model is trained using labeled data and then predicts unlabeled data, selecting a subset. The proposed Semi-Supervised Training (SST) method involves training a model with labeled data, predicting unlabeled data, selecting a subset with high scores, annotating selected samples, and retraining iteratively. The process is described in Algorithm 1, with the classification network trained using softmax and cross-entropy loss. The selection network in the Semi-Supervised Training (SST) method uses a binary cross-entropy loss and a sigmoid activation function to estimate absolute confidence scores, avoiding the use of softmax to prevent high values for out-of-class samples. After learning the model in a supervised manner, the Semi-Supervised Training (SST) method utilizes classification results and selection scores to annotate and choose unlabeled samples. To prevent contamination in the training set, temporal co-training and ensemble methods are employed for selection scores. Temporal consistency of classification results from current and previous iterations is used to reduce uncertainty in sample selection. The Semi-Supervised Training (SST) method utilizes ensemble scores to select reliable unlabeled samples. By updating the ensemble score with previous network evaluations, the aim is to choose high-quality (pseudo-)labeled samples for training. Setting an appropriate threshold is crucial for determining the quality of added unlabeled samples in the next training iteration. In Semi-Supervised Training (SST), ensemble scores are used to select reliable unlabeled samples for training. The selection network is trained with target g i generated from classification score r i, resulting in a selection score s i close to 1.0. A threshold is set to control the selection process, and if the ensemble score z i exceeds the threshold, the unlabeled sample's pseudo-label is set to the classification result r i. It is important to maintain a balanced distribution of samples in the new training dataset to avoid degradation in classification performance. Simply creating a new training dataset without considering class balance may lead to poor performance. Fairly transferring selected samples to the new training set requires ensuring that the migration amount in each class does not exceed the number of the dominant class. The new training set T is created by combining labeled samples L and selected unlabeled samples U S, ensuring migration in each class does not exceed the least selected samples. The model is retrained with this dataset for K re epochs, obtaining pseudo-labels for the selection network. This process is repeated for M iterations until convergence. Ablation study on CIFAR-10 dataset shows the impact of data balancing and ensemble scores on performance. The proposed SST algorithm evaluates two types of experiments: conventional SSL with all in-class unlabeled data and new SSL with out-of-class data. In-class data benefits from gathering confident samples, while out-of-class data requires a strict threshold to avoid uncertain data. Experimentation includes decay mode for threshold reduction and fixed mode for threshold stability. The experimental setup includes fixed mode with a fixed threshold and cross-validation for determining epsilon. Experiments were conducted on synthetic datasets and popular datasets like SVHN, CIFAR-10, and CIFAR-100. Labeled versus unlabeled data separation settings were consistent across datasets. Additional details can be found in the supplementary material. The experiments on CIFAR-10 dataset involved three components: data balancing, selection score ensemble, and multiplication of selection score with softmax output. Results showed that using data balancing reduced error to 14.43% from 21.44%. Adding ensemble scheme further decreased error to 11.82%, but multiplication scheme had a slight drop in performance. The experiments on CIFAR-10 dataset involved data balancing, selection score ensemble, and multiplication of selection score with softmax output. Results showed that data balancing reduced error to 14.43% from 21.44%, while the ensemble scheme decreased error to 11.82%. However, the multiplication scheme had a slight drop in performance. The experiment results of supervised learning, conventional SSL algorithms, and the proposed SST on CIFAR-10, SVHN, and CIFAR-100 datasets are shown in TAB2. The baseline model with supervised learning performed slightly better than reported in other papers due to different settings such as Gaussian noise. The CIFAR-100 datasets were used for experiments with various models such as BID9, TempEns, VAT, and others. Different accuracy and data usage results were compared, including the proposed method SST combined with TempEns and SNTG. Experiments involved inputs, optimizer selection, batch normalizations, and learning rate parameters. The proposed method SST combined with TempEns and SNTG, labeled as SST+TempEns+SNTG, shows promising results in reducing test errors compared to other algorithms. The model achieves an error rate of 11.82%, while the SST+TempEns+SNTG model performs even better with an error rate of 4.74%. The SST model combined with TempEns and SNTG reaches 4.74% error, worse than TempEns+SNTG. Possible reasons include dataset imbalance and ease of SVHN dataset. Even with data balancing, SST still performs worse. The use of hard labels in SST may lead to performance deterioration. Optimization of hyper-parameters in TempEns and SNTG is also considered. Real-world application experiments involve categorizing datasets into animal and non-animal classes, similar to BID3. In CIFAR-100, experiments were conducted with labeled data for 6 animal classes and a pool of 20,000 images with mixed animal and non-animal classes as an unlabeled dataset. The presence of out-of-class samples required a strict threshold, with a fixed mode of criterion threshold used to avoid poor performance caused by adding out-of-class unlabeled data to the training set. More details can be found in Section 6.6 of the supplementary material. The fixed mode of criterion threshold in SST sets a high fixed threshold throughout training, which is more suitable for real applications but shows lower performance compared to the decay mode. The difference lies in the unchangeable threshold and initial ensemble. Setting a critical threshold value for the fixed mode is essential for comparison. In CIFAR-10, as the number of iterations increases, the threshold in the decay mode decreases, and more unlabeled data is added. The addition of non-animal data in the unlabeled dataset shows varying trends in training. Selective data addition in the method begins at different iterations depending on the composition of the unlabeled dataset. The threshold value for data addition is set between 40th and 55th iteration for more reliable selection. In the case of SST, no unlabeled data is added to the new training set for 5 iterations. Selective self-training (SST) improves performance by adding stable data samples, especially in cases where the origin of data is unknown. When the unlabeled dataset consists of in-class animal data, performance is enhanced, but degraded with only out-of-class data. SST with a fixed threshold value maintains performance even at 100% non-animal ratio, showing more improvement with fewer out-of-class samples in the pool. This method is more suitable for real applications compared to supervised learning. Selective self-training (SST) improves performance by selectively sampling unlabeled data and training the model with a subset of the dataset. It shows competitive results on conventional SSL problems and can be combined with other algorithms for further improvement. The accuracy of SST varies depending on the dataset but generally improves with more data. Additionally, experiments with out-of-class data show potential performance enhancements. The combined experiments of SST and other algorithms demonstrate performance improvement. SST maintains performance in new SSL problems with in-class and out-of-class data. Adjusting the selection network threshold affects performance, with different trends for in-class and out-of-class data. SST prevents erroneous data addition to the training dataset, ensuring accuracy in real environments. In supervised learning, the model is trained using batch size 100 for 300 epochs, followed by retraining with the same batch size for 150 epochs. The learning rate is adjusted during training phases. The number of training iterations and thresholding are crucial parameters in the algorithm. In supervised learning, the model is trained using batch size 100 for 300 epochs, followed by retraining with the same batch size for 150 epochs. The learning rate is adjusted during training phases. The number of training iterations and thresholding are crucial parameters in the algorithm. The validation accuracy is evaluated using cross-validation, with the number of training iterations set to 100. The growth rate of data is adjusted in log-scale, starting at a small value (10^-5) and increasing until validation accuracy saturates. The stopping criterion is when the accuracy of the current iteration reaches the average accuracy of the previous 20 steps, with adjustments made to the growth rate based on the stopping iteration. In the experiments, the data growth rate is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN, and every 27 iterations in CIFAR-100. Two types of networks were used, with the network structure for CIFAR-10, SVHN, and CIFAR-100 consisting of convolutions. Standard batch normalization and Leaky ReLU with 0.1 were applied. In experiments with CIFAR-10, SVHN, and CIFAR-100 datasets, standard data normalization and augmentation were used, including random flipping and translation. The SST algorithm was compared to conventional SSL algorithms using a popular setting. The validation set for cross-validation to obtain the reduction rate of epsilon was extracted from the training set. The standard labeled/unlabeled split for CIFAR-10 was 4k labeled data and 46k unlabeled data. Synthetic datasets like two moons and 4 spins were also tested. The SST algorithm was tested on synthetic datasets like two moons and 4 spins, each with 6,000 training and test samples. Due to the small number of labeled data points, random sampling was constrained by a minimum Euclidean distance of 0.7. The algorithm was iterated 50 times with a learning rate increasing from 10^-7 to 10^-4.5. Figures 3, 5, and 6 illustrate the dataset settings and algorithm progress. The SST algorithm improves performance by gradually expanding data in a synthetic dataset. Test errors for CIFAR-10 with 1k and 2k images were 23.15% and 15.72% respectively, outperforming \u03a0 model but falling short of Mean Teacher in 1k test. In 2k test, SST performed better than \u03a0 model and similarly to Mean Teacher. In SVHN, SST showed worse results with 1,000 labeled images and 45,000 balanced unlabeled images. The SST algorithm improves performance by gradually expanding data in a synthetic dataset. However, the SST algorithm still performs worse than other algorithms due to incorrectly estimated samples. BID3 adds four unlabeled classes and tests based on the ratio of unlabeled classes. Experimental results show that self-training without a threshold does not improve performance, and applying SST as a threshold without a selection network improves performance. In the new SSL problem, an experiment in decay mode aims to find a gap between two points of data addition in non-animal data. The growth rate of epsilon in CIFAR-10 is applied to CIFAR-100, affecting the difference in iterations. The number of data increases slightly in 0-75% iterations, while selected samples are added in 100% iterations. A fixed threshold is set at 35 iterations. In the decay mode, performance improves at 0% and degrades at 100%. In the fixed mode, there is no performance degradation. The difference between 0% and 100% in CIFAR-100 is smaller than in CIFAR-10 due to a smaller gap between animal and non-animal data. Experimental results are shown in FIG5."
}