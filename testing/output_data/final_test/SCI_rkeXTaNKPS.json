{
    "title": "rkeXTaNKPS",
    "content": "Semi-Supervised Learning (SSL) methods using Convolutional Neural Networks (CNNs) have shown success in tasks like image classification. A new approach using Optimal Transport (OT) for providing pseudo-labels to unlabeled data is proposed, improving SSL training. The method outperforms state-of-the-art SSL algorithms on standard datasets. Recent developments in CNNs have shown promising results in machine learning and computer vision tasks. Transfer Learning (TL) and Semi-Supervised Learning (SSL) are two solutions to deal with the lack of annotated training data. SSL methods aim to learn discriminative models using information from unlabeled data. Semi-Supervised Learning (SSL) algorithms utilize structural assumptions like continuity, cluster, or manifold to make use of unlabeled data. These assumptions help in classifying data based on their proximity or clustering patterns.SSL methods aim to learn discriminative models using information from unlabeled data. The theory of information geometry and Optimal Transport (OT) are two principal directions that model geometrical structures underlying discrete probability measures. Information geometry focuses on invariance between probability measures under transformations, while OT utilizes prior geometric knowledge on the base space of random variables. Computing OT or Wasserstein distance achieves optimal coupling between random variables. The Wasserstein distance is optimal for minimizing transportation costs between variables and capturing the geometry of the base space. It is useful for applications where the base space's structure plays a significant role. Inspired by this, a CNN model is used to represent data and provide pseudo-labels for unlabeled data in SSL methods. Our SSL method utilizes Wasserstein distances to map unlabeled data to labeled data, providing pseudo-labels for training a CNN model. The method is based on hierarchical modeling and constructs a measure of measures for both labeled and unlabeled datasets. Cuturi (2013) introduced a method that relaxes the optimal transport problem using entropic regularization, making it computationally faster and differentiable. This approach has gained attention in machine learning applications such as data generation, loss function design, domain adaptation, and clustering. Pseudo-Labeling is a method where a model uses its own predictions on unlabeled data to enhance training, but it can't correct its mistakes, especially when the domain of unlabeled data differs from labeled data. This limitation can lead to error amplification during training. Consistency Regularization forces higher confidence predictions for unlabeled data with low entropy, exploring a smooth manifold. State-of-the-art SSL methods include stochastic perturbations, \u03c0-model, mean teacher, and Virtual Adversarial Training. Pseudo-ensembles techniques were first introduced in Bachman et al. (2014). Pseudo-ensembles regularization techniques aim to maintain model predictions stable under data perturbations by adding a weighted loss term. However, these methods can be unstable due to changing target predictions during training. To address this issue, temporal ensembling and mean teacher methods were proposed to achieve a more stable target output. Temporal ensembling and mean teacher methods aim to stabilize target output predictions during training. Mean teacher, inspired by temporal ensembling, uses an exponentially accumulated average of model outputs as a prediction function. It outperforms temporal ensembling in practice. On the other hand, VAT introduces a small perturbation to the input to significantly change the model's prediction, followed by a consistency regularization technique to minimize the difference between predictions with and without the perturbation. Entropy minimization is a technique applied to force a model to produce confident predictions on unlabeled data, penalizing decision boundaries near data points. When used in conjunction with VAT, it can achieve state-of-the-art results. Entropy minimization, when combined with VAT, can achieve state-of-the-art results by assuming a fixed virtual label prediction. The Wasserstein space of order k for probability measures on a subset \u03b8 is defined using Euclidean distance. The k-th Wasserstein distance between two probability measures is the optimal cost of moving mass from one to the other, proportional to the Euclidean distance raised to the power k. The abstract space of Borel measures on subset \u03b8 is represented by S k (S k (\u03b8)), endowed with a Wasserstein metric W k (.) of order k. This metric calculates the optimal cost of transporting mass from one cloud of measures to another, where the cost is proportional to the power k of the Wasserstein distance in the space of support, S k (\u03b8). The algorithm uses optimal transport to provide pseudo-labels for unlabeled data in SSL training. It assumes that labeled and unlabeled data from the same class come from the same distribution, leveraging OT metric to map similar measures. OT exploits the structure of the metric space to measure similarity between empirical measures. The algorithm utilizes optimal transport to assign pseudo-labels to unlabeled data in semi-supervised learning. It leverages OT metric to map similar measures between labeled and unlabeled data, assuming they come from the same distribution. OT measures similarity in the metric space by constructing a measure of measures. The SSL method uses optimal transport to find a coupling between labeled and unlabeled data measures that minimizes transportation cost. This approach aims to assign pseudo-labels to unlabeled data by mapping similar measures between the two datasets. The SSL method utilizes optimal transport to minimize transportation cost between labeled and unlabeled data measures, assigning pseudo-labels based on optimal coupling. OT cost function in Eq. (3) defines an optimal coupling matrix T to infer labels for unlabeled data based on similarity between measures within M and M. The regularized optimal transport (OT) method uses Wasserstein distance to compute similarity between data points. The optimal coupling matrix T is defined by the Frobenius dot-product between matrices T and X. The entropy E(T) of T is used for regularization, with a hyperparameter \u03bb balancing the terms. The iterative Sinkhorn algorithm is used to obtain the optimal coupling solution. Unlabeled data is represented as a measure of measures M, with each measure constructed from data of the same class. The label of the unlabeled data is unknown. The unlabeled data, with unknown labels, can be explored using unsupervised methods like clustering. Wasserstein metric is leveraged to investigate these unknown measures in the data, specifically by relating clustering to exploring the Wasserstein barycenter. This barycenter concept was introduced by Agueh & Carlier (2011) and involves probability measures to define the Wasserstein barycenter. The Wasserstein barycenterR l,\u00b5 is defined for discrete measures with finite elements and uniform weights. The problem is recast to search on a set of probability measures with at most r support points in \u03b8. An efficient algorithm for exploring local solutions of the Wasserstein barycenter problem has been studied. K-means clustering is connected to the quantization problem. The K-means problem aims to find a set of atoms that minimize a specific objective function, which can be viewed as a Wasserstein barycenter problem. An algorithm introduced by Cuturi & Doucet (2014) offers an alternative to the Loyds algorithm for finding local minimums in K-means. This algorithm is used to compute Wasserstein barycenters of empirical probability measures to uncover clusters within unlabeled data. Our SSL method leverages unlabeled image data annotated by pseudo-labels from the OT along with supervision signals of initial labeled data to train the CNN classifier using cross entropy as the discriminative loss function. The total loss function for SSL training balances between labeled and unlabeled data, with a hyperparameter \u03b1. Initially, the CNN is trained with labeled data, then pseudo-labels from OT are used to train the CNN with unlabeled data. Our SSL method involves using OT to provide pseudo-labels for unlabeled data, which are then used to train the CNN along with initial labeled data in each epoch. The SSL technique is evaluated following recommendations from Oliver et al. (2018), including using a common CNN architecture for comparative analysis and reporting fully-supervised performance as a baseline. The SSL algorithm aims to outperform fully-supervised settings by utilizing labeled and unlabeled data efficiently. Realistic small validation sets are analyzed to ensure applicability in real-world scenarios. The 'WRN-28-2' model with batch normalization and leaky ReLU nonlinearities is used for evaluation. In experiments using CIFAR-10 and SVHN datasets, SSL challenges were tackled with labeled and unlabeled data from the same distribution. Training sets were split into labeled and unlabeled data, using Adam optimizer with default hyperparameters. A batch size of 100 was used without early stopping, monitored validation set performance. The Sinkhorn algorithm stopping criteria is maxIter = 10,000 or tolerance = 10 \u22128. Data augmentation and normalization were applied to SVHN and CIFAR-10 datasets. For SVHN, pixel intensity values were converted to floating point values in the range of [-1, 1] with random translation by up to 2 pixels. CIFAR-10 underwent global contrast normalization and random translation by up to 2 pixels. Training and validation sets were split accordingly. The data augmentation on CIFAR-10 includes random translation, horizontal flipping, and Gaussian input noise. The training set has 45,000 images, and the validation set has 5,000 images. SSL aims to improve performance using unlabeled data. Error rates of SSL models using labeled data alone and leveraging unlabeled data are compared. Performance of various SSL algorithms using unlabeled data is also reported. The SSL algorithms in Table 1 leverage unlabeled data during training with a common CNN model. Results show test error at the lowest validation error point. For fair evaluation, labeled and unlabeled data splits were chosen for CIFAR-10 and SVHN datasets. The SSL algorithm was run five times with different splits, showing mean and standard deviation of test error rate in Table 1. Results indicate performance improvement on both CIFAR-10 and SVHN datasets. The results in Table 1 show that the fully-supervised baseline and ROT have a larger performance gap compared to other SSL methods on CIFAR-10 and SVHN datasets. The model leverages unlabeled data effectively, using soft pseudo-labels generated by OT for training the CNN. The comparison of results in Table 1 shows that using one-hot targets in ROT outperforms soft pseudo-labels. SSL methods based on entropy minimization encourage confident predictions. ROT, based on OT measure, is compared with two baselines using GNN for pseudo-label assignment. The first baseline, S-S-GNN, annotates unlabeled data based on softmax layer outputs. The comparison of results between ROT, S-S-GNN, and S-M-GNN on SVHN and CIFAR-10 shows the advantage of using measure OT for SSL training with CNNs. Instead of CNNs producing pseudo-labels for unlabeled data, measure OT assigns labels based on the closest labeled samples in the training set. OT problem has a closed form when transporting from a Dirac to a probability measure, resulting in Wasserstein distance computation. The Wasserstein barycenters were used to cluster unlabeled data, matching them with labeled data for pseudo-labeling. Comparing this clustering-based method (ROT) with CNN pseudo-labeling, ROT showed greater positive influence on training the CNN classifier. The accuracy of pseudo-labels obtained by ROT was reported, indicating its benefit in CNN training. During training, the ROT strategy outperformed the baseline CNN classifier in predicting pseudo-labels on unlabeled data. This indicates that the CNN network can benefit more from ROT than training solely with its own predicted labels. The accuracy of pseudo-labels generated by ROT was higher for both CIFAR and SVHN datasets compared to the baseline classifier. The ROT strategy outperformed the baseline CNN classifier in predicting pseudo-labels on unlabeled data. Transportation cost decreased as the CNN was trained with a better feature set. Evaluating ROT with varying initial labeled data showed performance recovery with SSL method on the testing set. In this experiment, the SSL method was tested multiple times with different labeled and unlabeled data splits for each dataset. The results, shown in Fig. 2(a) and Fig. 2(b), indicate that the error rate tends to converge as the number of labels increases. The SVHN dataset includes additional digit images from the SVHN-extra dataset, allowing for more unlabeled data evaluation. The SSL method tested with different labeled and unlabeled data splits for each dataset. Increasing the amount of unlabeled data improves performance, but the improvement is not significant with 40k unlabeled data. A new SSL method based on optimal transportation technique was proposed, mapping between labeled and unlabeled data masses to infer pseudo-labels for training a CNN model. The method was experimentally evaluated for its potential and effectiveness. The SSL method was experimentally evaluated for leveraging unlabeled data when labels are limited during training. It involves Discrete Optimal Transport, where data points are represented as sets U and V, and a transportation problem is solved using weighting vectors and a matrix M to encode the geometry of the data points. The transportation polytope P (a, b) in R n\u00d7m + represents a feasible set of non-negative matrices with row and column marginals a and b. It is used to solve the transportation problem by indicating the amount of mass transported from i to j. The Wasserstein distance W k (U, V) is calculated based on a parametric linear program on a cost matrix M and marginals a and b. The Wasserstein distance is calculated using a Linear Program with marginals a and b. Cuturi (2013) introduced a method to regularize the solution matrix using entropy. The solution matrix T\u03b3 is updated iteratively using vectors u and v. The Sinkhorn algorithm updates vectors u and v iteratively between steps 1 and 2. The CNN maps images to a c-dimensional representation with parameters \u03b8 n. The Wasserstein barycenter of the unlabeled set X u is equivalent to Lloyd's algorithm. The algorithm updates vectors iteratively using the Sinkhorn algorithm for optimal transport. It computes the Wasserstein barycenter of unlabeled data for clustering, involving steps like training CNN parameters and clustering data. The regularized optimal transport problem reduces time complexity by introducing a strong convex regularizer to the cost function, allowing for interpolation between exact optimal transport and Maximum Mean Discrepancy. This approach is used for clustering unlabeled data by pseudo-labeling clusters with the highest mass transport. The regularized optimal transport problem reduces time complexity by introducing a strong convex regularizer to the cost function. It computes the Wasserstein distance between output of the CNN obtained from labeled and unlabeled data, reducing over-fitting chances. The OT problem in regularized form is recast with a hyperparameter \u03b3 balancing two terms. The solution matrix T\u03b3 is updated iteratively using vectors u and v. The Sinkhorn algorithm updates vectors u and v iteratively in two steps. An optimal coupling \u03c0 \u2208 \u03a0(M, M) achieves the infimum of the cost function. The existence of optimal couplings in \u03a0(R, S) and \u03a0(M, M) is guaranteed due to the continuity of the cost function and tightness of the spaces. Lebesgue space L1, Polish probability spaces (X, \u00b5) and (Y, \u03bd), and upper semi-continuous functions a and b are defined for the problem. The Sinkhorn algorithm updates vectors u and v iteratively to find an optimal coupling \u03c0 \u2208 \u03a0(M, M) that minimizes the cost function. The existence of optimal couplings in \u03a0(R, S) and \u03a0(M, M) is guaranteed by the continuity of the cost function and tightness of the spaces. Tight subsets R \u2282 P(X) and S \u2282 P(Y) are considered, with transference plans in \u03a0(R, S) being tight in P(X \u00d7 Y). The Sinkhorn algorithm updates vectors u and v iteratively to find an optimal coupling \u03c0 \u2208 \u03a0(M, M) that minimizes the cost function. The cost function c is a lower semi-continuous function, and can be written as the point-wise limit of a non-decreasing family of continuous real-valued functions. Prokhorov's Theorem states that a set R \u2282 P(X) is precompact for the weak topology if and only if it is tight. The Sinkhorn algorithm iteratively updates vectors to find an optimal coupling that minimizes the cost function. Prokhorov's Theorem states that a set in P(X) is precompact for the weak topology if it is tight. Using Lemma 1, the coupling set \u03a0(\u00b5, \u03bd) in P(X \u00d7 Y) is compact. By passing to the limit, \u03c0 is shown to be minimizing. Further details of the proof are available in Villani's book. One interesting argument in Oliver et al. (2018) discusses the challenge of model selection for SSL models due to hyperparameter tuning on small validation sets. Most SSL datasets have a validation set much larger than the training set, making it impractical for real-world applications. For example, the SVHN dataset used in the study has 7000 labeled data in the validation set, seven times larger than the training set. This setup may not be feasible for practical use. The work presented in Oliver et al. (2018) discusses the challenge of model selection for SSL models due to hyperparameter tuning on small validation sets. Using a large validation set as the training set for tuning hyperparameters is recommended to avoid instability in evaluation metrics. The Hoeffding inequality is used to analyze the relationship between the size of the validation set and the variance in estimating a model's accuracy. The Hoeffding inequality suggests that a large validation set is needed for accurate estimation of validation error. Oliver et al. (2018) empirically measure this phenomenon by training SSL methods with 1,000 labels and evaluating on various validation set sizes. The ROT algorithm performs better with larger validation sets, leading to more confident hyperparameter selection. The study maintains consistency in training and validation set sizes for fair comparison with other SSL methods on standard datasets. The ROT algorithm performs better with larger validation sets, ensuring confident hyperparameter selection. Training and validation set sizes are consistent for fair comparison with other SSL methods on standard datasets. For SVHN, 65,932 images are used for training and 7,325 for validation, while for CIFAR-10, 45,000 images are used for training and 5,000 for validation. Error rates of the ROT algorithm on SVHN and CIFAR validation sets are shown in Fig. 3(b) and Fig. 4(b) for different \u03bb values in the transportation plan, with \u03b1 fixed in Eq. (5) during \u03bb tuning."
}