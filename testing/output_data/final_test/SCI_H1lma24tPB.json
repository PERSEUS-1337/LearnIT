{
    "title": "H1lma24tPB",
    "content": "Hypernetworks are meta neural networks that generate weights for a main neural network in an end-to-end differentiable manner. Weight initialization methods like Glorot & Bengio (2010) and He et al. (2015) fail when applied directly on a hypernet. Principled techniques for weight initialization in hypernets lead to stable mainnet weights, lower training loss, and faster convergence. Meta-learning involves learning to learn and hypernetworks are used to generate weights for a main neural network in a differentiable way. Hypernetworks generate weights for a main neural network in an end-to-end differentiable manner. They induce weight-sharing and achieve model compression by training the same meta network to learn weights for different layers. Hypernetworks have various applications including weight pruning, neural architecture search, Bayesian neural networks, multi-task learning, continual learning, generative models, ensemble learning, and hyperparameter optimization. The problem of optimizing hypernetworks remains understudied despite the intensified study of applications such as weight pruning, neural architecture search, and hyperparameter optimization. Prior work in this area mostly relies on ad-hoc approaches due to the lack of principled training methods. The use of the Adam optimizer has been successful in training hypernetworks, but issues such as loss divergence can still occur in large models. The use of Adam optimizer in training hypernetworks can lead to noisy training dynamics and lower generalization compared to non-adaptive gradient methods. Adam also incurs a computational overhead and requires more memory. Neural network optimization community emphasizes the importance of architecture-specific initialization schemes for robust training of deep networks. Weight initialization is crucial for training deep networks, especially hypernetworks. Classical methods fail to produce correct mainnet weights for hypernetworks, leading to issues like exploding activations and losses. Developing principled techniques based on variance analysis is necessary for hypernetwork weight initialization due to unique challenges they pose. The hypernet case presents unique challenges, with asymmetry between forward and backward pass. Proper initialization is crucial to mitigate exploding activations and gradients. Our methods ensure correct hypernet weights, leading to stable mainnet weights and faster convergence. Section 2 covers technical preliminaries, while Section 3 reviews current ad-hoc methods used by hypernetwork practitioners. A hypernetwork is a meta neural network that generates weights for a main network in a differentiable manner. Unlike a classical network, the weights of the main network are not model parameters, requiring backpropagation to update the hypernetwork weights. Novel approaches to weight initialization, optimization dynamics, and architecture design are needed for hypernetworks. We propose using Ricci calculus as a mathematical language for hypernetworks, allowing easy reasoning about derivatives of higher-order tensors. Index-based notation is explained, with explicit summations and Einstein summation convention used. Square brackets denote different layers, and weight initialization formulae are derived for feedforward networks. Glorot & Bengio (2010) and He et al. (2015) derived weight initialization formulae for feedforward neural networks, focusing on symmetrical activation functions and ReLU activation functions. The initialization is based on harmonic mean and Xavier Assumptions, ensuring symmetric weights around 0 for activations and gradients propagation. In the context of weight initialization for neural networks, He et al. (2015) discussed the use of forward or backward versions of formulas, with terms like 'Xavier init' and 'Kaiming init' being interchangeable. Ha et al. (2016) identified dynamic and static classes of hypernetworks, proposing Orthogonal init for the dynamic class but omitting initialization discussion for the static class. The static class has since become the dominant variant. The static class of hypernetworks is the dominant variant, used for all types of non-recurrent networks. Hypernet practitioners typically rely on the Adam optimizer for training and choose from four weight initialization methods: M3 Kaiming init, M4 Kaiming init, M1 classical neural network initialization, and a linear hypernet example generating a linear mainnet with T + 1 layers. The linear hypernet generates a linear mainnet with T + 1 layers using embeddings and weights sampled from specific distributions. The size of the embedding vector may cause vanishing gradients. Different initialization methods like Xavier and Kaiming init affect activations and gradients exponentially with mainnet depth. Additional hyperparameters in M2 and M3 resemble older, less elegant methods before classical initialization techniques were introduced. The linear hypernet generates a linear mainnet with T + 1 layers using embeddings and weights sampled from specific distributions. Different initialization methods like Xavier and Kaiming init affect activations and gradients exponentially with mainnet depth. A general hypernet weight initialization strategy is proposed to approximate weights from classical neural network initialization. The hypernetwork architectures use a linear output layer for direct gradient flow. Hyperfan-in and hyperfan-out init methods are developed for weight initialization based on variance analysis. The hypernet generates weights but not biases of the mainnet, with biases initialized to zero. Hyperfan Assumptions are made at initialization, following Xavier assumptions for all layers. The Hyperfan Assumptions at initialization involve using fan-in init to initialize weights for the hypernet, ensuring Xavier assumptions hold in the mainnet. The hypernet generates both weights and biases for the mainnet, with weight and bias generation formulas involving h, g, (H, \u03b2), and (G, \u03b3). Modifications to Hyperfan Assumption 2 include additional terms and the assumption of Var(x j ) = 1 for data standardization. Fan-in init is used to initialize weights for h and g. At initialization, using fan-in init for weights in the hypernet ensures Xavier assumptions in the mainnet. Introducing a factor of 1/2 divides variance between weight and bias generation, allowing for controlled variance adjustments during training. This approach helps prevent exploding activations in the mainnet. Similar principles apply to the backward pass, preserving input and output activations. At initialization, using fan-in init for weights in the hypernet ensures Xavier assumptions in the mainnet. Gradients shrink when flowing from the mainnet to the hypernet due to fan-in init. Symmetry between forward and backward versions remains if hypernets only generate mainnet weights. However, if biases are also generated, symmetry is lost as biases affect hypernet gradient flow. G can be initialized to help hyperfan-out init preserve activation variance. We summarize the variance formulae for hyperfan-in and hyperfan-out init in Table 1. Hypernets can be reused to generate different parts of the mainnet. Initialization methods include fan-in init, uniform init, and normal init. Experimental evaluation was conducted on various hypernetwork use cases. In an illustrative experiment, a hypernetwork with different initialization methods generates weights for a feedforward network on MNIST. Hyperfan init methods reproduce mainnet weights similar to Xavier init on a classical network, while Xavier init on a hypernetwork causes exploding activations. Properly scaled initialization stabilizes the magnitude of generated weights. The hypernetwork, when properly initialized, stabilizes the magnitude of generated weights quickly, leading to a more stable training regime. Replacing the activation function with ReLU and using Kaiming init results in even bigger activations at initialization. If the hypernet generates both weights and biases of the mainnet, it takes more time to stabilize weights compared to Xavier init. Continual learning with a hypernetwork helps in learning tasks sequentially without forgetting prior tasks. In a study by von Oswald et al. (2019), a hypernetwork was used to learn task embeddings for efficient regularization and prevent catastrophic forgetting. Different initialization schemes were compared on the hypernetwork implementation, showing that hyperfan methods produced smaller training losses and eventually converged to smaller losses for each task. Ha et al. (2016) also applied a hypernetwork on a convolutional network for image classification on CIFAR-10. Residual connections were not handled in the initialization methods, which are important for future study. In a study by von Oswald et al. (2019), hyperfan methods in a hypernetwork implementation led to smaller training losses and successful convergence. The fan-in version of Kaiming init failed to train even with small learning rates, while the fan-out version started training around epoch 270. Hyperfan-in and hyperfan-out init enabled immediate successful training, showing the importance of a good initialization for training models effectively. Ukai et al. (2018) found that classical initialization approaches led to diverging losses even with batch normalization in the mainnet. The experiment showed that the fan-in version of Kaiming init resulted in higher initial losses and slower training compared to hyperfan methods, even when batch normalization was used. The fan-out version of Kaiming init had similar results to hyperfan methods, indicating that batch normalization may be sufficient to correct bad initializations. This approach offers a principled solution to the problem, unlike trial-and-error heuristics used by Ukai et al. (2018). In Bayesian neural network applications, the interaction between batch normalization and initialization is not well-understood, prompting a direction for future research. Hypernet models with a large number of parameters limit the use of big batch sizes essential for batch normalization performance. The findings suggest that batch normalization may not fully address problems caused by bad initialization, especially in memory-constrained scenarios. In experiments, hyperfan-in and hyperfan-out both successfully train hypernetworks with SGD. The importance of weight initialization strategies like Xavier and Kaiming init for training deep nets is highlighted. This work introduces principled weight initialization methods for hypernetworks, emphasizing the significance of model initialization in learning complex representations. Our work aims to advance the development of techniques for building and training hypernetworks to improve neural network generation methods. Considerations for avoiding exploding activations in weight generation are crucial. When using the same hypernet function to generate different mainnet weight layers, some independence assumptions may be compromised, but this is not always a significant issue in practice. The use of the same hypernet for different layers can help combat the shrinking effect by summing gradients. However, it is advised to avoid this design to simplify initialization formulas. Ha et al. (2016) used a two-layer hypernet to generate weight chunks for a main convolutional network. The highest common factor among mainnet layer sizes is K = 16, with a receptive field size of n2 = 9. Simplifying notation, i, j, k, and l are used for different variables. The output layer in the hypernet is crucial for hyperfan-in and hyperfan-out initialization in the mainnet. Training on MNIST for 30 epochs with batch size 10, the hypernets had one linear layer with embeddings of size 50. The mainnet's hidden layers were generated by the hypernet output layer with different embeddings. Mean cross entropy loss was used for training, while summed cross entropy loss was used for testing. Activation and gradient plots were shown for cases where the hypernet generated only weights or both weights and biases of the mainnet. The plots were based on averaging across a fixed set of examples from the test set. The x-axis displays activations/gradients values corresponding to mainnet weights from the hypernet output layer. The y-axis in Figures 2, 7, 10, 15, 19, 22, 25, and 28 represents mean activations/gradients values. Different means of initialization formulae were tested, but no significant benefits were observed. The mainnet is a feedforward network with two hidden layers and ReLU activation function. We confirm that either the fan-in or fan-out version suffices. Kaiming (fan-in) could only be trained with a specific learning rate, while other rates led to diverging losses. Each task was trained for 6000 iterations with batch size 32. The hypernet used batch size 32 and trained on CIFAR-10 for 500 epochs with an initial learning rate of 0.0005. The weight generation in the hypernet occurs in blocks of (96, 3, 3) with embeddings of size 50. Mean cross entropy loss is used for training, while summed cross entropy loss is used for testing. Ukai et al. (2018) demonstrated the development of a Bayesian neural network using a hypernetwork to express a prior. A Bayesian neural network was developed using a hypernetwork to express a prior distribution without major changes to the vanilla hypernetwork setting. The method involved L2-regularization on model parameters and sampling from stochastic embeddings. A linear hypernet was trained to generate weights for a MobileNet mainnet architecture, with L2-regularization factor of 0.0005. Training was done on ImageNet with batch size 256 and learning rate 0.1 for 25 epochs. Testing was conducted with 10 Monte Carlo samples. Carlo samples were taken, omitting test loss plots due to computational expense."
}