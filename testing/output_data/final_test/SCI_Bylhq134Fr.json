{
    "title": "Bylhq134Fr",
    "content": "Bayesian inference provides a way to train neural networks with calibrated uncertainty. This paper introduces a Gaussian process-based hierarchical model for network parameters and input-dependent contextual variables for weight priors to address challenges in specifying meaningful priors and dealing with weight correlations in the posterior. The paper introduces a Gaussian process-based hierarchical model for network parameters and input-dependent contextual variables for weight priors to address challenges in specifying meaningful priors and dealing with weight correlations in the posterior. The models provide desirable test-time uncertainty estimates, demonstrate cases of modeling inductive biases for neural networks with kernels, and show competitive predictive performance on an active learning benchmark. The question of which priors to use for Bayesian neural networks remains largely unanswered, balancing the need for tractable inference in high-dimensional weight posteriors with expressing beliefs about modeled functions compactly. Structured posterior models, such as radial posteriors or rich weight posteriors based on Gaussian processes, are proposed to cope with richer posterior inference than mean-field typically achieves. The paper proposes a Gaussian process prior over weights to model weight priors and correlations in neural networks, combining ideas from hyper-networks and Gaussian processes. Compositional kernels are explored to add input-dependence to the model. The paper introduces a Gaussian process prior over weights in neural networks, using compositional kernels to incorporate input-dependence. Each unit in the network has corresponding latent hierarchical variables used to construct weights, linking them to the latent variables. The relationship between weight codes and weights in a neural network is described using a meta mapping model. Variational inference is used to infer latent variables and weights, with a nonparametric Gaussian process prior replacing the neural network in the mapping function. The text discusses using a nonparametric Gaussian process prior for functional mapping in neural networks, introducing correlations for weight predictions. The GP-MetaPrior (metaGP) performs one-dimensional regression from latent variables to weights while capturing their correlations using an exponentiated quadratic kernel with ARD lengthscales. The text introduces input-conditional weight models and utilizes compositional kernels to describe relationships between data points. The weight priors are now local to each data point due to private contextual inputs to the meta mapping. Multiple useful kernels from the GP literature can be used to model these relationships. The text introduces input-conditional weight models using compositional kernels from the GP literature to describe data relationships. This novel form of functional regularization constrains the network's function space. The joint density of variables is determined with a weight prior in a neural network, and empirical studies are conducted on kernel choice and input-dependence in regression and generalization tasks. The text explores the use of contextual variables in modeling inductive biases for neural networks and evaluates predictive performance on a regression example. Different models and inference methods are tested, including BNN with MFVI, metaGP, and metaGP with contextual variables using EQ and periodic kernels. Results show that the periodic kernel allows for better long-range predictions by encoding periodicity compared to the EQ kernel. The text discusses the use of metaGP with contextual variables in neural network modeling for calibrated predictive uncertainty on out-of-distribution samples. Results show competitive performance with Gaussian MFVI, highlighting the limitations of MAP estimation for uncertainty estimates on such samples. The text introduces a GP-based hierarchical prior for neural network weights and a variational inference scheme to capture weight correlations and contextual variables. The structured variational inference approach addresses challenges in inferring latent functions in neural networks. The model's performance will be evaluated on more complex decision-making tasks, with plans to extend the scheme for continual learning. The text discusses a GP-based hierarchical prior for neural network weights and a variational inference scheme to capture weight correlations and contextual variables. It introduces a structured variational inference approach to address challenges in inferring latent functions in neural networks. The model's performance will be evaluated on complex decision-making tasks, with plans to extend the scheme for continual learning. The text introduces a structured variational inference approach for neural network weights, utilizing a correlated multivariate Gaussian approximation. This allows for a tractable variational lower bound, with the expectation approximated using Monte Carlo. The integration of contextual variables is discussed, opting for analytical integration over sampling. This approach differs from GP regression and classification due to the higher-dimensional integration required for weight parameters. The text discusses the use of a correlated multivariate Gaussian approximation for neural network weights, with an analytical integration approach for contextual variables. The diagonal approximation retains weight correlations after integrating out latent variables, with efficient estimation using the local reparameterization trick. The final lower bound is optimized to obtain the desired results. The final lower bound is optimized to obtain variational parameters for q(u), q(z), noise estimates in the meta-GP model, kernel hyper-parameters, and inducing inputs. Using the proposed model and inference scheme yields comparable or better predictive errors with a similar number of queries, highlighting the inferior performance of MFVI compared to MAP and metaGP."
}