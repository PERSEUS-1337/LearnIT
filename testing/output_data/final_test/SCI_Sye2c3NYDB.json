{
    "title": "Sye2c3NYDB",
    "content": "Recent research has introduced the lottery ticket hypothesis, which suggests the existence of trainable sub-networks within deep neural networks that perform as well as or better than the original model with equivalent training steps. However, finding these sub-networks requires iterative training and pruning, leading to high costs that limit practical applications. The study shows that a subset of these sub-networks can converge much faster during training, reducing the overall cost. Extensive experiments demonstrate the consistent presence of such sub-networks across various model structures under specific hyperparameters. This discovery can be applied practically to accelerate adversarial training and improve robustness. Pruning has been effective in reducing computation and storage costs in neural networks. The lottery ticket hypothesis suggests that sub-networks obtained through pruning at initialization can perform as well as or better than the original model. This finding allows for more efficient adversarial training and improved robustness. Boosting tickets are sub-networks that converge significantly faster when trained from certain initialization after pruning. Unlike winning tickets, which require iterative pruning and excessive training, boosting tickets show faster convergence across various model architectures. The standard technique for identifying winning tickets does not always find boosting tickets, as the requirements are more restrictive. The study investigates factors that contribute to this boosting effect in large model architectures. The boosting effect in large model architectures depends on factors like learning rate, pruning ratio, and network capacity. By controlling these factors, high validation/test accuracy can be achieved after just one training epoch on CIFAR-10. Boosting tickets have practical applications in accelerating adversarial training for robust models against adversarial examples. Boosting tickets are perturbed inputs that can fool a classifier and accelerate the training process while maintaining high accuracy. They have practical applications in adversarial training for robust models against adversarial examples, saving up to 49% of total training time on CIFAR-10 with WideResNet-34-10. The text discusses the discovery that pruning a non-robust model can lead to finding winning/boosting tickets for a strongly robust model, accelerating the adversarial training process. Network pruning has been studied for compressing neural networks, and recent research shows that pruned networks can still perform similarly to the full model. This relates to the concept of boosting tickets, which are perturbed inputs that can deceive a classifier and speed up training while maintaining accuracy. The lottery ticket hypothesis suggests that a randomly-initialized dense neural network contains a sub-network that can learn as fast as the original network and match its test accuracy. An iterative pruning method is used to find these sub-networks by repeatedly pruning weights with the smallest magnitudes and resetting remaining weights to the initial state. This process is repeated until the desired pruning ratio is achieved, resulting in pruned networks that can be trained to similar accuracy as the original full networks. In this paper, the authors aim to show the existence of boosting tickets, a subset of winning tickets that not only perform equally well as the original model but also converge much faster. Adversarial examples are perturbed versions of inputs that are mis-classified by the classifier. The paper discusses adversarial attacks in machine learning, focusing on the Fast Gradient Sign Method (FGSM) and its stronger multi-step variant, Projected Gradient Descent (PGD). While FGSM is fast but weak, PGD is more powerful but computationally expensive. The authors also mention the importance of defending against such attacks. In this paper, the authors propose a method to combine the advantages of adversarial training using PGD and FGSM during training to quickly train a robust model. Previous studies have shown success in achieving compactness and robustness in trained networks. Our approach aims to reduce training time while maintaining robust accuracy compared to the full model. By investigating boosting tickets on VGG-16 and ResNet, we found that with proper hyperparameters, these tickets can be trained much faster. The process involves randomly initializing a neural network, training it until convergence, and then identifying the boosting tickets. The network is randomly initialized and trained until convergence. A proportion of weights with the smallest magnitudes are pruned, resulting in a mask where pruned weights are set to 0. This mask is applied to the saved initialization to obtain sub-networks, known as boosting tickets. The pruned weights remain at 0 during the entire training process. The algorithm uses a small learning rate for pruning and retraining the sub-networks with a learning rate warm-up. In the context of pruning neural networks, it is shown that using a small learning rate for pruning and retraining, along with a learning rate warm-up, can lead to the discovery of boosting tickets. These boosting tickets, found through one-shot pruning and retraining, demonstrate superior performance compared to randomly initialized models and even outperform winning tickets with a higher convergence rate. The study discusses the use of early stopping for measuring convergence rates in neural networks. However, early stopping is not compatible with the learning rate scheduling used in the experiment, causing issues in evaluation. Despite this, boosting tickets show faster convergence rates and equally good performance compared to winning tickets. Boosting tickets show faster convergence rates and tend to overfit after 50 epochs. To address this, an experiment gradually increases the total number of epochs from 20 to 100 while still dropping the learning rate at specific stages. This allows for a better understanding of convergence speed without worrying about overfitting. Comparing boosting tickets and winning tickets on VGG-16, the improvement in convergence rates is much clearer. The validation accuracy of boosting tickets catches up with the one trained for 100 epochs after 40 epochs. Winning tickets lag behind until 100 epochs when they finally match. Test accuracy for winning tickets gradually increases with more training steps, while boosting tickets peak at 60 epochs and start to overfit at 100 epochs. Boosting tickets hypothesis: A sub-network in a randomly initialized dense neural network converges faster than the original network and other winning tickets. In the following sections, we investigate the influence of different learning rates on finding boosting tickets. Four learning rates (0.005, 0.01, 0.05, 0.1) were used for pruning to obtain winning tickets with improved accuracy. All tickets found with these rates show accuracy improvement over randomly reinitialized sub-models, meeting the criteria of winning tickets. The influence of different learning rates on finding boosting tickets is investigated. Tickets found with smaller learning rates exhibit stronger boosting effects, with models trained at 0.1 showing the least boosting effects. Training with too small a learning rate compromises eventual test accuracy to some extent. Tickets found at 0.01 and 0.05 learning rates are considered boosting tickets for VGG-16 and ResNet-18, respectively, as they converge faster and achieve the highest final test accuracy. Pruning ratio is crucial for winning tickets, and its effect on boosting tickets is explored using early-stage validation accuracy as a measure of boosting strength. In Figure 4, validation accuracy after the first and fifth epochs of models for different pruning ratios for VGG-16 and ResNet-18 is shown. Boosting tickets consistently achieve higher accuracy than randomly reinitialized sub-models. Boosting effects are strongest with pruning ratios between 60% to 90%, reaching around 80% and 83% validation accuracy for VGG-16 and 76% and 85% for ResNet-18 after the first and fifth epochs. The increase in accuracy between the first and fifth epochs decreases as boosting effects appear, indicating convergence saturation due to the large initial learning rate. The study investigates how model capacity, specifically the depth and width of models, affects boosting tickets. Using WideResNet with fixed depth at 34, increasing width from 1 to 10 shows models with larger capacity have a more significant boosting effect. The largest model, WideResNet-34-10, achieves 90.88% validation accuracy after one training epoch. The boosting effects remain the same when the depth is larger than 22. In the adversarial training setting, the lottery ticket hypothesis and boosting ticket hypothesis are applicable. Pruning on a weakly robust model helps find the boosting ticket for a strongly robust model, saving training cost. Experiments involve using naturally trained and adversarially trained models to obtain tickets through pruning. Retraining these pruned models with the same adversarial training shows accuracy on original and adversarially perturbed validation sets. The study validates the lottery ticket hypothesis and boosting ticket hypothesis in adversarial training. Models trained with FGSM and PGD attacks show superior performance and faster convergence compared to randomly initialized models. Pruned models from these attacks demonstrate improved accuracy on both clean and robust datasets, confirming the applicability of the hypotheses. The study confirms the lottery ticket and boosting ticket hypotheses in adversarial training. Models pruned with FGSM and PGD attacks exhibit similar performance, suggesting training with FGSM first and then retraining with PGD can yield robust models. The FGSM-trained model has 89% robust accuracy against FGSM but only 0.4% against PGD, yet retraining with PGD on boosting tickets from the FGSM model shows robustness. In Ye et al. (2019), the authors argued that the lottery ticket hypothesis fails in adversarial training due to limited model capacity. Pruning small models like a CNN with two convolutional layers resulted in degraded performance, while using a larger model like VGG-16 showed winning tickets again. Adversarial training requires larger model capacity, and pruning small models can undermine their performance. The study found that pruning small models could undermine their performance in adversarial training, as larger model capacity is required. Boosting tickets were identified as providing faster convergence, but only in specific training settings. Training for 60 epochs was shown to achieve similar robust accuracy as training for 100 epochs. This confirms the existence of boosting tickets across different models and training schemes, providing insights into the behavior of pruned models. Boosting tickets can accelerate adversarial training by finding them with FGSM-based training, significantly speeding up PGD-based training. The cost of FGSM-based training is minimal compared to the time saved. Adversarial training on WideResNet-34-10 for 40, 70, and 100 epochs showed improved accuracy under various attacks. Boosting tickets significantly accelerate adversarial training by utilizing FGSM-based training, saving 49% of the total training time compared to the original method. The experiments were conducted on a workstation with 2 V100 GPUs in parallel, showcasing improved accuracy under various attacks. The lottery ticket framework accelerates adversarial training by utilizing boosting tickets, saving 49% of the total training time. This approach is distinct from knowledge distillation and shows improved robustness against PGD attacks. Boosting tickets accelerate PGD-based adversarial training by focusing on reducing the number of epochs required for convergence. This method, coupled with certain initialization, allows for significantly faster training convergence rates. Combining this approach with other methods, such as recycling gradients, could further reduce training time. In adversarial training, pruning a weakly robust model can save up to 49% of training time to achieve a strongly robust model. The study explores finding boosting tickets without training the full model beforehand. The setup includes BatchNorm, weight decay, decreasing learning rate schedules, and augmented training data. One-shot pruning is used instead of iterative pruning on the CIFAR-10 dataset with 5,000 images for validation. Test accuracy is measured on the whole testing set. In experiments, models are trained on Tesla GPUs with data parallelism for time-sensitive tasks. The number of parameters and sizes of model architectures are summarized in Table 4. Training models using iterative and one-shot pruning show similar performance in terms of boosting effects and final accuracy. Validation accuracy of models from both approaches is plotted in Figure 9. In experiments, models are trained on Tesla GPUs with data parallelism for time-sensitive tasks. The number of parameters and sizes of model architectures are summarized in Table 4. Training models using iterative and one-shot pruning show similar performance in terms of boosting effects and final accuracy. Validation accuracy of models from both approaches is plotted in Figure 9. In this section, experiment results on MNIST using LeNet with two convolutions and two fully connected layers are reported. Early stopping is used to determine convergence speed, with boosting tickets converging faster than winning tickets. Results for C&W attacks and transfer attacks are included in Table 6. In experiments, adversarial examples generated from one model can transfer to another model with a slight decrease in robust error. Our adversarial training strategy based on boosting tickets saves up to 49% of training time while achieving higher robust accuracy compared to regular adversarial training on the original full model."
}