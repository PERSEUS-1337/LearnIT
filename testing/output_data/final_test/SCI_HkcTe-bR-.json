{
    "title": "HkcTe-bR-",
    "content": "The design of small molecules for drug discovery is crucial, but challenges persist in computational methods. This work introduces 19 benchmarks for molecular design, expands datasets, and explores new reinforcement learning techniques. The benchmarks, created as OpenAI Gym environments, will be open-sourced to promote innovation. The study also examines the A2C and PPO algorithms for molecular design. The curr_chunk discusses the use of A2C and PPO algorithms in molecular generation, showing improved performance compared to standard reinforcement learning. It focuses on de novo design methodologies using computational methods to automate the design, synthesis, and testing of novel drugs. Symbolic approaches based on graph rewriting are used to create virtual molecules. Graph rewriting techniques have been utilized in domain-specific applications, requiring expert input. Global optimization methods like evolutionary algorithms are used to enhance molecule properties. Symbolic approaches have limitations in generating realistic molecules. Generative models have emerged to learn and generate drug-like molecules, with connections to natural language generation. NLG techniques are being applied to molecular design, utilizing tools like VAE, RNN models, GAN, and MCTS. This work aims to consolidate recurrent models for molecular design using reinforcement learning, proposing 19 benchmarks for de novo design. An implementation of these benchmarks as an OpenAI Gym is provided for further innovation, showcasing state-of-the-art performance with new reinforcement learning techniques. This paper focuses on predicting molecular structures using neural network models and sequence generation. It utilizes Simplified Molecular-Input Line-Entry System (SMILES) notation to encode molecules as sequences, linking them to language-focused neural network models for molecule generation. The study expands on previous work by using a larger dataset relevant to drug discovery. In this study, a larger dataset from the ChEMBL-23 collection is used for predicting molecular structures. The dataset consists of 1,735,442 compounds and their biological activities on 11,538 targets. The work offers 19 benchmarks for molecule generation, including basic physicochemical property optimizations and drug-likeness approximations. An OpenAI Gym interface to these benchmarks will be open-sourced to encourage the development of new RL algorithms for chemistry. The benchmark framework for molecule generation is versatile and not limited to sequence-based methods. It evaluates the percentage of valid molecules and uniqueness of generated samples. Previous work has focused on single objective maximization in molecular generation. The curr_chunk discusses optimization objectives in molecular design, including physicochemical properties like ClogP and MW, as well as SMARTS sequences for specifying substructures. The RDKit provides tools to calculate these functions for use in reinforcement learning scenarios. This work focuses on generating molecules similar to a target using a fingerprint of the target molecule in unexplored chemical space. Functional Connectivity Fingerprint Counts are used to encode molecular graphs, and Tanimoto similarity is calculated between the proposed and target molecules. Marketed drugs are used as molecular targets, and measures are taken to prevent data leakage between training and test sets. The study focuses on generating molecules similar to a target using fingerprint vectors. Tests for balanced optimization include Lipinski's Rule-of-Five parameters and a weighted multi-parameter optimization. The Ro5 evaluates druglikeness based on molecular weight, ClogP, HBD, and HBA. A multiobjective optimization approach is simulated with equally-weighted targets. In this work, a model using Long short-term memory (LSTM) RNN models is trained to process SMILES format strings. The model is connected to a layer of neurons equal to the vocabulary size of the SMILES language and trained using cross-entropy loss. This model, called pretrained maximum likelihood estimation (MLE) model, is used for training sequences of molecules. The pretrained maximum likelihood estimation (MLE) model, trained using supervised gradient descent techniques, is effective in generating realistic chemical sequences. The Hillclimb-MLE algorithm combines the pretrained MLE model with a reward function to maximize reward by sampling and retraining on the top sequences. Recently, advances in reinforcement learning (RL) have led to its application in drug discovery, balancing online learning with costly sample evaluation. RL allows exploration of chemical space without true loss gradients, using SMILES strings as sequences in a discrete state and action space. The goal of an RL agent in drug discovery is to develop a policy to maximize expected rewards by choosing actions in SMILES sequences. Molecule generation is episodic, with rewards only at the completion of the string. Training gradients are acquired for a neural network policy \u03c0 \u03b8. The log-derivative trick is used to acquire a training gradient for a neural network policy in drug discovery. The REINFORCE algorithm is utilized as the policy gradient model, with rewards distributed equally without temporal discounting in SMILES sequences. GANs have gained popularity for their minimax zero-sum game approach, applying novel architecture types to various domains. GANs have been applied as a novel architecture type in various domains, particularly in image synthesis. The method involves using a RL framework with GANs to optimize fitness functions through three phases: pretraining, RL generator phase, and discrimination phase. The optimization phase of GANs involves alternating between pretraining, RL generator phase, and discrimination phase. The reward used to reinforce the network balances the likelihood of the discriminator classifying the generated sequence as true data against the fitness objective. Monte-Carlo rollouts are used to estimate rewards during sequence generation, but this can lead to high variance and undersampling in molecule generation. Advantage Actor-Critic Networks aim to reduce variance in policy gradient training by subtracting an estimate of the reward from the true reward. This approach separates policy training from value estimation, with the actor representing the policy and the critic providing the value estimate. A2C training is a synchronous version of A3C and can utilize generalized advantage estimation. The goal is to simultaneously reduce variance in the loss function. The value estimation function V \u03c6 is a neural network itself, using a fully-connected network from the hidden state of the LSTM to predict reward. Proximal Policy Optimization algorithms have low sample complexity and are reliable in chemistry. The clipped PPO loss function is used to minimize the difference between expected and true rewards. The loss function in BID31 focuses on learning where the objective worsens, with a clipped boundary region to limit changes. A regularization factor on KL divergence can encourage robust learning for molecular generation. The original policy can be adjusted to encourage chemical distribution modeling. A weight parameter \u03bb is used with a pretrained policy \u03c0. Exploration in RL scenarios can be promoted by placing a cost on distribution entropy. The algorithms were implemented using PyTorch and RDKit, with benchmarks created in OpenAI Gym. The code was executed on an NVIDIA Tesla V100 GPU. The final RNN architecture for various models includes three layers of 512 LSTM neurons, with additional layers for embedding and output. Training on the ChEMBL dataset takes approximately eight hours for 70 epochs. The GAN architecture uses a convolutional network for the discriminator. Experiments involved 1.6M action-steps, generating 94.7% valid SMILES out of 10k. The baseline MLE model achieved 94.7% valid SMILES out of 10k generated molecules without fine-tuning, setting a competitive benchmark. A GVAE trained on the same dataset produced 37.1% valid molecules. 99.87% of MLE-sampled molecules were unique, showing high diversity. RL model results for 17 benchmark tasks, including multi-objective optimization and drug-fingerprint targeting, can be found in TAB0 and FIG3. Tanimoto-approach benchmarks involve designing similar drugs to a held-out drug using only its FCFC4 semantic hash. The baseline model samples 10k random molecules, keeping the best-performing ones. The alternating sampling and HC-MLE training algorithm was the most successful model, avoiding nonoptimal local minima and steadily increasing reward. PPO with the clipped objective was the fastest RL algorithm, but occasionally led to less optimal configurations. GAN was also mentioned in the context of molecule fitness function testing. The GAN architecture was found to not be optimal for the task due to the complexity of balancing the discriminator against the generator and choosing a correct architecture. Stronger discriminator weight forces generated molecules to closely match training data, which goes against the purpose of the generator for optimizing towards a given molecule. Future work can explore using GANs to coerce the architecture away from memorizing training data and towards producing diverse sequences. Extended sampling from a well-trained initial model benefits algorithms, with the regularized policy gradient model showing high peak performance but struggling to learn effectively. Sampling from a large, diverse model may be a viable strategy in the absence of effective learning. Temperature Sampling allows for the generation of diverse molecules but higher temperatures may not improve coverage of chemical space. Increasing temperature can lead to invalid SMILES strings, favoring short sequences that undersample the chemical space. To achieve high diversity, a different strategy may be needed. In this study, a large dataset and 19 benchmarks were used to evaluate models for molecule generation. Results showed that the Hillclimb-MLE model outperformed PPO with deep models and large datasets. PPO was effective in constrained compute and sample evaluations, converging faster than other reinforcement-learning algorithms. However, there is still a need for more efficient models for molecular design to impact drug, materials, and agrochemical discovery. The supplementary material provides additional information for reproducibility in molecular design. It includes a SMILES dictionary, drug list, and preprocessing steps for the Chembl-23 dataset. The MLE pretrained-model effectively maps and generates the space of ChEMBL molecules. The molecules are processed by stripping salts and filtering out molecules with SMILES sequences longer than 100 characters. Forbidden symbols and benchmark molecules are also filtered out. The remaining molecules are split into train, test, and development sets before encoding. Multi-character atoms are replaced with single-character substitutes before further processing. The symbols in the SMILES sequences are converted to integer tokens. Various parameters such as batch size, epochs, learning rate, and optimization settings are specified for the training process. The training process involves specifying parameters such as batch size, epochs, learning rate, and optimization settings. This includes values for weight, number of epochs, batch size, max rollout length, time-discount factor, learning rate, PPO settings, total number of actions, maximum gradient norm, and epsilon parameter for the Adam optimizer. The curr_chunk specifies parameters for the training process: Adam optimizer epsilon parameter, number of steps, batch size, entropy coefficient, and value loss weight."
}