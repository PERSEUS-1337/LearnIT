{
    "title": "SJA7xfb0b",
    "content": "The proposed Integral Probability Metric (IPM) is the Sobolev IPM, comparing mean discrepancy of distributions using weighted conditional Cumulative Distribution Functions (CDF) in high dimensions. It extends Von-Mises Cramer statistics to high dimensions and can be used in training Generative Adversarial Networks (GANs) and text generation. A variant of Sobolev GAN shows competitive results in semi-supervised learning on CIFAR-10. Generative Adversarial Networks (GAN) achieve competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN. The generator should mimic the distribution of real data, with recent studies focusing on training stability and data modalities like natural image generation using Deep Convolutional Generative Adversarial Networks (DCGAN). Success in training GANs is attributed to advances in loss functions, stable algorithms, and the representation power of convolutional neural networks. Generative adversarial networks (GANs) have shown success in semi-supervised learning on CIFAR-10, with recent focus on training stability and data modalities like natural image generation using Deep Convolutional GANs. Advances in loss functions, stable algorithms, and the representation power of convolutional neural networks have contributed to the success of GAN training. However, when it comes to neural generators of discrete sequences, GAN theory and practice are not well understood. Various methods such as maximum likelihood pre-training, augmentation, reinforcement learning techniques, Gumbel Softmax trick, and auto-encoders have been proposed for training GANs for discrete sequences generation. End-to-end training of GANs for discrete sequence generation remains an open problem. In this paper, a new Integral Probability Metric (IPM) called Sobolev IPM is proposed for comparing distributions. IPM looks for a witness function that discriminates between samples from two distributions. Different IPMs correspond to different types of witness functions, such as Lipschitz functions for Wasserstein-1 distance and Reproducing Kernel Hilbert Space for MMD distance. The Fisher IPM extends the definition to function classes with norms that depend on the distributions. In this paper, the Sobolev IPM framework is introduced for comparing distributions. Unlike Fisher IPM, which compares joint probability density functions, Sobolev IPM focuses on weighted conditional Cumulative Distribution Functions for all coordinates. Matching conditional dependencies between coordinates is essential for sequence modeling, and Sobolev IPM allows for this by incorporating it into the metric used to learn GANs. In sequence modeling, there is a tradeoff between the metric used (Sobolev IPM vs Fisher IPM) and the architectures (LSTM, GRUs) and conditioning in the generator. The choice of data-dependent function class for the critic impacts handling conditional long-term dependencies. The text discusses the importance of tradeoffs between metrics, data-dependent function classes, and architectures in advancing sequence modeling and structured data generation using GANs. It highlights the suitability of Sobolev IPM for discrete sequence matching and the stability of the ALM algorithm for training Sobolev GAN. Additionally, it explores the elliptic PDE satisfied by the critic of Sobolev IPM and its relation to the Fokker-Planck equation. Empirical studies on character-level text generation using Sobolev GAN are also presented. In character level text generation, Sobolev GAN's conditioning is crucial for success and stability. Text generation can succeed with implicit conditioning using Sobolev GAN or explicit conditioning using Fisher IPM. A variant of Sobolev GAN achieves competitive results in semisupervised learning on CIFAR-10 due to the smoothness from the Sobolev regularizer. The study also reviews different representations of probability distributions and metrics for comparing distributions, essential for training GANs. Metrics are essential for training GANs, focusing on probability measures with weakly differentiable PDFs. The score function of a density function is defined, and metrics between distributions are explored using variational forms and mean discrepancies of functions. \u03d5-divergences and Integral Probability Metrics are discussed in this context. The Sobolev IPM is proposed as a smoothness-constrained alternative to the Wasserstein Distance for GAN training. It generalizes the Cram\u00e9r Von-Mises Distance to high dimensions and avoids the need for a gradient penalty like the Energy Distance. The Sobolev IPM compares weighted conditional CDFs, making it suitable for comparing sequences. It relates to the Stein discrepancy and uses the score function to match distributions. In this section, the Fisher IPM is generalized by relaxing the function class constraints to a tractable data-dependent constraint on the second-order moment of the critic, constraining it to be in a Lebesgue ball. The unit Lebesgue ball is defined in the context of distributions on a space X. The Generalized Fisher IPM relaxes function class constraints to focus on the second-order moment of the critic, constrained within a Lebesgue ball. The Fisher distance and optimal critic function are determined based on maximizing the mean discrepancy between distributions P and Q. The role of the dominant measure \u00b5 in the Fisher distance is highlighted, with choices including symmetric chi-squared distance, implicit distribution \u00b5 GP, and non-symmetric divergence. The Sobolev IPM constrains the critic function in a restricted Sobolev Space. The Sobolev IPM constrains the critic function in a restricted Sobolev Space, moving from a Lebesgue constraint to a Sobolev constraint. This change results in a metric shift from joint PDF matching to weighted conditional CDFs matching, making Sobolev IPM suitable for comparing discrete sequences. Definitions on Sobolev Spaces are recalled, with a focus on functions in the Sobolev space W 1,2 (X , \u00b5) that vanish at the boundary. The Sobolev IPM is defined by restricting the critic of the mean. The Sobolev IPM is defined by constraining the critic function in a restricted Sobolev Space, shifting from a Lebesgue constraint to a Sobolev constraint. It compares high-order partial derivatives of cumulative distribution functions and is suitable for comparing discrete sequences. The main result is presented in Theorem 2, with additional theoretical results in the appendices. The Sobolev IPM compares weighted conditional CDFs and is approximated in a restricted function class like neural networks. The approximation in this class is proportional to the Sobolev IPM, with tightness governed by the optimal critic function. The Sobolev IPM approximation in a hypothesis class is measured using the Sobolev dot product. The convergence in the Sobolev sense to the optimal critic is crucial for GAN training. The space H needs enough representation power to express the gradient of the optimal critic. The optimal Sobolev critic is crucial for GAN training and is the solution to an elliptic PDE. The gradient of the critic defines a transportation plan for moving distribution mass. Theorem 2 compares higher order derivatives of cumulative distributions and discusses the role of the dominant measure \u00b5. Coordinate-wise Conditional CDFs are also compared. The Sobolev IPM compares conditional cumulative distributions of variables on a leave one out basis, emphasizing the comparison of coordinate-wise CDFs. In one dimension, it is equivalent to the Cram\u00e9r Distance. Conditioning is crucial for comparing sequences in this context. The Sobolev IPM compares conditional cumulative distributions of variables, while the Wasserstein distance measures the discrepancy between inverse CDFs. Using Wasserstein distance in GAN training provides signal for far distributions. CDF comparison is more suitable than PDF for comparing distributions on discrete spaces. The optimal Sobolev critic separates two distributions well by numerically solving a PDE on a rectangle with zero boundary conditions. The gradient of the optimal Sobolev critic is then computed on a 2D grid using numerical evaluation of the CDF. The optimal Sobolev critic on a 2D grid is evaluated using numerical methods. The gradient of the critic defines a transportation plan for moving distribution mass from Q to P. The goal is to learn GANs with Sobolev IPM, where a generator is trained to produce data close to a given distribution. Different choices for the distribution are considered, such as sampling from the real distribution or the generator's output. The Sobolev GAN involves choices for the distribution \u00b5 and utilizes an implicit distribution defined by interpolation lines between P and Q. A sphere constraint is imposed rather than a ball constraint, and an Augmented Lagrangian is defined with a quadratic penalty weight. The optimization alternates between the critic and the generator, with the constraint enforced during critic training. The critic is trained by solving for the maximum of the objective function, and then the generator weights are optimized to minimize the objective. The Sobolev GAN optimizes generator weights to minimize the objective function with critic parameters. The algorithm involves a penalty weight, learning rate, and batch size. It differs from WGAN-GP by enforcing constraints on average as a Sobolev norm. Sobolev IPM has important properties related to conditioning and diffusion, which are utilized in text generation and semi-supervised learning with Sobolev GAN. This allows for training GANs without explicit conditioning and enables CDF matching for discrete sequence generation. In this Section, an empirical study of Sobolev GAN in character level text generation is presented. The study focuses on end-to-end training of character-level GAN for text generation, exploring dimensions such as loss function, critic architecture, generator architecture, and sampling distribution. Experiments involve training a character-level GAN on the Google Billion Word dataset following a setup used in a previous study. The study focuses on training a character-level GAN for text generation on the Google Billion Word dataset. Evaluation is based on Jensen-Shannon divergence on 4-gram probabilities. Annealed smoothing of discrete distribution is used to smooth the support of the distribution. The study introduces annealed smoothing of discrete distribution to stabilize training in character-level GAN for text generation. Different noise levels are compared using Resnet architectures with 1D convolution. Noise smoothing is implemented by transforming data into one-hot vectors and sampling from a Gaussian distribution. The study introduces annealed smoothing of discrete distribution to stabilize training in character-level GAN for text generation. Different noise levels are compared using Resnet architectures with 1D convolution. Noise smoothing is implemented by transforming data into one-hot vectors and sampling from a Gaussian distribution. Results show Sobolev GAN achieves slightly better results than WGAN-GP with annealed noise smoothing. Sobolev GAN utilizes annealed smoothing to transition from a continuous to a discrete distribution, improving training stability. Comparisons are made with WGAN-GP using different architectures for generators and critics, highlighting issues with RNN critic optimization. Gradient clipping and parameter adjustments are necessary for stability, with a focus on ResNet generators and various critic architectures. The Fisher GAN fails at text generation with Resnet critics, but shows marginal improvement with RNN critics. RNN critics enable conditioning and factoring of the distribution, lacking in Fisher IPM. Training with recurrent generator and critic using Fisher GAN yields results following BID38's GRU architecture. Conditioning the generator on 32 characters and predicting the rest is done incrementally. The Fisher GAN fails at text generation with Resnet critics but improves with RNN critics. Training with recurrent generator and critic using Fisher GAN yields results following BID38's GRU architecture. Conditioning the generator on 32 characters and predicting the rest is done incrementally. Curriculum conditioning with recurrent critics and generators leads to successful training of Fisher GAN, reaching levels similar to Sobolev GAN and WGAN-GP. The need for explicit conditioning for Fisher GAN highlights the implicit conditioning induced by Sobolev GAN via the gradient regularizer. Using GANs as a regularizer in semi-supervised learning shows promise, with connections to the Laplacian through the Sobolev norm. In this section, a variant of Sobolev GAN achieves competitive performance in semi-supervised learning on the CIFAR-10 dataset without using internal activation normalization in the critic. A convolutional neural network is shared between training a K-class classifier and the GAN critic. The main IPM objective with N samples is followed in the training equations for the critic, classifier, and generator. The generator uses a \"K + 1 parametrization\" for the critic, incorporating the classifier directions to define the \"real\" direction. This adaptation improves results over the regular formulation, especially when applying gradient penalty constraints. The critic is constrained using Fisher and Sobolev constraints to improve SSL performance. The Fisher constraint ensures training stability, while the Sobolev constraint enforces smoothness on the \"fake\" critic. The \"fake\" critic in the training is constrained by Sobolev constraints to ensure smoothness and improve SSL performance. The shared CNN \u03a6 \u03c9 (x) is related to Laplacian regularization in semi-supervised learning. Results of SSL on CIFAR-10 comparing two formulations are shown in Table 2. Hyperparameter and model selection are done on the validation set, with baselines presented using a similar model architecture. The critic does not require normalization due to implicit whitening of feature maps from Fisher and Sobolev constraints. The \"K+1\" parametrization of the critic for semi-supervised learning achieves strong SSL performance without additional tricks. Baselines with * use additional models or data augmentation for an advantage. Results on CIFAR-10 error rates for varying labeled samples are shown in Table 2. The text discusses the performance of different models such as CatGAN, FM, ALI, Tangents Reg BID22, \u03a0-model, VAT BID31, and Bad Gan BID9. It introduces the Sobolev IPM and an ALM algorithm for training Sobolev GAN, highlighting the success of gradient regularization in text generation. The tradeoffs between implicit conditioning in Sobolev IPM and explicit conditioning in Fisher IPM are discussed, showing that Sobolev GAN achieves good results in text generation. The text discusses the success of Sobolev GAN in text generation without normalization, thanks to gradient regularization. It introduces the theoretical properties of Sobolev IPM and its optimal critic as a solution of a non-linear PDE. This approach opens possibilities for designing new regularizers for different types of conditioning in structured data beyond sequences. The optimal critic of Sobolev IPM satisfies a PDE, and the Stein Discrepancy is defined. The convergence of the ratio Qn(x)/P(x) to 1 is discussed, with a connection to the Fokker-Planck Equation. The Fokker-Planck Equation connects the evolution of particles with a density function. The gradient of the Sobolev critic acts as a drift, leading to \"Sobolev descent\" for particle evolution. The limit distribution of particles is P, and the Sobolev descent's relation to Stein Descent will be explored in future work. The Sobolev critic's gradient defines a transportation plan for moving particles with distribution Q. The gradient of the Sobolev critic defines a transportation plan to move particles from distribution Q to distribution P. The objective function and constraints are written in terms of dot product and norm in L2(X, \u00b5) \u2297d. The optimal critic in Equation (3) can be approximated in arbitrary space as long as it has enough capacity. The approximation error is measured with the Sobolev semi-norm, which is stronger than the Lebesgue norm. The proof of Theorem 3 follows similar arguments. The proof of Theorem 3 involves convex optimization and applying KKT conditions to find the optimal solution. The functional derivative of the Lagrangian is used to solve the partial differential equation. The value of \u03bb is determined using a constraint. The Stein operator BID37 is defined and used in defining the Stein discrepancy BID15 BID8 BID27. Barbour generator theory helps construct operators producing mean zero functions under \u00b5. The operator arises from overdamped Langevin diffusion, related to plug and play networks for generating samples. The PDE for the Sobolev Critic can be written in terms of the Stein Operator. The Sobolev IPM critic satisfies the Stein Discrepancy. The Sobolev IPM critic satisfies the Stein Discrepancy, leading to an inequality related to the fitness of the model. Convergence of the PDF ratio is discussed in relation to the Sobolev distance and the Stein discrepancy. Comparing annealed versus non-annealed scenarios is also considered. In comparing annealed versus non-annealed smoothing in Sobolev GAN, annealed smoothing outperforms non-annealed. Using RNN as the critic architecture for WGAN-GP and Sobolev GAN degrades performance due to optimization issues and difficulty in training RNN under the GAN objective without pre-training or conditioning."
}