{
    "title": "SkeKtyHYPS",
    "content": "This study analyzes the effects of adding different noise models to Convolutional Neural Network (CNN) architectures for data augmentation. The results provide new insights and recommendations on noise injection for optimal learning procedures in image classification tasks. Since the success of AlexNet in the 2012 ImageNet competition, training neural networks still requires various solvers, augmentation, and regularization techniques. This study focuses on analyzing noise injection as a crucial data augmentation technique for image processing tasks. Noise addition has been used in signal processing for regression-based methods to create more robust models. Holmstrom & Koistinen (1992) highlighted the use of additive noise as one of the oldest data augmentation methods for training feed forward networks. Noise injection is a crucial data augmentation technique for training neural networks, making models more robust against specific noise types. It is important to carefully select the type and level of noise to optimize network performance. Noise injection is essential for training neural networks to improve robustness against specific noise types. The correct type and level of noise should be chosen to optimize network performance. Techniques like Dropout layers and data augmentation methods such as \"cutout\" regularization can help improve generalization capacity. Studies have shown the importance of noise injection in training deep networks, but more research is needed to establish proper methodologies. This study focuses on determining the optimal noise types and levels to combine with training data for deep neural networks. It explores the effectiveness of active noise injection techniques on various noise models and evaluates the impact of different noise types on CNN architectures. The study also discusses proper methods for adding noise to CNNs for image classification tasks. The study discusses noise in image classification tasks, defining it as unwanted components in images sourced from various sources. Noise can be additive or multiplicative, with Gaussian noise being a common variant. The image signal can be decomposed into desired and unwanted components, with noise impacting CNN architectures. The study discusses multiplicative noise in images, specifically speckle noise assumed to have a Gaussian density function. This noise can be encountered in coherent light imaging like SAR images and those with laser-based illumination. Salt and pepper (S&P) noise and Poisson noise are common types of noise that can affect the performance of CNNs in digital images. S&P noise manifests as a few extremely noisy pixels in an image, while Poisson noise, also known as photon counting noise, has a particular probability density. Poisson noise, also known as photon counting noise, has a specific probability density function with a variance and mean represented by \u03bb. It is signal-dependent and requires a magnitude factor to adjust intensity values. Occlusion noise occurs when important features are obscured in an image, resulting in zero-intensity squares. The magnitude of occlusion noise is determined by the size of the occluding object. In image processing, various types of noise are added to images to assess robustness. Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) are common quality metrics. However, PSNR may lose validity when images have different content or codecs. Structural Similarity (SSIM) is an image quality metric that addresses limitations of PSNR, such as sensitivity to different types of perturbations. SSIM combines luminance, contrast, and structure information to assess similarity between two image signals, providing a score from 0 to 1 where 1 indicates highest similarity. In image classification tasks, SSIM is shown to provide a quality metric closer to human opinion than PSNR. PSNR values of noisy images have higher kurtosis than SSIM counterparts, except for S&P noise, as demonstrated on the Imagewoof subset of ImageNet dataset. The study evaluates the effects of injecting different noise types into training data using SSIM as the primary metric. Different noise magnitudes and types are tested on subsets of the ImageNet dataset, Imagenette and Imagewoof. The Imagenette and Imagewoof datasets consist of 10 classes each with varying numbers of training and test samples. A sweep for mean SSIM is conducted on the Imagewoof dataset to select noise magnitudes, with similar results observed on the Imagenette dataset. Fitting polynomial curves to the noise types is done based on the shapes of the curves. The training model chosen is a 18-layer deep residual network (ResNet18V2) for its well-known architecture and depth. Adam solver with learning rate 1e-4 is used for training without dropout or weight decay. Batch Normalization layers are included. Models are trained for 20 epochs. The chosen CNN architecture, ResNet18V2, is trained with noise injected to the training data for all noise models. A total of 52 networks are trained, with categorical accuracy shown in Figures 4 and 5. The robustness of the models against noise is tested on the noisy test set in Figure 8. Heatmaps in Figures 6 and 7 illustrate the models' robustness against input noise. The results confirm a trade-off between noise robustness and clean set accuracy in image classification tasks. A degradation resulting in 0.8 MSSIM over training data is optimal for creating a robust model against noise. However, SSIM and PSNR may not accurately capture degradation levels for non-homogeneously distributed noise models. The neural networks optimize on the noise level they are trained with, showing robustness for each trained model as long as the MSSIM does not drop below 0.5. Noise removal techniques should be considered if MSSIM drops below 0.5. Occlusion noise type was not thoroughly analyzed due to insufficient comparative data, but other models lack robustness towards this noise type. The lack of robustness in other models towards occlusion noise highlights the importance of \"cutout\" regularization for data augmentation. Gaussian, speckle, and Poisson noises improve model performance and robustness, with Gaussian noise being recommended based on results. Using only one type of noise injection at a certain level is sufficient for image classification tasks involving RGB images of daily objects. In this study, an extensive analysis of noise injection to training data has been conducted. Gaussian noise is recommended for model performance, while S&P noise may not be resolved easily. Two methodologies are suggested: filtering S&P noise or alternating it with Gaussian noise during data augmentation. The constant behavior of models towards occlusion noise lacks a satisfactory explanation, but a longer training procedure may help resolve these issues. The study analyzed noise injection in training data, confirming existing notions and providing new guidelines for CNN training. Future research may focus on extending \"cutout\" regularization and studying the distribution behavior of SSIM and PSNR metrics in relation to previous work."
}