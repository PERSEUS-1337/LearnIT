{
    "title": "HJeb9xSYwB",
    "content": "Adversarial examples are perturbations to input that cause misclassifications in machine learning models. Adversarial training with Voronoi constraints replaces the commonly used $L_p$-ball constraint, leading to robust models that outperform state-of-the-art on MNIST and are competitive on CIFAR-10. This approach has shown significant improvements in computer vision, natural language processing, and robotics. Adversarial examples in machine learning models have led to a cat-and-mouse game between defenses and attacks. Researchers are developing robust methods to counter adversarial attacks in various applications such as healthcare and autonomous vehicles. In response to the challenge of adversarial attacks in machine learning models, a modification to adversarial training is proposed in this paper. The standard L p -ball constraint is replaced with Voronoi cells of the training data, offering advantages such as adaptive perturbation size and improved robustness. The method of constructing adversarial examples within Voronoi cells and incorporating Voronoi constraints into training is detailed, showing promising results in defending against attacks. Adversarial training with Voronoi constraints is shown to provide state-of-the-art robustness on MNIST and competitive results on CIFAR-10. Goodfellow et al. (2014) proposed adversarial training with a strong adversary using projected gradient descent, leading to empirically robust models. This approach was not fully circumvented by later attacks, unlike other methods surveyed. The TRADES algorithm aims to balance robustness and natural accuracy by decomposing robust error into natural error and error near the decision boundary. It constructs a decision boundary far from the data distribution, similar to other heuristic defenses. This approach is compared exclusively against such defenses due to the frequency of heuristic defenses being defeated by stronger attacks. The initial work on certifiable robustness aimed to guarantee no perturbation within an L pball radius could change the classifier's classification. It approximated possible activations of perturbations by propagating upper and lower bounds through the network. However, subsequent works have tried to address drawbacks, such as scalability issues and dependency on specific details. Randomized smoothing is a new approach to certified robustness that does not rely on specific architecture details. It constructs a new classifier that is certifiably robust under adversarial L2 perturbations by leveraging the classifier's ability to perform well on Gaussian noise. However, randomized smoothing is currently limited to L2 perturbations and has been combined with adversarial training for even more robustness. The combination of randomized smoothing and adversarial training produces more robust classifiers in L2. Previous work has explored the theory of adversarial examples and the sample complexity required to produce robust models. Adversarial examples may arise from computational constraints, as shown by pairs of distributions that differ only in a k-dimensional subspace. The work of Gilmer et al. (2018) and Shafahi et al. (2019) explore the pervasiveness of adversarial examples in high-dimensional geometry. They suggest that adversarial examples may be unavoidable due to the data's high-dimensional nature. These examples occur on the data manifold, and distinguishing between distributions in a k-dimensional subspace requires exponentially many queries. The main drawback of previous works is the assumption that data distribution has full dimension, while in practice, it is often supported on a low-dimensional subset. Khoury and Hadfield-Menell (2018) address this by considering adversarial robustness in low-dimensional manifolds. The role of co-dimension, the difference between embedding space and data manifold dimension, is highlighted as a key factor in adversarial vulnerability. This idea is also explored in Nar et al. (2019). In Nar et al. (2019) and Khoury and Hadfield-Menell (2018), the focus is on adversarial examples and robustness in high-codimension settings. Goodfellow et al. (2014) proposed adversarial training using L p -balls, but recent work explores alternative adversaries. In high-codimension settings, Khoury and Hadfield-Menell (2018) discuss issues with using L p -balls for constructing adversarial examples. They show that a nearest neighbor classifier covers the neighborhood around the data manifold more effectively than a robust empirical risk minimization oracle. The vulnerability increases with the codimension d - k of the manifold. The Voronoi diagram for a dense sample drawn from a low-dimensional distribution with two classes, one in red and one in black, shows cells varying in size based on proximity to samples in the other class. Voronoi cells are defined as the set of points closer to a sample x than any other in X, replacing the L p -ball constraint. The Voronoi cell constraint offers advantages over the L p -ball constraint. Voronoi cells partition the entirety of R d without intersecting, making them suitable for high codimension settings. The size of Voronoi cells adapts to the data distribution, with cells closer to samples from a different class being smaller. This eliminates the need to set a value for in the optimization procedure. The Voronoi constraint allows the adversary to explore the neighborhood around M during adversarial training. The inner optimization problem must be solved at each iteration to generate an adversarial example. Goodfellow et al. (2014) use the fast gradient sign method (FGSM), while we use projected gradient descent. Incorporating Voronoi constraints requires solving the inner optimization problem to maximize L(x, y; \u03b8) at each iteration of the outer training loop. The inner optimization problem involves maximizing L(x, y; \u03b8) with Voronoi cells. For p = 2, convex cells allow projection using quadratic programming. Problem 2 can be solved with projected gradient descent. When p = 2, non-convex cells require heuristic methods like barrier and penalty methods. Adversarial examples are generated by iterative steps in the gradient direction, checking for Voronoi constraint violations before updating. The procedure for constructing an adversarial example within a Voronoi cell involves taking iterative steps in the gradient direction, checking for constraint violations, and finding nearest neighbor samples from other classes. Few samples contribute to the Voronoi cell of x, especially those shared by samples in different classes. In adversarial training with Voronoi constraints, the objective is formalized by introducing a robust objective function. The main contribution is using a strong adversary with projected gradient descent. Voronoi constraints are incorporated by replacing the L p -ball constraint with the Voronoi cell at x. The optimization procedure described in Section 3 is used to solve the inner optimization problem. In 2018, a synthetic dataset called PLANES was introduced to study the impact of dataset codimension on robustness. The dataset consists of two 2-dimensional planes with specific axis boundaries. The training and test sets are sampled in a specific way to ensure perfect natural accuracy. Experiments were also conducted on MNIST and CIFAR-10 datasets using a fully connected network with specific parameters. Our experiments on MNIST and CIFAR-10 use a model with 1 hidden layer, 100 hidden units, and ReLU activations. The learning rate for Adam is set at \u03b1 = 0.1, and results are averaged over 20 retrainings. We train the MNIST model using Adam for 100 epochs and the CIFAR-10 model using SGD for 250 epochs. Adversarial attacks such as projected gradient descent (PGD) and fast gradient sign method (FGSM) are applied to evaluate model robustness. The Voronoi constraints reduce model accuracy, and the robust classification accuracy is plotted for each dataset. The normalized area under the curve (NAUC) is used to measure robustness, with higher values indicating more robust models. Implementation involves constructing adversarial examples within Voronoi cells, requiring a nearest neighbor search query. Nearest neighbor search query is crucial for data augmentation during training. 16 threads with k-d trees pull mini-batches for queries. Adversarial training with Voronoi constraints runs similarly to standard training. Khoury and Hadfield-Menell (2018) found less robustness with increasing codimension in the PLANES dataset due to L2-balls covering smaller fractions. The L2-balls with radius 1 around the dataset covered a smaller fraction of the neighborhood. Adversarial training with Voronoi constraints was evaluated on MNIST and CIFAR-10, outperforming the Madry model on MNIST for perturbations greater than 0.3. The model adapts to the maximum perturbation size locally on the data distribution, maintaining 76.3% accuracy at perturbation size 0.4. Our model outperforms the Madry model on MNIST, achieving 76.3% accuracy at perturbation size 0.4 compared to 2.6%. It also achieves an NAUC of 0.81, a 20.8% improvement over the baseline. On CIFAR-10, both models have an NAUC of 0.29, but our model prioritizes robustness to larger perturbations over natural accuracy. Increasing the maximum perturbation size during training results in significantly worse robustness. The approach of not requiring to set a maximum amount of robustness achievable may not be known a-priori. Adversarial training with Voronoi constraints improves robustness by allowing the adversary to explore the neighborhood around the data distribution, unlike the L p -ball constraint which has drawbacks in highcodimension settings."
}