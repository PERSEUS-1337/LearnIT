{
    "title": "rkxg4a3m9N",
    "content": "The importance of interpretability in AI agent behavior for human-AI interaction has led to interest in generating interpretable behavior. One approach is to design the agent's environment to prevent uninterpretable behaviors. This position paper explores different types of interpretable behaviors and environment redesign, focusing on explicability, legibility, and predictability. The design of human-aware AI agents must ensure interpretable behavior to prevent cognitive overload and increase trust. Uninterpretable behavior can lead to reduced productivity and increased risk. BID5 emphasizes the importance of humans being able to interpret agent activities. Uninterpretable behavior may occur if the human has incorrect beliefs about the agent, is unaware of its goals, or cannot predict its plan. Interpretable behavior of human-aware AI agents is crucial to prevent cognitive overload and increase trust. The agent must consider the human's expectations, known as the human mental model, to ensure its behavior is understandable. Three key properties of interpretable behavior are explicability, legibility, and predictability. Existing works focus on how the agent can adjust its planning process to generate these behaviors. The agent altered its planning process using the human's mental model to exhibit desired behavior. Environment design can optimize behaviors by modifying the environment to achieve specific objectives. Environment design is crucial for optimizing behaviors in structured settings with repetitive objectives or multiple actors. Goal recognition design involves updating the domain for more explicable behavior, inducing legible behavior with dividing walls, and inducing predictable behavior by reducing uncertainty. The goal of this paper is to bridge the gap between legible behavior and predictable behavior in environment design. By reducing uncertainty and inducing predictable behavior, the design aims to make actor goals easier to recognize. This approach complements human-aware decision-making and emphasizes the importance of interpretable behavior in structured settings. The design of environments aims to make behavior more interpretable to observers by offloading the generation of interpretable behavior from the actor to the design process. This approach allows for the actor to still plan traditionally while producing interpretable behavior. Design in the traditional sense means behavior is interpretable to observers without relying on the agent's cooperation. It complements communication by changing the observer's beliefs in model space. For example, in an office setting, an office assistant robot is about to be deployed and supervised by office security. The new office assistant robot is more flexible than previous models, as it can carry both mail and coffee at the same time. This added feature may confuse the security guards who expect the robot to carry one item at a time. If the robot acts optimally, it could potentially confuse the observers, so it may need to adjust its behavior to be more interpretable. The new office assistant robot, designed to carry both mail and coffee simultaneously, may need to adjust its behavior to conform to expectations and ensure interpretability. Designers can prioritize explicability by disabling the coffee holder, prompting the robot to deliver items sequentially. This change helps users differentiate between models early on. To enhance predictability and interpretability, disable the coffee holder and introduce obstacles for the robot. Use a tray for carrying both items simultaneously, allowing users to predict the plan early on. Additional obstacles can further restrict possible plans. An interpretable decision-making problem involves an actor and an observer in an environment. An interpretable decision-making problem consists of a tuple, P Int = P A , P O , Int, where P A is the actor's decision-making problem, P O is the observer's mental model of the actor, and Int is the interpretability score used to evaluate agent plans. The solution to P Int is a plan that solves P A and meets desired properties of interpretable behaviors. These properties could include explicability, legibility, or predictability of the plan. The actor's behavior is considered explicable if it aligns with the observer's expected plans. Legibility aims to reduce the size of possible models, while predictability focuses on generating behavior that can be easily predicted by the observer. In the context of behavior predictability, the observer should be able to predict the completion of an actor's plan based on a given prefix. The shorter the prefix, the more predictable the plan. A general formulation for designing interpretable behaviors is presented, assuming the actor generates cost-optimal plans. The interpretability of a plan can vary from high to low, requiring a measure to quantify it. The worst-case interpretability score wci is introduced to quantify the interpretability of an actor's set of cost-optimal plans. It is defined as the minimum interpretability score of a plan. Changes in the environment affect the worst-case interpretability score of the actor's plans. The worst-case interpretability score of an actor's cost-optimal plans can be affected by changes in the environment. The design problem for interpretability involves initial models, modifications to the environment, and model transition functions. The set of possible modifications includes changing states, action preconditions, action effects, action costs, initial state, and goal. The design problem for interpretability involves modifications to the environment, such as changing states, action preconditions, effects, costs, initial state, and goal. Each modification has an associated cost, and the objective is to maximize the interpretability score of the decision-making problem while minimizing design costs and plan costs. Solutions involve sequences of modifications to achieve the desired interpretability. The explicability of an actor's plan depends on how different it is from the expected plans of the observer, quantified by an explicability score function. This function computes the distance between the actor's plan and the observer's mental model, typically based on cost differences. The legibility of an actor's plan is determined by its unique cost optimal completion in the observer's mental model. This is quantified by a legibility score function, which is a special case of the design problem for explicability. Goal Recognition Design (GRD) is a related concept involving an actor and observer in understanding plan legibility. The Goal Recognition Design (GRD) problem involves an actor and an observer with different goals, aiming to reveal the actor's true goal early on. The interpretability problem focuses on the observer's mental model being different from the actor's planning model, requiring the plan to be predictable for the observer. The predictability of a plan is determined by the length of its shortest prefix, impacting the observer's mental model. A predictability score can be quantified for an actor's plan, which relates to the plan recognition design problem. The proposed framework supports a generative model of observer expectations but has limitations that may be addressed in the future. The proposed framework highlights limitations in handling multiple decision-making problems and suggests extensions for future applications. The interpretability score of agent behavior can exhibit properties of being explicable, legible, and predictable simultaneously. The formulation in Equation 2 allows for the combination of these properties in handling various scenarios. The design objective is to minimize the worst-case interpretability score while maximizing scores for each property in the modified environment. Offloading computational load to the offline design stage is advantageous for interpretability. Environment design has a more permanent effect compared to operating on the human mental model. Interpretable behavior does not usually affect the actor going forward, but designing the environment requires the actor to adapt. The design of the environment has a lasting impact on the actor, affecting all decisions and interactions. This loss of autonomy due to environment design should be considered in the design process."
}