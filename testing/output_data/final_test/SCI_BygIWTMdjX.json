{
    "title": "BygIWTMdjX",
    "content": "Network pruning is a technique to reduce the size of deep neural networks by removing unimportant connections. New methods allow for training sparse networks directly, leading to smaller memory footprints. Dynamic parameter reallocation during training helps find optimal subnetworks, converging early to a highly trainable subnetwork for deep convolutional networks. Dynamic parameter reallocation improves the trainability of deep convolutional networks by reallocating parameters during training, leading to successful learning. This process plays a similar role as overparameterization without the associated memory and computational costs. Training high-performance compact networks often involves compressing a larger network through techniques like pruning, distillation, or low-rank decomposition. Network pruning is a common method to uncover high-performance subnetworks within an original network. Dynamic parameter reallocation (DPR) improves the trainability of deep convolutional networks by reallocating parameters during training, leading to successful learning. This search-based procedure starts with a sparse network and continuously reallocates non-zero parameters based on a simple heuristic, resulting in high-performance subnetworks that rival those obtained through iterative pruning of overparameterized models. This challenges the belief that overparameterization is necessary for successful learning. Dynamic parameter reallocation (DPR) is proposed as an effective approach to improving the trainability of deep convolutional networks. Various methods like BID4, Mocanu et al. [2018], and BID1 have utilized DPR for training sparse networks. BID1, in particular, outperformed other methods when training deep CNNs. This method is used to investigate how DPR trains high-performance sparse networks, with similarities to Neural architecture search (NAS) techniques. Neural architecture search (NAS) techniques, similar to DPR methods, search for compact high-performance networks. One-shot architecture search evaluates subnetworks of a large network to find the best performing one. Algorithm 1 summarizes the DPR mechanism introduced by BID1, reallocating parameters in a two-step procedure during training. Neural architecture search techniques, like DPR methods, aim to find compact high-performance networks. Algorithm 1 from BID1 describes the DPR mechanism, reallocating parameters during training. WRN-28-2 BID2 was trained on CIFAR10 with data augmentation and DPR hyperparameters, achieving high generalization performance. After training with DPR, the structure of the final subnetwork was retained and re-trained without DPR, but failed to reach the same accuracy. The network structure and initialization both contribute to reaching high accuracies. The combination of final structure and original initialization in the DPR training instance fell short of achieving the same accuracy as DPR training. Training with random initialization performed better than random subnetwork and initialization trials. DPR discovered trainable network structures early in training. Interestingly, DPR found trainable network structures early in training, contradicting the lottery ticket hypothesis that overparameterization leads to better performance. Structure and initialization alone were not enough to train compact sparse CNNs successfully, highlighting the importance of dynamics and extra degrees of freedom provided by DPR. DPR, similar to overparameterization, enhances network trainability by allowing exploration of additional degrees of freedom. Unlike overparameterization, DPR introduces extra degrees of freedom through simultaneous exploration of different subnetwork structures during training. It is a more efficient method in terms of computational and memory resources compared to overparameterization."
}