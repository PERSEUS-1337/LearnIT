{
    "title": "rkxbn735IH",
    "content": "The goal is to design an algorithm for robust one-bit compressed sensing to recover sparse target vectors. In this paper, a new framework is proposed for the one-bit sensing problem, where sparsity is enforced through a generative network. The goal is to recover the target using empirical risk minimization. In a new framework for the one-bit sensing problem, recovery of the target $G(x_0)$ is proposed through unconstrained empirical risk minimization under a sub-exponential measurement assumption. The ERM estimator achieves an improved statistical rate of $m=\\tilde{\\mathcal{O}} (kn\\log d /\\epsilon^2)$ for recovering any $G(x_0)$ uniformly up to an error $\\varepsilon$. The empirical risk, despite non-convexity, has no stationary point outside small neighborhoods around the true representation $x_0$ and its negative multiple. The global minimizer of the empirical risk remains within the neighborhood around $x_0\". Our analysis explores the potential of inverting a deep generative model with partial and quantized measurements, complementing the use of deep generative models for inverse problems. Quantized compressed sensing focuses on recovering a high-dimensional vector from limited quantized measurements. One-bit compressed sensing, aiming to recover a target vector from single-bit observations, poses a significant challenge. Previous theoretical successes rely on assumptions of Gaussianity of sensing vectors and sparsity of the target vector on a given basis. In this work, a new framework for robust dithered one-bit compressed sensing is introduced, addressing limitations in generating Gaussian vectors and high-dimensional targets. The target vector structure is represented by a ReLU network, and a recovery algorithm is proposed based on unconstrained ERM. The algorithm shows favorable properties when measurements are sub-exponential random vectors. The ERM solution for robust dithered one-bit compressed sensing can recover the true vector up to error \u03b5 with a sample size of m \u2265 O(kn log 4 (\u03b5 \u22121 )(log d + log(\u03b5 \u22121 ))/\u03b5 2 ). This result does not require REC type assumptions and weakens sub-Gaussian assumptions. It meets the minimax optimal rate for sparse recovery and improves upon the best known statistical rate for computationally tractable algorithms. The computation of the true representation x 0 \u2208 R k is tractable, with a descent direction always existing with high probability. Our result guarantees a descent direction outside small neighborhoods around x 0, improving computational guarantees for ReLU generative signal recovery in linear models with small noise. The proof utilizes the piecewise linearity property of ReLU networks, allowing for improved performance without REC type assumptions and under quantization errors. The current scenario introduces novel arguments for sub-Gaussian free type bounds and discusses the ease of binary embedding into Euclidean space compared to bounded k sparse sets. Random variables and vectors are defined as sub-exponential, and constants are denoted for reference. The focus of the paper is on one-bit recovery. In this paper, the focus is on one-bit recovery in a high-dimensional scenario where the dimension of the representation space is much less than the ambient dimension. A fixed ReLU neural network is used, and the quantized measurements involve random vectors and thresholds. The number of layers in the network is smaller than the ambient dimension, and the quantization threshold follows a uniform distribution. The paper focuses on one-bit recovery in high-dimensional scenarios using a fixed ReLU neural network with quantized measurements. The measurements involve random vectors and thresholds following a uniform distribution bounded by a parameter \u03bb. The goal is to compute an ERM for x m, with statistical guarantees based on assumptions about the measurement vector and noise. The paper discusses one-bit recovery in high-dimensional scenarios using a fixed ReLU neural network with quantized measurements. The goal is to compute an ERM for x m with statistical guarantees based on assumptions about the measurement vector and noise. The sample complexity is enforced by a constant C, and the result is a uniform recovery bound with high probability for any target x 0 satisfying G(x 0 ) 2 \u2264 R. The paper introduces the Weighted Distribution Condition (WDC) for matrices in the context of sparse one-bit sensing with a fixed ReLU neural network. It defines various notations and representations for the network, emphasizing practical recovery algorithms without the need for prior knowledge of bounds. The paper introduces the Weighted Distribution Condition (WDC) for matrices in the context of sparse one-bit sensing with a fixed ReLU neural network. It defines various notations and representations for the network, emphasizing practical recovery algorithms without the need for prior knowledge of bounds. Theorem 2.2 shows that under certain conditions, local minimum can only lie in small neighborhoods of two points x 0 and its negative multiple \u2212\u03c1 n x 0. The Weighted Distribution Condition (WDC) is introduced for matrices in sparse one-bit sensing with a fixed ReLU neural network. Theorem 2.2 states that local minimum can only be in small neighborhoods of points x 0 and its negative multiple \u2212\u03c1 n x 0, based on certain conditions. The global minimum of the loss function for a ReLU network with WDC is determined in Theorem 2.3. It states that with certain conditions, the loss function is minimized in specific neighborhoods of points x 0 and its negative multiple \u2212\u03c1 n x 0. The significance of Theorem 2.3 lies in showing that the value of ERM is smaller around x 0 compared to its negative multiple \u2212\u03c1 n x 0. It also guarantees that the global minimum of L(x) stays around x 0 when the accuracy level \u03b5 wdc is small. If \u03b5 wdc \u2264 cn \u221276 for some constant c, then the global minimum of the proposed ERM is in a specific neighborhood around x 0. Further optimization of this dependency is left for future work."
}