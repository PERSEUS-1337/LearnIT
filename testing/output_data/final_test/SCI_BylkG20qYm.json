{
    "title": "BylkG20qYm",
    "content": "Adversarial examples test neural seq2seq model robustness by perturbing inputs without changing semantics. A new evaluation framework for adversarial attacks in machine translation considers meaning preservation. Proposed constraints for attacks on word-based MT systems result in more semantically similar inputs. Adversarial training with meaning-preserving attacks improves model robustness. Attacking machine learning models with adversarial perturbations improves adversarial robustness without impacting test performance. Adversarial attacks reveal vulnerabilities in models, especially in systems like self-driving cars. While originally studied in computer vision systems, attacks in natural language sentences are more noticeable even with small changes. In the context of natural language processing, the concept of adversarial perturbations is explored. Previous mathematical frameworks are not directly applicable to discrete text data, and there is no canonical distance metric for textual data like in real-valued vector spaces. A criterion for adversarial examples in NLP is proposed, focusing on preserving meaning on the source side while destroying it on the target side. This approach contrasts with previous work on adversarial examples for seq2seq models. The paper discusses adversarial examples for seq2seq models and the importance of meaning preservation in model outputs. It introduces a method for formalizing meaning-preserving perturbations and proposes constraints for gradient-based word substitution attacks to increase the chance of preserving meaning. Experiments are conducted to evaluate meaning preservation in model outputs. The experiments in the paper evaluate meaning preservation in seq2seq models by proposing a \"source-meaning-preserving, target-meaning-destroying\" criterion for adversarial examples. The study shows that chrF metric correlates better with human judgments than BLEU and METEOR metrics. Constrained substitution attacks are found to preserve meaning better than unconstrained attacks, but still degrade system performance. Adversarial training with semantically similar inputs improves robustness to adversarial attacks. The section presents a procedure for evaluating adversarial attacks on seq2seq models, focusing on measuring target similarity between original and perturbed translations. The goal is to identify failure cases for the model by assessing the similarity between the target sentences. When evaluating adversarial attacks on machine translation models, it is important to quantify the discrepancy between the original and adversarial input sentences in a way that captures meaning. This is done by defining target relative score decrease, which allows for comparison across different models or languages. The effectiveness of attacks may vary between language pairs, with some being easier to translate than others. The language pair's effectiveness in adversarial attacks varies, with some pairs easier to attack than others. It is recommended to report both the source and target scores in attack results. If a single number is needed, the success of the attack can be represented by S := s src + d tgt, where S > 1 indicates more destruction of target meaning than source meaning. The semantic similarity scores s src and s tgt are not precisely defined, and different metrics can be used to calculate them. In the context of adversarial attacks, a 6-level evaluation scheme is proposed for semantic similarity, considering meaning preservation and fluency of the output. The scheme rates the similarity between sentences based on different levels of meaning and key information. Automatic evaluation metrics are used as alternatives to human evaluation, especially for low-resource languages. Three commonly used metrics are described: BLEU, METEOR, and BID25. BLEU is based on n-gram precision with a penalty for shorter sentences, while METEOR estimates alignment and computes unigram F-score. METEOR uses stemming, synonymy, and paraphrasing for alignments, but requires language-specific resources. chrF: BID26 is based on character n-gram F-score, with chrF2 score emphasizing recall. It operates on a sub-word level, reflecting semantic similarity without language-specific knowledge. Comparison to human judgment will be done in \u00a74.2. In this section, the paper discusses adversarial attacks on a word-based translation model using gradient-based techniques to maximize an adversarial loss function. The optimization problem involves finding the position and word that maximize the adversarial objective, which can be solved through brute-force with a specific space and time complexity. The paper discusses adversarial attacks on a word-based translation model using gradient-based techniques to maximize an adversarial loss function. The optimization problem involves finding the position and word that maximize the adversarial objective, with a space complexity of O(n|V|) and a time complexity bottlenecked by a |V|d times nd matrix multiplication. The attacks benefit from normalizing the gradient by taking its sign, and extending the approach to finding optimal perturbations for more than 1 substitution would require exhaustive search. The paper discusses adversarial attacks on a word-based translation model using gradient-based techniques to maximize an adversarial loss function. In contrast to previous methods, the proposed approach focuses on creating adversarial perturbations that preserve meaning by restricting word substitutions to similar words. Two sets of constraints are compared: kNN, which replaces words with their 10 nearest neighbors in the embedding space to ensure semantic similarity, and CharSwap, which requires substituted words to be obtained by swapping. CharSwap constraint requires swapping characters to generate substituted words, aiming to preserve meaning. Additional constraint of substitution not in vocabulary may be meaning-destroying. In cases where character swaps are not possible, the last character of the word is repeated. The base attack without constraints is referred to as Unconstrained. Examples of perturbations under different constraints are provided in TAB0. The experiments in the current work focus on evaluating adversarial attacks and comparing their effectiveness in preserving meaning. Different constraints, such as character-based or subword-based models, were considered but deemed beyond the scope of the study. The evaluation framework aims to determine which automatic metrics align better with human judgment when assessing adversarial attacks. Adversarial attacks constrained to preserve meaning tend to receive higher assessment scores. In experiments evaluating adversarial attacks, constraints preserving meaning yield better assessment scores. Data from IWSLT2016 dataset is used for experiments in various language directions. Two NMT models are tested: LSTM with attention and Transformer with self-attention. The second model is a self-attentional Transformer with 6 encoder and decoder layers and 512 dimensional word embeddings. Models are trained with Adam, dropout of probability 0.3, and label smoothing with value 0.1. During inference, <unk> tokens are replaced with source words with highest attention value. Evaluation metrics include BLEU score, METEOR, and ChrF using sacreBLEU implementation. Source code and experimental setup are available at redacted_url. In an experiment, the correlation between automatic metrics and human judgment for adversarial attacks on the BiLSTM model in fr-en is examined. 900 sentence pairs are randomly selected for the French side, with varying levels of perturbation. On the English side, 900 pairs of reference translations and translations of adversarial input are selected. These sentences are rated by English and French speaking annotators. The correlation between automatic metrics and human judgment for adversarial attacks on the BiLSTM model in fr-en is examined. 900 sentence pairs are rated by English and French speaking annotators. chrF shows higher correlation with human judgment compared to METEOR and BLEU. chrF is significantly better than METEOR in French but not in English due to METEOR having access to more language-dependent resources in English. In the following sections, attack results are reported in terms of chrF in the source (s src) and relative decrease in chrF (RDchrF) in the target. Attacks are conducted on the validation set using 3 substitutions, with results reported on a scale of 1 to 100 for readability. A graphical representation of the results is provided for better understanding of the different variables (language pair, model, attack). Adding Constraints Helps Preserve Source Meaning: Constrained attacks positively impact meaning preservation compared to unconstrained attacks. Results show that source chrF is useful for comparison. Different models have varying performance when faced with adversarial attacks, as seen in the target-side results. The Transformer models are less robust to small embeddings perturbations compared to BiLSTM models. CharSwap constraint consistently produces successful attacks. Adversarial training involves augmenting data with adversarial examples. In adversarial training, the loss function is replaced with an interpolation of the original sample and an adversarial sample. While adversarial training improves robustness, it may harm test performance on non-adversarial input. The study investigates the impact of adversarial attacks that are largely meaning-preserving. Adversarial examples are generated on the fly under the CharSwap constraint to maintain training speed. This approach results in weaker adversaries but makes training time less than 2\u00d7 slower than normal training. Comparisons are made with adversarial perturbations chosen using a gradient-based approach. Adversarial training with CharSwap attacks improves robustness to adversarial attacks without impacting test performance on non-adversarial data. The study shows that adversarial training has a positive influence on robustness, as observed in the RDchrF of CharSwap attacks on validation data. This is likely because CharSwap adversarial inputs are more constrained and less likely to change the meaning of the source sentence. The text discusses adversarial attacks in NLP, including gradient-based attacks and defense mechanisms. It mentions attacks on sentiment, malware, gender, and toxicity classification. While some methods focus on target side evaluation, there are works on meaning-preserving adversarial attacks through paraphrase generation or rule-based approaches. These attacks are highly engineered and primarily focused on English. This paper discusses the performance of meaning-preserving adversarial perturbations for NLP models, focusing on seq2seq. It introduces a general evaluation framework for adversarial perturbations and explores automatic metrics as alternatives to human judgment. The study reveals that \"naive\" attacks in machine translation do not preserve meaning and suggests remedies. Additionally, the utility of adversarial training in this context is demonstrated, aiming to improve the evaluation of meaning conservation in future research."
}