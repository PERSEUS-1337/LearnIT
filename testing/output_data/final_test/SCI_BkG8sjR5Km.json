{
    "title": "BkG8sjR5Km",
    "content": "In a challenging competitive multi-agent soccer environment with continuous simulated physics, decentralized, population-based training with co-play leads to a progression in agents' behaviors towards cooperation. Challenges in large scale multi-agent training in continuous control are highlighted, showing that optimizing simple rewards can result in long-horizon team behavior. An evaluation scheme based on game theoretic principles assesses agent performance without predefined tasks or human baselines. Recent breakthroughs in AI have been achieved by combining deep reinforcement learning with self-play, leading to superhuman performance in games like Go and Poker. Competitive games have a natural curriculum property that allows complex behaviors to emerge in simple environments through competition between agents. End-to-end RL has been used to address challenging collaborative-competitive multi-agent environments, such as visually complex first-person 2v2 video games. Robot soccer, including simulated leagues, remains a longstanding challenge in AI that has been tackled with machine learning techniques but not yet mastered by end-to-end methods. In the context of recent AI breakthroughs, machine learning techniques have been used to tackle challenges in robot soccer, but end-to-end reinforcement learning has not yet mastered this. A new research environment with competitive multi-agent soccer games is introduced to study complex behaviors through competition. The environment uses simulated physics and focuses on continuous multi-agent reinforcement learning challenges like coordination and shaping rewards. In the context of recent AI breakthroughs, machine learning techniques have been used to tackle challenges in robot soccer. A framework for continuous multi-agent RL based on decentralized population-based training (PBT) is studied, focusing on challenges like coordination, shaping rewards, exploitability, and evaluation. The framework demonstrates that end-to-end PBT can lead to emergent cooperative behaviors in the soccer domain. PBT provides a mechanism for evolving shaping rewards over time based on competitive match results. The reward is decomposed into separate weighted channels with individual discount factors, optimized online. PBT evolves agents' shaping rewards towards team objectives, leading to cooperative and strategic behaviors in robot soccer. Evaluation in competitive multi-agent domains remains an open question, traditionally relying on handcrafted bots or human baselines. In competitive research domains, handcrafted bots or human baselines are often used, but they can be hard to design. This paper discusses the issue of diversity and exploitability in evaluators by observing non-transitivities in agents' rankings. The evaluation scheme is based on Nash averaging, with agents tested against pre-trained ones. The soccer domain is treated as a multi-agent reinforcement learning problem, where agents interact with the environment to optimize rewards. MARL can be cooperative, competitive, or a mix of both, depending on agents' rewards alignment. The text discusses the evaluation of agents in competitive research domains, focusing on diversity and exploitability. It introduces the concept of Nash averaging for evaluating agents in the soccer domain as a multi-agent reinforcement learning problem. The Markov game model includes state space, multiple agents with observation and action sets, reward functions, observation functions, and transition functions. The training process involves population-based training for multi-agent reinforcement learning. The text discusses population-based training for multi-agent reinforcement learning, where agents play TrainingMatches and update network parameters using Retrace-SVG0. PBT is extended to incorporate co-play BID21 for optimizing agents in MARL. Agents inherit network parameters and hyperparameters from stronger agents with additional mutation, allowing hyperparameters to evolve during training. Population-based training (PBT) for multi-agent reinforcement learning involves optimizing agents in TrainingMatches using Retrace-SVG0. PBT achieves robustness by training a population of learning agents against each other. PBT-MARL is described for a population of N agents in this work, utilizing Stochastic Value Gradients (SVG0). In multi-agent reinforcement learning, Stochastic Value Gradients (SVG0) BID15 is used as the reinforcement learning algorithm for continuous control. A recurrent critic is employed to estimate correct values for the game by implicitly considering other players' behavior. Recurrent Q-function, learned from partial unrolls, is found to perform well in practice. In reinforcement learning, reward shaping is challenging, especially in continuous control and cooperative multi-agent settings. Sparse rewards may require additional signals to encourage exploration and prevent degenerate behaviors. Team rewards in multi-agent settings can be difficult to optimize due to complex credit assignment. In reinforcement learning, reward shaping is challenging in cooperative multi-agent settings. Designing shaping rewards to induce desired behavior is difficult. Shaping rewards are weighted to optimize co-operative behavior, using population-based training. The shaping rewards are simple individual rewards to aid exploration but can lead to degenerate behaviors if not scaled properly. The weighting of shaping rewards can be automatically optimized online using the environment reward signal. One enhancement introduced is optimizing separate discount factors for each reward channel, enabling agents to learn to optimize sparse environment rewards far in the future with a high discount factor. This separation allows for myopic optimization of dense shaping rewards, making value-learning easier. Specific shaping rewards for soccer are detailed, and 2v2 soccer is simulated using the MuJoCo physics engine with 4 players having a 3-dimensional action space. In a simulated 2v2 soccer match, players can rotate and apply force to \"jump\" or \"kick\" the football. Proprioception and task features are observed in a 93-dimensional input vector. Matches last up to 45 seconds and end when a team scores. Contacts between players are disabled, but enabled between players, pitch, and ball to avoid fouls. The ball is reset by automatic \"throw in\" when kicked out-of-bounds. Players in a simulated 2v2 soccer match can rotate, jump, and kick the ball. The ball is reset by automatic \"throw in\" if it goes out-of-bounds. Agents are trained on a randomized field size and tested on a fixed size field. Population-based training with 32 agents is used, with an agent evolving if its win rate drops below 0.47. A low learning rate of 0.1 is used for Elo, due to high stochasticity in game results. In a simulated 2v2 soccer match, agents are trained on a randomized field size and tested on a fixed size field. Population-based training with 32 agents is used, with a \"burn-in\" period before evolution to maintain diversity. Evaluation of agent performance in novel domains remains an open question, with proposed solutions including Elo ratings and head-to-head tournaments for measuring performance. In multi-agent learning, agent performance is evaluated using Nash-Averaging, which ensures invariance to redundant agents. This method involves a meta-game with a pair-wise win-rate matrix between N agents, aiming for a non-exploitable strategy. This approach is used to meaningfully evaluate learned agents. The evaluation of learned agents in multi-agent learning involves Nash-Averaging to ensure invariance to redundant agents. Evaluation teams are chosen by Nash-averaging from a population of 10 teams produced by diverse training schemes. 1M tournament matches were collected between the set of 10 agents, showing diverse policies with non-transitive performance. Nash Averaging assigned nonzero weights to 3 teams, demonstrating the diversity in their policies. In a multi-agent learning evaluation, agents A, B, and C demonstrate diverse policies. Elo ratings show agent B as the best, but qualitative analysis reveals different outcomes. Experimental results show the progression from random to coordinated behaviors through population-based training. A tournament between trained agents is detailed in the appendix. In a multi-agent learning evaluation, agents A, B, and C demonstrate diverse policies. Elo ratings show agent B as the best, but qualitative analysis reveals different outcomes. Experimental results show the progression from random to coordinated behaviors through population-based training. A tournament between trained agents is detailed in the appendix. We incrementally introduce algorithmic components and evaluate them against 3 evaluation agents, comparing performance using expected goal difference weighted by Nash averaging. Annotated algorithmic components include ff, evo, rwd shp, lstm, lstm q, and channels with individually evolving discount factors. Population-based Training with Evolution is introduced first, showing Evolution kicking in at 2B steps in FIG2. Population-based training coupled with evolution shows a natural progression of learning rates, entropy costs, and discount factor. Critic learning rate decreases as training progresses, while the discount factor increases, focusing more on long-term return. Entropy costs decrease, indicating a shift from exploration to exploitation. Two dense shaping rewards are introduced in addition to sparse environment rewards. The introduction of dense shaping rewards in addition to sparse environment rewards facilitates learning early in training. Agents with dense shaping rewards quickly outperform those without, showing improved performance against the dummy evaluator. However, shaping rewards can lead to sub-optimal policies, which is mitigated by coupling training with hyper-parameter evolution. The population adjusts the importance of shaping rewards over time, initially prioritizing scoring over conceding, but shifting towards a more balanced approach by the end of training. The introduction of dense shaping rewards improves agent performance early in training, with a shift towards prioritizing scoring over conceding. Recurrence in the action-value function significantly impacts agent performance, with a recurrent policy underperforming compared to a feedforward policy. The importance of shaping rewards decreases over time, leading to a more balanced approach by the end of training. The discount factor increases over time during the evolution process, with different reward components requiring varying discount factors. Agents focus on long planning horizons for sparse environment rewards, while prioritizing short-term returns once basic movements are learned. The agent achieved faster learning compared to other methods and attained the highest Elo in a tournament. Future work may involve introducing diversity in the training regime. Assessing cooperative behavior in soccer is challenging. Indicators range from behavior statistics to qualitative gameplay to demonstrate agent cooperation. Videos on the website show agent value-functions and reward components. PBT learns to balance sparse and dense rewards for optimal performance. Future work may focus on introducing diversity in training. The videos on the website demonstrate agents learning cooperative behaviors in soccer, such as passing and kicking the ball upfield. Statistics collected during matches are shown, including the evolution of agents' behavior as training progresses. The teammate-spread-out evolution shows how teammates' positions on the pitch spread out over time, with a shift towards more useful behaviors. Pass/interception behaviors remain consistent, but there is a notable increase in long-range passes over 10m during training. During training, there is a shift towards more long-range passes over 10m, impacting the agent's policy. Counterfactual policy divergence is analyzed by replacing subsets of observations with alternatives to measure policy distribution changes. This is not applicable to recurrent policies, so a feedforward policy network is studied instead. Five types of counterfactual information are examined, showing the strong influence of ball position on the agent's behavior. The impact of ball position on the agent's policy distribution is more significant than player and opponent positions during training. Initially, ball position peaks quickly while counterfactual player/opponent positions plateau until 5B training steps. The agent optimizes shaping rewards greedily, with teammate/opponent positions affecting policies more from 5B steps onwards. Teammate position has a larger impact than individual opponent positions, suggesting players learn to coordinate with teammates before focusing on opponents. The progression observed in counterfactual policy divergence provides evidence for emergent cooperative behaviors among players. A probe task was designed to test agents for coordination, where blue0 possesses the ball and teammates are introduced on either side. Traces of agents' behaviors show successful passes and interceptions. The comparison between two snapshots (5B vs 80B) of the same agent shows a progression towards more coordinated behavior. Initially, blue0 tends to dribble individually regardless of blue1's position, but later on, it actively seeks to pass, showing a high level of coordination. This behavior includes two consecutive passes between blue0 and blue1, resembling 2-on-1 passes in human soccer games. The population-based training used here focuses on complex multi-agent interactions in continuous control in simulated physics. The RoboCup competition is a grand challenge in AI, with top-performing teams using elements of reinforcement learning but not end-to-end RL. The environment is a research platform for studying complex multi-agent interactions and can be easily extended for various learning scenarios. In the context of the RoboCup competition, research has focused on multi-agent learning, coordination, and cooperation in deep reinforcement learning. Previous studies have explored centralized approaches, but a new framework demonstrates complex coordinated behaviors with fully independent asynchronous learning. A new 2v2 soccer domain has been introduced for continuous multi-agent reinforcement learning research, showcasing coordinated behavior and competition between agents. In a new 2v2 soccer domain for continuous multi-agent reinforcement learning research, a framework of distributed population-based training with continuous control and automatic optimization of shaping reward channels was demonstrated. The idea of optimizing separate discount factors for shaping rewards was introduced to align with sparse long-horizon team rewards and cooperative behavior. A novel method of counterfactual policy divergence was used to analyze agent behavior, revealing non-transitivities in pairwise match results and the need for robustness in future work. The environment serves as a platform for multiagent research in continuous physical worlds, scalable to more agents and complex bodies for future research. In a soccer environment for multi-agent reinforcement learning, the reward is consistent across players, allowing for the simplification of the critic's learning process. Identifying other agents' actions is crucial as their true state and identities are not revealed during gameplay. Modeling the Q-function involves using an agent's history of observations, typically summarized in an LSTM's internal state, to implicitly consider other players. The Q-function in a neural network with weights \u03c8 conditions on other players' behavior and generalizes over player diversity using trajectory data from an experience replay buffer. Q is learned by minimizing the k-step return TD-error with off-policy retrace correction, using a separate target network for bootstrapping. The target network and policies are periodically synced with the online action-value critic and policy for stability. In experiments, the agent's internal memory state is stored in replay when modeling Q using an LSTM. The LSTM is optimized using backpropagation through time with unrolls truncated to length 40. Elo rating is used to measure an agent's performance and determine eligibility for evolution within the population of learning agents. Elo rating is utilized to measure agent performance and eligibility for evolution within the learning agent population. It predicts win rates against other agents, driving evolution of hyperparameters. Using Elo as the fitness function optimizes agents' internal hyperparameters for maximizing win rates. Neural networks are used to parametrize each agent's policy and critic, with observation preprocessing applied first. Observation preprocessing is applied to each player's data using a shared neural network to embed it into a consistent space. The network architecture is invariant to the order of features. Both the critic and actor networks have multiple layers with different activations and sizes. Weights are not shared between them. Gaussian policies are learned using SVG0 and the critic is trained with the Adam optimizer. The study utilized SVG0 and the critic with the Adam optimizer BID22 for gradient updates. A round robin tournament was conducted with 50,000 matches between agents from 5 populations, showing the benefits of shaping rewards, a recurrent critic, and separate reward and discount channels. The agent with full recurrence and separate reward channels achieved the highest Elo score. Performance against Nash evaluators was varied, indicating non-transitivities. The study conducted a round-robin tournament with 50,000 matches between agents from 5 populations, showing benefits of shaping rewards, a recurrent critic, and separate reward and discount channels. Performance against Nash evaluators was varied, indicating non-transitivities. Evaluators in Section 5.1 had mixed results, highlighting the need for robustness to opponents. A single experiment was replicated with 3 different seeds, showing consistent evolution of critic learning rate and entropy regularizer. Performance was found to be more sensitive to critic learning rate than actor learning rate. The study conducted a round-robin tournament with 50,000 matches between agents from 5 populations, showing benefits of shaping rewards, a recurrent critic, and separate reward and discount channels. Performance against Nash evaluators was varied, indicating non-transitivities. Evaluators in Section 5.1 had mixed results, highlighting the need for robustness to opponents. A single experiment was replicated with 3 different seeds, showing consistent evolution of critic learning rate and entropy regularizer. Performance was found to be more sensitive to critic learning rate than actor learning rate. Win rate matrix and hyperparameter evolution for different teams were analyzed, along with visualizations of agent behavior."
}