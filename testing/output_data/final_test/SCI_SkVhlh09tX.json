{
    "title": "SkVhlh09tX",
    "content": "Self-attention is a mechanism for generative models in language and images, determining context element importance by comparing each element to the current time step. A lightweight convolution can compete with self-attention, and dynamic convolutions are simpler and more efficient. Predicting separate convolution kernels based on the current time-step improves context element importance determination. Dynamic convolutions scale linearly in input length operations, outperforming self-attention in machine translation, language modeling, and summarization tasks. Dynamic convolutions achieve a new state of the art of 29.7 BLEU on the WMT'14 English-German test set. Self-attention models, RNNs, CNNs, and attention mechanisms have all made recent progress in sequence modeling. Self-attention operates over the current sequence, while source-target attention is used in machine translation. Self-attention is content-based, comparing the current time-step to all elements in the context to compute attention weights. This ability to compute comparisons over unrestricted context sizes is a key characteristic. In this paper, lightweight convolutions are introduced as an alternative to self-attention for modeling long-range dependencies in sequences. These convolutions are depth-wise separable, softmax-normalized, and share weights over the channel dimension, resulting in significantly fewer weights compared to standard convolutions. Unlike self-attention, lightweight convolutions reuse the same weights for context elements, regardless of the current time-step. Dynamic convolutions, which build on lightweight convolutions, predict a different convolution kernel at each time-step. Dynamic convolutions predict different convolution kernels at each time-step, similar to locally connected layers but with dynamically generated weights. This approach reduces complexity by performing attention within blocks of the input sequence and allows for more fine-grained attention over each feature. Lightweight convolutions perform competitively to strong self-attention in experiments. Dynamic convolutions outperform self-attention in various tasks, achieving a new state of the art in translation and faster runtime. They also excel in language modeling and document summarization, building on non-separable and depthwise separable convolutions. Sequence to sequence learning in machine translation involves an encoder network that computes representations for the source sequence and a decoder network that generates a target sequence based on the encoder output. The self-attention module in BID58 applies projections to the input to obtain key, query, and value representations, with multiple heads learning separate attention weights. Depthwise convolutions perform convolutions independently over every channel. LightConv is a depthwise convolution that shares output channels and normalizes weights across the temporal dimension using softmax. It has a fixed context window and determines context element importance with fixed weights. Models with LightConv show better generalization and can compete with self-attention models. LightConv is a depthwise convolution that shares output channels and normalizes weights across the temporal dimension using softmax. It reduces the number of parameters significantly, making dynamic convolutions feasible on current hardware. LightConv is a module that integrates BID60, tying weights across the temporal dimension using softmax. It includes an input projection, GLU, lightweight convolution, and output projection. DropConnect is used as a regularizer, removing some temporal information within a channel. Implementation challenges were addressed by finding a faster solution than existing CUDA primitives for convolutions. LightConv did not perform well, so a faster solution was found for short sequences. The solution involves copying and expanding normalized weights to a band matrix, reshaping and transposing inputs, and performing batch matrix multiplication. A dynamic convolution with time-step dependent kernels is proposed, building on LightConv to reduce parameters. DynamicConv uses a linear module to compute time-step dependent kernels. DynamicConv is a linear module with learned weights that changes the weights assigned to context elements over time. Unlike self-attention, DynamicConv's weights are a function of the current time-step only, scaling linearly in sequence length. Experiments show that models using DynamicConv match or exceed the performance of state-of-the-art models with context-based self-attention, challenging typical intuitions in natural language processing. The architecture used is an encoder-decoder model for sequence to sequence learning, closely following specific choices presented in previous work. The self-attention baseline in BID58 uses Transformer Big architecture with encoder and decoder networks containing N blocks each. Encoder blocks have self-attention, LightConv, or DynamicConv modules followed by feed-forward modules. Decoder blocks are similar but include source-target attention. Words are embedded in d dimensions with sinusoidal position embeddings for absolute word position encoding. The model uses position embeddings for word position encoding and computes a distribution over vocabulary V. LightConv and DynamicConv are variations of Transformer Big with different convolution modules. The models have fewer parameters per block and use different kernel sizes for encoder and decoder. Evaluation is done on machine translation, language modeling, and other tasks to understand their limitations. The study evaluates LightConv and DynamicConv on machine translation tasks for various language pairs using different training data and vocabulary sizes. For machine translation tasks on various language pairs, LightConv and DynamicConv are evaluated using different training data and vocabulary sizes. The setup includes 160K training sentence pairs and a 10K joint BPE vocabulary. Data is lowercased for this benchmark. Different evaluation metrics are used for different language pairs, such as case-sensitive tokenized BLEU for WMT En-De and WMT En-Fr, and detokenized BLEU for WMT Zh-En. Ablations are performed on the validation set, and test accuracy is reported based on the seed with the highest validation BLEU. Beam search with varying beam widths is used for different datasets, and length penalty and number of checkpoints are tuned accordingly. Language modeling evaluation is done on the Billion word dataset with nearly 800K types in the vocabulary. In dataset BID4, there are 768M tokens with a vocabulary of nearly 800K types. Models are evaluated based on perplexity on valid and test portions. The model's ability to process long documents is tested on the CNN-DailyMail summarization task BID23 BID37. Evaluation is done in terms of F1-Rouge. For translation tasks, different dropout rates are used for various language pairs, and models are optimized with Adam and a cosine learning rate schedule. For WMT models, optimization is done with Adam and a cosine learning rate schedule. Training steps vary for different tasks, with 30K steps for WMT En-De, 40K steps for WMT Zh-En, and 80K steps for WMT En-Fr. For IWSLT De-En, training is done for 50K steps on a single GPU. Floating point 16 precision is used, with gradient accumulation for 16 batches. Batches contain varying numbers of tokens based on the task. Label smoothing with 0.1 weight is applied for the uniform prior distribution over the vocabulary. Language modeling setup follows a similar approach. For the Billion word benchmark, a language model is trained without the encoder module using an adaptive softmax output layer to reduce computational burden. Training is done on 32 GPUs with batches of 65K tokens for 975K updates using Nesterov's accelerated gradient method with a momentum value of 0.99. The learning rate is warmed up linearly from 10^-7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with one cycle. Summarization is trained with Adam using a cosine learning rate schedule with weight decay 1e-3 and dropout 0.3. Results are reported on WMT En-De and WMT En-Fr, comparing to self-attention based models. LightConv and DynamicConv models perform competitively on large translation benchmarks, with DynamicConv even outperforming the state of the art on WMT En-De. Content-based self-attention is not necessary for achieving good accuracy. On a smaller benchmark like IWSLT, a smaller architecture is used with self-attention as the baseline. In this section, the impact of choices made for LightConv and DynamicConv models is evaluated. Limiting the maximum context size of self-attention has no effect on validation accuracy. Self-attention blocks are replaced with non-separable convolutions and depthwise separable convolutions, resulting in improved results. Switching to depthwise separable convolution with input and output projections of size d = 1024 improves accuracy. Increasing kernel width from lower to higher layers further enhances accuracy, narrowing the gap to self-attention to only 0.5 BLEU. DropConnect shows a slight performance improvement, while weight sharing does not decrease performance. Softmax normalization to the weights is only 0.3 BLEU below the baseline accuracy. Dynamic convolutions achieve the same validation accuracy as self-attention with fewer parameters and higher inference speed. Softmax normalization is crucial for DynamicConv models. GLU is not introduced after the input projection for comparability. Our re-implementation of averaged attention networks (AAN) is efficient, outperforming the approach of using self-attention in the encoder network. In language modeling on the Billion word benchmark, DynamicConv achieves slightly better perplexity than the self-attention baseline. Dynamic convolutions achieve the same validation accuracy as self-attention with fewer parameters and higher inference speed. DynamicConv and LightConv outperform self-attention baseline in language tasks. Lightweight convolutions show competitive results with a small parameter footprint. Dynamic convolutions predict different kernels at each time-step, similar to self-attention weights. DynamicConv and LightConv, which are lightweight convolutions with dynamic weights computed by self-attention, outperform a strong self-attention baseline on various translation and summarization tasks. They are 20% faster at runtime than self-attention and achieve comparable results on language modeling. The future of dynamic convolutions looks promising for tasks like question answering and computer vision. In comparing DynamicConv to non-autoregressive models, different alternatives to softmax-normalization were tested on WMT English-German newstest2013. Results showed that softmax-normalization performed the best, stabilizing the training procedure. Generation speed for DynamicConv was measured to be comparable with other models in the literature. DynamicConv with a single decoder layer outperforms all previously reported non-autoregressive results in terms of speed and accuracy. BID20 and BID32 achieve a speedup over DynamicConv with a small drop in BLEU by distilling autoregressive models into non-autoregressive models. The effects of different GPU types are likely negligible with batch size one. Inference speed comparison of non-autoregressive models and small decoder versions of DynamicConv on WMT English-German newstest2014, with decoding speed derived from sentence generation latency in the literature."
}