{
    "title": "B1ydPgTpW",
    "content": "In Chinese societies, superstition plays a significant role, and vehicle license plates with desirable numbers can fetch high prices in auctions without pre-allocated estimates. In Chinese societies, superstition influences the prices of vehicle license plates. A deep recurrent neural network is used to predict plate prices in Hong Kong based on characters. The model outperforms previous ones, explaining over 80% of price variations. The model can also function as a search engine for plates and estimate price distributions. The Beijing Olympic opening ceremony and the Bank of China (Hong Kong) opening date are examples of significant events occurring on auspicious dates. License plates, a public display of numbers, can fetch high prices at auctions, with governments using them to generate revenue. Predicting a license plate's price based on its characters is a challenging task due to the large number of combinations and plates per auction. Predicting license plate prices based on characters is like a natural language processing task. In Chinese societies, numbers hold superstitious value based on rhyming characters. For example, \"168\" means \"all the way to prosperity\" and plates with this number can fetch high prices at auctions. Conversely, numbers with negative connotations are undesirable. License plates with numbers that rhyme with negative connotations are undesirable. Adding a 5 in front of the highly sought after number 888 drastically lowers its value. Using a deep recurrent neural network, a good estimate of a license plate's price can be obtained, explaining over 80 percent of price variations. The predictions from this study's deep RNN were significantly more accurate than previous attempts. The findings in this paper demonstrate the value of deep networks and NLP in accurate price predictions for license plates. The model can be used for arbitrage to detect underpriced plates and construct a search engine for historical plate prices. This study highlights the superiority of deep networks over shallow ones in price prediction research. License plates in Hong Kong have been sold through government auctions since 1973, with restrictions on reselling. Between 1997 and 2009, an average of 3,812 plates were auctioned per year. Traditional plates consist of a two-letter prefix or no prefix, followed by up to four digits. They can be special plates or ordinary plates issued by the government for new vehicles. License plates in Hong Kong have been sold through government auctions since 1973. Vehicle owners can return assigned plates and bid for new ones in auctions. Only ordinary plates can be resold. Personalized plates are also available through auctions. Auctions are open to the public twice a month, with payment settled on the spot. The number of plates auctioned ranges from 90 to 280 per day. Limited literature exists on modeling the price of license plates using hedonic regressions. Limited literature exists on modeling the price of license plates using hedonic regressions with handcrafted features. In contrast, a deep RNN in this study learns the value of character combinations from auction prices without handcrafted features. Neural networks are extensively used for price predictions in various areas such as stock prices, commodity prices, real estate prices, and more, typically using small, shallow networks. The study focuses on predicting prices of license plates using deep neural networks with up to 1,024 hidden units per layer and 9 layers. It differs from sentiment analysis in predicting actual price levels rather than just price movement direction. The approach is feasible due to the strong causal relationship between sentiment and price for license plates. The study utilizes a Long-Short-Term Memory (LSTM) network to analyze price movements of Japanese stocks. Deep RNNs are used for predicting license plate prices based on textual information, as they excel in tasks involving sequential data. The model predicts a single value based on up to six characters, making training feasible on a small volume of auction data. The study uses plate auction data to train a deep RNN model with character embeddings. The neural network includes bidirectional recurrent layers and fully connected layers for improved context understanding. Batch normalization is also utilized for faster training. The deep RNN model includes bidirectional recurrent layers and fully connected layers for context understanding. Batch normalization is used for faster training, with hyperparameters such as character embedding dimension and dropout rate needing to be selected before training. The data used for training the deep RNN model are the Hong Kong license plate auction results from January 1997 to July 2010, containing 52,926 auction entries. Prices in the data are highly skewed, with a median sale price of $641 and a mean sale price of $2,073. Log prices were used to address the skewness, with ordinary plates starting at a reserve price of HK$1,000 and special plates at $5,000. 5.1 percent of the plates in the data were unsold. The data used for training the deep RNN model are the Hong Kong license plate auction results from January 1997 to July 2010, containing 52,926 auction entries. 5.1 percent of the plates were unsold and were dropped from the dataset, leaving 50,698 entries available for the experiment. The data were divided into three parts, randomly and sequentially, with the sequential division representing a more realistic scenario for practical deployment. This scenario is more challenging as plates released alphabetically through time may not be available in sequentially split data. The deep RNN model was trained on Hong Kong license plate auction data from January 1997 to July 2010. Training was done with 64% of the data, validation with 16%, and the rest for testing. A grid search was conducted with various hyperparameters like character embeddings dimension, recurrent layers, fully connected layers, hidden units, and dropout rate. 1080 sets of hyperparameters were investigated in three passes. Training of the deep RNN model on Hong Kong license plate auction data involved multiple passes with different hyperparameters. The training process included multiple repetitions for each set of hyperparameters, with a focus on minimizing validation RMSE. The training duration in the second and third passes was 120 epochs, utilizing an Adam optimizer with a learning rate of 0.001. The training was conducted on four NVIDIA GTX 1080s with a large mini-batch size of 2,048 to fully utilize the GPUs. The training of the deep RNN model on Hong Kong license plate auction data involved multiple passes with different hyperparameters. The time on a single GPU ranged from 8 seconds to 7 minutes 50 seconds for various network configurations. Additionally, models from previous studies and fully connected networks were trained for comparison. The model could explain over 80 percent of the price variation when the data was randomly split. The deep RNN model outperformed other models in explaining price variation, with over 80 percent accuracy. Comparatively, BID32 and BID24 models could only explain up to 70 percent. The fully connected network (MLP) performed poorly, explaining less than 66 percent. N-gram models based on median prices were significantly inferior, explaining only 40 percent of the variation. For n > 2, models with unlimited features perform poorly due to generating rare features. Restricting features based on occurrences and allowing a range of n improves performance but never surpasses simple unigram. Median and mean price models show similar performance. Sequential data splitting significantly impacts model performance, especially on the test set. The best model maintains good performance across a wide range of prices. The model performed well for a wide range of prices but consistently underestimated the price of the most expensive plates. Neural networks are susceptible to fluctuations due to convergence to local maxima, which can be smoothed out by combining predictions from multiple runs. Retraining the model over time is important to prevent obsolescence due to changing tastes or economic environments. The study focused on retraining a RNN model with sequentially-split data to improve prediction accuracy. Results showed that yearly retraining significantly outperformed no retraining, with a 8.6% lower RMSE and a 6.9% higher R2. Monthly retraining had smaller benefits compared to yearly retraining, with only a 3.3% reduction in RMSE and a 2.6% increase in explanatory power. Combining multiple models was also found to enhance prediction accuracy. The section discusses combining different models to improve prediction accuracy, including a neural network and regression model. It considers various features such as auction details, plate characteristics, and economic indicators. The ensemble model's performance with different retraining frequencies showed minimal differences in RMSE and R2, with monthly retraining slightly outperforming no retraining. The ensemble model outperformed the RNN alone, showing improved accuracy and stability with monthly retraining. The RNN struggled with extreme prices, while handcrafted features in the ensemble model helped predict outliers. Periodical retraining showed benefits up to a certain threshold, but understanding RNN predictions remains challenging due to the large number of parameters involved. The RNN model's prediction rationale involves extracting a feature vector from the last recurrent layer to explain predictions. This vector is used in a k-nearest-neighbor model to provide a rationale. The procedure is demonstrated using the best RNN model to generate feature vectors for training samples, which are then used in the k-NN model to provide examples as rationale for predictions. The k-NN model was used to predict prices of plates, with examples showing how it focused on numeric and alphabetical parts for different value plates. It also highlighted the challenge of predicting prices for high-value plates due to variability in historical prices. Unlike RNN models that provide a single price estimate, auctions typically give both high and low estimates. The section introduces the use of a Mixture Density Network (MDN) to estimate the distribution of plate prices, addressing the limitations of the k-NN model for rare, high-value plates. The network output vector is generated by a neural network with a single fully-connected layer, trained using the Adam optimizer for 5000 epochs. The estimated density of prices closely resembles the distribution of common plates. The study demonstrates that a deep recurrent neural network can accurately estimate license plate prices, outperforming other models. The model can learn prices from raw plate characters, making it feasible to constantly retrain for accuracy. Future research could focus on constructing a model for personalized plates with more complex meanings. Future research could focus on designing separate models for different types of license plates or pre-training on another text corpus to enhance model performance. Special thanks to Travis Ng for providing the license plate data used in this study."
}