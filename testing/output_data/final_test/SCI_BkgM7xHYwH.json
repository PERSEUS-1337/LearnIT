{
    "title": "BkgM7xHYwH",
    "content": "Orthogonal recurrent neural networks solve the vanishing gradient problem by using an orthogonal matrix for recurrent connections. Linear Memory Networks, with a feedforward layer and linear recurrence, offer an alternative solution for memorizing long sequences. An initialization schema approximates a linear autoencoder for input sequences, making it adaptable to any recurrent architecture. Our approach, utilizing an autoencoder for initialization, outperforms random orthogonal initialization in memorizing long sequences. Empirical analysis demonstrates competitive results on sequential tasks like MNIST and TIMIT. Orthogonal RNNs address the vanishing gradient issue by enforcing orthogonal connections. This is crucial for tasks requiring the memorization of extended patterns, such as music generation. In this paper, the problem of memorization with orthogonal RNNs and linear autoencoders is addressed. The objective is to find a solution for memorizing long sequences efficiently. By training an autoencoder with a small number of hidden units on low-dimensional manifold input sequences, the entire sequence can be encoded. The optimal autoencoder in the linear case can be computed with a closed-form solution, which can be used to initialize recurrent architectures. In experiments, the Linear Memory Network (LMN) with autoencoder initialization is shown to be equivalent to Elman RNN, addressing the vanishing gradient problem and improving performance on real-world datasets. The model also performs well on tasks where strict orthogonal parameterization struggles, such as the TIMIT benchmark. Our approach focuses on the forward step, investigating the effectiveness of an autoencoder-based solution for memorization. The model requires memorization of the entire sequence within hidden state activations, which may be inefficient for tasks not needing complete memorization. Allowing the model to diverge from orthogonality is crucial in such cases. The paper proposes a novel initialization schema for explicit memorization of long sequences, highlighting the connection between orthogonal models and linear autoencoders. An empirical analysis demonstrates the effectiveness of this schema for tasks requiring long memorization. The Linear Memory Network (LMN) is a recurrent neural network that computes hidden and memory states. The Linear Memory Network (LMN) is a recurrent neural network that computes a hidden state h t and a separate memory state m t using linear transformations and non-linear activation functions. The model update equations involve parameters W xh, W mh, W hm, W mm, and a non-linear activation function. The linearity of the memory update can be exploited to ensure constant gradient propagation by constraining W mm to be an orthogonal matrix, thus avoiding the vanishing gradient problem. The vanishing gradient problem can be addressed by truncating the gradient between memory states to ensure constant gradient propagation. This approach is similar to truncated backpropagation in LSTM models. The modification to the training algorithm is easy to implement with automatic differentiation packages. The linearity of the memory update in LMN can be seen as a limitation, but it can always be rewritten as an equivalent RNN. The linearity of the LMN memory update can be rewritten as an equivalent RNN. Differences between the two architectures become important during training, with the LMN's linear update being prone to the exploding gradient issue. Regularization is necessary for stability in LMN training, with control over gradient propagation achieved through tuning the spectral radius of the recurrent matrix. Soft orthogonality constraints can be applied to address this. In experiments, soft orthogonality constraints are used to address instability in LMN training. Regularization terms are added to control memory state growth and hidden activations. Linear autoencoder for sequences is utilized for LMN and RNN model initialization. The linear autoencoder for sequences (LAES) is a model used for encoding input sequences into a minimal hidden state. It consists of an encoder, decoder, and internal state equations. Sperduti (2013) provides a closed-form solution for the optimal linear autoencoder, allowing for the reconstruction of the decoder matrix. This approach enables the construction of a simple linear recurrent model that encodes input sequences efficiently. The linear recurrent model uses an autoencoder to encode input sequences into a single vector, the memory state. A linear layer is trained to predict the target using the autoencoder states. The model approximates a linear RNN, with the quality depending on the hidden activations. The tanh activation function plays a key role in the approximation. The tanh activation function in the initialized RNN degrades quickly, affecting the correspondence with the linear autoencoder. To address this, a similar initialization scheme is proposed for the LMN model, approximating the linear RNN. This initialization focuses on the input transformation rather than the recurrent part, resulting in a closer approximation to the original linear RNN. The experimental results in Section 5 compare the accuracy of the initialized LMN and linear RNN architectures. The autoencoder-based memorization mechanism enables operations like reconstructing sequences and processing them with a feedforward layer. The optimal autoencoder with parameters A and B can reconstruct sequences with zero error, and the output can be computed using a linear layer. The autoencoder-based memorization mechanism allows for reconstructing sequences and processing them with a feedforward layer. The output layer can reconstruct the entire input sequence using the decoder, with parameters that grow linearly with the sequence length. This model combines operations within a single matrix multiplication, making training more efficient. The autoencoder-based model can reconstruct sequences and process them efficiently with a feedforward layer. The unrolled model demonstrates the expressiveness of the autoencoder, while an orthogonal parameterization may lead to inefficient memorization. The performance of the orthogonal LMN and a proposed initialization scheme is evaluated on synthetic tasks and real-world datasets. The performance of the orthogonal LMN and a proposed initialization scheme is evaluated on synthetic tasks and real-world datasets, including the copy task, digit classification with sequential and permuted MNIST, and framewise classification with TIMIT. While orthogonal models excel in permuted MNIST, they may reduce performance on TIMIT. LAES initialization for RNN and LMN is compared, with LMN showing better practical performance. Models are trained with Adam and a soft orthogonality constraint is added. The copy task involves memorizing a sequence of elements and repeating them after a certain number of timesteps. A soft orthogonality constraint is added to the cost function, and different values of \u03bb are tested. The network architecture consists of a single layer with 100 hidden units. The copy task involves memorizing a sequence of elements and repeating them after a certain number of timesteps. The LMN model can solve the task using a saturating nonlinearity, combining nonlinear activations with the memorization properties of orthogonal linear models. Results on the copy task for different parameters are shown in Table 1. The LMN architecture with a saturating nonlinearity can solve tasks requiring long-term dependencies, beating the baseline in both settings. Results from the copy task for different parameters are shown in Table 1, confirming the LMN's ability to combine nonlinear activations with memorization properties. The LSTM also beats the baseline but does not solve the task optimally. The RNN with a tanh nonlinearity fails to outperform the baseline even with T = 100. The study uses MNIST datasets to compare LAES initialization to random orthogonal initialization. The LMN architecture with a saturating nonlinearity outperforms the baseline in tasks requiring long-term dependencies. Results on the MNIST datasets show that the AntisymmetricRNN achieves the best results, while the LMN with LAES initialization performs slightly lower. The LAES initialization improves results compared to other orthogonal models in framewise classification tasks. TIMIT, a speech recognition corpus, is more challenging for orthogonal models than non-orthogonal models. The LAES can efficiently compress sequences and achieve high performance on MNIST using linear models. The LMN and LAES initialization were compared in a challenging setup with models trained for 50 epochs. Results showed that penalizing the memory state norm solved performance degradation issues. The best model on the benchmark was LMN, with a penalty parameter selected through cross-validation. The LMN outperformed using LAES initialization on a benchmark, with the best configuration using orthogonal initialization without soft orthogonality constraints. This confirms the model's ability to handle problems without explicit memorization. LAES initialization can lead to errors due to nonlinearity, as shown in training a linear autoencoder. The RNN initialization error was significant, highlighting the importance of proper initialization techniques. The results indicate that RNN initialization errors can significantly impact accuracy, with the initialized model dropping to 11.7 from 85.5 of the linear RNN. However, the initialized LMN achieves the same accuracy as the linear RNN. Orthogonal RNNs address the vanishing gradient problem by using orthogonal or unitary matrices for recurrent connections. Various approaches ensure orthogonality, such as specific parameterization, soft or hard constraints, and factorization of the matrix. In previous studies, the impact of RNN initialization errors on accuracy was highlighted. Orthogonal RNNs use orthogonal or unitary matrices for recurrent connections to address the vanishing gradient problem. Various methods ensure orthogonality, such as specific parameterization and constraints. Wisdom et al. (2016) and Lezcano-Casado & Mart\u00ednez-Rubio (2019) have shown that hard orthogonality constraints can hinder training speed and final performance. Linear autoencoders for sequences can be optimally trained with a closed-form solution (Sperduti, 2013) and have been used to pretrain RNNs (Pasa & Sperduti, 2014). The LMN (Bacciu et al., 2019) is a recurrent neural network with a separate linear connection. Untrained RNNs with random weight initialization have a Markovian bias, leading to input clustering in the hidden state space (Tino et al., 2004). Short-term memory properties of linear and orthogonal memories have been studied by Ti\u0148o & Rodan (2013) and White et al. (2004). In this study, the focus was on building an autoencoder-based memorization mechanism that encodes and decodes input sequences to compute the desired target. By approximating a linear autoencoder of the input sequences, a recurrent neural network can be initialized. The architecture utilizes a linear recurrence to improve the autoencoder approximation, showing effectiveness in learning memorization tasks. Future research may explore the impact of the autoencoder during training and optimize linear component parameters for faster learning. The linear component parameters can be optimized for faster training by exploiting linearity in dedicated learning algorithms."
}