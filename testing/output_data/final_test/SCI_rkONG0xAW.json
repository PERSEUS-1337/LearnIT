{
    "title": "rkONG0xAW",
    "content": "The paper introduces Recursive Binary Neural Networks for efficient learning on embedded and mobile devices with limited on-chip storage. By recycling weight data during training, the model allows for training larger neural networks on constrained devices, improving classification accuracy while reducing off-chip storage accesses. Tested on MNIST and voice activity detection benchmarks, the model achieves a data storage requirement as low as 2 bits/weight for deep neural networks, outperforming conventional binary neural network models. Our model allows for training larger neural networks on constrained devices with the same data storage as conventional models, achieving lower test error rates. It also requires significantly less data storage for weights in comparison, resulting in improved accuracy and storage savings for convolutional neural network classifiers. Deep Neural Networks (DNN) have shown superior performance in cognitive tasks like computer vision and speech recognition. Implementing DNNs in mobile and embedded devices, known as Internet of Smart Things, is gaining traction. While most current works focus on implementing the inference function of DNNs on these devices, on-device learning is becoming more crucial for custom-built models tailored to specific devices, users, and environments. On-device learning faces challenges in algorithms, data, and systems, with high energy consumption being a significant issue due to dense computation and data access. This is crucial as the number of deployed devices increases in the era of Internet of Things (IoT), leading to a rise in learning tasks that can be computationally challenging even for powerful cloud computers. Additionally, training data from mobile and embedded devices may contain sensitive information, making users hesitant to upload data onto cloud servers. High energy consumption in embedded devices is caused by dense computation and data access, with fetching DNN weights from external DRAM leading to high overhead. Limited on-chip data storage necessitates storing DNN parameters externally, making training prohibitive on devices like ARM Cortex M3 processor. Accessing off-chip DRAM incurs significantly more energy and delay overhead compared to on-chip SRAM, making training challenging on mobile and embedded devices. Several techniques have been proposed to compress DNN parameters for implementation on mobile and embedded devices, reducing computation overhead. However, existing methods focus on weight size compression after training, while data storage requirements during training remain the same. Binary Neural Networks (BNN) models use sign bits of weights to reduce computational complexity, but each weight still needs to be represented in high precision. The Recursive Binary Neural Network (RBNN) model aims to efficiently utilize on-chip data storage during training by recycling non-sign-bit portions of weights to increase the neural network size for accuracy improvement. This process is repeated recursively until accuracy plateaus or all on-chip storage is used. The RBNN model was tested on a Multi-Layer Perceptron (MLP) and a convolutional neural network. The proposed RBNN model demonstrates improved performance on MLP and CNN classifiers for MNIST and VAD benchmarks, achieving up to 9\u00d7 savings in data storage requirement and 2.4% less test error. It outperforms conventional BNN models in terms of storage efficiency, with a reduction of data storage requirement by up to 6\u00d7 for MNIST and 9\u00d7 for VAD. The proposed RBNN model achieves 9\u00d7 savings in data storage requirement for the VAD benchmark. The paper discusses related works on distillation, compression, BNNs, and low-precision weights in different sections. During training, weights are stored in high-precision numbers, leading to a need for multi-bit data storage. Techniques like weight pruning, quantization, sharing, and compression coding can reduce the data size of weights. Binary information of weights, activations, and inputs can be used to reduce computational complexity, especially in Multiply-and-Accumulate (MAC) operations. During training, weights are stored in high-precision numbers, requiring multi-bit data storage. Binary Neural Network (BNN) techniques can reduce computational complexity in Multiply-and-Accumulate (MAC) operations. However, BNN techniques struggle to scale the storage requirement of weights. Lowering the precision of weights through quantization has shown to have a tolerable impact on training and post-training operations of Deep Neural Networks (DNN). Studies have demonstrated success with 16-bit fixed-point weights and dynamic fixed-point representation to reduce precision requirements. The proposed Recursive Binary Neural Network (RBNN) model aims to reduce precision requirements by using fixed-point representation for weights. Unlike conventional BNN techniques, RBNN only uses sign bits of weights during inference, leading to significant savings in weight storage. The Recursive Binary Neural Network (RBNN) model reduces weight precision requirements by binarizing weights and recycling storage for additional trainable weights. This iterative process enlarges neural networks while maintaining data storage efficiency. Figure 1 illustrates the RBNN learning model with a multilayer neural network classifier. The neural network classifier starts with one input, two sets of two hidden, and one output neurons, each with eight weights of n bits. The network is trained using back-propagation, then weights are binarized by keeping only the sign bit. An incremental BNN is created by adding eight additional weights to the trained BNN, expanding the network size to 1 \u00d7 4 \u00d7 4 \u00d7 1. The enlarged BNN consists of one trained BNN with eight weights and one incremental BNN with eight weights under training. The incremental BNN is updated while trained together with the BNN. In each iteration, eight more weights are integrated, and the bit-width of newly-added plastic weights is reduced by one. The network grows to 1 \u00d7 2k \u00d7 2k \u00d7 1 with 8k binary weights after k iterations. The proposed RBNN model involves training a BNN with binary weights, then reducing the weight bit-width by one and training an incremental BNN. This process allows for better classification accuracy with the same weight storage or reduced weight storage for the same accuracy level. The network grows to 1 \u00d7 2k \u00d7 2k \u00d7 1 with 8k binary weights after k iterations. The proposed RBNN model involves training a BNN with binary weights and then reducing the weight bit-width by one to train an incremental BNN. This incremental BNN training method aims to improve the performance of the enlarged BNN by updating only the plastic weights in the incremental BNN during back-propagation and parameter-update. This method allows for better classification accuracy with the same or reduced weight storage. The implementation of sub-word operations for synaptic weights in Incremental BNN Training simplifies weight multiplication as shifts. The experiment setup and results for MLP-like classifier and MNIST benchmark are described. The RBNN model is also applied to CNN classifiers and VAD benchmarks. The performance of the RBNN model on MLP-like classifier is tested using permutation-invariant MNIST dataset. Training and testing data are normalized to [-1, 1]. The training and testing data for the MLP-like classifier and MNIST benchmark are normalized to [-1, 1]. The last 10,000 images of the training set are used as a validation set. Data augmentation, pre-processing, and unsupervised pre-training were not considered. Only weights of incremental BNN are updated. The neural network has one or two hidden layers with varying numbers of neurons (200 to 800). The input and output units are 784 and 10. Activation functions used are tanh for the hidden layer and softmax or linear for the output layer. Stochastic Gradient Descent (SGD) is used for optimization. Storage constraints are considered for embedded systems. The study utilized Stochastic Gradient Descent (SGD) for optimization without advanced techniques like dropout or ADAM. Fixed-point arithmetic was used for all computations and data access, with translations to narrow fixed-point and binary numbers done through simple decimation. The study used fixed-point arithmetic for computations and data access, optimizing the dynamic range for better accuracy. The network size increased through recursive iterations, reducing storage requirements and improving error rates. The study optimized dynamic range for accuracy using fixed-point arithmetic. Through recursive iterations, the neural network size became 784\u00d7800\u00d7800\u00d710 with 2 bits/weight for a test error of 2.17%. The proposed RBNN model allows for enlarging the network using recycled data storage. The tiled approach was chosen for easier hardware implementation, showing similar test error to fully-connected networks. The proposed RBNN model achieved 1% less test error than the conventional BNN model with similar data storage. It requires 3-4\u00d7 less data storage to achieve similar test error. TAB2 compares six neural networks, three trained by RBNN and three by BNN. The proposed RBNN model (R1, R2, R3) requires twice as many add and shift operations for training compared to the conventional BNN model (B1, B2, B3). However, both RBNN and BNN have the same amount of multiply operations for training. For inference, RBNN and BNN have the same amount of shift and add operations, with no multiplication needed. Energy dissipation for training is compared in TAB2, with Etotal = Earith + Eacc. The energy dissipation of arithmetic operations and data storage access for weights in the RBNN model is calculated based on the number of shifts, adds, multiplications, and accesses to SRAM and DRAM. The energy costs are normalized to the bit-widths of operations, with DRAM access consuming 173 times more energy. The RBNN model reduces energy consumption by utilizing only SRAM for weight access during training, resulting in \u223c100\u00d7 less energy dissipation compared to conventional BNN models. This new learning model efficiently uses limited on-chip data storage resources by recycling wasted storage to add and train more weights to a neural network classifier. The proposed model was verified with MLP-like DNN and CNN classifiers on the MNIST and VAD benchmark under typical embedded device storage constraints. The proposed RBNN model achieves significant storage savings and reduced computation complexity compared to conventional models. It uses both non-plastic and plastic weights, updating only the plastic weights to improve efficiency. Future work includes exploring binarization of activation functions and applying the model to ensembles of neural networks and mixtures of experts. The proposed RBNN model implements sub-word operations using mask and bitwise logical operations for weight training. It utilizes already-trained weights and a mask word to update weights efficiently. This approach reduces storage requirements and computation complexity compared to conventional models. The proposed RBNN model implements sub-word operations using mask and bitwise logical operations for weight training, requiring only two bitwise-AND and one bitwise-XOR operations. It consists of two convolution layers with 5-by-5 feature maps, followed by average pooling. The LeNet5 model includes a fully-connected classifier with binary weights for forward and backward propagation. Multiple CNN classifiers were trained on the MNIST benchmark dataset with varying configurations of the FC classifier, ranging from 200 to 800 neurons. The proposed RBNN model utilizes sub-word operations for weight training with minimal operations. It includes two convolution layers with 5-by-5 feature maps and a fully-connected classifier with binary weights. The model achieves 2.4% less test error than conventional BNN models while requiring significantly less weight data storage. The RBNN model achieves 6X data storage savings and 2.4% less test error compared to conventional BNN models. It is applied to train MLP-like DNN classifiers for the VAD benchmark, which includes various noise scenarios. The experiment confirms the RBNN's ability to save up to 9X data storage while maintaining similar detection accuracy levels. The RBNN model achieves significant data storage savings and lower test error compared to conventional BNN models when training DNN classifiers. The experiment demonstrates the RBNN's ability to save up to 9X data storage while maintaining similar detection accuracy levels. The proposed RBNN is used to train a fully-connected DNN classifier by recycling data storage from binarization in each recursive iteration. The experiment involves expanding a DNN in a tiled manner and adding weights connecting the tiles. The DNN becomes 1/4-connected in the fourth iteration, with 7-bit weights. Subsequent iterations connect hidden layers of different tiles similarly. The fully-connected DNN classifier shows similar accuracy to the tiled structure."
}