{
    "title": "r1GkMhAqYm",
    "content": "In this work, a goal-driven collaborative task called CoDraw is proposed, involving language, vision, and action in a virtual environment. The game consists of two players, a Teller and a Drawer, communicating through natural language to reconstruct scenes using clip art objects. The CoDraw dataset includes ~10K dialogs and ~138K messages exchanged between human agents, with protocols and metrics defined to evaluate agent effectiveness. In this study, a novel \"crosstalk\" condition is introduced for evaluating agents trained on different subsets of data. Various models are presented, including neural networks trained with imitation learning and goal-driven training. The goal is to build agents that can interact with humans using natural language, perceive their environment, and take actions. Incorporating both vision and action is crucial for grounding language understanding. In this paper, the Collaborative Drawing (CoDraw) task is proposed as a unified testbed for training and evaluating language generation capabilities in an interactive setting. The task combines grounded language understanding with effective goal-driven communication, focusing on successfully conveying information to achieve goals through perception, communication, and actions. The Collaborative Drawing (CoDraw) task combines perception, communication, and actions in a virtual environment where two players, Teller and Drawer, communicate to reconstruct an image without seeing it. Effective communication and rich grounded language are required for the Teller to describe the scene and the Drawer to carry out actions to recreate it. The CoDraw task involves two players, Teller and Drawer, communicating to reconstruct an image without seeing it. The performance is based on the quality of reconstructed scenes, indicating successful communication. Separate training of the agents on disjoint data subsets prevents them from developing a shared \"codebook\" that deviates from natural language, known as crosstalk. The CoDraw task involves two players, Teller and Drawer, communicating to reconstruct an image without seeing it. A novel CoDraw dataset is collected for evaluating natural language communication effectiveness. A cross-talk training protocol prevents agents from learning joint uninterpretable codebooks, ensuring effective communication with humans. The CoDraw task involves two players, Teller and Drawer, communicating to reconstruct an image without seeing it. Long-term planning and context reasoning are key challenges in the conversation. CoDraw involves multiple rounds of interactions between agents, requiring them to build a mental model of each other to collaborate effectively. CoDraw requires agents to manipulate clip art pieces to create a meaningful scene, requiring cooperation and adaptation. The task has a clear communication goal for objective success measurement and enables end-to-end goal-driven learning. Traditional goal-driven agents use 'slot filling' while end-to-end neural models are also proposed for goal-driven dialog. In goal-driven and goal-free dialog approaches, symbols are not grounded into visual objects. Language grounded in environments allows agents to change the environment using natural language. Grounded instruction following relies on pre-generated action sequences annotated with human descriptions. Language grounding has also been studied for robot navigation, manipulation, and environment mapping. Emergent communication in cooperative games between agents has been studied, where communication protocols emerge as a result of training agents to achieve shared goals. These methods focus on learning to communicate small amounts of information, rather than complete, semantically meaningful scenes like in the CoDraw task. The learned communication protocols are often not natural or interpretable, unlike the goal of developing agents that can assist and communicate with humans. The CoDraw dataset enables people to draw semantically rich scenes on a canvas using a library of clip arts. Scenes depict children playing in a park and consist of 6 objects on average. An abstract scene is created by dragging and dropping clip art. A drag-and-drop interface allows real-time interaction between two people in creating semantically rich scenes using clip art objects. The Teller describes the scene while the Drawer reconstructs it based on instructions, with each side sending one message at a time. The participants in the interaction are limited to sending one message at a time, with a maximum length of 140 characters to encourage frequent pauses. Both sides must be confident that the Drawer accurately reconstructed the scene before submitting the task. Teller is not allowed to continuously observe Drawer's canvas to focus on high-level semantics, but visual feedback may be necessary for details. The dataset and infrastructure for live chat with live drawing will be publicly available. The CoDraw task involves pairs of people completing scenes with one dialog per scene. Dialogs contain 138K utterances and snapshots of the Drawer's canvas. 10% of scenes are reserved for testing and development sets, with the remaining dialogs used for training. The message length distribution for the Drawer is skewed towards shorter replies. The CoDraw task involves pairs completing scenes with one dialog per scene. Dialogs contain 138K utterances and snapshots of the Drawer's canvas. The vocabulary size is 4,555, with a focus on abstract scenes. The goal-driven nature of the task allows for evaluation of communication effectiveness through a scene similarity metric. See Appendix B for a detailed analysis of the dataset. The CoDraw task involves pairs completing scenes with one dialog per scene, allowing for evaluation of communication effectiveness through a scene similarity metric. The metric compares machine-machine, human-machine, and human-human pairs in completing the task by computing scene similarity based on clipart pieces. The metric is normalized to penalize missing or extra clip art, resulting in a 0-5 scale. The CoDraw task evaluates communication effectiveness through scene completion on a 0-5 scale. Assumptions are made regarding the Teller and Drawer models, including the omission of the Drawer's ability to ask questions. This assumption does not reduce the modalities needed to solve the task. Assumptions are made regarding the Teller and Drawer models in the CoDraw task, including the omission of the Drawer's ability to ask questions. This does not reduce the modalities needed to solve the task. Future work includes creating models that can detect when a clarification is required and generate natural language clarification questions. The data collection process for humans allows the Teller to peek at the Drawer's canvas, a behavior not included in the models. Omitting this behavior does not decrease the number of modalities needed. Future work also includes creating models that can peek at the time that maximizes task effectiveness. Additionally, the drawer models can select from the full clip art library, unlike humans. The drawer models can select from the full clip art library, giving them more flexibility to make mistakes and detect incorrect groundings. Models are initially built using nearest-neighbors and rule-based approaches, with a rule-based dialog policy for describing clip art in a fixed order. The Teller follows a fixed order in conversation, starting with objects in the sky, then objects in the scene, and ending with small objects. They produce an utterance by looking up nearest neighbors in a database of Teller utterance-clip art pairs. The database is constructed from instances where the Drawer added a single clip art piece to the canvas in response to the Teller's message. Instances with multiple clip art pieces or changes to existing pieces are not used. The Drawer model in CoDraw task uses a rule-based approach to add clip art objects to the scene based on the Teller's utterance. Another approach involves a neural network for contextual reasoning in the task. The CoDraw task involves contextual reasoning where the model conditions on past conversation history to generate coherent scenes using clip art pieces. The Teller uses an LSTM language model with attention to the scene, while the Drawer uses a BiLSTM encoding to decide which clip art pieces to add. Intermediate supervision is used to predict clip art not described yet, and reinforcement learning calculates rewards based on the scenes produced. The CoDraw task involves contextual reasoning where the model conditions on past conversation history to generate scenes using clip art pieces. The Drawer considers the most recent utterance from the Teller and the current canvas to decide what to draw next. The current state of the canvas is represented using indicator and real-valued features for each clip art type. Incorporating additional context did not improve performance. The canvas representation includes binary features for clip art presence and indicator features for attributes. A bi-directional LSTM processes the Teller's message for the Drawer's neural network to generate an output action vector. The canvas representation includes features for clip art presence and attributes. A neural Teller model uses a scene2seq architecture for continuous relaxation of canvas encoding. Scores determine if clip art should be added, with log-probabilities for attribute values and positions trained using losses. The scene2seq architecture in Teller models incorporates scene information before and after each LSTM cell using an attention mechanism. Clip art objects in the scene are represented by learned embeddings for different attributes. During testing, the Teller constructs messages by decoding from the language model. Effective communication requires the Teller to track described scene parts and generate language that helps achieve the task objective. The scene2seq model in Teller models uses alternative training objectives to improve dialog coherence for novel scenes. This includes introducing an auxiliary loss based on predicting drawings and fine-tuning Teller models with reinforcement learning to optimize task completion. State tracking is incorporated into the architecture through an auxiliary loss to maintain end-to-end training and improve test-time decoding. The scene2seq model in Teller uses an auxiliary loss to predict clip art drawings, improving dialog coherence by reducing repetition and omissions. This additional signal helps the agent imitate human behavior while maintaining end-to-end training and test-time decoding. The Teller is trained with reinforcement learning to improve scene reconstructions by using effective language. The scene2seq Teller architecture remains unchanged, with actions outputting words or special tokens. A neural Drawer model is used to make scene changes based on the Teller's actions, with rewards based on scene similarity. To address the issue of no incentive to end the conversation in scene descriptions, a penalty of 0.3 is applied to the reward when the Drawer makes no changes. Models are trained using REINFORCE and evaluated by pairing them with other models or humans. Automated evaluation protocols are also used to assess the quality of different Drawer models. The Drawer models are evaluated by pairing them with a Teller that replays recorded human conversations. The models outperform humans in scene similarity but may not always correspond in meaning to actions taken. The setup does not capture the full interactive nature of the task, but the Drawer model still reconstructs scenes based on human descriptions without the ability to ask clarifying questions. Automatically evaluating agents, especially in the machine-machine paired setting, requires caution to prevent co-adaptation where agents communicate in a shared code that doesn't resemble natural language. Overfitting to training data can lead to this, as seen with rule-based nearest-neighbor agents. In the CoDraw task, a Drawer-Teller pair trained on the same data outperforms humans, but limited generalization is observed. To address co-adaptation, a training protocol called \"crosstalk\" splits the data for separate training of agents to ensure they learn generalizable language. The dataset split for experiments includes 40% Teller training data, 40% Drawer training data, 10% development data, and 10% testing data. Results show that the Neural Drawer outperforms the rule-based nearest-neighbor baseline and approaches human performance. Validity of script-based Drawer evaluation is confirmed through interactive pairing with human Drawers. The scene2seq Teller model trained with imitation learning achieves over 91% effectiveness compared to human Tellers. Pairing models with humans shows improvement with auxiliary loss and reinforcement learning. However, there is still a gap in performance compared to human Tellers. Participants noted unclear instructions and frustration with model responses during interactive pairing. The Teller models in the study struggle with reasoning about context, planning ahead, and making errors like referencing objects not present in the scene. Automated evaluation aligns with human evaluation, showing promise as a proxy for human assessment in the CoDraw task. The Teller models in the study struggle with reasoning about context, planning ahead, and making errors like referencing objects not present in the scene. In a sample of 5 scenes, the rule-based nearest-neighbor describes non-existent objects 11 times, while the scene2seq Teller trained with imitation learning only does so once. The scene2seq Teller often re-describes objects multiple times or forgets to mention some, with the addition of an auxiliary loss and RL fine-tuning reducing these errors. On the Drawer side, the most common mistakes made by the neural network are highlighted. The study highlights common mistakes made by neural networks in a collaborative task called CoDraw, focusing on semantically inconsistent placement of clip art pieces. The task combines language, perception, and actions for learning effective communication in a grounded context. The paper introduces a dataset, models, and a crosstalk training + evaluation protocol for studying emergent communication. The study introduces models for studying emergent communication in a grounded context, focusing on long-term planning and contextual reasoning challenges. The CoDraw task aims to improve agents' natural language abilities and coherence in dialogues, grounded in perception and actions. The interface for the Teller and Drawer is shown, with the Drawer given clip art objects to reconstruct a scene based on questions from the Teller. The study introduces models for studying emergent communication in a grounded context, focusing on long-term planning and contextual reasoning challenges. The CoDraw task aims to improve agents' natural language abilities and coherence in dialogues, grounded in perception and actions. The interface for the Teller and Drawer is shown, with the Drawer given clip art objects to reconstruct a scene based on questions from the Teller. The Drawer can draw on the canvas in a drag-and-drop fashion and send messages using a given input box. The peek button is disabled for the Drawer, only the Teller can use it. Approximately 13.6% of human participants disconnect voluntarily in an early stage of the session, but participants who stay in the conversation and post at least three messages are paid. Incomplete sessions are excluded from the dataset, only completed sessions are used. The dataset consists of 9,993 completed sessions with 616 unique participants. The 5 most active workers completed 26.63% of all tasks. The CoDraw dataset includes 138K utterances in 9,993 dialogs describing abstract scenes. Drawer messages are typically short, while Teller messages have a smoother distribution. The dataset includes 9,993 completed sessions with 616 unique participants. Teller messages have a smoother distribution with a median length of 16 tokens. The vocabulary size is 4,555 due to conversations describing abstract scenes. Most interactions have fewer than 20 rounds, with a median of 7. The median session duration is 6 minutes, with a 20-minute maximum limit. Conversations show improvement in scene similarity after about 5 rounds, with longer conversations continuing to 23 rounds showing room for further improvement. The scene similarity between ground-truth and predicted scenes is defined based on various factors such as clip art direction, facial expression, body pose, and size. Parameters are used to balance these components, ensuring similarities are between 0 and 5. Examples from the CoDraw dataset are provided to illustrate qualitative aspects. Examples from the CoDraw dataset show scenes and dialogs, with the Drawer and Teller models illustrated in Figure 9. The images depict the progression of the Drawer's canvas during conversations, with corresponding dialogues between the Teller and Drawer. There are no restrictions on who starts or ends the dialogues. The curr_chunk describes a scene with a small hot air balloon in the bottom right corner, a large bear on the left facing right, a surprised girl running and facing right, a small pine tree behind her, and a small boy standing in front of the tree holding a hot dog. Additionally, there is a mad mike with hands in front facing left. The scene is compared to descriptions generated by Teller models in a figure."
}