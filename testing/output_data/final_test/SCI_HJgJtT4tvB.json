{
    "title": "HJgJtT4tvB",
    "content": "Recent powerful pre-trained language models have achieved remarkable performance on popular datasets for reading comprehension. Introducing a new Reading Comprehension dataset requiring logical reasoning (ReClor) from standardized graduate admission examinations. To evaluate models' logical reasoning ability on ReClor, biased data points are identified and separated into EASY and HARD sets. State-of-the-art models excel at capturing biases in the dataset with high accuracy on EASY set but struggle on HARD set. State-of-the-art language models excel at capturing biases in datasets with high accuracy on easy tasks but struggle on harder tasks, indicating the need for enhanced logical reasoning abilities. Machine reading comprehension is a fundamental task in NLP, with models like GPT-2, BERT, XLNet, and RoBERTa achieving saturated performance on popular datasets. It is time to challenge these models with more difficult tasks to advance comprehensive analysis and reasoning. In natural language understanding, logical reasoning is crucial for examining, analyzing, and evaluating arguments in ordinary language. Existing reading comprehension datasets lack data requiring logical reasoning, highlighting the need for enhanced reasoning abilities in NLP tasks. To advance logical reasoning models from simple relationships to complex reasoning at passage-level, a reading comprehension dataset targeting logical reasoning is essential. This dataset includes a context, a question, and four options, requiring readers to identify logical connections and select the correct answer. Extensive training is needed for complex reasoning, making it challenging for crowdsourcing workers. Building on the need for a reading comprehension dataset targeting logical reasoning, a dataset called ReClor was created by selecting logical reasoning questions from standardized exams like GMAT and LSAT. This dataset consists of 6,139 questions, aiming to assess logical reasoning abilities without biases commonly found in human-annotated datasets. The presence of biased data points can lead neural network models to exploit shortcuts for high testing accuracy. The study proposes to categorize biased data points as EASY and HARD sets to evaluate logical reasoning ability of models. Results show that current models struggle on HARD set, indicating a need for improved reasoning capabilities. The introduction of ReClor dataset aims to address this gap by requiring logical reasoning skills for comprehension tasks. Several state-of-the-art models were evaluated on the ReClor dataset, revealing that while they perform well on biased data points categorized as EASY, they struggle on the HARD set, indicating a lack of real logical reasoning capabilities. Various reading comprehension datasets, such as MCTest and SQuAD, have been introduced to promote the development of this field. The authors analyzed question-answer pairs on Wikipedia articles, finding that examples mainly require reasoning of lexical or syntactic variation. Different datasets like MCTest and SQuAD have varying levels of logical reasoning questions. The RACE dataset was introduced for middle and high school students, with around 70% of samples requiring word matching, paraphrasing, or single-sentence reasoning. More reading comprehension datasets are being introduced to encourage deeper comprehension of language. The curr_chunk discusses the introduction of new reasoning types in NLP tasks, such as iterative reasoning, multi-hop reasoning, commonsense knowledge reasoning, and numerical discrete reasoning. It also mentions the lack of datasets targeting logical reasoning in reading comprehension tasks. This work introduces a new dataset to address this gap. The curr_chunk discusses logical reasoning in NLP, focusing on natural language inference tasks and argument reasoning comprehension tasks. It mentions datasets like SNLI and MultiNLI for sentence-level logical relationship reasoning and a dataset introduced by Habernal et al. for passage-level logical reasoning. The task focuses on passage-level logical reasoning, specifically identifying warrants. It aims to integrate various logical reasoning types into reading comprehension to develop models from simple to complex reasoning types. Several datasets from human standardized exams in NLP are mentioned, such as RACE, NTCIR QA Lab, CLEF QA Entrance Exams Task, and ARC dataset targeting science questions for different grade levels. ReClor dataset focuses on logical reasoning in reading comprehension, containing 7,787 science questions for grades 3 to 9. It differs from other datasets by emphasizing logical reasoning. The data format includes a context, a question, and four answer options, with one correct option. The dataset requires complex logical reasoning skills, making it challenging for crowdsourcing workers. The dataset contains 6,139 logical reasoning questions sourced from GMAT and LSAT exams. Answer options are shuffled and one wrong option is randomly deleted to comply with fair use laws. The dataset will be available for non-commercial research and educational use. 91.22% of the data points are from GMAT and LSAT exams. The ReClor dataset consists of 6,139 logical reasoning questions from GMAT and LSAT exams, with 91.22% from actual exams. It is divided into training, validation, and testing sets. Compared to other datasets, ReClor has a large vocabulary size and shorter context passages. Each sentence in ReClor is important, focusing on logical reasoning ability rather than information extraction. Answer options in ReClor are the largest among similar datasets. The ReClor dataset contains 6,139 logical reasoning questions from GMAT and LSAT exams, categorized into 18 question types. Examples of question types are provided, along with human solving methods. The dataset may contain biases, which models can exploit without truly understanding the text. Bugert et al., 2017; Poliak et al., 2018; Gururangan et al., 2018; Zellers et al., 2019) analyze data biases in the ReClor dataset, focusing on lexical choice and sentence length differences between right and wrong options. They use BERT BASE for tokenization and analyze conditional probabilities of tokens contributing to prediction. Various neural network models like FastText, Bi-LSTM, GPT, and GPT-2 are also considered. The curr_chunk discusses the purpose of laws in relation to people's happiness and the evaluation of existing laws. It also presents a question about flawed reasoning in an argument. The reasoning process of humans is briefly mentioned. The curr_chunk discusses the reasoning process of humans in identifying flaws in arguments. It emphasizes the importance of understanding the specific task of a question and analyzing the argument in context. The conclusion is based on the argument that existing laws acquire legitimacy simply because they are the laws, which is flawed reasoning. The distractors in the question are different types of reasoning flaws, requiring prior knowledge of basic logical rules to answer correctly. The psychologist's statements suggest that some children taught by the whole-language method can still learn how sounds are represented by letters, enabling them to read alphabetic languages. The ability to read an alphabetic language is acquired by learning how to represent sounds with letters. The whole-language method teaches many children this skill, implying that it meets the necessary conditions for learning a language. Children must recognize symbolic letters to read alphabetic languages. Option B is correct as symbolic letters are necessary for reading alphabetic languages. Options C and D are incorrect as other methods may also teach symbolic letters, but they are not directly linked to reading alphabetic languages. The current pattern of human consumption of nonrenewable resources, like metal ore, must change. The current pattern of human consumption of nonrenewable resources, such as metal ore, must change. Ultimately, we cannot do without nonrenewable resources, but we must turn to renewable resources to replace them. Some theorists argue for value-neutral literary criticism, but it cannot be completely neutral as critics' evaluations influence readers' judgments. Some theorists argue for value-neutral literary criticism, but critics' evaluations influence readers' judgments. The argument's conclusion depends on assumptions about the goals of literary criticism. Since Henry's move to Wednesday evenings, it has seen a significant drop in viewership. This suggests that its previous popularity was due to its time slot following That's Life, rather than its own appeal. A blood test can detect high levels of a specific protein linked to gland cancer early. Doctors recommend early aggressive treatment for those with high protein levels. Weakens recommendation: patients with normal protein levels have not developed gland cancer. In a study, trained dogs showed aversion to unfair treatment by disobeying commands when unrewarded. RoBERTa neural models have achieved impressive results in NLP tasks and are being challenged with ReClor. Biases are prevalent in these models. In a study, trained dogs showed aversion to unfair treatment by disobeying commands when unrewarded. RoBERTa neural models have achieved impressive results in NLP tasks and are being challenged with ReClor. Biases are prevalent in these models. The implementation details are in Appendix A and B. Biases in human-annotated datasets are exploited by models to perform well without truly understanding the text. It is necessary to identify biased data points in ReClor to evaluate models comprehensively. Five strong baseline models (GPT, GPT-2, BERT BASE, XLNet BASE, and RoBERTa BASE) are fed ONLY THE ANSWER OPTIONS for each problem to identify problems that can be answered correctly by exploiting biases in answer options without knowing the context and question. The task involves multiple-choice questions with 4 options, aiming to eliminate random guessing by selecting data points predicted correctly by all models with different random seeds to form the EASY set. This process reduces the impact of random guessing, as the probability of a data point being guessed correctly consecutively by all models is low. The sets of data points consistently predicted right by each model are combined to account for different biases learned by each model. Average performance of models trained with four different random seeds is shown in Table 6. Table 6 displays the average accuracy of models trained with different random seeds using only answer options as input. The dataset is divided into an EASY set and a HARD set based on the number of correct predictions. Previous studies have highlighted the effectiveness of pre-training on similar tasks before fine-tuning on the target dataset for transfer learning. After pre-training on RACE, BERT BASE performance significantly improves on MC500 and DREAM datasets. However, fine-tuning on SQuAD first does not yield significant improvements. ReClor is a multiple-choice dataset used for fine-tuning. The dataset consists of questions for graduate school applicants, with 100 samples randomly chosen for testing. The ReClor dataset consists of carefully chosen high-quality questions from standardized graduate entrance exams. The performance of fastText is better than random guessing, showing that word correlation can improve performance. Bi-LSTM struggles to converge on the dataset. Transformer-based pre-training models perform close to graduate students' baseline. The models perform well on the EASY set but poorly on the HARD set, capturing dataset biases. Humans maintain good performance on both sets. Training on RACE and fine-tuning on ReClor improves model performance, approaching that of graduate students. Similar results are seen on the DREAM dataset. Transfer learning for reasoning tasks has shown potential on the DREAM dataset. Despite fine-tuning on RACE, strong baselines still perform around 50% on the HARD set, lower than graduate students. Different input settings were tested, with questions and answer options not significantly improving performance compared to answer options only. Adding context greatly boosts performance, highlighting the importance of context informativeness. The model performance of BERT LARGE, XLNet LARGE, and RoBERTa LARGE was analyzed with respect to different question types of logical reasoning. They performed poorly on certain types like STRENGTHEN, WEAKEN, and ROLE on the HARD set, which require extensive logical reasoning. However, they performed relatively better on types like CONCLUSION and SUMMARY/MAIN POINT. XLNet LARGE showed significant improvement after fine-tuning on RACE, with similar accuracy improvements on different question types on the EASY set, but significant improvements on certain question types on the HARD set. Pre-training on RACE dataset enhances logical reasoning ability, especially on simpler question types like EVALUATION, CONCLUSION, and SUMMARY/MAIN POINT. More methods are needed to improve performance on complex reasoning types. XLNet LARGE +Fine-Tune model shows significant improvement on HARD set question types. Introduction of ReClor dataset aims to advance logical reasoning research in NLP from sentence-level to passage-level. The study advances logical reasoning in NLP by identifying biased data points and splitting the testing set into EASY and HARD groups. Transformer-based models excel at exploiting biases but struggle with non-biased data, highlighting the need for real logical reasoning abilities in deep learning models. The work suggests using a similar split technique for model evaluation and shows improved performance after fine-tuning on RACE and ReClor datasets. Transfer learning using fine-tuning on RACE and ReClor datasets improves model performance for reasoning tasks. Models like fastText, LSTM sentence encoder, GPT, and GPT-2 are utilized for this purpose. The GPT-2 model is pre-trained on BooksCorpus and WebText, while BERT is trained on BooksCorpus and English Wikipedia using unsupervised tasks. During fine-tuning, the final hidden vector is used as the aggregate representation followed by fully connected layers to compute the score. XLNet is another transformer-based model with a different input format. XLNet, GPT, GPT-2, BERT, RoBERTa, and Bi-LSTM models use Adam optimizer with specific parameters. FastText is used with default hyperparameters. Bi-LSTM utilizes GloVe 300d word embedding, max-pooling, and a fully-connected layer during training. The model is trained for 100 epochs with a batch size of 64 and learning rate of 0.1. For pre-training models on ReClor, modifications were made to PyTorch-Transformers code from Hugging Face. The batch size used was 24 for fine-tuning over 50 epochs. Different models had varying fine-tuning learning rates selected from 6.25e-05 to 5e-06. The maximum input sequence length for all models was set at 256. The argument discusses the impact of slash-and-burn agriculture on tropical forests, stating that this method leads to the permanent eradication of forests in the region. The assumption made is that forests in the tropics do not regenerate well enough to restore themselves once cleared using this method. The geologist's argument questions the usefulness of a new earthquake forecasting method that can only predict earthquakes within a range of two and a half points on the Richter scale. The assumption is that this method is unlikely to be useful due to the significant difference in earthquake intensity within that range. The geologist questions the usefulness of earthquake forecasting methods that predict within a narrow range on the Richter scale. Reliable predictions are essential for a method to be considered useful. In a survey on financial success and happiness, only one-third of successful respondents reported being happy. The survey results suggest a disconnect between financial success and happiness. DNA fingerprinting is a biochemical procedure using genetic material to match suspects to crime scene specimens. Proponents claim high odds against chance matches, assuming independence between genetic characteristics. A. The skill of technicians, B. The theoretical basis for interpreting patterns, C. Genetic subgroups in the population, and D. Techniques for genetic disease investigation are factors that cast doubt on the claims. DNA fingerprinting is used in genetic disease investigations to trace disease transmission among large families. Logical reasoning type - Weaken Type: Evaluation involves identifying information to evaluate an argument. In a scenario about global warming, relevant factors like unusual weather patterns are crucial for evaluation. To evaluate George's argument, it is important to consider whether unusually warm weather is occurring more frequently globally. In a scenario about global warming, factors like unusual weather patterns are crucial for evaluation. DNA fingerprinting is used in genetic disease investigations to trace disease transmission among large families. The curr_chunk discusses the paradoxical result of birds attracted by trees not eating mosquitoes, and the inefficiency of buying elaborate screensavers in terms of cost and employee time. The curr_chunk discusses a flawed pattern of reasoning in an argument about thieves gaining access to a museum from below ground level. The curr_chunk discusses flawed reasoning in various scenarios, including selling shirts at a loss, insecticide safety, census data on marriage, and government support for art. The curr_chunk discusses flawed reasoning in different scenarios, such as government support for art, and presents options for similar patterns of reasoning. The answer is option D. The curr_chunk discusses the relevance of remote considerations in determining the seriousness of an offense. It argues that if such considerations were relevant, it would be impossible to apply the proportionality principle. The answer is option A. The Bay of Fundy has one of the world's largest tidal ranges, exceeding seventeen meters, due to gravitational forces from the sun and moon. A flaw in the reasoning is not considering how conditions can affect tidal range sizes. PhishCo plans to drill down to an aquifer to raise fish in ponds for increased farm efficiency. The ponds will also help reduce heat in the area. The plan's efficiency will be increased if organic waste from fish fertilizes fields used for irrigation."
}