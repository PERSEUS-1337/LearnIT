{
    "title": "HJx9EhC9tQ",
    "content": "Object-based factorizations provide a useful level of abstraction for interacting with the world. Our model, Object-Oriented Prediction and Planning (O2P2), learns object-centric representations for physical scene understanding without direct supervision. It includes a perception function, physics interaction function, and rendering function. The model's accuracy in physical predictions and utility for downstream tasks are evaluated. Humans possess a natural physical intuition that can be acquired and refined through experience, aiding in everyday tasks. Despite efforts in artificial intelligence and computer vision research, machines still struggle with physical scene understanding. Cognitive scientists argue that humans' ability to interpret the physical world stems from a structured apparatus, grouping the world into objects and their relations. Applying this insight to machine learning methods remains challenging. Applying human-like physical intuition to machine learning methods is challenging due to the difficulty in designing an interface between raw observations and structured object representations. Existing works either assume an idealized object space or require supervision to learn mappings between raw inputs and object properties. Access to training labels for all object properties is impractical due to scalability issues. Our method uses learned object representations to guide planning for building observed castles. Object factorization and physics engine structure are employed without assuming supervision of object properties. Joint learning of perception, physics, and rendering functions is necessary. The challenge lies in defining attributes like orientation without clear guidelines. In this paper, Object-Oriented Prediction and Planning (O2P2) is proposed to train an object representation for physical interactions without direct supervision of object attributes. Segments or proposal regions in video frames serve as supervision for intuitive physics reasoning. A perception module, object-factorized physics engine, and neural renderer are jointly trained on a physics prediction task with a pixel generation objective. The model is evaluated on prediction quality and the use of learned representations. The method described in this section focuses on learning object-based representations for physical reasoning tasks without direct supervision of object attributes. It involves a perception module that maps images to object encodings and a physics module for training jointly. The model, called O2P2, consists of three components and is evaluated based on prediction quality and the use of learned representations. The perception module is a four-layer convolutional encoder that maps an image observation to object representation vectors. It is applied to each object segment independently. The model also includes a physics module to predict the time evolution of objects and a rendering engine that produces an image prediction from a variable number of objects. The perception module maps image observations to object representation vectors, while the physics module predicts the effects of simulating object interactions forward in time. The physics engine consists of a unary transition function and a binary interaction function applied to all pairs of object representations. The k th object is represented by two-layer MLPs, focusing on action planning rather than physical interactions. A single physics prediction is used to estimate object configurations indefinitely. Training the model involves mapping objectvector predictions back to images at the pixel level. The learned renderer uses two convolutional decoders to construct a single image from a collection of objects. The first network predicts images for each object, and the second network produces heatmaps for object visibility. The final scene image is a weighted average of object renderings based on heatmap predictions, prioritizing objects with lower heatmap values. The model is trained on image pairs showing objects before and after a new object is dropped. Object representations are predicted using a perception module based on segmented images. The perception module predicts object representations and the physics module predicts their time-evolution. The rendering engine then predicts images from the object representations. Image predictions are compared to ground-truth using L2 distance and a perceptual loss. Actions are sampled and observed to update existing object segments. The perception, physics, and rendering modules are used in a planning task to build a block tower matching an observed image. The model must output a sequence of actions to construct the tower, considering the sequential ordering of placing cubes and triangles. The model was trained on a pixel-prediction task and never shown valid configurations. The planning procedure for constructing towers in O2P2 involves encoding the goal image, sampling actions, and evaluating them based on errors in the learned object representation space. The perception module encodes the goal image, actions are sampled in categorical and continuous forms, and the evaluation process is guided solely through errors in the object representation space. The planning procedure for constructing towers in O2P2 involves encoding the goal image, sampling actions, and evaluating them based on errors in the learned object representation space. Actions are encoded as object vectors and compared with the goal object to minimize the L2 distance. The selected action is executed in MuJoCo BID25, and this process is iteratively repeated until the goal is achieved. Our model predicts object representations, simulates them with a physics engine, and renders the scene's appearance. Actions are sampled from a distribution and executed until the goal is achieved using the crossentropy method. In practice, CEM is used with five iterations and 1000 samples per iteration, selecting the top 10% for the next iteration. The experimental evaluation focuses on O2P2's ability to reason about physical interactions and the benefits of its object factorization structure. Training involves reconstructing observed objects and predicting their configuration after simulating physics. Training data is generated by simulating block interactions on a platform. The study involved simulating block interactions on a platform to collect training data for the O2P2 model. The model was trained to reconstruct observed objects and predict their configurations after simulating physics. The physics engine produced plausible steady-state configurations of the observed scenes, even when the model's predictions differed from the ground truth images. After training on random block configurations, the model's parameters were fixed for further evaluation. After training O2P2 on random block configurations, its parameters were fixed for building tower configurations using a planning procedure. Comparisons were made with models like No physics and Stochastic adversarial video prediction (SAVP). Qualitative results showed the ability to recreate tower configurations using learned object representations and a predictive model in O2P2. The comparison was made with an ablation, an object-agnostic video prediction model, and two 'oracles' using the MuJoCo simulator. The oracles evaluated samples directly in pixel space and with segmentation masks, showing qualitative results in Figure 4 and quantitative evaluation in TAB1. Tower stacking success was evaluated by comparing the built configuration to the goal tower's ground-truth state, considering position, identity, and color errors. The threshold for error in tower stacking is arbitrary but maintains relative model ordering. SAVP struggled with complex configurations, leading to low stacking success. Physics simulation was crucial for accurate tower stacking, as shown in the comparison with an ablation model. The 'oracle' model with access to ground-truth physics faced challenges in pixel space comparisons. The physics simulator was hindered in pixel space comparisons. The model often dropped a large block to cover multiple smaller blocks in the goal image. O2P2's planning and execution process for stacking blocks was visualized, showing the importance of dropping objects in a stable sequence. The No-physics ablation model did not represent this sequence implicitly. The model scores action samples based on (x, y) position, selecting a stable sequence of actions. It correctly determines that block height doesn't matter as they will drop to the correct height. Heatmaps show model predictions after releasing blocks. The model can be used for various tasks besides matching towers, such as planning a sequence of actions to maximize tower height without physics simulation. In Figure 7b, a single unstable block is shown, requiring planning with O2P2 for multiple steps at once. O2P2 is used to plan for maximizing tower height and stabilizing blocks. Evaluation on a Sawyer robotic arm using real image inputs was conducted. More evaluation examples can be found in Appendix B and videos at people.eecs.berkeley.edu/\u223cjanner/o2p2. The embedder module mapped actions directly to object representations, eliminating the need to manually move blocks in front of the camera. It was supervised by predicted object representations from the perception module on real image inputs. A small dataset of the Sawyer gripper holding each object at various positions was collected for training. The embedder module replaced lines 6-8 of Algorithm 1 and updated the objective for action selection. It included pixelwise L2 distance calculations for observed and rendered object segments. End-effector position control on the Sawyer gripper allowed for the same action space in real settings. Automated picking motions were used to select blocks based on position components of sampled actions. Our work automated picking motions to select blocks based on shape and color components. Real-world evaluation used colored wooden cubes and rectangular cuboids. Object segments were estimated using a color filter and connected components. O2P2 correctly built seventeen out of twenty-five goal configurations. Our work explores two paradigms in object representation: one enforces supervision of object properties, while the other does not require extra supervision. These approaches have been applied in image and video understanding, leveraging the stability of static observations to improve 3D scene understanding algorithms. We consider learning physical representations from data instead of predetermined forms, with some methods encoding scenes for rendering engines and physics simulators. Our work focuses on training a perception module to map object segments into a markup language without assuming access to supervision of object properties. We use object representations learned with minimal supervision, such as segmentation masks, for downstream tasks related to predicting and reasoning about physical phenomena. This approach allows for the development of models capable of decision making based on inferred physics. Learning and inferring physics without access to ground truth supervision. Using a physics formulation with update rules composed of object-interaction functions. Considering physics prediction as an image-to-image translation or classification problem. Evaluating model predictions for downstream tasks. Our approach introduces a method of learning object-centric representations for physical interactions without relying on traditional supervision of object properties. This approach outperforms existing methods in tasks involving building structures out of blocks and can be used for difficult planning problems. The method introduces object-centric representations for physical interactions, outperforming traditional approaches in tasks like block tower matching without requiring further adaptation."
}