{
    "title": "SJQHjzZ0-",
    "content": "Generative adversarial networks (GANs) have been highly effective in approximating complex distributions of high-dimensional input data samples, with significant advancements in understanding and enhancing GAN performance in theory and application. In this paper, the performance of various types of GANs is evaluated using divergence and distance functions typically used for training. Consistency is observed across the proposed metrics, with test-time metrics not favoring networks that use the same training-time criterion. The proposed metrics are also compared to human perceptual scores. GANs aim to approximate a data distribution using a parameterized model distribution, achieving this through joint optimization of generative and discriminative networks. In this work, new metrics are proposed for evaluating the realism of samples generated by GANs. These metrics are based on divergence between distributions and aim to address the lack of quantitative methods for assessing GAN generators. Different choices of \u00b5, \u03c5, and F correspond to various f-divergences or integral probability metrics. J(Q) can be estimated using samples from P and Q without needing to estimate P(x) or Q(x). The divergence measure between distributions P and Q is related to the GAN criterion when using neural network functions and generators. LS-DCGAN used b = 1 and a = 0. The metric \u00b5 \u03c5 Function Class GAN involves log f \u2212 log(1 \u2212 f ) X \u2192 R + with a Least-Squares approach. The proposed metrics for evaluating the performance of the generator network in GAN training involve adaptive measures based on different f-divergences or IPMs between distributions P and Q. Four metrics are compared, including those for the original GAN, Least-Squares GAN, Wasserstein GAN, and Maximum Mean Discrepancy GAN criteria. The method can be extended to other f-divergences or IPMs. Our method for evaluating GANs involves comparing various metrics through experiments with different GAN architectures and datasets. The proposed metrics were found to better reflect human perception and produce consistent rankings across different GAN frameworks. The analysis included Deep Convolutional Generative Adversarial Networks (DCGAN), Least-Squares GANs (LS-DCGAN), and Wasserstein GANs (W-DCGAN), with each metric agreeing on the best GAN framework for each dataset. Our analysis of GAN frameworks found that different metrics agreed on the best framework for each dataset. Factors like architecture size, noise dimension, and update ratio were considered, showing that larger architectures yield better results and the best update ratio varies by dataset. W-DCGAN and LS-DCGAN outperform DCGAN as training examples increase, allowing for hyper-parameter tuning based on our method. Several automatically computable metrics have been proposed for evaluating the performance of probabilistic generative models and GANs. While GANs can generate samples from an approximate distribution, evaluating their likelihood on test samples is challenging. The efficacy of log-likelihood of test data, estimated using Annealed Importance Sampling (AIS), was evaluated. Likelihood often does not provide good rankings of how realistic samples look, which is the main goal of GANs. The AIS method estimates the likelihood of test samples by considering intermediate distributions. The accuracy of this estimation is crucial, as shown in Section 4. The Generative Adversarial Metric measures the relative performance of two GANs by comparing the likelihood ratio of their models. The Generative Adversarial Metric evaluates the relative performance of two GANs by comparing the likelihood ratio of their models. It involves swapping pairs of discriminators and generators to measure how frequently one model fools the other. The metric has limitations in that it only compares pairs of models and requires similar performance from the discriminators on a calibration dataset. The Inception Score (IS) measures GAN performance by comparing class predictions from a third-party neural network trained on Imagenet. IS is widely used but discards important sample information and requires a separate supervised neural network. The Fr\u00e9chet Inception Distance (FID) extends upon the Inception Score (IS) by comparing the mean and covariance of representations from a late layer of a third-party network for generated and training samples. Classifier Two-Sample Tests (C2ST) proposes training a classifier to distinguish real samples from generated ones. Classifier Two-Sample Tests (C2ST) proposes training a classifier, similar to a discriminator, to distinguish real samples from generated ones, using the error rate as a measure of GAN performance. They used single-layer and k-nearest neighbor classifiers trained on representations computed from a late layer of a third-party network. C2ST is an IPM with a different function class corresponding to the chosen family of classifiers. To evaluate the quality of a generator G \u03b8, divergence between true data distribution P and generated distribution Q \u03b8 is estimated using different divergence measures. Training involves optimizing parameters \u03b8 for the generator G and \u03d5 for the discriminator D on a training set, with performance evaluated on a separate test set. Various metrics, including f-divergence and Integral Probability Metric (IPM), are considered for training Generative Adversarial Networks (GANs). The goal is to minimize a specific function involving the prior distribution p(z) and the generator function G \u03b8 (z) mapped to the data space. The training of a neural network with parameter \u03b8 involves using a sigmoid activation function for positive output. A Least-Squares GAN is trained with a Pearson \u03c7 2 divergence, setting a = 0 and b = 1 for training. The maximum mean discrepancy metric considers differences in expectations over a unit ball of RKHS H. The Improved Wasserstein Distance (IW) utilizes the dual representation of the Wasserstein distance for GAN training. IW and MMD are proposed metrics for evaluating GANs, with the critic network parameter \u03d5 initialized for training. In our experiments, we evaluated metrics for GANs and different GAN frameworks like DCGAN, LS-DCGAN, and W-DCGAN. We assessed the impact of discriminator and generator network sizes on performance and the sensitivity to training data set size. In evaluating GAN frameworks like DCGAN, LS-DCGAN, and W-DCGAN, different test metric names were used. The evaluation included comparing proposed metrics to commonly used IS and FID metrics, as well as human perception assessments on datasets like MNIST, CIFAR10, LSUN Bedroom, and Fashion MNIST. The MNIST, CIFAR10, LSUN Bedroom, and Fashion MNIST datasets were considered for evaluation. MNIST and FashionMNIST have 60,000 training and 10,000 test images of 28x28 pixels each. CIFAR10 has 45,000 training, 5,000 validation, and 10,000 test images of 32x32x3 pixels. LSUN Bedroom dataset has 90,000 training and 90,000 validation images of 64x64 pixels. The learning rates and convolutional kernel sizes were selected based on a held-out validation set. Pre-trained logistic regression and residual networks were used for evaluation on MNIST and CIFAR10 datasets. Log-likelihood was measured using AIS 2 on GANs, with DCGAN on MNIST tested with different variances. The log-likelihood curve of DCGAN on MNIST with varying variances indicates that a fixed Gaussian observable model may not be ideal for GANs. IS and MMD metrics show discrepancies in visual quality evaluation. Samples generated by a failed DCGAN exhibit higher IS scores despite being visually darker, possibly due to network training for intensity invariance. The failure case for MMD is illustrated in FIG2, where samples on the left are visually meaningless but still receive lower distances from MMD due to sensitivity to image intensity. In contrast, IS is less sensitive to intensity. Further experiments in Section 4.2.1 compare these metrics with human perceptual scores across different GAN frameworks in TAB1, 4 on MNIST, CIFAR10, and LSUN datasets. After training different GANs with proposed metrics, it was found that LS-DCGAN performed best for MNIST and CIFAR10, while W-DCGAN performed best for LSUN. DCGAN was unstable to train and excluded as a metric for experiments. Different critic CNN architectures were tested, and a variety of GAN frameworks were evaluated using pre-trained GANs. The evaluation of various GAN frameworks using pre-trained models showed that DRAGAN performed the best, followed by LS-DCGAN and DCGAN. The metrics were consistent for both MNIST and FashionMNIST datasets. LS-GAN criterion showed significant performance differences among the frameworks, while the IW criterion did not. The size of the validation set did not affect the metrics' consistency. LS is more robust to validation data set size compared to IW. LS critics agreed 88% of the time, while IW critics only agreed 55% of the time. Measuring LS distance is faster than measuring IW distance due to the extra computational time required for estimating IW. Training a critic using the GC criterion (corresponding to a DCGAN) was found to be unstable. The LS-DCGAN and W-DCGAN models were proposed to address the instability of training a critic using the GC criterion. DCGAN was found to be difficult to train, with LS-DCGAN being the simplest and most stable model. Visualizations of the loss surface of GANs showed the effectiveness of LS-DCGAN. LS-DCGAN is the easiest and most stable to train compared to DCGAN models. Metrics like LS, IW, MMD, and IS were compared to human perception for the CIFAR10 dataset. Volunteers were trained to distinguish between real and generated samples, with LS-DCGAN performing well. In comparing metrics like LS, IW, MMD, and IS to human perception on the CIFAR10 dataset, LS-DCGAN was found to be the most stable. IW slightly outperformed LS, with both metrics performing better than IS and MMD. Examples were shown where metrics disagreed with human perception, indicating a need for further investigation into deep network architectures for performance improvement. In analyzing the performance of GANs with varying feature map sizes, it was observed that increasing the number of feature maps led to logarithmic improvements in LS score. Different sizes of feature maps were tested for discriminative and generative networks, with results showing that a large number of feature maps for the discriminator yielded better scores than for the generator. The discriminator's better score with a large number of feature maps is confirmed by samples from various architectures. W-DCGAN shows agreement between LS and IW metrics but conflicts with MMD and IS. Comparing architectures (a) and (e) shows that a larger generator and smaller discriminator yield worse results. Having a larger generator than discriminator does not give good results. The study found that having a larger generator than discriminator does not yield good results. Results were consistent with MNIST dataset. Experimentation showed that the size of noise vectors affects the generator's ability to create meaningful images. The study experimented with LS-DCGAN and W-DCGAN models, finding that LS-DCGAN performs best with a noise dimension of 50 and W-DCGAN with a dimension of 150. The models fall into different categories of f-divergences and Integral Probability Metric, affecting their behavior in training. The update ratio of the discriminator and generator can influence the performance of GANs. The update ratio of the discriminator and generator can impact GAN performance. Experimenting with different ratios (5:1, 1:1, 1:5) on MNIST and CIFAR10 datasets showed varying results. No single ratio was superior across both datasets, with 1:1 working best for CIFAR10 and different ratios performing better for MNIST models. Dynamic tuning of update ratios for each model is necessary. DCGANs are known to be unstable, with the generator suffering as the discriminator improves. The LS score curve of DCGAN grows slowly compared to W-DCGAN and LS-DCGAN when trained with different numbers of examples on CIFAR10. W-DCGAN and LS-DCGAN show faster performance increases with more training examples compared to DCGAN. In this paper, four distance functions were used to evaluate DCGAN, W-DCGAN, and LS-DCGAN families. Performance differences were observed in terms of average experiments, but some were not statistically significant. The performance of GANs under different hyper-parameter settings was thoroughly analyzed. Future evaluations will include GRAN BID18, IW-DCGAN BID12, BEGAN BID4, MMDGAN, and CramerGAN. Ensemble approaches to GANs, such as Generative Adversarial Parallelization BID19, have also been investigated. Ensemble approaches like Generative Adversarial Parallelization BID19 have been explored in GANs. Evaluating other models like NVIL BID30, VAE BID22, DVAE BID17, DRAW BID10, RBMs BID15 BID35, NICE Dinh et al. (2014) is also considered. Model evaluation is crucial, with different metrics impacting model selection and research direction. Four distance metrics from \u03c6-divergence and IPMs classes were examined in this paper. BID38 demonstrated that optimal risk function is linked to a binary classifier with P and Q distributions conditioned on a class. The optimal risk function for a binary classifier with specific discriminant functions can be understood as the IPM, where MMD and Wasserstein distance play a key role. These distances are equivalent to the optimal risk function with 1-Lipschitz classifiers and a RKHS classifier with unit length. By appropriately choosing the loss function, MMD and Wasserstein distance can be seen as the optimal L-risk associated with binary classifiers. The study trained two critics on training and validation data, evaluated on test data, and trained six GANs on MNIST and FashionMNIST with 50,000 examples. Test scores from critics trained on training and validation data were presented, showing close results for FashionMNIST but gaps for MNIST dataset. No overfitting was observed. The study trained two critics on training and validation data, evaluated on test data, and trained six GANs on MNIST and FashionMNIST with 50,000 examples. Test scores from critics trained on training and validation data were presented, showing close results for FashionMNIST but gaps for MNIST dataset. There are gaps between the scores for the MNIST dataset and the test scores from critics trained on the validation set. Participants are trained by selecting between random samples generated by GANs versus samples from data distribution, with positive and negative rewards based on their selections. After enough training, they choose the better group of samples among two randomly selected sets. The training involved using different numbers of filters for the discriminator and generator, with a focus on convergence of the training curve. The use of linear output units for the critic network led to an increase in the IW distance curves, which could be addressed by adding a sigmoid at the output of the critic network."
}