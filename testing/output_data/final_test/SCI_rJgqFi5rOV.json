{
    "title": "rJgqFi5rOV",
    "content": "In this work, the SHREWD method is introduced to improve hierarchical retrieval scores using binary hash codes instead of real valued ones. The method incorporates semantic hierarchies to form distance metrics between sample labels, promoting similar distances between deep neural network embeddings. Additionally, an empirical Kullback-Leibler divergence loss term is used to encourage binarization and uniformity of the embeddings in a weakly supervised hashing setting. In a weakly supervised hashing setting, learning is achieved without explicit class labels but based on label similarities. Content-Based Image Retrieval (CBIR) often uses hashing for efficient nearest neighbor search. Deep hashing methods using CNNs have shown success over traditional methods, with supervised techniques relying on binary similarity matrices. Semantic hierarchies can provide richer affinity information for hashing. In the context of deep hashing methods, BID13 focuses on semantic hierarchy for non-deep hashing, while BID14 explores class-wise deep hashing using clustering-like operations to form a loss between samples of the same class. USDH utilizes pre-trained VGG model embeddings to minimize distance between binarized hash codes and pre-trained embeddings for semantic relations. In contrast to BID0's approach of using continuous representations and requiring the embedding dimension to equal the number of classes, we learn compact quantized hash codes of arbitrary length for practical real-world retrieval. Our method, SHREWD, learns compact quantized hash codes for practical retrieval performance without requiring fixed target embeddings for classes. It utilizes weakly supervised learning with relative semantic distances, where the neural network embeddings are learned to match similarities derived from labels. Training examples consist of image pairs (x, y) with supervised targets like labels or captions, and the network aims to learn similarity estimates between targets. The network learns similarity by matching embeddings with a predefined norm. Class hierarchies inform the loss function, but labels are not directly required. Different metric spaces can be used, such as web-mined tag distances or ranked image similarities. Utilizing the hashing space helps in efficient retrieval using the Hamming distance. The curr_chunk discusses the introduction of a KL divergence term in the loss function to drive embeddings close to binary values for a CNN model. The loss function includes a traditional categorical cross-entropy loss and a KL loss term to improve retrieval performance. The curr_chunk introduces the SHREWD and SHRED algorithms for weakly and fully supervised learning, respectively. It discusses using semantic similarity metrics to find affinity between normalized distances in the embedding space and semantic space. The algorithm defines DISPLAYFORM0 with terms like Manhattan distance and additional weights for similar example pairs. The curr_chunk discusses the use of normalization scale factors and empirical loss for minimizing KL divergence in embedding distributions. It also mentions the importance of including L KL for regularization in binary embeddings. The algorithm defines DISPLAYFORM0 with terms like Manhattan distance and additional weights for similar example pairs. The curr_chunk discusses the use of the beta distribution for regularization in embedding vectors towards binary values. This helps in maintaining semantic distances when quantized. The focus is on measuring retrieval performance accurately without relying solely on mAP scores. The curr_chunk focuses on measuring retrieval performance by considering semantic hierarchical relations using mAHP and hamming distance of binary codes. Testing is done on CIFAR-100 and ILSVRC 2012 datasets with Resnet architectures and hash code lengths. Results are compared with previous methods in TAB2 and TAB3. The curr_chunk discusses the improvement in hierarchical precision scores using quantized embeddings for retrieval. Results show a drop in precision after binarization without KL loss, but less severe with KL loss. The approach combines loss functions for code binarization and distance matching, achieving state-of-the-art results for semantic hierarchy based image retrieval on CIFAR and ImageNet datasets."
}