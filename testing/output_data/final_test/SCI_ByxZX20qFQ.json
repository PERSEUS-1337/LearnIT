{
    "title": "ByxZX20qFQ",
    "content": "We introduce adaptive input representations for neural language modeling, extending the adaptive softmax to input representations of variable capacity. Our systematic comparison of input and output layer factorization choices for a self-attentional architecture shows that models with adaptive embeddings train more than twice as fast as character input CNN models with fewer parameters. Achieving 18.7 perplexity on the WikiText-103 benchmark and 23.02 perplexity on the Billion Word benchmark, our approach significantly outperforms previous results. Recently, advancements in neural methods for statistical machine translation have been made using LSTMs, gated convolutional networks, and self-attentional networks. Different choices for modeling units include full words, characters, sub-words, and output vocabulary structuring to reduce computational burden. Hierarchical softmax and adaptive softmax are techniques used to simplify normalization and improve output word embeddings. Adaptive input embeddings extend the adaptive softmax to input word representations, assigning more capacity to frequent words and reducing overfitting to rare words. They reduce the number of parameters in input and output layers by 23% while achieving higher accuracy. When tied with an adaptive softmax in the output, the total parameter reduction is 61%. Experiments compare models based on word inputs, character inputs, and sub-word units using a self-attention architecture. Adaptive input embeddings extend the adaptive softmax to input word representations, assigning more capacity to frequent words and reducing overfitting to rare words. They reduce the number of parameters in input and output layers by 23% while achieving higher accuracy. Experiments compare models based on word inputs, character inputs, and sub-word units using a self-attention architecture. Models with adaptive word representations can outperform character-based models while training more than twice as fast, achieving a perplexity of 18.7 on the WIKITEXT-103 benchmark. Adaptive word representations, inspired by the adaptive softmax work, offer a GPU-friendly way to construct a hierarchical softmax. This approach performs competitively compared to a full softmax, with faster speed and lower memory usage. The modified adaptive softmax used by BID18 allows for weight sharing with input embeddings, leading to improved performance. Variable-sized input embeddings are shown to outperform fixed-sized embeddings, enabling weight sharing with an adaptive softmax output layer. Weight sharing with an adaptive softmax output layer is compared for word-based and character-based input vocabularies, as well as a sub-word factorization. The adaptive softmax leverages the Zipfian distribution of word types in natural language to improve output probability computation. Clusters are defined to partition the frequency-ordered vocabulary, aiming to reduce the number of parameters and enhance model capacity. Clusters are created to partition the frequency-ordered vocabulary into head (most frequent words) and tail (least frequent words). Each cluster's capacity is reduced by a factor of k, typically set at 4. Linear projections are added to map embeddings of each cluster to dimension d for easy use by the subsequent model. The adaptive input embedding layer partitions words into clusters, performs separate lookups, and projects to dimension d. In the context of partitioning vocabulary into clusters and mapping embeddings to dimension d, weight sharing in the adaptive softmax can reduce parameters and improve performance. Sharing parameters for words and projections is straightforward, except for the head cluster which requires additional embeddings not shared with the input. Performance declined when a head projection was added to the adaptive softmax. Performance decreased when a head projection was added to the adaptive softmax in the output, regardless of sharing. Different architectural choices were made compared to BID34, including using only a decoder network and applying layer normalization before self-attention and FFN blocks. Sinusoidal position embeddings were added to the input layer, with N=16 blocks each containing multihead self-attention and feed-forward modules. We use sub-blocks with a residual connection BID11 and apply dropout rates for effective training. Hyperparameters are kept consistent for models trained on the same dataset. Experiments are conducted on BILLION WORD and WIKITEXT-103 benchmarks with large word token and vocabulary sizes. The dataset for training data BID17 consists of shuffled Wikipedia articles with context carrying across sentences. For BILLION WORD, individual sentences are batched due to the lack of document structure. Training data is partitioned into blocks of 512 tokens, with evaluation requiring complete sentences totaling up to 512 tokens per block. The number of tokens per GPU is limited to a maximum threshold B, with examples of similar length added until the threshold is reached. When training on multiple GPUs, each GPU processes B tokens using the same model parameters, increasing the effective batch size. For BILLION WORD models, B = 2048 is used and typically trained on 32 GPUs, resulting in an effective batch size of 65K tokens. The smaller vocabulary of enables increasing B to 4096. The effective batch size is 65K tokens for training on 8 GPUs. Large batch training is beneficial for the dataset, with gradient updates accumulated over two batches. Embeddings sizes vary, with 512 for WIKITEXT-103 and 256 for BILLION WORD. Character inputs are modeled using convolution, max pooling, highway layers, and projection. Character embeddings have size 1024 for BPE inputs and outputs. Character inputs are processed using convolution, max pooling, highway layers, and projection. Different filter sizes are used, with character embeddings of size 128. Highway layers vary between datasets, with one for WIKITEXT-103 and two for BILLION WORD. Start and end of word markers are not included. Unknown tokens are present in both inputs and outputs. Adaptive input representations and adaptive softmax are utilized, with embeddings of size 1024 for the head and reduced sizes for subsequent clusters. Adaptive softmax output layers are used for training models with large vocabularies. The curr_chunk discusses the use of byte-pair encoding (BPE) for sub-word models in different benchmarks, resulting in vocabulary sizes of 33,337 tokens and 32,347 tokens. The evaluation is based on word-level perplexity, utilizing Nesterov's accelerated gradient method with specific parameters for learning rate adjustment. The curr_chunk discusses training models on BILLION WORD and WIKITEXT-103 datasets with specific learning rate adjustments and cycle details. Experiments were conducted on DGX-1 machines with 8 NVIDIA V100 GPUs using 16-bit floating point precision. Additionally, larger setups were considered with increased batch sizes and model dimensions. The curr_chunk presents the performance of models trained on BILLION WORD dataset, with the adaptive input model outperforming previous results with fewer parameters. A large model achieves a new state of the art perplexity of 24.14, while a very large model achieves 23.02 perplexity. The Char-CNN model performs slightly worse than the adaptive input model. Additionally, results show adaptive inputs achieving 18.7 perplexity with a different training data partitioning approach. The curr_chunk discusses the partitioning of training data into blocks of 3072 tokens for evaluation, using word units with embeddings of size 1024. Different models are considered, including adaptive softmax and characters as input. Results show that adaptive input representations with tied layers achieve the highest accuracy. The curr_chunk compares different models for language modeling, showing that input representations with tied layers achieve the highest accuracy. ADP-T outperforms BPE models in accuracy and speed, while CNN is slower despite having a fast adaptive softmax. Sub-word units train quickly and perform better than fixed word embeddings. ASM improves over SM and speeds up training. ADP-T is faster than ADP due to fewer parameters, training more than twice as fast as CNN with higher accuracy. The comparison of different language modeling models shows that ADP-T trains faster than CNN with higher accuracy. Regularization is crucial for WIKITEXT-103 models, while BILLION WORD models benefit from increased capacity. Tying weights improves performance on rare words, while fixed size word embeddings struggle with them due to underfitting on common words. The comparison of different language modeling models reveals that ADP-T outperforms CNN in training speed and accuracy. Weight sharing is beneficial for BILLION WORD models, with CNN excelling on rare words but being surpassed by ADP in other scenarios. Context size is crucial for training block size, as shown in TAB7. Increasing the training block size from 512 to 3072 improves perplexity by 1.2 without an inference context window. Adding context size at inference time further reduces perplexity by 0.6 for the largest training block size. Adaptive softmax benefits from additional regularization for rare words, outperforming standard softmax on fixed size output word embeddings. Adaptive softmax benefits from additional regularization for rare words, outperforming standard softmax on fixed size output word embeddings. Varying the size of input word embeddings can improve accuracy while reducing model parameters, especially when sharing parameters with an adaptive softmax. Different input and output layer factorizations were compared, including word inputs, character inputs, and sub-word units. Our experiments with adaptive input embeddings show faster training and higher accuracy compared to character input CNNs. We achieved new state-of-the-art results on BILLION WORD. Future work will apply variable-sized input embeddings to other tasks. Ablation results in TAB10 show the benefits of reducing the capacity of fixed-size word input embeddings on WIKITEXT-103. Different settings of the SM and SM-T models were also explored. Additionally, various band sizes for adaptive input word embeddings were tested in TAB11."
}