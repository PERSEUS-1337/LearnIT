{
    "title": "rJgFtkhEtr",
    "content": "Neural architecture for amortized approximate Bayesian inference over latent random permutations of two sets of objects by approximating permanents of matrices of pairwise probabilities. Each sampled permutation includes a probability estimate, not available in MCMC approaches. Illustrated on 2D points and MNIST images. In this work, an amortized approach is proposed for posterior inference in generative models with discrete latent variables representing random permutations. Unlike variational methods, the focus is on fast posterior inference without learning a generative model. This approach has been explored in various contexts such as Bayesian networks and sequential Monte Carlo. Our method is inspired by techniques in Bayesian networks, sequential Monte Carlo, probabilistic programming, neural decoding, and particle tracking. It involves using neural networks to express posteriors as multinomial distributions with fixed-dimensional representations. After training, we can obtain approximate posterior samples for any new observations, allowing for the computation of approximate expectations. The method involves using neural networks to express posteriors as multinomial distributions with fixed-dimensional representations, allowing for approximate posterior sampling for new observations and computation of approximate expectations. The generative model considers a uniform distribution over permutations, with the goal of sampling the posterior of variables using decomposition and factorization techniques. The method utilizes neural networks to approximate posterior distributions as multinomial distributions, enabling sampling for new observations and expectation computation. It involves permutation symmetries captured by introducing functions, leading to an approximation for the function R. Our proposed Neural Permutation Process (NPP) uses neural networks to approximate pairwise densities and learn parameters \u03b8 through stochastic gradient descent. By minimizing the expected KL divergence, we can train a neural network to capture ambiguities in observations accurately. The Neural Permutation Process (NPP) utilizes neural networks to approximate pairwise densities and learn parameters \u03b8 through stochastic gradient descent, capturing ambiguities in observations accurately. The approach introduces a new function to handle additional symmetry in noisy pairs, showing higher likelihood with componentwise multiplication. This encoding method has not been previously studied and shows promise for posterior inference over latent permutations in simple datasets. Further exploration is planned for more complex generative models with latent permutations. The curves display mean training negative log-likelihood/iteration in the MNIST example, with different approaches for handling latent permutations being explored."
}