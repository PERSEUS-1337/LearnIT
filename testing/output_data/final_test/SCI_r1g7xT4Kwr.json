{
    "title": "r1g7xT4Kwr",
    "content": "We propose a hierarchical policy learning method for indoor navigation, involving two agents - a Planner and an Executor. The Planner suggests sub-goals to the Executor, who provides feedback to the Planner for the next sub-goal. This setup improves sample efficiency and overcomes challenges in navigating complex environments with sparse rewards. Our approach for indoor navigation involves hierarchical policy learning with a Planner and an Executor, improving sample efficiency in complex environments. Research in navigation has seen a resurgence with learning-based approaches, emphasizing the importance of high-level understanding for performing tasks in real-world environments. Learning-based approaches have advantages over classical methods in complex environments with limited sensor data. However, there is a need for long-term planning with sparse reward signals, leading to limitations in navigation towards goals. Combining hierarchical reinforcement learning with imitation learning has shown promising results by leveraging expert trajectories and policy sketches. In this work, hierarchical control is studied for indoor navigation using two agents - a Planner and an Executor. The Planner proposes sub-goals to the Executor, who achieves them at a low level. Communication between the agents is two-way, aiding in decision-making for the next sub-goal. Our PLEX framework utilizes a hierarchical policy approach where a Planner proposes sub-goals for an Executor to act on. The Planner receives a top-down view with target location and summary from the Executor, while the Executor receives visual sensory data and a sub-goal from the Planner. PLEX addresses sample inefficiency in deep reinforcement learning by sharing information between agents and offering a hierarchical reinforcement learning approach. Our method benefits from improved sample efficiency by distributing the time horizon between the Planner and Executor in a hierarchical reinforcement learning approach. This approach mitigates problems in long-horizon planning and scales deep reinforcement learning in complex real-world environments. In hierarchical reinforcement learning, various approaches have been proposed to improve learning efficiency and exploration in different environments. These approaches include the use of predefined options, sub-goals with semantic meaning, and expert trajectories embedded in the environment. The goal is to reduce the cost of exploration and improve performance in tasks with sparse rewards. In embodied agent learning, an agent is placed in an environment to complete tasks like question answering, point goal navigation, and roaming. This method allows the Planner to propose sub-goals related to the environment without external annotations for supervision. Various simulation environments have been designed to train embodied agents on tasks like point goal navigation, roaming, and scene exploration. These environments aim to provide a realistic indoor simulation for training agents, allowing them to scale the sample hungry nature of the problem. Savva et al. (2019) trained an agent using the Proximal Policy Optimisation algorithm, augmenting it with memory. The recent work by Fang et al. (2019) proposed a memory component using the transformer network to address the limitations of LSTM memory units. This approach mitigates the linear growth of memory usage but increases computational complexity. In contrast, the framework discussed here has a shorter Executor's rollout, demonstrating a different approach to addressing these issues. The hierarchical approach discussed involves a Planner and an Executor policy, each treated as independent agents with respective policies. The framework includes setting sub-goals closer to the agent's current state, reducing the required exploration. The PLEX system consists of a Planner and Executor agent. The Executor's task is to move towards a target location provided by the Planner, while the Planner generates sub-goals to reach the end goal. The environment's state includes an RGB-D observation, an egocentric map, and a point-goal vector. The environment's state includes an RGB-D observation, an egocentric map, and a point-goal vector. The Executor receives a sub-goal from the Planner and returns an Executor Latent Information (ELI) vector summarizing its rollout. The Planner uses the egocentric map and ELI vector, creating a two-way communication channel between the agents. The rewards for the Executor are sampled using the sub-goal provided by the Planner. The Executor policy provides an Executor Latent Information (ELI) vector summarizing its rollout experience to the Planner, creating a cooperative feedback loop between them. The Planner modifies goals and rewards for the Executor by generating planned sub-goals based on the ELI vector, facilitating communication and feedback between the two agents. The Planner receives feedback from the Executor through an ELI vector, optimizing its sub-goals. Directly affecting the Executor is avoided to prevent redundancy in the Planner. The approach focuses on indoor PointGoal navigation tasks. In a realistic indoor setting, the agent must navigate around obstacles to reach the target in a different room. The environment provides RGB-D sensory information and a coarse egocentric top-down map. The map has a resolution of 0.5m per pixel, aiding in the agent's navigation. The Planner receives a 32x32 egocentric top-down map, point-goal vector, and Executor Latent Information (ELI). It outputs binary category distribution for continue or stop operations, distance and direction sub-goal distributions. The Executor gets RGB-D sensory information, Planner's sub-goal, and emits a four category distribution for actions. The Planner uses a perception model with a CNN for extracting embeddings from the egocentric map. It consists of 2 convolutional layers with filter sizes of 4 \u00d7 4 and 3 \u00d7 3. The output is concatenated with the ELI vector and transferred to a fully connected linear layer. The Executor's perception model has 3 convolution layers with filter sizes of 8 \u00d7 8, 4 \u00d7 4, and 3 \u00d7 3. The output depends on the model variant, either 512 or 128. The LSTM and SMT memory models have different configurations, with the LSTM using a GRU formulation and the SMT employing an 8 multi-head attention mechanism. The output of either model is a 128-value action embedding, which also serves as the ELI vector. The Planner and Executor policies are trained using the Proximal Policy Optimisation algorithm with specific parameters. For our experiments, we use the Gibson dataset and perform the PointGoal navigation task. The environment accepts 4 actions: [Forward, Turn left. The environment for the PointGoal navigation task comprises 1k tasks with 4 actions available. The reward structure includes a reward of 10 for success, a penalty for obstacle collision, and a penalty for distance from the target. A long rollout horizon negatively impacts exploration efficiency, especially in complex environments with obstacles and more actions. To determine the appropriate rollout length for the Executor in a given environment, a short experiment was conducted analyzing a random exploration policy. 500 starting locations were randomly chosen from training scenes, with different initial starting points for recurring scenes. The agent can take 3 actions: Forward, Turn Left, Turn Right. The geodesic distance between starting and end positions of the policy, as well as the normalized geodesic distance per number of actions taken, were inspected as the number of random actions increased (N = [2, 25]). This normalized distance indicates exploration efficiency. The exploration efficiency of the random policy declines as the rollout horizon increases. Setting N EX too high can hinder exploration. A value of N EX = 10 strikes a balance between exploring the environment and maintaining efficiency. The approach is compared to baseline methods PPO LSTM and PPO SMT. For comparison, two variants of the PLEX framework, PLEX LSTM and PLEX SMT, are evaluated alongside baseline methods PPO LSTM and PPO SMT. Evaluation metrics include reward, success ratio, and defining success as the agent issuing a stop action when the geodesic shortest path distance to the goal is less than 0.2m. Shortest Path Length (SPL) is not used as an evaluation metric. Our method shows steady learning rates compared to baseline methods, which exhibit a slowdown at around 15M environment steps. We focus on reward and success rates as key indicators of performance, penalizing undesirable behaviors like obstacle collision. Quantitative results in Figs. 4a and 4b, along with Table 1, display the final rewards and success ratios for each method. The ELI vector provides additional information to the Planner, aiding in generating sub-goals by summarizing previous observations from the Executor's LSTM component. This integration results in a compressed representation that helps achieve similar results with fewer environment steps compared to other methods. The Executor's LSTM component provides a summary of previous observations, adapting a hierarchical framework to a policy-based approach. Our PLEX LSTM variant outperforms baseline models significantly, achieving higher results with fewer environment steps. Our PLEX framework outperforms baseline methods by enabling communication from the Executor back to the Planner, resulting in a more consistent learning pace and improved performance. Our PLEX framework shows robustness and consistent learning pace compared to baseline methods. A structured approach is necessary for tasks with goals further away, highlighting a potential drawback of LSTM-like units for long time horizon tasks. The test set is divided into subsets based on goal distance, with results indicating limitations of PPO LSTM. Our proposed hierarchical reinforcement learning approach outperforms the PPO LSTM baseline in PointGoal navigation tasks by achieving higher scores in reaching far goals. The method involves two agents, an Executor and a Planner, working together through a communication channel to solve the task efficiently. The PPO LSTM requires more training iterations to navigate to further targets, highlighting the effectiveness of our cooperative learning strategy. The hierarchical approach combines Executor Latent Information with sub-goals from the Planner, showing improved sampling efficiency in tasks compared to baseline methods. Extracting 3D points from agent trajectories and camera poses, a global set is iteratively updated to create an egocentric map for task completion. The hierarchical approach combines Executor Latent Information with sub-goals from the Planner for improved sampling efficiency in tasks. By extracting 3D points from agent trajectories and camera poses, an egocentric map is created for task completion. The egocentric map is derived by projecting all points in M to a plane, cropping and aligning a box using the agent's facing direction. The moment generating function of S N is given, and the probability of the Bernoulli sum being greater than \u03b1N is bounded using the Chernoff bound. The proof involves analytically solving the bound to obtain \u03b8 * = log \u03b1 1\u2212\u03b1, where \u03b1 > 1 2. Substituting \u03b8 * into the equation and defining \u03b2 concludes the proof. Algorithm 1 presents a pseudo algorithm of Executor Policy Gradient; Actor-Critic variant. The Planner goal and reward are initialized, state, action, and reward buffers are set up. A policy is sampled for action and latent information, the environment is sampled for state and terminal, and rewards are accumulated. The policy is updated using buffers, similar to PPO. The final state, Planner reward, terminal, and latent information are returned."
}