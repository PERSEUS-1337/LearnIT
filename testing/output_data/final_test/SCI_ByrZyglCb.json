{
    "title": "ByrZyglCb",
    "content": "Deep networks are vulnerable to universal perturbations, small image-agnostic changes that lead to misclassification. This paper analyzes classifier robustness to these perturbations and links it to decision boundary geometry. The curvature of deep networks plays a key role in their robustness, with shared positively curved directions enabling small universal perturbations. This analysis offers a geometric method for computing universal perturbations. State-of-the-art classifiers are highly vulnerable to universal perturbations, small image-agnostic changes that cause misclassification. These perturbations exploit deep networks' properties to misclassify natural images with minimal changes. The vulnerability of state-of-the-art classifiers to universal perturbations is explained by the geometry of decision boundaries in deep neural networks. Shared directions with high curvature along the decision boundary play a key role in this vulnerability. Two decision boundary models are introduced to analyze this vulnerability in terms of geometric properties. The locally flat and curved models analyze the vulnerability of classifiers to universal perturbations based on the geometry of decision boundaries in deep neural networks. The flat model assumes a linear approximation locally, while the curved model considers curvature information. The contributions include showing vulnerability to universal directions when normals to decision boundaries are correlated, and the role of curvature in robustness to perturbations. The developed theoretical framework analyzes the vulnerability of deep neural networks to universal perturbations by considering the curvature of decision boundaries. State-of-the-art deep nets exhibit shared directions with positively curved decision boundaries, leading to large vulnerability. This framework also provides a geometric method for computing universal perturbations and explains properties observed in robustness to perturbations. The paper focuses on analyzing the robustness of classifiers to universal noise in natural images. Universal perturbations, which are fixed image-agnostic perturbations causing label changes for many images, have been shown to make classifiers vulnerable. These perturbations differ from adversarial perturbations, which are specific to individual images. The paper analyzes the robustness of classifiers to universal noise in natural images by quantifying the distance from x to the decision boundary. The decision boundary corresponds to points in the input space equally likely to be classified as i or j. The paper introduces two models on the decision boundary and evaluates the robustness of classifiers to universal perturbations. It focuses on the vulnerability of deep networks to perturbations, starting with a locally flat decision boundary model. The analysis seeks a universal direction v that can fool all data points under this model. The paper introduces a local decision boundary model to analyze the vulnerability of deep networks to universal perturbations. It focuses on finding a universal vector that can fool half of the data points, assuming balanced classes. The model approximates the decision boundary at a data point x as x + {v : r(x)T v = r(x) 2 2 }, capturing the local geometry of the boundary. The local classification region is defined as r(x)T v \u2264 r(x) 2 2 , with a different label assigned outside this region. The decision boundary model assumes a ball of radius \u03c1 centered at 0, with linear classifiers satisfying this model. It focuses on the boundary of the classifier, not the classification function, allowing for non-linear decision boundaries. An example of a nonlinear decision boundary that fits this model is shown in Fig. 2a. The decision boundary model allows for nonlinear boundaries, as shown in Fig. 2a. Classifiers following this model are not robust to small universal perturbations when normals to the decision boundary belong to a low-dimensional subspace. Theorem 1 states conditions for this lack of robustness. The existence of a universal noise vector v with 2 norm \u2264 \u03c1 is proven in the supplementary material. Classifiers are vulnerable to universal perturbations due to shared geometric properties of the decision boundary near data points. The theorem shows that normal vectors r(x) near data points approximately reside in a low-dimensional subspace S. This results in the existence of universal perturbations of 2 norm of order \u221am, where m < d. Theorem 1 explains the vulnerability of linear classifiers to universal noise, with magnitude proportional to \u221a L - 1. This vulnerability is also observed in deep networks with locally flat decision boundaries. The dimensionality of the subspace near data points in deep nets is larger than the number of classes, leading to high vulnerability to universal perturbations. Second order information of the decision boundary, specifically curvature, plays a crucial role in explaining this vulnerability. A model of the decision boundary leveraging the curvature of nonlinear classifiers is studied to understand the existence of universal perturbations. In this section, the existence of universal perturbations in nonlinear classifiers is studied by analyzing the positively curved decision boundary. A quadratic approximation of the decision boundary is used to capture the curvature, with the Hessian matrix representing the second-order information. The model assumes a local decision boundary bounded by a quadratic form near data points. The existence of universal perturbations in nonlinear classifiers is analyzed by studying the positively curved decision boundary. A quadratic model is used to capture the curvature, with a theorem stating the existence of universal perturbations under certain conditions. The robustness of classifiers to perturbations is quantified in terms of the curvature of the decision boundary. Theorem 2 assumes the decision boundary is locally located inside a disk of radius 1/\u03ba along normal sections. It does not require curvature in all directions but rather in a subspace S where it is positively curved. No assumption is made on the normal vectors. The existence of a subspace S with positive curvature near natural images is assumed, leading to vulnerability of classifiers and a way to find universal perturbations. Random vectors from this subspace are predicted to be universal perturbations. This construction works well for deep networks, validating the assumption of a low dimensional positively curved subspace for decision boundaries. The directions with large positive curvature near a set of training points correspond to eigenvectors of the average Hessian matrix. The subspace S c is constructed from the first m eigenvectors of H and satisfies the assumption of Theorem 2. This subspace helps determine if the decision boundary is positively curved in most directions. The average curvature across random directions in subspace Sc for points on the decision boundary is computed to determine if it is positively curved. Empirical evidence suggests the existence of a subspace Sc where the decision boundary is positively curved for most data points in the validation set. The assumption of Theorem 2 is satisfied, showing a strong link between universal perturbation vulnerability and positive curvature of the decision boundary. Visualizations of normal sections of the decision boundary in the direction of universal perturbations reveal highly positively curved directions. The decision boundary seeks common directions in the embedding space with positive curvature, leading to small universal perturbations. Highly curved directions are rare, as most principal curvatures are close to zero. Theorem 2 proposes a novel method to generate universal perturbations, with random perturbations predicted to be effective. Fig. 7 (b) illustrates the fooling rate of universal perturbations. Fig. 7 (b) shows the fooling rate of universal perturbations generated using a novel method, sampled uniformly from the unit sphere in subspace Sc and scaled to a fixed norm. The LeNet network used has two convolutional layers and three fully connected layers, achieving 78.4% accuracy on the test set. For ImageNet, the ResNet-18 architecture was trained on CIFAR-10 with stochastic gradient descent, achieving high accuracy on the test set. The curr_chunk discusses the effectiveness of universal perturbations generated using random perturbations in different subspaces, achieving a high fooling rate of 85% on a validation set with minimal training points. This method compares favorably to a previous algorithm while requiring significantly fewer training points. The effectiveness of universal perturbations generated using random perturbations in different subspaces is discussed. The high fooling rate achieved confirms that curvature is the key factor controlling classifier robustness. Using curvature information significantly outperforms neglecting it, as shown in experiments on different architectures. The proposed approach generates universal perturbations through random sampling from a subspace, achieving high fooling rates. Using curvature information significantly outperforms neglecting it, leading to misclassification of most unseen images in the validation set. The fooling rate of this perturbation on ImageNet is 67.2%. The fooling rate of the perturbation on ImageNet is 67.2%, higher than using S f only (38%) but lower than the original algorithm (85.4%). The small number of samples due to computational restrictions partially explains this gap. Universal perturbations are random vectors in a subspace S c, leading to high diversity. Sampling random directions from S c for CIFAR-10 shows diverse universal perturbations. Retraining with perturbed images does not significantly improve network robustness due to the variety of directions in S c. In this paper, the analysis focused on the robustness of classifiers to universal perturbations under two decision boundary models: locally flat and curved. The study revealed that classifiers with curved decision boundaries are not robust to universal perturbations, emphasizing the importance of curvature information in assessing deep nets' robustness. The subspace S c, where universal perturbations exist, is likely shared across datapoints and different networks to some extent. The study focused on the robustness of classifiers to universal perturbations, particularly those with curved decision boundaries. It was found that classifiers with curved decision boundaries are not robust to universal perturbations. The existence of universal perturbations was empirically verified for deep nets, providing a geometric approach for computing such perturbations. Other authors have explored robustness properties of SVM classifiers and new approaches for constructing robust classifiers. The paper explores the robustness of classifiers to universal perturbations, specifically focusing on image-agnostic perturbations that can deceive every image in a dataset. It differs from previous works by studying universal perturbations rather than image-specific ones. The study challenges explanations based on deep network outputs being approximated by linear functions, as this assumption is often violated even in small networks. The study focuses on the vulnerability of classifiers to universal perturbations, showing a link between robustness and the curvature of the decision boundary. It challenges the assumption that deep network outputs can be approximated by linear functions, and proposes a constructive approach to compute universal perturbations based on curvature analysis. The study analyzes the robustness of classifiers to universal perturbations by examining the geometry of the decision boundary. It emphasizes the importance of suppressing shared positive directions to enhance robustness, potentially through regularization of the objective function. The analysis utilizes a lemma to prove results related to projection matrices and random vectors drawn from the unit sphere. The study focuses on the robustness of classifiers to universal perturbations by analyzing the geometry of the decision boundary. It highlights the need to suppress shared positive directions for improved robustness, possibly through regularization. The analysis uses a lemma to prove results regarding projection matrices and random vectors from the unit sphere. The lemma is applied to show the existence of a universal noise vector with a certain norm. The study analyzes the geometry of decision boundaries to assess classifier robustness to universal perturbations. A lemma is used to prove the existence of a universal noise vector with a specific norm. The analysis emphasizes the importance of suppressing shared positive directions for enhanced robustness. The proposed approach generates universal perturbations with high fooling rates on VGG-16 and ResNet-18 architectures. Transferability of perturbations is explained by sharing subspace Sc across different networks. The curr_chunk discusses optimization for machine learning in a book chapter by Suvrit Sra, Sebastian Nowozin, and Stephen J Wright published in 2012."
}