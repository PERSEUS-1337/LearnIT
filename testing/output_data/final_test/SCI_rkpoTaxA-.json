{
    "title": "rkpoTaxA-",
    "content": "This paper explores self-ensembling for visual domain adaptation, derived from the mean teacher variant of temporal ensembling. Modifications were made for challenging domain adaptation scenarios, achieving state-of-the-art results in various benchmarks, including winning the VISDA-2017 challenge. The approach outperforms prior art in small image benchmarks and can achieve accuracy close to supervised training. Semi-supervised learning aims to reduce the need for ground truth labels in training datasets. Unsupervised domain adaptation transfers knowledge from a labeled source dataset to an unlabeled target dataset. Domain adaptation allows training models using labeled synthetic data and unlabeled real data, as seen in the VisDA-17 challenge. Our winning solution, inspired by recent work, utilizes self-ensembling with random image augmentations for domain adaptation. We have further enhanced this approach with confidence thresholding and class balancing to achieve state-of-the-art results in various benchmarks, even approaching those of traditional supervised learning. Our method is versatile and can be applied to different network architectures. Our paper discusses self-ensembling based semi-supervised methods and domain adaptation techniques. Recent work in this area has shown promising results in semi-supervised learning scenarios. Specifically, we focus on the self-ensembling approaches of BID13 and BID28, which form the basis of our technique. BID13 and BID28 present self-ensembling approaches for semi-supervised learning. BID13's models include the \u03a0-model and temporal model, achieving state-of-the-art results in SVHN and CIFAR-10 benchmarks. BID28 improves on BID13's temporal model by using an exponential moving average of network weights. Their approach involves two networks, a student and a teacher network, trained using gradient descent. The student network is trained using gradient descent, while the teacher network's weights are the exponential moving average of the student's. The unsupervised loss for training the student is the mean square difference between student and teacher predictions. Deep learning methods for domain adaptation are focused on, with a mention of auto-encoders reconstructing input samples in a latent space. BID5's auto-encoder model reconstructs samples from both source and target domains, with a classifier predicting labels from domain invariant features in the latent representation. BID0 and BID4 propose models for domain adaptation by separating latent representations into domain-specific and domain-invariant features. BID4's bifurcated classifier includes a gradient reversal layer to confuse the domain classifier. The feature extraction layers aim to confuse the domain classifier by extracting domain invariant features. An alternative implementation minimizes label cross-entropy loss in the feature and label classification layers, while maximizing it in the domain classification layers. Tzeng et al. (2017) use separate feature extraction sub-networks for source and domain samples, training the model in two distinct stages. BID21 utilizes tri-training with three classifier sub-networks, where the first two are trained on source domain samples and a weight similarity penalty encourages them to learn different weights. Pseudo-labels generated for target domain samples are used to train the final classifier. Generative Adversarial Networks (GANs) are used in domain adaptation to generate samples that match the distribution of a dataset. Some models use GANs to learn domain invariant embeddings, while others transform samples from one domain to another. BID1 proposed a GAN that adapts synthetic images to match real images, training a classifier alongside the GAN to predict annotations for both source and adapted samples. The BID25 model uses a refiner network and discriminator with a limited receptive field to make local changes while preserving ground truth annotations. BID20 introduces a bi-directional GAN that transforms samples between source and target domains, encouraging label class consistency. This work is similar to CycleGAN by Zhu et al. (2017) and aims to minimize the difference between feature distributions in domain adaptation models. Deep CORAL Sun & Saenko (2016) minimises the difference between feature covariance matrices for source and target domains. Tzeng et al. (2014) and BID16 minimize the Maximum Mean Discrepancy metric. Adaptive batch normalization, a variant of batch normalization, learns separate statistics for the source and target domains in a two-pass process, achieving state-of-the-art results. The model builds upon the mean teacher semi-supervised learning model of BID28. The mean teacher model of BID28 is a semi-supervised learning model that involves training a student network using gradient descent and a teacher network with exponential moving average weights. Different parameters are used for dropout, noise, and image translation in the student and teacher pathways. The training process includes both labeled and unlabeled samples, with a loss function that combines supervised and unsupervised components. The unsupervised loss in the mean teacher model penalizes differences in class predictions between student and teacher networks. A timedependent weighting is applied to prevent the network from getting stuck in a poor solution. The loss function combines cross-entropy loss for labeled samples and self-ensembling loss for unlabeled samples. The mean teacher model applies cross-entropy loss to labeled source samples and unsupervised self-ensembling loss to target samples. The self-ensembling loss measures the difference between predictions of student and teacher networks. The model is designed for unsupervised domain adaptation problems with separate source and target paths. The model described achieves state of the art results in small image benchmarks, with additional modifications needed for certain benchmarks. Replacing the Gaussian ramp-up factor with confidence thresholding stabilized training in challenging domain adaptation scenarios. The model achieves state of the art results in small image benchmarks, with modifications for certain benchmarks. Confidence thresholding stabilizes training in challenging domain adaptation scenarios by filtering out low confidence predictions. This approach improves the signal-to-noise ratio and helps the student learn correct labels from the teacher network. The success of the approach is attributed to a higher signal-to-noise ratio. Confidence thresholding achieves state-of-the-art results in various benchmarks, recommending it as a replacement for time-dependent Gaussian ramp-up. Three data augmentation schemes were explored, including minimal, standard, and affine schemes. The use of translations, horizontal flips, and affine transformations has a significant impact on benchmark performance in various experiments involving digit and traffic sign recognition datasets. However, it can impair performance when used with photographic datasets, as observed in the STL \u2192 CIFAR-10 experiment. The challenging MNIST \u2192 SVHN benchmark remains undefeated due to training instabilities. The error rate on the SVHN test set decreases at first, then rises before training completes. The rise in error rate correlated with predictions evolving towards most samples being predicted as belonging to the '1' class. This was caused by class imbalance in the SVHN dataset, leading to the network settling in a degenerate local minimum. A class balance loss term was introduced to address this issue. To address class imbalance in the SVHN dataset, a class balance loss term was introduced. This loss penalizes predictions with large class imbalances by computing the mean class probabilities and comparing them to a uniform probability vector using binary cross entropy. The strength of the class balance loss is balanced with the self-ensembling loss by adjusting it based on the confidence threshold mask. This approach is similar to the entropy maximisation loss in the IMSAT clustering model. The implementation of BID10; IMSAT in PyTorch encourages uniform cluster sizes and unambiguous cluster assignments. Results are available at http://github.com/Britefury/self-ensemble-visual-domain-adapt. Training details and network architectures are provided in the appendices. In domain adaptation experiments, various architectures and augmentation parameters were used. Training sets of small image datasets were used for training, while test sets were used for reporting scores. Significant improvements were achieved over prior art in adaptation directions, nearly matching supervised learning performance. Data augmentation helped the 'train on source' baseline surpass previous domain adaptation methods. Strong performance was observed in the CIFAR-10 \u2194 STL adaptation. The study achieved strong performance in the CIFAR-10 \u2192 STL adaptation, with the 'train on source' baseline outperforming a network trained on the STL target domain due to the small size of the training set. Self-ensembling results surpassed both baseline performance and the theoretical maximum, providing further evidence of its effectiveness. The Syn-Digits dataset, designed for domain adaptation with SVHN, showed improved results, reducing error rates significantly. Similarly, the Syn-Signs dataset targeting GTSRB also outperformed competing approaches. Our approach slightly surpassed supervised learning upper bounds in both cases. Our approach significantly outpaces other techniques in adapting from SVHN to MNIST, achieving close to supervised learning accuracy. Adapting from MNIST to SVHN required additional work, including class balancing loss and data augmentation. Additional augmentation such as random intensity flips and scales resulted in a 37% accuracy score. The hyper-parameters for data augmentation included random intensity flips, scales, and offsets. These modifications significantly improved performance, surpassing prior art and approaching supervised classifier accuracy. Applying these augmentations to the MNIST dataset yielded good results, with a small improvement seen when applied to the SVHN dataset. This augmentation scheme elevated the performance of the 'train on source' baseline. The task involves a 12-class domain adaptation problem with training, validation, and test sets. The objective is to predict the class of real images using labeled computer-generated images. The network is based on a pretrained ResNet-152 BID9 model with modifications to the classification layer. The competition involved a 12-class domain adaptation problem using a pretrained ResNet-152 BID9 model. Results from original submissions and newer results with data augmentation are presented. Test time augmentation was applied to achieve competition results. Our domain adaptation algorithm achieved state-of-the-art results in various benchmarks, almost matching supervised learning accuracy on digit recognition datasets like MNIST and SVHN. The approach is flexible for different network architectures and is based on self-ensembling methods for label propagation. Our algorithm, based on label propagation, achieved state-of-the-art results in domain adaptation. In experiments like MNIST \u2192 SVHN, intensity augmentation was used to align dataset distributions for effective label predictions. Pre-trained networks were also utilized, as seen in the VisDA-17 challenge. Effective domain adaptation involves aligning source and target dataset distributions and refining their correspondence, with self-ensembling being a suitable method. Additional data preparation was required for experiments involving datasets described in TAB3 to match resolution and format. In TAB3, additional data preparation was needed to align resolution and format of input samples for classification targets. USPS images were upscaled to match MNIST, STL images were downscaled to match CIFAR-10, 'frog' and 'monkey' classes were removed, resulting in a 9-class problem. GTSRB images were scaled to match Syn-Signs, and MNIST was aligned with SVHN. The MNIST images were adjusted to 32 \u00d7 32 resolution and converted to RGB to match SVHN. Networks were trained for 300 epochs using Adam BID12 with a learning rate of 0.001. Mini-batches of 256 samples were used, except in specific experiments where 128 samples were used. The self-ensembling loss and class balancing loss were weighted accordingly. Teacher network weights were updated using an exponential moving average formula. In experiments with small target datasets, one epoch was defined as a pass over the larger source dataset. Early stopping was driven by the proportion of samples passing a confidence threshold. The final score was based on the target test set performance at the epoch with the highest confidence threshold pass rate. Training for C VISDA-17 used 160 \u00d7 160 images, a batch size of 56, self-ensembling weight of 10, confidence threshold of 0.9, and class balancing weight of 0.01 with the Adam gradient descent algorithm. The Adam BID12 gradient descent algorithm was used with a learning rate of 10^-5 for the final two layers and 10 for the pre-trained layers. Data augmentation included scaling images to 176 pixels, random cropping, horizontal flipping, and competition data augmentation with intensity scaling, rotations, and desaturation. The curr_chunk describes color manipulation techniques in an image processing model, including desaturation and rotations in color space. It also outlines the architecture of a neural network model for image classification."
}