{
    "title": "HJWLfGWRb",
    "content": "A capsule network consists of layers of capsules, each representing different properties of an entity. Capsules vote for the pose matrix of capsules in the layer above using trainable transformation matrices. These votes are weighted by assignment coefficients updated iteratively with the Expectation-Maximization algorithm. Transformation matrices are trained discriminatively through backpropagation. Convolutional neural networks are based on the idea of sharing knowledge across all locations in an image by tying the weights of feature detectors. In contrast, capsule networks extend this concept to include knowledge about part-whole relationships in familiar shapes. Capsules reduce test errors by 45% on the smallNORB benchmark compared to state-of-the-art methods and show greater resistance to white box adversarial attacks. Capsules aim to utilize the underlying linearity in the pose matrix to handle viewpoint variations and improve segmentation decisions. They use high-dimensional coincidence filtering to detect familiar objects by finding agreement between votes for the pose matrix from different parts. This approach helps in assigning parts to wholes by identifying tight clusters of high-dimensional votes that agree amidst irrelevant votes. One way to solve the problem of assigning parts to wholes is through a fast iterative process called \"routing-by-agreement\" that updates the probability of part assignment based on proximity to other parts. This segmentation principle uses familiar shapes for segmentation rather than low-level cues like color or velocity. Capsules differ from standard neural nets as they compare multiple incoming pose predictions for activation. Capsules in a neural network use a complex non-linearity to convert activation probabilities and poses into the next layer. Each capsule has a pose matrix and activation probability, similar to activities in a standard neural net. Capsule networks consist of multiple layers of capsules denoted as \u2126 L, with trainable connections between capsules in different layers. In capsule networks, trainable transformation matrices and biases are used to cast votes between capsules in different layers. A non-linear routing procedure, similar to the Expectation-Maximization algorithm, adjusts the poses and activations of capsules in each layer iteratively. This routing-by-agreement process is detailed in an appendix, explaining its relation to fitting a mixture of Gaussians. In capsule networks, transformation matrices and biases are used for voting between capsules in different layers. A routing procedure, akin to the EM algorithm, adjusts poses and activations iteratively. The process relates to fitting a mixture of Gaussians. When deciding to activate a higher-level capsule, a fixed cost is incurred for describing lower-level capsule poses. The cost in capsule networks involves paying fixed costs for activating higher-level capsules and additional costs for describing discrepancies between lower-level means and predicted values. The cost is computed using the negative log probability density of a data point's vote under a Gaussian distribution fitted by the assigned higher-level capsule. This approach is simpler but incorrect, as explained in the appendix. The cost in capsule networks involves paying fixed costs for activating higher-level capsules and additional costs for describing discrepancies between lower-level means and predicted values. The incremental cost of explaining a data point using an active capsule with an axis-aligned covariance matrix is calculated by summing the cost of explaining each dimension of the vote. This is determined by the negative log probability density under a Gaussian model, with variance and mean values for each dimension. The activation function of capsules in a layer is defined by a fixed schedule for \u03bb as a hyper-parameter. The EM algorithm is used to finalize pose parameters and activations in layer L + 1 after they have been finalized in layer L. The non-linearity in capsule layers is implemented through EM Routing, which is a form of cluster finding algorithm. The EM Routing algorithm returns activation and pose of capsules in layer L + 1 based on activations and votes from layer L. The model architecture includes a 5x5 convolutional layer with 32 channels and capsule layers starting with primary capsules. The primary capsules' activations are determined by applying the sigmoid function. The primary capsules' activations are produced by applying the sigmoid function to weighted sums of lower-layer ReLUs. Two 3x3 convolutional capsule layers follow, each with 32 capsule types and strides of 2 and 1. The final capsule layer maintains information about the location of convolutional capsules by sharing transformation matrices and adding scaled coordinates to the vote matrix. The routing procedure, known as Coordinate Addition, encourages fine position representation in capsule layers. Convolutional capsules send feedback within their receptive field, with instances closer to the border receiving fewer feedbacks. \"Spread loss\" is used to stabilize training sensitivity to initialization and hyper-parameters. The model uses \"spread loss\" to maximize the gap between target class activation and other classes. The smallNORB dataset contains gray-level stereo images of 5 classes of toys. The smallNORB dataset consists of stereo images of 5 toy classes, with 24,300 image pairs in total. The images are downsampled to 48x48 pixels and normalized. During training, random crops and adjustments are made to the images. Testing involves cropping a 32x32 patch from the center, resulting in a 1.8% test error. Averaging class activations over multiple crops reduces the error to 1.4%. The best reported error without using meta data is 2.56%. The study achieved a 2.56% error rate on smallNORB without using meta data by adding stereo pairs of input images and applying affine distortions. They also tested their model on NORB, achieving a 2.6% error rate. Their CNN model with two convolutional layers and a fully connected layer with dropout performed well on generalization to novel viewpoints. The baseline CNN achieved a 5.2% test error rate on smallNORB with 4.2M parameters. The capsule network with 68K trainable parameters achieved a 2.2% test error rate, outperforming the prior state-of-the-art. EM routing adjusts vote assignments and capsule means to find tight clusters in the votes. The histograms display the distribution of vote distances to the mean of each class capsule during routing iterations. Initially, votes are evenly distributed among 5 final layer capsules. In subsequent iterations, most votes are assigned to detected clusters, while others receive scattered votes. Capsule activations can be viewed as mixing proportions in a mixture of Gaussians. The routing procedure in capsule networks adjusts mixing proportions in a mixture of Gaussians, with histograms showing vote distances to class capsules. The model correctly routes votes in examples like the truck and human, but may confuse objects like the plane with a car in rare cases. The test error rate on smallNORB increases to 4.5% due to the routing iterations. Tab. 1 summarizes the effects of routing iterations, loss type, and usage. The effects of routing iterations, loss type, and the use of matrices for poses are summarized in Tab. 1. The capsules architecture achieves low test error rates on MNIST and Cifar10. Generalization tests with limited viewpoints show promising results for the capsule model. The capsule model outperforms the baseline CNN on all viewpoints. To address confounding factors, training was stopped when the capsule model matched the CNN's performance on familiar viewpoints. Results show a 30% decrease in test error rate on novel viewpoints. Neural networks are vulnerable to adversarial examples, which can drastically reduce accuracy in image classification tasks. The capsule model outperforms the baseline CNN on all viewpoints, showing a 30% decrease in test error rate on novel viewpoints. Adversarial attacks using FGSM can drastically decrease accuracy in image classification tasks. The model's robustness was tested by generating adversarial images from the test set, with results showing significantly less vulnerability to attacks. Our model shows significantly less vulnerability to both general and targeted FGSM adversarial attacks compared to traditional convolutional models. Additionally, our model remains robust even against more sophisticated attacks like the Basic Iterative Method. The capsule model's robustness to black box attacks was tested using adversarial examples generated with a CNN. It did not outperform a different CNN in this task. Different approaches to improving neural networks' ability to handle viewpoint variations include achieving viewpoint invariance and viewpoint equivariance. Spatial Transformer Networks aim for viewpoint invariance by altering CNN sampling based on affine transformations. De Brabandere et al. extend this concept by adapting filters during inference for each locality in the feature map. The approach in the curr_chunk focuses on generating different filters for each locality in the feature map to capture covariance effectively. Unlike traditional CNNs, a capsule is activated only if transformed poses match, leading to models with fewer parameters. This method extends translational equivariance to include rotational equivariance, as seen in Harmonic Networks. The recent approach in Harmonic Networks achieves rotation equivariant feature maps using circular harmonic filters and complex numbers to represent orientation. This method is more parameter-efficient than data augmentation or duplicating feature maps. Our approach focuses on general viewpoint equivariance rather than just affine 2D rotations. Symmetry networks use iterative Lucas-Kanade optimization to find poses supported by low-level features. BID19 proposes DetNet, a feature detection mechanism equivariant to affine transformations. DetNet detects points in images under different viewpoints, potentially useful for implementing the de-rendering first-stage in capsule layers. Our routing algorithm acts as an attention mechanism, similar to BID10 and BID22's use of attention mechanisms for improving decoder performance and matching query sequences to input sequences. Our algorithm improves translation using attention in the opposite direction, focusing on higher-level capsules. Unlike previous systems, we do not require an external transformation matrix. Our method overcomes deficiencies in segmenting overlapping digits by using pose vector length to represent entity probability. The proposed capsule system uses the length of the pose vector to represent entity presence probability. It also utilizes the cosine of the angle between pose vectors to measure their agreement, and employs a vector of length n to represent a pose. This system includes a logistic unit for entity presence and a 4x4 pose matrix. The proposed capsule system uses a 4x4 pose matrix to represent entity pose and introduces iterative routing between capsule layers based on the EM algorithm. This system achieves better accuracy on the smallNORB dataset compared to CNNs, reducing errors by 45%. It is also more robust to white box adversarial attacks. The capsules model is planned to be implemented on larger datasets like ImageNet. Dynamic routing occurs between adjacent capsule layers. Dynamic routing is performed between two adjacent layers of capsules in the proposed capsule system. The routing process resembles fitting a mixture of Gaussians using EM, with higher-level capsules acting as Gaussians and lower-level capsules as datapoints. The dynamic routing procedure is derived from modifications to the EM algorithm, alternating between E-step and M-step to assign probabilities to each datapoint for each Gaussian. The dynamic routing process in the proposed capsule system involves fitting a mixture of Gaussians using EM. Higher-level capsules act as Gaussians, and lower-level capsules act as datapoints. The M-step adjusts each Gaussian to maximize the sum of weighted log probabilities, while the E-step minimizes the free energy by adjusting assignment probabilities. The dynamic routing process in the proposed capsule system involves fitting a mixture of Gaussians using EM. The free energy is minimized by adjusting assignment probabilities, which is done in the E-step. The best trade-off is to make the assignment probabilities proportional to exp(\u2212E), known as the Boltzmann distribution in physics or the posterior distribution in statistics. The free energy is an objective function for both E-step and M-step in the routing procedure. The dynamic routing process in the proposed capsule system involves fitting a mixture of Gaussians using EM. The free energy is minimized by adjusting assignment probabilities in the routing procedure. The objective function minimizes Eq. 4, which includes the MDL cost and negative entropy of activations. The dynamic routing process in the proposed capsule system involves fitting a mixture of Gaussians using EM. Each Gaussian in the higher-layer sees a dataset transformed by different matrices, resulting in varying views for different Gaussians. The proposed capsule system fits a mixture of Gaussians using EM, with each Gaussian having a different view of the data. To prevent collapse issues, transformation matrices are learned discriminatively in an outer loop, while dynamic routing modifies means, variances, and probabilities of datapoints assigned to Gaussians. Different determinants of transformation matrices can lead to a subtle version of the collapse problem. In a capsule system, datapoints in a subset are transformed into clusters in higher-level capsule spaces j and k. The determinant of transformation matrices affects model quality, with larger determinants indicating a better model. Dilution of probability density occurs when mapping points between capsule spaces. Learning transformation matrices discriminatively mitigates this issue. When dynamic routing maximizes the probability of transformed datapoints, it does not maximize the probability of untransformed points. To avoid the determinant issue, taking the mean in pose space of a higher-level capsule and mapping it back into the pose space of lower-level capsules using inverse transformation matrices is suggested. The mean in a higher-level pose space will map to different points in the pose spaces of different lower-level capsules due to the predictions made for different parts of the whole. By measuring misfits in the higher-level pose space, we avoid matrix inversions and the need to multiply by inverses in each iteration of dynamic routing. This allows for multiple iterations of dynamic routing at the same computational cost as one forward propagation through transformation matrices. In a mixture of transforming Gaussians, Gaussians differ in the transformation matrices they use, with each Gaussian having an activation parameter for being \"switched on\" for the current dataset. These activation parameters are not mixing proportions and are set based on the probability of being switched on. To determine the activation probability of a higher-level capsule, the description lengths of coding poses are compared, with the difference put through a logistic function. The function minimizes free energy based on the energy differences of the alternatives, which are also used for fitting Gaussians and computing assignment probabilities. The lower-level capsules have activities and assignment probabilities as probabilities, not just 1 or 0. The product of these probabilities is used as a multiplier on the description length of each lower-level mean. Supplementary figures show smallNORB images at different viewpoints with histograms independently log scaled. The distance range considered is 60 with a large number of bins."
}