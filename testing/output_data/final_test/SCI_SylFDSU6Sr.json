{
    "title": "SylFDSU6Sr",
    "content": "A disentangled representation of a data set aims to capture underlying factors. Using Euclidean space for latent variable models may not capture certain geometrical structures, like periodic angles in images. The NeurIPS2019 Disentanglement Challenge featured a Diffusion Variational Autoencoder with a hyperspherical latent space to address this issue. The model incorporates a modified version of the Evidence Lower Bound for better encoding capacity. Variational Autoencoders (VAEs) aim to estimate generative models using latent variables. A disentangled representation is achieved when these variables capture independent underlying factors with semantic meaning. VAEs maximize log-likelihood through variational inference using neural networks to approximate the posterior distribution. Training involves maximizing the Evidence Lower Bound for each data point. The training of the VAE involves maximizing the Evidence Lower Bound (ELBO) for each data point by adjusting neural network weights. To achieve disentanglement of latent variables, a parameter \u03b2 is used to control the encoding capacity of the posterior distribution. Capacity annealing is implemented by fixing \u03b2 before training and linearly increasing a certain value C each epoch. This approach combines ideas from previous studies to optimize the training objective. The data sets for the NeurIPS2019 Disentanglement challenge have a geometrical/topological structure that traditional Euclidean latent variables cannot capture, known as manifold mismatch. These datasets, like Cars3D, have at least one underlying factor with a periodic structure, such as the azimuthal angle of rotation of a car. Diffusion Variational Autoencoders \u2206VAE by P\u00e9rez Rey et al. offer a method to implement closed manifolds like hyperspheres for latent spaces. Diffusion VAEs propose using a \u2206VAE with a high dimensional hyperspherical latent space for better representations of periodic latent variables. The model includes elements like a hyperspherical latent space embedded in Euclidean space, a uniform prior over the hypersphere, and a posterior distribution parameterized by the heat equation solutions. The curr_chunk discusses the use of a family of solutions to the heat equation over a hypersphere, a decoder distribution from normal distributions, and neural networks for parameter calculations. It also mentions key procedures during training, such as the reparameterization trick and the calculation of the Kullback-Leibler divergence. The reparameterization trick proposed by BID4 approximates the first term of the ELBO by sampling from a hypersphere using a random walk method. Parameters for the posterior distribution are calculated using neural networks, and the Kullback-Leibler divergence is approximated between the prior and posterior distributions. The divergence between the prior and posterior distributions is approximated using a formula by P\u00e9rez BID6, with hyperparameter values chosen based on implementations from BID3, BID5, and Diffusion VAE P\u00e9rez Rey et al. (2019). Exact values are detailed in Appendix A."
}