{
    "title": "Bke-6pVKvB",
    "content": "Machine learning algorithms are vulnerable to poisoning attacks where an adversary injects malicious points in the training dataset to degrade performance. A novel generative model is introduced to craft systematic poisoning attacks against machine learning classifiers, generating adversarial training examples that look like genuine data points but reduce classifier accuracy. This approach includes a Generative Adversarial Net with a generator, discriminator, and target classifier to model detectability constraints in realistic attacks. Our experimental evaluation demonstrates the effectiveness of poisoning attacks on machine learning classifiers, including deep networks. Learning algorithms are vulnerable to data poisoning, where malicious points injected during training can compromise system performance. Data poisoning poses a significant security threat in applications relying on large datasets. In applications relying on large datasets, data poisoning poses a security threat. Adversaries can manipulate sensor measurements in IoT environments to evade detection. Optimal poisoning attack strategies have been proposed against machine learning algorithms in the research literature. Solving bi-level optimization problems for generating poisoning points is challenging, especially for large datasets or deep networks. Without detectability constraints, the generated poisoning points can be easily filtered out. Realistic attackers would aim to remain undetected, increasing the complexity of the attack. Detectability constraints can be modeled to address this issue, but they further complicate the attack strategy. In this paper, a novel poisoning attack strategy against machine learning classifiers using Generative Adversarial Nets (GANs) is proposed. The approach involves crafting poisoning points systematically to maximize the error of the target classifier while minimizing detectability by the discriminator. The scheme, pGAN, consists of a generator, discriminator, and target classifier components. The generator generates poisoning points, the discriminator distinguishes them from genuine data, and the classifier minimizes loss on a training dataset containing poisoning points. The pGAN model allows for the systematic generation of poisoning points to degrade the performance of machine learning systems during training. It enables the creation of adversarial examples similar to genuine data at scale, particularly useful for large training datasets or complex attack strategies. The model also includes a mechanism to control the detectability of the generated poisoning points by maximizing a combination of losses for the discriminator and classifier. The pGAN model allows for systematic generation of poisoning points to compromise machine learning classifiers. It introduces a trade-off between attack effectiveness and detectability, enabling testing at different risk levels. Experimental evaluation shows pGAN can bypass various defense mechanisms, including outlier detection and label sanitization. The trade-off analysis reveals the balance needed to avoid detection while impacting the target classifier effectively. Most poisoning attacks can be detected as outliers. Initial attacks were proposed for spam filtering and anomaly detection but did not easily generalize to different algorithms. Biggio et al. introduced a systematic approach for optimal poisoning attacks against SVMs. Xiao et al. proposed a similar approach for feature selection methods. Mei & Zhu developed a general framework for convex classifiers, while Mu\u00f1oz-Gonz\u00e1lez proposed a method for optimal poisoning attacks. Mu\u00f1oz-Gonz\u00e1lez et al. (2017) introduced back-gradient optimization for optimal poisoning attacks against multi-class classifiers, reducing computational complexity. However, previous attacks lacked appropriate detectability constraints, resulting in easily identifiable outliers. Koh et al. (2018) demonstrated the continued feasibility of crafting effective attacks. In a recent study, Shafahi et al. proposed a new method for crafting targeted attacks on deep networks using influence functions. This approach involves creating adversarial training examples by learning small perturbations that alter predictions for specific test points. Yang et al. also demonstrated the possibility of performing targeted attacks without control over labels for poisoning points. The study by et al. (2017) introduced a poisoning attack using generative models with autoencoders to create malicious points. Our model, pGAN, is a GAN-based model with three components to systematically generate adversarial training examples. The attacker's knowledge of the targeted system includes aspects like the learning algorithm, objective function, feature set, and training data. We consider perfect knowledge attacks where the attacker knows everything about the target system. The attacker has detailed knowledge of the target system, including training data, features, loss function, and machine learning model. The attack strategy can also work with limited knowledge, exploiting the transferability property of poisoning attacks. The attacker can manipulate a fraction of the training data and all features to craft poisoning points within the feasible domain. Additionally, the attacker can control the labels of the injected poisoning points. The attacker can manipulate training data and features to craft poisoning points within the feasible domain. The Generator aims to generate poisoning points that increase the error of the target classifier while resembling genuine data points to evade detection. The Generator receives noise as input to define a distribution of poisoning points, while the Discriminator distinguishes between genuine data and generated points. The Classifier represents the attacked algorithm and can be a surrogate model in blackbox attacks. During training, the Classifier is fed honest and poisoning training points. In pGAN, the Classifier (C) is trained with honest and poisoning points, controlled by \u03bb. The objective function for C involves a minimax game with D and is based on a mixture of honest and poisoning points. The poisoning points belong to a subset of class labels Yp, while genuine points are from all classes. The pGAN formulation involves a minimax problem with a parameter \u03b1 controlling the balance between evading detection and impacting classifier performance. Higher \u03b1 values prioritize evasion, while lower values result in more detectable attacks. The pGAN formulation involves a minimax problem with a parameter \u03b1 controlling the balance between evading detection and impacting classifier performance. For \u03b1 = 0, pGAN does not consider any detectability constraint, serving as a suboptimal approximation of optimal attack strategies. Training pGAN involves a coordinated gradient-based strategy to update the parameters of the generator, discriminator, and classifier using mini-batch stochastic gradient descent/ascent. The training algorithm for pGAN involves updating parameters of three blocks with different iterations. The formulation allows for error-generic and error-specific poisoning attacks to increase classifier errors. Limitations on errors can be imposed by the classes available for injecting poisoning points. Surrogate models can be used to generate targeted attacks or specific errors by including only relevant classes or samples. In pGAN, a binary classifier is used to increase classification error for specific classes i and j. Techniques like dropout, batch-normalization, and one-side label smoothing are applied to improve training. The generator is trained to maximize log(D(G(z|Y p ))) to avoid small gradients in early stages. Unlike standard GANs, pGAN focuses on the learned distribution. In pGAN, the learned distribution of poisoning points is different from genuine points, leading to the discriminator's accuracy always being greater than 0.5. The stopping criteria for training pGAN cannot rely on the discriminator's accuracy, but rather on finding a saddle point where objectives are maximized for D and G. The value of \u03bb is crucial in training pGAN, as a small \u03bb can result in the generator focusing more on evading detection by the discriminator, leading to blunt attacks. In pGAN, the value of \u03bb is crucial for successful poisoning attacks. Larger values of \u03bb are preferred to increase attack effectiveness. Experimental analysis shows the distribution of poisoning and genuine data points, with G generating malicious points to evade detection by D. In pGAN, the value of \u03bb is crucial for successful poisoning attacks. Larger values of \u03bb are preferred to increase attack effectiveness. As \u03b1 decreases, the distribution of red points shifts towards the overlap region of green and blue points. For \u03b1 = 0, pGAN focuses on increasing classifier error without interpolating between genuine classes. The pGAN algorithm focuses on increasing classifier error near the decision boundary, rather than interpolating between genuine classes. Experimental evaluations were conducted on MNIST, Fashion-MNIST, and CIFAR-10 datasets using Deep Neural Networks and Convolutional Neural Networks. The effectiveness of pGAN in generating stealthy poisoning attacks was tested using a defense strategy proposed by Paudice et al. The defender uses outlier detectors to pre-filter training data points before training, based on a distance-based anomaly detector. Parameters like k=5 and s=20 are set for the detector. The defender uses outlier detectors with parameters like k=5 and s=20 to pre-filter training data points. The \u03b1-percentile is set to 0.95 to control the fraction of genuine points retained. The attack targets binary classifiers with specific classes chosen for each dataset. Different generators are trained for each \u03b1 value explored, ranging from 0.0 to 0.9. Testing is done using 500 genuine samples. The study used outlier detectors and classifiers to evaluate the effectiveness of an attack on MNIST and FMNIST datasets. The attack involved poisoning points generated by pGAN, with results showing increased error rates for \u03b1 = 0.1 in MNIST. The attack is more effective for \u03b1 = 0.1 in MNIST, increasing the error from 2.5% to over 12% when 40% of the training dataset is compromised. For larger values of \u03b1, the attack's impact is limited. In FMNIST, the attack with \u03b1 = 0.1 produces more effective poisoning data points but has a more limited overall effect compared to MNIST. When \u03b1 = 0, the attack is mitigated by the outlier detector in most cases, with only some effectiveness for larger fractions of poisoning points. The impact of the attack depends on the separation between classes and the classification problem's topology. Poisoning examples generated by pGAN show malicious data points exhibiting features from multiple classes. In some cases, it is difficult to detect these points as outliers, as they closely resemble genuine data points. In other cases, the malicious data points appear as an interpolation of the two classes. The poisoning examples generated by pGAN can be difficult to detect as they resemble genuine data points, showing features from multiple classes. The attack is more effective without detectability constraints, bypassing outlier detectors due to the complexity of the classification tasks. This highlights limitations in existing defenses against data poisoning attacks. The test classification error increases from 12% to 24% after injecting 40% of poisoning points. The increase is more moderate for larger values of \u03b1. Examples generated by pGAN with \u03b1 = 0.3 exhibit characteristics from the two classes: cars and trucks. Outlier detection shows the fraction of data points pre-filtered by the detectors in MNIST dataset as a function of \u03b1. The fraction of rejected genuine data points is 10% and 5% for \u03b1-percentile values of 0.90 and 0.95 respectively. The fraction of rejected malicious points for \u03b1 \u2265 0.1 is smaller than for genuine points for the two detectors. The generated poisoning points are conservative, with less attention to low-density regions in the data distribution of genuine points. Even if the generated points look like a different class, they are close to the distribution of genuine points when targeting a non-linear classifier. Sensitivity analysis of pGAN with \u03bb on MNIST dataset (digits 3 and 5) showed varying effectiveness of the attack. In 10 runs for each generator and \u03bb value, 20% poisoning points were injected for attacks. The classification error on the test dataset decreases slightly with \u03bb, as pGAN focuses more on optimizing the discriminator's objective. Existing poisoning attacks are computationally expensive for the network and dataset sizes used in the experiments. The experiments in Fig. 2 show the impact of network and dataset size on detectability constraints. A fair comparison between label flipping and pGAN attacks was implemented by considering detectability constraints. Training samples from the target class were flipped based on their proximity to the source class, making the malicious points harder to detect. In a comparison with pGAN, the label flipping attack increases false positive and false negative rates, while pGAN specifically aims to increase false positive rates, giving the attacker more control over the errors produced in the classifier. The number of training data points impacts the attack effectiveness, with pGAN generators tested on classifiers with varying training points. The performance difference between poisoned and clean classifiers decreases as the number of training samples increases. Learning algorithms trained with large datasets are still vulnerable to targeted data poisoning attacks. The pGAN can be used to generate more targeted attacks in these scenarios. The pGAN can be used to generate targeted attacks on classifiers, increasing the error of specific misclassifications. By training pGAN with a surrogate classifier for the targeted classes, such as digits 3 and 5, the attack can significantly impact the classification error. The pGAN can generate targeted attacks on classifiers, increasing specific misclassifications. The attack impacts the classification error significantly, with digit 3 detection rate decreasing by 11% and error of digit 3 being classified as a 5 increasing by 12%. pGAN was tested against different defences, including Sever Diakonikolas et al. (2019) meta-algorithm for robust optimization. The study applied Sever and PCA defences to detect and remove outliers that negatively impact the learning algorithm. Sever was used with a parameter value of 0.1, while PCA had a parameter set to 0.05. These defences aim to identify poisoning points by analyzing the data's low-rank subspace. In the experiment, the pGAN attack was tested against a label sanitization technique and three defences (Sever, PCA, and outlier detection) with \u03b1 = 0.1 for MNIST and FMNIST datasets. Results showed that pGAN successfully bypassed all defences, with Sever performing worse than the outlier detector defence for MNIST. Sever performs worse than the outlier detector defence for MNIST 3, but is more effective for FMNIST with fewer poisoning points. PCA-based defence is less effective than the outlier detector in MNIST, while in FMNIST the difference is minimal. Label sanitization fails to defend against the attack in MNIST. The pGAN attack successfully generates poisoning points that evade detection by KNN-based relabelling algorithms, leading to misclassification of genuine points in the target class. Results on FMNIST show similar outcomes. pGAN is effective against state-of-the-art defences and allows for modeling attackers with varying aggressiveness levels. The pGAN attack efficiently generates poisoning points that evade detection by various mitigation strategies, including outlier detection and label sanitization. It allows for studying tradeoffs in the adversarial model and system performance as the fraction of poisoning points increases. The attack bypasses state-of-the-art defences and can model attackers with different levels of aggressiveness. The procedure involves training three components using mini-batch stochastic gradient descent/ascent. Data points are sampled from different distributions for the generator, discriminator, and classifier. Training alternates between the components with different numbers of steps. In practice, the discriminator and classifier are updated more frequently than the generator. The procedure involves training three components using mini-batch stochastic gradient descent/ascent. Data points are sampled from different distributions for the generator, discriminator, and classifier. In a synthetic experiment, training and test data points are sampled from two bivariate Gaussian distributions. A poisoning attack effect on the decision boundary is shown in Fig. 9. The classifier is trained with genuine examples and 20% poisoning points using Stochastic Gradient Descent for 1,000 epochs. No outlier detector is used in this case. In a synthetic experiment, the classifier is trained with genuine examples and 20% poisoning points using Stochastic Gradient Descent for 1,000 epochs with a learning rate of 0.01. The attack effectiveness decreases as the value of \u03bb increases, making the attack blunt. The classifier is stable due to a small number of features and simple problem topology. The poisoning attack's impact is reduced when detectability constraints are considered. The purpose of this synthetic example is to illustrate pGAN's behavior with \u03bb, not to show a highly effective attack scenario. In a synthetic experiment, the classifier is trained with genuine examples and 20% poisoning points using Stochastic Gradient Descent for 1,000 epochs with a learning rate of 0.01. The attack effectiveness decreases as the value of \u03bb increases, making the attack blunt. The classifier is stable due to a small number of features and simple problem topology. The poisoning attack's impact is reduced when detectability constraints are considered. The purpose of this synthetic example is to illustrate pGAN's behavior with \u03bb, not to show a highly effective attack scenario. Here we provide complete details about the settings for the experiments described in the paper, including the characteristics of the datasets used and the parameters for training pGAN for MNIST, FMNIST, and CIFAR. The text describes the use of pGAN to generate samples for different datasets with varying values of \u03b1. The generated examples exhibit characteristics from two classes involved in the attack, with pGAN trying to preserve features from the original poisoning class. Sensitivity analysis of Sever performance to pGAN attack is shown with different values of the parameter controlling the fraction of training points removed. The performance of Sever was tested for different values of \u03b1, showing that the attack is not very sensitive to the parameter for both MNIST and FMNIST datasets. Additionally, the PCA defense was evaluated for various values of \u03b1, with results indicating a degradation in performance as the parameter value increases, especially for FMNIST. Comparing label sanitization and outlier detection defenses, it was found that label sanitization performs poorly against pGAN attacks, leading to a significant increase in average test error for both MNIST and FMNIST datasets."
}