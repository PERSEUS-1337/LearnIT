{
    "title": "HJgY6R4YPH",
    "content": "Domain adaptation involves training a classifier in one domain and using it in another. To address the issue of embedded features not aligning well, labels are propagated using a manifold structure and cycle consistency is enforced to align feature clusters. Results show improved clustering and classification accuracy in various domain adaptation scenarios. Domain adaptation involves training a classifier in a source domain with labeled data and using it in a target domain with only unlabeled data. The goal is to overcome the difference in data distributions between the two domains, known as domain shift, to improve classification accuracy. Domain adaptation addresses the domain shift issue by fine-tuning on the target domain with a small set of labeled data or finding discriminative features that are invariant between domains. Approaches like domain-adversarial neural networks aim to reduce the distance between feature distributions, but there is still room for improvement in ensuring that decision boundaries align with embedded features from the target domain. In this work, a novel domain adaptation method is proposed to align the manifolds of the source and target features in class-level. Label propagation is used to evaluate the relation between manifolds, and cycle consistency is reinforced to align them by drawing features from both domains to converge or diverge based on their proximity. The proposed method utilizes label propagation to exploit manifold information for aligning class-level distributions of source and target domains. It outperforms other baselines by incorporating global and local manifold information, leading to aligned manifolds. The role of cycle consistency in achieving this alignment is theoretically explained. The proposed method utilizes label propagation to align class-level distributions of source and target domains, outperforming other baselines. Cycle consistency plays a key role in achieving this alignment, leading to aligned manifolds and state-of-the-art performance in unsupervised domain adaptation. In unsupervised domain adaptation, techniques like domain discrepancy minimization and class-level alignment are used to enhance model accuracy. Methods include separating shared representation from individual characteristics, using centroids for alignment, and converting target data to source data directly. Metric learning is the direct conversion of target data to source data, aiming to transfer image style while preserving content. It works well on similar pixel-level images but struggles with complex high-level feature mappings. Metric learning involves learning a metric distance to measure data similarity or dissimilarity, improving classifier accuracy by reducing distances between similar data and increasing distances between distinct data. It is beneficial for domain adaptation with limited labeled data. Sener et al. (2016) combined metric learning and unsupervised domain adaptation with cycle consistency enforcement. AssocDA (Haeusser et al., 2017) enforces feature alignment between source and target data. Graph-based learning, related to metric learning, achieves clustering using distance information. Label consistency and propagation improve semi-supervised learning by enforcing label consistency. Adaptive learning of distances between nodes increases accuracy in both semi-supervised and few-shot learning. The algorithm uses label propagation and cycle consistency to learn features from source and target domains, making them indistinguishable and close within the same class. Manifold learning extracts intrinsic structures from data by constructing a graph based on embedded features. The graph is created using Gaussian similarity to determine edge weights between data points. Graph-based methods are sensitive to scale. Graph-based methods are sensitive to the scale parameter \u03c3, with a large \u03c3 creating a uniformly connected graph and a small \u03c3 producing a sparse graph. To adapt \u03c3 to embedded features, it is taken as a trainable variable. Label propagation is a method of manifold regularization that uses a repeated random walk through the graph of features to assign labels to target data. Label propagation is a method of manifold regularization that uses a repeated random walk through the graph of features to assign labels to target data. The process involves transforming the label vector using similarity matrices and normalization operations, with the source data labels remaining unchanged as absorbing nodes. In label propagation, the labels of the target domain are assigned to the propagated labels of the source data through infinite transition. The method enforces cycle consistency to align features and segregate different classes of source and target data. Theorem 1 states that in ideally clustered source data, there exists a nonnegative vector for each class that separates the data into positive elements belonging to that class and zeros for others. The proof is provided in Appendix A. Theorem 1 states that in ideally clustered source data, there exists a nonnegative vector for each class that separates the data into positive elements belonging to that class and zeros for others. The illustration and proof are given in Appendix A. The conclusion implies that if a target data can be predicted as the i-th class through label propagation, then the target data is segregated from the source data in different classes. The use of DANN prevents the target data distribution from being distinct from the source data distribution, ensuring that target features lie around source features. The proposed method aims to improve classification performance by ensuring that each target data belongs to only one cluster, leading to more discriminative shared features. By considering relations within the same domain through label propagation, the method differs from AssocDA which only considers relations across domains. Our method utilizes both T ts and T tt, while AssocDA ignores T tt, which can contain useful information. Our method differs from AssocDA by considering relations within the same domain through label propagation. It utilizes both T ts and T tt, allowing moderate intra-class variance. The introduced cycle consistency enforces alignment of source and target features at the class level. Limited mini-batch size may restrict graph information availability. Limited mini-batch size may restrict graph information availability. Localized GAN (LGAN) is employed to approximate the local manifold of each data and sample a marginally perturbed image along the local manifold from the given data. LGAN focuses on learning and linking patches of local manifolds in its training procedure to impose local manifold consistency of the classifier. The LGAN generator, G L (x, z), generates locally perturbed images along the local manifold. The method aims to learn a clustered feature representation that is consistent across different domains. The approach includes cross-entropy loss, GAN loss, and discriminator output probabilities. The goal is to separate source features effectively. The cycle loss and local loss aid in clustering by enforcing consistency in the feature representation across domains. A toy example is used to demonstrate the effectiveness of the cycle loss compared to AssocDA. The synthetic dataset in 2D space shows source data vertically and target data slightly tilted and translated. The negative gradients of AssocDA loss and cycle loss show the movement of features during iteration. AssocDA attracts target data close to source data with different labels, leading to misclassification. Cycle loss results in correctly aligned manifolds and prevents overfitting by not attracting features too closely. Our method, AssocDA, prevents overfitting by not attracting features too closely at the end of updates. It aligns source and target clusters correctly without the need for dann loss. The performance of our method is demonstrated on two real visual datasets: Digit & Object dataset (SVHN, Synthetic Digits, STL, CIFAR) and ImageCLEF-DA. The features from different domains form similar and tight clusters, achieving the key objective of our method. Our method, AssocDA, outperformed other approaches on Digit & Object datasets by using different network architectures and pretrained models. Results from three models with different loss functions showed superior performance, suggesting that enforcing alignment improves overall accuracy. Our method, AssocDA, outperformed other approaches on ImageCLEF-DA dataset experiments, showing better or comparable performance to baselines such as CAT Deng et al. (2019). The approach utilizes label propagation and cycle consistency to leverage the manifolds of the source and target domains, resulting in well-aligned and clustered features. The proposed method, AssocDA, outperformed other approaches on ImageCLEF-DA dataset experiments by leveraging manifold information beyond one-to-one relations. The method achieved better accuracy without the local loss compared to with it, indicating potential for improved performance with better local manifold approximation. Our method, AssocDA, achieved superior performance on various benchmark datasets by correctly aligning manifolds. The method relies on graph construction, with potential performance improvements through graph pruning or defining a similarity matrix based on underlying geometry. It can also be applied to semi-supervised learning with slight modifications. Future work includes exploring these modifications further. The method AssocDA aligns manifolds by constructing a graph and utilizing the Perron-Frobenius Theorem. It achieves superior performance on benchmark datasets and can be adapted for semi-supervised learning with modifications. The Perron-Frobenius Theorem is used to show that a positive matrix has a real and positive eigenvalue with positive left and right eigenvectors. The modified version of Thm. 1 is offered when source features are perturbed but the assumption y s =\u0177 s holds. The perturbation of source features is represented by T ss, with \u03b4T ss assumed to be small. Eigenvalues and eigenvectors of T j can be approximated using perturbation theory. Using Big-O notation, Eq. 16 from Theorem 1 is still valid. Applying Eq. 19, Eq. 24 holds for the j th block. Applying Eq. 19 to the right-hand side, where i = j, and Eq. 24 holds for the j th block elements of v (0) j. The left-hand side of Eq. 22 can be transformed similarly. Combining Eq. 24 and Eq. 26, Eq. 27 implies that with a small perturbation and accurate target data prediction, transitions from source data of other classes to the target data are negligible. This segregation effect is reinforced as the target data is more strongly classified as the i th class. The coefficients for L cycle and L cycle are scheduled to cluster source features correctly early in training. A weight balance factor is applied to reduce noisy signals from L dann and L cycle. Parameter \u03b3 determines the rate of increase of the balance factor. Throughout experiments, \u03b3 was set to 10. During training, the Adam optimizer with a learning rate of 10^-3 was used, along with exponential moving averaging. The protocol adopted involved using a small set of labeled target domain data as a validation set, with specific sample sizes for different experiments. The batch size for the optimization trajectory was set to 128 for the Digit & Object dataset and 36 for the ImageCLEF-DA dataset due to limited computing resources. Experiments showed little improvement beyond a batch size of 128. The MNIST database consists of hand-written digit images with 10 classes, while MNIST-M blends MNIST digits with natural color patches from the BSDS500 dataset. The MNIST dataset consists of hand-written digit images with 10 classes, while MNIST-M blends digits with natural color patches from the BSDS500 dataset. The colors of the MNIST images were randomly inverted, and the size of the images was upscaled to 28x28. The evaluation protocol of CYCADA is adopted for the experiment. Additionally, the SVHN dataset contains images of house numbers acquired from Google Street View, with images upscaled to 32x32. The SYN DIGITS dataset is similar to SVHN but with untidy backgrounds. CIFAR and STL datasets are 10-class datasets with non-overlapping classes for domain adaptation. Twelve common classes from Caltech-256, ImageNet, and PASCAL VOC2012 are used for visual domain adaptation tasks with hyperparameter search. The study conducted experiments with various hyperparameters and perturbations to the LGAN generator. The importance of setting the scale parameter, \u03c3, was highlighted, with experiments showing that adaptively learning \u03c3 led to better accuracy compared to fixing it to a specific value. The study emphasized the significance of adaptively learning the scale parameter \u03c3, showing better accuracy and stability. Initial \u03c3 values had minimal impact on accuracy, with \u03c3 adapting to transfer tasks. L2 and cross entropy loss were used for cycle consistency, with l1 norm achieving the highest accuracy in Digit dataset adaptation experiments. In experiments, l1 norm achieved the highest accuracy for cycle consistency. The Amazon Reviews dataset was used for domain adaptation experiments, showing our method outperforming DANN, VFAE, and ATT in six out of twelve experiments. Our method outperformed DANN in nine out of twelve settings, with approximately 2.0% higher classification accuracy on average."
}