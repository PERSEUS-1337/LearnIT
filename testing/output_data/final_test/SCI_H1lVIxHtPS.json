{
    "title": "H1lVIxHtPS",
    "content": "Learning communication through deep reinforcement learning has proven effective for solving cooperative multi-agent tasks. A new fully differentiable framework for communication and reasoning has been introduced to address the challenge of determining beneficial information for decision-making. This framework enables agents to solve tasks in partially-observable environments by facilitating explicit reasoning between agents using a memory-based attention network. The model communicates through reasoning steps that decompose intentions into learned representations, allowing for the computation of relevance of communicated information and extraction of information from memories. By selectively interacting with new information, the model effectively learns. The model effectively learns a communication protocol in cooperative multi-agent tasks, improving performance compared to baselines. Communication is crucial for cooperation in multi-agent systems, enabling agents to reason based on information communicated by teammates and develop sophisticated coordination strategies. Building agents that can solve complex cooperative tasks requires understanding how agents learn to communicate effectively. Humans exhibit complex collaboration strategies through communication and reasoning, allowing them to recognize important task information. Significant progress in multiagent deep reinforcement learning has been made in learning effective communication protocols. Effective communication strategies in multiagent deep reinforcement learning involve broadcasting private observations, selective communication using soft-attention networks, and communication through a shared memory channel. These approaches enable agents to share information and learn collectively, although some limitations exist in enriching the actions of all agents. In multiagent deep reinforcement learning, various communication strategies are utilized to enhance collective learning. For instance, Memory Driven Multi-Agent Deep Deterministic Policy Gradient (MD-MADDPG) implements a shared memory state among agents, while Targeted Multi-Agent Communication (TarMAC) uses soft-attention for communication. These approaches aim to improve information sharing and coordination among agents. Relational reinforcement learning (RRL) combines inductive logic programming and reinforcement learning to reason over entities in the environment. Attention, a widely adopted framework in NLP and VQA tasks, generates an attention distribution over entities based on importance for the task. (Vaswani et al., 2017) proposed a communication mechanism to infer the importance of each agent's information without using memory in the communication step. The paradigm of using relations in agent-based reinforcement learning was introduced by (Zambaldi et al., 2018) through multi-headed dot-product attention. In the context of multi-agent cooperation, a method for computing relations between agents is implemented, drawing inspiration from soft-attention and memory-based attention networks. This framework enables memory-based communication that performs attentive reasoning over new information and past memories, enhancing the capacity to learn from past experiences in MADRL architecture. Our architecture in multi-agent deep reinforcement learning (MADRL) emphasizes structured reasoning between non-local entities using soft-attention for computational efficiency and adaptivity. Attention informs agents of important entities, while reasoning extracts relevant shared information from past memories, enabling deliberate decision-making. Our novel architecture in multi-agent deep reinforcement learning enables agents to selectively choose information from past memories based on new information, learned without supervision through task-specific rewards. The framework emphasizes structured reasoning and communication, demonstrating efficacy in solving cooperative multi-agent tasks with varying team sizes and environments. The team of N agents is modeled as a cooperative multi-agent extension of a partially observable Markov decision process. The framework involves state values, control actions, and observations for multiple agents in an environment. Agents select actions through a stochastic policy with parameters, generating rewards and local observations. The objective is to maximize expected return over a time horizon using deterministic policy gradient algorithms. Parameters of the policy are updated to optimize the objective function. DDPG is an off-policy method that uses experience replay buffers to sample system trajectories for stabilizing training. It employs neural networks to approximate the policy and critic, with a target network to reduce learning variance. MADDPG is a multi-agent extension of DDPG that utilizes centralized learning and decentralized execution. The MADDPG framework combines centralized learning with decentralized execution, where each agent's policy is based on its local observation. A centralized critic estimates joint actions, while a communication architecture facilitates interaction between agents using structured reasoning. Memories from previous time-steps are stored separately for each agent to aid in decision-making. The MADDPG framework combines centralized learning with decentralized execution. The model interacts with memories and communications from other agents to produce a memory containing valuable information for the task. Before taking action, each agent performs four operations: Thought Unit, Question Unit, Memory Unit, and Action Unit. The Structured Attentive Reasoning Network (SARNet) consists of a thought unit, question unit, memory unit, and action unit. The thought unit transforms an agent's observations into vector representations, the question unit attends to information from other agents, the memory unit extracts relevant information, and the action unit predicts actions based on memory and observations. The SARNet model includes a thought unit, question unit, memory unit, and action unit. The question unit helps agents determine relevant information for reasoning. The attention mechanism is used to compute the importance of each agent in the environment. The SARNet model incorporates an attention mechanism that allows agents to compute the importance of each communicated information from other agents. This is achieved through a soft attention-based weighted average using query and keys from all agents. The SARNet model uses new scalars for each agent to generate weights specific to each agent, allowing for importance to be assigned to each element in the information vector. The memory unit decomposes new values into relevant information for the current time-step by interacting with the memory from the preceding time-step. The retrieved information is measured in terms of relevance based on the importance of each agent generated in the Question unit. The SARNet model computes direct interactions between new values from agents and its own memory, enabling relative reasoning between new information and previous memory. This process highlights important information from new communications. The model then uses current knowledge to compute a new representation for the final attention stage. Finally, important information is aggregated based on weighting calculated in the Question unit, generating a weighted average of new information gathered from the reasoning process. The SARNet model computes interactions between new information and memory, highlighting important details. A linear transformation prepares the information for the action unit to predict the agent's final action. The actor-critic model learns policy parameters for each agent, with parameter sharing to ease training. The policy network produces actions evaluated by the critic to minimize loss. The critic Q \u00b5 \u03b8 i minimizes the loss function by observing actions in the experience replay buffer. Target Q-value is defined as y, with delayed updates to stabilize training. The goal of the loss function is to minimize the difference between current and target action-state function. Gradient of policy with communication maximizes rewards expectation. Communication architecture evaluated on OpenAI's multi-agent particle environment. In a multi-agent environment, agents cooperate to complete tasks against static goals or compete against non-communicating agents. A communication architecture, SARNet, is compared to other mechanisms like CommNet and TarMAC. Agents need to cooperate to reach landmarks, observing positions of neighbors and landmarks. Rewards are based on proximity to landmarks, with penalties for collisions. Our model outperforms baselines in a cooperative task where agents aim to maximize a shared global reward by minimizing distance to landmarks and collisions. The architecture incorporates a memory of previous communications from other agents, leading to a significant reduction in collisions and lower distance to the landmark compared to TarMAC. The implementation of soft-attention for communication in a cooperative navigation task involving multiple agents. Agents can observe nearby agents and landmarks, with collisions and distance measured at the end of each episode. Predators are rewarded for colliding with prey in a slower-moving team chasing faster agents in the presence of static landmarks. In a cooperative navigation task, predators are rewarded for colliding with prey, while prey are penalized for moving out of the environment. Various training methods are used for predators and prey. Agent dynamics show a focus on information exchange, leading to improved performance compared to baseline methods. Different observation configurations are tested, showing varying levels of agent visibility. In a cooperative navigation task, predators are rewarded for colliding with prey, while prey are penalized for moving out of the environment. Agents can observe 3 predators, 1 prey, and 3 landmarks. The success rate of communicating agents N and adversary M is measured. SARNet and MADDPG agents spread out over all landmarks, achieving a higher mean reward. Visualizing attention predicted by agent i over all agents during the initial steps. The study explores the importance of memory in communication architecture by introducing noise in the memory at each time-step. Despite the corrupted memory, the agent's policy remains robust to adversarial noise, as it learns to infer important information through thorough reasoning in communication. The study introduces a novel communication framework, SARNet, for multi-agent deep RL. It focuses on structured attentive reasoning between agents to enhance coordination skills. By decomposing communication representations into reasoning steps, agents outperform baseline methods. The architecture learns end-to-end and computes task-relevant importance of information from cooperating agents. Despite introducing noise in memory, the agent's policy remains robust, showcasing the benefits of gathering insights from memories and internal representations. The study introduces SARNet, a communication framework for multi-agent deep RL. It focuses on structured attentive reasoning between agents to enhance coordination skills. Policy gradient methods are popular for reinforcement learning tasks. Various variations of PG have been developed, primarily focused on estimating Q values. The REINFORCE algorithm and actor-critic algorithms are used for estimating Q values in reinforcement learning. Hyperparameters include batch synchronous method, replay buffer size, Adam optimizer, learning rate, discount factor, and soft update target network. The hyperparameters for the algorithm include a discount factor of 0.96, soft update target network parameter of 0.001, batch normalization, dropouts in the communication channel, and exploration noise through Orhnstein-Uhlenbeck process. Experimental results are averaged over 3 runs with different random seeds. Baseline policies for TarMAC, CommNet, and MADDPG are instantiated as MLP with specific layer sizes. The critic in the network is a 2-layer network with 1024 and 512 units, while the network size is 128 units. TarMAC and CommNet use 1-stage communication with specific unit sizes for queries, keys, and values. Training parameters are similar to SARNet implementation."
}