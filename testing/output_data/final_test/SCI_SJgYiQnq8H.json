{
    "title": "SJgYiQnq8H",
    "content": "The goal of standard compressive sensing is to estimate an unknown vector from linear measurements under the assumption of sparsity in some basis. Recent research suggests that fewer measurements may be needed if the unknown vector lies near the range of a generative model. Lower bounds on sample complexity are established in this paper using minimax statistical analysis. The results show that $O(k\\log L)$ random Gaussian measurements suffice for accurate recovery with a bounded and $L$-Lipschitz generative model, and $O(kd \\log w)$ measurements are enough for $k$-input ReLU networks with depth $d$ and width $w. The necessary number of measurements for generating group-sparse signals using an $L$-Lipschitz generative model is $\\Omega(k \\log L)$. Additionally, two-layer ReLU networks of high width require $\\Omega(k \\log w)$ measurements, while lower-width deep ReLU networks need $\\Omega(k d)$ measurements. These scaling laws are deemed optimal or near-optimal without further assumptions, building on the well-understood sparse estimation via linear measurements in compressive sensing. The success of deep generative models in various applications has led to a new perspective on compressive sensing, where the sparsity assumption is replaced by the assumption that the signal can be well-modeled by a generative model, typically a deep neural network. This approach has shown impressive performance in experiments, with significant reductions in the number of measurements compared to sparsity-based methods. Theoretical guarantees have been provided for an L-Lipschitz generative model with bounded k-dimensional inputs, showing reliable recovery with m = O(k log L) random Gaussian measurements. Additionally, for a ReLU network generative model from R k to R n with width w and depth d, m = O(kd log w) measurements are sufficient. In this paper, lower bounds on the number of measurements needed for compressive sensing with generative models are established using minimax statistical analysis. The dependencies m = O(k log L) and m = O(kd log w) cannot be improved without further assumptions, based on a reduction to compressive sensing with a group sparsity model. The paper establishes lower bounds on the number of measurements required for compressive sensing with generative models, based on a reduction to compressive sensing with a group sparsity model. It presents a Lipschitz-continuous generative model for generating bounded k-group-sparse vectors and provides information-theoretic lower bounds matching the upper bound in Corollary 1. The generative function G maps input z to output x in k-group-sparse signals. Each block of x is a function of z i, with specific mappings shown in Figure 1. The model divides the interval [-r, r] into intervals for non-zero entries in x. The generative model G follows similar steps to k-sparse recovery. The generative model G maps input z to output x in k-group-sparse signals using neural networks with ReLU activations. A minimax lower bound for k-group-sparse recovery is obtained, with a sample complexity lower bound proved. The model focuses on generative models given by neural networks with ReLU activations. The generative model G uses ReLU-based networks to produce k-group-sparse signals. A lower bound of m = \u2126(k log n) is established, without assumptions on network weights or domain size. A wide and/or deep ReLU network is constructed to generate all (kk 0 )-group-sparse signals with non-zero entries \u00b1\u03be. The construction of the generative model G uses ReLU networks to produce k-group-sparse signals. Theorem 2 establishes a lower bound for compressive sensing with generative models under noise, showing that if n0 \u2265 C0k0, then a suitable choice of \u03be yields certain results. The generative function G can be implemented as a ReLU network with specific parameters, and the sample complexity behaves according to certain conditions. The upper and lower bounds match up to a constant factor or a log n factor in different scenarios. The generative function G is implemented as a ReLU network with specific parameters, and the sample complexity matches up to a constant factor or a log n factor in different scenarios. Lower bounds on sample complexity for compressive sensing with generative models are established by constructing models capable of producing group-sparse signals and applying a minimax lower bound for group-sparse recovery. For bounded Lipschitz-continuous generative models, the sample complexity matches the O(k log L) scaling law. ReLU-based generative models show optimal or near-optimal dependence on both width and depth in the O(kd log w) bound. Future research could explore additional assumptions to further reduce sample complexity."
}