{
    "title": "rylt7mFU8S",
    "content": "Many biological learning systems like the mushroom body, hippocampus, and cerebellum are made up of sparsely connected networks of neurons. The study focuses on the function spaces induced by sparse random features and the ability to learn certain functions. Sparsity offers advantages such as limiting the curse of dimensionality, stability to outlier noise, and scalability. This research aims to promote kernel theories of networks among computational neuroscientists. Kernel function spaces are popular in machine learning for understanding artificial neural networks trained via gradient descent. Computational neuroscientists have not widely applied these tools to describe biological networks' function approximation abilities. The concept of using fixed random weights in neural networks dates back to Rosenblatt's perceptron model. Random features have reappeared in various forms such as radial basis function networks, Gaussian processes, and extreme learning machines. Random feature networks, inspired by kernel methods, have been proposed to improve performance in machine learning. Computational neuroscientists have also studied random networks to understand real brain neurons, which appear to be randomly organized. Sparse connectivity in hidden neurons, receiving input from random subsets, is being explored in this study. The sparse random feature approximation to an additive sum of sines is inspired by the connectivity in feedforward brain networks like the cerebellar cortex, invertebrate mushroom body, and dentate gyrus of the hippocampus. These areas perform pattern separation and associative learning, with the cerebellum important for motor control. The sparsity observed in these areas may be optimized to balance the dimensionality of representation. Sparse connectivity can optimize representation dimensionality while balancing wiring cost. It has been used to compress artificial networks and speed up computation. Sparse random features approximate additive kernels with varying orders of interaction based on hidden neuron in-degree. These features offer generalization advantages in high dimensions, stability under input perturbations, and computational efficiency. The mathematical setting for random features generating kernels is introduced in a single hidden layer neural network context. The neural network has a single hidden layer of size m, receiving input from a layer of size l. The activity in the hidden layer is determined by the weights and biases. Random features networks draw their input-hidden layer weights at random, leading to a reproducing kernel Hilbert space of functions. The kernel defines an associated RKHS for functions in a finite network of width m. The main result discusses the general form of random feature kernels with sparse, independent weights in a neural network with a single hidden layer. The kernel is computed using closed form expressions, where in-degrees can be chosen independently according to a degree distribution. The resulting kernel is obtained by sampling weights from a distribution and considering the neighborhood of each node. The degree distribution determines the probability mass function of hidden node in-degrees. The induced kernel is based on the in-neighborhood of each node. Sparse connectivity leads to a mixture of kernels that depend on combinations of inputs, making it an additive kernel of order d. The degree distribution kernel shows that sparsity results in a mixture of additive kernels of different orders, known as additive GPs. For a regular degree model with d = 1, the neighborhoods are individual indices of the input space. The RKHS for k reg 1 (x, x ) is a direct sum of subspaces, defining a first-order additive model with pairwise interactions. The degree distribution D(d) determines the weight on different degrees of interaction. First-order additive models do not suffer from generalization issues in high dimensions. Stone proved that first-order additive models do not suffer from the curse of dimensionality, with the excess risk not depending on the dimension. Kandasamy and Yu extended this to dth-order additive models, finding a bound on the excess risk for kernels with polynomial or exponential eigenvalue decay rates. Dropout regularization in deep networks may improve generalization by enforcing approximate additivity. The presence of different norms and the sign function in networks can provide stability to outlying coordinates in the input. Sparse features are less affected by noise, offering denoising advantages. Regressors built from these features are stable if the coefficient is small. This stability can also ensure robustness to sparse adversarial attacks. Sparse random features offer significant scaling improvements, especially for additive models with high dimensions. The method requires O(nmd) computations for evaluation, making it faster than dense connectivity methods. For ridge regression, the estimator computation time is reduced to O(nm^2 + nmd) and memory usage to O(nm + md). In small animals like flying insects, space is limited, making sparsity advantageous. Sparse random networks of neurons offer advantages in terms of wiring cost and learning speed in small animals like flying insects. Inspired by biological systems, these networks demonstrate additivity, stability, and scalability, making them powerful function approximators. The theory of dimensionality in neuroscience suggests that learning is easier in additive function spaces. Learning is easier in additive function spaces due to low dimensionality, which may explain few-shot learning in biological systems. The theory of dimensionality in neuroscience, complementing existing theories, measures dimensionality in the space of nonlinear functions using kernel theory. However, the model ignores time, neuromodulatory context, and other factors like sparsity of activity in the cerebellum. Further research is needed to explore how this theory can be applied. The theory of dimensionality in neuroscience measures dimensionality in the space of nonlinear functions using kernel theory. Kandasamy and Yu created a theory of generalization properties of higher-order additive models, supplemented by an empirical study using their SALSA implementation of additive kernel ridge regression. Performance comparison was made with sparse random feature approximation. In Figure 2, sparse random features are compared to SALSA in terms of performance. The training and testing errors of the sparse model are slightly higher than the kernel method, except for the forestfires dataset. The speed of learning for a test function is also studied, with a sparse polynomial plus a linear term used for the function to be learned. Figure 3 shows the test error and selected ridge penalty for different values of d. In Figure 3, the test error and selected ridge penalty for different values of d and n are shown. The model with d = 1 performs best with small data (n < 250) to avoid overfitting. For intermediate data (250 < n < 400), the model with d = 3 is optimal. With large data (n > 400), models with interactions d \u2265 3 perform similarly. The ridge penalty adapts complexity based on data, with more complex models having higher penalties. Sparse random features are stable for spike-and-slab input noise in this linear model example. In a regression problem with corrupted inputs, linear regression fails to achieve a high test score compared to kernel ridge regression. The noise in the inputs is sparse but large, affecting the performance of linear regression. The kernel method and trimming before regression outperform linear regression in this scenario. The sparse feature kernel shows little noise amplification when it is sparse, even for large amplitude. Trimming outliers before linear regression yields the best performance. Sparse random features and their kernels may be useful for dealing with noisy inputs in learning problems. The eigenvalues of the kernel matrix on a dataset of size n = 800 points show that changing noise amplitude does not significantly affect amplification, but denser outliers lead to more amplification. The eigenspace spanned by the largest eigenvalues is crucial for learning problems. Common random features and their kernels with fully-connected weights are discussed, along with the impact of introducing sparsity in input-hidden connections. Translation invariant kernels are also explored. Translation invariant kernels are discussed, with Gaussian weights and Fourier nonlinearity leading to the Gaussian radial basis function kernel. Moment generating function kernels use exponential functions for monotone firing rate curves, with evaluations based on moment generating functions. For multivariate Gaussian weights, the equation becomes more interpretable. The multivariate Gaussian weights w \u223c N (m, \u03a3) can be simplified by setting m = 0 and \u03a3 = \u03c3 \u22122 I, and normalizing the input data. Dot product kernels k(x, x ) = v(x x ) are radial basis functions on the sphere S l\u22121 = {x \u2208 R l : x 2 = 1}, with eigenbasis as spherical harmonics. Arc-cosine kernels, induced by monotone \"neuronal\" nonlinearities, lead to different radial basis functions on the sphere. Standard normal weights w \u223c N (0, I) with threshold polynomial functions + as nonlinearities result in a kernel given by a known function J p (\u03b8) where \u03b8 = arccos. This kernel is also a dot product kernel. The terms x are replaced by x/\u03c3, but this does not affect \u03b8. With p = 0, corresponding to the step function nonlinearity, we have J 0 (\u03b8) = \u03c0 \u2212 \u03b8. The resulting kernel does not depend on x or x. We also consider a shifted version of the step function nonlinearity, the sign function sgn(z), equal to +1 when z > 0, \u22121 when z < 0, and zero when z = 0. The sparsest networks possible have d = 1, leading to first-order additive kernels. Two simple nonlinearities allow for an explicit formula for the additive kernel, related to a robust distance metric. The step function nonlinearity \u0398(\u00b7) leads to the arc-cosine kernel, while the sign nonlinearity results in the normalized Hamming distance kernel. The sparsity induces quantization via the sign function on vectors, and in binary hypercube data, the kernel is the normalized Hamming distance. The sign nonlinearity, with h(\u00b7) = sgn(\u00b7) = 2\u0398(\u00b7) \u2212 1, results in a kernel that differs from the step function. The sign nonlinearity does not exhibit quantization effects and depends on the 1-norm rather than the 0-norm. A basic uniform convergence result is shown for random features using Lipschitz continuous nonlinearities. The random feature expansion approximates the kernel uniformly over X, with rates for Lipschitz nonlinearities similar to those obtained for Fourier features. The only non-Lipschitz functions are the step function and sign nonlinearities, which converge to the kernel in a weaker sense. The result applies to the rectified linear nonlinearity. Theorem 1 guarantees uniform approximation using random features with m = O features, similar to random Fourier features. The limitation is that it only applies to the rectified linear nonlinearity. Theorem 1 guarantees uniform approximation using random features with m = O features, similar to random Fourier features, but only for the rectified linear nonlinearity. Further analysis by Bach, Rudi, and Rosasco, as well as Sun et al., show faster rates for SVMs with optimized features. Techniques by Sutherland, Schneider, Sriperumbudur, and Szabo can improve constants and prove convergence in other L p norms, especially in the sparse case where additional probability space is needed to capture randomness. The degrees are distributed independently according to D, with neighborhoods chosen uniformly among all subsets. Nonzero weights and bias are drawn from a distribution on Rd+1. Weight values do not depend on the neighborhood, but strictly speaking, they do. Expectation is denoted by E over all variables, and E \u00b5|d for a given degree. The degrees are distributed independently according to D, with neighborhoods chosen uniformly among all subsets. Nonzero weights and bias are drawn from a distribution on Rd+1. Weight values do not depend on the neighborhood, but strictly speaking, they do. Expectation is denoted by E over all variables, and E \u00b5|d for a given degree. Corollary 2 discusses kernel approximation with sparse features, assuming compact X, degree distribution D, and finite second moments for conditional distributions. The random feature \u03c6 is defined by \u03c6(x) = h(w x \u2212 b), where w follows the degree distribution model. Conditions on conditional distributions imply conditions in Theorem 1. The distribution D has finite support, leading to differences in sparsity with sparse random features. Scaling the weights to keep E \u00b5|d w 2 constant for all d is beneficial. The number of sparse features needed for error is the same as in the dense case. The random feature expansion only needs to approximate the average of terms, not all individual terms. The proof of Theorem 1 follows a similar approach as Claim 1 in [2] for random Fourier features. Trigonometric functions are bounded and differentiable. Define the direct sum norm on X + as \u03be + = x + x. Our goal is to show that |\u1e21(\u03be)| \u2264 for all \u03be \u2208 X + with high probability. X + can be covered with an -net using at most T balls. Show that |\u1e21(\u03be)| \u2264 for all \u03be \u2208 X + by proving that f i has certain properties. The proof involves showing that |\u1e21(\u03be)| \u2264 with high probability for all \u03be in X + using Hoeffding's inequality and a union bound. This is achieved by demonstrating properties of f i and g i, leading to a probability of failure that can be bounded."
}