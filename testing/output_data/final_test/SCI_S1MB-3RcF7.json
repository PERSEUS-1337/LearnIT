{
    "title": "S1MB-3RcF7",
    "content": "Recent literature has shown promising results in training Generative Adversarial Networks using multiple discriminators instead of the traditional single adversary approach. This work revisits the multiple-discriminator method as a multi-objective optimization problem, comparing it with hypervolume maximization and multiple gradient descent on various datasets. The study suggests that hypervolume maximization offers a better balance between sample quality and other factors. Hypervolume maximization is considered a better compromise between sample quality, diversity, and computational cost compared to previous methods. Generative Adversarial Networks (GANs) offer a new approach to generative modeling, addressing issues such as lack of useful gradients and mode collapse. Research efforts have focused on stabilizing GAN training, with architectures like BEGAN using auto-encoders as discriminators to improve training stability. Recent works in the field of Generative Adversarial Networks (GANs) have proposed various methods to stabilize training and improve sample quality. These methods include modifying discriminator architectures, introducing spectral normalization, and using multiple discriminators. These approaches aim to address issues such as GANs instability and mode collapse, ultimately leading to the generation of high-quality samples with diverse characteristics. In this paper, the authors propose a GAN variation where one generator is trained against multiple discriminators, each seeing a fixed random projection of inputs. They aim to stabilize GAN training by treating the loss signal from each discriminator as an independent objective function and minimizing them simultaneously using multi-objective optimization techniques. They suggest using more efficient alternatives like maximizing the hypervolume of the region between a. The authors propose a GAN variation where one generator is trained against multiple discriminators, each seeing a fixed random projection of inputs. They aim to stabilize GAN training by treating the loss signal from each discriminator as an independent objective function and minimizing them simultaneously using multi-objective optimization techniques. Experiments on MNIST show that hypervolume maximization (HV) presents a good compromise in the computational cost-samples quality trade-off compared to other approaches. Increasing the number of discriminators improves the generator's robustness, sample quality, and diversity. The method described increases the generator's robustness, sample quality, and diversity. Experiments on CIFAR-10 show higher quality generator samples. Image quality and sample diversity improve with more discriminators. Contributions include a new perspective on multiple-discriminator GAN training and a method for training multiple-discriminator GANs called Hypervolume maximization. Section 2 introduces definitions on multiobjective optimization and MGD. Section 3 describes prior relevant literature. Hypervolume maximization is detailed in Section 4, with experiments and results in Section 5. Conclusions and future work directions are drawn in Section 6. Definitions regarding multi-objective optimization literature are provided in this section. Multi-objective optimization is defined as a problem with K objectives, variables space \u2126, and decision vector x. Pareto-dominance is explained for two decision vectors x1 and x2. In multiobjective optimization, Pareto-dominance is defined for decision vectors x1 and x2. A vector x is non-dominated if it is not dominated by any other vector in the objective space. Pareto-optimality refers to a solution that is non-dominated. The Pareto-optimal Set (PS) consists of all Pareto-optimal solutions, while the Pareto front (PF) is the set of objective vectors for Pareto-optimal solutions. Pareto-stationarity is a necessary condition for Pareto-optimality, where F is Pareto-stationary at a point x if certain conditions are met. Multiple Gradient Descent (MGD) was proposed for multi-objective optimization assuming convex, differentiable functions. It finds a common descent direction for all objectives by defining the convex hull of gradients and updating parameters with a learning rate. The Generative Multi-Adversarial Networks (GMAN) proposed by BID9 train the generator against an average of K discriminators to provide more useful gradients during training. The goal is to privilege worse discriminators and improve the generator's performance. Experiments were conducted with different values of \u03b2 and K in training GAN models. Results showed that using the simple average of discriminator's losses yielded the best performance. Neyshabur et al. proposed training GANs with K discriminators, each seeing a different randomly projected version of the input image. Theoretical results suggest that the generator's distribution will converge to the real data distribution with a sufficient number of discriminators. The authors propose training GANs with K discriminators, each seeing a different randomly projected version of the input image. The generator loss is defined as the sum of losses from each discriminator, not exploiting individual discriminator performance. Additionally, hypervolume maximization for multi-objective optimization is discussed. The Lebesgue measure and nadir point in a space covered by Pareto-optimal fronts can be quantified using hypervolume, which is beneficial for multi-objective optimization. A variation of GAN is introduced where the generator solves a multi-objective problem by minimizing losses from K discriminators. Training involves alternate updates between discriminators and the generator. The MGD algorithm is a natural choice for generator updates, but it can be computationally expensive for large neural networks. An alternative scheme for multi-objective optimization is proposed, aiming for more efficient versions of MGD without the need to solve a quadratic program. Maximizing hypervolume yields Pareto-optimal solutions, which is a subset of solutions obtained using MGD. The generator loss is defined as the negative loghypervolume. The generator loss is defined as the negative loghypervolume, and it is optimized by minimizing each component. The gradient of the generator loss with respect to its parameters is computed as a weighted sum of the discriminator losses, with weights based on the inverse distance to the nadir point components. This approach assigns more importance to higher losses in the final optimization. The selection of \u03b7 directly affects the importance assignment of gradients provided by different discriminators in the multi-objective GAN game. The choice of \u03b7 plays a crucial role in the final quality of samples, especially in the single-solution case. However, this effect becomes less relevant as the number of discriminators increases. An adaptive scheme for \u03b7 is proposed to optimize the final quality of samples. An adaptive scheme for \u03b7 is proposed to optimize the final quality of samples in the multi-objective GAN game. The scheme enforces higher importance on discriminators with worse performance as training progresses, ensuring a better approximation between the marginals of real and generated distributions. The text discusses different methods for optimizing GANs with multiple discriminators, focusing on the concept of Hypervolume (HV) maximization. It explains how HV prioritizes discriminators with high loss to maintain overall loss, favoring a balance in loss distribution. Various algorithms, such as average loss minimization and GMAN's weighted average, are described as two-step processes involving consolidating gradients and updating parameters accordingly. The text discusses different methods for optimizing GANs with multiple discriminators, focusing on the concept of Hypervolume (HV) maximization. It compares alternative training methods for GANs, such as average loss minimization, GMAN's weighted average loss, and HV, using MNIST as a testbed. The study evaluates how varying the number of discriminators affects robustness and compares sample quality and coverage among different approaches. Experiments were conducted with CIFAR-10 to assess the impact of varying the number of discriminators on robustness. HV's performance was compared to baseline methods, analyzing sample quality and diversity using the stacked MNIST dataset. Generators trained on various datasets showed samples in the Appendix, using the same architecture and hyperparameters. Adam was used for training with specific parameters, and FID BID16 was employed for evaluation. In experiments with MNIST, a mini-batch size of 64 was used. The Fr\u00e9chet Inception Distance (FID) BID16 was employed for comparison. MGD was utilized, requiring a quadratic program to be solved before parameter updates. The generator and discriminator had three and four fully connected layers, respectively, with LeakyReLU activations. Dropout was used in the discriminator, and a random projection layer reduced MNIST's dimensionality. FID computation was done using a pretrained LeNet. Results over 100 epochs with 8 discriminators are shown in Fig. 2 and Fig. 3. Fig. 2 shows box-plots of FID results from 30 computations over 10000 images. MGD outperforms other methods but is costly for datasets beyond MNIST. Hypervolume maximization performs similarly to MGD without extra cost. Convergence analysis in Fig. 4 shows all methods reaching similar norms, indicating different solutions may vary in sample quality. FID over time is depicted in Figure 22. The sensitivity of HV performance to initialization and \u03b4 choice was analyzed by training models on MNIST with 8, 16, 24 discriminators for 50 epochs. Increasing the number of discriminators resulted in less variation in FID. HV performance was compared to baseline methods using CIFAR-10 dataset, with FID computed using a pretrained ResNet BID15. In experiments with CIFAR-10 dataset, FID was computed using a pretrained ResNet BID15. Different models like DCGAN and WGAN-GP BID14 were included for reference. Increasing the number of discriminators improved performance, with HV outperforming other methods in sample quality. Results showed that performance increased with more discriminators. In experiments with CIFAR-10 dataset, increasing the number of discriminators improved performance, with HV outperforming other methods in sample quality. Models trained against more discriminators converge to smaller values and achieve lower FID. Training with multiple discriminators supports fully parallel training, unlike in WGAN where the discriminator is serially updated. In experiments with the CIFAR-10 dataset, increasing the number of discriminators improved performance, with HV outperforming other methods in sample quality. Serial implementations of discriminators updates with 8 and 16 discriminators were faster than WGAN-GP. The experiments aimed to analyze how the number of discriminators impacts the sample diversity of the corresponding generator when trained using hypervolume maximization on the stacked MNIST dataset. Results for 8, 16, and 24 discriminators were obtained and compared using HV results for 10k and 26k generator images averaged over 10 runs. The number of covered modes and KL divergence between the generated mode distribution and test data are reported in Table 1. In experiments with the CIFAR-10 dataset, increasing the number of discriminators improved performance, with HV outperforming other methods in sample quality. Results for 8, 16, and 24 discriminators were obtained and compared using HV on the stacked MNIST dataset. HV with 16 and 24 discriminators achieved state-of-the-art coverage values, showing that employing multiple discriminators improves generator's coverage. Training details and architectures information are presented in Appendix B. This approach allows trading extra capacity for higher quality and diversity of generated samples. The proposed multi-objective optimization framework for multiple discriminator GANs consistently produced higher quality samples in terms of FID. Increasing the number of discriminators enhanced sample diversity and generator robustness. Future investigation will focus on the penalty term || K k=1 \u03b1 k \u2207l k || to potentially reduce the need for a high number of discriminators. In previous work, the squared Fr\u00e9chet distance between Gaussians was used as a quality metric, computed using Inception V3 for data representation. The use of Inception V3 for data representation in GANs is proposed, with the metric Fr\u00e9chet Inception Distance (FID) used for comparison. Pretrained classifiers' output layers were utilized for FID computation on different datasets. Generator and discriminator architectures are detailed, with batch normalization in all layers. RMSprop was used for training with specific parameters. The study utilized Inception V3 for data representation in GANs, comparing results using Fr\u00e9chet Inception Distance (FID). Different models were trained with specific parameters, achieving the best FID scores on 1000 generated images. The norm of the update direction for the best models showed similar convergence behavior regardless of the method used. The study compared different methods using Inception V3 for data representation in GANs and computed extra scores on 10000 images generated by the best model. Results were reported in Table 5, showing the ratio between given models and DCGAN scores. Inception score and FID were used for evaluation, with FID-VGG and FID-ResNet computed using pretrained models. Minimum FID-ResNet and computational costs for training GANs with 1 and 24 discriminators were compared in TAB9. The computational cost of training GANs with multiple discriminators is higher due to increased FLOPS and memory usage compared to single discriminator settings. However, this additional cost results in improved performance across various well-known approaches. Different architectures were used for single discriminator models (DCGAN) and 24 discriminator models, with all models trained for 150 epochs with a minibatch size of 64. The computational cost of training GANs with multiple discriminators is higher due to increased FLOPS and memory usage compared to single discriminator settings. However, the wall-clock time can be made close to single discriminator cases by implementing training with different discriminators in parallel. Experiments with CIFAR-10 were conducted with an upscaled version of the dataset, and now repeated with the dataset in its original resolution to compare with previous methods. The study repeated experiments from Miyato et al. (2018) for the standard CNN model, removing spectral normalization from discriminators and adding projection input. Results show improved performance in FID and Inception score with multiple discriminators and hypervolume maximization, bringing metrics in line with recent GAN advancements. In this experiment, the proposed multiple discriminators setting was tested for generating higher resolution images using CelebA at 128x128 size. Both generator and discriminator networks had a similar architecture with a convolutional layer of 2048 feature maps added due to the larger image size. Models were trained with 6, 8, and 10 discriminators for 24 epochs, showing that the multiple-discriminators setting can scale to higher resolutions even with a small dataset. In this experiment, batch normalization was removed from all layers, and a stride of 1 was used at the last convolutional layer. The Cats dataset was utilized with 1740 training samples at 256x256 resolution. The model was trained with 24 discriminators and Adam optimizer. Generator samples after 288 training epochs are shown in FIG3. Additionally, experiments with 1 to 6 discriminators on the CelebA dataset for 15 epochs were conducted to illustrate the effect of using an increasing number of random projections in training a GAN. Samples from the generator on the CelebA dataset improve as the number of random projections (and discriminators) increases."
}