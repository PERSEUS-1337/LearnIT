{
    "title": "r1gGpjActQ",
    "content": "Machine translation is a crucial application, with AutoRegressive Translation (ART) models achieving high accuracy but suffering from slow decoding. Non-AutoRegressive Translation (NART) models were introduced to reduce inference time but had lower accuracy. To enhance NART model accuracy, this paper suggests using hints from a well-trained ART model, specifically hints from hidden states and word alignments, to optimize NART models. Experimental results demonstrate that NART models trained with hints outperform previous NART models in translation performance. The NART models achieve better translation performance than previous models on various tasks, with significantly higher BLEU scores for WMT14 En-De and De-En tasks. Despite the attention from the research community and industry, most neural machine translation models estimate conditional probability in an autoregressive manner. Non-AutoRegressive neural machine Translation (NART) models aim to speed up the inference process by using a general encoder-decoder framework. These models generate contextual embeddings and predicted length without the need for sequential token generation like AutoRegressive neural machine Translation (ART) models. The limitation of ART models is the linear inference time with respect to the target sequence length, making it unsuitable for industrial applications. The NART models aim to speed up inference by generating contextual embeddings and predicted length without sequential token generation. However, their accuracy is lower than ART models due to the conditional independence assumption. Various approaches have been proposed to improve decoder input expressiveness and accuracy, such as introducing fertilities from statistical machine translation models and using an iterative refinement process. The NART models aim to speed up inference by generating contextual embeddings and predicted length without sequential token generation. However, to improve translation accuracy, an autoregressive submodule with discrete latent variables is embedded, impacting inference speed. Instead of introducing new submodules for decoder input, the focus is on providing guided signals through better regularization for optimization. This approach is chosen as the encoder input contains all semantic information for translation. The NART model aims to improve translation accuracy by utilizing contextual embeddings and predicted length without sequential token generation. To optimize the model, the focus is on providing guided signals through better regularization rather than introducing new submodules for the decoder input. Leveraging a well-trained ART model can provide rich information for improved training of the NART model. The NART model aims to enhance translation accuracy by using contextual embeddings and predicted length without sequential token generation. It faces challenges in defining and utilizing hints from a teacher model due to differences in architecture. The study investigates the model's poor performance and proposes hints to address the issues. Based on an empirical study, differences between NART and ART models were identified in hidden states and attention distributions. Hints were designed from ART model to improve NART training. Experiments on WMT14 tasks showed significant performance gains, with BLEU scores of 25.20 for En-De and 29.52 for De-En tasks. In the WMT14 De-En task, significant performance gains were achieved with a BLEU score of 29.52. Autoregressive translation involves generating target language words one by one based on conditional probabilities modeled by deep neural networks in an encoder-decoder framework. Design choices in training from ART to NART models are illustrated in Figure 1. The encoder-decoder framework for NART models includes design choices like RNN-based BID1, CNN-based BID6, and self-attention based BID19 approaches. While ART models excel in translation quality, inference time remains a challenge due to autoregressive behavior in decoding. This sequential generation of tokens hinders computational efficiency. To speed up inference, non-autoregressive translation models use encoder-decoder framework with separate module for predicting target sentence length and decoder inputs. Different configurations increase output diversity. Previous works focus on design choices of the prediction function. The design choices of BID7 and BID15 introduce fertilities and define z differently, impacting the expressiveness and computational overhead of the models. While BID7 achieves a 15\u00d7 speedup with a simpler design choice, BID15 only achieves a 5\u00d7 speedup. The hidden state cosine similarities of different models are visualized in the above figures. A hint-based training algorithm is introduced to train the NART model, following the Transformer BID19 design with an additional positional attention layer. Linear combinations of source token embeddings are used to avoid overhead. Details about the model can be found in the appendix. The NART model's translations show incoherent phrases and missing tokens compared to ART models. To address this issue, cosine similarities between decoder hidden states of both models are visualized to understand why NART produces repetitive words and misses relevant translations. The cosine similarities between hidden states in the NART model are visualized to understand why it produces repetitive words and misses relevant translations. The heatmap shows that positions with highly-correlated hidden states are more likely to generate the same word, leading to output repetition. In contrast, the teacher model does not have this issue, with most cosine similarities between hidden states being less than 0.5. Additionally, encoder-decoder attentions are visualized to study accurate translation in sampled cases. The attentions of the ART model cover all source tokens, while the NART model misses some tokens, leading to worse translation results. The ART model's intermediate hidden information is used to guide the learning process of the NART model. The ART model's hidden information is utilized to aid the training of the NART model through hint-based training. Layer-to-layer hints are defined from the teacher model to guide the student model's training process. The method is discussed using a paired sentence (x, y) and losses are averaged over all training data in real experiments. The hint-based training framework utilizes hints from hidden states and word alignments to train the NART model using the ART model's hidden states as a guide. The method involves defining layer-to-layer hints from the teacher model to assist the student model's training process. In the hint-based training framework, a penalty function is used to penalize hidden states that are highly similar in the NART model but not in the ART model. This implicit loss helps the student model avoid incoherent translation results by acting towards the teacher at the hidden-state level. Additionally, the attention mechanism significantly improves the performance of the ART models and is considered a crucial building block. The attention mechanism is a crucial building block in improving the performance of models. To address untranslated words and ambiguous attention distributions, word alignment information from the ART model is used to train the NART model. The training loss includes minimizing KL-divergence between encoder-to-decoder attention distributions and a negative log-likelihood loss on bilingual sentence pairs. Hyperparameters control the weight of different loss terms. The curr_chunk discusses the evaluation of machine translation methods on two datasets, IWSLT14 German-to-English and WMT14 English-to-German. Transformer models are pretrained as teacher models, achieving certain BLEU scores. The student model is designed similarly to the teacher models. Hyperparameters for hints based training are also mentioned. The curr_chunk discusses the experimental settings for training the models on WMT14/IWSLT14 tasks, including hyperparameters, optimizer, and GPU usage. The model is implemented based on tensor2tensor and will be released soon. During testing, the length of the target sentence needs to be predicted for each source sentence. The curr_chunk discusses a method to determine target sentence length based on input length, allowing for multiple translation results with different lengths. Various models like FT, LT, and IR are used for evaluation to select the best translation result. The evaluation process involves selecting the translation result with the highest probability using BLEU score BID16. Different values for C are set during inference for different datasets. The teacher model is used to rescore with 9 candidates in total. Average per-sentence decoding latencies are evaluated on NVIDIA TITAN Xp GPU card. The model is compared with LSTM-based, convolution-based, self attention-based ART models, and the fertility-based NART model. The experimental results of different non-autoregressive models, including the fertility-based NART model, deterministic iterative refinement-based NART model, and Latent Transformer, show state-of-the-art performance with significant improvements over previous models. The method outperforms the NART model by 6.54/7.11 BLEU score on WMT En-De and De-En tasks. It achieves comparable results to LSTM-based models on WMT En-De task with a speedup of 30.2 times for outputting a single sentence. The NART model outperforms previous works with significant speedups, showcasing better translation results with hint-based training. The model without hints struggles with translating certain phrases accurately, highlighting the importance of hint-based training for improved target sentences. The NART model outperforms previous works with significant speedups, showcasing better translation results with hint-based training. The model without hints struggles with translating certain phrases accurately, highlighting the importance of hint-based training for improved target sentences. I know that we can do it and as far as I'm concerned, that's something that the world needs now. Every morning, they fly 240 miles to the farm. But there are over 48 hours of video uploaded to YouTube every minute. The NART model with hint-based training improves translation accuracy by decreasing hidden state similarities and making attention distributions clearer. The study evaluates the effectiveness of using hints from well-trained autoregressive translation models to enhance non-autoregressive translation models. Results show improvements in translation accuracy, outperforming previous baselines and achieving comparable accuracy to LSTM-based models with faster inference. Future work will focus on developing new architectures and training methods for non-autoregressive models to match state-of-the-art autoregressive models. Methods for NART models to achieve comparable accuracy as state-of-the-art ART models like Transformer through Knowledge Distillation (KD), where a small student network is trained from a large teacher network. The student network's training objective includes standard classification loss and matching output distributions with the teacher network using KL-divergence. Sentence-level KD has also been beneficial for non-autoregressive translation. However, KD only utilizes the teacher model's outputs, neglecting valuable hidden information. BID17 introduced hint-based training to leverage intermediate representations learned by the teacher model to improve the student model's performance. Dataset specifications include IWSLT14 with 153K/7K/7K sentence pairs and WMT14 with 4.5M parallel sentence pairs. Newstest2014 and Newstest2013 are used as test and validation sets. Model specifications for WMT14 use the base Transformer model architecture from BID19 with a 6-layer design. The default network architecture in BID19 includes a 6-layer encoder and decoder with hidden nodes and embeddings set to 512. For the IWSLT14 dataset, a smaller 5-layer encoder and decoder with hidden states and embeddings set to 256 are used. Hyperparameters are set to make the scales of loss components similar. BLEU scores are used for evaluation, with different settings for different datasets. Our proposed hint-based training algorithm effectively reduces the percentage of repetitive words in translation outputs by more than 20%. Additionally, our model outperforms the baseline model by more than 3 points in terms of BLEU score on long sentences in the IWSLT14 De-En dataset. This improvement is consistent across sentences of all lengths. The text discusses the power of imagination in creating reality and mentions a study that had a profound impact. The curr_chunk discusses athletes and sports featured in an issue, highlighting the importance of ideas that precede our thinking."
}