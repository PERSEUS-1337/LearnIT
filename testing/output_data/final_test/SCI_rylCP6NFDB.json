{
    "title": "rylCP6NFDB",
    "content": "As reinforcement learning advances into new territories, the need for effective algorithms in sparse reward environments becomes crucial. This paper introduces Hindsight Trust Region Policy Optimization (HTRPO), a method that optimizes policies within trust regions while ensuring learning stability. By adapting the TRPO objective function to the distribution of hindsight data from alternative goals and using Monte Carlo with importance sampling to estimate KL-divergence, HTRPO efficiently utilizes interactions in sparse reward conditions. HTRPO optimizes policies within trust regions, using an f-divergence approximation to decrease variance and improve stability during policy updates. Experimental results show faster convergence compared to previous methods, with high data-efficiency in sparse reward environments. Reinforcement Learning, including policy gradient methods, plays a crucial role in addressing real-world problems. Trust region-based reinforcement learning methods like Trust Region Policy Optimization and Proximal Policy Optimization have shown stable and effective performance in various applications. They have been combined with advanced techniques and are being used in skill learning, multi-agent control, and imitation learning. One challenge in reinforcement learning is efficiently training agents in sparse reward environments, where high feedback is only given upon reaching the final goal state. This eliminates the need for complex reward mechanisms. Sparse reward scenarios in reinforcement learning pose a challenge as policy gradient methods struggle to converge in complex environments with limited rewards. Recent works focus on solving this issue through hierarchical reinforcement learning approaches. Recent works have introduced methods like Hindsight Experience Replay (HER) and Hindsight Policy Gradient (HPG) to address sparse reward reinforcement learning problems. HER considers ending states as alternative goals to improve training data quality, while HPG enhances sample efficiency in sparse reward environments. However, HPG's learning curve for policy updates still fluctuates significantly. The learning curve for policy updates in HPG fluctuates due to the high variance of policy gradient methods. Introducing hindsight can exacerbate this variance, causing instability during optimization. The main challenge is balancing on-policy data needed for training with off-policy hindsight experience. Preserving advantages like approximated monotonic convergence in TRPO when trained with hindsight data remains unsolved. In this paper, a methodology called Hindsight Trust Region Policy Optimization (HTRPO) is proposed to address the challenge of balancing on-policy data with off-policy hindsight experience. HTRPO extends the policy optimization procedure within trust region to handle sparse reward environments by estimating objective function and KL divergence using generated hindsight data. An f-divergence is applied to approximate KL divergence, proving to be more efficient and stable. HTRPO shows effectiveness on benchmark tasks. HTRPO is demonstrated to significantly improve performance and sample efficiency in sparse reward scenarios while maintaining learning stability. It can be applied to both simple discrete tasks and continuous environments, with little impact on performance across different hyperparameter settings. The reinforcement learning formulation includes states, actions, policy, initial state distribution, reward function, and discount factor. The paper discusses the reward function, discount factor, and policy optimization in reinforcement learning. It introduces state-action value function, state value function, and advantage function. Trust Region Policy Optimization (TRPO) is proposed as an iterative method for optimizing policy. The TRPO method optimizes policy by maximizing per-iteration policy improvement. It formalizes the optimization problem with parameters \u03b8 and \u03b8, representing old and new policies. HPG extends hindsight to goal-conditioned policy gradient, showing that policy gradient can be computed in expectation. The Hindsight Trust Region Policy Optimization (HTRPO) introduces the idea of hindsight to TRPO, aiming to improve policy performance and sample efficiency for reinforcement learning with sparse rewards. It demonstrates how to redesign the objective function and constraints from TRPO, focusing on the derivation of the HTRPO objective function. The objective function in Hindsight Trust Region Policy Optimization (HTRPO) is derived from the original optimization problem in TRPO. It considers a goal-conditioned objective function and the use of importance sampling to correct trajectory distribution differences when changing the goal. This approach aims to improve policy performance and sample efficiency in reinforcement learning with sparse rewards. The HTRPO objective function is derived from TRPO, focusing on a goal-conditioned form with importance sampling for trajectory distribution correction. Theorem 3.1 presents the hindsight objective function conditioned on a goal, while Theorem 3.2 provides the gradient for optimization. The objective function is solved under a KL divergence expectation constraint, aiming to compute expected return based on advantage with new-goal-conditioned experiences generated from old goals. The gradient of the HTRPO objective function with respect to \u03b8 is computed using techniques to estimate KL-divergence expectation and reduce variance. Hindsight is applied to the constraint function of TRPO, where KL divergence under \u03c1\u03b8(s) is estimated by averaging values conditioned on states collected using the old policy. Monte Carlo estimation is used when only hindsight experience data is available. The KL divergence estimation is transformed to an expectation under occupancy measure \u03c1\u03b8(s, a) using importance sampling to correct the changed distribution. Another f-divergence is used to reduce the variance of estimation for more stable training. The constraint function in KL-divergence can be converted to a logarithmic form, facilitating a more explicit version of the conversion in Appendix B.1. The expectation of KL-divergence over states s \u223c \u03c1\u03b8(s) is discussed, highlighting issues with high variance and possible negativity. Theorems 4.2 and 4.3 provide techniques to reduce variance and a strict proof for variance decrease. Theorem 4.2 approximates the constraint function for policy \u03c0\u03b8(a|s) and \u03c0 \u03b8 (a|s), showing that the expectation of log \u03c0\u03b8(a|s)\u2212 log \u03c0 \u03b8 (a|s) can be estimated by the expectation of its square when \u03b8 and \u03b8 are close. The expectation of an f-divergence is discussed in detail, showing a decrease in variance and the elimination of negative KL-divergence. This estimation method guarantees stable performance and reduces variance effectively. The variance is effectively reduced by introducing hindsight data in the constraint function for HTRPO optimization. The final form of the optimization problem is explicitly demonstrated in the appendix. The validation of HTRPO on sparse reward benchmark tasks includes investigating its effectiveness, component contributions, performance in continuous environments, and sensitivity to network architecture and key parameters. HTRPO is implemented on various reinforcement learning environments such as Bit Flipping, Grid World, and Fetch, with both discrete and continuous versions of experiments conducted. In Fetch experiments, reward mechanisms are modified to sparse regulations. An additional policy entropy bonus is applied for more exploration. Goals are chosen randomly for training and evaluation. Episodes end when time steps are maxed out or the goal state is reached. Performance is evaluated based on 10 learning trials. Network architecture in Bit Flipping and Grid World consists of two hidden layers with 64 hyperbolic tangent units. In Fetch experiments, the network architecture includes two hidden layers with 64 hyperbolic tangent units. The network in discrete and continuous implementations contains two 256-unit hidden layers. HTRPO is compared with baseline algorithms HPG and TRPO. HPG is adapted for continuous environments. Time scaling differs from previous studies. Each component of HTRPO is ablated to assess its impact on performance. In discrete environments, both the official HPG and our HPG implementation are tested, while for continuous environments of Fetch, only our HPG is tested due to lack of support in the official version. Input normalization is applied, and our HPG converges faster than the official HPG in some cases. In comparison to HTRPO, our HPG shows faster convergence and better performance in both discrete and continuous environments. HTRPO demonstrates higher sample efficiency and outperforms HPG, especially in complex control tasks like Fetch Push and Fetch Slide. TRPO struggles with these tasks, while HTRPO can learn effective policies. HTRPO outperforms TRPO in complex control tasks like Fetch Push and Fetch Slide due to its ability to learn effective policies. The components of HTRPO contribute to its effectiveness, with \"HTRPO with KL1\" and \"HTRPO without WIS\" performing worse than the complete version. Estimating the KL-divergence using the \"vanilla\" method causes instability, leading to skipped iterations and policy updates, resulting in worse and more unstable performance. Weighted Importance Sampling is known for significantly improving performance. The study of Weighted Importance Sampling is known for reducing variance, as shown by the results of \"HTRPO without WIS\". While it matches the full HTRPO in simple environments, it performs poorly in complex environments like Fetch Push and Fetch Slide. The performance degradation of \"HTRPO without WIS\" compared to the full version is evident. The performance of policy gradient methods trained with hindsight data in continuous environments remains unexplored, and this section aims to address this gap by implementing HTRPO in continuous control tasks. In this section, HTRPO is implemented in continuous control tasks such as Fetch Reach, Fetch Push, and Fetch Slide. Results show that HTRPO outperforms HPG in all environments, achieving a success rate of 92% for Fetch Push and 82.5% for Fetch Slide. The sensitivity of HTRPO to different network architectures is studied, with HTRPO performing well with all settings while HPG only converges under certain conditions. Based on the learning curves, Hindishgt TRPO with more alternative goals achieves better converging speed. HTRPO extends TRPO to handle sparse reward environments using hindsight methodology, showing effective performance and sample efficiency in various environments. This work demonstrates HTRPO's potential in solving sparse reward reinforcement learning problems by updating policy quality in both discrete and continuous scenarios. The objective function of HTRPO is provided for the original goal and an alternative goal, showcasing its effectiveness in various environments. Following importance sampling techniques, the objective function is rewritten as a new goal. The gradient of the HTRPO objective function with respect to \u03b8 is computed for the original and alternative goals. The proof involves expanding equations and computing gradients based on relevant terms. The variation of distributions and Taylor expansion are also discussed in the context of the objective function. Expansion of log q(x) at p(x) is discussed, along with the approximation of the constraint function for policy \u03c0\u03b8(a|s). The variance of Y is denoted as Var(Y). The proof involves converting equations and showing the relationship between variables X1, X2, and Y. The transitivity of inequality is used to analyze the difference between log \u03c0\u03b8(a|s) and log \u03c0 \u03b8 (a|s). The transitivity of inequality is used to analyze the difference between log \u03c0\u03b8(a|s) and log \u03c0 \u03b8 (a|s). The constraint between policies for original and alternative goals is given by a specific equation. The constraint function is manipulated and converted into a goal-conditioned form. Importance sampling is used to convert the constraint for a new goal. The constraint for a new goal in the HTRPO optimization problem is derived by converting it into a goal-conditioned form. The feasibility of the algorithm is ensured by using estimators for the objective function and the KL-divergence constraint. Hindsight experience is generated by sampling alternative goals, and the Monte Carlo estimation is obtained from interacting with the environment under a specific goal. The distribution of alternative goals is followed in the training data. During training, the algorithm encourages the agent to achieve alternative goals, following their distribution. This approach is common in hindsight methods but may lead to unstable learning due to excessive variance. To address this, weighted importance sampling is used to stabilize the optimization process. The optimization problem is converted to a specific form, with a solution method provided in Appendix C.2. Weighted importance sampling can introduce bias similar to HPG, but this bias decreases inversely with data increase. Balancing bias reduction and batch size enlargement is crucial due to limited resources. The experiments show the benefits of weighted importance sampling with an appropriate batch size. The HTRPO optimization problem is briefly described, simplifying it to a linear objective function with quadratic constraints. The optimization problem is simplified to a linear objective function with quadratic constraints. The KKT conditions are used to solve the problem, with policies represented by a neural network. The Hessian matrix is computed using the conjugate gradient algorithm. In this section, a more comprehensive demonstration of the experiments of HTRPO is provided. It includes a full introduction to each environment, sensitivity analysis of HTRPO under different network architectures and numbers of alternative goals, and supplementary materials showing learning curves and success rates during training. Hyperparameters are finetuned based on experience due to limited computing resources. In the experiments of HTRPO, the agent performs tasks such as k-Bit Flipping and Grid World. In k-Bit Flipping, the agent flips bits in an array to match a target array within a limited number of time steps. The Grid World task involves navigating obstacles in an 11x11 grid to reach a random position. The experiments demonstrate the performance of HTRPO under different conditions. In the experiments, the agent navigates obstacles in a grid world environment with 2-dimensional coordinates and a maximum of 32 time steps. The environment includes Empty Maze and Four Rooms setups, as well as a Fetch environment with a robotic arm and gripper. In Fetch Push and Fetch Slide tasks, the Fetch robotic arm interacts with randomly placed blocks to push or slide them towards specific goal positions. The experiment evaluates the performance of HTRPO with different network architectures, including 16-unit, two 64-unit, and two 256-unit layers. Results are compared across different network settings for each environment. The experiment evaluates the performance of different network architectures in Fetch Push and Fetch Slide tasks. Networks with more hidden layers and neurons speed up convergence, with HTRPO converging quickly in all settings compared to HPG. The trust region search helps rapid convergence and robustness. The impact of the number of alternative goals on HTRPO performance is studied with discrete and continuous environments. In various environments, the performance of HTRPO is evaluated under different numbers of alternative goals. Results show that more alternative goals lead to faster convergence in simple environments, while 30 and 100 goals perform well in complex environments. The use of more alternative goals improves converging speed, similar to data augmentation. Success rates of HTRPO during evaluation and training are demonstrated, with actions sampled during training and chosen greedily during evaluation. The success rates of Fetch Push and Fetch Slide were evaluated, with mean values computed from 1000 test results in each iteration. These environments were chosen as they are the most complex. The experiments showed that the approximation of equation 13 significantly reduced the variance of KL expectation estimation, improving performance. Comparison between HTRPO and HTRPO with KL1 demonstrated the efficiency of this approximation. Both methods used the estimation method in equation 13, with the only difference being the adoption of weighted importance sampling in HTRPO. From Figure 9, \"HTRPO\" shows the least variance. The curves for KL1 are lower than equation 13 in TRPO due to step size adjustment based on KL divergence estimation. HTRPO's variance is lower as its KL divergence estimation is close to the expected value, reducing the need for step size adjustment."
}