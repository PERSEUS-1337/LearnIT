{
    "title": "Hyig0zb0Z",
    "content": "The paper introduces a new speech recognition system using a letter-based ConvNet acoustic model. It does not require alignment annotations or forced alignment during training. The decoder only needs a word list and language model, with letter scores from the acoustic model. Gated Linear Units and high dropout are key components of the acoustic model, showing near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features. The paper presents an end-to-end speech recognition system using a letter-based ConvNet acoustic model trained on the LibriSpeech dataset. The system eliminates the need for phonetic transcription and achieves near state-of-the-art results in word error rate. Improvements in acoustic modeling with deep neural networks (DNNs) and convolutional neural networks (CNNs) have advanced training pipelines. The current state of the art on LibriSpeech involves speaker adaptation. Recent advancements include GMM-free training and training with recurrent neural networks (RNNs) for phoneme transcription. End-to-end approaches with RNN layers have shown competitive results but are computationally expensive. Complex ConvNets+RNNs models are still leading in conversational speech recognition. The paper proposes an end-to-end training model using graphemes directly, without the need for phonetic annotation. It introduces a convolutional network architecture for the acoustic model, achieving clean speech performance comparable to previous models. The model is trained on 960h of data from LibriSpeech, outperforming on clean speech but performing worse on noisy speech. The paper presents a convolutional network architecture for acoustic modeling, with an overview of the model and experimental results on LibriSpeech. The acoustic model uses MFSC features fed to a Gated ConvNet, with scores for each letter in the dictionary and MFSC frame. The model is trained with a variant of the CTC criterion to promote sequences leading to the transcription sequence. The acoustic model is trained with a variant of the CTC criterion and uses MFSC features. Dropout is applied during training, and a decoder with beam search is used at inference. MFSCs are utilized for dimensionality compression and are more robust to time-warping deformations compared to spectrogram coefficients. The acoustic model uses MFSC frames and a 1D Gated Convolutional Neural Network architecture to output letter scores for each input frame. Gated ConvNets reduce the vanishing gradient problem by providing a linear path for gradients while retaining non-linear capabilities. The MFSC input sequences are normalized for convolutional processing, with padding used to account for border effects. Large speech databases often lack letter segmentation, making manual labeling tedious for training acoustic models. Several solutions have been explored in the speech community to alleviate the issue of manual labeling for training acoustic models. HMM/GMM models use an iterative EM procedure to optimize the model, while hybrid HMM/NN systems leverage criteria like MMI and MBR. Standalone neural network architectures have also been trained using CTC to jointly infer transcription segmentation and improve overall scores. In this paper, a variant of Connectionist Temporal Classification (CTC) is used to jointly infer transcription segmentation and improve overall scores. CTC considers all possible sequence sub-word units, with a special \"blank\" state to model \"garbage\" frames and identify separations between sub-word units in a transcription. The CTC acceptance graph over T frames for a given transcription is denoted as G ctc (\u03b8, T). CTC minimizes the Forward score over the graph G ctc (\u03b8, T) using the \"logadd\" operation. Blank labels introduce complexity when decoding letters into words, as they create multiple entries in the transcription dictionary. The sub-word unit transcription dictionary introduces complexity with blank labels, leading to a simplified graph denoted as G asg (\u03b8, T). Adding unnormalized transition scores on each edge fixes training issues observed with normalized transitions. This approach requires sequence-level normalization to prevent model divergence. The ASG criterion involves sequence-level normalization to prevent model divergence. It combines promoting the score of correct sequences and demoting the score of all sequences. Dropout is applied to all layers of the acoustic model to retain outputs with a certain probability. The implementation of Gated ConvNets includes weight normalization and gradient clipping to speed up training convergence. A one-pass decoder with beam-search, beam thresholding, and language model smearing was developed, relying on KenLM for language modeling. No model adaptation or word graph rescoring was implemented before decoding. The decoder in the language modeling part accepts unnormalized acoustic scores as input and aims to maximize a probability equation with hyper-parameters controlling the weight of the language model. It tracks paths with the highest scores by bookkeeping language model and lexicon states, merging paths with identical states for diversity in the beam. Traditional decoders combine scores of merged paths with a max operation. The decoder in the language modeling part accepts unnormalized acoustic scores and aims to maximize a probability equation with hyper-parameters. It tracks paths with the highest scores by bookkeeping language model and lexicon states, merging paths for diversity in the beam. The system was benchmarked on LibriSpeech with 16 kHz sampling rate, using CLEAN and OTHER setups for training and tuning hyper-parameters. The letter vocabulary contains 30 graphemes. The letter vocabulary L contains 30 graphemes, including the standard English alphabet, apostrophe, silence (<SIL>), and special \"repetition\" graphemes. Decoding is done with a custom decoder using a 4-gram language model from LibriSpeech. MFSC features are computed with 40 coefficients. Implementation was done using TORCH7, with the ASG criterion and decoder in C. Acoustic model architectures were tuned using grid search on dev sets. Two architectures were considered with varying dropout levels. The architectures considered have low and high dropout levels, with parameters increasing linearly with depth. Gated Linear Units are used, duplicating each layer. The models have about 130M and 208M trainable parameters. Performance on LibriSpeech development sets is shown for the LOW DROPOUT architecture. LER and WER are well correlated. In TAB1, WERs are reported for LOW DROPOUT and HIGH DROPOUT architectures on LibriSpeech development sets. Increasing dropout regularizes the acoustic model, impacting generalization, especially on noisy speech. The decoder uses max(\u00b7) operation for path aggregation in the beam. In TAB2, the system is compared with other top systems on LibriSpeech, highlighting acoustic model architectures and sub-word units. Phone-based acoustic models output senomes selected through a phonetic-context-based decision tree. The curr_chunk discusses the complexity of phone-based systems in speech recognition, involving phonetic-context-based decision trees and speaker adaptation. It also mentions the use of iVectors for speaker embedding and additional resources like lexicons and acoustic models. The system performance is compared with other systems, including Deep Speech 2, highlighting different techniques like fMLLR for speaker transformation. The curr_chunk compares our system, a simple ConvNet, with DEEP SPEECH 2, a more complex neural network using a ConvNet and RNN. Both systems rely on letters for acoustic modeling, with DEEP SPEECH 2 using more speech data and a large language model. Our system shows competitive performance in WER compared to other systems. Our system, a simple ConvNet, outperforms DEEP SPEECH 2 on clean data despite being trained with less data. The Gated ConvNet model is effective at modeling true words, achieving fast decoding times on test sets. The system combines a large ConvNet acoustic model with a simple sequence criterion and beam-search decoder for automatic speech recognition. Our approach, based on a ConvNet model, achieves competitive decoding results on the LibriSpeech corpus without the need for HMM/GMM pre-training or forced alignment. It is less computationally intensive than RNN-based approaches and relies on a publicly available dataset for reproducibility. Future work could involve leveraging speaker identity, training from raw waveforms, data augmentation, and using better language models."
}