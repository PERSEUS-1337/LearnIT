{
    "title": "HylAoJSKvH",
    "content": "The problem of unconstrained minimization of a smooth objective is considered. In the setting of function optimization in $\\mathbb{R}^d$, a stochastic zeroth-order method with heavy ball momentum, SMTP, is proposed and analyzed. This momentum version of the stochastic three-point method shows improved performance for non-convex, convex, and strongly convex functions. SMTP is tested on various learning tasks and outperforms other derivative-free optimization algorithms and policy gradient methods. Additionally, SMTP with importance sampling (SMTP_IS) is introduced, with convergence analysis provided for different types of objectives. In Derivative-Free Optimization (DFO), the objective function f is \"smooth\" but not necessarily convex, bounded from below by f(x*), and assumed to be L-smooth. DFO settings do not have access to the derivatives of f, making evaluations only possible through simulations of black-box engines or software. This optimization problem setting is common in applications like computational medicine and fluid dynamics. The literature on Derivative-Free Optimization (DFO) spans various applications such as computational medicine, fluid dynamics, localization, and continuous control. Initial approaches were based on deterministic direct search (DDS) methods, with complexity bounds recently established by Vicente and colleagues. Variants of DDS include randomized approaches. Several variants of DDS, including randomized approaches, have complexity bounds derived for randomized methods. Recent works have proposed accelerated and non-accelerated zero-order methods for both smooth and non-smooth functions. An extension for non-Euclidean proximal setup has also been proposed. Gorbunov et al. (2018) proposed a method for smooth stochastic convex optimization with inexact oracle. Bergou et al. (2019) introduced Stochastic Three Points (STP), a randomized direct search method that compares objective function values at three points to choose the new iterate. STP is known for its simplicity, generality, and practicality in implementation. The STP method is a flexible zeroth-order optimization method that covers various strategies for choosing search directions. It focuses on faster convergence for first-order methods, with a special technique introduced by Polyak in 1964. Despite its long history, there is still an open question on whether the heavy ball method globally converges with acceleration. The heavy ball method's global convergence with acceleration remains an open question for twice continuously differentiable, L-smooth, and \u00b5-strongly convex objective functions. While non-accelerated global convergence has been proven for this class of functions, recent work has shown asymptotic accelerated global convergence for the special case of quadratic strongly convex and L-smooth functions. Despite this uncertainty, the heavy ball method is widely used in practice. Additionally, importance sampling has been extensively studied in various optimization methods, with the recent proposal of STP_IS as the first DFO algorithm with importance sampling. The authors proposed the first DFO algorithm with importance sampling, STP_IS, which improves the leading constant in gradient-based methods. They introduced momentum into the STP method, creating a new DFO algorithm with heavy ball momentum (SMTP) for non-convex, convex, and strongly convex functions under generic sampling directions D. This method is not a straightforward generalization of STP and Polyak's method. The first DFO algorithm with importance sampling, STP_IS, improves the leading constant in gradient-based methods. The new DFO algorithm, SMTP, incorporates heavy ball momentum and importance sampling for zeroth-order methods. This method is not a simple extension of STP and Polyak's method. The new DFO algorithm, SMTP, incorporates heavy ball momentum and importance sampling for zeroth-order methods with theoretical guarantees for non-convex, convex, and strongly convex functions. Extensive experiments on continuous control tasks from the MuJoCo suite show its practicality compared to model-free reinforcement learning methods. SMTP_IS achieves state-of-the-art results in continuous control, outperforming DFO and policy gradient methods. The analysis is based on key assumptions about the probability distribution. The analysis in the previous paragraphs is based on key assumptions about the probability distribution. Examples of distributions meeting these assumptions are described in Lemma 3.4 by Bergou et al. (2019). The STP method can be viewed as substituting the gradient in the update rule with a sample from a distribution satisfying Assumption 3.1. The focus then shifts to Polyak's heavy ball method for more efficient first-order optimization. Lemma 3.1 states that for the iterates of SMTP with non-convex functions, certain inequalities hold. The complexity results for Algorithm 1 show that SMTP guarantees bounds similar to classical bounds, with a query complexity matching its iteration complexity. Theorem 3.1 confirms these results under specific assumptions. SMTP with \u03b3 k produces points {z 0 , z 1 , . . . , z K\u22121 } and z K is chosen randomly. Choosing \u03b3 = \u03b30 \u221a K reduces complexity and guarantees convergence to a stationary point. The relationship between SMTP and STP is similar to Polyak's heavy ball method and gradient descent. Complexity results for convex functions show bounds similar to classical bounds. The text discusses the complexity results for the SMTP method with a constant step size and decreasing step sizes when the function is \u00b5-strongly convex. Assumptions and inequalities are presented to show the convergence of the method to a stationary point. The relationship between SMTP and STP is compared to Polyak's heavy ball method and gradient descent. In this section, complexity results for Algorithm 1 are presented for \u00b5-strongly convex functions. Assumptions and theorems are discussed regarding solution-dependent and solution-free stepsizes in the context of the SMTP method. In this section, Assumption 4.1 is introduced for problems with coordinate-wise L-smoothness in the objective function. The SMTP_IS method is presented in Algorithm 2, which adjusts sampling probabilities for better convergence guarantees based on known Lipschitz constants. The key result establishing the inequalities for the iterates of SMTP_IS is presented. Complexity results of SMTP_IS are provided in the Appendix. Experimental Setup includes extensive experiments on challenging non-convex problems in continuous control tasks using the MuJoCO suit. In this study, the authors address the problem of model-free control of a dynamical system using policy gradient methods. They compare their proposed method, SMTP, against state-of-art DFO based methods and classical policy gradient methods. Experiments are conducted on various environments with varying difficulty levels. The mean of function values is used due to the stochastic nature of the problem. Values of f over K observations are chosen based on validation performance. For SMTP, \u03b2 = 0.5 is set. SMTP_IS uses smoothness constants of a \u03b8 parameterized smooth function. Further implementation details can be found in Bibi et al. (2019). Our method (SMTP) uses directions from a standard Normal distribution in each iteration, while SMTP_IS samples from columns of a random matrix B. We compare algorithms in terms of reward vs. sample complexity, showing that SMTP improves sample complexity significantly, especially for high dimensional problems like Ant-v1 and Humanoid-v1. Our method, SMTP, outperforms state-of-the-art DFO and policy gradient algorithms in environments like Swimmer-v1, Hopper-v1, HalfCheetah-v1, and Ant-v1. For Humanoid-v1, our method shows comparable sample complexity. SMTP is the first heavy ball momentum DFO algorithm with convergence rates for various functions, including non-convex, convex, and strongly convex functions. We further improve rates with SMTP_IS, a version with momentum and importance sampling for different functions. Our method, SMTP, outperforms DFO and policy gradient algorithms in controlling dynamical systems. The key lemma established in the paper is used to prove the theorems. The probability distribution D on R d satisfies certain properties, and the iterates of SMTP satisfy specific inequalities. Our method, SMTP, outperforms DFO and policy gradient algorithms in controlling dynamical systems. The key lemma established in the paper is used to prove the theorems. Let Assumptions A.1 and A.2 be satisfied. Let SMTP with \u03b3 k \u2261 \u03b3 > 0 produce points {z 0 , z 1 , . . . , z K\u22121 } and z K is chosen uniformly at random among them. Moreover, if we choose \u03b3 = \u03b30 \u221a K the complexity reduces and for this choice we have Proof. Assumption C.1 states that f is convex, has a minimizer x * and has bounded level set at x 0. Theorem C.1 (Constant stepsize) states that if Assumptions A.1, A.2 and C.1 are satisfied, setting \u03b3 k \u2261 \u03b3 < leads to specific inequalities for the iterates of SMTP. For the iterates of SMTP method, if Assumptions A.1, A.2, and C.1 are satisfied and \u03b3 k \u2261 \u03b3 < , then the inequality holds. By setting \u03b3 = and running SMTP for k = K iterations, we get E f (z). Using a technical lemma, we prove the inequality for a k by induction. The induction step is proven by showing that the right-hand side is upper bounded by \u03b8C \u03b1(k+1)+\u03b8. This is achieved by multiplying both sides by (\u03b1k + \u03b8)(\u03b1k + \u03b1 + \u03b8)(\u03b8C). The inequality holds for all k \u2265 0 when (1\u2212\u03b2)R0 and \u03b8 \u2265 2 \u03b1. Assumption D.1 states that f is \u00b5-strongly convex with respect to the norm \u00b7 * D. Theorem D.1 states that if Assumptions A.1, A.2, and D.1 are satisfied, then solution-dependent stepsizes can be determined. Theorem D.1 states that solution-dependent stepsizes can be determined if Assumptions A.1, A.2, and D.1 are satisfied. By setting appropriate parameters, the iterates of SMTP can satisfy certain inequalities, ensuring convergence within a specified number of iterations. Theorem E.1 discusses the convergence of SMTP_IS with solution-dependent stepsizes under certain assumptions. It shows that the points produced by SMTP_IS converge to a specific point when chosen uniformly at random. The convergence of SMTP_IS with solution-dependent stepsizes is discussed in Theorem E.1. It shows that points produced by SMTP_IS converge to a specific point chosen uniformly at random among them. By setting \u03b3 accordingly, the rates of convergence improve. Theorem E.4 discusses the convergence of SMTP_IS with solution-dependent stepsizes, showing that points produced by the method converge to a specific point chosen uniformly at random among them. By setting \u03b3 accordingly, the rates of convergence improve. Theorem E.5 introduces solution-free stepsizes for the SMTP_IS method, allowing for the same convergence rate as in the previous theorem with just one extra function evaluation. This approach overcomes the drawback of depending on unknown values, offering a practical solution for optimization. The text discusses optimization methods, specifically focusing on estimating values and complexities in the context of SMTP and SMTP_IS algorithms. It compares the complexities for non-convex and convex cases, highlighting differences in norms used. The approach of using solution-free stepsizes in SMTP_IS is introduced to overcome dependencies on unknown values, offering practical optimization solutions. In the non-convex case, different norms are used for SMTP and SMTP_IS algorithms to guarantee specific conditions. Holder's inequality is applied to ensure the conditions are met for SMTP. In the convex case, Cauchy-Schwartz inequality is used to compare complexities. The text introduces the use of solution-free stepsizes in SMTP_IS for practical optimization solutions."
}