{
    "title": "HyydRMZC-",
    "content": "Recent studies have shown that Deep neural networks (DNNs) are vulnerable to adversarial examples, with algorithms leveraging L_p distance to generate perturbations. Various defense methods have been explored to counter these attacks. This paper focuses on spatial transformations instead of directly manipulating pixel values. Spatial transformation perturbations result in large L_p distance measures, making them difficult to defend against. This new approach in adversarial example generation produces realistic examples with smooth image deformation. Deep neural networks have shown vulnerability to adversarial examples, which are perturbations added to input images. These examples can have negative impacts on tasks like malware detection and autonomous navigation systems, posing security risks in various applications. Understanding the attention of deep networks with different types of adversarial examples is crucial for mitigating these risks. Adversarial examples are perturbations that can impact security-related applications. Different methods like FGSM and C&W have been proposed to generate these examples. The traditional L2 norm distance metric is not ideal as it is sensitive to changes in lighting and viewpoint, leading to inaccurate evaluations of perturbed images. In this paper, the focus is on creating perceptually realistic adversarial examples by changing pixel positions instead of values. The proposed stAdv method aims to make adversarial examples less distinguishable from real instances. Various defense methods, including adversarial training, have shown promising results in improving the robustness of deep networks against adversarial attacks. The stAdv method generates spatially transformed adversarial examples by minimizing local geometric distortion instead of pixel errors. These examples pose a new challenge for defense methods as they are visually imperceptible. Visualizations show smooth spatial deformations and attention shifts in deep neural networks when faced with different attack algorithms. The stAdv method generates adversarial examples using spatial transformations, making them visually imperceptible and challenging for defense systems. These examples are shown to be more resilient across different defense models and harder to detect compared to other attacks. Visualizations demonstrate small, locally smooth geometric changes that maintain high perceptual quality. Adversarial examples are created using spatial transformations, making them visually imperceptible and challenging for defense systems. These examples consistently mislead robust deep networks compared to other attack methods. Adversarial attacks can be targeted or untargeted, with the goal of modifying inputs to be misclassified by the target model. In adversarial attacks, the adversary aims to cause misclassification of the perturbed input in a different class from its ground truth. White-box attacks involve full knowledge of the classifier and training data, while black-box attacks have zero knowledge. This work focuses on white-box attacks to explore the capabilities of a powerful adversary. The text discusses adversarial attacks focusing on changing lighting and material properties while assuming the underlying geometry remains constant. It explores using spatial transformers to model geometric transformations for more realistic results in visual recognition tasks. In response to the emergence of adversarial examples, various defense methods have been explored, including adversarial training, distillation, gradient masking, and feature squeezing. Adversarial training has shown the best performance, utilizing fast gradient sign attack and ensemble adversarial training. This work evaluates the effectiveness of spatial transformation-based adversarial examples under these defense methods. Spatially transformed adversarial examples are generated to attack classifiers, aiming to predict a specific target class. Current methods modify pixel values directly, with the fast gradient sign method (FGSM) using a first-order approximation of the loss function for untargeted attacks. The algorithm generates adversarial examples to attack classifiers by optimizing perturbations based on certain constraints, achieving untargeted and targeted attacks. The process involves a single gradient ascent step and a spatial transformation to manipulate pixel values for generating adversarial images. The algorithm generates adversarial examples by optimizing perturbations to attack classifiers. It introduces a spatial transformation model to smoothly change the scene's geometry while maintaining the original appearance, creating more realistic adversarial examples. The process involves synthesizing the adversarial image using a per-pixel flow field to manipulate pixel values from the input image. The algorithm creates adversarial images by optimizing pixel displacements using a flow vector. Bilinear interpolation is used to transform the input image with the flow field, generating realistic adversarial examples. The algorithm generates adversarial images by optimizing pixel displacements with a flow vector, aiming to mislead the classifier while minimizing local distortion. The flow field captures spatial transformations needed to deceive the classifier, with a focus on perceptual quality rather than small perturbations in pixel space. The algorithm generates adversarial examples to mislead the classifier by minimizing spatial transformation distance and balancing losses. The goal is a targeted attack where the input image is transformed to ensure a different class prediction. The objective function enforces attack confidence level and smooth spatial perturbations. The algorithm minimizes spatial transformation distance to generate adversarial examples for targeted attacks, achieving high success rates against deep networks and current defense methods. The optimization is solved using an L-BFGS solver, and the estimated flows for adversarial examples are visualized. Additionally, attention regions of DNNs are analyzed for better understanding. The algorithm minimizes spatial transformation distance to create adversarial examples for targeted attacks, achieving high success rates against deep networks and current defense methods. The attention regions of DNNs are analyzed for better understanding of attack properties. Experiment setup includes using \u03c4 as 0.05, confidence \u03ba = 0, and L-BFGS solver for generating adversarial examples with high perceptual quality on MNIST and CIFAR-10 datasets. The algorithm stAdv generates adversarial examples for targeted attacks with high success rates against deep networks. It deforms digits smoothly to evade detection by humans, achieving attack success on CIFAR-10 models. The stAdv algorithm successfully generates targeted adversarial examples with high success rates on different models. These examples are difficult for humans to distinguish from the original images. Comparisons of adversarial examples targeted to the same class are also shown. The stAdv algorithm generates targeted adversarial examples with high success rates on various models, focusing on object boundaries and edges to mislead classifiers. Visualizations on MNIST, CIFAR-10, and ImageNet datasets confirm the importance of edges in adversarial attacks. The stAdv algorithm successfully generates targeted adversarial examples on different datasets, with a focus on object boundaries to deceive classifiers. Visualizations on ImageNet dataset BID4 show targeted adversarial examples with high success rates, where spatial transformations mainly concentrate on the target object. A user study on Amazon Mechanical Turk evaluates the perceptual realism of stAdv's adversarial examples. In a user study on Amazon Mechanical Turk, participants choose between stAdv's adversarial examples and original images for realism. Each trial shows both images for 2 seconds, with unlimited decision time. 2,740 annotations from 93 users show stAdv's examples are nearly indistinguishable from natural images, chosen as more realistic in 47.01% of trials. In the white-box setting, adversarial examples are generated and tested against different defense methods to evaluate their strength. Three defense strategies - FGSM adversarial training, ensemble adversarial training, and projectile gradient descent adversarial training - are applied. Adversarial examples are generated based on L \u221e bound for MNIST and CIFAR-10 datasets. Results show the effectiveness of the defense strategies on both datasets. The three defense strategies tested against adversarial attacks show high performance on FGSM and C&W attacks, but low defense performance on stAdv. This suggests the need for new defense systems to combat evolving adversarial strategies. The spatial transformation-based attack introduces challenges in bounding the distance using L p norm, leading to high perceptual quality adversarial examples. Testing against a 3\u00d73 average pooling restoration mechanism further validates the effectiveness of the defense strategies. The 3x3 average pooling restoration mechanism can improve classification accuracy up to 70% on fast gradient sign examples. However, it only increases model accuracy to around 50% on stAdv examples, indicating their robustness. A perfect knowledge adaptive attack against this defense strategy showed limited effectiveness. The success rate of an adaptive attack using stAdv is nearly 100%, consistent with previous findings. Class Activation Mapping is used to visualize attention regions in the ImageNet inception_v3 model for original and adversarial images. Comparison of attention regions in naturally trained and adversarial trained models is also conducted. The success rate of adaptive attacks using different algorithms is nearly 100%. When the adversarial examples are targeted to the label \"missile,\" the attention region shifts to the tower. However, when tested on a robust model, the attention regions revert back to the original image, with FGSM and C&W failing to attack the model. The robust model misclassifies FIG5 (h) as \"missile\" and the CAM visualization shows attention on the tower. Adversarial examples generated by stAdv are challenging for current ImageNet models. stAdv uses spatial transformation for perturbation, making it harder for humans to distinguish from original instances. These examples are difficult to defend against, prompting the need for more robust defense algorithms. In this study, adversarial examples generated by stAdv are evaluated against different model architectures, including the 3x3 average pooling restoration mechanism. The classification accuracy of recovered images after performing 3x3 average pooling on various models is shown. Adversarial examples are also generated for the target inception_v3 model and Model B. The original images with correct labels and adversarial examples with target labels are visualized, highlighting the effectiveness of the attack. In this study, adversarial examples generated by stAdv are evaluated against different model architectures, including Model B. Adversarial examples are shown for the target ResNet-32 model on CIFAR-10. The magnitude of the generated flow is measured using total variation (TV) and L2 distance on various datasets."
}