{
    "title": "BkzeUiRcY7",
    "content": "In this paper, the focus is on achieving optimal coordination among self-interested worker agents in multi-agent reinforcement learning. A super agent, the manager, is trained to infer the agents' minds and assign tasks through contracts with bonuses to maximize productivity and minimize payments. The paper proposes Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) to train the manager in optimizing ad-hoc worker teaming. The approach is evaluated in Resource Collection and Crafting environments, showing effectiveness in modeling worker agents' minds and achieving optimal teaming with good generalization and fast adaptation. Self-interested agents collaborate effectively with proper incentives and contracts, a common phenomenon in daily life. In this paper, the Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) is proposed to optimize ad-hoc worker teaming. The manager receives external rewards for task completion, while each worker has unique skills and preferences. The manager is unaware of the workers' skills and preferences, and may not receive rewards until tasks are completed. The manager gives additional incentives in the form of contracts to self-interested workers with different skills and preferences. Each contract assigns a goal and a bonus for achieving the goal to a worker, leading to optimal collaboration. The manager uses contracts with goals and bonuses to incentivize workers to adjust their intentions and work together effectively. By employing reinforcement learning, the manager can assess workers' skills, preferences, and intentions in real-time to optimize contract assignments. This approach differs from traditional mechanism design by avoiding strong assumptions and focusing on dynamic goal and reward allocation. The proposed framework, Mind-aware Multi-agent Management Reinforcement Learning (M3RL), aims to optimize contract assignments by estimating workers' minds and generating contracts using deep reinforcement learning. It includes agent modeling to track workers' internal states and policy learning for contract generation. The approach is evaluated in Resource Collection and Crafting environments in 2D Minecraft to simulate multi-agent interactions. The proposed framework, M3RL, optimizes contract assignments by estimating workers' minds and generating contracts using deep reinforcement learning. It includes agent modeling to track internal states and policy learning for contract generation. Experiments show the manager can estimate worker behavior, motivate task completion, adapt to changing teams, and generalize well in different team sizes and environments. Our approach demonstrates consistent performance in various settings, including scenarios with stochastic worker policies and multiple levels of bonuses. Unlike traditional multi-agent reinforcement learning methods, we train a manager to oversee self-interested workers for optimal collaboration, addressing principal-agent problems. The curr_chunk discusses principal-agent problems in economics, highlighting the challenges of information asymmetry and setting up incentives for agents. Traditional economic approaches use mathematical models with known utility functions, while the paper offers a practical solution. Our paper presents an end-to-end computational framework for addressing principal-agent problems in economics without assuming agents' utilities or decision-making processes. The framework is adaptive to changes in agents' preferences and capabilities, and we evaluate it in more complex game settings than current literature. Our approach differs from mechanism design by considering not only preferences but also the possibility of agents having other factors influencing their decisions. In our work, we address principal-agent problems by considering agents' skills and dynamic contract changes. We use deep RL for optimal reward design and meta-learning to manage multi-agents in complex tasks efficiently. Our approach differs from traditional reward design and focuses on addressing ad-hoc teaming problems. The paper addresses ad-hoc teaming problems by using meta-learning to train from a limited worker population. Inspired by the theory of mind, the work extends agent modeling to understand optimal multi-agent management. Each worker is modeled using an independent Markov Decision Process, with different minds and goals. The paper discusses ad-hoc teaming problems using meta-learning to train from a limited worker population. Each worker is modeled with a Markov Decision Process, with preferences, intentions, and skills. The key concepts include contracts, worker's mind, preferences, and reward functions. The worker agent's reward function is defined based on the goal state and intention. Workers may choose goals to maximize expected return or utility, leading to potentially deceptive behavior. This approach aims for a more realistic simulation in ad-hoc teaming problems. In this work, the focus is on achieving a more realistic simulation in ad-hoc teaming problems. Workers decide whether to sign assigned contracts based on their intentions. The manager's objective is to maximize utility by assigning contracts optimally, considering worker commitment. The manager gains rewards based on the worker's consistency in achieving goals. The manager's reward function is defined based on the utility of assigned goals, with a focus on maximizing expected return. A population of worker agents is maintained for training, allowing the manager to handle various worker compositions. In testing, workers are sampled from a new population not seen in training. The approach includes a performance history module, mind tracker module, and manager module. Worker identities are inferred to distinguish them from others. Previous methods identify agents based on past trajectories, but this is impractical as trajectories depend on the manager's policy. Our network architecture includes a performance history module for agent identification, inspired by the UCB algorithm for MAB problems. Worker performance is estimated through matrices and encoded into a history representation. A mind tracker module updates the manager's belief of a worker's mental state using current and past information. The manager considers all workers' information to generate contracts. Besides individual policies, the manager estimates team productivity. Instead of a centralized value function, they use successor representation to estimate goal achievements and bonus payments. The manager uses successor representation to estimate goal achievement and bonus payments for team productivity. They employ advantage actor-critic and imitation learning for on-policy updates and mind tracker improvement, respectively. Experimental results demonstrate the effectiveness of mental state representation in challenging scenarios. The manager utilizes mental state representation to improve performance in goal achievement and bonus payments for team productivity. An exploration strategy called agent-wise \u03b5-greedy is adopted to better understand each worker's skills and preferences, preventing premature contract terminations. This strategy assigns a worker a random goal at the beginning of an episode, which remains unchanged throughout, aiding the manager in assessing goal attainment. Further details can be found in the rollout procedure in Appendix B. In Resource Collection, workers are rule-based agents with specific resource collection goals. There are 4 types of resources on the map, and each worker can collect up to three types, including their preferred type. Alternatively, they may only collect one type, which may not be their preference. In another setting, workers have a random preference in each episode. In Resource Collection, workers can take actions like \"move forward\", \"turn left\", \"turn right\", \"collect\", and \"stop\". Their skill is reflected by the effect of the \"collect\" action. The manager receives a reward for every resource collected and can choose to pay a worker with a bonus. Crafting top-level items requires crafting intermediate items first at four work stations. Each top-level item is worth a reward, but collecting raw materials and crafting intermediate items have no reward. Certain materials are needed for crafting both top-level items. In Resource Collection, workers can take actions like \"move forward\", \"turn left\", \"turn right\", \"collect\", and \"stop\". The manager receives a reward for every resource collected and can choose to pay a worker with a bonus. Crafting top-level items requires crafting intermediate items first at four work stations. Each top-level item is worth a reward, but collecting raw materials and crafting intermediate items have no reward. The manager must strategically choose which item to craft, with raw materials sufficient for crafting one to two top-level items in each episode. Workers have specific collecting goals and can only craft one type of item. The manager can choose a bonus from 0 to 2 for contracts. In Resource Collection, workers can take actions like \"move forward\", \"turn left\", \"turn right\", \"collect\", and \"stop\". The manager receives a reward for every resource collected and can choose to pay a worker with a bonus. Crafting top-level items requires crafting intermediate items first at four work stations. Each top-level item is worth a reward, but collecting raw materials and crafting intermediate items have no reward. The manager must strategically choose which item to craft, with raw materials sufficient for crafting one to two top-level items in each episode. Workers have specific collecting goals and can only craft one type of item. The manager can choose a bonus from 0 to 2 for contracts. Learning a value function directly without successor representations, removing action prediction loss, replacing agent-wise exploration with conventional -greedy exploration, encoding agent trajectories in the most recent 20 episodes, applying UCB BID2 for managing multi-armed bandit sub-problems, revealing ground-truth worker skills and preferences, and maintaining a population of 40 worker agents during training. During training, a population of 40 worker agents is maintained. Sampling a few workers in each episode, the learning curves consistently show superior performance. In challenging settings like S3 of Resource Collection and Crafting, techniques like IL, SR, and agent-wise -greedy exploration play crucial roles. Particularly in Crafting, SR and IL are vital for providing training signals. Agent identification through recent trajectories learns slowly. During training, techniques like IL, SR, and agent-wise -greedy exploration are crucial for superior performance. Agent identification through recent trajectories learns slowly in Resource Collection and fails in Crafting. To adapt to evolving worker skills, comparisons are made between agent-wise -greedy and temporal -greedy exploration with constant exploration coefficient. Testing involves replacing 75% of workers every 2,000 episodes. Performance of the baseline that knows ground-truth agent is also shown for reference. During testing, the baseline performance with ground-truth agent information is compared to our agentwise -greedy exploration method. Significant changes in skill distribution require policy adjustments, resulting in improved learning efficiency and rewards. Our approach shows stable policies in moderate changes, while achieving similar rewards as other methods in training. Our approach allows the manager to quickly adapt to new teams, achieving higher rewards as the number of workers increases. It outperforms other methods in all settings, even in novel environments with added walls. In novel environments with added walls, the complexity of inferring workers' intentions increases, leading to decreased performance in certain tasks. Randomizing worker actions can affect performance, with moderate randomness still maintaining comparable results. In this paper, the proposed M 3 RL approach aims to solve collaboration issues among workers with different skills and preferences. It combines imitation learning and reinforcement learning to train a manager to infer workers' minds and assign contracts for maximizing productivity. Additional tests include the effect of contract duration, multiple bonus levels, and training RL agents as workers. The approach combines imitation learning and reinforcement learning to train a manager for maximizing productivity by inferring workers' minds and assigning contracts. Techniques like high-level successor representation and agent-wise exploration improve model performance. Results show effective learning, good generalization, and fast adaptation. The text discusses the use of imitation learning and reinforcement learning to train a manager for maximizing productivity. It mentions the use of successor representations and a two-phase learning curriculum to encourage exploration in tasks with unknown dependencies. In practice, a fixed number of episodes are set at the beginning of training for the warm-up phase in Crafting. The rollout algorithm and learning algorithm are summarized in Algorithm 1 and Algorithm 2. The manager's commitment determines how frequently goals and bonuses can be changed, with short-term commitment allowing quick updates and long-term commitment leading to more accurate skill assessment. In Resource Collection, shorter commitment works better than in Crafting, where a longer commitment is needed. The manager selects bonuses ranging from 1 to 4 for Resource Collection and 0 to 4 for Crafting. Rewards include 5 for every collected resource in Resource Collection and 10 for a top-level item in Crafting. The advantage of this approach is significant. Our approach in training a population of 40 RL worker agents for Resource Collection involves training each agent with a single goal and using different random seeds. This results in a population with similar skill distributions but different policies. Despite the slower training process due to less predictable and rational policies of RL agents, our approach gradually learns a good policy comparable to rule-based worker agents. The Performance History Module flattens worker performance matrices and encodes them into a 128-dim history representation. The Mind Tracker Module represents worker states using multiple channels for different item types. The worker state is represented by multiple channels for different items and orientation. Additional channels are used for actions, goals, and bonuses. The state is encoded into a 128-dim hidden state using convolutional and LSTM layers. An attention-based mechanism fuses this vector with the history representation, resulting in a fused vector. The manager module uses attention-based mechanisms to fuse worker representations with their states, pooling over individual workers to construct context vectors. These vectors are used to generate worker policies and bonuses through FC layers with softmax activation. All modules are trained with RMSProp. The worker modules are trained with RMSProp and use FC layers with softmax activation to generate policies and bonuses. Workers find the nearest location related to a goal and their skill is defined by their actions. They collect resources from the closest location matching the goal and craft items at unoccupied work stations. Random actions are sampled at each step. The RL worker agents are trained using a network architecture with additional channels for goal rewards. A convolution layer encodes the state, followed by FC and LSTM layers for policy prediction. Each goal has 10 agents trained with random rewards assigned per episode. Set the remaining channels to zeros, assuming all RL workers can perform \"collect\" and \"craft\" actions."
}