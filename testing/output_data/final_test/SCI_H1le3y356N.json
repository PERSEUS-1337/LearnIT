{
    "title": "H1le3y356N",
    "content": "The paper discusses how an attacker can steal the weights of a known convolutional neural network by accessing the softmax layer. They achieved high accuracy using stolen models with i.i.d. noise inputs. The susceptibility of weights to theft indicates dataset complexity. The study highlights the unique weight learnability of CNNs with i.i.d. noise input and presents initial results using the Ising probability distribution. The paper explores how an attacker can steal the weights of a known convolutional neural network by accessing the softmax layer. The goal is not to convince readers of the real-world validity of the scenario but to share initial findings. The study involves using noise as input to populate the weights of an MNIST-trained CNN model. Preliminary work is under review by the International Conference on Machine Learning (ICML). The paper discusses the risk of model weight theft through empirical experiments and open-sourcing the findings. It focuses on the interplay between dataset, architecture, and noise distribution in model extraction attacks. The structure includes a literature survey, methodology description, and further sections on the attack. The paper discusses model theft in machine learning, focusing on attacks on ML-as-a-service platforms. Authors trained a copycat network using non-problem domain images to steal labels for facial expression, object, and crosswalk classification. Knockoff Nets showed querying with random images from a different distribution can extract black box target training data. The attacker successfully trained a knockoff network using noise images as querying inputs, achieving high accuracy even with a different architecture. State-of-the-art CNNs were found to be not robust enough to provide a uniform softmax output when noninput-domain noise was inputted. This was studied in the context of fooling images, generating synthetic images unrecognizable to humans but classified with high confidence by CNNs. The authors focused on extracting model weights using Bernoulli noise-samples as inputs without needing samples from the true dataset. They assumed knowledge of the model's architecture and input dimensionality. The experiments were conducted on a 28x28 input tensor. In experiments, a black box model F (\u00b7) is trained on a 28x28 input tensor with values in [0, 1]. A dataset of 600000 binary tensors is generated with pixel values sampled from a Bernoulli distribution. A new model, F extract (\u00b7), is trained on stimulus response pairs. We train a new model, F extract (\u00b7), on stimulus response pairs without regularization. The architecture is the same as F (\u00b7) but with dropout layers removed to encourage overfitting. Training is done for 50 epochs using Adadelta optimizer with an initial learning rate of 1.0 and minibatch size of 128. Class imbalance is addressed by computing class weights based on softmax vectors. The proposed framework is evaluated on four MNIST datasets: MNIST, KMNIST, Fashion MNIST, and notMNIST. The study evaluated the effect of sampling random data from different distributions on the performance of F extract (\u00b7) on the MNIST validation set. Various distributions were used, including the uniform distribution, standard normal distribution, standard Gumbel distribution, Bernoulli distribution, and samples from an Ising model simulation. The experiment focused on the role of the inverse temperature \u03b2 parameter of the Ising sampler. In the experiment, the role of the inverse temperature \u03b2 parameter of the Ising sampler in training F extract (\u00b7) was evaluated. Stimulus response pairs were partitioned into subsets with 7000 examples each for different \u03b2 parameters. The performance of F extract (\u00b7) was tested on MNIST, KMNIST, Fashion MNIST, and notMNIST datasets. F (\u00b7) achieved a validation accuracy of 99.03% and F extract (\u00b7) achieved a validation accuracy of 95.93%. The argmax of Y rand in Figure 2 shows class 6 as the most underrepresented with 198 out of 600000 examples. Experiments on KMNIST resulted in F (\u00b7) achieving 94.79% validation accuracy and F extract (\u00b7) achieving 81.18% accuracy. Class 8 had the fewest representatives with 272 examples. On Fashion MNIST, F (\u00b7) achieved 92.16% accuracy and F extract (\u00b7) achieved 75.31% accuracy. The Fashion MNIST dataset had class 7 (sneaker) as the most underrepresented with only 12 examples out of 600000. Common mispredictions included predicting class 5 (sandal) instead of class 7 (sneaker) and class 9 (ankle boot). The notMNIST dataset had a more uniform class distribution, with class 9 (the letter j) having the fewest representatives. Performance using different noise distributions varied, with ISING achieving 98.02% accuracy. Despite this, the model F extract (\u00b7) failed to perform effectively. The extracted model F extract (\u00b7) failed to generalize to the notMNIST validation set, achieving an accuracy of 10.47%. Sampling from the Ising model attained the highest accuracy at 98.02% when evaluating F extract (\u00b7) on the MNIST validation set. This is attributed to the modelling of spatial correlations, lacking in other distributions like uniform, standard normal, and Bernoulli. The model extraction hardness is measured by the ratio of post-extraction validation accuracy to pre-extraction accuracy. Ratios for MNIST, KMNIST, Fashion MNIST, and notMNIST are 0.9687, 0.8564, 0.8171, and 0.1181 respectively. Loss and accuracy exhibit 'phase transitions' with varying \u03b2 values. Losses are minimized around \u03b2 = 0.3, but behavior differs for larger \u03b2 values across datasets. The values of \u03b2 vary across datasets, indicating different spatial correlation distributions. Accuracy peaks at \u03b2 = 0.4 for MNIST, \u03b2 = 0.3 for KMNIST and Fashion MNIST, and \u03b2 = 0.2 for notMNIST. The framework extracts model parameters by training on random impulse response pairs from victim neural network output. Model extractability varies based on the dataset. The framework can measure relative dataset complexity. In experiments, we aim to relate Ising prior temperature to stolen model accuracy using different architectures. We explore an unknown scenario where the attacker uses a fixed architecture with learned weights. Methods for constructing X rand for more uniform distributions over argmax(Y rand) are evaluated for their impact on F extract(\u00b7) performance."
}