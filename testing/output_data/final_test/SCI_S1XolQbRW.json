{
    "title": "S1XolQbRW",
    "content": "Deep neural networks (DNNs) are advancing in various tasks, with a focus on efficiently executing deep models in resource-constrained environments. This paper introduces two new compression methods, quantized distillation and differentiable quantization, to optimize the training process of smaller student networks using distillation from larger teacher networks. The paper introduces two compression methods, quantized distillation and differentiable quantization, to train smaller student networks using distillation from larger teacher networks. Experiments show that quantized shallow students can achieve similar accuracy to full-precision teacher models with significant compression and inference speedup. This enables DNNs in resource-constrained environments to benefit from advances in architecture and accuracy. Neural networks are effective for solving real-world problems like image classification, translation, voice synthesis, and reinforcement learning. Large models are debated for their necessity in achieving good accuracy, with the hypothesis that overcomplete representations enable learning by transforming local minima into saddle points or discovering robust solutions. Large models may not require precise weight values for robust solutions during training. Research on training quantized neural networks and compressing already-trained models shows that neural networks can converge to good solutions even with constrained weights. Various compression techniques have been proposed to reduce the size of deep models significantly while preserving accuracy. Recent research focuses on compressing deep models by reducing weights and speeding up inference. While existing literature mainly explores compression schemes without altering the model structure, distillation has been introduced as a method to transfer model behavior to different structures for compression purposes. However, the student model must be sufficiently large for successful learning, as a shallow or narrow model may result in missed necessary units. In this work, the focus is on leveraging distillation and quantization together for better model compression. The goal is to improve the performance of quantized models using highly-accurate teacher models and to achieve better compression through quantization compared to distillation alone. Two methods are presented to compress models in terms of depth and width by distilling a shallower student network with similar accuracy to a deeper teacher network and quantizing the weights of the student to a limited set. Quantized models leverage distillation loss by quantizing student weights to limited integer levels and using fewer weights per layer. Two methods, quantized distillation and differentiable quantization, are implemented to compress models through training and stochastic gradient descent. Empirical validation on various network architectures shows the effectiveness of quantized shallow students. Our work focuses on quantized shallow students reaching high accuracy levels similar to full-precision and deeper teacher models on various datasets. This approach offers significant compression and inference speedup, with a focus on techniques like knowledge distillation and learning with privileged information. The optimization of quantization points during the learning process is a key aspect of our differentiable quantization method. Our work focuses on using distillation in differentiable quantization to improve the accuracy of quantized models. We refine the idea by matching or even surpassing the accuracy of full-precision models. For example, our 4-bit quantized ResNet18 outperforms the full-precision ResNet18. We define a scaling function to normalize vectors and optimize quantization points during the learning process. The quantization functions involve scaling vectors to values in [0, 1] using a linear scaling function. This approach helps in bucketing and addressing magnitude imbalance in large vectors, improving the quantization process. To address magnitude imbalance in large vectors, bucketing with BID1 is used to apply scaling function separately to consecutive value buckets. This improves quantization accuracy but requires storing two floating-point scaling factors per bucket. The quantization function Q can be defined with uniform or non-uniform placement of quantization points, with uniform quantization considering s + 1 equally spaced points between 0 and 1. The deterministic and stochastic versions of quantization involve assigning vector coordinates to quantization points, with the stochastic version using probabilistic rounding for unbiased estimation. The uniform quantization function with s + 1 levels is defined, and for non-uniform quantization, elements are quantized to the closest quantization points. The deterministic version is defined for non-uniform quantization. The uniform quantization function in neural networks adds a zero-mean error term that is asymptotically normally distributed to the output of each layer before the activation function. This error term's variance depends on the quantization level, connecting quantization to the concept of adding noise for improved performance. The connection between quantization and adding noise to neural networks as a regularizer is explored, along with the strategy of distillation for compressing student models using a trained teacher model. This approach aims to achieve similar accuracy with a shallower and quantized student model. The text discusses the use of model compression via quantization, focusing on transferring knowledge from a teacher to a student using distillation loss. The approach involves using a weighted average between cross entropy with soft targets and cross entropy with correct labels. The implementation includes projected gradient descent to train a quantized neural network, accumulating errors at each projection step for the next iteration. The text discusses model compression through quantization, utilizing distillation loss for efficient learning. It involves projected gradient descent to prevent getting stuck in local minima, using variable bit-width quantization functions and bucketing. This approach differs from BinaryConnect by incorporating distillation and not restricting to binary representation. The text introduces differentiable quantization to improve the accuracy of a quantized neural network by utilizing non-uniform quantization. It discusses the process of computing gradients on the quantized model with respect to distillation loss and focuses on deterministic quantization over stochastic quantization. The text discusses using stochastic gradient descent to find optimal quantization points for minimizing accuracy loss in quantizing neural networks. The challenge lies in the discrete decision of which quantization points to use, leading to zero gradients, requiring the use of the straight-through estimator to address this issue. The model's gradient with respect to quantization points can be computed using a scaling factor \u03b1 i, allowing for optimization with standard SGD algorithm. This process may be slower than training the original network due to additional quantization steps. See Algorithm 2 for more details. Upon close inspection, the method involves differential quantization, which converges faster with fewer iterations compared to standard methods. It utilizes weight sharing through k-means clustering to determine quantization points for clusters, updating them using SGD. The method involves utilizing weight sharing through k-means clustering to determine quantization points for clusters, updating them using SGD. The assignment of weights to centroids never changes, but re-assigning weights to the closest quantization point at every iteration can have drastic effects on the learning process. To avoid issues like degeneracies or reduced diversity, a set of heuristics is relied upon. Future work may explore adding a reinforcement learning loss. Future work will focus on adding a reinforcement learning loss for weight assignment. Starting quantization points can be initialized uniformly or based on weight quantiles. Different layers may require different accuracy levels, determined by the norm of the weight gradient. In future work, reinforcement learning loss for weight assignment will be added. Quantization points can be initialized uniformly or based on weight quantiles. Different layers may need varying accuracy levels based on the weight gradient norm. The algorithm estimates the gradient of weight vectors in each layer and allocates points accordingly, using Huffman encoding to reduce bit complexity. In the context of weight quantization, using distillation loss can optimize the quantized model to mimic the unquantized model's results. Hyperparameter optimization is crucial for achieving the best results. The algorithm involves optimizing with b bits and bucket size of k, considering the space savings and the size of full precision weights. The algorithm optimizes weight quantization by using b bits and bucket size of k, achieving space savings. For example, at 256 bucket size, using 2 bits per component yields 14.2\u00d7 space savings w.r.t. full precision. Huffman encoding can be used for additional space savings. The algorithm optimizes weight quantization by using b bits and bucket size of k, achieving space savings. Huffman encoding can be used for additional space savings by computing the frequency for every index across all the weights of the model and computing the optimal Huffman encoding. The mean bit length of the optimal encoding represents the amount of bits used to encode the values, explaining the presence of fractional bits in some size gain tables. These compression numbers serve as a ballpark figure, with potential additional implementation costs impacting practical savings. In the context of weight quantization optimization, different training techniques were tested on the accuracy of the distilled model for image classification on CIFAR-10. The methods included Quantized Distillation, Differentiable Quantization, and PM quantization with and without bucketing. Results were obtained using a bucket size of 256, showing a good compression-accuracy trade-off. Refer to Appendix A for dataset and model details. The study tested training techniques on the accuracy of a distilled model with varying CNN parameters like quantization levels and model size. Results showed that a student model quantized to 4 bits achieved better accuracy than the teacher, with a compression factor of over 7\u00d7. Differentiable quantization using a wide residual network also led to higher accuracies. Overall, quantized distillation showed the best accuracy across different bit ranges. Quantized distillation is the most accurate method across various bit widths and architectures, outperforming PM significantly for 2bit and 4bit quantization. It achieves accuracy within 0.2% of the teacher at 8 bits on a larger student model, with minor accuracy loss at 4bit quantization. Differentiable quantization is a close second with faster convergence. PM quantization method with bucketing shows good accuracy at higher bit widths. In CIFAR-100 experiments, image classification with 2bit and 4bit quantization on a reduced student architecture also yielded promising results. The results confirm that distilled and differential quantization maintain accuracy within 1% at 4bit precision, but accuracy loss is significant at 2bit precision due to reduced model capacity. Differentiable quantization performs best in recovering accuracy for this challenging task. The OpenNMT integration test dataset consists of 200K train sentences and 10K test sentences for a German-English translation task, using the OpenNMT PyTorch codebase with added quantization algorithms and distillation loss. The study evaluates quantization algorithms and distillation loss using perplexity and BLEU score. Target models include an embedding layer, LSTM encoder and decoder with varying layers, and a linear layer. Results show BLEU scores for student models with different LSTM sizes, distilled models, and quantized versions. Model sizes are detailed in the appendix. Results from the study on quantization algorithms and distillation loss show that recurrent neural networks are not necessarily harder to quantize than convolutional neural networks. Medium and large-sized student models can achieve similar scores as the teacher model. Cell size is crucial for accuracy, but reducing cell size and layers does not lead to significant loss. Additional experiments were conducted on the WMT13 dataset, showing that bucketing PM and quantized distillation perform equally well for 4bit quantization. On the ImageNet dataset, experiments were conducted using ResNet architecture. A wider model, 2xResNet18, was tested to address the 4% accuracy loss when quantizing the standard student model. This wider model doubled the number of filters for each convolutional layer. The quantized distillation technique with 4 bits of precision outperformed the teacher model in terms of BLEU score and had similar perplexity. Previous work on wide ResNet architectures and wider quantized networks was referenced. After experimenting with a 2xResNet18 model using 4-bit quantization, a validation accuracy of 73.31% was achieved after 62 epochs, surpassing the unquantized ResNet18 model. The model is more than 2\u00d7 smaller than ResNet18 but with higher accuracy, and 4\u00d7 smaller than ResNet34. A 4-bit quantized 2xResNet34 student showed similar accuracy to a ResNet50 teacher, with a 2.5\u00d7 smaller size and 50% shallower depth. This approach demonstrates state-of-the-art performance for 4-bit models with 18 layers. The distillation loss is shown to be superior when quantizing models, as demonstrated by higher accuracy in CIFAR-10 and OpenNMT datasets. The 4-bit quantized student achieves better results with distillation loss compared to normal loss, indicating the effectiveness of this metric. See Section A.4.1 in the Appendix for more details. In-depth study of heuristics impact on differentiable quantization found redistributing bits based on gradient norm essential for accuracy. Quantiles and distillation loss also improve accuracy. Shallow students lead to linear decrease in inference cost. For example, in CIFAR-10 experiments, teacher forward pass takes 67.4 seconds, student takes 43.7 seconds, a 1.5x speedup for 1.75x depth reduction. On ImageNet test set, ResNet34 forward pass takes 263 seconds, ResNet18 and student take 169 seconds each. Inference speed for ResNet34 is 263 seconds, ResNet18 and 2xResNet18 take 169 seconds each. The 2xResNet18 model has the same speed as ResNet18 despite having more parameters. Combining distillation and quantization in compressing deep neural networks shows that leveraging large, accurate models through distillation loss is beneficial. Two methods for this are quantized distillation and differentiable quantization. Our experimental results show that distillation and quantization methods can compress models significantly while maintaining accuracy and improving inference speed. Naive uniform quantization with bucketing performs well in various scenarios due to its ability to parametrize noise induced by quantization. Manual architecture search for student model depth and bit width is time-consuming and error-prone. In future work, the potential of reinforcement learning or evolution strategies will be examined to discover the structure of the student model for best performance within space and latency constraints. The immediate direction is to explore the speedup potential of these methods in conjunction with existing compression techniques like weight sharing and low-precision computation frameworks. The CIFAR10 model used is based on BID31 with minor modifications, employing standard data augmentation techniques and a specified learning rate schedule. The experimental models consist of convolutional, dropout, and max pooling layers. The model architecture consists of convolutional, dropout, and max pooling layers, followed by linear layers. Different models are defined with varying numbers and sizes of layers. Dropout layers are not used when training with distillation loss. The model architecture includes convolutional, dropout, and max pooling layers, with different configurations for varying models. Dropout layers are used when training with distillation loss at a temperature of T = 5. Experiments include deeper student models and training for 200 epochs with an initial learning rate of 0.1 using the openNMT-py codebase. In the main text, the openNMT-py codebase is used with modifications for distillation loss and quantization methods. Models are trained with standard options, starting at a learning rate of 1 and halving every epoch where perplexity doesn't drop. Distillation loss is computed with a temperature of T = 1. Positive effects of using distillation loss during quantization are highlighted by comparing models trained with normal loss and distillation loss. The results on CIFAR10 and openNMT integration test datasets show that distillation loss significantly improves the accuracy of quantized models. Quantization works better when combined with distillation, suggesting it should be utilized when quantizing a neural network. Different heuristics were tested on the Smaller model 1 architecture on the cifar10 dataset, with results indicating the importance of heuristics when using 4 bits. Results suggest that using 4 bits in experiments is robust, while redistributing bits based on gradient norm is essential for 2 bits. Quantiles starting point provides a small improvement, but distillation loss is not crucial. The section discusses the uniform quantization function, proving its asymptotic normality. Neural network operations mainly involve scalar product computation. The section discusses the asymptotic normality of the quantized weights and inputs in neural networks operations, using a generalized version of the central limit theorem. The quantization function is proven to be unbiased, leading to the convergence of the error term to a normal random variable. In section 2.1, it is shown that the quantized weights and inputs in neural networks are independent random variables. The Lyapunov condition is proven to hold with \u03b4 = 1."
}