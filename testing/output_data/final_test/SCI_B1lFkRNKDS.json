{
    "title": "B1lFkRNKDS",
    "content": "The convolutional layer in Convolutional Neural Networks (CNNs) extracts local patterns but lacks global context modeling. Recent research focuses on incorporating global context information into local features before feeding them into convolutional layers. However, neuroscience suggests that neurons dynamically modify their functions according to context, which is essential for perceptual tasks. To address this, a novel Context-Gated Convolution (CGC) is proposed to adaptively modify convolutional layer weights based on global context. The proposed Context-Gated Convolution (CGC) adapts convolutional layer weights based on global context, improving CNN performance in various tasks like image classification, action recognition, and machine translation. CNNs excel at capturing local patterns but struggle with global context, limiting their effectiveness. The CGC addresses this issue by modulating convolution kernels to extract representative local patterns and compose discriminative features. The recent research on neuroscience emphasizes the importance of neurons' awareness of global context for interpreting visual scenes and processing complex tasks. Various methods have been proposed to incorporate global context modeling modules into CNN architectures, known as global feature interaction methods. These methods modulate intermediate feature maps by combining global context with local feature representation, improving the overall performance of CNNs. Global feature interaction methods aim to enhance CNNs by incorporating global context modeling modules. These methods reassemble local features based on global correspondence, influencing neurons in two distinct ways: through various forms of attention and adaptive processing. Previous work has focused on modifying intermediate features, but there is potential in explicitly modulating convolution kernels for improved performance. In this paper, the authors propose Context-Gated Convolution (CGC) as a new approach to complement CNNs with global context awareness. CGC modulates convolution kernels based on global context to guide the composition of local features, enabling the capture of representative patterns. Their contributions include introducing the concept of adaptive processors in convolutional layers. The authors introduce Context-Gated Convolution (CGC) to enhance CNNs with global context awareness. CGC modulates convolution kernels based on global context to improve feature composition, enabling better pattern capture. The proposed CGC consists of a Context Encoding Module, Channel Interacting Module, and Gate Decoding Module. It enhances local pattern capture and feature composition, improving performance in tasks like image classification, action recognition, and machine translation with minimal complexity increase. The authors propose Context-Gated Convolution (CGC) to improve CNNs by modulating convolution kernels based on global context. CGC enhances local pattern capture and feature composition, benefiting tasks like image classification, action recognition, and machine translation. In order to address the limitations of standard convolution, the proposal is to incorporate global context information during the convolution process. This involves directly modulating the convolution kernel with a gate generated from a context vector, aiming to improve feature representations. The proposal aims to decompose the gate G into two tensors to reduce complexity in convolutional layers. However, directly generating these tensors with linear layers still poses challenges in modeling channel-wise and spatial interactions. The proposal introduces a novel Context-Gated Convolution (CGC) to incorporate global context information during convolution. It consists of three modules: Context Encoding, Channel Interacting, and Gate Decoding. The CGC aims to model spatial and channel-wise interactions separately to reduce complexity. The Context-Gated Convolution (CGC) introduces a gate G constructed from latent representation C and projected representation O via spatial interaction. A pooling layer reduces spatial resolution before feeding the feature map to the Context Encoding Module, extracting global context information. A linear layer projects the feature map to a latent vector, followed by normalization and activation functions. The weight E is shared across channels, with output dependent on c channels. The Context Encoding Module outputs C \u2208 R c\u00d7d after a linear layer, followed by the Channel Interacting Module projecting to output dimension o. The Gate Decoding Module decodes latent representations using linear layers with shared weights across channels. A gate G is generated from global context to modulate convolutional layer weights for rich context information incorporation. The proposed CGC incorporates rich context information by modulating the kernel to capture representative patterns and compose features of interest. The computational complexity is independent of input spatial size, making it efficient to replace standard convolution. Previous works have dynamically modified convolution operations, but our approach stands out due to its adaptability to global context. Our proposed CGC significantly outperforms Dynamic Convolution (Wu et al., 2019) by leveraging global context awareness. Dynamic filters in previous works generate weights of convolution kernels using features extracted from input images. The proposed CGC dynamically modulates the weight of each convolutional layer, improving global context modeling in CNNs. It incorporates 1D, 2D, and 3D context information in convolutions, enhancing performance in image classification, action recognition, and machine translation tasks. Extensive experiments show consistent performance improvement with negligible parameter increase on benchmark datasets like ImageNet. The performance of modern CNNs is enhanced on benchmark datasets like ImageNet, CIFAR-10, Something-Something, and IWSLT'14 De-En. Experiments are conducted using PyTorch, with Batch Normalization for computer vision tasks and Layer Normalization for natural language processing tasks. Different sets of scaling and shifting factors are learned for specific connections. Average pooling is used with specified dimensions, and only convolution kernels larger than size 1 are replaced. For Point-wise convolutions, linear layers are used without modulation. The Channel Interacting Module uses I \u2208 R 3\u00d764 for the first convolutional layer. Experiments are conducted on ImageNet and CIFAR-10 datasets, with training on ImageNet 2012 set and testing on CIFAR-10 set. Minimum training tricks are applied to isolate the contribution of the proposed CGC. The study involves training and testing models on ImageNet and CIFAR-10 datasets. The proposed CGC significantly improves the performance of baseline models on both datasets. On ImageNet, CGC enhances the accuracy of ResNet-50 by 1.12% with minimal additional parameters and FLOPs. GC-ResNet-50 is difficult to train from scratch, but using the fine-tuning protocol by Cao et al. (2019) can help. The CGC model converges faster and more stably compared to vanilla ResNet-50, thanks to adaptiveness to global context and a gating mechanism that reduces gradients. Ablation studies on CIFAR-10 show the effectiveness of the module design, with the default setting striking a good balance between parameter increment and performance gain. The experiments validate the effectiveness of the default setting in constructing the gate and the necessity of the bottleneck structure. Shared Norm and Two Es techniques are explored for channel interacting, along with variations in feature maps and pooling methods, all supporting the default setting. Different layer configurations are also tested with the CGC model. The experiments validate the effectiveness of the default setting in constructing the gate and the necessity of the bottleneck structure. Different layer configurations are tested with the CGC model, showing that more layers are better. Baseline methods like TSN, P3D-A, and TSM are adapted to evaluate the effectiveness of CGC in incorporating 1D temporal and 3D spatial-temporal context. The Gate Decoding Module generates gates for convolutional layers. The Something-Something (v1) dataset has 86,017 training videos and 11,522 validation videos with 174 categories. Models are trained for 45 epochs with specific learning rates and batch sizes. Performance comparisons show that the CGC significantly enhances baseline CNN models compared to Non-local modules. Non-local blocks modify input feature maps by reassembling local features based on global correspondence, but their performance gain is inconsistent when training from scratch. Our proposed CGC consistently improves the performance of baseline models, including TSM, yielding state-of-the-art results without Kinetics pretraining and with negligible parameter increment. The LightConv model by Wu et al. achieves better performance compared to Transformer, and when augmented with CGC, it outperforms Non-local modules in enhancing CNN models. The input sequence is resized for average pooling, padding shorter sequences with zeros. Global context is defined for the encoder only. Training follows a protocol by Wu et al. with BLEU-4 used for evaluation. Beam width is set to 6 for all models. Performance comparisons show the benefits of replacing Lightweight Convolutions. Replacing Lightweight Convolutions in the encoder of LightConv with our CGC significantly outperforms LightConv and LightConv + Dynamic Encoder by 0.37 and 0.18 BLEU, yielding state-of-the-art performance. Our CGC incorporates global context, improving translation quality, and is more efficient than Dynamic Convolution with only 0.01M extra parameters compared to 30\u00d7 more for Dynamic Convolution. There has been significant effort in enhancing CNNs with context information, categorized into three types: adding backward connections, modifying feature representations with attention mechanisms, and dynamically generating convolution parameters. The effectiveness of feedback mechanisms in CNNs remains unclear, with challenges in training complex updating strategies like the Alternately Updated Clique proposed by Yang et al. (2018). The third category of works focuses on incorporating global context information into CNNs through Context-Gated Convolution (CGC), inspired by neuroscience research on adaptive processors. The proposed Context-Gated Convolution (CGC) incorporates global context information into CNNs by modulating convolution kernels. Three modules efficiently generate a gate to modify the kernel, allowing for extraction of representative local patterns based on global context. Experimental results demonstrate consistent performance improvements across various tasks. Future work includes designing task-specific gating modules to fully utilize CGC's potential. The text discusses the training process for neural networks using data augmentation and specific parameters. It also mentions modifications made to the ResNet-50 model, including the addition of a temporal convolution layer and specific convolutional layer configurations. The text discusses the addition of a pooling layer with specific parameters after the first convolutional layer in the modified ResNet-50 model. It visualizes how the Context-Guided Convolution (CGC) helps capture more informative features by extracting context information. The CGC-ResNet-50 produces feature maps covering more informative regions compared to the vanilla ResNet-50. The proposed Context-Guided Convolution (CGC) refines feature maps by extracting context information from target objects, improving classification accuracy. For instance, CGC correctly identifies a Gold Fish image where Vanilla ResNet-50 misclassifies it as a Sea Slug due to focusing only on fish tails. CGC considers the entire object, guiding convolution with context information. Validation shows CGC uses object context to guide convolution, enhancing classification performance. The Context-Guided Convolution (CGC) successfully extracts class-specific context information and modulates convolution kernels to extract representative features. Visualization shows clear clusters of modulated kernels aligned with classes, with inter-class distances larger than intra-class distances in over 93.99% of cases. This supports the effectiveness of CGC in refining feature maps and improving classification accuracy. Adjusting kernels adaptively benefits correct classification, consistent with neuroscience research motivating the Context-Guided Convolution (CGC)."
}