{
    "title": "BygpAp4Ywr",
    "content": "Recent studies have shown the vulnerability of deep convolutional neural networks to adversarial examples. To address this, a new framework called Embedding Regularized Classifier (ER-Classifier) is proposed to embed high-dimensional input images into a low-dimensional space for classification. This approach improves the robustness of deep neural networks against adversarial attacks, as demonstrated by state-of-the-art performance on benchmark datasets. Deep neural networks (DNNs) have achieved revolutionary successes in various fields but are vulnerable to adversarial examples. These examples can change network predictions without affecting human recognition, raising security concerns. Recent research considers black-box and white-box attack settings. In the white-box attack setting, the attacker can compute the model's gradients and has full access to the model architecture and weights. Recent research shows a relationship between input dimension and neural network vulnerability, suggesting that reducing data dimension can improve network robustness. In the context of white-box attacks, reducing input dimension can enhance neural network robustness. High-dimensional data like images can be embedded in a lower-dimensional space, leading to a defense framework that uses a deep encoder to reduce input dimension and improve model resilience. The Embedding Regularized Classifier (ER-Classifier) framework utilizes distribution regularization in the embedding space to constrain mapping functions from high-dimensional input to low-dimensional space. It introduces a discriminator to enforce the embedding space to follow a prior distribution, enhancing model robustness against white-box attacks. The ER-Classifier framework projects input data to a low-dimensional space for classification, minimizing distribution discrepancies to retain important features. It outperforms other defense methods against adversarial attacks on various datasets. A robust deep neural network framework, ER-Classifier, is proposed to defend against adversarial attacks by projecting input images to a low-dimensional space for classification. The framework minimizes distribution differences to preserve important features, outperforming other state-of-the-art approaches on benchmark image datasets. Our method establishes a connection between Wasserstein distance regularization and deep neural network robustness against adversarial examples. It summarizes related work on attack methods, defense mechanisms, and optimal transport theory in unsupervised learning settings. White-box attacks use network structure and gradients to generate adversarial examples. The fast gradient sign method (FGSM) and other white-box attack methods like C&W and PGD are used to generate adversarial examples by manipulating the gradient of the loss function. C&W attack, proposed by Carlini and Wagner, formulates the generation of adversarial examples as an optimization problem to increase the target class probability and minimize the distance from the original input image. The PGD attack, proposed by Madry et al., updates in the direction that decreases the probability of the original class most and allows direct control of distortion level. It is more flexible than the C&W attack in terms of tuning parameters. Both attacks are commonly used to evaluate defense algorithms' effectiveness, with l \u221e -PGD untargeted attack being the focus in this paper. The algorithm proposed by Moosavi-Dezfooli et al. in 2017 creates a universal perturbation for natural images, leading to misclassification. Various methods aim to enhance deep neural network robustness against adversarial examples, including augmenting training data with adversarial examples, enforcing model properties like smoothness, and removing adversarial perturbations from inputs. Three effective defense methods against adversarial attacks in deep neural networks are adversarial training, random self-ensemble, and Defense-GAN. Adversarial training involves augmenting training data with adversarial examples to increase robustness. Random Self-Ensemble introduces a noise layer that improves network robustness. Defense-GAN leverages GANs for defense against attacks. Defense-GAN and MagNet are two defense methods against adversarial attacks in deep neural networks. Defense-GAN uses GANs to remove adversarial perturbations, while MagNet filters out noise using a generative model. ER-Classifier focuses on dimension reduction for classification, while Defense-GAN and MagNet perform classification in the original dimension space. Defense-GAN has been shown to be more robust than MagNet. In comparison to Defense-GAN, Zhang et al. (2018) and Miyato et al. (2018) propose methods for improving generalization performance through different approaches. Zhang et al. (2018) uses Gaussian Mixture Model regularization and KL-divergence optimization, while Miyato et al. (2018) employs virtual labels to smooth output label distribution for semi-supervised learning. These methods are not specifically designed for defending against adversarial attacks. In contrast to Defense-GAN, Miyato et al. (2018) focus on improving generalization performance rather than defending against adversarial examples. Adversarial examples are shown to be a human phenomenon, with models learning non-robust features that generalize well. Wasserstein distance regularization is proposed to identify robust features. Notations include l \u221e and l 2 distortion metrics for measuring similarity, with probability distributions denoted by capital letters and corresponding densities by lowercase letters. The ER-Classifier defense framework aims to project image data to a low-dimensional space to remove noise and stabilize the classification model by minimizing the optimal transport cost between true label distribution and the output distribution. The encoder and discriminator structures work together to reduce the impact of adversarial perturbations, projecting input data to a lower dimension for classification based on the low-dimensional embedding. The ER-Classifier defense framework utilizes generated codeZ for classification, minimizing Wasserstein distance between true label distribution and output distribution. Kantorovich's distance is induced by optimal transport problem, measuring divergence between probability distributions. The goal is to find a conditional distribution Q(Z|X) with marginal distribution Q Z identical to prior distribution P Z. The ER-Classifier utilizes a prior distribution P Z, with Q Z identical to it. The final objective involves a deterministic or stochastic encoder, with \u03bb > 0 as a hyper-parameter and D as a divergence between Q Z and P Z. A GAN-based framework is used to estimate the divergences, minimizing the 1-Wasserstein distance. Weight clipping is applied to stabilize the training of discriminator D \u03b3. The training algorithm involves mapping input to a low-dimensional space using encoder Q \u03c6, generating code (z), and sampling an ideal code (z) from a prior distribution. Discriminator D \u03b3 distinguishes between the ideal and generated codes, while classifier C \u03c4 predicts image labels. Updates are made using Adam optimization. At inference, only encoder Q \u03c6 and classifier C \u03c4 are used to map input images to low-dimensional space. The ER-Classifier utilizes a low-dimensional space mapping by the encoder to remove adversarial perturbations. The framework is trained with min-max robust optimization and incorporates two Wasserstein distances. Algorithm 1 minimizes the W-distance between the latent embedding distribution and the prior distribution. The ER-Classifier framework, trained with min-max robust optimization, minimizes the second W-distance to ensure focus on classification. Converting a deterministic encoder to stochastic maintains distribution matching. The ER-Classifier framework, trained with min-max robust optimization, minimizes the second W-distance to ensure focus on classification. The proposed regularization helps the encoder identify robust features and preserve global label frequency in the dataset. The framework can be seen as a supervised variant of Generative Adversarial Network or Wasserstein Autoencoder, where a Classifier generates labels from low-dimensional latent embeddings while preserving global label frequency. In the ER-Classifier framework, a Gaussian prior is used for W-distance minimization to shape latent embeddings globally while allowing freedom for class-specific distributions. The classifier determines optimal distributions, and future work includes aligning latent embeddings with robust features. Performance comparison with other defense methods is done on benchmark datasets like MNIST. The dataset consists of 60,000 training images and 10,000 testing images in 28 \u00d7 28 black and white format across ten classes. Various defense methods, including Madry's adversarial training and Random Self-Ensemble, have been proposed to improve neural network robustness. Another method, Defense-GAN, utilizes a generative adversarial network for defense tasks. The Defense-GAN method utilizes a generative adversarial network to model the training data distribution and remove adversarial perturbations. The ER-Classifier is trained with min-max robust optimization to improve adversarial robustness through dimension reduction. The proposed ER-Classifier is compared with Defense-GAN on MNIST for performance evaluation. Defense-GAN and ER-Classifier are compared against l \u221e -PGD untargeted attack. ER-Classifier outperforms Defense-GAN on MNIST, CIFAR10, and Tiny Imagenet, especially on CIFAR10. ER-Classifier's advantage lies in easier regularization of the embedding space without min-max robust optimization. Our novel Wassserstein distance regularization is compared to Defense-GAN, showing ER-Classifier's robustness against l2 \u2264 0.005 threshold attacks. Defense-GAN's evaluation method is based on (Athalye et al., 2018) and code is available on github. ER-Classifier outperforms Defense-GAN on CIFAR10, STL10, and Tiny Imagenet. ER-Classifier, Madry's adversarial training, and Nattack were evaluated on CIFAR10. ER-Classifier's accuracy was 43%, outperforming Madry's adversarial training. The framework includes an encoder and classifier, showing robustness without a discriminator. The study compared the robustness of a framework with only the encoder and classifier (E-CLA) to the ER-Classifier framework. Results showed that ER-Classifier was much more robust on MNIST, CIFAR10, and Tiny Imagenet datasets. However, it was less robust on STL10 due to limited training images and lower resolution. The ER-Classifier is more robust than the E-CLA structure, even with limited training images. Regularization on the embedding space improves adversarial robustness. The performance of E-CLA is similar to models without defense methods on various datasets. VAE-CLA uses Variational auto-encoder for projection and regularization, but does not perform as well as ER-Classifier. The experimental results show that VAE-CLA does not perform as well as ER-Classifier due to difficulties in balancing Kullback-Leibler loss and classification loss. Different prior distributions, including standard Gaussian, Uniform(-3, 3), and Cauchy(0, 1), were tested on MNIST and CIFAR10 datasets. All models were trained without min-max robust optimization, with results shown in Figure 4. The experimental results show that different prior distributions were tested on MNIST and CIFAR10 datasets. Standard Gaussian performed best, with Ding et al. proving that robustness is sensitive to data distribution. Regularizing the embedding space can improve robustness by concentrating the data distribution. Details of hyper-parameter selection, model structure, and code are included in the supplementary part. In this paper, a new defense framework called ER-Classifier is proposed to remove adversarial perturbations by projecting input images to a low-dimensional space. The framework aims to minimize the discrepancy between true label distribution and output distribution, showing robustness on benchmark datasets. Future work will focus on exploring the low-dimensional space to enhance deep neural network robustness. Mathematically, input images are projected to a low-dimensional embedding vector through an encoder, with a discriminator distinguishing between generated and ideal codes, and a classifier performing classification based on the generated code. The ER-Classifier framework minimizes the distribution difference between true labels and framework outputs by mapping latent codes to classification outputs. The density of the output is defined using a standard Gaussian prior distribution. The goal is to minimize the optimal transport cost between the true label distribution and the classifier output distribution. In this paper, the optimal transport theory is used to define the distance between target and model distributions. Traditional divergences like Kullback-Leibler and Jensen-Shannon are not suitable for lower dimensional manifolds, making optimal transport cost more sensible. Kantorovich's distance measures the divergence between probability distributions P(Y) and P(C) using a measurable cost function. The p-Wasserstein distance measures the divergence between probability distributions P(Y) and P(C). To minimize the optimal transport cost between the true label distribution (P(Y)) and the ER-Classifier output distribution (P(C)), a conditional distribution Q(Z|X) is needed with a marginal distribution Q(Z) identical to the prior distribution P(Z). Theorem 1 states that optimizing over the objective is equivalent to minimizing the discrepancy between the true label distribution (P(Y)) and the ER-Classifier output distribution (P(C)). The core idea of the paper is to summarize high-dimensional data in a lower-dimensional space without losing important classification features. The ER-Classifier objective involves minimizing the discrepancy between true label distribution (P(Y)) and output distribution (P(C)). A penalty term is added to relax the constraint on Q(Z), with the final objective incorporating a hyper-parameter \u03bb and an arbitrary divergence D between Q(Z) and P(Z). A GAN-based framework is used to estimate divergences, with the 1-Wasserstein distance providing more stable training and better results. The paper aims to summarize high-dimensional data in a lower-dimensional space using the ER-Classifier objective, which minimizes the discrepancy between true label distribution and output distribution. The framework utilizes the weight clipping method from Wasserstein GAN to stabilize training. The proof of Theorem 1 is adapted from previous work, considering joint probability distributions of input images, framework outputs, and latent codes. The joint distributions and couplings between values are defined, with encoding and generating distributions playing key roles. The paper introduces the ER-Classifier objective for summarizing high-dimensional data in a lower-dimensional space. The proof of Theorem 1 focuses on joint probability distributions of input images, framework outputs, and latent codes, emphasizing the role of encoding and generating distributions. Theorem 1 demonstrates how to factor the distribution through Z, particularly when P C (U |Z) are Dirac measures. The paper introduces the ER-Classifier objective for summarizing high-dimensional data in a lower-dimensional space. The proof of Theorem 1 focuses on joint probability distributions of input images, framework outputs, and latent codes, emphasizing the role of encoding and generating distributions. Based on the definition, P(X,Z) does not depend on the choice of conditional distributions P(U|Z). The tower rule of expectation and conditional independence properties are used to prove Theorem 1, which applies to the non-deterministic case as well. The upper bound on the Wasserstein distance between ground-truth and predicted label distributions is derived under certain assumptions. The ER-Classifier objective focuses on embedding high-dimensional data into a lower-dimensional space. The dimension of the embedding space is a crucial hyper-parameter, as it affects the extraction of useful information and noise levels. The intrinsic dimension of each image dataset is calculated using maximum likelihood estimation, guiding the selection of the embedding dimension. The sample size for calculating intrinsic dimension is 1,000, and it is not significantly influenced by changing the sample size. Based on intrinsic dimension calculations by Levina & Bickel (2005), different values were tested to evaluate models against l \u221e -PGD attack. Models were trained without min-max robust optimization, and results are shown in Figure 5. The final embedding dimension was chosen based on robustness, parameters, and testing accuracy without attack. The embedding dimension close to the intrinsic dimension usually performed better, except for MNIST, a simple handwritten digit dataset. Epsilon is an important hyper-parameter. In adversarial training, epsilon ( ) is a crucial hyper-parameter. Experiment results show that = 0.3, 0.03, 0.03 are used for Madry's adversarial training on MNIST, CIFAR10, and STL10 respectively, while = 0.01 is used for Tiny Imagenet. The comparison between Encoder+Classifier structure (E-CLA) and ER-Classifier embeddings on various datasets is done without min-max robust optimization. Testing data is embedded using the encoder, projected to 2-D space, and adversarial images are generated using l \u221e -PGD attack. The adversarial embedding is visualized in 2-D space for E-CLA and ER-Classifier using l \u221e -PGD attack. Misclassified points are marked as \"down triangle\" and correctly classified points as \"point\". E-CLA shows good separation of classes on legitimate images, but under attack, some points mix together. ER-Classifier can generate good separated embeddings on both legitimate and adversarial images, while E-CLA fails to do so. Code for reproduction will be available on github. Pseudocode for training ER-Classifier is provided in Listing 1."
}