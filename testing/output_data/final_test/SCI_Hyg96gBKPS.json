{
    "title": "Hyg96gBKPS",
    "content": "In this paper, a new attention mechanism called Monotonic Multihead Attention (MMA) is proposed for simultaneous machine translation. MMA introduces the monotonic attention mechanism to multihead attention and includes novel approaches for latency control. The study demonstrates improved latency-quality tradeoffs compared to the previous state-of-the-art approach MILk. Code will be released upon publication. Simultaneous machine translation enables real-time translation, useful for live video captions and multilingual conversations. Unlike traditional models, simultaneous models start translating before reading the entire sentence, using learned policies like monotonic attention mechanisms. Recent research explores variants like hard monotonic attention for improved performance. Recent research in simultaneous machine translation has introduced various attention mechanisms, including hard monotonic attention, monotonic chunkwise attention (MoChA), and monotonic infinite lookback attention (MILk). MILk has shown superior quality/latency trade-offs compared to fixed policy approaches like wait-k or wait-if-* policies. MILk outperforms hard monotonic attention and MoChA by computing a softmax attention over all previous encoder states, leading to improved latency-quality trade-offs. However, these attention mechanisms, including MILk, are built on RNN-based models, which have been surpassed by Transformer models in recent advancements. The recent state-of-the-art Transformer model has outperformed previous models in machine translation. A new monotonic multihead attention (MMA) approach is proposed, with variants MMA-H and MMA-IL. Two novel latency regularization methods are also introduced. MMA-H is designed for streaming systems with limited attention span, while MMA-IL focuses on translation system quality. The main contribution is the novel monotonic attention mechanism enabling improved translation quality. The curr_chunk introduces a novel monotonic multihead attention mechanism for online decoding in the Transformer model, showing better latency-quality tradeoffs compared to previous models on translation benchmarks. It includes analyses on attention span control and the relationship between head speed and layer. The model design is motivated by an ablation study on decoder layers and heads, with strategies for latency and coverage control. The curr_chunk introduces the hard monotonic attention mechanism for online linear time decoding in RNN-based encoder-decoder models. It explains the process of generating a target sequence with a Bernoulli selection probability for moving forward or staying at the current position. The hard monotonic attention mechanism in RNN-based encoder-decoder models uses a Bernoulli selection probability for generating target sequences. It achieves online linear time decoding but limits the decoder to attending to only one encoder state, potentially impacting translation quality and reordering. The model lacks a mechanism to adjust latency based on different decoding requirements. Monotonic Chunkwise Attention (MoChA) addresses the lack of latency adjustment in decoding by allowing the decoder to apply attention to a fixed-length subsequence of encoder states. Alternatively, Monotonic Infinite Lookback Attention (MILk) enables the decoder to access encoder states from the beginning of the source sequence. The proposed approach explores the power of the Transformer model's multihead attention module, which has become the state-of-the-art for machine translation. Monotonic Multihead Attention (MMA) combines multihead attention with monotonic attention for low latency. Multihead attention allows each decoder layer to compute different attention distributions using queries Q, keys K, and values V. The Transformer model uses multihead attention in the Encoder, Decoder, and Encoder-Decoder attention. Monotonic Multihead Attention (MMA) combines multihead attention with monotonic attention for low latency in the Transformer model. Each decoder layer has separate encoder-decoder attention, with each head operating as a separate monotonic attention. Two types of MMA are investigated: MMA-H(ard) and MMA-IL(infinite lookback), each with specific calculation methods for expected alignment and attention. Monotonic Multihead Attention (MMA) combines multihead attention with monotonic attention for low latency in the Transformer model. MMA-H hardattends to one encoder state per head, while MMA-IL can attend to all previous encoder states. MMA-IL allows more information leverage for translation, while MMA-H may be better for efficient streaming systems. The models use unidirectional encoders for simultaneous translation. The decoding strategy involves sampling processes and applying hard alignment or partial softmax attention from encoder states to generate tokens. Monotonic Multihead Attention (MMA) combines multihead attention with monotonic attention for low latency in the Transformer model. The MMA model allows each head to adjust its speed on-the-fly, with some heads reading new inputs while others retain source history information. Even with the hard alignment variant (MMA-H), the model can preserve history information by setting heads to past states. In contrast, the hard monotonic model loses previous information at the attention layer. Effective simultaneous machine translation must balance quality and latency. The MMA model allows each head to adjust its speed on-the-fly for low latency in simultaneous translation. Latency measures how many source tokens the model reads before generating a translation. The fastest head determines overall latency, with the possibility of maximum latency if a head continuously reads new input without producing output. MMA-IL and MMA-H have different attention behaviors, with MMA-IL providing maximum information at the end of a sentence, while MMA-H only gives a hard alignment to the end-of-sentence token. The MMA model allows each head to adjust its speed for low latency in translation. MMA-H attention head may stay at the beginning of a sentence, degrading model quality. Two latency control methods are introduced to address these issues. The MMA-H model introduces latency control methods to adjust head speeds for low translation latency. A weighted average latency loss is used, but outliers skipping tokens require additional control. Head divergence loss is introduced to address outliers and control expected delays. The head divergence loss is introduced to control the range of attention heads and manage latency in the MMA-H model. Quality is evaluated using tokenized BLEU for IWSLT15 En-Vi and detokenized BLEU for WMT15 De-En, while latency is measured using Average Proportion, Average Lagging, and Differentiable Average Lagging metrics. The study evaluates the method on machine translation datasets IWSLT14 En-Vi and WMT15 De-En, using tokenization with Moses tokenizer and replacing low-frequency words. Different validation and test sets are used for each dataset, with settings followed from previous studies. Byte pair encoding is applied to construct a shared vocabulary, and evaluation is done on MMA-H and MMA-IL models. The study evaluates MMA-H and MMA-IL models on newstest2013 and newstest2015 datasets. The MILK model is evaluated on IWSLT15 En-Vi, using unidirectional encoders and greedy decoding. Latency metrics are computed on BPE tokens for WMT15 De-En and on word tokens for IWSLT15 En-Vi. In this section, the main results of the model implementation using the Fairseq library are presented. The study analyzes the impact of variance loss on attention span, the influence of decoder layers and heads on quality and latency, and provides a case study on attention head behavior. Additionally, the relationship between attention head rank and layer is studied, with quality-latency curves plotted for MMA-H and MMA-IL models. For MMA-H and MMA-IL models, quality-latency curves were plotted. Differentiable average lagging was used to set latency range. Models showed better translation quality at given latency, with MMA-H having a slight quality gain as latency decreases. Larger latency was due to outlier attention heads, skipping source information. Attention variance loss was introduced to eliminate outliers. In subsection 2.3, the attention variance loss was introduced to MMA-H to prevent outlier attention heads from affecting latency or attention span. The model represents a step towards handling longer inputs without segmentation, as both encoder and decoder self-attention have infinite lookback. The effectiveness of this method on latency was evaluated in subsection 4.1. The difference between the fastest and slowest heads at each decoding step was also measured. The average attention span is reduced with increasing L var. MMA aims to adapt the Transformer model for online decoding. Testing the effect of separate attention layers and multihead attention on quality and latency. Quality measured by detokenized BLEU and latency by DAL on the WMT13 validation set. Lambda values set for MMA-IL and MMA-H models. The offline model benefits from multiple decoder layers, with the best performance seen with 3 layers and 2 heads. The MMA-IL model performs best with 6 layers and 4 heads, while MMA-H shows improved performance with more heads, reaching the best results with 16 heads in 6 layers. Performance generally improves with an increase in effective heads up to a certain point. The introduction of the MMA model is motivated by the observation that the performance of effective attention heads plateaus or degrades after a certain point. Latency also increases with the number of effective attention heads, requiring adjustments in loss weights to control latency. Attention behaviors are characterized through examples of MMA-H and MMA-IL models, showing how attention heads behave differently based on the order of source and target tokens. The MMA model introduces a new approach to attention heads behavior in translation models. It shows that MMA-IL model generates better translations by retaining history information and having a near-diagonal trajectory for streaming inputs. This behavior leads to improved translation quality compared to MMA-H model. The MMA-IL model outperforms MMA-H in translating \"isolate the victim.\" In MMA-IL, lower layers have higher rank heads, making them slower. MMA-H shows smaller rank differences and frequent head order changes. Recent work on simultaneous translation includes rule-based policies like WaitIf-* and wait-k for decoding. In the second category, flexible policies are learned from data. Various approaches include using Markov chains, reinforcement learning, and continuous rewards policy gradient for online alignments in machine translation and speech recognition. New methods such as hard alignment with variational inference and restricted dynamic oracle are also proposed for online decoding. In the context of online decoding for machine translation and speech recognition, Zheng et al. (2019b) introduce a restricted dynamic oracle and restricted imitation learning for simultaneous translation. They propose two variants of the monotonic multihead attention model to improve translation quality. By introducing new targeted loss terms, we control latency and attention span in a simultaneous machine translation attention model. This approach improves quality-latency trade-offs using the Transformer architecture. Ablation studies support the efficacy of our method, aiming to enhance real-time interpretation. Adjusting system latency at inference time is possible without retraining models, as shown in pilot experiments on the IWSLT15 En-Vi dataset. In pilot experiments on the IWSLT15 En-Vi dataset, we found that using a weighted average loss in MMA-H models did not reduce latency effectively and negatively impacted translation quality. However, our proposed divergence loss in Equation 16 efficiently reduced latency while maintaining good translation quality."
}