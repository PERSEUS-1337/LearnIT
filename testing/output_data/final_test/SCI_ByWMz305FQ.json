{
    "title": "ByWMz305FQ",
    "content": "Multilingual Neural Machine Translation (NMT) systems struggle with zero-shot translation, where they translate between language pairs not seen during training. To address this, explicit language invariance losses are proposed to improve NMT encoder performance. These strategies enhance zero-shot translation on various language pairs and match the quality of pivoting approaches for the first time. Neural models in machine translation have improved translation quality and reduced the need for pipelined components. Multilingual NMT models can translate between multiple languages, enhancing translation quality over bilingual models. Multilingual NMT models improve translation quality over bilingual models by sharing parameters between encoder and decoder, aiming to learn a universal representation for translating multiple languages. This shared representation should enable reasonable performance on zero-shot translation, offering practical benefits like reduced latency costs. The curr_chunk discusses the importance of enabling zero-shot translation with sufficient quality to simplify translation systems and move towards a single multilingual model. The paper aims to bridge the quality gap in zero-shot translation and proposes a multilingual NMT model with two training objectives. The curr_chunk discusses the invariance loss imposed on encoder representations for translation training. It highlights the correlation between language representation separation and zero-shot translation performance, drawing from domain adaptation literature. In this work, auxiliary losses are developed to align encoder representations of different languages with English in a shared subspace, improving zero-shot translation performance. Experiments show significant enhancements in zero-shot translation on WMT English-French-German and IWSLT 2017 shared task without regression. The study introduces auxiliary losses to align encoder representations of various languages with English in a shared space, enhancing zero-shot translation performance on WMT English-French-German and IWSLT 2017 shared task. The analysis shows that explicit alignment losses encourage the use of shared representations, leading to improved generalization in multilingual neural machine translation. In modern NMT, sequence-to-sequence models with attention mechanism are used to learn the distribution p(y|x; \u03b8). The model consists of an encoder and a decoder parameterized with \u03b8 enc and \u03b8 dec respectively. In multilingual training, a single model is trained to translate between multiple source and target languages. Special tokens are used to indicate the target language, and the model is trained on parallel data for supervised translation directions. For zero-shot translation to be effective, the encoder must generate language invariant features. Previous methods share weights between encoders, decoders, or attentions across languages to encourage clustering of translated sentences in a common representation space. However, training solely on end-to-end translation may lead to overfitting and language-specific encoder representations. To improve zero-shot translation, regularizers are used to align encoder representations of different languages with English as a pivot. This alignment loss is controlled by a hyper-parameter \u03bb in the loss function. Target risk is bounded by source risk and a discrepancy metric between source and target domains, viewing translation as domain adaptation. The text discusses minimizing the discrepancy between source and target feature distributions for zero-shot translation by optimizing a domain adversarial loss. Unlike previous methods, the focus is on aligning encoder representations of all languages to English. The discriminator used is a binary predictor independent of the number of languages being trained on. The text discusses using a feed-forward network as a discriminator to align encoder representations in zero-shot translation. Adversarial approaches aim to align marginal distributions but are challenging to optimize and prone to mode collapse. More complex discriminators considering sequential nature may be more effective, but are not explored in this work. In zero-shot translation, aligning encoder representations using paired data is more effective than aligning marginal distributions. Various similarity measures like Euclidean distance and cosine distance can be used to improve similarity between equivalent sentences in different languages. In zero-shot translation, aligning encoder representations using paired data is more effective than aligning marginal distributions. Various similarity measures like Euclidean distance and cosine distance can be used to improve similarity between equivalent sentences in different languages. To compare sentences, a bag-of-words assumption is made and the pooled representation is aligned. Max pooling and minimizing cosine distance between parallel sentence representations work well. A multilingual model with a single encoder and decoder is used as a baseline, trained on translation loss first and then fine-tuned with alignment losses. In zero-shot translation, aligning encoder representations using paired data is more effective than aligning marginal distributions. Various similarity measures like Euclidean distance and cosine distance can be used to improve similarity between equivalent sentences in different languages. To compare sentences, a bag-of-words assumption is made and the pooled representation is aligned. Max pooling and minimizing cosine distance between parallel sentence representations work well. A multilingual model with a single encoder and decoder is used as a baseline, trained on translation loss first and then fine-tuned with alignment losses. We compare the performance of the baseline model against aligned models on supervised and zero-shot translation directions, as well as against pivoting performance. Training models on standard datasets, preprocessing data, oversampling German portion, and applying BPE to obtain subwords are part of the process. Newstest-2012 and newstest-2013 are used as dev and test sets, respectively. In experiments with Transformers BID36, the model is trained on parallel sets newstest-2012 and newstest-2013. Training includes a learning rate of 1.0, 4000 warmup steps, and synchronized training with 16 Tesla P100 GPUs for 500k steps. The model is directed to translate input sentences into different languages using a unique < tl > token. For alignment experiments, a pre-trained multilingual model is fine-tuned on alignment and translation losses. Adversarial alignment involves a feedforward network with 3 hidden layers and a tuned \u03bb value. The model with 3 hidden layers of dimension 2048 using leaky ReLU nonlinearity was fine-tuned with SGD and achieved convergence within a few thousand updates. Zero-shot results showed significant improvements in translation quality for both directions, closing the gap to the strong pivoting baseline. No significant differences were observed between the two alignment methods, and these improvements came at no additional cost to quality. The proposed alignment methods showed improvements in translation quality without affecting the supervised directions. The adversarial regularizer was sensitive to initialization and hyper-parameters, while the cosine distance loss was more stable. Analyzing the outputs of the multilingual model revealed issues with zero-shot performance, which were addressed by an explicit alignment loss. While investigating the high variance of zero-shot translation scores during multilingual training, it was found that many examples were not being translated into the target language. Instead, they were either translated to English or copied. This issue stemmed from the fact that German and French source sentences were always translated into English during training, leading to improper attribution of the target language. Changing the token at test time was ineffective. The number of sentences in each language was counted using an automatic language identification tool, with results shown in Table 2. The results in Table 2 show little to no code-switching in the output tokens of a given sentence. The decoder bias towards emitting tokens in one language is explained as a cascading effect. Explicit alignment removes target language information from source token representations, allowing more control over translation direction. Table 3 displays BLEU scores for examples predicted in the correct language. The system's gains are isolated to improvements in transferable features and decoding to the desired language. The baseline model shows strong performance in zero-shot translation, with improvements in quality attributed to representation alignment in the adapted model. The study aims to assess the cross-lingual representations learned by a multilingual translation model. By comparing decoder outputs for different languages, the functional equivalence of representations is evaluated during various training stages. The baseline model demonstrates effective zero-shot translation, with quality enhancements linked to representation alignment in the adapted model. The study evaluates cross-lingual representations in a multilingual translation model. Results show that incremental training with alignment losses improves language-agnostic representation, enhancing zero-shot performance. The model aligns encoder representations during training but stagnates without external incentives. Scaling experiments to multiple languages show promising results. The study tests scalability with multiple languages using the IWSLT-17 dataset. They focus on English to/from other languages for training, using transformer base architecture with specific parameters. The baseline model's performance on IWSLT-17 is similar to bridging, possibly due to the multi-way parallel training data. The study demonstrates improved performance over the baseline model on the IWSLT-17 dataset by utilizing multi-way parallel training data and shared representations with English sentences as pivots. Previous works have explored multilingual NMT models for zero-shot translation, with varying success in achieving this goal. BID32 proposes a novel method to modulate sharing between languages in a multilingual NMT system by using a parameter generator. They achieve higher zero-shot translation scores with this approach. Various approaches have been explored for learning coordinated representations with parallel data, including auto-encoder networks and paired feed-forward networks with different similarity objectives. Adversarial approaches have also been proposed to learn domain invariant representations. The curr_chunk discusses the use of aligned parallel data to learn shared representations in crosslingual or multilingual settings. Different methods, such as word level alignments, document level alignment, and sentence level alignments, are utilized to obtain cross-lingual representations. Recent work has shown that representations learned by multilingual NMT systems are widely applicable across tasks and languages. Parameter sharing based approaches have also been explored in this context. In the context of multilingual NMT systems, recent work explores using aligned parallel data to learn shared representations for improved translation quality. While parameter sharing approaches have been tried, enforcing a shared latent space is not crucial, and cycle consistency loss alone suffices. This work proposes explicit alignment losses as an additional constraint to enhance zero-shot translation performance, comparable to strong pivoting based approaches. Our proposed alignment losses enhance zero-shot translation performance, comparable to strong pivoting based approaches, while maintaining performance on supervised directions. The methods have been reliably tested on WMT English-French-German and IWSLT 2017 datasets."
}