{
    "title": "HylTXn0qYX",
    "content": "We present an algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks in two-layer ReLU networks. The algorithm categorizes parameter space into regions based on data points, reducing computation to one convex quadratic program per hidden node. In the benign case, a single equality constrained QP is solved exponentially fast with projected gradient descent. In the bad case, a few more inequality constrained QPs are solved with manageable time complexity. Our algorithm efficiently solves inequality constrained QPs, with exponential time complexity only in the number of constraints. Experimental results show that our approach is efficient in most cases. The success of deep neural networks has sparked interest in their optimization, despite the NP-hardness of training to global optimality. Bridging the gap between theory and practice in neural network optimization is an active research area. Many studies have explored why optimization works well for neural networks by analyzing the loss surface and the role of gradient-based methods. Convex optimization offers an optimality test for termination, providing a certificate of approximate optimality. In contrast, practitioners in deep learning often lack good termination criteria, relying on running first-order methods for a fixed number of epochs. The solutions obtained at the end of training neural networks may not be global or local minima due to lack of good termination criteria. Nonlinear neural networks pose challenges for global optimality tests, especially with ReLU activation functions. Nondifferentiability at t=0 in ReLU networks complicates analysis, making precise understanding elusive. The empirical risk of a ReLU network is continuous but piecewise differentiable, with nondifferentiable points on affine hyperplanes. Critical points occur on nonsmooth boundaries, and analyzing such points is challenging. Previous works have not addressed these nonsmooth points comprehensively. In this paper, a theoretical algorithm is presented to test second-order stationarity for nondifferentiable points of the empirical risk of one-hidden-layer ReLU networks. The algorithm determines if a point is a local minimum, a second-order stationary point, or provides a descent direction for function value decrease. This helps in understanding critical points on nonsmooth boundaries of ReLU networks. The algorithm presented in this paper tests second-order stationarity for nondifferentiable points in the empirical risk of one-hidden-layer ReLU networks. It determines if a point is a local minimum, a second-order stationary point, or provides a descent direction for function value decrease, aiding in understanding critical points on nonsmooth boundaries of ReLU networks. The algorithm allows for a mix of first and second-order methods, enabling the escape from nonsmooth second-order saddle points. The algorithm presented in the paper addresses the challenge of nondifferentiable points in ReLU networks, specifically focusing on boundary data points that cause input 0 to ReLU hidden nodes. These points divide the parameter space into regions with different loss surface slopes, leading to nondifferentiability. The algorithm efficiently handles these boundary data points using polyhedral geometry and addresses the complexity of solving nonconvex QPs for second-order tests. The algorithm in the paper addresses the challenge of nondifferentiable points in ReLU networks, focusing on boundary data points causing input 0 to ReLU hidden nodes. It efficiently handles these points using polyhedral geometry and solves nonconvex QPs for second-order tests. The algorithm involves solving nonconvex QPs efficiently, despite their NP-hard nature. The loss function R is assumed to be twice differentiable and convex in w, with no d x + 1 data points lying on the same affine hyperplane. This section discusses difficulties at nondifferentiable points and proposes solutions without using advanced tools from nonsmooth analysis. The behavior of data points at the boundary is crucial due to the activation function's nondifferentiability at 0. The loss function R is assumed to be twice differentiable and convex in w, with no data points lying on the same affine hyperplane. The behavior of data points at the boundary is crucial due to the activation function's nondifferentiability at 0. When a point is on the boundary, the slope depends on the direction of perturbation, leading to nondifferentiability. Boundary data points bisect the space of perturbations into two halfspaces, and having M boundary data points can lead to 2M regions. There is a way to address this issue without testing all 2M regions separately. The text discusses testing local minimality or stationarity for each region using second-order optimality conditions. It explains a Taylor-like expansion for perturbations and how the gradient and Hessian depend on the direction of perturbation. The space of perturbations is divided into at most 2M regions, and the approach efficiently solves all 2M problems. The algorithm efficiently tests local optimality conditions for each region using second-order expansion and necessary/sufficient conditions. The process involves three steps: testing first-order stationarity, executing tests sequentially, and identifying failure points. The algorithm efficiently tests local optimality conditions for each region using second-order expansion and necessary/sufficient conditions. The tests are executed from Step (a) to (c), with a focus on first-order stationarity and efficient testing methods using convex QPs and linear programs. The algorithm aims to identify failure points and terminate when a test fails, returning a strict descent direction \u03b7. The algorithm efficiently tests local optimality conditions using second-order expansion and necessary/sufficient conditions. In Step (b), flat extreme rays are recorded for later use in Step (c). In Step (c), the algorithm tests if the second-order perturbation can be negative for directions where a specific constraint is met. The algorithm solves constrained nonconvex QPs, with different complexities depending on the presence of flat extreme rays. Despite the NP-hardness of general QPs, the algorithm proves that the specific form of QPs used are tractable in most cases. In this section, a more precise notion of generalized stationary points is defined, introducing additional symbols to streamline the algorithm description. The Clarke subdifferential is used due to dealing with nondifferentiable points in nonconvex R. The algorithm efficiently tests local optimality conditions using second-order expansion and necessary/sufficient conditions. The Clarke differential of f at point z * in \u2126 is Lipschitz and differentiable in \u2126 \\ W. The perturbed output \u1ef8 (x) of the network with perturbation dY (x) can be expressed using the Jacobian matrix J(x). The matrix J(x) is diagonal, with entries determined by the derivative of the hidden layer function h. The text discusses defining terms for linear and quadratic perturbations in a neural network algorithm. Special treatment is given to boundary data points in the hidden layer, with a focus on the subspace spanned by certain vectors. Parameters and perturbations are also defined in matrix form. The text presents algorithms for checking if a tuple of parameters is a SOSP or a local minimum in a neural network. Lemmas are provided to show the implications of certain assumptions, with proofs in the appendix. The perturbed empirical risk is analyzed for first and second-order perturbations. The algorithm SOSP-CHECK tests for local minimality in a neural network by checking first-order stationarity, positivity of a function, and specific conditions. If a descent direction is found, the algorithm terminates and returns the direction. The full algorithm and proof of correctness are provided in the appendix. The algorithm SOSP-CHECK tests for local minimality in a neural network by checking first-order stationarity, positivity of a function, and specific conditions. Testing for W2 R and b2 R involves checking if they are singletons with zero, with more details in Appendix A.1.1. Testing for W1 and b1 is more complex due to dependencies on \u22061 and \u03b41. A convex QP can be solved to test for DISPLAYFORM0 ,\u00b7 R, and if the objective value is not zero, a descent direction can be returned. Further details can be found in FO-SUBDIFF-ZERO-TEST (Algorithm 3) and Appendix A.1.2. The feasible set of LP (3) has d x + 1 linearly independent constraints, forming a pointed polyhedral cone with extreme rays. Checking nonnegativity of the objective function for all extreme rays suffices to prove local minimality in a neural network. The algorithm does not solve the LPs but checks the extreme rays for computational efficiency. The feasible set of LP (3) has d x + 1 linearly independent constraints, forming a pointed polyhedral cone with extreme rays. Testing extreme rays can be done with a single inequality test instead of multiple separate tests, greatly reducing computation. FO-INCREASING-TEST computes all possible extreme rays and checks if they satisfy certain conditions. If an extreme ray does not satisfy the inequality, it is a descent direction. If the inequality holds with equality, it is a flat extreme ray that needs further testing. The presence of flat extreme rays introduces inequality constraints in the QP, making it harder to solve. Lemma A.1 in Appendix A.2 outlines the conditions for flat extreme rays. The second-order test checks for \"flat\" \u03b7's satisfying g(z, \u03b7) T \u03b7 = 0. This is done using the SO-TEST function (Algorithm 5) to solve the QP with equality constraints. The constraints in the current chunk ensure that the feasible sets of QPs are {\u03b7 | g(z, \u03b7)T \u03b7 = 0}. If there are flat extreme rays, additional QPs with equality and inequality constraints need to be solved. Efficiency of solving specific QPs (4) is highlighted despite general QPs being NP-hard. Three termination conditions are identified for QP (4) with feasible set S. ECQPs can be solved iteratively using projected gradient descent, as shown in Lemma 3. The efficiency of solving specific QPs is emphasized, with three termination conditions identified. The proof for the convergence rate is deferred to Appendix B.3, and the computation time for updates is O(p^2). The ICQP can be solved in polynomial time as long as the number of flat extreme rays is small. Lemma 4 introduces a method to check conditions (T1)-(T3) in O(p^3 + r^(3/2)). The method checks conditions (T1)-(T3) in O(p^3 + r^(3/2)) time by transforming \u03b7 and using results in copositive matrices. The proof is in Appendix B.4. SOSP-CHECK returns immediately if any QP terminates with (T3), concludes with \"SOSP\" if any terminates with (T2), and returns \"Local Minimum\" if all terminate with (T1). Experiments used artificial datasets and trained ReLU networks with squared error loss. To approach nondifferentiable points, Adam was used with full-batch gradient for 200,000 iterations and decaying step size. The decaying step size led to descending deeper into the valley. The number of approximate boundary data points satisfying certain conditions was counted to estimate M k. The QP was solved for these points to check for (approximate) FOSPs. Optimal values were close to zero after solving, and the number of s * i 's ending with 0 or 1 estimated L - K. Through experiments with different settings, it was observed that there are many boundary data points (M) but usually very few flat extreme rays (L). This suggests that many local minima are on nondifferentiable points, making the analysis meaningful. Additionally, L is usually very small, indicating that only ECQPs (L = 0) or ICQPs with very small values need to be solved. The text discusses the efficiency of solving ECQPs or ICQPs with small inequality constraints when dealing with boundary data points in model dimensions and training sets. The algorithm tests second-order stationarity and escapes saddle points for empirical risk of shallow ReLU-like networks. The computation is reduced to d_h convex QPs, O(M) equality/inequality tests, and a nonconvex QP, which can be efficiently solved with projected gradient descent. The text discusses the efficiency of solving ECQPs or ICQPs with small inequality constraints, suggesting that the algorithm can be efficiently implemented in most cases. However, the algorithm needs to be extended to handle points close to nondifferentiable ones and to check for approximate second-order stationarity. Implementing these extensions requires significant additional work, leaving practical implementation for future work. Additionally, extending the test to deeper neural networks is considered an interesting future direction. The text discusses the efficiency of solving ECQPs or ICQPs with small inequality constraints, suggesting that the algorithm can be efficiently implemented in most cases. However, the algorithm needs to be extended to handle points close to nondifferentiable ones and to check for approximate second-order stationarity. Implementing these extensions requires significant additional work, leaving practical implementation for future work. Additionally, extending the test to deeper neural networks is considered an interesting future direction. In this section, the detailed operation of SOSP-CHECK (Algorithm 2) and its helper functions are presented. Lemmas 1 and 2 state that M k := |B k | \u2264 d x and vectors {x i } i\u2208B k are linearly independent. The algorithm tests for first-order stationarity for W 2 and b 2, checking if \u2202 W2 R = {0 dy\u00d7d h } and \u2202 b2 R = {0 dy }. The text discusses testing for first-order stationarity for W 1 and b 1 in the context of solving ECQPs or ICQPs with small inequality constraints. It involves handling boundary data points and checking for zero in the subdifferential to determine descent directions. The algorithm aims to disprove local minima by scaling \u03b3 sufficiently small. The text discusses solving convex QPs and testing for local minimality by perturbing the subdifferential. It involves defining LPs for polyhedral cones and checking for zero minimum values to disprove local minima. The text discusses solving convex QPs and testing for local minimality by perturbing the subdifferential. It involves defining LPs for polyhedral cones and checking extreme rays for computational efficiency. The text discusses solving convex QPs and testing for local minimality by perturbing the subdifferential. Line 3 of Algorithm 4 computes extreme rays, shared by polyhedral cones. If a direction is a descent direction, it is returned with True. Line 13 of Algorithm 2 uses the direction for perturbations to find a point with a lower value. If a direction satisfies certain conditions, the sign of boundary data points is added for future use in the second-order test. The text discusses solving convex QPs and testing for local minimality by perturbing the subdifferential. Line 3 of Algorithm 4 computes extreme rays, shared by polyhedral cones. If a direction is a descent direction, it is returned with True. Line 13 of Algorithm 2 uses the direction for perturbations to find a point with a lower value. If a direction satisfies certain conditions, the sign of boundary data points is added for future use in the second-order test. The operation with S i,k will be explained in detail in Appendix A.3. The strength of our approach is that by solving the QPs (2), connections to the well-known KKT conditions are provided. The approach involves solving QPs to compute Lagrange multipliers for subproblems efficiently. The second-order test checks for feasibility using specific conditions. The proof of Lemma A.2 is provided in Appendix B.5, showing that FOSP always has a rank-deficient Hessian. The equality constraints in the QP force specific values for certain variables. The eigenvalue constraints on u k 's and v k 's force \u03b7 to be orthogonal to the loss-invariant directions. Feasible sets defined by equality/inequality constraints correspond to regions where specific variables have fixed values. There are d h + M \u2212 L equality constraints and L inequality constraints in each nonconvex QP, all linearly independent. The approach involves solving QPs with specific conditions to efficiently compute Lagrange multipliers for subproblems. The algorithm efficiently solves QPs with flat extreme rays, reducing the number of QPs needed compared to the naive approach. It returns two booleans and a perturbation tuple, indicating the presence of a descent direction or lack of a solution. The algorithm efficiently solves QPs with flat extreme rays, reducing the number of QPs needed. It returns two booleans and a perturbation tuple, indicating a descent direction or lack of a solution. If all QPs terminate with a unique minimum at zero, \"Local Next, assume for the sake of contradiction that the data points are linearly dependent, implying they are on the same affine space. The algorithm efficiently solves QPs with flat extreme rays, reducing the number of QPs needed. It returns two booleans and a perturbation tuple, indicating a descent direction or lack of a solution. If all QPs terminate with a unique minimum at zero. Points are on the same affine space, contradicting Assumption 2.B.2. PROOF OF LEMMA 2: (w, y) is twice differentiable and convex in w. Taylor expansion at (Y (x i ), y i ) where the first-order term. In each divided region of \u03b7, J(x i ) stays constant, making g(z, \u03b7) and H(z, \u03b7) piece-wise constant functions. Parameter space is partitioned into polyhedral cones, with orthonormal basis of row(A) and R p. Orthogonal matrix W with columns w 1 , w 2 , . . . , w p. Submatrix \u0174 with columns w q+1 , . . . , w p. I \u2212 A T (AA T. The algorithm efficiently solves QPs with flat extreme rays, reducing the number of QPs needed. It returns two booleans and a perturbation tuple, indicating a descent direction or lack of a solution. If all QPs terminate with a unique minimum at zero. Points are on the same affine space, contradicting Assumption 2.B.2. The Pareto spectrum of Q can be calculated by computing eigensystems of all 2 r \u2212 1 possible Q J, which takes O(r 3 2 r) time in total, and from this we can determine whether a symmetric Q is copositive. The algorithm efficiently solves QPs with flat extreme rays, reducing the number of QPs needed. It returns two booleans and a perturbation tuple, indicating a descent direction or lack of a solution. If all QPs terminate with a unique minimum at zero. Points are on the same affine space, contradicting Assumption 2.B.2. The Pareto spectrum of Q can be calculated by computing eigensystems of all 2 r \u2212 1 possible Q J, which takes O(r 3 2 r) time in total, and from this we can determine whether a symmetric Q is copositive. To prove Lemma 4, we transform \u03b7 to eliminate equality constraints and obtain an inequality constrained problem. By testing positive definiteness and copositivity of matrices, we can determine the category the QP falls into. Testing in total is done in O(p 3 + r 3 2 r) time. Before stating the results from Martin & Jacobson (1981), assume without loss of generality that B = B1B2 where B1 \u2208 Rr\u00d7r is invertible. Define another change of variables and state the lemmas."
}