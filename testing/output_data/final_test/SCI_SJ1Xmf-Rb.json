{
    "title": "SJ1Xmf-Rb",
    "content": "FearNet is a generative model for incremental class learning that does not store previous examples, making it memory efficient. It uses a brain-inspired dual-memory system for memory consolidation and a module inspired by the basolateral amygdala for determining which memory system to use. FearNet achieves state-of-the-art performance in incremental class learning on image and audio classification benchmarks. FearNet is a brain-inspired system for incremental learning that outperforms previous methods in dealing with catastrophic forgetting in deep neural networks. An ideal incremental learning system would assimilate new information without storing the entire training dataset, crucial for real-time operation on embedded platforms like smart toys and robots. FearNet, an incremental learning framework, addresses this need by using less storage and computational power. FearNet is an incremental learning framework with three brain-inspired sub-systems for memory storage and recall. It uses pseudorehearsal to consolidate recent memories into long-term storage, mitigating catastrophic forgetting. The framework consists of modules based on mPFC, HC, and BLA for memory recall decisions during incremental class learning sessions. The learner receives a batch of data with labeled training samples at time t. The data is not assumed to be independent and identically distributed. The learner can store information from prior study sessions in its memory. FearNet's architecture includes three neural networks inspired by different brain regions for memory storage and recall. FearNet utilizes a dual-memory model inspired by different brain regions for memory storage and recall. It employs a generative autoencoder for pseudorehearsal during sleep to prevent catastrophic forgetting. The model achieves state-of-the-art results on large datasets with a small memory footprint, addressing the plasticity-stability dilemma in DNNs. French argued that catastrophic forgetting in neural networks can be mitigated by having two separate memory centers: one for long-term storage of older memories and another for quickly processing new information. Mixing old examples with new examples during training can help reduce catastrophic forgetting by simulating iid conditions. Rehearsal and pseudorehearsal are methods to reduce catastrophic forgetting in neural networks. Rehearsal involves mixing examples from previous study sessions, while pseudorehearsal generates new examples for a given class instead of replaying past training data. This approach was introduced to address the inefficiency of storing all training examples. Recently, there has been renewed interest in solving catastrophic forgetting in supervised learning. New methods are designed to mitigate forgetting when each study session contains a permuted version of the entire training dataset. PathNet uses an evolutionary algorithm to find the optimal path through a large DNN and freezes the weights along that path. It assumes all classes are seen in each study session. Elastic Weight Consolidation (EWC) redirects plasticity to less important weights after each study session, but struggles with incremental class learning. Fixed Expansion Layer (FEL) model prevents catastrophic forgetting by using sparse updates and connectivity constraints in the second hidden layer. This limits dense shared representations and reduces the risk of forgetting. GeppNet is a new approach for incremental learning that uses a self-organizing map (SOM) to reorganize input onto a two-dimensional lattice, serving as long-term memory for classification. It prevents forgetting older data too quickly by only updating the SOM when input is novel. GeppNet also incorporates rehearsal with previous training data and a variant, GeppNet+STM, uses a fixed-size memory buffer for storing novel examples. GeppNet+STM uses a fixed-size memory buffer to store novel examples and trains during consolidation phases. iCaRL BID33 is an incremental class learning framework that updates a DNN using study session data and stored examples. After learning the dataset, iCaRL retains a set number of exemplars per class. iCaRL retains a set number of exemplars per class in its dataset, and uses a DNN to compute embeddings for each example. The mean embedding for each class is then computed. To classify a new instance, the DNN computes an embedding and assigns it to the class with the nearest mean embedding. iCaRL's performance is influenced by the number of examples stored. FearNet is inspired by the dual-memory model of mammalian memory, where HC and mPFC operate as complementary memory systems. GeppNet is a DNN based on this theory, explored independently in the 1990s. The dual-memory model suggests that the hippocampus (HC) is responsible for recent memories, while the medial prefrontal cortex (mPFC) is responsible for long-term memories. HC facilitates new memories through adult neurogenesis, while mPFC plays a role in memory consolidation during REM sleep. HC may reactivate recent memories during sleep to prevent forgetting, causing them to replay in mPFC, possibly leading to dreams. Recent research by BID25 suggests that memories are transferred from the hippocampus (HC) to the medial prefrontal cortex (mPFC) during sleep, potentially leading to dreams. BID25 conducted contextual fear conditioning experiments in mice, finding that the amygdala (BLA) shifts where it retrieves memories from (HC or mPFC) as memories are consolidated over time. FearNet, based on BID25's theory, has a short-term memory system in HC for recent recall and a deep neural network (DNN) in mPFC for storing remote memories. Additionally, FearNet includes a separate BLA network that determines memory retrieval. FearNet has a short-term memory system in the hippocampus (HC) for recent recall and a deep neural network (DNN) in the medial prefrontal cortex (mPFC) for storing remote memories. FearNet uses a generative model during sleep to consolidate data from HC to mPFC through pseudorehearsal. The HC model computes class conditional probabilities using stored training examples, which are then consolidated into mPFC. The mPFC in FearNet is implemented using a DNN trained for reconstructing input and computing P mP F C (C = k|x). It is responsible for long-term memory storage, while the BLA is used for memory recall. Pseudorehearsal is used, and the loss function for mPFC includes supervised classification and unsupervised reconstruction losses. The number of hidden units in layer j, h encoder, (i,j) and h decoder, (i,j) are the outputs of the encoder/decoder at layer j respectively, and \u03bb j is the reconstruction weight for that layer. mPFC is similar to a Ladder Network BID32, combining classification and reconstruction for regularization, especially in low-shot learning. The \u03bb j hyperparameters are empirically determined, prioritizing the reconstruction task to generate realistic pseudo-examples. During training, data in HC is passed through the encoder to extract dense feature representations, computing mean feature vectors and covariance matrices for each class. These are used to generate pseudo-examples during consolidation. During FearNet's sleep phase, original inputs stored in HC are transferred to mPFC using intrinsic replay with pseudo-examples created by an autoencoder. Pseudo-examples for each class are generated by sampling a Gaussian distribution and passed through the decoder for fine-tuning mPFC using backpropagation. After consolidation, all units in HC are deleted. FearNet utilizes the BLA network during prediction. During prediction, FearNet uses the BLA network to determine whether to classify an input using HC or mPFC. The output of BLA is a value between 0 and 1, with 1 indicating mPFC should be used. Combining BLA's output with those of mPFC and HC improved results. The predicted class is computed using the probability of the class according to HC weighted by the confidence that the associated memory is stored in HC. The BLA network determines whether to use HC or mPFC during prediction. It has the same number of layers as the mPFC encoder and uses a logistic output unit. Incremental learning performance is evaluated using three metrics proposed in BID23. The model's ability to retain base-knowledge and recall new information is measured after each study session. The model's ability to recall new information is measured by \u2126 values relative to an offline MLP model, allowing for better comparison across datasets. Evaluation is done on CIFAR-100, CUB-200, and AudioSet datasets, each with specific characteristics. The dataset used for evaluation includes a 100 class subset from AudioSet, with ResNet-50 image embeddings for CIFAR-100 and CUB-200, and audio CNN embeddings for AudioSet from pre-training on YouTube-8M dataset. FearNet is compared to other models like FEL, GeppNet, iCaRL, and one-nearest neighbor. FearNet is compared to FEL, GeppNet, GeppNet+STM, iCaRL, and 1-NN for incremental class learning. iCaRL was modified to a fully connected network for the experiments. FearNet was implemented in Tensorflow for mPFC and BLA. FearNet was implemented in Tensorflow for mPFC and BLA, using exponential linear unit activation functions and Xavier initialization for weight layers. mPFC and BLA were trained using NAdam, with mPFC trained on base-knowledge set for 1,000 epochs and BLA for 20 epochs. Hyperparameter search was conducted for model shape and depth, with mPFC and BLA performing best with two hidden layers. In preliminary experiments, adding weight decay to mPFC did not provide any benefit, likely due to the regularization effect of the reconstruction task. Each class is only seen in one unique study-session, with the first baseknowledge session containing half the classes. FearNet achieves the best performance in terms of \u2126 base and \u2126 all on all three datasets, closely resembling the offline MLP baseline. FearNet achieves high test accuracy on new classes but forgets quickly compared to FEL. 1-NN is memory inefficient and slow. Offline MLP normalization yields 69.9% for CIFAR-100. FearNet performance varies with memory location. BLA novelty detection evaluation is conducted. FearNet's BLA shows good performance in predicting which network to use, but there is a decrease in \u2126 new, indicating that it sometimes uses the wrong network. Sleep deprivation impairs new learning and causes forgetting during sleep. FearNet's mPFC weights are perturbed during sleep, leading to gradual forgetting of older memories. Sleeping less deteriorates HC's recall performance. FearNet's multi-modal incremental learning experiment results are shown in Table 4. FearNet's multi-modal incremental learning experiment results are shown in Table 4, where it incrementally learns and retains information from different datasets, such as CIFAR-100 and AudioSet. The performance of FearNet is evaluated for three training paradigms, including learning each dataset separately as base knowledge and then incrementally learning the other dataset, as well as a combined approach with a 50/50 split. FearNet's performance in multi-modal incremental learning is influenced by the size of base-knowledge, with better performance observed with a larger base-knowledge. The model struggles to learn new information incrementally if starting with lower base-knowledge. Detailed plots are available in the Supplemental Material. FearNet's mPFC is trained to discriminate and generate new examples, enabling pseudorehearsal and robustness to catastrophic forgetting. Unsupervised networks like FearNet are slower to forget old information due to the lack of target outputs. Memory requirements for training CIFAR-100 models are shown in Table 5. Table 5 displays memory requirements for models in Sec. 6.1 learning CIFAR-100 and a hypothetical 1,000 classes. FearNet's small memory footprint is due to storing class statistics instead of raw training data, making it suitable for deployment. An open question is how to handle storage and updating of class statistics across study sessions. One approach is using a running update for class means and covariances, favoring data from the most recent session. FearNet assumes the mPFC encoder output is normally distributed per class, suggesting potential use of a Gaussian Mixture Model for more complex class modeling. FearNet's model can be enhanced with a Gaussian Mixture Model for more complex class modeling. Pseudorehearsal strengthens weights associated with randomly generated vectors. FearNet's memory size is impacted by storing covariance matrices for each class. A variant using a diagonal \u03a3 c shows degraded performance but still functions. FearNet can adapt to unsupervised learning and regression tasks by adjusting its mPFC's loss function and grouping input feature vectors. FearNet is a brain-inspired framework that excels in incremental class learning on image and audio benchmarks. It can recall and consolidate recently learned information while retaining old data efficiently. Future work includes integrating BLA directly into the model. Future work for FearNet includes integrating BLA directly into the model, replacing HC with a semi-parametric model, learning feature embedding from raw inputs, and replacing the pseudorehearsal mechanism with a more memory-efficient generative model. Model hyperparameters and training parameters for FearNet and the iCaRL framework are also discussed. The text discusses experimentation with regularization strategies to improve accuracy in a neural network model. It also explores outlier detection algorithms for a classifier model that determines whether to use recent or remote memory for predictions. In the experimentation, three outlier detection algorithms were used to set a rejection criterion for test samples. Performance of different BLA variants was compared, and pseudocode for FearNet's training and prediction algorithms was provided. The multi-modal learning experiment involved three base-knowledge scenarios with FearNet showing good performance when adequately learning the data. FearNet's performance in the multi-modal experiment is influenced by the base-knowledge it learns. When FearNet learns the base-knowledge well, performance is good; however, poor learning leads to deteriorating performance in incremental learning. The size of the base-knowledge has no effect on the model's ability to recall new information immediately, but there is a slight decrease in performance in some cases. Overall, there is an increase in performance in all scenarios. In some cases, mPFC sees an increase in performance due to fewer sleep phases perturbing older memories."
}