{
    "title": "rylDfnCqF7",
    "content": "The variational autoencoder (VAE) combines deep latent variable models with variational learning techniques. VAEs use a neural inference network to approximate the model's posterior on latent variables, optimizing a lower bound on data likelihood. However, VAE training can lead to \"posterior collapse,\" where the model ignores latent variables. This paper explores posterior collapse in VAE training dynamics, suggesting a simple modification to address this issue. The paper proposes a modification to VAE training to reduce inference lag by optimizing the inference network based on the mutual information between latent variables and observations before each model update. This approach effectively prevents posterior collapse without adding complexity. Empirically, it outperforms autoregressive baselines on text and image benchmarks in terms of likelihood and is competitive with more complex techniques while being faster."
}