{
    "title": "HygtHnR5tQ",
    "content": "We propose a framework for extreme learned image compression using Generative Adversarial Networks (GANs) to achieve visually pleasing images at lower bitrates. Our method includes a generator/decoder trained with a multi-scale discriminator and can synthesize unimportant regions from a semantic label map. User study shows our approach is preferred over state-of-the-art methods at low bitrates. Deep compression systems, like BID6 and BID33, are competitive with modern codecs such as WebP and JPEG2000. They can be adapted to specific domains like stereo or medical images, offering efficient processing and indexing directly from compressed representations. However, these systems are optimized for traditional distortion metrics like PSNR and MS-SSIM, which lose significance at very low bitrates. In this paper, a generative adversarial network (GAN)-based framework is proposed for extreme image compression, targeting bitrates below 0.1 bpp. The focus is on developing new training objectives beyond traditional metrics like PSNR and MS-SSIM, with the use of adversarial losses to capture global semantic information and local texture for high-resolution image generation. Our generator/decoder operates on full-resolution images and is trained with a multi-scale discriminator. Two modes of operation are considered: generative compression (GC) preserves overall image content while generating structure of different scales, and selective generative compression (SC) generates parts of the image from a semantic label map while preserving user-defined regions with detail. GC does not require semantic label maps. It is useful in bandwidth-constrained scenarios to preserve image quality. Selective generative compression (SC) preserves image details by generating content from a semantic label map, reducing the size of compressed images significantly. This approach is useful in scenarios where preserving specific regions with detail is important, such as in video calls. Generative Compression (GC) system produces visually appealing results compared to BPG and AEDC compression algorithms. GC models outperform BPG by using fewer bits on Kodak and RAISE1K BID11 datasets, and even on Cityscapes BID9 dataset for street scene images. This is the first evidence of a deep compression method surpassing BPG on the Kodak dataset. Deep compression method outperforms BPG on Kodak dataset in user study with large margins. System combines preserved image content with synthesized content, achieving over 50% bitrate reduction without notable image quality degradation. Popular DNN architectures for image compression include auto-encoders and recurrent neural networks. Deep compression systems rely on context models to reduce coding rates. Generative adversarial networks (GANs) have become popular for learning generative models in an unsupervised manner, despite stability issues. They can generate realistic images at high resolutions. Conditional GANs have also shown impressive results in image-to-image translation tasks. Conditional GANs have shown impressive results in image-to-image translation tasks, reaching resolutions as high as 1024 \u00d7 2048px. Related work uses adversarial loss for deep compression systems, but focuses on suppressing artifacts rather than generating image content. Other approaches use GAN frameworks for generative models and compression artifact removal. Generative Adversarial Networks (GANs) can approximate data distribution through a generator trained with a discriminator using a mini-max objective. The original paper uses the \"Vanilla GAN\" objective with specific scalar functions. Generative Adversarial Networks (GANs) aim to approximate data distribution by training a generator with a discriminator using a mini-max objective. The Jensen-Shannon Divergence is minimized between the distribution of x and G(z). Different choices of f and g can minimize various f-divergences, with the Least Squares GAN being one option. Conditional Generative Adversarial Networks (cGANs) model the conditional distribution p(x|s) when additional information s is given. Deep Image Compression involves using a generator G(z, s) and discriminator D(z, s) with access to side information s. An encoder E maps an image x to a latent feature map w, quantized to L levels to obtain a representation \u0175. The decoder G then reconstructs the image from \u0175. The trade-off between reconstruction quality and bitrate is optimized using a loss function d. The proposed GAN framework for extreme image compression combines GANs and learned compression. It involves an encoder E and quantizer q to encode the image x into a compressed representation \u0175. This representation can be concatenated with noise v from a fixed prior p v to form the latent vector. The weight \u03b2 controls the model's bitrate, with the option to set \u03b2 = 0 and adjust L and/or dim(\u0175) through the architecture of E to control the maximum bitrate. The proposed GAN framework for extreme image compression combines GANs and learned compression. It involves an encoder E and quantizer q to encode the image x into a compressed representation \u0175, optionally concatenated with noise v from a fixed prior p v to form the latent vector z. The decoder/generator G then generates an image x = G(z) consistent with the image distribution p x while recovering the encoded image x to a certain degree. The saddle-point objective for generative compression balances distortion, GAN loss, and entropy terms. The discriminator computes the same f divergence L GAN, allowing for compression of real images without generating completely new ones. The proposed GAN framework for extreme image compression involves an encoder E and quantizer q to encode the image x into a compressed representation \u0175. By constraining the entropy of \u0175, the encoder and generator need to balance the GAN objective and the distortion term to make the generated image look \"realistic\" while preserving the original image. The proposed GAN framework for extreme image compression involves an encoder E and quantizer q to encode the image x into a compressed representation \u0175. In this setting, z is random and independent of x, and the objective reduces to a standard GAN plus the distortion term, acting as a regularizer. Generative compression (GC) balances reconstruction and generation automatically over the image, and can be extended to a conditional case with additional semantic label map information. Selective generative compression (SC) involves guiding the network on which regions to preserve and synthesize using a binary heatmap. This setting differs from the automatic trade-off between generation and preservation in generative compression (GC). Selective generative compression (SC) involves guiding the network on which regions to preserve and synthesize using a binary heatmap. The fully synthesized regions have the same semantics as the original image, which are separately stored and fed through a feature extractor before being input to the generator. The network is guided with the semantics by masking the distortion only over the preserved regions and zeroing out the compressed representation in the regions to be synthesized. This approach greatly reduces the bitrate needed to store the compressed representation, resulting in significant bitrate savings. Selective generative compression (SC) involves guiding the network on which regions to preserve and synthesize using a binary heatmap. Two different training modes are considered: Random instance (RI) and random box (RB). The architecture for the encoder E and generator G is based on a global generator network. The entropy term \u03b2H(\u0175) is simplified for bitrate control. The upper bound bitrate for GC with 2 channels is 0.0181bpp. The actual entropy of H(\u0175) is generally smaller due to the learned distribution not being uniform or i.i.d. An arithmetic encoder is used to encode the channels of \u0175 to a bit-stream. Additional features (s) are fed to D, resulting in a bitrate of 0.035bpp and 21.8dB BPG. The GC network produces sharper images compared to a baseline model on Cityscapes. The GC network outperforms BPG and MSE baseline in reconstructing Cityscapes with sharper and more realistic textures, despite lower PSNR. Domain-specific training alone is insufficient for sharp reconstructions at low bitrates. Context modeling and adaptive arithmetic encoding reduce bitrates by 8.8%, with further reductions possible through post-processing or joint training. Distortion term uses MSE with coefficient \u03bb = 10, along with feature matching and VGG perceptual losses. The feature matching and VGG perceptual losses are used in the GC network for image synthesis from semantic label maps. These losses are not masked in SC to stabilize the GAN. The GC models are trained on diverse natural images from the Open Images dataset and evaluated on the Kodak dataset. Training with semantic label maps on the Cityscapes dataset is also explored for improved performance. The proposed SC method is evaluated using Cityscapes dataset and compared to BPG and AEDC network. AEDC network is trained on Cityscapes with early stopping to prevent overfitting. The model achieves a bitrate of 0.07 bpp and slightly better MS-SSIM than BPG. The model achieves a bitrate of 0.07 bpp and slightly better MS-SSIM than BPG. Investigating the effect of the GAN term in the total loss, a baseline model with MSE loss only is trained. In extreme compression, common quality measures like PSNR and MS-SSIM lose significance as they penalize local structure changes. The study compares reconstructions from GC models with MSE baseline and BPG, focusing on texture variance rather than visual quality. User study on perceptual quality includes GC models with different configurations trained on Open Images and Cityscapes, alongside BPG at various rates. Questionnaires present reconstructions side by side for comparison. The study compares reconstructions from GC models with MSE baseline and BPG, focusing on texture variance. User study on perceptual quality includes GC models with different configurations trained on Open Images and Cityscapes, alongside BPG at various rates. Questionnaires present reconstructions for comparison, with 20 users rating each pair. Mean preference scores and standard errors are reported for each pairing of methods. Only users correctly identifying the original image in probing comparisons are considered for preference percentage computation. All images used in the user studies will be released for future comparisons. The semantic quality of SC models is evaluated by measuring their capacity to preserve image semantics and blend them with the preserved regions. Mean intersection-over-union (IoU) is computed between label maps for decompressed images and ground truth. Generative Compression models are compared to BPG in terms of preference percentage. The mean preference percentage of GC models compared to BPG is shown on Kodak, RAISE1K, and Cityscapes datasets. Visual comparisons with BPG are provided in validation images. User study results for GC models on different datasets are presented, with details on training methods and error computation. The study compares GC models to BPG on Kodak, RAISE1K, and Cityscapes datasets. GC models outperform BPG even when BPG uses significantly more bits. The GC models produce images with finer detail and fewer artifacts compared to BPG. The study compares GC models to BPG on Kodak, RAISE1K, and Cityscapes datasets. GC models outperform BPG in reconstructing texture in natural objects. BPG requires more bits at extreme bitrates, with gains being maximal. State-of-the-art compression methods are discussed in comparison to GC and BPG. The study compares GC models to BPG on Kodak, RAISE1K, and Cityscapes datasets, with GC models outperforming BPG in reconstructing texture in natural objects. State-of-the-art compression methods, including BID33, are discussed in terms of classical metrics like PSNR and MS-SSIM on the Kodak dataset. Notably, adaptive arithmetic coding using context models for improved compression performance is employed by all methods except BPG, Rippel et al., and Minnen et al. Additional savings of 10% were achieved with the implementation of context models. Comparisons with Rippel et al. and Minnen et al. were limited due to the release of only a selection of decoded images at higher bitrates. Qualitative comparisons with BID33 were made in Figs. 12-14 in the Appendix. In the Appendix, qualitative comparisons were made with BID33, showing that our models produce images of comparable or better quality despite using fewer bits. Our results were also compared to BPG, which remains visually competitive with moderate bitrate savings. Our proposed system offers significant advancements in visually pleasing compression. The proposed system represents a significant advancement in visually pleasing compression at extreme bitrates. The compressed representations are explored by sampling the latent space, resulting in a \"soup of image patches\" reflecting the trained domain. The low dimensionality of the latent space makes it interesting to learn the true distribution, leading to an experiment with an improved Wasserstein GAN trained on the extracted latent space. The study utilized a ResNet architecture to extract BID14 from Cityscapes, generating sharp 1024 \u00d7 512px images with a powerful generative model. Results showed that feeding the MSE baseline with uniform and learned code samples produced noisier and blurrier images compared to the GC network. Mean IoU was compared for different networks on the Cityscapes validation set, showcasing the effectiveness of the GC and SC networks over the MSE baseline. The pix2pixHD baseline BID43 was also trained from scratch. The pix2pixHD baseline BID43 was trained from scratch for 50 epochs using 1024 \u00d7 512px training images. The SC networks preserve semantics better than pix2pixHD, generating texture from label maps faithfully. Cityscapes validation images produced by the SC network trained in the RI mode with C = 8 showcase different semantic classes. Additional visual results for SC networks on Cityscapes can be found in Appendix F.7. Our SC networks generate texture from label maps faithfully and seamlessly merge preserved and generated image content. They lead to significant reductions in bpp compared to baseline methods, with visual quality sometimes surpassing BPG at the same bitrate. The SC networks can generate high-quality images from semantic label maps, with better visual quality than BPG in some cases. Despite limitations in complex object synthesis, as GAN technology improves, so will SC networks. The semantic label map incurs a small overhead for downscaled images but becomes negligible as image size increases. Our GAN-based compression framework outperforms previous methods for low bitrates in terms of visual quality. Our research focuses on compression of natural images, specifically street scene images, to achieve storage savings. We aim to combine synthesized and preserved image content for even larger savings. Future work includes developing mechanisms for better preservation of faces and determining regions to preserve using saliency information. Additionally, combining our compression approach with GANs shows promise in learning high-resolution generative models. In our research on compressing natural images, we use an arithmetic encoder to store frequencies for each channel separately and achieve smaller bitrates compared to the upper bound. The semantic label map for SC is compressed by quantizing coordinates and encoding them relative to preceding coordinates. The encoder E convolutionally processes the image x and label map s into a feature map, which is then projected down to C channels. The generator processes the feature map w by quantizing it over L centers to obtain the discrete\u0175, projecting it up to 960 channels, and processing it with 9 residual units. The feature extractor for SC processes the semantic map s down to the spatial dimension of \u0175, which is concatenated for generation. The encoder downscales by 8\u00d7 instead of 16\u00d7, resulting in dim(\u0175) = W /8 \u00d7 H /8 \u00d7 C. The discriminator D uses a multi-scale architecture to measure the divergence between p. The architecture of BID43 for discriminator D measures divergence between p x and p G(z) locally and globally. Encoder GC and encoders SC process inputs for generator/decoder. Generator/decoder structure includes multiple layers for processing input data. E is the encoder for image x and semantic label map s, with quantization layer q. Subsampled heatmap is used for spatial bit allocation. The generator/decoder G produces the decompressed image x, while the discriminator D is used for adversarial training. Features are extracted by F from the input. The networks are trained using the ADAM optimizer with a learning rate of 0.0002 and a mini-batch size of 1. Training iterations are 150000 on Cityscapes and 280000 on Open Images. Instance normalization is used for normalization, with fixed batch statistics implemented in the second half of Open Images training to reduce artifacts and color shift. GC models are trained for compression of natural images using 200k randomly sampled images from the Open Images dataset. The training images for the GC models are rescaled to 768px on the longer side and high saturation images are discarded. The models are evaluated on the Kodak image compression dataset and 20 images from the RAISE1K dataset. The GC models are trained using 200k randomly sampled images from the Open Images dataset. The benefits of training GC models with semantic label maps on the Cityscapes dataset are evaluated on 20 validation images. The SC method requires semantic label maps for training and deployment, using the same preprocessing as GC. Visual comparisons of GC models with BID33 are provided in the following sections. In Section F.4 and F.5, visual comparisons of GC models with BID33 on a subset of images from the Kodak dataset are presented. Section F.6 showcases visualizations of the latent representation of GC models, while Section F.7 provides additional visual results for SC. Our model shows comparable output to BID33 with fewer bits, producing smoother lines and better textures in various elements. The text discusses training a model with C = 4 for MSE and generative compression on Cityscapes, showing differences in decoded images. They experiment with learning the distribution of \u0175 = E(x) using an improved Wasserstein GAN. Results include sharper images obtained by the SC network using semantic label maps estimated from the input image via PSPNet. Additional results for discussion with reviewers are provided for easy reference. In Table 2, PSNR is computed on the Cityscapes test set by varying the entropy constraint. Results show that relaxing the constraint allows the network to optimize the distortion term for higher PSNR. Optimizing for MSE only leads to superior PSNR but blurry images. Turning off distortion losses causes the network to output repetitive textures, indicating the importance of distortion losses for training stability. Loss curves for training the model on OpenImages BID24 are shown in FIG2. In experiments on Cityscapes and OpenImages datasets, loss fluctuates due to small batch size, but smoothed losses remain stable. Different models show varying entropy values and PSNR results. The effect of GAN loss, distortion losses, and entropy constraint on PSNR is analyzed. Convergence plots for distortion losses in training the GC model on OpenImages are shown."
}