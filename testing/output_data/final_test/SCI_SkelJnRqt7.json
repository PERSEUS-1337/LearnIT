{
    "title": "SkelJnRqt7",
    "content": "Neural Egg Separation is a new method for separating mixed distributions in machine learning and signal processing. It addresses the challenge of extracting an unobserved distribution mixed with a signal from an observed distribution. The method outperforms current approaches and includes GLO Masking for a good initialization. Our method, Neural Egg Separation, outperforms current approaches in separating mixed distributions. Humans excel at separating mixed signals, while artificial intelligence struggles with this task. Understanding mixed signals is crucial as they are common in nature. Previous work focused on different degrees of supervision, with full supervision being one approach. The work focuses on full supervision in learning to separate mixed signals, where the learner has access to a training set with ground truth sources. This strong supervision allows direct learning of the mapping from the mixed signal to its sources. However, obtaining such supervision is often unrealistic, as it requires manual separation of mixed signals. The fully supervised setting does not allow clean extraction of signals that cannot be observed in isolation, like music of a street performer or car engine noises. The learner has access to a training set with mixed signal samples but not the ground truth sources. This setting is more realistic than full supervision and involves learning to separate synthetic mixtures of randomly sampled sources. Another scenario is no supervision, where the learner only has access to mixed signal samples without the sources. In the semi-supervised setting, the task is to unmix signals from a mixture where one signal comes from an unobserved distribution X and the other from an observed distribution B. The learner has access to a training set with clean samples from B and mixed samples from Y, labeled to indicate their origin. The goal is to learn a parametric function for separation. Neural Egg Separation (NES) is a method for learning a parametric function to separate mixed signals into sources from unobserved distribution X and observed distribution B. It involves iterative estimation of X samples, synthesis of mixed signals, and training a separation regressor. GLO Masking (GLOM) is introduced to provide NES with a strong initialization by training two deep generators using GLO. Our method utilizes GLO to train two deep generators for modeling observed and unobserved sources (B and X). GLOM initialization is crucial for strongly correlated X and B, like in separating musical instruments. It outperforms adversarial methods in effectiveness. Experiments across various domains validate our method's superiority over current approaches with the same level of supervision. Our semi-supervised method competes well with fully supervised methods, requiring minimal assumptions and lightweight supervision for single-channel source separation. Unsupervised single-channel source separation methods like ICA, RPCA, and HMM with temporal priors have been researched. Supervised methods use learned dictionaries or neural networks for signal separation. Some techniques utilize RNNs for long audio signals, while others focus on short audio clips. Generative Adversarial Source Separation is another direction using adversarial training. Adversarial Masking (AM) method is introduced for semi-supervised audio source separation, addressing correlated sources and mixture collapse issues. Compared to non-adversarial methods, AM outperforms NMF in experiments. This method significantly improves source separation performance. Generative models like GAN, VAE, and GLO are used for source separation, with GLO being non-adversarial and more suitable than VAE and GAN. The method presented focuses on separating a mixture of sources with known and unknown distributions. The method focuses on separating a mixture of sources with known and unknown distributions by learning a parametric function T(). In the fully supervised setting, it reduces to a standard regression problem optimizing the Euclidean or L1 loss. Mixed-unmixed pairs are often unavailable, but synthetic pairs can be created to optimize the function. The method focuses on separating sources with known and unknown distributions by learning a parametric function T(). In cases where X and B are correlated, random synthetic mixtures might not be representative of Y. Semi-supervision is discussed where samples from the distribution of one component are available, but the other is unobserved. Neural Egg Separation (NES) is a novel algorithm that solves the semi-supervised task by iteratively addressing supervised problems without clean training examples of X. The method aims to separate mixtures of observed samples from distribution B combined with estimates of unobserved distribution samples. The Neural Egg Separation (NES) algorithm creates synthetic mixtures by sampling from an unobserved distribution and combining with training samples. A neural separation function is trained on these pairs to separate training mixture samples into their sources. The refined estimates are used for creating synthetic pairs for finetuning in the next iteration. The NES algorithm creates synthetic mixtures by sampling from an unobserved distribution and training a neural separation function to separate the sources. The separation function is initialized with random weights and optimized for a certain number of epochs. The estimates of unobserved distribution samples are updated during the process. The NES algorithm utilizes synthetic mixtures by sampling from an unobserved distribution and training a neural separation function. The algorithm uses SGD with ADAM update and performs 10 iterations with 25 epochs each. GLO Masking is introduced to address limitations in correlation between X and B, and initialization issues when X and B are dependent. GLO Masking (GLOM) separates mixtures by enforcing a distributional constraint through generative modeling. It trains per-sample latent codes using direct gradient descent, making it suitable for the scenario. GLO Masking jointly trains generators for B and X to produce mixture samples. The GLO Masking (GLOM) method separates mixtures by training generators for source B and X, using per-sample latent codes. The optimization problem involves constraining latent codes to lie within a unit ball for generalization. Once trained, latent codes are inferred for new mixture samples to estimate the sources. The GLO Masking (GLOM) method separates mixtures by training generators for source B and X with per-sample latent codes. A separation mask is learned to specify the fraction of the signal coming from B, improving results compared to synthesizing the signal directly. The masking function is implemented using the product of the signal and the mask, ensuring gradients are preserved within the range [0, 1]. Initializing Neural Egg Separation by GLOM involves refining the estimate by computing an effective mask from the element-wise ratio of estimated sources. The method includes training GLOM on the training set, inferring the mask for each mixture, upsampling the mask for audio, and running NES on observed B spectrograms and estimated X spectrograms. This initialization scheme improves NES to be competitive with other methods. The study shows that the initialization scheme in Neural Egg Separation by GLOM improves performance, making it competitive with fully-supervised training. Experiments were conducted on various real-world domains, including images, speech, and music, with correlated and uncorrelated signals. The method was compared against 3 baseline methods, including Constant Mask and Sparse Adversarial Masking. The new semi-supervised method based on adversarial training, called AM, aims to enhance the shallow NMF baseline. The loss functions for AM in adversarial training include LS-GAN BID18 and Spectral Norm is used on the discriminator to alleviate mode collapse. GLO Masking is applied on mel-spectrograms or images at 64 \u00d7 64 resolution, with NES method for initialization. Fully supervised baseline is used for comparison with paired data of b i \u2208 B, x i \u2208 X, and y i \u2208 Y. The method uses the same architecture as other regression methods to regress synthetic mixtures to unmixed sources, with more supervision. The effectiveness of the method is evaluated on image mixtures using MNIST, Shoes, and Handbags datasets. Experimental protocol involves splitting MNIST dataset into two classes and combining images to create training sets. The method evaluates performance on image mixtures using MNIST, Shoes, and Handbags datasets. Images are randomly sampled and combined to create training and test sets. Results show NMF, GLOM, AM, and NES performance on the test set, with NES achieving strong performance close to fully supervised upper bound. The study evaluated performance on image mixtures using MNIST, Shoes, and Handbags datasets. NMF failed to preserve fine details, while GLOM performed better due to using a VGG perceptual loss. The study evaluated performance on image mixtures using MNIST, Shoes, and Handbags datasets. GLOM outperformed NMF due to its use of a VGG perceptual loss. GLOM and AM had similar performance, with NES performing the best. Finetuning from GLOM helped NES achieve performance close to fully-supervised upper bound. The task of separating environmental noise from speech is a challenging problem in signal processing. The study evaluated performance on audio signals using clean speech segments from the LRW Dataset and environmental audio recordings from the ESC-50 dataset as additive noise. Noisy speech clips were created synthetically by adding random noise to clean speech segments. The study evaluated performance on audio signals using clean speech segments from the LRW Dataset and environmental audio recordings from the ESC-50 dataset as additive noise. Noisy speech clips were created synthetically by adding random noise to clean speech segments. The input audio feature had parameters like hop length, FFT size, and power-law compression. GLOM outperformed Semi-Supervised NMF, while AM training performed better than GLOM. NES showed strong performance even with a constant mask initialization, close to fully supervised results. Separating vocal music into singing voice and instrumental music has been a standard task in signal processing. In a study on audio signal processing, GLOM outperformed NMF in settings where X and B are dependent. Using the MUSDB18 Dataset, GLOM showed competitive results with NES on Vocal-Instrumental separation. Initialization was crucial for NES due to the dependence between sources and low SNR. Initialization was crucial for NES due to the dependence between sources and low SNR. Finetuning NES from GLOM masks performed much better than all other methods and was competitive with the supervised baseline. GLOM was much better than AM initialization, achieving 0.9 and 2.9. GLO masking as a stand-alone technique usually performed worse than Adversarial Masking, but finetuning from GLO masks was far better than finetuning from adversarial masks. Supervision is important for source separation. Supervision is crucial for source separation, with blind source separation being ineffective. Full supervision with labeled masks is unrealistic, but limited supervision specifying if a sound sample contains the target source is feasible and cost-effective. This reduced supervision approach can be easily provided and used to finetune deep sound classifiers for improved efficiency. To showcase the generality of our method, we chose not to encode task specific constraints. Using signal-specific constraints can enhance performance, such as repetitiveness of music, sparsity of singing voice, and smoothness of natural images. Non-adversarial generative methods like GLOM may outperform adversarial methods for signal separation tasks. A stable global minimum of NES can achieve perfect signal separation by ensuring synthetic mixtures match real mixtures and real mixtures are perfectly separated. In this paper, a novel method called Neural Egg Separation (NES) is proposed for separating mixtures of observed and unobserved distributions. NES converged to different local minima in experiments, showing it may not reach a global minimum. The method outperforms others and is competitive with full-supervision, especially when using GLO Masking for initialization. GLOM and AM utilize the same architectures for audio and image processing, operating on mel-scale spectrograms at 64 \u00d7 64 resolution. The Masking Network for AM operates on 64 \u00d7 64 mel-scale audio spectrograms with convolutional and deconvolutional layers. The NMF semi-supervised source separation baseline BID26 uses full linear spectrograms without compression and trains a decomposition with non-negative weights and latent codes. Regularization is crucial for the process. Regularization is important for the performance of the method. L1 regularization is used to ensure sparsity of weights. GLO captures general features but struggles with fine details. Masking operation in GLOM helps recover fine-grained details and results in cleaner separations. NES evolution is shown in Fig.2 as a function of iteration for the same examples. In the context of regularization and feature capturing methods, NES shows quick convergence and further improvement with increasing iterations, as demonstrated on the Speech dataset in terms of SDR."
}