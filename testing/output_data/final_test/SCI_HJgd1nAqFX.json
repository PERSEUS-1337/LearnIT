{
    "title": "HJgd1nAqFX",
    "content": "Building agents for web interaction can lead to significant improvements in knowledge understanding and representation learning. The DOM-Q-NET architecture is introduced for deep reinforcement learning in web navigation, addressing challenges with large action spaces and varying actions between states. This model uses separate networks for clicking DOM elements and typing inputs, utilizing a graph neural network to represent web page structures. Results show performance matching or surpassing existing work in the MiniWoB environment, with a 2x improvement in sample efficiency in multi-task training. Deep reinforcement learning has been successful in tasks like playing arcade games and manipulating robotic arms. However, most methods focus on simulated environments with limited real-world knowledge. Agents need to understand the semantic meaning of texts, images, and their relationships to navigate the World Wide Web effectively. In this work, the authors propose a novel architecture, DOM-Q-NET, to learn the semantic meaning of texts and images on the web by interacting with the Document Object Model (DOM) in HTML. They address the challenges of web tasks such as diversity, large action space, and sparse reward signals, and aim to improve upon previous imitation learning methods like BID10 in MiniWoB benchmark tasks. The authors propose DOM-Q-NET, a novel architecture for web navigation using factorized Q functions. Graph Neural Network is used for state and action representations, with separate MLPs for different action categories. The model is fully differentiable and evaluated on multitask learning, showing transferability of learned behaviors on web interfaces. The authors introduce DOM-Q-NET, a novel architecture for web navigation using factorized Q functions. They show that a multi-task agent in MiniWoB achieves 2x sample efficiency compared to a single-task agent. The Document Object Model (DOM) defines the logical structure of HTML documents, with DOMs connected in a tree structure. Browsers use attributes like \"tag\" and \"class\" to render web pages. In reinforcement learning, agents interact with Markov Decision Processes (MDP) to maximize future rewards. An MDP is defined by a tuple (S, A, T, R, \u03b3) where S and A are state and action spaces, T (s |s, a) is transition probability, R is immediate reward, and \u03b3 is discount factor. Q-value function represents expected future discounted reward. Optimal Q-value function satisfies Bellman optimality equation. MPNN framework updates node-level feature representations in two phases. The forward pass updates node-level features and graph-level vectors using message passing and readout functions. Graph neural networks have been used in robot locomotion to transfer policies effectively. NerveNet and DOM-Q-NET utilize GNNs for policy parametrization and Q function modules. The graph structure of a robot is static, while a web page's structure can change. Locomotion tasks have dense rewards, while web navigation tasks have sparse rewards. BID15 created Mini World of Bits for web navigation tasks. BID10 introduced DOM-NET for web tasks, achieving state-of-the-art performance. In web navigation tasks, a new approach aims to achieve state-of-the-art performance without expert demonstrations or prior knowledge. Using NEURAL DOM Q NETWORK and GNN, different modules are utilized to compute Q values for navigating through web pages and menus to achieve various goals presented in natural language sentences. The RL agent in web navigation tasks receives sparse rewards for accomplishing goals presented in natural language sentences. DOM-Q-NET is proposed to address challenges in representing the state-action value function, such as the enormous action space and varying number of actions. This approach differs from typical RL tasks by requiring actions on web pages through interaction with buttons and text fields. The RL agent in web navigation tasks uses controller's joint movements for Atari BID11. It frames actions on the web into three categories: DOM selection, word token selection, and mode selection (click or type). The environment receives a tuple of actions at each time step. The RL agent in web navigation tasks uses controller's joint movements for Atari BID11, framing actions into three categories: DOM selection, word token selection, and mode selection. The state-action value function considers permutations of DOM and token, leading to a large action space for MiniWoB tasks. To reduce this space, a factorized state-action value function is proposed where action values are independent. The optimal Q-value function is defined as the sum of individual value functions for three action categories. By assuming independence, the optimal policy can be found by selecting greedy actions for each Q-value function. This approach reduces computation cost and is linear in the number of DOM elements and word tokens. A neural network architecture is designed to capture invariance in web page attributes while being flexible with varying numbers of elements and tokens. The agent needs to consider both local and global information when locating information on the web. The proposed GNN-based RL agent, called DOM-Q-NET, computes factorized Q-values for each DOM on a web page using tree-structured HTML information to guide state-action representations. The model learns concatenated embedding vectors for local, neighbor, and global information, providing relational information among DOM elements to the agent. The GNN-based RL agent, DOM-Q-NET, computes factorized Q-values for each DOM on a web page using tree-structured HTML information. The Local Module concatenates embedded attributes of the DOM, measuring soft alignment with goal tokens. The Neighbor Module incorporates neighbor context using a graph neural network for message passing between nodes. The Local Module initializes the process by using the weights of the tree. An intermediate state, m t, is used for message passing steps with Gated Recurrent Units BID1 for vertex updates. This is done for T steps to get neighbor embeddings. The module contains the state representation of the current page and the action representation of clicking a DOM for approximating the Q-value function. The Global Module is the high-level feature representation of the entire web page after the readout phase, used by all three factorized Q networks. Two readout functions are investigated for obtaining global embeddings with or without incorporating goal information: max-pooling and goal-attention. The Local Module uses tree weights to initialize the process and utilizes Gated Recurrent Units for message passing steps to get neighbor embeddings. The Global Module represents the entire web page after the readout phase and incorporates goal information through max-pooling and goal-attention. The goal vector is created by concatenating goal tokens with positional encoding vectors before applying a feedforward network and scaled dot product attention with local and neighbor embeddings. The Q-value function for choosing the DOM, word token, and mode is parametrized by a two-layer MLP. Model parameters, including embedding matrices, are learned from scratch and updated accordingly. The method of concatenating node-level features with the goal vector is effective but increases model size. The model parameters are updated by minimizing the squared TD error where transition pairs are sampled from the replay buffer. To assess transferring learned behaviors and solving multiple tasks, a single agent is trained in multiple environments with transitions collected in a shared replay buffer. The network is updated after each action in each environment. Generalization capability is evaluated for large action space and sample efficiency is investigated for multitask learning. The text chunk discusses the gain in sample efficiency with a model from multitask learning using the Q-learning algorithm with four components. Tasks involve clicking DOM elements and typing strings, with rewards given for correct completion. The model, DOM-Q-NET, achieves a 100% success rate for most tasks except for click-widget, social-media, and email-inbox. It still reaches an 86% success rate for social-media and successfully solves click-widget and social-media with 100% success rate using goal-attention. No prior knowledge or constraints were used during exploration. Our model, DOM-Q-NET, successfully solves a long-horizon task, choose-date, that previous works with demonstrations could not solve. It demonstrates sample efficiency in multitask learning, achieving 2x efficiency with about 63000 frames for 9 tasks. The multitask agent achieves 2x sample efficiency by using 63000 frames for 9 tasks, compared to single-task agents using 127000 frames. The plots show that multitask learning is more efficient for complex tasks, with significant gains in sample efficiency. The results show that the model allows for positive transfers of learned behaviors between tasks. Ablation experiments were conducted to justify the effectiveness of using different modules for the Q dom stream. The failure case for click-checkboxes demonstrates that DOM selection without the neighbor module will not work due to similar DOM attributes leading to identical representations. The study addressed the issue of hand-crafting message passing in BID10 and highlighted the faster convergence of DOM-Q-NET. The model solves most MiniWoB tasks without feeding goal representation, except for click-widget. Comparisons with different goal encoding methods were shown, with goal-attention proving beneficial for sample efficiency in multitask learning. The study introduces a new architecture for parameterizing factorized Q functions using goal-attention, local word embeddings, and a graph neural network in multitask learning settings. The agent learns to pay attention to different parts of the DOM tree based on goal instructions, showing significant sample efficiency gains. The model solves challenging tasks without demonstrations and transfers learned behaviors, essential for web navigation. Future work includes exploring exploration strategies for tasks like email-inbox. The study introduces a new architecture for parameterizing factorized Q functions using goal-attention and a graph neural network in multitask learning settings. The agent learns to pay attention to different parts of the DOM tree based on goal instructions, showing significant sample efficiency gains. Future work includes exploring exploration strategies for tasks like email-inbox. The study presents a new architecture for parameterizing factorized Q functions using goal-attention and a graph neural network in multitask learning. The model utilizes message passing with weights computed using a scaled dot product attention mechanism. Benchmark results for multitask learning and various tasks are compared. The agent learns to focus on different parts of the DOM tree based on goal instructions, leading to improved sample efficiency. The study introduces a new architecture for factorized Q functions in multitask learning, utilizing goal-attention and a graph neural network. Results are based on 536 experiments with specific hyperparameters and task configurations. In an ablation study, 6 discounted models were compared, and 580 experiments were conducted for single-task and multitask learning. Learning curves were presented for different goal-encoding modules, showing sparsity of rewards and exploration failures. The learning curves for 9 tasks in multitask learning are shown, excluding plots for simple tasks. The plots display moving average rewards, fraction of positive transitions in replay buffer, and unique number of positive transitions per training batch."
}