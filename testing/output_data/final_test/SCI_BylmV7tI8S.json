{
    "title": "BylmV7tI8S",
    "content": "Recordings of neural circuits in the brain show complex dynamics and variability, with dimensionality reduction techniques revealing low-dimensional structures. This study uses recurrent neural network (RNN) models to investigate the role of dimensionality in behavior and task learning. The findings suggest that RNNs adapt their dimensionality based on task demands, shedding light on how neural networks solve tasks with robust representations. Artificial recurrent neural networks (RNNs) are powerful tools for studying dynamical representations and theoretical hypotheses in controlled settings. They can be thought of as dynamical brain circuits, with well-defined systems governing the neural representation of recurrent units. In this work, a network is tasked with binary classification by classifying inputs into two classes after a delay period for processing. Dimensionality reduction is commonly used in high-dimensional systems like RNNs to reveal that dynamics are often constrained to regions linked to the complexity of the function the neural circuit fulfills. The relationship between task and representation dimension is intriguing, as high-dimensional representations can handle complex computations while low-dimensional representations preserve essential features for specific tasks, allowing learning with fewer parameters and examples. The behavior of RNNs in balancing dimensionality reduction and expansion of input data can depend on initialization, with chaotic networks tending to expand low-dimensional inputs. Dynamical chaos in brain networks explains repeatable structure and variability. Chaos-driven dimensionality expansion with fixed recurrent weights has been explored, but less is known about how this phenomenon evolves through training. The study focuses on the behavior of RNNs with evolving recurrent weights during training. A standard RNN architecture is used with hidden unit dynamics and outputs. Simulations are conducted with 200 hidden units, initialized weights, and inputs drawn from Gaussian distributed clusters. The coupling strength parameter governs the network's chaotic behavior, with higher values leading to more chaotic dynamics. The study examines RNNs with evolving recurrent weights during training using a standard architecture with hidden unit dynamics. Simulations involve 200 hidden units, Gaussian distributed inputs, and a coupling strength parameter influencing chaotic behavior. Clusters in a random subspace are assigned class labels, and weights are adjusted to minimize a loss function for classification performance evaluation. The study explores RNNs with evolving recurrent weights during training, comparing networks initialized near the edge of chaos (EOC) and strongly chaotic (SC). Both networks achieve perfect testing accuracy. The EOC network forms stable fixed point attractors for each class, while the SC network separates classes into chaotic attractors that mix back together. The effective dimensionality (ED) of the representation captures this compression phenomenon. The effective dimensionality (ED) of the representation in RNNs with evolving recurrent weights drops with increasing time, becoming highly compressed at evaluation time. This compression is due to increasing distances between different classes and decreasing distances within classes, a phenomenon observed in networks trained with different loss functions and activation functions. The representation in RNNs becomes highly compressed at evaluation time, with linear separability preserved by network dynamics. Logistic regression achieves perfect training accuracy on the representations of SC and EOC networks. Generalization is measured in Fig. 2c. The compressed representation in RNNs shows improved generalization over time, allowing for perfect classification accuracy on held-out clusters. The SC network performs near-perfect classification in a two-dimensional space, while the EOC network is less successful. The SC network achieves near-perfect classification by increasing dimensionality over time, leading to linear separability of classes and good generalization. The EOC network is not as successful due to non-monotonic behavior influenced by the learning rate. The EOC network struggles to expand dimensionality and solve challenging tasks, unlike the SC network. RNNs reduce input dimensionality for linearly separable tasks, while chaotic initializations are needed for nonlinear tasks with low-dimensional inputs. Chaos-driven dimensionality expansion leads to linear separation boundaries in representations. Expansion results in representations with linear separation boundaries, suggesting that RNNs learn minimal dimensionalities for simple class separation with chaotic initialization. Further exploration is needed on learning strategies modulating dimensionality and the functional roles of brain circuit variability. It is also important to study nonlinear measures of dimensionality and explore the connection between nonlinear dimension and RNN dynamics. The study suggests roles for dimensionality modulation and variability in neural circuits, with interest in exploring dimension compression and expansion through mathematical analysis and experiments in recurrent network models for improved performance."
}