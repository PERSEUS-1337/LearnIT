{
    "title": "SyGjjsC5tQ",
    "content": "A new method called Stable Opponent Shaping (SOS) is introduced in this paper, which combines aspects of Learning with Opponent-Learning Awareness (LOLA) and a stable variant named LookAhead. The goal is to improve learning dynamics in differentiable games by avoiding 'arrogant' behavior exhibited by LOLA agents. The authors prove that LookAhead converges locally to equilibria and avoids strict saddles in all types of games. SOS, a new method introduced in this paper, combines aspects of LOLA and LookAhead to improve learning dynamics in differentiable games. Machine learning has traditionally focused on optimizing single objectives, but GANs have shown the potential of dealing with multiple interacting goals. Differentiable games involve cooperating and competing agents, such as the generator and discriminator in GANs. In GANs, convergence in differentiable games remains a challenge due to the interdependence of parameters. While gradient descent has been successful in single functions, it can fail in two-player zero-sum games. No algorithm has been proven to converge in all differentiable games yet. Previous research has focused on convergence in convex n-player games, but non-convex games like GANs require further study. Some success has been seen in non-convex settings for specific game classes. In non-convex settings for restricted classes of games, various algorithms have been proposed, such as Symplectic Gradient Adjustment (SGA), which aims to converge to stable fixed points but may prioritize stability over individual loss. SGA has been independently discovered by different researchers and draws on variational inequalities. LOLA modifies learning objective by predicting and differentiating through opponent learning steps, encouraging cooperation in settings like IPD. LOLA has no guarantees of converging or preserving fixed points. A tandem game is constructed where LOLA agents converge to non-fixed points due to 'arrogant' behavior. LookAhead successfully preserves fixed points and locally converges in all differentiable games, filling a theoretical gap in multi-agent learning. In a theoretical gap in multi-agent learning, LookAhead preserves fixed points and converges locally. An algorithm named Stable Opponent Shaping (SOS) balances stability and exploitation by interpolating between LookAhead and LOLA. SOS shows strong convergence guarantees and opponent shaping capabilities in experimental IPD games. In the tandem game, SOS consistently outperforms LOLA, demonstrating practical consequences of theoretical guarantees. Additionally, in a more complex GAN setup, SOS effectively prevents mode collapse and mode hopping when learning Gaussian mixture distributions, matching dedicated algorithms like CO. The problem of multi-agent learning is framed as a differentiable game, where each player has parameters and losses that are twice continuously differentiable. In a differentiable game, each player's parameters and losses are twice continuously differentiable. The game aims to reach a Nash equilibrium where each player's strategy is a local best response to current opponent strategies. The simultaneous gradient of the game is defined as the concatenation of each player's gradient. Each agent minimizes their loss independently using Gradient Descent with learning rate \u03b1. This naive learning approach can fail to converge, as shown in Example 1 with a specific game scenario. The simultaneous gradient in the game leads to divergence from the origin due to the non-stationarity of each agent's loss, causing a contradiction in optimization. This failure demonstrates that gradient descent does not generalize well to differentiable games. An alternative solution concept to Nash equilibria is considered before introducing LOLA. The optimal solution in the game is (x, y), but converging to the origin is highly undesirable as better outcomes can be achieved in the anti-diagonal direction. Nash equilibria are not ideal in multi-agent learning. Stable fixed points are defined using the 'Hessian' of the game, where a point \u03b8 is stable if H(\u03b8) > 0, unstable if H(\u03b8) < 0, and a strict saddle if H(\u03b8) has an eigenvalue with negative real part. Stable fixed points (SFPs) are defined using the 'Hessian' of the game, where a point \u03b8 is stable if H(\u03b8) > 0. Invertibility of H(\u03b8) at SFPs is crucial for convergence results. This definition is an improved variant on previous ones, making the class of SFPs as large as possible for theoretical results. The Hessian determines if a point is a local minimum, maximum, or saddle point. Invertibility is crucial for ensuring SFPs are 'local minima'. Theorem 6 is stronger and more general than results for SGA by BID0. LOLA modifies learning by predicting opponent steps. Agent 1 optimizes L1 with respect to \u03b81, aligning opponent learning steps with the direction of greatest decrease in L1. LOLA agents actively shape opponent learning by exploiting opponent dynamics to reduce losses. This approach has been successful in reaching cooperative equilibria in multi-agent learning, including games like tit-for-tat in the IPD. Preserving both terms in the adjustment process is crucial for developing stable opponent shaping in n-player LOLA. However, while experimentally successful, LOLA fails to preserve fixed points of the game. LOLA fails to preserve fixed points of the game, even if a Nash equilibrium is present. The update process can push fixed points away, leading to worsened losses for all agents. An example of this is shown in a tandem game where agents face incompatible sub-goals, making it difficult to accelerate forwards. LOLA does not preserve the fixed points in this scenario, only converging to sub-optimal outcomes. LOLA fails to preserve fixed points of the game, even if a Nash equilibrium is present. The update process can push fixed points away, leading to worsened losses for all agents. In Appendix C, it is shown that LOLA converges to sub-optimal scenarios with worse losses for both agents, due to agents trying to shape opponent learning and enforcing compliance by accelerating forwards. Removing the shaping term \u03c7 can preserve fixed points, but it raises questions about the perspective of each agent in the optimization process. LookAhead, discovered before LOLA by BID24, converges to Nash equilibria in two-player, two-action bimatrix games. It proves local convergence to SFP and non-convergence to strict saddles in all differentiable games. By discarding the shaping term, LOLA's capacity is eliminated. SOS algorithm proposes a solution to preserve advantages of both LookAhead and LOLA agents by optimizing a modified objective function. The algorithm trades between shaping and stability based on a parameter p, but fixed points are only preserved when p is infinitesimal. A two-part criterion is proposed to choose p at each learning step for guarantees to hold. The SOS algorithm combines p-LOLA with a two-part criterion to choose p at each learning step, ensuring convergence to fixed points and local convergence. This approach optimizes the objective function while preventing arrogant behavior and maintaining stability. The SOS algorithm combines p-LOLA with a criterion to ensure convergence to fixed points and local convergence. Theoretical results are independent of hyperparameters a and b, and convergence is proved using Ostrowski's Theorem. Theorem 2 states that for invertible H with symmetric diagonal blocks, there exists a positive stability parameter such that (I \u2212 \u03b1H)H is positive stable for 0 < \u03b1 < . This result is non-trivial as it does not rely on positive definiteness or eigenvalue analysis. Using a similarity transformation trick, positive stability is achieved through invariance under change of basis. Corollary 3 shows that LookAhead converges locally to stable fixed points for small \u03b1 > 0. Theorem 4 and Proposition 5 demonstrate that SOS converges locally to stable fixed points for small \u03b1 > 0, unlike LOLA. Theorem 6 shows that SOS can avoid strict saddles almost surely when initialized randomly. Theorem 6 states that SOS locally avoids strict saddles almost surely for small \u03b1 > 0. This also applies to LookAhead and can be extended to global initializations with a boundedness assumption on H 2. SOS retains opponent shaping capacity while LookAhead does not, as demonstrated in experiments. In three differentiable games, SOS outperforms LOLA, especially in the tandem game by decaying p to avoid arrogant behavior. In the Prisoner's Dilemma game, agents have 5 parameters each, including the probability of cooperating at different states. Nash equilibriums like tit-for-tat are discussed, with training episodes run for various algorithms. In the context of the Prisoner's Dilemma game, parameters are initialized for different algorithms with specific values. The SOS algorithm uses a = 0.5 and b = 0.1, while the Tandem game ensures local convergence. Performance is robust to hyperparameters a, b, with a setup similar to BID0 for learning a Gaussian mixture distribution using GANs. Data is sampled from a highly multimodal distribution to avoid collapsing onto a subset. The generator and discriminator networks have 6 ReLU layers of 384 neurons each. Learning rates are chosen by grid search at iteration 8k, with a = 0.5 and b = 0.1 for SOS. Results are shown in FIG1, displaying end-run probabilities of cooperating for each memory state. SOS and LOLA mostly succeed in playing tit-for-tat in the Prisoner's Dilemma game. In the Prisoner's Dilemma game, agents cooperate at the start state and use a tit-for-tat strategy. Most points for LA/CO/SGA/NL accumulate at the bottom left, indicating a tendency to defect. Recent proposals like SGA and CO show limited effectiveness. SOS and LOLA have similar trained parameters and losses, with SOS outperforming LOLA in opponent shaping. In the Tandem experiment, SOS successfully decreases p to reach equilibria, while LOLA fails to preserve fixed points. SOS avoids overshooting, unlike LOLA. In practical applications, the SOS criterion is effective in avoiding overshooting and outperforming LOLA in opponent shaping. Results show that SOS achieves convincing outcomes by distributing mass across all Gaussians, while CO/SGA/LOLA also perform well. LookAhead is slower, NL fails due to mode collapse and hopping. Visual inspection and KL divergence provide evidence of SOS and CO's superiority in optimization metrics. SOS is versatile, while CO is tailored for two-player zero-sum GAN optimization. The paper introduces a unified framework for LookAhead in all differentiable games, addressing the lack of convergence guarantees in algorithms dealing with interacting goals. It emphasizes the importance of stable fixed points and opponent shaping in achieving robust convergence results. The paper introduces a unified framework for LookAhead in differentiable games, emphasizing stable fixed points and opponent shaping for robust convergence. A new approach, SOS, combines LookAhead and LOLA to achieve stronger convergence guarantees and practical superiority in the tandem game. SOS outperforms other algorithms in the IPD by promoting tit-for-tat policies and learning Gaussian mixtures on par with dedicated algorithms. Nash equilibria are shown to be inadequate in multi-agent learning. Equilibria in multi-agent learning are insufficient, as demonstrated by a simple game where the origin is a global Nash equilibrium but a saddle point of the losses. Singular Hessians at certain points prevent gradient descent from converging to all local minima, highlighting the limitations of Nash equilibria as a solution concept. The Hessian being singular at (0, 0) results in x 0 = 0 almost surely, highlighting the importance of local minima with invertible Hessian for local convergence claims. The definition of SFP involves fixed points with positive semi-definite Hessian, allowing for second-order-tractable local minima. It is crucial to impose positive semi-definiteness to maintain a large class of solutions, even though strict positive definiteness may hold for single losses. Imposing a weaker condition than invertibility would be incorrect, as local convergence of gradient descent on single functions cannot be guaranteed without it. The necessity of imposing positive semi-definiteness in optimization methods attempting to generalize gradient descent is highlighted. The original formulation of SFP by the authors included an extra requirement in a neighborhood of \u03b8, which was later dropped due to being more restrictive and weakening the analogy with tractable local minima. The only benefit of imposing semi-positivity in a neighborhood is that SFP becomes a subset of Nash equilibria. The introduction of strict saddles in optimization methods is discussed, showing that they are a subset of unstable fixed points. The LOLA gradient adjustment is proven to be valid under certain assumptions. In the tandem game, agents aim for x \u2248 \u2212y to minimize the leading loss. Fixed points for NL/LOLA are (x, 1 \u2212 x) with corresponding losses summing to 0 for any x. LOLA fails to preserve fixed points and converges to points satisfying certain conditions. LOLA is not a strong algorithm candidate for all differentiable games due to worse losses for both agents. Convergence is slow with small learning rates, and fixed points do not always sum to 0. Ostrowski's theorem is used for proving local convergence, along with a topological result in Lemma D.9. The text discusses convergence of iterative procedures in differentiable games, focusing on fixed points and stability conditions. The convergence rate is shown to be at least linear under certain conditions, with a proof provided for local convergence using a specific iterative procedure. The text discusses the convergence of iterative procedures in differentiable games, focusing on fixed points and stability conditions. It mentions Ostrowski's Theorem and applies it to LookAhead, showing that (I \u2212 \u03b1H o )H is positive stable for small \u03b1 > 0. The Hessian DISPLAYFORM7 is used as an example to demonstrate that G = (I \u2212 \u03b1H o )H always has positive eigenvalues for small \u03b1 > 0, but is not positive definite due to a negative eigenvalue in its symmetric part. The proof involves a similarity transformation to show positive definiteness with respect to a different inner product, a technique not commonly found in multi-agent learning literature. The trick is to find a positive definite matrix similar to G, sharing the same positive eigenvalues. The matrix H d is block diagonal with symmetric blocks, making (I + \u03b1H d ) symmetric and positive definite for all \u03b1 \u2265 0. The matrix H d is eliminated from G 1 using a similarity transformation provided by matrix M, ensuring positive semi-definiteness. A Taylor expansion of M in \u03b1 yields specific results depending on the value of u Hu. If u Hu > 0, then a certain condition holds for small \u03b1. If u Hu = 0, H can be decomposed into symmetric and antisymmetric parts, leading to further implications on the matrix properties. The text discusses the positive semi-definiteness of matrices and the stability of fixed points for small values of \u03b1. It also mentions the decomposition of matrix H into symmetric and antisymmetric parts. The local convergence results are shown to descend to SOS. The lemma establishes that the criterion for p is C 1 in neighborhoods of fixed points, crucial for invoking analytical arguments like Ostrowski's Theorem. If \u03b8 is a fixed point and \u03b1 is small, then p = \u03be 2 in a neighborhood of \u03b8. Theorem D.8 states that SOS converges locally to stable fixed points for small \u03b1. The proof involves showing that if \u03b8 is a stable fixed point, then \u2207\u03be p (\u03b8) is positive stable for small \u03b1. Despite p(\u03b8) not being continuous everywhere, it is continuously differentiable in a neighborhood U of \u03b8 where p = \u03be 2. This leads to positive stability for all 0 < \u03b1 < . The SOS method converges locally to stable fixed points for small \u03b1. The proof involves showing positive stability of \u2207\u03be p (\u03b8) for small \u03b1 when \u03b8 is a stable fixed point. This stability holds for all 0 < \u03b1 < . The SOS method converges locally to stable fixed points for small \u03b1. The proof involves sequences of real numbers and the iterative procedure for fixed points of the game. The SOS method converges locally to stable fixed points for small \u03b1, avoiding strict saddles using the Stable Manifold Theorem. The theorem states that for a fixed point x of a local diffeomorphism F, there exists a stable center manifold W with tangent space E s at x, ensuring convergence to stable points. The SOS method locally avoids strict saddles almost surely for small \u03b1 by converging to stable fixed points using the Stable Manifold Theorem. If \u2207F(x) has eigenvalues |\u03bb| > 1, then Eu has dimension at least 1, and W has measure zero in Rd, proving that initial points converging through SOS to a strict saddle \u03b8 have measure zero. The SOS method avoids strict saddles by converging to stable fixed points using the Stable Manifold Theorem. If \u2207F(x) has eigenvalues |\u03bb| > 1, then Eu has dimension at least 1, and W has measure zero in Rd, proving that initial points converging through SOS to a strict saddle \u03b8 have measure zero. The local stable set W has measure zero, implying that local convergence to \u03b8 occurs with zero probability. The SOS method converges to stable fixed points using the Stable Manifold Theorem, avoiding strict saddles. The local stable set W has measure zero, leading to zero probability of local convergence to \u03b8. In the Gaussian mixture experiment, LOLA and SGA spread mass across all Gaussians, while LookAhead initially displays mode collapse but eventually incorporates further mixtures. After running further iterations, LookAhead slowly spreads mass across all mixtures. Comparing with NL/CO/SOS, CO/SOS/LOLA/SGA are equally successful qualitatively. SOS/CO show slightly better results in KL divergence after 6-8k iterations, with LOLA being faster initially but often deviating from the correct distribution. SOS remains stable in the long run due to its convergence criterion, while LOLA tends to exploit opponent learning. SOS converges rapidly to the correct distribution, as shown in Figure 7. SOS converges rapidly to the correct distribution, while NL perpetually suffers from mode hopping and LA lags significantly behind."
}