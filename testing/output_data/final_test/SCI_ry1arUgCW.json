{
    "title": "ry1arUgCW",
    "content": "Exploration in Reinforcement Learning is typically done through stochastic action-selection, but it can be more efficient when focused on gaining new world knowledge. Visit-counters are useful for directed exploration, but they have a limitation in their locality. While there are model-based solutions to this issue, a model-free approach is still needed. $E$-values are proposed as a way to evaluate exploratory value in state-action trajectories, improving learning and performance in Reinforcement Learning compared to traditional counters. This method can be implemented with function approximation for efficient learning in continuous MDPs, outperforming state-of-the-art performance in games like Freeway Atari 2600. The agent's goal in Reinforcement Learning is to find an optimal policy \u03c0* that maximizes the state-action value function Q*. There are two main approaches for learning the optimal policy: model-based and model-free. The ideas presented in this paper are relevant to model-free learning. In model-free learning of MDPs, Q-Learning is a common method for updating Q values based on actions and rewards. Balancing between Exploration and Exploitation is a challenge in Reinforcement Learning, where the agent must choose between exploiting known options for rewards and exploring new options for potential better outcomes. Exploration is crucial for learning in Q-Learning, as the algorithm must visit each state-action pair infinitely many times for convergence. Random exploration is sufficient for asymptotic convergence, but has limitations in finite learning processes. These limitations include not utilizing current knowledge to guide exploration and not favoring unvisited trajectories over visited ones. In Q-Learning, exploration is essential for learning. Random exploration has limitations in finite learning processes, such as not favoring unvisited trajectories over visited ones. Different methods like the -greedy schema and Boltzmann Distribution aim to improve exploration but still lack direction towards gaining more knowledge. In Q-Learning, exploration is crucial for learning. Various methods like -greedy and Boltzmann Distribution aim to enhance exploration but lack direction towards gaining more knowledge. Efficient exploration involves Sample Complexity and Delayed Q Learning, which uses exploration bonuses based on counting, recency, and value difference measures. Counter-based methods are widely used in practice and theory for exploration evaluation. The goal of this work is to develop a measure for exploratory values of state-action pairs in model-free settings, considering not only immediate outcomes but also potential knowledge gained from trajectories. This is similar to learning value functions, which involve the sum of expected rewards over a trajectory. The exploration-value of a state-action should consider both immediate and future knowledge gained. A model-based approach uses dynamic programming to compute future exploration bonus, but this is not feasible in a model-free setting. Propagating directed exploration in model-free reinforcement remains a challenge. In this section, a novel approach for directed exploration in model-free reinforcement learning is proposed. It involves using two parallel MDPs, with one MDP estimating the value function and the other MDP having no rewards associated with state-actions. By learning \"action-values\" in this new MDP, known as E-values, directed exploration can be propagated by initializing E-values to 1 and creating an optimistic bias. The proposed approach involves using two parallel MDPs, with one estimating the value function and the other having no rewards. E-values are initialized to 1 for directed exploration, with the agent learning both Q and E values concurrently. The SARSA algorithm is used for learning E, with an update rule that includes a learning rate parameter \u03b1 E. The learning rule updates E-values based on E(s t+1 , a t+1) rather than max a E(s t+1 , a), ensuring exploration values decrease when repeating the same trajectory. The logarithm of E-Values acts as a generalization of visit counters, propagating values along state-action pairs. In the case of \u03b3 E = 0, the update rule is given by DISPLAYFORM0. The value of the state-action pair after n visits is (1 \u2212 \u03b1) n, where \u03b1 is the learning rate. Logarithm transformation shows that log 1\u2212\u03b1 (E) = n. For non-terminal states with \u03b3 E > 0, E decreases slowly, affecting the rate of log 1\u2212\u03b1 E. Each visit contributes less to the counter for state-actions leading to many potential states. In a tree MDP, actions chosen sequentially affect the outcome. The E-values in MDPs decrease over repetitions, with each round of choices equivalent to a generalized counter. The maximal value of E (s i , a i ) decreases after completing a trajectory in the MDP. The logarithm of E-values can be seen as a logarithmic transformation. The logarithm of E-values in MDPs can be seen as a generalization of counters, which can be utilized in algorithms for model-based and model-free RL. E-Values can replace standard counters, as demonstrated by testing an -greedy agent with an exploration bonus based on E-values on a bridge MDP. The learning progress was measured by calculating the mean square error on the optimal policy per episode. Adding exploration bonuses based on E-values to the reward in the -greedy algorithm on the short bridge environment leads to faster learning. The value of \u03b3 E affects the speed of learning, with larger values resulting in faster learning. Generalized counters outperform standard counters in assisting exploration, and they can also be added to estimated Q-values for action-selection. Several action-selection rules incorporating counters have been proposed. In reinforcement learning, action-selection rules incorporating counters have been proposed to improve exploration. E-values can be used to generalize these rules, replacing counters with log 1\u2212\u03b1 (E). Deterministic equivalents of standard stochastic rules can be derived using this approach, enhancing exploratory behavior in RL. In reinforcement learning, deterministic equivalents of stochastic action-selection rules can be derived using E-values. This allows for improving exploration behavior by ensuring equal frequencies of actions selected in the limit of infinitely many steps. In reinforcement learning, deterministic equivalents of stochastic action-selection rules can be derived using E-values to improve exploration behavior. Fixed Q-values are considered to define this equivalence, where a deterministic policy is a determinization of a stochastic rule if it avoids choosing actions visited too frequently. The frequency of action choices is denoted by f(a), with a counter C(a) tracking the number of choices. Theorem 3.1 states that for any sub-linear function b(t), a deterministic policy choosing actions at step T can be determined. Theorem 3.1 in reinforcement learning shows that a deterministic policy can be derived using E-values to improve exploration behavior. Two possible determinization rules are provided, one using arg min a C(a) - f(a) and the other using arg max a f(a) C(a). By replacing visit counters with generalized counters, Directed Outreaching Reinforcement Action-Selection (DORA) can transform any stochastic or counter-based action-selection rule into a deterministic one where exploration propagates over states. The DORA algorithm transforms stochastic action-selection rules into deterministic ones for improved exploration behavior. Experiments were conducted on Bridge environments using various agents, including -greedy, Softmax, and their LLL determinizations. Additionally, a counter-based agent with a UCB-like algorithm was tested with exploration bonus. The DORA algorithm transforms stochastic action-selection rules into deterministic ones for improved exploration behavior. Two variants of the algorithm were tested using ordinary visit counters and E-values, with hyperparameters and temperature fitted separately for optimization. Results show that E-value based agents outperform counter-based and stochastic agents on the bridge problem, reaching low error values faster. The use of E-values in learning leads to more efficient exploration compared to counter-based learning. E-values better represent the agent's missing knowledge than visit counters during learning, as shown in a study on a bridge environment. Visit counters do not accurately capture the amount of missing knowledge, as convergence depends on factors beyond just the counter value. Generalized counters are a useful measure of missing knowledge in large MDPs where tabular methods are impractical. Achieving directed exploration in continuous MDPs is challenging due to the difficulty of revisiting state-action pairs and storing individual values for all pairs. In large MDPs, storing individual values for all state-action pairs is impractical. Counterbased methods like -greedy or softmax BID1 are often used for exploration. E-values can be estimated using standard model-free techniques in function-approximation scenarios. Testing E-values as generalized visit-counters in the MountainCar problem showed promise. The E-values were used by an -greedy agent independently of Evalues. E-values were effective for counting visits in continuous MDPs. LLL agents outperformed stochastic agents on a sparse-reward MountainCar problem. A neural network was trained with two streams to predict Q and E-values for the Freeway Atari 2600 game. The network was trained with an exploration bonus based on E-values. Action-selection was done using an -greedy rule. The E-values were initialized at 0.5 and satisfied 0 < E < 1 throughout training. Comparing to a DQN baseline and density model counters, the use of E-values outperformed both baselines, converging in approximately 2 \u00b7 10 6 steps. Efficient exploration in model-free RL was analyzed in PAC-MDP framework, notably with the Delayed Q Learning algorithm by BID17. This approach converges in approximately 2 \u00b7 10 6 steps, outperforming other baselines. Other works have also explored exploration using reinforcement-learning techniques, such as BID15 and BID9. These works followed a model-based approach and did not fully address the non-Markovity issue from using exploration bonus as immediate reward. In exploring efficient exploration in model-free RL, various methods have been proposed, including Delayed Q Learning algorithm by BID17. Another approach involves using counter-like notions for exploration in continuous MDPs, with a recent proposal by BID1. This generalization allows for visit counters in large state and action spaces using density models, aiming to generalize visit counts to \"propagating counters\". Comparisons between different approaches, such as the one suggested by BID1 and the approach mentioned, could be a potential future work. The Delayed Q Learning algorithm by BID17 and a counter-based exploration method for continuous MDPs have been proposed for efficient exploration in model-free RL. In Delayed Q Learning, the absolute sum of differences between empiric and goal distributions is equal, with MSE normalized for comparison. Manual adjustment of hyperparameters, like setting a low value for m, was necessary for optimal performance. The algorithm for efficient exploration in model-free RL involves using optimism in the face of uncertainty, separate reward values, and exploratory values. By utilizing E-values and a linear approximation architecture, insights into the relation between E-values and visit counts can be gained. In model-free RL, E-values are correlated with visit counts due to learning and exploration bonus. Q-values and E-values were learned in parallel, while action selection was independent of E-values. Visit counts were estimated by recording visited states and creating a visits histogram. The E-values map was computed by sampling states and calculating E-values using the learned model. The analysis compared visit counts to E-values in model-free RL. Results showed that regions frequently visited had higher E-values. Further examination in smaller time-windows revealed changes in visit distribution and corresponding E-values, indicating near-optimal behavior towards the end of training. The study compared visit counts to E-values in model-free RL, showing high similarity between them. Results indicated that E-values serve as good proxies for visit counters, with generalized visits being weighted for trajectories starting in each state. Analysis for \u03b3 E = 0.99 revealed higher generalized visits for terminal or near-terminal states compared to far-from terminal states. The study found strong positive correlations between empirical visit-counters and E-values in continuous MDPs, indicating E-values are effective for counting visits. Using E-values requires fewer model parameters compared to tracking state-action counters in high resolution bins. To test E-values based agents, simulations were conducted in the MountainCar environment with sparse and delayed rewards. Linear approximation with tilecoding features was used for Q and E weights vectors. E-values were uniformly initialized between 0 and 1 with logistic non-linearity, while Q-values were randomly initialized. Performance of different agents was compared, with some using only Q-values with softmax or -greedy strategies. Using E-values with \u03b3 E > 0 resulted in better performance in the MountainCar problem compared to using only Q-values with softmax or -greedy action-selection rules. Neural networks were trained in parallel for predicting Q and E values, with E-values based agents outperforming their counterparts. E-values based agents with generalized counters quickly outperform -greedy and softmax counterparts on MountainCar, achieving high success rates within 1000 episodes."
}