{
    "title": "SJtChcgAW",
    "content": "Recent DNN pruning algorithms have successfully reduced parameters in fully connected layers without significant loss in classification accuracy. A new pruning algorithm based on difference of convex (DC) optimization is proposed in this paper, offering faster results compared to existing methods. The method allows for controlled accuracy degradation and outperforms Hard Thresholding at sparsity levels above 90%. Deep neural networks have high computational requirements and memory storage needs, limiting their use in embedded systems and portable devices. Various methods, such as introducing bayesian priors and binarization, have been proposed to reduce DNN size without sacrificing classification performance. Some methods for compressing deep neural networks include the hashing trick, tensorisation, and efficient matrix factorisations. Trained DNN models are used as feature extractors for transfer learning, but a cheap compression method is needed for those without dedicated hardware. Recent work includes Net-Trim and LOBS algorithms for layerwise pruning by loss function approximation. In this work, a cheap pruning algorithm for dense layers of DNNs is introduced, along with a theoretical analysis of how pruning affects the Generalization Error of the trained classifier. The authors conduct a theoretical analysis using the Lipschitz properties of DNNs to show the stability of latent representations after pruning. The sparsity-inducing objective proposed in previous work is also discussed, with empirical results showing the effectiveness of the algorithm. Our algorithm for pruning dense layers of DNNs is significantly faster than other methods, allowing for controlled degradation in Generalization Error. The theoretical analysis shows the stability of latent representations after pruning, with empirical results supporting the effectiveness of the approach. Notation is used to denote matrices, vectors, scalars, and sets. The Net-Trim formulation can be represented as a difference of convex functions problem in the context of pruning dense layers of DNNs. The algorithm is faster than other methods, allowing for controlled degradation in Generalization Error, with stable latent representations post-pruning. Notation is used to denote matrices, vectors, scalars, and sets. The optimization problem involves inputs a and outputs b of a fully connected layer with a rectifier non-linearity. The objective is non-convex but can be cast as a difference of convex functions problem. The algorithm allows for controlled degradation in Generalization Error with stable latent representations post-pruning. The optimization problem for a fully connected layer with a rectifier non-linearity can be solved using the DCA algorithm, which linearizes the non-convex part of the objective function. DCA is known for efficiently optimizing DC programs and has been successful in optimizing fully connected DNN layers. Proximal Stochastic Gradient Descent (Prox-SG) is proposed to solve the linearized problem efficiently. Algorithm 2a details the procedure, where a minibatch A and B is drawn at each iteration for gradient calculation and step updates. FeTa, Fast and Efficient Trimming Algorithm, is proposed as an improvement over Proximal Stochastic Gradient Descent (Prox-SG). It introduces Accelerated Proximal SVRG (Acc-Prox-SVRG) for better convergence. The hyperparameters for Acc-Prox-SVRG are the acceleration parameter \u03b2 and the gradient step \u03b7, with \u03b2 = 0.95 and \u03b7 \u2208 {0.001, 0.0001} giving the best results in experiments. The Trimming Algorithm compares the original and pruned representations in a testing set, using linear operators with frame bounds. The distance between the two representations is analyzed, and the generalization error of the pruned architecture is bounded using tools from the robustness framework. Deep Neural Network classifiers are defined, with layers represented by parameters. The output of each layer is denoted as z_l. The Trimming Algorithm compares original and pruned representations using linear operators with frame bounds. Deep Neural Network classifiers have layers with parameters, denoted as z_l. The classification margin and score definitions from BID19 are used to measure generalization error. The Trimming Algorithm compares original and pruned representations of Deep Neural Network classifiers using linear operators with frame bounds. The algorithm prunes classifier g(x) on layer i to obtain a new classifier g. The classification margin and score definitions from BID19 are used to measure generalization error. The pruning algorithm prunes classifier g 1 (x) on layer i to obtain a new classifier g 2 (x). The bound for the classifier depends on constants related to the data manifold, intrinsic data dimensionality, and spectral norm of hidden layers. More complex datasets lead to less robust DNNs, as observed empirically. The pruning error is multiplied by the factor i>i ||W i || 2, making the result pessimistic. The pruning algorithm prunes classifier g 1 (x) on layer i to obtain a new classifier g 2 (x). The result is pessimistic as the pruning error \u221a C 2 is multiplied by the factor i>i ||W i || 2, leading to exponential growth in the GE with respect to the remaining layer depth. This aligns with previous findings that layers closer to the input are less robust. The algorithm is applied to fully connected layers of a DNN, closer to the output, and can be extended to include pruning of multiple layers. The pruning algorithm prunes classifier g 1 (x) on layer i to obtain a new classifier g 2 (x). The bound predicts that when pruning multiple layers the GE will be much greater than the sum of the GEs for each individual pruning. Experimental comparisons were made between FeTa, LOBS, and NetTrim-ADMM using two optimized versions of FeTa. Core i7 and RAM 16GB 1600 MHz DDR3. Comparison of execution time between FeTa, LOBS, and NetTrim-ADMM algorithms. FeTa aims for 95% sparsity with specific input and output dimensions. FeTa's computational advantage over LOBS and NetTrim-ADMM in large input dimension settings is validated through synthetic experiments. In this section, experiments are conducted on a compression scheme with feedforward neural networks. The original full-precision network is compared with compressed networks such as FeTa 1, FeTa 2, Net-Trim, LOBS, and Hard Thresholding. Experiments were performed on the MNIST dataset containing 28x28 gray images from ten digit classes. The MNIST dataset consists of 28x28 gray images from ten digit classes. Training is done with 55000 images, validation with 5000, and testing with 10000. The LeNet-5 model is used. The CIFAR-10 dataset has 60000 32x32 color images for ten object classes, with 50000 used for training. Data augmentation includes random cropping and flips. A smaller variant of the AlexNet model is used for testing compression schemes like FeTa 1, FeTa 2, Net-Trim, LOBS, and Hard Thresholding. Hard Thresholding shows adequate performance up to 85% sparsity. FeTa, NetTrim, and LOBS show good results for various sparsity levels in pruning. FeTa outperforms Hard Thresholding above 85% sparsity, with FeTa 2 showing a 3% - 5% improvement over FeTa 1. FeTa achieves similar accuracy as Net-Trim for the LeNet-5 model but is faster. FeTa also has marginally lower accuracy than LOBS but significantly outperforms Thresholding. Overall, FeTa demonstrates competitive accuracy results while being efficient. FeTa achieves competitive accuracy results and is able to prune the dense layer much faster compared to other approaches. Net-Trim is not feasible for the CifarNet model due to high RAM requirements. FeTa is 8\u00d7 to 14\u00d7 faster than LOBS with marginally lower accuracy. Retraining can recover lost classification accuracy after pruning. Pruning both fully connected layers to the same sparsity level shows FeTa achieving similar or slightly worse results but with significant computation speedups for MNIST. Thresholding achieves a notably bad result of 64% accuracy, making it essentially inapplicable for multilayer pruning. Applying low-rank regularization on the learned matrix U, two methods are compared: FeTa 1 with optimized Acc-Prox-SVRG and Hard Thresholding of singular values. Results show significant degradation in classification accuracy for Hard Thresholding above 85% compression ratio. Theoretical analysis shows exponential GE drop as pruning moves from output layer. LeNet-5 trained to high accuracy, then single layer sparsity increased using Hard Thresholding. Layers closer to input less robust to pruning. Sudden accuracy increase at 90% sparsity possibly due to DNN size. Empirical results show smooth degradation in larger networks. Multilayer pruning bound tested, accuracy loss not simply additive for layer groups. Theoretical analysis demonstrates exponential drop in generalization error (GE) as pruning moves from output layer. Two pruned networks are compared, assumptions made to calculate ratio of GE values. Assumptions include k \u2248 40 and use of average layerwise errors. In this paper, an efficient pruning algorithm for fully connected layers of DNNs is presented, significantly faster than other methods with controlled Generalization Error degradation. Theoretical analysis shows that layers closer to the input are more sensitive to pruning. Experiments on feedforward architectures validate the results. The text discusses the smooth approximation of the rectifier non-linearity and introduces prior results for robust classifiers. It also presents generalization error bounds for classifiers operating on regular manifolds and DNN classifiers. The proof of Theorem 3.2 is outlined, emphasizing the importance of robustness in generic classifiers. The text introduces a loss function and defines expected error and training error. It presents Theorem 6.3 on robust classifiers and provides a corollary for C m-regular manifolds. Theorem 6.4 discusses classification margin and the classification of manifolds. The text discusses the classification margin for DNNs, specifically focusing on the training sample with the smallest score and the second best guess of the classifier. It also introduces notation for the proof of the main result. The proof introduces notation and calculations to derive a bound on the classification margin for DNNs, building on the concept of the training sample with the smallest score and the second best guess of the classifier. The proof of Theorem 3.3 involves deriving a bound on the classification margin for DNNs by considering the scores of points on the decision boundary between classifiers. The theorem is established through the application of Corollary 3.1.1."
}