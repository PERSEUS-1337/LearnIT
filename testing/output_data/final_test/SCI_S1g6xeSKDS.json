{
    "title": "S1g6xeSKDS",
    "content": "Recent work has shown that using geometric spaces with non-zero curvature instead of plain Euclidean spaces with zero curvature can enhance performance in Machine Learning tasks. A Mixed-curvature Variational Autoencoder has been developed as a way to train a VAE with a latent space that is a product of constant curvature Riemannian manifolds, allowing for the learning of per-component curvatures. This approach generalizes the Euclidean VAE to curved latent spaces, offering a more versatile model for generative tasks. Generative models aim to model data distribution in high-dimensional spaces, with some data having non-Euclidean latent structures. Images are often thought to exist on a lower-dimensional \"natural image manifold,\" allowing for continuous changes in interpretable images. Changing the geometry of the latent space can impact the underlying structure. Generative models aim to represent data in high-dimensional spaces with non-Euclidean latent structures. Recent methods introduce learning embeddings in spherical, hyperbolic, and combined spaces to match data geometry closely. An open question remains on choosing dimensionality and curvatures of partial spaces. Variational Autoencoders offer a solution to model data distribution effectively. Variational Autoencoders (VAEs) offer a solution to model data distribution effectively by sidestepping the intractability of joint probability models. Recent variants of VAEs have been introduced for spherical and hyperbolic latent spaces. A new approach generalizes VAEs to products of constant curvature spaces, allowing for better dimensionality reduction without significantly increasing model complexity. The resulting latent space is a non-constantly curved manifold in an ambient Euclidean space, providing more flexibility in assuming shapes similar to intrinsic manifolds. The text discusses a framework for manipulating representations and modeling probability distributions in products of constant curvature spaces. It introduces a method to generalize Variational Autoencoders for learning latent representations on these spaces, outperforming benchmarks on various datasets. Constant curvature spaces are defined by the sectional curvature of two linearly independent vectors at a point. In constant curvature spaces, the curvature is denoted as K, with a radius R = 1/|K|. Three types of manifolds can be defined based on the curvature sign: positively curved, \"flat,\" and negatively curved spaces. Realizations include the hypersphere, Euclidean space, and hyperboloid. Exponential, logarithmic maps, and parallel transport are essential in all considered spaces. The exponential and logarithmic maps, as well as parallel transport, are key operations in constant curvature spaces like the hypersphere, Euclidean space, and hyperboloid. These operations are essential for training VAEs in these spaces. However, the hypersphere and hyperboloid have a drawback where the norm of points does not converge as the curvature approaches 0, causing points to move away from the origin to satisfy the manifold's definition. The hyperboloid and hypersphere diverge from the origin as curvature approaches 0, making them unsuitable for learning sign-agnostic curvatures. Non-Euclidean spaces like the Poincar\u00e9 ball and projected sphere preserve angles through conformal projections, providing alternatives with isometric distance functions. The projection function \u03c1 K is defined for points in non-Euclidean spaces using stereographic projections, preserving angles and conformal metric tensors. The metric tensors are the same except for the sign of curvature K, with a defined conformal factor \u03bb K x. The curvature of the projected manifold is the same as the original manifold, with the inner product defined at x \u2208 M having a specific form. Two models corresponding to K > 0 and K < 0 can be defined, with the n-dimensional Poincar\u00e9 ball P n K for K < 0 having an induced distance function. Gyrovector spaces in non-Euclidean geometry are analogous to vector spaces, and the M\u00f6bius addition \u2295 K of x, y \u2208 M K is defined for both signs of K. The addition \u2295 K of x, y \u2208 M K is defined for both signs of K, leading to the definition of \"gyrospace distances\" for spaces with alternative curvatures. These distances are equivalent to their non-gyrospace variants, converging to the Euclidean distance function as K approaches 0. The concept of duality in constant curvature spaces is highlighted, connecting trigonometric, hyperbolic trigonometric, and exponential functions. In constant curvature spaces, the addition of x, y \u2208 M K is defined for both signs of K, leading to \"gyrospace distances\" for spaces with alternative curvatures. These distances converge to the Euclidean distance function as K approaches 0. The concept of duality in constant curvature spaces connects trigonometric, hyperbolic trigonometric, and exponential functions. Ganea et al. (2018a) and Tifrea et al. (2019) used gyrovector spaces to define exponential and logarithmic maps, as well as parallel transport in the Poincar\u00e9 ball, which can be applied to the projected hypersphere. Gyration (Ungar, 2008) is needed for parallel transport in both spaces. The operations in all manifolds are summarized in Table 1 and Table 2, proposing learning latent representations in products of constant curvature spaces. Our latent space M consists of component spaces Ki with constant curvature, resulting in a manifold M with non-constant curvature. Operations on the manifolds are element-wise, decomposing representations into parts x(i), applying operations on each part Ki(x(i)), and concatenating the results back. The product space's parametrization has degrees of freedom per component: the model M, dimensionality ni, and curvature Ki. In order to train Variational Autoencoders, we must choose a probability distribution as a prior and a corresponding posterior distribution family. These distributions need to be differentiable, have a differentiable KL divergence, and be reparametrizable. For distributions where the KL divergence does not have a closed-form solution or is too difficult to compute, alternative methods need to be considered. The Euclidean VAE uses a Gaussian distribution as a prior for its latent representations, satisfying VAE requirements and having maximum entropy properties. Different approaches to generalizing the Normal distribution to Riemannian manifolds are discussed, including the Wrapping approach that samples from a Gaussian distribution in the tangent space at \u00b5 0. The \"Restricted Normal\" approach involves mapping a point sampled from a Gaussian distribution in the tangent space at \u00b5 0 onto the manifold using parallel transport and the exponential map. This method is computationally effective but loses some theoretical properties. An example is the von Mises-Fisher distribution, which has a single scalar covariance parameter \u03ba. Maximizing entropy of distribution with known mean and covariance matrix leads to Riemannian Normal distribution. Wrapped Normal distributions are computationally efficient to sample from. The Riemannian Normal distributions are efficient for sampling and computing log probabilities, but can be computationally expensive. Different manifolds require defining an \"origin\" point for computation. The log-probability of samples can be computed using the distribution on various manifolds. The log-PDF for different spaces H, S, D, and P is derived in Appendix B. Variants of this distribution are reparametrizable and differentiable. The Wrapped Normal distribution converges to the Gaussian distribution as K \u2192 0. Learning latent representations in Riemannian manifolds requires changing the parametrization of mean and covariance in the VAE forward pass, and choosing prior and posterior distributions based on the manifold. In VAEs, the choice of prior and posterior distributions depends on the manifold type. Different distributions like Wrapped Normal, vMF, and Riemannian Normal are used based on the space type. The curvature constant can be changed in each space during training, allowing for flexibility in learning latent representations. During training, the curvature constant can be adjusted in each space. This flexibility allows for different training procedures, fixed curvature, and learnable curvature VAEs. The motivation behind changing the curvature in non-Euclidean spaces is to account for the specific positions of points in the space, affecting the decoder's dependency on pairwise distances. In fixed curvature VAEs, all latent spaces have a fixed curvature selected beforehand and remain constant throughout training and evaluation. Curvature values range from 0.25 to 1.0, with minimal impact on performance. In fixed curvature VAEs, latent spaces have a fixed curvature selected beforehand and remain constant throughout training and evaluation. Learnable curvature VAEs differentiate the ELBO with respect to the curvature K, treating it as a learnable parameter optimized using Stochastic Gradient Descent. The radius is constrained to be strictly positive in non-Euclidean spaces. Universal curvature VAEs require selecting the \"partitioning\" of the latent space beforehand. The proposed method involves partitioning the latent space into 2-dimensional components, initializing them as Euclidean, then splitting them into groups to become hyperbolic, spherical, or remain Euclidean. This approach aims to optimize the selection of components and their curvatures for training encoder/decoder maps. The method involves partitioning the latent space into 2-dimensional components and optimizing their curvatures for training encoder/decoder maps. The approach allows components to change from positively curved to negatively curved spaces. The universal curvature VAE assumes positively curved space as D and negatively curved space as P. For experiments, four datasets are used: Branching diffusion process (BDP), Dynamically-binarized MNIST digits, Dynamically-binarized Omniglot characters, and CIFAR-10. Models are trained with early stopping on training ELBO with specific epochs for each dataset. Models are compared using marginal log-likelihood with importance sampling. For experiments, four datasets are used: Branching diffusion process (BDP), Dynamically-binarized MNIST digits, Dynamically-binarized Omniglot characters, and CIFAR-10. Models are trained with early stopping on training ELBO with specific epochs for each dataset. Models are compared using marginal log-likelihood with importance sampling. Latent space dimension is estimated using 500 samples, except for CIFAR which uses 50 due to memory constraints. Standard gradient-based optimization methods are used for VAE parameters with Adam optimizer. Encoder and decoder architectures vary for different datasets. The decoder architecture consists of dense and transposed convolutional layers with different channel sizes. Models are trained with a fixed curvature for the first 10 epochs before switching to Stochastic Gradient Descent. All models use the Wrapped Normal distribution for optimization. The observation model for the reconstruction loss term uses Bernoulli distributions for MNIST and Omniglot, and standard Gaussian distributions for BDP and CIFAR. VAEs with fixed constant curvature in their latent space are used as baselines. Our models with a single component are equivalent to the Euclidean VAE. The Riemannian Normal and von MisesFischer distributions have a spherical covariance matrix. In evaluating VAE approaches with spherical covariance parametrization, the study found that VMF spherical components paired with other types performed better than Wrapped Normal components. Riemannian Normal VAEs performed well on their own, with the fixed Poincar\u00e9 VAE obtaining the best score. Single-component VAEs performed worse than product VAEs when learning curvature. The universal curvature VAE achieved good results, second only to the Riemannian Normal baseline. The study found that the VMF spherical components paired with other types outperformed Wrapped Normal components. The Riemannian Normal VAEs performed well, with the fixed Poincar\u00e9 VAE achieving the best score. The universal curvature VAE achieved good results, second only to the Riemannian Normal baseline. The study compared various VAE models with different curvatures, finding that the Riemannian Normal Poincar\u00e9 ball VAE performed the best. Single-component VAEs generally outperformed multi-component ones, with fixed curvature variants showing slightly better results. Our universal VAEs with negative curvatures perform similarly to the Euclidean baseline VAE. Models with fixed curvature perform better with small components, while learnable curvature is better for single big component VAEs. The Poincar\u00e9 VAE is the best baseline model for a latent space dimension of 6. The hyperbolic VAEs outperform spherical VAEs on the dataset, with models like D2xE2xP2 and U6 showing promise. NonEuclidean models perform better than the Euclidean baseline on CIFAR-10 reconstruction, with fixed hyperboloid H6-1 and learnable hypersphere S6 standing out. Curvatures for learnable models converge around -0.15 to +0.15. The Riemannian Normal Poincar\u00e9 ball VAE RN Pn is a strong model, but has practical limitations. The Wrapped Normal VAEs outperform von Mises-Fischer spherical VAEs in modeling latent spaces on constant curvature manifolds. Universal curvature models generally outperform Euclidean VAE baselines in lower-dimensional latent spaces and maintain competitive performance as dimensionality increases. Transforming latent spaces onto Riemannian manifolds allows for learning representations on curved space. Learning VAEs on products of constant curvature spaces involves deriving necessary operations in different models of constant curvature spaces, extending probability distribution families, and generalizing VAEs to latent spaces that are products of smaller component spaces with learnable curvature. This approach is competitive on various datasets and can recover the Euclidean VAE when curvatures go to 0. Riemannian manifolds are real, smooth manifolds that resemble linear spaces locally. A Riemannian manifold is a tuple (M, g) where M is a smooth manifold and g is the metric tensor. The manifold defines local geometry and induces global quantities by integrating local contributions. The metric tensor induces a norm on the tangent space, and a measure is induced as well. Straight lines are generalized to constant speed curves on the manifold. On a Riemannian manifold (M, g), geodesics are shortest paths between points x, y \u2208 M. The exponential map moves from a point x in a direction v with constant velocity. For geodesically complete manifolds, the exponential map is well-defined on the full tangent space. Parallel transport connects vectors in tangent spaces. Parallel transport PT x\u2192y : T x M \u2192 T y M is an isomorphism between tangent spaces, keeping transported vectors parallel to the connection. It defines a way to connect tangent spaces by moving vectors along geodesics. Different models of constant curvature space have advantages and disadvantages for learning latent representations using VAEs. Hyperboloid and hypersphere spaces have fewer numerical instabilities during optimization compared to projected spaces. However, when curvature approaches zero, point norms go to infinity, which can be a challenge when transitioning between hyperboloid and sphere spaces. Points in n-dimensional projected hypersphere and Poincar\u00e9 ball models can be visualized using real vectors of length n. However, optimizing functions over these models can lead to numerical instabilities due to points lying close to boundaries or far away from the origin. The distance function in n-dimensional space can lead to numerical instabilities when points are close to boundaries or far from the origin. Experimentation is done with projected spaces, hyperboloid, and hypersphere to compare performance empirically. The exponential map in Euclidean space is derived from the Pythagorean theorem. Parallel transport is not needed in Euclidean space as sampling from a Normal distribution is sufficient. Theorems for the hypersphere are trivial corollaries in the hyperboloid, with differences in using hyperbolic trigonometric functions. The hyperboloid can be projected from the ambient space using Euclidean distance normalization. The coordinates of a point on the hyperboloid are co-dependent, satisfying x, x L = 1/K. Points on the hyperboloid go to infinity as K approaches 0, indicating a flattening of the hyperboloid away from the origin. The hyperboloid can be projected from the ambient space using Euclidean distance normalization. Points on the hyperboloid go to infinity as K approaches 0, indicating a flattening away from the origin. The model is unsuitable for learning sign-agnostic curvatures, similar to the hypersphere. The logarithmic map in H n K maps y to a tangent vector at x, derived as an inverse function to the exponential map. The assumption |\u03b1| > 1 holds for all points x, y \u2208 H n K, due to Cauchy-Schwarz. The distance function in P n K is derived from the hyperbolic trigonometric functions sinh, cosh, and tanh, with notable differences from the Poincar\u00e9 ball. The theorems for the projected hypersphere are essentially trivial corollaries of their equivalents in the Poincar\u00e9 ball, leveraging hyperbolic trigonometric functions. The distance function in P n K is derived from hyperbolic trigonometric functions. Theorems show equivalence between Poincar\u00e9 distance and gyrospace distance, with convergence to Euclidean distance as K approaches 0. The Poincar\u00e9 distance in P n K converges to the Euclidean distance between fixed points x, y \u2208 P n K. The exponential map and parallel transport operations have been derived and implemented for the Poincar\u00e9 ball. Theorems for the hypersphere are trivial corollaries of their equivalents in the hyperboloid, with differences in the use of Euclidean trigonometric functions. The operations in the hypersphere use Euclidean trigonometric functions like sin, cos, and tan. Projections involve normalizing vectors to the hypersphere using the Pythagorean theorem. The n + 1 coordinates of a point on the sphere are interdependent, satisfying x, x 2 = 1/K. This allows for computing missing coordinates to place vectors on the sphere, useful for orthogonally projecting points. Points on the hypersphere are norm-constrained, going to infinity as K approaches 0. The sphere grows \"flatter\" as K approaches 0, with points on the sphere going to infinity. The model is unsuitable for learning sign-agnostic curvatures. The logarithmic map in S n K maps y to a tangent vector at x, with \u03b1 = K x, y 2. The text discusses parallel transport in S n K using a generic formula and the spherical logarithmic map formula. It mentions that the theorems for the projected hypersphere are essentially trivial corollaries of their equivalents in the Poincar\u00e9 ball, with differences in the use of Euclidean trigonometric functions. The text discusses trigonometric functions sin, cos, and tan, the Pythagorean theorem, and the homeomorphism between S n K and R n. It also mentions the distance function in D n K and the convergence of gyrospace distance to Euclidean in D n K. The proof involves heavy algebra and Mathematica. The spherical projected gyrospace distance between points x, y in D n K converges to the Euclidean distance as K approaches 0. The exponential map in D n K is derived using trigonometric functions and the geodesic has a specific form. The inverse formula for the exponential map in D n K simplifies for x := \u00b5 0 = (0, . . . , 0) T. Parallel transport on the projected sphere is derived similarly to the Poincar\u00e9 ball. The M\u00f6bius addition converges to Euclidean vector addition in both spaces. The exponential map converges to its Euclidean variant. The logarithmic map converges to its Euclidean variant due to the theorem of limits of composed functions and Lemma A.16. The transformation function has the form f = exp, and the derivative of parallel transport can be computed using an orthonormal basis. The determinant of the exponential map Jacobian is computed using an orthonormal basis. The basis vectors are orthogonal, and the determinant is a product of the norms of the computed changes for each basis vector. Additionally, certain properties hold, leading to a proof similar to Theorem B.1 on a manifold changing from H n. The curr_chunk discusses the transformation function on a manifold changing from H n K to S n K, with the determinant of the exponential map Jacobian computed using an orthonormal basis. The changes with respect to each basis vector are computed, showing how the basis vectors are orthogonal and parallel. The curr_chunk discusses the properties of an orthonormal basis and the determinant of the exponential map Jacobian. It also mentions the duality between spaces of constant curvature and various theorems related to geometry. The curr_chunk discusses a unified model of geometries using the null cone of a Minkowski space, including the hyperboloid Hn and stereographic projections. The area of a circle of radius r in a space of constant curvature K is also mentioned, with A(r) depending on the sign of K. Li et al. define a universal geometry of constant curvature spaces. The variational autoencoder (VAE) was introduced by Kingma & Welling (2014) and Rezende et al. (2014). Improvements on the VAE include different encoder and decoder maps such as neural networks. Extensions like GraphVAE using graph convolutional neural networks were proposed for different data domains. Autoregressive flows and small changes to the ELBO loss have also enhanced the basic VAE framework. An important work in the area of variational autoencoders is \u03b2-VAE, which improves sample quality and disentanglement of latent dimensions. Geometric deep learning leverages non-Euclidean geometry for representation learning, expanding from knowledge-base and graph representation methods to non-Euclidean spaces. Research in geometric deep learning has expanded to include approaches for hyperbolic and spherical spaces, requiring new Riemannian optimization methods. Gu et al. (2019) proposed generalizing to products of constant curvature Riemannian manifolds, showing benefits in graph reconstruction and word analogy tasks. Non-Euclidean geometry benefits learning lower-dimensional representations. Anonymous (2020) used product spaces with constant curvature components to train Graph Convolutional Networks. Arvanitidis et al. (2018) explored how a Euclidean VAE improves sample and latent representation quality with a non-Euclidean Riemannian metric. Gaussian distribution works well in VAEs due to known variance and no constraints on higher-order moments. Non-Euclidean geometry is being used in learning variational methods. Recently, non-Euclidean geometry has been utilized in learning variational autoencoders. Different approaches have generalized VAEs to spherical and hyperbolic spaces using distributions like von MisesFischer, Wrapped Normal, and Riemannian Normal. Our approach generalizes on geometrical VAE work by employing a \"products of spaces\" approach and unifying different approaches into a single framework for all spaces of constant curvature. There are promising directions for exploration, such as trying mixed-curvature VAEs on graph data and investigating the variance in obtained results. The study aims to investigate the variance in results across runs and reduce it, noting optimization challenges in spherical spaces. Introducing products of spaces has helped address some optimization issues, with observations that VAEs in higher dimensions may benefit from a subdivided space approach. In higher dimensions, VAEs with learnable curvature diverge compared to those with fixed curvature. Normalizing Flows show promise for variational inference, offering flexibility in modeled distributions but are computationally expensive. Extending the defined approach could be an interesting future direction. Extending the defined geometrical models to train GANs in constant curvature spaces could improve sample quality. Adversarially trained autoencoders in Riemannian manifolds aim for good sample quality and a well-formed latent space. Results show learned curvature across epochs and a qualitative comparison of reconstruction quality on the MNIST dataset. Results from models trained on the MNIST dataset show a summary of results with a latent space dimension of 6 and diagonal covariance parametrization on the CIFAR dataset. Standard deviation values below indicate instability in the experiments."
}