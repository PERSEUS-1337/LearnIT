{
    "title": "HkxjYoCqKX",
    "content": "Neural network quantization is a key research area for deploying large models on resource-constrained devices. A differentiable quantization procedure is introduced to train networks that can be discretized without performance loss. This involves transforming continuous distributions to categorical distributions over a quantization grid, allowing for efficient gradient-based optimization. The method is validated on MNIST, CIFAR 10, and Imagenet classification tasks. Neural network compression, specifically quantization, is crucial for deploying large models on resource-constrained devices. Quantization reduces precision of arithmetic operations in the network by discretizing weights and activations. This method allows for efficient gradient-based optimization and has been validated on various classification tasks. The purpose of this work is to introduce Relaxed Quantization (RQ), a novel quantization procedure that smooths the non-differentiability of the quantization operation during training. It allows for the optimization of quantization targets with gradient descent and discretizes the network by converting distributions over weights and activations to categorical distributions. The proposed Relaxed Quantization (RQ) procedure smooths the non-differentiability of quantization during training by converting distributions over weights and activations to categorical distributions over a quantization grid. Stochastic rounding is shown to be a special case of this framework. The proposed framework introduces a quantizer for discretizing weights and activations in a neural network. The quantizer converts a continuous signal into a countable set of values, but it is inherently lossy and non-invertible. The rounding function is a simple quantizer, but it cannot be directly applied due to its non-invertible nature. Training neural networks with quantization effects can help improve robustness and decrease the performance gap between full-precision and discretized versions. However, the non-differentiable nature of the rounding process hinders direct optimization with stochastic gradient descent. A \"smooth\" quantizer is proposed to enable gradient-based optimization, consisting of a vocabulary, noise model, discretization procedure, and relaxation step. The quantization procedure involves a vocabulary grid of output values and parameters for adaptability to input signals. The noise model determines the likelihood of input signal values moving to grid points. The quantization procedure involves discretizing the input signal with noise modeled as a logistic distribution. This noise is added to the input signal and governed by a distribution, resulting in a smooth function of its input. The noise distribution is assumed to be zero mean logistic with standard deviation \u03c3, leading to a sigmoid function for evaluation and backpropagation. The quantization procedure involves discretizing the input signal with noise modeled as a logistic distribution, leading to a sigmoid function for evaluation and backpropagation. Gaussian distributions were found to be less effective in preliminary experiments. The quantizer determines appropriate assignments for each realization of the input signal x, resulting in a stochastic quantizer due to the stochastic nature of x. A categorical distribution is constructed over the grid points to select the appropriate assignment for x. The quantization procedure involves discretizing the input signal with noise modeled as a logistic distribution. A categorical distribution is constructed over the grid points to select the appropriate assignment for x, emulating quantization noise to prevent overfitting. This noise can be reduced by clustering weights and activations around grid points and decreasing logistic noise \u03c3. During optimization, the logistic noise \u03c3 is initialized in a sensible range to cover a significant portion of the grid, allowing for gradient flow. The optimization procedure aims to minimize the noise level to reduce the gap between training and test time computations. Additionally, a continuous relaxation method is used for non-differentiable categorical distribution sampling to address high variances. The optimization procedure aims to minimize noise levels for gradient flow and reduce the gap between training and test time computations. A continuous relaxation method is used to address high variances by replacing the categorical distribution with a concrete distribution, encouraging real-valued weights to cluster into multiple modes. The network converged to ternary weights for the input and binary weights for the output layer, resembling a \"noisy\" softmax. The \"Relaxed Quantization (RQ)\" algorithm introduces a fully differentiable \"soft\" quantization procedure using a concrete distribution with a temperature parameter \u03bb. This allows for stochastic gradients for quantizer parameters and input signals in neural networks. The algorithm addresses the issue of noise at the input by enabling gradient-based optimization for both the input signal x and the noise parameter \u03c3. The \"RQ ST\" algorithm proposes an alternative approach to ensure that only grid points are sampled during quantization. It uses a variant of the straight-through estimator for gradient estimation, which may be biased but still effective in practice. This method allows for a \"hard\" quantization procedure after convergence. After convergence, a \"hard\" quantization procedure can be achieved by selecting points from the grid at test time. This paper opts for rounding to the nearest grid point instead of using a categorical distribution, aligning better with low-resource environments. Two quantization grids with learnable parameters are employed for weights and activations. Sampling based on drawing random numbers for the concrete distribution can be costly for larger values of K. During testing, quantization of weights and activations in a neural network can be computationally intensive and memory-consuming. To address this, a localized grid around the signal x is used by truncating the probability distribution to points within a certain distance. This simplifies computation by focusing on the closest grid point to x. The localized grid around the signal x simplifies computation by focusing on the closest grid point for determining probabilities. Stochastic rounding, introduced by BID8, is a popular technique for training neural networks with reduced numerical precision. Stochastic rounding is a powerful quantization scheme that relies on biased gradient estimates. RQ provides a way to optimize a surrogate objective to circumvent this bias, while RQ ST uses the straight-through estimator to avoid optimizing a surrogate objective, at the cost of biased gradients. Compared to stochastic rounding, RQ ST allows sampling of probabilities for the two closest grid points. In hardware-oriented quantization approaches, RQ ST allows sampling of grid points based on input noise \u03c3, enabling larger steps in the input space without decreasing variance. Quantizing all operations within the network aims to reduce execution times, different from focusing only on weight quantization and network compression. Other approaches consider binarizing or ternarizing weights. The work explores binarizing or ternarizing weights and activations using the straight-through gradient estimator for fast convolution implementations. Another approach involves quantizing networks through regularization, with a variational method for determining the required bit-width precision for each weight. Additionally, a quantizing prior is proposed to encourage ternary weights during training. Regularization strength decreases with training data size, affecting effectiveness on large datasets. Non-uniform quantization in neural networks is more efficient due to uneven weight distribution within layers. Various methods like stochastic quantization and gradient descent-based code book learning have been proposed. Clustering-based quantization is also effective, representing weights by cluster centroids. Recent work on non-uniform grids for quantizing neural networks shows promise for efficient implementations on modern hardware. Different approaches in the literature, such as knowledge distillation and modifying architecture for lower precision computations, offer potential for further improvements. Some methods involve step-by-step quantization from input to output layers, allowing for easier adaptation to rounding errors. Proposed procedures with concrete sampling and the Gumbel-softmax straight-through estimator are used in subsequent experiments. The procedure uses the Gumbel-softmax estimator for gradients without optimizing grid offsets to represent zero exactly, allowing for sparsity and zero-padding. Experimental details are provided in Appendix A. Results of stochastic rounding with dynamic fixed point format are also presented. Experiments were conducted using TensorFlow and Keras libraries on different network architectures for MNIST and CIFAR 10 datasets. Our method achieves competitive results on the CIFAR 10 dataset, improving upon recent works on neural network quantization. We found that gradient variance for low bit-widths needs to be controlled with appropriate learning rates. The effectiveness of our approach was demonstrated on tasks involving Resnet-18 and Mobilenet models trained on the Imagenet dataset. The Mobilenet model was initialized with a pretrained model from the tensorflow github repository and quantized for various bit-widths through fine-tuning. Quantizing the first and last layers can significantly increase computation, as shown by the BOPs metric. Keeping the first layer in full precision requires roughly 1.3 times as many BOPs for one forward pass through the network. Comparing various quantization methods on Resnet-18 and Mobilenet, the study shows that keeping the first layer in full precision requires 1.3 times more BOPs for one forward pass through the network. The comparison includes fixed-point quantization methods like BID8 and BID9, as well as other techniques like rounding and Bayesian Comp. In the study comparing quantization methods on ResNet-18 and MobileNet, RQ variants, SYQ, Apprentice, and Jacob et al. (2017) form the \"Pareto frontier\" for accuracy and efficiency trade-off. RQ shows improvement over its pretrained model, while SR+DR underperforms and is worse than simple rounding for 5 to 8 bits. For MobileNet, RQ is competitive with existing approaches, while simple rounding results in random chance for all bit configurations. SR+DR performs well for 8-bit scenarios. Relaxed Quantization (RQ) is a powerful algorithm for learning low-bit neural networks, outperforming competitive approaches in lower bit regimes. It allows for better trade-offs between accuracy and bit operations per second, and can be easily extended for non-uniform quantization in future hardware. BID17 BID25 demonstrate the benefits of low-bit floating point weights that can be efficiently implemented in hardware. Relaxed Quantization (RQ) is a powerful algorithm for learning low-bit neural networks, outperforming competitive approaches in lower bit regimes. It allows for better trade-offs between accuracy and bit operations per second. In future work, exploring non-uniform quantization is a possibility. The experiments involved implementing batch normalization as a sequence of convolution, batch normalization, and quantization. On low-precision chips, batch normalization would be integrated into the convolution kernel and bias, resulting in rounding to low precision. The proposed algorithm involves folding batchnorm into the convolution kernel and bias, rounding to low precision. Quantization is essential for fast model evaluation on low-precision hardware, and is complementary to network pruning methods like L0 regularization BID21. The grid width \u03b1 is initialized based on the input values and bit-width, with a slightly larger width for stochastic inputs. The algorithm involves folding batchnorm into the convolution kernel and bias, rounding to low precision for fast model evaluation on low-precision hardware. The grid width \u03b1 is initialized based on input values and bit-width, with a slightly larger width for stochastic inputs. For activations, \u03b1 is adjusted based on the bit-width to control quantization noise. The standard deviation of logistic noise is set to be three times smaller than \u03b1. Moving averages of layer statistics for batch normalization may not accurately reflect the quantized model's statistics. Even though RQ aims to minimize the gap between training and testing phase, aggregated statistics with batch normalization parameters lead to decreased test performance. Insights from BID26 were applied to recompute quantized model statistics before reporting final test error rate. Final models were determined through early stopping using validation loss with minibatch statistics. MNIST experiment involved rescaling input to [-1, 1], no regularization, training with Adam and batch size of 128. Local grid used for bit width larger than 2 for weights, biases, and ReLU outputs. Temperature \u03bb of 2 for 8 and 4 bit networks, and 2 bit networks. For the CIFAR 10 experiment, hyperparameters were adjusted compared to the LeNet-5 experiments. The 8 and 4 bit networks were trained for 300 epochs with a learning rate of 1e-4 and a batch size of 100. A weight decay term of 1e-4 was included for the 8 bit networks. The 2 bit model started with a learning rate of 1e-3. The VGG model utilized batch normalization layers after each convolutional layer, following max pooling if present. Training with RQ added a sampling burden for every weight and activation. The study investigated the impact of noise introduced by training a neural network with RQ on convergence speed. Results showed that a 2/2 bit RQ-VGG network on CIFAR 10 had similar trends to the full precision baseline, indicating noise was not detrimental for this model. Training the RQ model with a full grid took 15 times longer than the high-precision baseline. The study involved training neural networks with RQ on CIFAR 10 using a single Titan-X Nvidia GPU. Input images were preprocessed by subtracting the mean and dividing by the standard deviation. Data augmentation included random crops and flips. The base Resnet-18 model was trained with stochastic gradient descent, weight decay, and specific learning rate adjustments. The quantized model fine-tuning phase used Adam with different parameters. Error rates for Resnet-18 and Mobilenet are shown in TAB4. The study involved training neural networks with RQ on CIFAR 10 using a single Titan-X Nvidia GPU. Input images were preprocessed by subtracting the mean and dividing by the standard deviation. Data augmentation included random crops and flips. The base Resnet-18 model was trained with stochastic gradient descent, weight decay, and specific learning rate adjustments. The quantized model fine-tuning phase used Adam with different parameters. Error rates for Resnet-18 and Mobilenet are shown in TAB4. The code used for training was obtained from a specific GitHub repository and modified to include quantization operations. The high-precision Resnet18 model was trained for 90 epochs with a batch size of 128, with a learning rate scheduling involving a \"warm up\" period. The final quantized model achieved 29.53% top-1 error and 10.44% top-5 error after being fine-tuned for 10 epochs with a constant learning rate of 1e \u22124."
}