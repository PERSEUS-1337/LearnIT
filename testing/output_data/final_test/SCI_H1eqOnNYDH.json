{
    "title": "H1eqOnNYDH",
    "content": "Modern deep artificial neural networks achieve impressive results with regularization techniques like weight decay and dropout. Data augmentation increases training examples without reducing model capacity or introducing sensitive hyper-parameters. In this paper, the effectiveness of data augmentation is compared to explicit regularization on various architectures and datasets. Results show that data augmentation alone can achieve equal or better performance compared to regularized models, with higher adaptability to changes in architecture and training data. Regularization is crucial in deep learning to prevent overfitting due to the large number of parameters in neural networks. Regularization techniques such as weight decay, dropout, and batch normalization are essential in deep learning to prevent overfitting. Various methods, including unsupervised pre-training and adversarial training, also contribute to regularization effects in modern deep neural networks. In deep learning, various techniques like weight decay, dropout, and batch normalization help prevent overfitting. Research efforts focus on training deeper networks with larger capacity, but explicit regularization methods like weight decay and dropout can limit this capacity. Data augmentation, unlike explicit regularization, improves generalization without wasting capacity. Data augmentation is a widely used technique in deep learning that involves synthetically expanding a dataset by applying transformations to existing examples. It has been identified as a critical component in successful models like AlexNet, All-CNN, and ResNet. While popular in computer vision, data augmentation has also proven effective in speech recognition, music source separation, and text categorization. This technique serves as an implicit regularizer, improving generalization without reducing model capacity. Recently, the deep learning community has recognized the importance of data augmentation and new techniques like cutout and augmentation in the feature space have been proposed. Some models now automatically learn data transformations, setting a promising path for future research. However, a study found that traditional data augmentation remains one of the most successful techniques for object recognition. Despite its popularity, there is a lack of systematic analysis on the impact of data augmentation on convolutional neural networks compared to explicit regularization. The importance of data augmentation in deep learning has been recognized, with traditional techniques remaining successful for object recognition. However, there is a lack of systematic analysis on the impact of data augmentation on convolutional neural networks compared to explicit regularization. Zhang et al. (2017) included data augmentation in their analysis of generalization of deep networks, but it was questionably considered an explicit regularizer. The first systematic contrast between data augmentation and explicit regularization was done by Hern\u00e1ndez-Garc\u00eda & K\u00f6nig (2018b). This work aims to extend that study with more empirical results and a theoretical discussion, proposing definitions of explicit and implicit regularization to address ambiguity in the literature. The text discusses the importance of data augmentation in deep learning compared to explicit regularization. It includes a theoretical discussion on statistical learning theory, an empirical analysis of model performance, adaptability to learning from fewer examples, and a discussion on the benefits of data augmentation in deep learning. Zhang et al. (2017) raised the idea that explicit regularization may improve generalization performance but is not necessary or sufficient on its own. The authors suggest rethinking generalization in deep learning by considering the role of implicit regularization and data augmentation. Zhang et al. (2017) may have underestimated implicit regularization and viewed data augmentation as a form of explicit regularization. The authors propose clear definitions for explicit and implicit regularization techniques in deep learning. Explicit regularization reduces model capacity, while implicit regularization reduces generalization error without affecting capacity. This distinction aims to clarify the use of these terms in the literature. In machine learning, explicit regularization techniques like L p -norm regularization and weight decay constrain the model's capacity by penalizing the L 2 norm of parameters. Dropout and stochastic depth are examples of explicit regularization that reduce the hypothesis set during training. Implicit regularization in deep neural networks also helps in reducing generalization error without affecting model capacity. Implicit regularization in machine learning refers to a regularization effect that can be provided by various elements, such as stochastic gradient descent and batch normalization. Early stopping also offers implicit regularization by reducing effective capacity without limiting representational capacity. Previous work has sometimes categorized data augmentation as explicit regularization based on practitioner intention. Data augmentation is considered implicit regularization, affecting model capacity. Generalization of model class H can be analyzed using complexity measures like Rademacher complexity. For binary classification and linear separators, generalization error can be bounded using Rademacher complexity with probability 1 \u2212 \u03b4. Tighter bounds for model classes like fully connected neural networks can be obtained. Theoretical insights on generalization of neural networks, explicit vs implicit regularization, and the impact of data augmentation on increasing training examples for improved generalization guarantees. The impact of data augmentation on generalization is complex and not fully understood. Some studies have started analyzing the effects of data transformations on generalization. Explicit regularization methods aim to improve generalization by constraining the hypothesis class, reducing complexity, and ultimately reducing generalization error. Data augmentation leverages domain knowledge, while explicit regularization methods focus on constraining hypothesis classes. Recent research has shown that weight decay and dropout, commonly used as regularization methods for neural networks, have limited impact on generalization bounds and confidence margins. While dropout's effects on neural networks are still not fully understood, it can also be seen as a form of data augmentation without domain knowledge. Recent research has shown that weight decay and dropout, commonly used as regularization methods for neural networks, may not be necessary as neural networks are already implicitly regularized. Dropout can be viewed as a form of data augmentation without domain knowledge, and many forms of explicit regularization are approximately equivalent to adding random noise to training examples. This suggests that more sophisticated data augmentation techniques could overshadow the benefits of explicit regularization. Neural networks are implicitly regularized by elements like SGD, convolutional layers, normalization, and data augmentation. Linear models optimized with SGD converge to solutions with small norm without explicit regularization. Experiments compare data augmentation with weight decay and dropout in deep neural networks. The curr_chunk discusses three popular architectures for object recognition tasks: All-CNN, WRN, and DenseNet. Each network has specific features and parameters, with All-CNN being relatively shallow with few parameters. The experiments aim to compare data augmentation and explicit regularization in response to changes in architecture. In addition to the All-CNN, WRN, and DenseNet architectures discussed earlier, experiments were conducted on shallower and deeper versions of the networks to analyze the adaptability of data augmentation and regularization. The WRN-28-10 version and DenseNet-BC with specific parameters were used for the experiments on the ImageNet dataset. The role of data augmentation was analyzed by training network architectures with two different augmentation schemes on ImageNet: light augmentation with horizontal flips and translations, and heavier augmentation with affine transformations, contrast, and brightness adjustments. The choice of transformations was arbitrary to ensure objects remained recognizable. Details of the heavier scheme can be found in the supplementary material. The experiments compared the impact of data augmentation and explicit regularization on convolutional neural networks. Different architectures were trained on various datasets with and without regularization. Performance was evaluated on test sets using Keras on TensorFlow with a single GPU. Results of the experiments are presented, starting with the original architectures in section 5.1. The experiments in sections 5.2 and 5.3 compare models trained with different levels of data augmentation and explicit regularization. Results are shown using purple bars for models without regularization and red bars for models with it. The figures highlight relative performance compared to a baseline, with detailed results in the supplementary material. CIFAR results show top-1 test accuracy, while ImageNet results show top-5 accuracy. The results on CIFAR show top-1 test accuracy, while on ImageNet, top-5 accuracy is reported. Figure 1 illustrates the improvement from data augmentation and explicit regularization on baseline models. Data augmentation alone can outperform models with weight decay and dropout. The comparison between data augmentation and weight decay/dropout regularization on original networks is shown in Figure 1. Key conclusions can be drawn from the results in Figure 1 and Table 1. Training with data augmentation alone improves performance as much as or more than training with both augmentation and explicit regularization. Models trained with explicit regularization show a significant drop in performance compared to models trained with only data augmentation, especially as the amount of training data decreases. This surprising result indicates that even state-of-the-art architectures with weight decay and dropout may not outperform models with data augmentation alone. Simply removing weight decay and dropout, while keeping all other hyperparameters intact, improved accuracy in 4 out of 8 cases studied. Data augmentation had a larger impact on accuracy compared to weight decay and dropout regularization. Light augmentation increased accuracy by 8.46% on average. Heavier augmentation schemes improved test performance on CIFAR-10 and CIFAR-100 datasets but not on ImageNet due to its complexity. The effects of weight decay and dropout regularization on model performance are more consistent without explicit regularization. Data augmentation alone can achieve and often improve performance gains. Explicit regularization techniques have poor adaptability to changes in hyperparameter conditions, as shown by a drop in performance when the architecture is altered in explicitly regularized models. In experiments without explicit regularization, models show slight variations from the original architecture when trained with fewer examples. The models are evaluated using the same test set and data subset. Results show that explicit regularization alone struggles to maintain performance when training data is reduced. When explicit regularization is applied, it struggles to maintain performance with reduced training data. Models trained with explicit regularization alone achieve lower accuracy compared to models without it. Combining explicit regularization with data augmentation also leads to worse performance. The combination is only slightly better than training without data augmentation. The combination of explicit regularization and data augmentation slightly outperforms training without data augmentation. This could be due to the original regularization hyperparameters not adapting well to new conditions and reducing the model's capacity to take advantage of augmented data. Models trained without explicit regularization adapt better to reduced data availability, achieving 91.5% performance with 50% of the data and nearly 70% with 10% of the data. This highlights the effectiveness of data augmentation in improving model performance. The adaptability of data augmentation and explicit regularization to changes in the depth of the All-CNN architecture is tested. Models trained with weight decay and dropout suffer a drop in performance when the architecture changes, while models trained without explicit regularization show better performance. The models trained without explicit regularization show better performance with a deeper architecture, while models with regularization suffer a drop in performance. Data augmentation benefits are more pronounced on CIFAR-100 due to differences in hyperparameters tuning. The effectiveness of data augmentation hyperparameters depends on the type of data, not the architecture or training data amount. Removing explicit regularization and using data augmentation increases model flexibility. Data augmentation plays a crucial role in deep convolutional neural networks for object recognition, compared to explicit regularization techniques like weight decay and dropout. Definitions of explicit and implicit regularization were clarified to facilitate the discussion. Data augmentation is argued to have advantages over explicit regularization in deep learning. Empirical evidence shows that explicit regularization like weight decay and dropout may not be necessary, as data augmentation alone can achieve similar generalization gains. Weight decay and dropout are less adaptable to changes in architecture and training data. The study used different network architectures and datasets to increase the validity of the conclusions, which should be confirmed by future research. Future work will explore a wider range of models, datasets, and domains such as text or speech. The study took a conservative approach by keeping all hyperparameters the same as in the original models, including weight decay, dropout, and light augmentation. The findings suggest that explicit regularization may not be necessary for generalization in deep learning, as data augmentation alone can achieve similar gains. The study did not focus on optimizing data augmentation techniques, but future work will propose schemes that can be more effectively utilized by deep models. The study suggests that explicit regularization may not be essential for generalization in deep learning, as data augmentation alone can yield similar benefits. Implicit regularization elements like stochastic gradient descent, convolutional layers, and data augmentation already effectively regularize models, making explicit regularizers potentially unnecessary. Data augmentation is often seen as cheating in machine learning, while weight decay and dropout are widely accepted. The deep learning community should reconsider the role of data augmentation and explicit regularization. Data augmentation offers advantages such as increasing model robustness without reducing representational capacity. Data augmentation increases model robustness and can be seen as a data-dependent prior. It yields models with smaller sensitivity to perturbations and can lead to representations similar to the IT cortex. Deep neural networks are well suited for data augmentation due to their ability to shatter the augmented training set. Unlike explicit regularization, data augmentation can be performed in parallel to gradient updates. Data augmentation is a strong alternative to explicit regularization techniques, adapting naturally to different architectures and training data amounts. It can be designed for a broad range of data and tasks, offering robustness and model sensitivity. The appendix details network architectures used in experiments: All-CNN, Wide Residual Network (WRN), and DenseNet. All-CNN is a simple network with few layers, WRN is deeper with residual connections, and DenseNet is densely connected and parameter effective. The network architectures used in experiments include All-CNN with 16 layers and 9.4 million parameters for ImageNet, and 12 layers with 1.3 million parameters for CIFAR. Different versions with varying depths and parameters were also tested. The experiments used All-CNN networks with different depths and parameters, trained with specific settings including batch normalization, Nesterov momentum, learning rate decay, and Xavier uniform initialization. WRN-28-10, a modification of ResNet, was chosen for the experiments. The WRN-28-10 version with 28 layers and 36.5 M parameters was used in experiments, achieving best results on CIFAR. The architecture includes residual blocks with batch normalization, average pooling, and fully connected layers. Training parameters such as SGD, Nesterov momentum, and learning rate decay were kept consistent with the original paper. On ImageNet, the first convolution has a stride of 2, while within residual blocks it is 1 except in the first block of 4 where it is 2 for subsampling. Training on CIFAR involved batch size of 128 for 200 epochs with learning rate decay at specific epochs. The DenseNet-BC model used in the experiment has 16 layers arranged in blocks with a bottleneck compression rate of \u03b8 = 0.5 and growth rate k = 12. The architecture features dense connections between layers, allowing for deep networks with fewer parameters. The model has nearly 0.8 million parameters and utilizes a dense block structure with convolutional blocks concatenated together. Each convolutional block includes layers whose output is combined with the input for the next block. The DenseNet-BC model used in the experiment has 16 layers with a bottleneck compression rate of \u03b8 = 0.5 and growth rate k = 12. It features dense connections between layers and includes convolutional blocks with a transition block downsampling the feature maps. Training hyper-parameters are kept consistent with the original paper, using SGD with Nesterov momentum 0.9, learning rate of 0.1, decayed at epochs 150 and 200, and training for 300 epochs. Batch size is 64, initialized with He initialization. The appendix details a heavier data augmentation scheme with affine transformations and presents results of main experiments and additional experiments not shown. The results of training models with different configurations, including dropout and weight decay, on ImageNet data are presented. The interaction between weight decay and dropout is not always consistent, with some cases showing better results with both regularizers active, while in other cases only dropout achieves better generalization. Data augmentation consistently improves performance in the experiments. The effect of data augmentation is consistent, with light augmentation achieving better results than training with the original data set. Batch normalization also improves generalization in All-CNN and combines well with data augmentation. However, combining explicit regularization with All-CNN results in inconsistent outcomes, with weight decay and dropout improving generalization but reducing performance with only dropout on CIFAR-10 and CIFAR-100 without augmentation. Regularization hyperparameters may need readjustment with changes in architecture. The gap in performance between models trained with and without batch normalization is smaller when trained without explicit regularization and with heavier data augmentation. Batch normalization benefits training with fewer examples, but removing it only slightly affects performance. Data augmentation alone is effective in resisting the lack of training data. Section 4.2 discusses how data augmentation is more effective in handling the lack of training data compared to explicit regularization. Even with minimal data, data augmentation provides better results, while explicit regularization can hinder both fitting the data and generalization. The use of explicit regularization needs careful tuning of hyperparameters and may not always be beneficial, as shown in the performance differences of models trained with and without it. The appendix provides computations of the Frobenius norm of weight matrices for models trained with different levels of explicit regularization and data augmentation. Heavier data augmentation leads to larger norm solutions, except for some All-CNN models without batch normalization. Weight decay limits the norm of the learned function. Models without batch normalization show less consistency in regularization and augmentation levels. The paper discusses the poor performance of regularized models on different versions of All-CNN compared to models without explicit regularization. The amount of regularization may not be properly adjusted through hyperparameters, as seen in the norm of learned weights. However, the norm alone does not fully explain the performance differences. Further analysis is needed to understand why regularized models struggle to generalize well. The paper also defines explicit and implicit regularization for deep neural networks. The paper defines explicit and implicit regularization for deep neural networks, addressing the ambiguity in the literature. Previous works have discussed the role of implicit regularization in controlling the effective capacity of neural networks, but clear definitions were lacking. Zhang et al. (2017) compared regularization techniques but did not provide definitions, arguing data augmentation as explicit regularization. The paper challenges this view, emphasizing the distinction between explicit and implicit regularization. The paper challenges previous views on regularization techniques, discussing explicit and implicit regularization for deep neural networks. Kuka\u010dka et al. (2017) conducted a taxonomy review of regularization techniques, while Guo et al. (2018) distinguished between data-independent and data-dependent regularization methods. Data-independent regularization imposes constraints on the hypothesis set, similar to explicit regularization, while data-dependent regularization techniques are more specific in constraining the optimization problem. The paper discusses explicit and implicit regularization for deep neural networks, challenging previous views on regularization techniques. It suggests a taxonomy that distinguishes between data-independent and data-dependent regularization methods, highlighting the ambiguity of techniques like batch-normalization. The authors propose a distinction between domain-specific data augmentation and other forms of data-dependent regularization, emphasizing the goal of creating plausible transformations of real-world objects. The paper proposes a taxonomy for regularization methods in deep neural networks, distinguishing between data-independent and data-dependent techniques. It emphasizes the importance of creating plausible transformations of real-world objects through domain-specific data augmentation. One example of data-dependent regularization is mixup, studied by Guo et al. (2018)."
}