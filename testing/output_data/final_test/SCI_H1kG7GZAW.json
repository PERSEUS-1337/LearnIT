{
    "title": "H1kG7GZAW",
    "content": "Disentangled representations offer benefits like ease of deriving invariant representations, transferability to other tasks, and interpretability. A variational inference based approach is proposed for unsupervised learning of disentangled representations from unlabeled data. A regularizer on the approximate posterior over data encourages disentanglement, along with a new disentanglement metric aligned with qualitative decoder output. Significant improvements are observed in disentanglement and data likelihood compared to existing methods. Feature representations of raw data are crucial for machine learning success, capturing relevant generative factors while ignoring inconsequential ones. Disentangled representations reveal generative factors in separate feature dimensions, offering advantages in invariance, transferability, and interpretability. Several attempts have been made to recognize the significance of disentangled representations in machine learning. Earlier work assumed supervision in terms of access to generative factors per instance, knowledge about the nature of generative factors, and knowledge about changes in generative factors across observations. In machine learning, previous attempts focused on supervised access to generative factors for recognizing disentangled representations. However, in realistic scenarios, raw observations lack supervision on generative factors. Recent approaches using Generative Adversarial Networks (GAN) and Variational AutoEncoder (VAE) aim to learn generative models with disentangled factors, but face challenges with inference mechanisms. In this work, a principled approach for inferring disentangled latent factors is proposed based on amortized variational inference. Unlike \u03b2-VAE, this approach does not create tension between observed data likelihood and disentanglement, resulting in better quality generated samples. Our approach for inferring disentangled latent factors through amortized variational inference outperforms \u03b2-VAE in generating higher quality samples without sacrificing entanglement. We introduce a new disentanglement metric, SAP, which aligns better with qualitative disentanglement in decoder output. The problem of inference involves computing the posterior of latents conditioned on observations, given a finite set of samples from the true data distribution. Variational inference uses an optimization approach to approximate the true posterior distribution. Amortized inference involves sharing information across observations by using a recognition model. This model encodes an inverse map from observations to approximate posteriors, known as a variational autoencoder (VAE). The recognition model parameters in variational autoencoder (VAE) are learned by optimizing the evidence lower bound (ELBO), which lower bounds the log-likelihood of observed data. The ELBO can be efficiently optimized using stochastic first order methods with mini-batches. In VAE models, a low variance estimate of the gradient over the base distribution can be obtained through coordinate transformation. Generative models often use a simple and disentangled prior, with complexity absorbed in the conditional distribution. The goal is to infer disentangled latents for various objectives. The text discusses inferring disentangled latents for various goals by considering the density over inferred latents. This involves minimizing a distance between the inferred prior and the generative prior, with the objective of variational inference being to bound the distance between the two distributions. The variational inference aims to infer disentangled latent variables by encouraging factors close to being disentangled. The original VAE shows some disentangling behavior on simple datasets but not on complex ones without extra supervision on generative factors. This discrepancy may be due to the distance between the true and inferred distributions and the non-convexity of the ELBO objective. Minimizing the gap between the true and inferred distributions is crucial for achieving disentanglement in variational inference. Adding a suitable distance metric to the objective, such as DIP-VAE, can help control disentanglement during inference. Optimizing this directly is challenging when using KL-divergence, leading to the need for alternative approaches. One approach to estimating the KL-divergence between distributions involves using a variational formulation that requires samples from both distributions. However, this method introduces optimization challenges and may require optimizing for a third set of parameters. Using a different distance metric, such as Wasserstein distance, could potentially simplify the optimization process but still presents challenges in optimizing for multiple sets of parameters. To simplify the optimization process, an alternative method of matching the moments of two distributions is adopted instead of using variational forms of distances. The covariance of the distributions is crucial for decorrelating dimensions. Existing VAE models typically use a specific form for q \u03c6 (z|x), with parameters derived from a neural network. The goal is to make the covariance close to the Identity matrix by measuring proximity using the squared 2-norm. The regularization terms in the objective function aim to decorrelate dimensions by penalizing off-diagonals and adjusting diagonal entries of the covariance matrix. Two options for the disentangling regularizer are discussed, with separate hyperparameters controlling the importance of diagonal and off-diagonal entries. This approach helps optimize the covariance matrix efficiently. The optimization problem in DIP-VAE-II involves efficiently optimizing terms using SGD and estimating Cov p(x) [\u00b5 \u03c6 (x)] with the current minibatch. Penalizing off-diagonals of Cov p(x) [\u00b5 \u03c6 (x)] reduces the magnitude of its diagonals, which is important for disentanglement. DIP-VAE-II is more suitable than DIP-VAE-I when the number of generative factors is less than the latent dimension to prevent splitting of attributes across multiple dimensions. Matching higher order central moments of q \u03c6 (z) and the prior p(z) is also possible. Matching higher order central moments of q \u03c6 (z) and the prior p(z) is possible, with \u03b2-VAE proposing to modify the ELBO by upweighting the KL term to encourage disentangled factors. Empirical results show \u03b2 values ranging from 4 to 250, with higher values pulling \u00b5 \u03c6 (x) towards zero. For high values of \u03b2, \u03b2-VAE tries to pull the approximate posterior q \u03c6 (z|x) towards zero and the identity matrix, leading to worse sample quality compared to VAE. In contrast, the proposed method maintains sample quality on par with VAE while encouraging disentanglement by working directly on Cov q \u03c6 (z) (z). The proposed method works on Cov q \u03c6 (z) (z) to maintain sample quality while encouraging disentanglement. A metric called SAP Score is used to evaluate disentanglement performance by sampling generative factors and computing the absolute difference of inferred vectors. A low capacity classifier is trained on these difference vectors to predict generative factors. The proposed method uses a metric called SAP Score to evaluate disentanglement performance by training a low capacity classifier on the absolute difference of inferred vectors of generative factors. The Z-diff score, which measures disentanglement, is not well correlated with qualitative disentanglement observed in latent traversal plots. A new metric, SAP score, is introduced to better align with qualitative disentanglement and does not require training a classifier. The SAP score is computed by constructing a score matrix based on linear regression or classification to predict generative factors using inferred latents. The R2 score indicates how well the latent explains variability in the factor, while classification accuracy is used for classification factors. The proposed method introduces the SAP score as a better metric for evaluating disentanglement performance. The SAP score is calculated based on the difference of top two entries in the score matrix for each generative factor, indicating how well each factor is captured in only one latent dimension. A high SAP score suggests strong disentanglement performance. The SAP score is calculated based on how well each generative factor is captured in one latent dimension. A high score indicates strong disentanglement performance, but a low score does not rule out good disentanglement in certain cases. The SAP score computation can be adapted by grouping latent dimensions based on correlations to get a more accurate score. DIP-VAE is evaluated on three datasets: CelebA, 3D Chairs, and 2D Shapes. CelebA dataset contains RGB face images of celebrities, 3D Chairs dataset includes chair CAD models, and 2D Shapes is a synthetic dataset of binary 2D shapes. Training and testing splits are specified for each dataset. The study evaluates DIP-VAE on CelebA, 3D Chairs, and 2D Shapes datasets, using various hyperparameters and network architectures. The experiments involve varying \u03bb od values and penalizing third order central moments for different datasets. Additionally, \u03b2-VAE is tested with different \u03b2 values. For \u03b2-VAE, experiments were conducted with different \u03b2 values ranging from 1 to 256. Results for CelebA and 2D Shapes datasets were shown in terms of Z-diff score, SAP score, and Z-diff disentanglement score. The plots varied \u03b2 for \u03b2-VAE and \u03bb od for DIP-VAE-I and DIP-VAE-II. DISPLAYFORM0 was computed for each attribute using the training set, and a bias was learned by minimizing the hinge loss. Accuracy on other attributes remained consistent across all methods. The proposed SAP score, reconstruction error, and Z-diff score were evaluated for 3D Chairs data. The Z-diff score was calculated using a linear SVM with specific weights. Results for CelebA data comparing \u03b2-VAE and DIP-VAE were shown through plots varying \u03b2 and \u03bb od. The Z-diff score and SAP score were compared for CelebA and 2D Shapes data using different hyperparameters. DIP-VAE-I showed higher Z-diff scores with minimal impact on reconstruction error compared to VAE and \u03b2-VAE. However, high Z-diff scores did not always indicate disentanglement in the decoder's output for single latent traversals. For 2D Shapes data, DIP-VAE-I has a higher Z-diff score and lower reconstruction error than \u03b2-VAE for \u03b2 = 60. However, \u03b2-VAE shows better disentanglement in latent traversals. The SAP score correlates well with disentanglement, with \u03b2-VAE scoring higher than DIP-VAE-I. DIP-VAE-II offers a better balance between disentanglement and reconstruction error for 2D Shapes data. DIP-VAE-I may negatively impact disentanglement by splitting generative factors across multiple latents for 2D Shapes. However, it performs well on real datasets like CelebA, yielding lower reconstruction error and higher SAP scores. Additionally, binary attribute classification for CelebA is successful using inferred values from \u00b5 \u03c6 (x). The proposed DIP-VAE outperforms VAE and \u03b2-VAE for most attributes, with \u03b2-VAE's performance worsening as \u03b2 increases. Adversarial autoencoder matches q \u03c6 (z) to the prior p(z) but does not aim to minimize KL(q \u03c6 (z|x)||p \u03b8 (z|x)). Disentanglement is linked to invariance and equivariance of representations. Equivariance of representations implies that a transformation of the input results in a corresponding transformation of the feature. Disentanglement requires that the transformation acts only on a small subset of dimensions of the feature. Equivariance is a more general notion encompassing disentanglement, with invariance being a special case of equivariance. Invariance can be easily obtained from disentangled representations by marginalizing the appropriate subset of dimensions. The text discusses prior work on equivariant and invariant feature learning, proposing a variational framework for inferring disentangled latents from unlabeled observations. The proposed variational objective avoids conflicts between data log-likelihood and disentanglement of latents, as shown in empirical results. Additionally, a new disentanglement metric called SAP is introduced, which correlates better with qualitative disentanglement in latent traversals compared to the Z-diff score. Future work may consider sampling biases in the generative process. In the generative process, natural and artificial biases make the problem challenging and less well-defined. Effective use of disentangled representations for transfer learning is a promising direction for future work."
}