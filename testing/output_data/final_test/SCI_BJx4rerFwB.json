{
    "title": "BJx4rerFwB",
    "content": "Weakly-supervised video moment retrieval aims to locate the video segment described by a sentence without temporal annotations. The proposed Weakly-supervised Moment Alignment Network (wMAN) uses a multi-level co-attention mechanism and positional encodings to learn richer multimodal representations for aligning visual and language features. Transformers learn visual-semantic representations with contextual information of relative positions in temporal sequences. Experiments on DiDeMo and Charades-STA datasets show the effectiveness of the wMAN model, outperforming weakly-supervised methods significantly and even surpassing strongly-supervised methods on some metrics. Video understanding in AI research has focused on improving spatio-temporal representations for better video reasoning. The video moment retrieval task combines language and video understanding to identify video segments described by natural language sentences. The task involves identifying the most relevant video segment to a given natural language sentence. Existing methods learn joint visual-semantic representations using temporal annotations, but these are costly and ambiguous. Mithun et al. propose a weakly-supervised approach using full video-sentence pairs as weak labels, circumventing the need for temporal annotations. Existing methods for identifying relevant video segments to natural language sentences are hindered by a lack of temporal annotations. Mithun et al. propose a Text-Guided Attention mechanism to address this issue but fail to consider contextual information and fine-grained semantics of each word. This results in a limited ability to reason comprehensively about the alignment between visual and language representations. Our proposed approach improves weakly-supervised video moment retrieval by leveraging fine-grained temporal and visual relevance of each video frame to each word. It aggregates contextual information from all frames using graph propagation for accurate retrieval, addressing limitations of current methods. Our proposed Weakly-Supervised Moment Alignment Network (wMAN) enhances video moment retrieval by utilizing fine-grained temporal and visual relevance of each frame to each word. It incorporates a multi-level co-attention mechanism with a Frame-by-Word interaction module and a Word-Conditioned Visual Graph (WCVG) to create visual-semantic representations for accurate retrieval. The Weakly-Supervised Moment Alignment Network (wMAN) improves video moment retrieval by updating frame nodes with relational information from visual-semantic nodes. Visual-semantic nodes contribute dynamically weighted information to frame nodes based on similarity, incorporating positional encodings for contextual information. wMAN utilizes a Multiple Instance Learning (MIL) framework similar to the Stacked Cross Attention Network (SCAN) model for learning temporally-aware multimodal representations. The Weakly-Supervised Moment Alignment Network (wMAN) enhances video moment retrieval by utilizing a Word-Conditioned Visual Graph for richer visual-semantic context through frame-by-word alignment. It distinguishes itself by extracting temporally-aware multimodal representations from videos and their descriptions, unlike other models that only work on images. The curr_chunk introduces a novel application of positional embeddings in video representations to learn temporally-aware multimodal representations. Extensive experiments over two datasets show outperformance of state-of-the-art models in video moment retrieval based on natural language queries. The Moment Alignment Network (MAN) and TGN model utilize structured graph networks and frame-by-word interactions for video moment retrieval. Unlike other methods, wMAN considers multimodal interactions and operates in a weakly supervised setting. The proposed approach outperforms existing models in learning temporally-aware multimodal representations for video moment retrieval tasks. In video moment retrieval, the task is to find the most relevant video moment based on a given description. Current models use structured graph networks and frame-by-word interactions for this task, with the weakly supervised wMAN model showing improved performance in learning multimodal representations. The weakly-supervised version of video moment retrieval involves learning to label video segments as relevant or not based on a given sentence. A new model, wMAN, is introduced to learn context-aware visual-semantic representations through frame-by-word interactions. Our network, wMAN, incorporates representation learning from Frame-By-Word attention and Positional Embeddings, as well as a Word-Conditioned Visual Graph for updating video segment representations. Video segment relevance to attended sentence representations is determined using LogSumExp pooling similarity metric. Input sentences are encoded with GloVe embeddings and processed through a Gated Recurrent Unit (GRU). The GRU outputs W = {w1, w2, ..., wQ} for each sentence, where Q is the number of words. Frame features from the input video are encoded using a pretrained CNN. These features are then fed into an LSTM to capture long-range dependencies. The LSTM output is concatenated with positional encodings to form the initial video representations V = {v1, v2, ..., vN}. PE features are included to indicate the relative position of each frame. The PE features, similar to TEF, provide temporal position information for each frame. With a hyper-parameter M = 10,000, these features are concatenated with LSTM encoded frame features before cross-modal interaction. Unlike prior work, which relates a sentence-level representation to each frame, aggregation is done instead. Representation of video frames and words is aggregated using Frame-By-Word (FBW) similarity scores to compute attention weights, identifying important frame and word combinations. Normalized relevance of words to frames is used to compute attention for each word, creating a weighted combination where correlated words gain high attention. The frame-specific sentence representation emphasizes relevant words in the frame, not participating in the iterative message-passing process but used to infer similarity scores. Attention weights determine the normalized relevance of each frame to each word, creating word-specific video representations for the Word-Conditioned Visual Graph. The Word-Conditioned Visual Graph integrates visual-semantic and contextual information to learn multimodal representations. Word representations are updated with video representations to create visual-semantic representations. A fully connected graph is constructed with visual features and attention of visual-semantic representations as nodes for iterative message-passing. The iterative message-passing process introduces a second round of interaction to infer the latent temporal correspondence between frames and visual-semantic representations. Visual nodes update their representations by summing up incoming messages weighted by edge weights. The newly computed FBW similarity matrix is used to update visual representations for computing relevance to sentence representations. Segments are defined as continuous sequences of visual features. The LogSumExp pooling similarity metric is adopted to determine relevance of proposal segments to the query. Lambda is a hyperparameter that weighs the relevance of salient parts of the video segment to corresponding sentence representations. The model uses a margin-based ranking loss to train, ensuring positive pairs are more similar than negative pairs. Sim LSE is the similarity metric used, ranking candidate segments at test time. Evaluation on DiDeMo and Charades-STA datasets shows wMAN's ability to localize video moments accurately based on natural language queries without temporal annotations. The Charades-STA dataset is created by breaking down video-level paragraph descriptions into sentence-level annotations and aligning them with video segments. It contains 12,408 training and 3,720 test query-moment pairs. Non-overlapping sliding windows of sizes 128 and 256 frames are used to generate candidate temporal segments for evaluation. The candidate temporal segments are used to compute similarities with the input query, and the top scored segment is returned as the localization result. The model utilizes GloVe embeddings for word representations and fine-tunes them during training. Different visual features are used for experiments on DiDeMo and Charades-STA datasets. The model is trained end-to-end using the ADAM optimizer with specific learning rate and margin parameters. Our model, trained using the ADAM optimizer, outperforms the TGA model significantly on all metrics, with Recall@1 accuracy almost doubling that of TGA at IOU = 0.7. The Recall@1 accuracies consistently improve across all IOU values, highlighting the importance of richer joint visual-semantic representations for accurate localization. Additionally, our model performs comparably to the strongly-supervised MAN model on several metrics. A comprehensive set of ablation experiments is presented to understand the contributions of each component of our model, which includes FBW, WCVG components, and the incorporation of PEs. The results of our FBW variant highlight the importance of capturing fine-grained frame-by-word interactions for inferring temporal alignment between modalities. The incorporation of WCVG components in the second stage of multimodal attention encourages learning of intermodal relationships. Additionally, adding positional encodings to visual representations improves Recall@1 accuracies. Experiments show that TEFs slightly decrease performance. Visualizations in Figure 3 provide insights into frame-word interactions. Our model effectively determines salient frames for each word, leveraging contextual information from all video frames and words. Results on the DiDeMo dataset show that our model outperforms the TGA model significantly, even tripling the Recall@1 accuracy. This highlights the impact of learning richer joint visual-semantic representations for accurate video moment localization. Our weakly-supervised Moment Alignment Network with WordConditioned Visual Graph utilizes a multi-level co-attention mechanism to improve Recall@1 accuracy in video moment localization by inferring latent alignment between visual and language representations at fine-grained levels. Our model utilizes context-aware visual-semantic representations to reason about events and entity relationships in natural language queries. Visualization shows relevance weights of words to frames, with comparisons of different methods in Table 5. The performance gains of our model with 18M parameters are notable compared to TGA with 3M parameters. Despite having more parameters than wMAN, TGA with 19M parameters performs significantly worse on all metrics. Additionally, the Language-Conditioned Graph Network (LCGN) lacks contextual information derived from visual representations, unlike the co-attention mechanism in the combined wMAN model. In experiments, three message-passing rounds were found to work best for learning visual-semantic representations on Charades-Sta and DiDeMo datasets. Ablation results emphasize the importance of integrating PEs and WCVG in the approach, leading to improved performance in learning temporally-aware visual-semantic representations."
}