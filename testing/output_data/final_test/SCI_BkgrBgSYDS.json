{
    "title": "BkgrBgSYDS",
    "content": "Modern neural network architectures utilize structured linear transformations like low-rank matrices, sparse matrices, permutations, and the Fourier transform to enhance inference speed and reduce memory usage. Introducing kaleidoscope matrices (K-matrices) offers a more efficient approach, allowing for automatic learning within end-to-end pipelines to improve model quality. For instance, replacing channel shuffles in ShuffleNet with K-matrices has shown up to a 5% increase in classification accuracy on ImageNet. K-matrices can improve accuracy on various tasks, such as speech recognition and image classification, with minimal loss. They also enhance model efficiency, reducing computation and memory usage. Additionally, K-matrices in a Transformer network can achieve 36% faster inference speed in language translation tasks. Structured linear maps, including specialized transforms like DFT and convolutions, are crucial for efficient storage and inference on edge devices. Sparsity and other forms of structure are key in recent ML advancements, particularly for on-device and energy-efficient models. Different classes of structured linear maps have varying tradeoffs in terms of speed and accuracy, with no single class working uniformly well across all applications. ML practitioners currently hand-pick specific classes of structured linear maps for their applications, which is a labor-intensive task. A universal representation for structured linear maps is needed to address this issue. This representation should be expressive, have a nearly optimal parameter count and runtime, be differentiable for easy integration into ML pipelines, and allow for efficient training and inference algorithms. Currently, no class of structured linear maps satisfies all criteria for efficient algorithms in training and inference. Existing classes like low-rank matrices do not capture all important types of structure, such as the DFT. Sparsity is another crucial structure type, with recent work focusing on sparse neural networks that achieve comparable quality with fewer parameters through pruning or identifying \"winning lottery tickets\". Recent theoretical results also highlight the importance of sparsity and structure in linear maps. The results by De Sa et al. (2018) demonstrate the fundamental link between sparsity and structure in linear maps. They show that any matrix can be factored into sparse matrices with total parameter count equal to the matrix's efficiency. Learning sparse representations is challenging due to the need to find sparsity patterns. Current methods for training sparse neural networks are either expensive or rely on hand-tuned heuristics. In contrast, the proposed representation of linear maps as products of sparse matrices with predefined sparsity patterns offers a solution. The proposed representation of linear maps as products of kaleidoscope matrices (K-matrices) satisfies the desiderata by retaining expressiveness, being differentiably learnable, and efficient. The main theoretical contribution shows that any structured linear map can be represented as a K-matrix with a nearly tight number of parameters and algorithmic complexity. The kaleidoscope representation is fully differentiable, allowing for learning using standard optimization algorithms. K-matrices can be learned using optimization algorithms like SGD, are practical and easy to use, and can replace linear components in deep learning models. They offer memory-and runtime-efficient implementations for training and inference. In experiments, K-matrices were used to simplify filter bank computation in speech preprocessing and improve ImageNet classification accuracy in ShuffleNet. In experiments, K-matrices were shown to improve ImageNet classification accuracy by up to 5%. They can recover latent structure and achieve higher accuracy in downstream CNN models. Additionally, K-matrices can speed up real-world tasks, such as translation, by replacing linear layers in networks like DynamicConv-Transformer for faster inference speed. The proposed kaleidoscope matrices, composed of butterfly matrices, demonstrate expressivity in representing all structured matrices efficiently. The concept of sparse product width (SPW) is introduced to describe the algorithmic complexity of matrix-vector multiplication. It is shown to be an optimal descriptor, and the sparsity constraint makes learning such factorizations challenging. The concept of sparse product width (SPW) is introduced to describe the algorithmic complexity of matrix-vector multiplication, which is challenging due to the non-continuous sparsity constraint. Butterfly matrices, used in numerical linear algebra and machine learning, are defined as a building block for kaleidoscope matrices. Butterfly factor matrices of various sizes and block sizes are discussed. The kaleidoscope hierarchy is defined using butterfly factor matrices, providing a differentiable alternative to sparse matrix products. Various structured matrix classes are located within this hierarchy. The kaleidoscope hierarchy is defined using butterfly factor matrices, providing a differentiable alternative to sparse matrix products. The main theoretical result shows that general linear transformations can be captured in the BB * hierarchy with low width, using low-depth linear arithmetic circuits. The BB * hierarchy uses butterfly factor matrices to represent linear transformations efficiently. Theorem 1 states that matrices can be represented with low parameters and runtime in this hierarchy, with a focus on efficient matrices with depth on the order of log n or poly log n. The proof involves representing sparse matrices as K-matrices. The kaleidoscope representation of sparse matrices in the BB * hierarchy uses butterfly factor matrices efficiently. It provides a novel characterization of sparse matrices with O(s log n) parameters and runtime, sacrificing optimality for full differentiability. This representation allows sparse matrices to be written as a sum of smaller matrices, each with at most n non-zero elements. The kaleidoscope representation in the BB * hierarchy efficiently uses butterfly factor matrices to represent sparse matrices as a sum of smaller matrices with at most n non-zero elements. This allows for a novel characterization of sparse matrices with O(s log n) parameters and runtime, offering a tight representation for structured linear maps common in ML. The curr_chunk discusses the efficiency of structured matrices in machine learning, such as discrete transforms and convolution, compared to other proposed matrix classes. It highlights the tight representation of K-matrices and their ability to capture common structures. More detailed discussions are available in the appendices. In Appendix L, it is proven that finding an efficient circuit for a ReLU network can be reduced to finding efficient circuits for each weight matrix. ReLU networks with kaleidoscope weight matrices have near-linear VC dimension, matching the bound for networks with unconstrained weight matrices. The Orthogonal butterfly matrices are discussed as a variant for improved stability. The OBB hierarchy consists of products of orthogonal butterfly matrices and diagonal matrices. The hierarchy of kaleidoscopes, consisting of orthogonal butterfly matrices and diagonal matrices, has the same expressiveness as the BB * hierarchy. Three claims validate the effectiveness of kaleidoscopes in learning different structure types in modern architectures: 1. Replacing hand-structured components with K-matrices improves architecture quality with minimal overhead. 2. A K-matrix-based relaxation of permutations improves accuracy in tasks with latent structure. 3. Kaleidoscopes show promise in learning various structures in modern architectures. In Section 3.3, the implementation of K-matrices can enhance the inference throughput of DynamicConv Transformer by 36% without significant loss in translation quality. The K-matrices are fully differentiable and can be trained jointly with the model using standard learning algorithms. They have shown to recover or improve the performance of hand-crafted structures in machine learning models, such as replacing hand-engineered filter banks in speech recognition tasks with minimal loss in accuracy. Replacing channel shuffles in ShuffleNet with learnable K-matrices improves classification accuracy on ImageNet by up to 5.0%. K-matrices simplify speech recognition data preprocessing pipelines, eliminating the need for hand-tuning. The \"kaleidoscope\" pipeline can replace complex handcrafted MFSC featurization in speech recognition tasks with minimal accuracy drop. This approach outperforms current methods for learning from raw audio input. In speech recognition, replacing handcrafted MFSC featurization with a learnable kaleidoscope layer simplifies data preprocessing. This approach, using a standard RNN architecture, outperforms methods relying on specialized architectures for learning directly from raw audio. The featurization process in speech recognition involves hand-designed steps like framing, dithering, pre-emphasis, windowing, FFT, mel scale mapping, and normalization. A trainable kaleidoscope layer simplifies this process by replacing the last six steps with end-to-end training. After hand-designed featurization steps in speech recognition, a kaleidoscope layer replaces the last six steps with a K-matrix multiplication and power spectrum computation before inputting into a Bi-LSTM model. Using K-matrices improves lightweight architectures like ShuffleNet, achieving up to 5% better classification accuracy without manual tuning. Grouped convolution is also utilized to reduce parameters and speed up inference. ShuffleNet uses a permutation matrix to shuffle channels after grouped convolution, while Zhao et al. propose using the Hadamard transform. In contrast, the approach in this study involves using K-matrices before and after grouped convolution, resulting in improved performance. The effectiveness of K-matrices is demonstrated in image classification on a permuted dataset, showing superior results compared to existing linear maps. The study introduces a differentiable relaxation using a K-matrix for image classification on a permuted dataset. Results show a 9-point increase in accuracy compared to methods without permutation learning, reaching 93.6%. CNNs struggle with permuted datasets due to destroyed locality, but this approach bridges the gap, coming close to the accuracy of un-permuted datasets. The study introduces a differentiable relaxation using a K-matrix for image classification on a permuted dataset. A K-matrix is used to represent a distribution over permutations, converging to a single permutation at the end of training. The learned permutation is evaluated by training a ResNet18 with the K-matrix permutation layer inserted at the beginning. The study introduces a differentiable relaxation using a K-matrix for image classification on a permuted dataset. In Table 3, comparisons are made with different approaches including ResNet18 models. The K-matrix enables effective learning of latent permutations, providing the ability to recover latent structure. Figure 3 illustrates the pipeline and examples of permuted and unpermuted images. The K-matrix is used to unscramble images and improve inference speed in a language translation model. By replacing dense matrices with K-matrices, a 36% speedup is achieved. Fast implementations of the algorithm are provided in C++ and CUDA. The algorithm provides fast implementations in C++ and CUDA, using K-matrices to replace linear layers in the decoder for a 25% smaller model with 36% faster inference time on CPU. The majority of inference time is spent in matrix-vector multiplication, with K-matrix multiplication being 2x faster than dense matrix-vector multiplication in the Intel MKL library. In Appendix C, the FFT and other routines are detailed. A universal family of kaleidoscope matrices, known as K-matrices, can represent any structured linear maps efficiently. Empirical validations show promise in using K-matrices to reduce manual engineering, capture latent structure, and improve model efficiency in machine learning applications. Future work aims to optimize hardware implementations of K-matrices for real-world applications. The DFT is essential in various machine learning applications such as data preprocessing and model compression. Fast random projection and kernel approximation methods rely on the Hadamard transform and convolution. Structured matrices like Toeplitz-like and LDR matrices are used for model compression but lack efficient GPU implementations. Linear transforms are common in deep learning architectures. Sparse neural networks are becoming popular in deep learning architectures, with approaches including pruning dense weights during training or post-training, as well as training dense networks and identifying sparse subnetworks for retraining. Another emerging approach aims to directly train unstructured sparse neural networks with constant sparsity levels throughout training. Sparse neural networks are gaining popularity in deep learning architectures. One approach involves maintaining a constant network sparsity level and evolving the sparsity pattern during training. However, storing the indices of nonzero entries increases memory requirements, and heuristic-based sparsity learning can be brittle. SincNet and Zeghidour et al. have developed CNN-based architectures for speech recognition without manual featurization. The WaveNet generative architecture, CLDNN model, and LSTM model have been used for learning from raw audio data. The kaleidoscope + LSTM model achieved a low error rate on the TIMIT test set. Permutation matrices are utilized in tasks like matching and sorting. Permutation matrices are used in tasks like matching and sorting. Techniques for obtaining posterior distribution over permutations have been developed, such as the exponential weights algorithm and the Gumbel-Sinkhorn network. Classifying images with permuted pixels is a standard task to benchmark RNNs' ability to learn long-range dependencies. Various RNN architectures, including those with unitary or orthogonal weight matrices, have been proposed and tested on this task. LSTM and GRU architectures have also been found to be competitive with these new RNN architectures. The baseline Bi-LSTM model from the PyTorch-Kaldi repository is a strong performer, matching state-of-the-art results for single input featurization models. The model processes waveform data by framing, dithering, applying pre-emphasis, using a Hamming window, applying FFT, and computing the power spectrum. The power spectrum is then mapped to the mel scale for human auditory perception. The model processes raw waveform data without normalization or preprocessing, using a complex-valued kaleidoscope matrix. The power spectrum of the output is computed and fed into a Bi-LSTM for training. The Bi-LSTM and kaleidoscope layer are trained together in an end-to-end fashion, with a 1.1M increase in parameters compared to using MFSC features. Training time is 7% greater, and FLOPs for inference-time are approximately 15% greater. Other linear transformations are also compared before the Bi-LSTM. The Bi-LSTM and kaleidoscope layer are trained together with various linear transformations before the Bi-LSTM, including fixed, trainable structured, and trainable unstructured layers. Grid search is done for the initial learning rate for the preprocessing layer, and the model is trained end-to-end with the RMSProp optimizer for 24 epochs. Validation set is used to select the best preprocessing learning rate, and final error rates are reported on a separate test set. The number of parameters in structured matrices matches the butterfly layer. Preprocessing matrix initialized to represent FFT. Experiment on combining MFSC featurization with learnable kaleidoscope layer for accuracy gains. In an experiment, the FFT step was replaced with a K-matrix initialized to the FFT, and the weights of the Hamming window function and the mel filter bank matrix were made learnable. The resulting \"hybrid\" model achieved a test PER% of 13.9 \u00b1 0.2, with 14.4M parameters. By combining manual domain knowledge with learnable structures, the model nearly matched the state-of-the-art accuracy on TIMIT. The state-of-the-art results in speech recognition involve a customized RNN architecture (LiGRU) and a complex pipeline using MFSC, MFCC, and fMLLR as input features. ShuffleNet utilizes a permutation matrix to shuffle channels in its architecture, with Zhao et al. proposing the use of Hadamard transforms before and after grouped 1x1 convolutions. In our architecture, we use a kaleidoscope matrix in OBB before and after each grouped 1x1 convolution. The structure for each block includes K-matrix, 1x1 group conv, Batch norm, ReLU, 3x3 depthwise conv, and 1x1 group conv. The K-matrices are learned along with the rest of the network. The CNN architectures are evaluated on the ImageNet dataset for image classification using standard data augmentation and training pipelines. We use standard data augmentation and training pipelines for our architecture, training with SGD on 8 GPUs for 90 epochs. The total batch size is 2048 with an initial learning rate of 0.8. For the 1.0 ShuffleNet g8 architecture, we adjust the batch size to 1792 and the learning rate to 0.7 to fit into GPU memory. Other hyperparameters remain the same as in the ShuffleNet paper. We report top-5 classification accuracy on ImageNet in Table 6 and observe that the total training time of our K-matrix approach is within 20% of vanilla ShuffleNet. The study trained 1.0 ShuffleNet g8 with fixed permutation or K-matrix for channel shuffling. K-matrices converge in a similar number of training steps as the baseline model due to easy initialization and orthogonality constraints. A bit-reversal permutation was applied to the CIFAR-10 dataset to test spatial locality reliance in architectures like CNNs. The model architecture described in Section 3.1 uses a fixed permutation parametrized as a K-matrix to learn the true permutation, followed by a standard ResNet18 architecture. The butterfly factor matrices in the K-matrix are constrained to be doubly-stochastic, allowing for easy extensibility with additional techniques. The model uses a fixed permutation parametrized as a K-matrix with doubly-stochastic constraints. Permutations are sampled for butterfly factor matrices, and unsupervised reconstruction loss is applied to the inputs. A regularization loss on the entropy of P encourages convergence towards a sharper doubly-stochastic matrix. Techniques are also applicable to specialized methods like Gumbel-Sinkhorn. The model utilizes a fixed permutation represented by a K-matrix with doubly-stochastic constraints. Techniques like Gumbel-Sinkhorn are specialized methods for recovering true permutations. Fully connected (FC) and gated recurrent unit (GRU) models are used for capturing long-range dependencies on permuted image datasets. In 2016, GRU was chosen as a baseline for its competitive performance on the Permuted MNIST dataset. The ResNet18 architecture was adapted for the CIFAR-10 dataset by modifying the first convolutional layer and removing the max-pooling layer. A dense layer of size 1024 \u00d7 1024 was added before the ResNet18 architecture. All models were trained for 200 epochs with the Adam optimizer and hyperparameters were tuned using Hyperband. The decoder architecture uses K-matrices from the B class, trained from scratch with the same hyperparameters as the DynamicConv model. The model with K-matrices trains faster than the default model. Inference speed was evaluated on the IWSLT-14 De-En test set using a singlethreaded mode on an Intel Xeon CPU E5-2690 v4 server. The study evaluated the inference speed of the decoder architecture using K-matrices on the IWSLT-14 De-En test set. Different classes of structured matrices were compared for speed-quality tradeoff in replacing fully-connected layers. In the study, different classes of structured matrices were compared for speed-quality tradeoff in replacing fully-connected layers. K-matrices showed the second fastest inference time, only 7% slower than low-rank matrices. The implementation of K-matrix multiplication is already close to the speed of low-rank matrices. Each K-matrix has an O(n log n) matrix-vector multiplication algorithm. Our implementation of the K-matrix multiplication algorithm is competitive with optimized subroutines on both GPU and CPU. In comparisons with specialized FFT implementations, our method is up to 23% faster on GPU and one to two orders of magnitude faster on CPU for inference. Our implementation is within a factor of 4x of specialized FFT implementations and is also memory efficient. Our implementation of the K-matrix multiplication algorithm is memory efficient, with activation memory required being O(bn) for input batch size b. We validate Theorem 1 on structured matrices used in machine learning, representing them using K-matrices, sparsity, and low-rank. The expressivity of each method is quantified in Table 7, with results for kaleidoscope matrices obtained from theoretical expressiveness results. Low-rank and sparse approximation have closed form solutions. The study compares the effectiveness of different structured matrices in representing common matrices, showing that kaleidoscope matrices outperform low-rank and sparse matrices in capturing out-of-class target matrices. The results are presented in Table 7, demonstrating the Frobenius error for matrices of dimension 256 using various structured representations. The study compares structured matrices' effectiveness in representing common matrices, with kaleidoscope matrices outperforming low-rank and sparse matrices in capturing out-of-class targets. Target matrices include kaleidoscope, low-rank, sparse, convolution, Fastfood, and random iid Gaussian matrices. Kaleidoscope approximation is found using SGD with Hyperband tuning for learning rate. The BB * hierarchy has 4n log n parameters for an n \u00d7 n matrix M \u2208 BB *. The BB * hierarchy has 4n log n parameters for an n \u00d7 n matrix M. Matrix-vector multiplication by S or ST involves sparse matrix multiplication algorithms. The BB* hierarchy has parameters for an n \u00d7 n matrix M. Matrix-vector multiplication by S or ST involves sparse matrix multiplication algorithms. Each Ei can be decomposed into matrices with non-zero entries. Matrices are contained within the hierarchy. Any n \u00d7 n matrix can be decomposed into orthogonal butterfly matrices and a diagonal matrix. The BB* hierarchy involves decomposing an n \u00d7 n matrix M into orthogonal butterfly matrices and a diagonal matrix. The hierarchy is strict up to a certain point, as shown by a proof involving matrix manipulation and parameter constraints. In this appendix, the main theoretical result is proven, showing the ability to capture general transformations using low-depth linear arithmetic circuits in the BB* hierarchy. Theorem 1 states that an n\u00d7n matrix M can be represented by a linear arithmetic circuit with s gates and depth d. Theorems 2 and 3 show that permutation matrices and matrices with s NNZ elements also belong to the BB* hierarchy. Proof of Theorem 1 involves representing matrix M as a product of d matrices, each of size s \u00d7 s. Define notation for gates and layers in C. Matrix M k in the k'th layer of C performs computations of the gates in that layer. Argument is made that v d contains outputs of all gates in C. The matrix product in the final step contains all output elements. By left multiplying with a permutation matrix P, the vector can be reordered to have the first n entries as Mv. The position of PMd...M2M1 within the BB* hierarchy needs to be argued. Each Mk matrix has a size less than 2s NNZ. Therefore, Mk can be represented as a product according to Theorem 3. In this appendix, Theorem 2 is proven by decomposing permutation matrix P into LR, with L \u2208 B and R \u2208 B*. The matrix can be represented as a product of O(1) matrices of size 2s, and an expansion from size n to size 2s is required. Theorem 1 provides an algorithm for matrix vector multiplication with a complexity of O(sd log s). The modular-balanced matrix L is decomposed into main diagonal blocks by permuting the red entries. The proof is done by induction on n, assuming all matrices of size n^2 are butterfly matrices. If L is not modular-balanced, a contradiction is reached. The matrix L is modular-balanced, with L1 and L2 also being modular-balanced. This implies that Bn is a butterfly factor of size n. The permutation matrix B is a butterfly factor of size n, performing row permutations of L. To transform P into a modular-balanced matrix, we use the permutation matrix Bk to swap columns k/2 apart, creating a butterfly factor of size k. The decomposition of a permutation matrix P into LR, where L is modular-balanced and R is a butterfly factor, is achieved by applying Lemma G.2 iteratively. Starting with a butterfly factor Bn, we balance columns of P to meet specific conditions until obtaining a matrix with all balance conditions satisfied. The final equation is P = LR, where B is a butterfly matrix and L is modular-balanced. This decomposition is proven in Lemma G.3, leading to Theorem 2. Theorem 2 is proven by decomposing a permutation matrix P into LR, where L is modular-balanced and R is a butterfly factor. Basic facts of the BB * hierarchy are presented, including the scaling properties of diagonal matrices on matrices in B (or B * ). The permutation P moves the first k rows of each E Ai into the top mk rows, showing that P \u2208 BB * . Theorem 2 proves the decomposition of a permutation matrix P into LR, where L is modular-balanced and R is a butterfly factor. The permutation P moves the first k rows of each E Ai into the top mk rows, showing that P \u2208 BB * . The decompositions of each E Ai can be done in parallel within the RHS block matrix, requiring total width w. If e = 1 in Lemma H.3, then P is unnecessary. The unused matrices 4ek and 2ek in S are noted, with L and R being butterfly factor matrices of size 4ek and P1 being a butterfly factor matrix of size 2ek. Through repeated application of the identity from Lemma H.2, M can be folded into the final summation factor Mm. Additionally, a permutation P2 moves the first k columns of the second block-column of M to the left. The invertibility of matrix M is proven in a series of steps, showing that M^-1 also belongs to B*. The Gaussian elimination operations on Ik ensure non-zeros only on the main diagonal and k positions apart within rows or columns. The curr_chunk discusses the properties of invertible butterfly factor matrices and their inverses, as well as a closure result for the Kronecker product. It also mentions a permutation and a proof for a theorem. The prev_chunk talks about the properties of matrix M and its invertibility through Gaussian elimination operations. The curr_chunk discusses proving Theorem 3 using matrices with at most n NNZ and Lemma I.1. It introduces horizontal and vertical step matrices for decomposition. The horizontal step matrix follows a \"Lipschitz-like\" condition. Lemma I.2 states that an n \u00d7 n horizontal step matrix can be decomposed into butterfly matrices of size n/2. This is proven by induction on n, where the base case n = 2 is trivial. The matrix can be decomposed into submatrices and diagonal matrices following a specific structure. Lemma I.2 proves that an n \u00d7 n horizontal step matrix can be decomposed into butterfly matrices of size n/2. The proof involves defining corner submatrices of H and showing that they must be horizontal step matrices. This is done by equating arbitrary entries of each submatrix. Lemma I.2 proves that an n \u00d7 n horizontal step matrix can be decomposed into butterfly matrices of size n/2. The proof involves defining corner submatrices of H and showing that they must be horizontal step matrices. This is done by equating arbitrary entries of each submatrix. In all cases, our decomposition correctly recovers the upper left corner of H, and analogous arguments show that the other three corners are also correctly recovered. Hence, our decomposition is correct, and by induction, H \u2208 B. Corollary I.1 states that a vertical step matrix V \u2208 B*. The proof of Lemma I.1 involves decomposing S into P1HP2VP3, where each P is a permutation matrix, H is a horizontal step matrix, and V is a vertical step matrix. To decompose matrix S into HV, we parameterize with non-zero entries indexed in row-major order. Matrix H has one non-zero entry in each of its first s columns, forming a horizontal step matrix. The non-zero entries in H are sorted due to the row-major order of \u03b8. Adjacent columns of H have non-zero entries at unique locations, confirming H as a horizontal step matrix. The matrix H is a horizontal step matrix with non-zero entries in adjacent columns at most one row apart. Similarly, the matrix V is a vertical step matrix with at most one non-zero entry per row. By permuting the rows of V, it can be represented as V = P2V, where P2 is a permutation matrix. Therefore, the decomposition S = P1HP2VP3 is achieved, where H is a horizontal step matrix and V is a vertical step matrix. The BB * hierarchy is compared to the BP hierarchy introduced by Dao et al. (2019). It is shown that matrices like the Discrete Fourier Transform and the Hadamard Transform belong to the BB * hierarchy. The Discrete Sine Transform and Discrete Cosine Transform are in the BB * hierarchy. The Discrete Fourier Transform and Hadamard Transform also belong to this hierarchy. Additionally, circulant matrices are shown to be in the BB * hierarchy as well. The Fastfood matrix class can be represented in the BB * hierarchy. Circulant matrices can be expanded into Toeplitz matrices. The BB * hierarchy includes various matrix classes such as AFDF and ACDC, which are tightly captured within it. AFDF matrices involve diagonal matrices and Fourier transforms, while ACDC matrices involve diagonal matrices and cosine transforms. These matrices are shown to belong to the BB * hierarchy. The BB * hierarchy includes matrix classes like AFDF and ACDC, tightly captured within it. AFDF involves diagonal matrices and Fourier transforms, while ACDC involves diagonal matrices and cosine transforms. C \u2208 (BB * ) 2, so C \u22121 \u2208 (BBS) 2 as well. AC \u22121 DC \u2208 (BB * ) 4. Columns in DFT and Hadamard transform matrices are orthogonal with norm 2. Factors can be divided by \u221a2 to make orthogonal matrices, countered by adding a diagonal matrix with \u221a2 log2(n) = \u221an. Larger matrices capture multi-dimensional versions of transforms. The separation property of the 2-D DFT allows us to express its action on an n \u00d7 n matrix as the composition of a 1-D DFT on each of its rows and columns. The 2-D DFT is viewed as an n^2 \u00d7 n^2 matrix, with inputs and outputs as column vectors of size n^2. The 2-D DFT is considered in four steps, where the first two steps perform the 1-D DFT row-wise, and the second two steps perform the 1-D DFT column-wise. Step 1 involves permuting the columns with a bit reversal permutation, while Step 2 multiplies each row by a butterfly matrix. Step 4: \nTo achieve the 2-D DFT, the rows are multiplied by a butterfly matrix and then permuted using a bit reversal permutation. This permutation can be combined with the column permutation into a single permutation matrix. Finally, each column is multiplied by a butterfly matrix to complete the process. The 2-D DFT can be achieved by multiplying rows with a butterfly matrix and permuting them using a bit reversal permutation. This process can be combined into a single permutation matrix. Each column is then multiplied by a butterfly matrix. Additionally, 2-D Discrete Sine, Discrete Cosine, and Hadamard Transforms can be expressed in terms of butterfly factor matrices. The 2-D DFT can be achieved by multiplying rows with a butterfly matrix and permuting them using a bit reversal permutation. Each column is then multiplied by a butterfly matrix. In Section K.1, the existing kaleidoscope hierarchy is modified to create the orthogonal kaleidoscope hierarchy OBB. Then, in Section K.2, it is argued that all orthogonal matrices can be expressed in this hierarchy in O(n). In Section K.3, permutation matrices and sparse matrices exist in the hierarchy in O(1) width, implying a result for matrices with low-depth arithmetic circuits. The definition of an orthogonal butterfly is provided, where all butterfly factors are orthogonal. The text discusses the construction of s-sparse matrices in the OBB hierarchy with the same width as the BB * hierarchy using Singular Value Decomposition. It also mentions the factorization of an arbitrary n \u00d7 n matrix and the decomposition of matrices into orthogonal components. The proof follows a structure similar to Theorem 3, starting with arguments about permutation and step matrices. The proof follows a structure similar to Theorem 3, arguing about permutation and step matrices. Matrices with at most n NNZ are contained in (BB * ) 5, and a modified sum closure lemma extends the argument to matrices of general s NNZ. Permutations are included in OBB as a corollary to Theorem 2, and sparse matrices are contained within the OBB hierarchy using Lemma K.3. In the OBB hierarchy, sparse matrices are included using Lemma K.3, which involves permutation and diagonal matrices. A similar result to the BB * hierarchy is presented in Lemma K.4, showing that horizontal step matrices can be decomposed into a diagonal matrix and an orthogonal butterfly factor. The butterfly factor in the OBB hierarchy involves permutation matrices and diagonal matrices. Each 2x2 block can be decomposed into a diagonal matrix times an orthogonal matrix. The construction ensures orthogonality, with rotation matrices being commonly used. The construction of butterfly factors involves diagonal matrices and orthogonal matrices. By decomposing each butterfly factor matrix into a diagonal matrix and an orthogonal butterfly factor, we can express any horizontal step matrix in the OBB hierarchy. This decomposition is done in parallel on each butterfly factor, leading to the conclusion that any butterfly factor matrix can be expressed as desired. Conjugation by P 2 k is an isomorphism from butterfly factors onto block diagonal matrices with 2 k\u22121 , 2 \u00d7 2 blocks. A block diagonal matrix composed of orthogonal matrices is orthogonal. The decomposition of vertical step matrices follows from the horizontal step matrix proof. Vertical step matrices can be decomposed into a diagonal matrix and an orthogonal matrix. Matrices with at most n NNZ belong to the OBB hierarchy. The construction from Lemma I.1, along with Lemma K.4 and Corollary K.3, is used to express S as a sum of matrices in the OBB hierarchy. An orthogonal-based construction is extended to capture matrices of general sparsity using Lemma K.5, leading to Corollary K.5 on general orthogonal sparsity. The proof involves a sum of matrices with at most n NNZ, utilizing Lemma K.5 for handling the sum of matrices. Lemma K.5 provides a proof for expressing matrices in the OBB hierarchy. It involves constructing matrices using a block diagonal matrix K and folding matrices into the rightmost OBB factor. The process includes handling scalar multiples and applying the identity multiple times to show that the resulting matrix belongs to (OBB)mw. The factor of \u221a2 can be multiplied through any diagonal matrix in the decomposition of M. Two orthogonal butterfly factor matrices can be folded into the rightmost R matrix. The sparsity result in Lemma K.5 allows for low-depth arithmetic circuits for matrix vector multiplication in the OBB hierarchy. Corollary K.6 states that if matrix-vector multiplication of M times a vector v can be represented by a linear arithmetic circuit with s gates and depth d, then M \u2208 (OBB). The linear network counterpart of a neural network with ReLU nonlinearities and few-gate weight matrices also has an arithmetic circuit with not many more gates. This suggests finding the smallest arithmetic circuits for matrix-vector multiplication of each weight matrix to represent a ReLU network efficiently. The network computes matrix-vector products using ReLU gates and arithmetic circuits. It involves passing vectors through circuits to compute ReLU values and requires additional gates for each layer. The process is repeated for each weight matrix to efficiently represent a ReLU network. The network computes matrix-vector products using ReLU gates and arithmetic circuits, requiring additional gates per layer. An arithmetic circuit augmented with ReLU gates can compute network activations without ReLU efficiently. The VC dimension of a ReLU network with bounded weight matrices is analyzed. The class of networks analyzed has VC dimension O(LW log W), leveraging results from previous studies on weight matrices interactions. The proof involves checking the degree of entries in the linear layer as polynomials of the parameters, with bounded width and expansion of weight matrices. The VC dimension of the analyzed networks is bounded by O(LW log W), which is achieved through weight matrices interactions. Arithmetic circuits are used to compute matrix-vector multiplication, with n input gates and m output gates. Internal gates are also present in the circuit. Arithmetic circuits consist of input gates, output gates, and internal gates performing addition, subtraction, multiplication, and division over a field F. The circuit size is determined by the number of gates used, and the depth is the minimum number of layers needed for gate inputs. However, arithmetic circuits assume exact operations over F, disregarding precision issues in real arithmetic. Arithmetic circuits are useful for matrix-vector multiplication complexity analysis. Most algorithms for this task imply an arithmetic circuit of similar size to the runtime. Linear Arithmetic Circuits involve constants from F in their multiplications. Linear arithmetic circuits only use addition, subtraction, and multiplication with fixed constants from field F. For matrix-vector multiplication, it is practical to use linear arithmetic circuits as the final function Ax is a linear function of its inputs. Any arithmetic circuit to compute Ax over an infinite field F can be converted into a linear arithmetic circuit without loss of generality. Linear arithmetic circuits are used for matrix-vector multiplication, with a circuit size of O(s) and depth O(d). The depth is determined by the sums in the circuit, and can be represented as a product of sparse matrices. Efficient matrix-vector multiplication algorithms are equivalent to small linear arithmetic circuits, such as the FFT for computing the Discrete Fourier Transform. The Discrete Fourier Transform is computed using linear arithmetic circuits, where each matrix in the decomposition represents a butterfly factor. The arithmetic circuit is illustrated for n = 4 in Figure 11, and represented as a product of a butterfly matrix and permutation in Figure 13. Existing efficient matrix vector algorithms use divide and conquer techniques like FFT for polynomial operations. The recent work of De Sa et al. presents a general structure on matrices implying near-linear size linear arithmetic circuits for matrix vector multiplication. This work combines orthogonal polynomial transforms and matrices with low displacement rank to solve the matrix vector multiplication problem efficiently. Structured matrices with low displacement rank have been utilized in neural network architectures to replace fully connected layers, as shown in previous studies (Sainath et al., 2013; Thomas et al., 2018)."
}