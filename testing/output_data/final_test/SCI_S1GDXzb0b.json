{
    "title": "S1GDXzb0b",
    "content": "Imitation learning from demonstrations often lacks action information in expert state trajectories. A model-based method is proposed to learn optimal actions from expert state trajectories using reinforcement learning and supervised learning. Experimental results show comparable performance to traditional methods. Our method can learn to act from video demonstrations of expert agents for simple games and achieve desired performance in fewer iterations compared to conventional reinforcement learning methods. Reinforcement learning involves training an agent to maximize future rewards based on a guiding signal, demonstrated in Atari games and robotics control tasks. Imitation learning involves copying expert behavior to achieve a task without a guiding reward signal. Expert trajectories are available, but the reward function used by the expert is unknown. The goal is to learn a new policy to mimic the expert's behavior. Imitation learning aims to mimic expert behavior by learning a new policy that maximizes the likelihood of given demonstration trajectories. This can be achieved through behavior cloning, where a parameterized function is trained with state and action pairs from expert trajectories. Behavior cloning is more sample-efficient than reinforcement learning and overcomes issues with credit assignment in model-free methods. Behavior cloning requires numerous training samples to reduce errors in action prediction for satisfactory imitation learning. Another approach involves setting up exploration in a Markov Decision Process (MDP) to recover a reward signal that explains expert trajectories. Inverse Reinforcement Learning (IRL) aims to find a reward signal from expert trajectories to create a policy that imitates expert behavior. Recent advancements in imitation learning have shown that using adversarial learning, such as Generative Adversarial Imitation Learning (GAIL), can significantly reduce the number of demonstrated trajectories needed to imitate expert behavior compared to behavior cloning. This approach utilizes a discriminator as a cost function in model-free reinforcement learning. Additionally, there have been extensions to model-based generative imitation learning using a differentiable dynamics model of the environment. In imitation learning, strategies combining variational autoencoders and GAIL have been proposed to imitate expert behavior. Unlike previous works, real-world tasks like skipping or jump rope often involve learning from visual input only, without access to optimal actions. To imitate expert trajectories, one must explore the environment to understand the dynamics and reproduce similar state trajectories. Learning to imitate an expert from state information only, without action details, involves understanding the dynamics of the environment through model-free exploration. This approach mirrors human learning by replicating expert state trajectories to gain insights into the dynamics. The forward model learned from expert state trajectories provides information on the environment dynamics. By maximizing the likelihood of the next state, errors can be back-propagated through the model for model-based policy updates. The proposed network can mimic expert behavior with fewer iterations compared to reinforcement learning methods. The paper introduces notations for a Markov Decision Process (MDP) and compares different reward strategies based on prediction errors. The Markov Decision Process (MDP) is defined by sets of states (S), actions (A), transition probabilities (P), rewards (r), initial state distribution (\u03c1 0), and discount factor (\u03b3). The policy (\u03c0) gives the distribution of actions given the current state, and the discounted reward (R(\u03c0)) is the expected sum of rewards. Expert trajectories consist of optimal state distributions, while model-free exploration trajectories include both states and actions. The forward model (f) represents the environment dynamics. The proposed training procedure involves learning a differentiable forward model of the environment. The goal is to imitate an expert using optimal state trajectories only, common in scenarios where only state information is available. A time series predictor is trained to predict the next state given the current state, followed by estimating a heuristic reward. The proposed algorithm involves model-based policy learning by training a differentiable forward model of the environment to imitate an expert using optimal state trajectories. This approach aims to address the limitations of heuristic methods in modelfree training and convergence guarantees. In the MDP framework, the imitation learning problem involves finding an optimal reward signal to maximize the likelihood of observed demonstration trajectories. However, estimating a uniquely optimal reward function is challenging, especially without action information in expert trajectories. To address this, heuristic reward estimation based on next state prediction error is used for model-free policy learning. This approach assumes that locally optimal rewards for predicting the next step globally optimize task learning. In the MDP framework, heuristic reward estimation based on next state prediction error is used for model-free policy learning to maximize expected rewards. The approach assumes that locally optimal rewards for predicting the next step globally optimize task learning. The proposed method involves updating a model-free policy using reward estimation from next state mismatch and storing samples in a replay buffer. These samples are used to update a differentiable dynamics model, providing error gradients for end-to-end model-based policy updates from state trajectories. The goal is to imitate an expert agent's policy from state trajectories by formulating a maximum likelihood problem with a parameterized model. The model assumes a directed graphical structure for state, action, and next state variables. Control actions are independent of past states and actions given the current state. The next state distribution is independent of past states given the current state and action. The model-based policy consists of an encoder network with action as the latent variable and a decoder network predicting the next state. Learning is done by minimizing the log-likelihood estimation loss using expert trajectories. The proposed method involves minimizing a cost function to ensure that the decoder network mimics the environment dynamics of the agent, improving the prediction accuracy of the next state given the current state and action. This approach aims to enhance the model-based policy network's performance by enforcing constraints on the loss function. The method involves updating dynamics model parameters and model-based policy parameters alternately. The encoder and decoder are trained on separate datasets, with the encoder acting as a model-based policy. The encoder and decoder are deterministic, and a single action is sampled from the model-based policy. The method involves updating dynamics model and model-based policy parameters alternately. Learning from heuristic reward in a MDP setting can be slow, so dynamics model based learning is used to propagate error gradients with respect to action information. By learning the state transition model, a robot can navigate through the environment and obtain the best action for optimal state trajectories. Solving for the desired action at each state is a maximum likelihood problem from expert state trajectories. During model-free learning, trajectories of state-action-next state triplets are stored and used to update dynamic model parameters through gradient descent. Neural networks are utilized as function approximators for this optimization process. Neural networks are used as function approximators in optimization processes, with recent techniques in stochastic gradient descent alleviating optimization problems. The proposed method is similar to behavior cloning but lacks true action information from the expert. The performance is limited by the behavior cloning model. The next state predictor predicts the entire next state of the agent, but it was found that predicting only a part of the state dependent on certain factors is sufficient. The next state predictor predicts a part of the state dependent on action, improving reward structure. This transformation benefits learning dynamics models for high-dimensional state information and facilitates transferring learned models between tasks. The algorithm for model-based policy learning involves using agent position or joint angles/velocities as \u03c6(s t ), updating dynamics model parameters, and training a model-free algorithm with heuristic reward function. This iterative process includes collecting system dynamics data, training a dynamics model, and then training an action policy using the model. The action policy is trained using the system dynamics model in the model-based part, with each iteration collecting additional data for a more precise dynamics model. The frequency of switching between model-free and model-based updates can vary based on the complexity of the dynamics model. Long Short-Term Memory Network is used for state predictions, while neural networks serve as function approximators for the policy and dynamics model. The state transformation is manually specified in each experiment. The proposed algorithm learns a common transformation between states to predict actions in various environments, including robotics arm reacher in 2D and learning to play games from raw video demonstrations. The algorithm demonstrates specific strengths in tasks such as reaching a target with a two-link robotic arm controlled by angular torque values. State values include angular position, velocity, end effector location, and target position. In this experiment, arm angles and angular velocities were used to determine the portion of state dependent on action. The goal is to improve policy learning using existing state trajectories and a known reward function. Neural networks with hidden layers were used for training over 2000 episodes with Deep Deterministic Policy Gradients (DDPG). The proposed method outperformed the DDPG algorithm in learning the dynamics model. The proposed method outperforms the DDPG algorithm by learning dynamics model through model-free exploration, leading to faster policy imitation. The algorithm demonstrates one-shot imitation learning on novel tasks without relying on reinforcement learning, showcased in a simple 2D environment. The environment in FIG1 shows an agent moving in 2D space with continuous position control. The goal is to reach a target while avoiding obstacles. The algorithm is trained on avoiding a single obstacle while reaching the target using state information and expert demonstrations. 800 out of 1000 demonstrations are used for training. The algorithm learns time series prediction for heuristic reward calculation using a MLP with (64, 64) hidden units. A single hidden layer neural network models the dynamics with 8 units for state and action input. The model-based policy and dynamics model are obtained using a switching frequency of 5 between model-free and model-based updates. The dynamics model trained on single obstacle avoidance is used for one-shot imitation learning to avoid two obstacles with 500 expert state trajectories. The proposed algorithm learns model-based control policies from raw pixels and demonstrates one-shot imitation learning in environments with the same dynamics. Results show comparable performance to behavior cloning, with an average test reward of 3805 on 50 episodes. The algorithm uses a MLP for time series prediction and a neural network for dynamics modeling, with a switching frequency of 5 between model-free and model-based updates. The algorithm learns model-based control policies from raw pixels in the Flappy Bird environment. It uses raw videos to learn action policies and estimates rewards based on prediction errors. The control action is a single command to flap or not flap the bird's wings, with state information from 4 consecutive frames. The algorithm learns model-based control policies from raw pixels in the Flappy Bird environment using 4 consecutive frames resized to (80 \u00d7 80 \u00d7 4). The model-free reward prediction step receives a reward signal based on the predicted next state by LSTM. This is used to train DQN for model-free policy update and collect data to train the dynamics model for the model-based policy network. Vanilla DQN is also trained using a standard reward, with a baseline reward of 0.1 for each time step and +1 reward for successfully passing through a pipe. The model-based policy uses a CNN with soft-max output for binary classification. The dynamics model is learned using an MLP with a single hidden layer. The model-based policy is found through behavior cloning on expert demonstration data. The study used class balancing techniques to address the imbalance in distribution of actions in behavior cloning. Comparison results showed that estimated reward led to faster convergence in model-based methods compared to original reward. Samples were taken from model-free training using prioritized sampling technique. The study utilized prioritized sampling from model-free training to perform model-based policy updates. Prioritized sampling was crucial for learning a good dynamics model. Model-based policy showed faster learning compared to model-free methods, but was outperformed by behavior cloning with an average test reward of 49.62. The model-based imitation learning method can learn from expert state trajectories without action information. Our method utilizes trajectories from model-free policy exploration to train a dynamics model of the environment. As the model-free policy improves over time, the forward model better approximates the environment dynamics, leading to enhanced gradient flow and improved model-based policy updates trained from expert state trajectories. When the dynamics model perfectly approximates the environment, our method is equivalent to behavior cloning even without action information. The proposed approach learns the desired policy in fewer iterations compared to conventional model-free methods and can transfer learning to other tasks in a similar environment in an end-to-end supervised manner. Future work aims to integrate model-based learning more tightly. Future work aims to integrate model-based learning more tightly by sharing information between the model-free policy and the model-based policy, as well as between the next state predictor and the dynamics model. Adversarial training is also considered to address limitations such as compounding errors and the need for a large number of demonstrations."
}