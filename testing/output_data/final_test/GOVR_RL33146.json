{
    "title": "RL33146",
    "content": "The VSS were developed in response to concerns about the unregulated voting technology industry in the 1970s and 1980s. An FEC advisory panel recommended voluntary standards in 1977, following a study by the National Bureau of Standards. Congress directed the FEC to study the matter in 1979, leading to the development of the VSS with federal funding. The VSS were first released in 1990. The VSS were first released in 1990 for computer-based voting systems, including punchcard, marksense, and DRE systems. Standards were developed for hardware and software, with functional requirements, testing procedures, and performance characteristics. The FEC planned to implement the VSS through cooperative action with NIST, ITAs, state governments, and voting system vendors, but the plan did not materialize as originally conceived. The National Association of State Election Directors (NASED) appointed a voting systems board to choose ITAs and administer a process for qualifying voting systems under the VSS. The testing program began in 1994, with states adopting the VSS and some requiring voting systems to be qualified under the standards before use. Calls for revising and updating the VSS increased as technology evolved, leading to the FEC approving a second version in May 2002 with a broader focus on the voting medium. The VSS was updated in 2002 with a broader focus on the voting medium, including paper-based versus electronic systems. Congress passed the Help America Vote Act of 2002 (HAVA), establishing the EAC and developing the VVSG. The Help America Vote Act of 2002 (HAVA) established the EAC and renamed the VSS as the Voluntary Voting System Guidelines (VVSG), with NIST playing a substantial role in providing support for the development of guidelines related to voting system usability, accuracy, security, accessibility, and integrity. The VSS was renamed as the Voluntary Voting System Guidelines (VVSG) under the Help America Vote Act of 2002. The development of the first version of the VVSG began in 2004 with the appointment of the TGDC. The EAC released the draft VVSG in June 2005 for public comment, with adoption expected in October 2005. Adoption of the guidelines will go into effect two years after being adopted. The Help America Vote Act (HAVA) transferred accreditation responsibility from NASED to EAC with support from NIST. NIST began accrediting laboratories in 2005 under NVLAP. EAC provided temporary accreditation to NASED-accredited laboratories. The draft VVSG released for public comment in 2005 is similar to the 2002 version. The draft version of the VVSG released for public comment in 2005 is similar to the 2002 update of the VSS, with major revisions focusing on usability, security, and testing procedures. Changes include the addition of a conformance clause, revised standards for accessibility and usability, security standards for voter-verified paper ballots used with electronic voting machines, and new appendices. A more extensive revision is reportedly underway. The VVSG, like the VSS, are divided into two volumes. Volume I provides performance guidelines for voting systems and is intended for a broad audience. It includes descriptions of functional requirements, performance standards, quality assurance, and configuration management. Volume II details the testing process for certification of voting systems. The curr_chunk discusses the testing process for certification of voting systems, aimed at vendors, testing laboratories, and election officials. It outlines data requirements for system testing, basic testing standards, and guidance for testing laboratories. Public comments on the draft VVSG are also mentioned, with input from various stakeholders. The VVSG are voluntary technical standards, not regulatory requirements. Some believe that federal voting system standards should be mandatory to ensure quality, while others argue that mandatory standards would limit state and local government flexibility. Most states already require adherence to the VSS or VVSG for purchasing new voting systems, making certification essential for vendors to market their products. The HAVA addresses the controversy of federal voting system standards by establishing specific requirements for voting systems but leaving implementation to the states. The EAC provides guidance for implementing the requirements, but it is not a technical standard and its use is voluntary. There are various types of standards classified by purpose and focus, such as product, process, testing, or interface standards, and performance standards that focus on function. The VVSG and VSS combine product, process, and testing requirements, aiming to be performance-based but also providing specific design details, which has been criticized by some observers. The VVSG and VSS have faced criticism for being too specific or vague, and are voluntary at the federal level but mandatory in some states. The development process has been partially open, involving NIST and EAC boards. Properly designed standards can ensure voting systems operate as expected. The VSS and NASED certification program aim to improve voting system performance, but limitations in standards development and implementation can lead to issues. Criticisms include testing in controlled environments rather than real-world conditions, which may not capture all potential errors, such as voter error in the 2000 presidential election. The Help America Vote Act (HAVA) addressed issues with voter error in the 2000 presidential election and the limitations of outdated standards in anticipating evolving technology and security threats. The development of the Voluntary Voting System Guidelines (VVSG) under HAVA involved a complex process with input from various committees and boards before final approval by the Election Assistance Commission (EAC). The development of standards for the Voluntary Voting System Guidelines (VVSG) under the Help America Vote Act (HAVA) can be a lengthy process involving input from various committees and boards before final approval by the Election Assistance Commission (EAC). The updating cycle for the VVSG is not specified in HAVA, but guidance for implementation must be updated every four years. The first version of the VVSG is expected to go into effect two years after adoption, with an initial revision cycle of at least two years. The development cycle for the Voluntary Voting System Guidelines (VVSG) is currently at least two years, with discussions on potentially extending it to four years to allow systems to be used for two federal election cycles without recertification. Some critics argue that the process is too slow given the rapid pace of technology advancement, which could pose financial and logistical challenges for vendors and election officials. There is a concern about the balance between adapting to evolving needs and technology while maintaining the quality of revisions. The Internet Engineering Task Force (IETF) addresses the challenge of rapid development and change by creating a system for developing standards largely online. Interested parties form open working groups to develop standards, with drafts posted online for comments and eventual review by the Internet Engineering Steering Group (IESG) before becoming standards. The draft guidelines for the VVSG are developed through an open, online process with rough consensus and independent review. However, adapting this method for voting technology is complex. Incorporating other accepted standards, such as NIST publications, raises issues in the VVSG development process. Some argue that product standards like the Common Criteria would be more suitable for security management instead of the VVSG. Concerns exist about public disclosure of certification testing plans and results due to proprietary information. Some argue that public disclosure of certification testing plans and results could deter investment and innovation in the industry, while others believe in full disclosure to maintain public trust in voting systems. Critics question the lack of competition in choosing testing laboratories under the NASED process, leading to higher costs and delays in certification for smaller manufacturers. The NVLAP accreditation process under EAC aims to address these concerns by increasing the number of involved laboratories, but it has also faced criticism for potential competition issues. The VSS has faced criticism for not adequately addressing various aspects of election administration, but some of these criticisms have been partially addressed in the VVSG. The VVSG is largely unchanged from the 2002 version of the VSS, with the addition of new sections on human factors and security. The two major areas of revision in the VVSG are considered crucial. The 2002 version of the VSS has been revised with a focus on usability, accessibility, and security concerns, particularly regarding DREs. Some believe that broader changes are urgently needed, advocating for an \"end-to-end\" approach with standards for technology, procedures, and personnel in election administration. The extent to which future versions of the VVSG will incorporate these changes remains unclear. Some observers have concerns about the first version of the VVSG, suggesting a delay in adoption for thorough revision. Others feel that the added provisions are insufficient to meet accessibility, language, and security needs, calling for broader and more stringent requirements. There are also criticisms that certain sections are recommendations rather than requirements, potentially not ensuring full compliance with HAVA's accessibility requirements. Some concerns exist regarding the VVSG not fully covering all disabilities as required by HAVA, with limited details provided for certain disabilities. Questions have also been raised about the revised security provisions, including the use of penetration testing in state-sponsored studies to address voting system security issues. Some observers have raised concerns about conflicting requirements in voting system security testing, particularly regarding the frequency of failures. There is debate over whether the current standard of failures occurring no more than every 163 hours is too low, with some arguing for a focus on system availability instead. Availability testing is done in controlled conditions and may not reflect real-world scenarios. In voting system security testing, there is debate over the frequency of failures and the focus on system availability. Achieving an availability score greater than 99% in testing may not translate to actual use due to factors like repairs and management guidelines for election administrators. The VVSG currently includes best practice recommendations in an appendix, raising questions about the development of administration sections in future versions. The VVSG guidelines are being finalized by the EAC after receiving over 4,000 comments. The guidelines are not set to go into effect until fall 2007, with federal certification of voting systems continuing based on the 2002 VSS until then. The EAC is finalizing VVSG guidelines to assist states in meeting HAVA requirements. Concerns exist about delays in procuring new voting systems due to uncertainties about VVSG development. There are worries about systems not being in compliance with requirements or of lower quality once VVSG go into effect. Additionally, there are concerns that systems certified under VSS but not VVSG could be effectively decertified. The EAC is finalizing VVSG guidelines to help states meet HAVA requirements. Concerns arise about delays in acquiring new voting systems and uncertainties regarding federal funding. Some suggest using systems conforming to the 2002 VSS to satisfy HAVA requirements, but there are doubts about its effectiveness. Delays in VVSG implementation may lead to using non-compliant systems for the 2006 federal election, raising concerns about the timeline for vendors and states to develop conforming systems before the 2008 election. The focus of the VSS has been on polling-place and central-office systems. The VSS has focused on polling-place and central-office systems involved in elections, while the VVSG guidelines do not cover technology for managing voter registration lists. Concerns have been raised about the lack of formal standards for voter registration data, potentially impacting state compliance with HAVA requirements by the January 2006 deadline. The VVSG guidelines focus on voting systems rather than individual components. Vendors must submit the specific system configuration for testing, which may stifle innovation by requiring agreements with voting-system vendors before certification. One proposal to address concerns about component certification is to allow separate certification of components, but require subsequent certification with each voting system. The use of COTS software in voting systems is controversial due to lack of control over code and complexity, despite cost benefits and compatibility with standard software like Microsoft Windows. Some argue against exempting unmodified COTS software from code examination in VVSG, citing security risks. Proponents defend COTS software use for voting systems, claiming it allows for better applications at lower costs. VVSG testing scheme only indicates pass or fail without providing detailed performance information to customers. The Voting System Performance Rating (VSPR) project was initiated in 2004 to develop performance rating standards for U.S. voting systems. The VVSG includes standards for voter-verified paper audit trails (VVPAT), mandated by several states, but there is no indication of when the VSPR standards will be released. The VVSG includes guidelines for Independent Dual Verification (IDV) systems, which provide two verifiable channels for processing votes. These systems are seen as essential for ensuring security and confidence in electronic voting, with some believing VVPAT is the most promising method. However, there are concerns about the costs and complexity of implementing IDV systems. Some believe VVPAT should be required in the VVSG guidelines, while others question its practical value and potential problems. Concerns also exist regarding accessibility and alternative language provisions not fully complying with federal requirements. There are calls for more stringent requirements on data recording for audit purposes. State-level trends towards VVPAT use for security raise questions on reconciling these requirements with HAVA accessibility constraints. Several bills introduced in the 109th Congress could affect the scope of the VVSG, including provisions on security of electronic data, submission and testing of voting machine software before an election, use of a voter-verified paper audit trail, verification methods for voters with disabilities, and best practices for voter-verification for persons with disabilities and languages other than English. Several bills introduced in the 109th Congress could affect the scope of the VVSG, including provisions on security of electronic data, submission and testing of voting machine software before an election, use of a voter-verified paper audit trail, verification methods for voters with disabilities, and best practices for voter-verification for persons with disabilities and languages other than English. The bills include requirements for conflict of interest standards, minimum number of voting machines at polling places, use of open-source software, prohibition of wireless communications, public disclosure of certification information, establishment of standards for early voting, and federal write-in absentee ballot standards. Several bills introduced in the 109th Congress could impact the VVSG, including requirements for security standards, EAC certification of technological security, and prohibiting states from not registering certain voters. These bills have not advanced in Congress. Section 1 of the guidelines outlines objectives for voting systems, emphasizing functionality, performance, documentation requirements, and evaluation criteria for certification. It also defines a voting system to include software and documentation. The voting system includes software and documentation related to the election process, categorizing systems as paper-based or direct recording electronic. It outlines guidelines for certification, exempting commercial off-the-shelf products if unmodified. The VVSG introduces a conformance clause defining implementation requirements. Section 2 of the VVSG outlines the functional capabilities expected in a voting system, including security, accuracy, error recovery, integrity, and system audit. These capabilities involve technical and administrative controls, redundant record-keeping, malfunction recovery, protection against outside interference, and automated generation of audit records. The Election Management System includes databases for officials to perform various functions, from defining political subdivisions to processing election audit reports. The Human Factors subsection has been expanded with standards based on nondiscriminatory access to voting, accurate ballot selection capture, and ballot secrecy preservation. It includes provisions for disabilities related to vision, dexterity, mobility, hearing, speech, cognition, limited English proficiency, and alternative language accessibility. The Election Management System includes databases for officials to perform various functions, from defining political subdivisions to processing election audit reports. The Human Factors subsection has standards for nondiscriminatory access to voting, accurate ballot selection capture, and ballot secrecy preservation. Provisions in the curr_chunk relate to usability, vote tabulating programs, ballot counters, telecommunications, data retention, prevoting functions, and voting functions to ensure accurate and secure casting of votes. The curr_chunk discusses requirements for voting system hardware, including performance, operational specifications, vote recording specifications, and ballot reading and processing specifications. It covers functioning devices used in voting, postvoting functions, maintenance, transportation, and storage of systems. The curr_chunk focuses on requirements for voting system hardware, including ballot boxes, DRE features, machine readers, printers, storage media, and data management. It emphasizes design, maintenance, reliability, and software specifications regardless of programming language or source. The curr_chunk discusses software requirements for voting systems, covering design, coding, data retention, security, audit records, and vote secrecy for DREs. It also includes telecommunications requirements for systems operation and reporting election data. Section 5 covers telecommunications requirements for systems operation and reporting election results, including performance, design, and maintenance characteristics. It includes technologies like dial-up communications, cable, wireless, and high-speed lines. Data transmission from election preparation to preservation of data and audit trails is covered, along with voter authentication, ballot definition, vote transmission, counts, and lists of voters. Requirements for accuracy, durability, reliability, maintainability, and availability are the same as in Section 3. Wide-area networks have restrictions to prevent outside access and single points of failure. Successful notifications to users are mandatory. Section 6 focuses on essential security capabilities for all voting systems components, emphasizing controls to minimize errors, protect against manipulation, identify fraudulent changes, and ensure voting secrecy. It covers access control, equipment and data security, software security, and distribution practices. Effective security practices by jurisdictions are crucial but not addressed in this section. The curr_chunk discusses detailed requirements for software security, telecommunications, data transmission, and optional voter-verified paper trail (VVPAT) in voting systems. It includes protection against malicious software, encryption for data protection, and caution regarding wireless communications. The curr_chunk discusses requirements for audit and election data, quality assurance programs for vendors, and configuration management. It also includes a glossary in Appendix A. Appendix A in the VVSG includes an expanded glossary with sources and keywords for each entry. Appendix B lists reference documents incorporated into the guidelines. Appendix C outlines best practices for election officials regarding usability and security requirements. Appendix D explains independent dual verification systems like VVPAT, which produce two separate ballot records for voter verification and post-election audit. The curr_chunk discusses different methods of voter verification in election systems, including paper-based verification, end-to-end systems with cryptographic techniques, witness systems using cameras, and direct systems like VVPAT. It also addresses issues in handling records produced by independent dual verification systems. Appendix E provides a technical guideline for vendors to meet requirements for improving readability for individuals with low vision or color blindness. It outlines the certification testing process, including operational accuracy, system performance, and documentation quality. Testing categories include functionality, hardware, software, system integration, and quality assurance practices. Vendors must follow a designated testing sequence and submit their systems for testing according to specified practices. Vendors must submit specific system configurations for testing, including new systems and modifications. Certain hardware is exempt from testing based on established standards. One laboratory must be designated as the lead, with independent observers present during testing. The EAC will resolve any interpretation issues of the VVSG. The Technical Data Package (TDP) must be submitted at the start of the certification process, detailing system descriptions. The Technical Data Package (TDP) is a crucial document that vendors must submit at the beginning of the certification process. It includes system design descriptions, functional capabilities, performance specifications, standards, compatibility requirements, and quality assurance practices. The TDP must also identify proprietary information and include a security specification. Modifications to previously certified systems require submission with notes detailing changes to the system, documentation, and vendor testing. Testing in Section 3 confirms functional capabilities, emphasizing flexibility to accommodate design variations and additional capabilities not in the VVSG. Section 4 further describes testing requirements. Section 4 details testing of hardware components to ensure proper functionality according to requirements. It includes operational and environmental tests for nonoperating equipment, such as handling, vibration, temperature, and humidity responses. Testing is also conducted for maintainability and reliability. Accuracy testing involves consecutive counting of ballot positions, with specific error thresholds for acceptance and rejection. Systems falling between these thresholds are discussed in Appendix C. Code examination for unmodified COTS software is not required unless it is general-purpose and used for voting. Source code is checked for conformance to vendor specifications, adherence to requirements, and use of specified control constructs and coding conventions. Testing in Section 6 ensures proper functioning of voting-system components under normal and abnormal conditions, including security capabilities and accessibility. Configuration audits are conducted to compare components with technical documentation. Section 7 examines a vendor's configuration management and quality assurance processes for VVSG requirements. On-site examination is not necessary. Appendix A provides an outline for the laboratory's procedures. Appendix A contains a recommended outline for the laboratory's testing plan, called the National Certification Test Plan, specific to a particular system being submitted for certification. Appendix B contains a recommended outline for the laboratory's test report, called the National Certification Test Report. The report includes a recommendation to the EAC for approval or rejection of the application for certification, as well as descriptions of any uncorrected deficiencies. Deficiencies not involving loss or corruption of data may not lead to rejection. Appendix C describes the principles used to design the test-design criteria. The curr_chunk discusses test-design criteria in the VVSG, balancing data production with cost, and definitions of terms like ANSI, BIOS, COTS, and DRE. The curr_chunk discusses the Electronic Voting System, EAC, FEC, FISMA, HAVA, and IDV. The curr_chunk provides definitions of key organizations related to electronic voting systems, including IDV, IEC, IEEE, IESG, IETF, and ISO. The curr_chunk provides definitions of key organizations related to electronic voting systems, including ISO, ITA, LAN, MTBF, NASED, and NIST. The curr_chunk provides definitions of key organizations related to electronic voting systems, including NVLAP, OEA, ROM, TDP, and TGDC. The curr_chunk explains the functions and standards related to electronic voting systems, including VSPR, VSS, VVPAT, VVSG, and WAN. The TGDC committee makes recommendations to the EAC on VVSG, while VSS are federal voluntary voting system standards. VVPAT provides a printed record of a voter's choices, and WAN is a wide-area network for computer communication."
}