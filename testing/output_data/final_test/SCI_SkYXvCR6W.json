{
    "title": "SkYXvCR6W",
    "content": "This paper introduces a new text to tensor representation that uses information compression techniques to assign shorter codes to frequently used characters. The representation is language-independent, requires no pretraining, and preserves all information. It efficiently describes text morphology, can represent unseen words, and is suitable for speeding up training with tensor processing libraries. When combined with convolutional neural networks (CNNs) for character-level text classification, it significantly reduces the number of parameters to optimize while maintaining competitive accuracy. Document classification is a key task in natural language processing, involving associating text with categories. Various approaches exist, with some focusing on word-based representations. This allows for efficient training on commodity hardware, reducing parameter optimization time while maintaining competitive accuracy. Representation in natural language processing often revolves around words as the basic unit of text analysis. However, the variability in language structures can limit the effectiveness of word-based methods. Deep learning techniques, such as convolutional neural networks, have revolutionized machine learning in recent years, aided by parallel computing libraries like Theano, Tensorflow, and Keras. These advancements have led to superior performance in specific domains where vocabulary is limited, but may require starting from scratch when applied to different languages. The success of deep learning and CNNs in image classification has sparked interest in applying these principles to document classification. Existing methods are mostly based on word tokenization and statistics inference, with Bag of Words (BoW) and Word2vec being popular strategies. The main challenge in replicating image classification success in documents is representing text as numerical tensors. A groundbreaking approach considers characters as the atomic elements of text. The text discusses a novel approach that represents text as matrices using one-hot encoded characters for text classification. This method combines CNNs with LSTMs to improve results but faces challenges due to high computational demands. Character-level representations are seen as more robust than word-level ones but are computationally expensive. In this paper, an efficient character-level encoding of words is proposed to represent texts, derived from the Tagged Huffman BID23 information compression technique. This encoding considers character appearance frequency to assign shorter codes to frequently used characters, making the idea more computationally accessible by reducing training requirements in terms of time and memory. The encoding allows for representing larger portions of texts in a less sparse form without losing information, even for words not present in the training dataset. The novel character-level text encoding proposed in this paper reduces input matrix size, leading to faster training times with comparable or better accuracy than existing methods. This opens up possibilities for more complex applications and the use of devices with lower computational power. In Section 3, the encoding procedure and neural network architectures for experiments are described. Section 4 replicates experiments to compare proposals under similar conditions. Section 5 concludes with final remarks and outlines future work directions. The use of Convolutional Neural Networks (CNNs) in text classification tasks is justified for better results and robust algorithms. The use of Convolutional Neural Networks (CNNs) in text classification tasks involves two approaches: bag of words (BoW) and Word2vec. BoW represents words in a vector with 1 at the word's position in the vocabulary. A subset of the vocabulary is chosen based on text representation, often using word frequency statistics or relevance metrics. In the Word2vec approach, words are represented by fixed-size embeddings based on their co-occurrence in a large text corpus. Pretrained vectors can be used or adjusted for new words. However, this method struggles to represent words not in the training dataset, such as typos, spelling errors, and complex language structures. Focusing on characters as the basic unit of text formation can improve robustness to typos, neologisms, and various textual forms like equations, chemical formulas, and internet language nuances. The paper proposes representing text as a sequence of characters instead of words, allowing for a reduced vocabulary and one-hot encoding. They created a matrix where each row corresponds to a position in the text sequence, with columns indicating the presence of a character. This approach, combined with a CNN, yielded competitive results in text analysis. The approach proposed in the paper involves representing text as a sequence of characters, utilizing a matrix for encoding. While achieving competitive results in text analysis, the main drawback is the significant computational requirement. To address this issue, the authors developed a method that reduces training time significantly, from hours to minutes and from days to hours per epoch, while maintaining accuracy. The approach proposed in the paper involves representing text as a sequence of characters, utilizing a matrix for encoding. To achieve faster execution, the approach consisted of two elements: obtaining a shorter representation using variable length encoding for each word and obtaining a sparse representation with many 0s and a few 1s for efficient tensor multiplications. Our approach involves encoding characters to form words efficiently using Tagged Huffman encoding. The encoding method ensures no character code is a prefix of another, allows for direct search in encoded documents, and serves as a compression technique to reduce input matrix size. The encoding approach discussed involves using Tagged Huffman encoding to efficiently encode characters into words. This compression technique allows for saving encoded text documents permanently in a binary system, requiring less storage space. The method is advantageous for extracting knowledge from files in a repository for various classifications. It also has the benefit of better handling unseen words in training data, particularly in languages with many declensions like Portuguese, Spanish, Italian, French, Russian, German, Arabic, and Turkish. The size of the vocabulary is not restricted, but less frequent characters may result in larger codes. Our main contribution is demonstrating that using a higher word code size reduces the dimensionality of the encoded matrix, significantly cutting down training time and enabling the use of devices with lower computational power while maintaining competitive accuracy. The encoding process involves obtaining a character frequency rank and creating a list sorted by frequency of occurrence, which remains stable for a given language. The encoding process involves creating a mapping from characters to compress codes based on frequency rank. Words are encoded by concatenating the codes of each character. This encoding method can represent words with the same prefix in vectors with similar initial coordinates. Slangs like tl;dr and u2 are also included in the encoding. The encoding process involves mapping characters to compress codes based on frequency rank. Words are encoded by concatenating character codes, representing words with the same prefix in vectors with similar initial coordinates. Mathematical expressions like e a could be manipulated in a matrix of words \u00d7 code size. Larger codes are represented up to a chosen limit, with unoccupied lines filled with 0. In an example using an 8 \u00d7 65 matrix, 256 coordinates were enough for encoding. In experiments, 256 coordinates efficiently represent 99.5% of words in databases. Text is encoded in a 128 \u00d7 256 matrix. The model consists of 9 layers, including convolutions and fully connected layers. Stochastic gradient descent is used with a minibatch size of 128. Results are obtained after at least 30 epochs. Three experiments, CNN1, CNN2, and LSTM, were conducted to verify the encoding procedure's efficiency. In experiments, 256 coordinates efficiently represent 99.5% of words in databases. Text is encoded in a 128 \u00d7 256 matrix. The model consists of 9 layers, including convolutions and fully connected layers. Three experiments, CNN1, CNN2, and LSTM, were conducted to verify the encoding procedure's efficiency. The CNN1 architecture, based on word2vec embedding with 256 features, achieved state-of-the-art results after 5 epochs. Another shallow convolution architecture was created following BID29's recommendations for parameter selection. The architecture includes a convolution width filter scanning from 1 to 7, with width 1 performing better. Max pooling is preferred for sentence classification. An LSTM model with a simple architecture of input layer 126 \u00d7 256, LSTM layer 300, Dropout layer .10, fully connected layer 128 units, and softmax layer was trained. The LSTM neural network architecture includes a fully connected layer of 128 units and a softmax layer, trained for 5 epochs. It is twice slower than CNN1. The datasets used for text classification include AG's news with four classes and Sogou news in Chinese. The datasets used for text classification include news articles in Chinese, Pinyin phonetic romanization, DBpedia with 14 classes, Yelp full with 5 classes, and Yelp polarity with transformed data for sentiment analysis. The datasets contain varying numbers of train and test samples. The dataset used for sentiment analysis includes Amazon full with 5 classes and Amazon polarity with 2 classes. The train and test samples are distributed equally in both datasets. The baseline models are compared based on accuracy. The paper summarizes different models for sentiment analysis, including Bag of Words (BoW) and Bag-of-ngrams (Ngrams) with their respective TFIDF variations. It also introduces Bag-of-means on word embedding using k-means on word2vec BID20. The paper introduces Bag-of-means on word embedding using k-means on word2vec BID20, with a dimension of 300. The LSTM BID10 model uses pretrained word2vec embedding of size 300, with a feature vector dimension of 512. No preprocessing strategy or data enhancement techniques were employed, and results are compared with traditional models in TAB4. The study compared traditional models with new approaches using graphical representations of results. Accuracy values were scaled to [0, 1] for each dataset, with Figure 3 showing the outcome. A running time comparison was also conducted. The research focused on a coding approach using characters to construct words, demonstrating reduced dimensionality of the encoding matrix and shorter optimization times. The article introduced a new approach for text datasets with unique characteristics not addressed by traditional methods like BoW and word2vec. The new technique aims to reduce training times by optimizing a smaller number of parameters compared to existing architectures. Performance comparisons were made across different datasets to showcase the effectiveness of the approach. The study focused on optimizing text datasets for faster training times by reducing the number of parameters. The use of generators to control matrix generation and GPU utilization was highlighted to manage RAM limitations on personal computers with 8 GB or less. The study demonstrated the feasibility of using a simple network with encoding approach to achieve competitive results. Advantages include faster algorithm for smaller datasets, k-folds validation, and achieving similar or better results than traditional techniques with LSTM layer. Our approach using a LSTM layer with proposed encoding outperforms CNN1 and CNN2 topologies, showing the importance of temporal dependence among words. The dimensionality reduction achieved enables exploration of other architectures and methods. The algorithm implementation with character-level encoding for text classification using Tagged Huffman BID23 compression method is efficient. The encoding process allows for quick updating of weights in GPU, with potential for even faster performance. The implementation of character-level encoding for text classification using Tagged Huffman BID23 compression method is efficient, allowing for quick updating of weights in GPU. This novel text encoding is robust to spelling errors, typos, and slangs, enabling compact representation of texts while reducing training time for CNNs. Experimental studies coupled the encoding with convolutional and LSTM architectures, showing promising results for complex applications and devices with lower computational power. The study focused on the viability of text encoding for text classification, showing promising results with LSTM architectures. Custom neural network architectures may be needed to further improve results by overcoming limitations. Future work will focus on devising new architectures to enhance results. The study focused on text encoding for text classification, showing promising results with LSTM architectures. Custom neural network architectures may be needed to further improve results. This study opens the door to information-theoretic-based methods for creating a numerical representation of texts, which could be beneficial for other classification algorithms."
}