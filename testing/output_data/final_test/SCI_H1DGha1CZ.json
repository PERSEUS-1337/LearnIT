{
    "title": "H1DGha1CZ",
    "content": "In this paper, the authors focus on the relationship between activation functions and batch normalization in training deep networks. They introduce a new activation function called Displaced Rectifier Linear Unit (DReLU) and compare its performance with other functions like ReLU, LReLU, PReLU, and ELU on VGG and Residual Networks using CIFAR-100 and CIFAR-10 datasets. The results show that DReLU improves learning speed and test accuracy significantly. The study introduced a new activation function called DReLU, which outperformed ReLU in test accuracy in all scenarios except one. This demonstrates the potential to enhance performance by replacing ReLU with DReLU. The advancements in deep learning have led to more accurate recognition systems in various fields, but improvements often require deeper or more complex models, increasing computational resources. To enhance deep learning performance, efficient activation functions like ReLU have been crucial. However, a new activation function called DReLU has shown potential to outperform ReLU in test accuracy. This advancement in deep learning aims to improve accuracy without the need for deeper or more complex models, reducing computational resources. Some researchers argue that ReLU has drawbacks, leading to the proposal of alternative activation functions like LReLU, PReLU, and ELU. Despite the debate on their effectiveness compared to ReLU, ReLU remains the most widely used activation function in deep learning. Batch normalization also plays a crucial role in training deep architectures by normalizing inputs of each layer before passing them through activation functions. The ReLU activation function skews normalized distributions by producing only non-negative activations. To address this issue, a new activation function called Displaced Rectifier Linear Unit (DReLU) is proposed, which extends the identity function beyond the origin to minimize damage to the normalization procedure. DReLU is a new activation function that extends the identity function beyond the origin, unlike ReLU. It is essentially a diagonally displaced ReLU in the third quadrant. CNN architectures like VGG and ResNets were used in experiments to evaluate the impact of replacing the activation function on performance. In a systematic comparative study, performance assessments were conducted using statistical tests with a 5% significance level. The input of a layer undergoes a transformation followed by a nonlinearity, such as ReLU, and batch normalization is added. The overall transformation performed by the layer is determined. After applying ReLU nonlinearity and batch normalization to the input layer, the activations distribution is analyzed. ReLU removes negative values, resulting in a positive mean output. By replacing negative activations with zeros, the variance of the distribution decreases. This adjustment is made despite the initial batch normalization. After applying ReLU and batch normalization, the activations are not perfectly normalized, leading to a bias shift effect. A new approach called DReLU is proposed to address this issue by allowing the inflection point to move diagonally. DReLU is a generalization of SReLU, allowing the inflection point to be set at (-\u03b4, -\u03b4) instead of always at (-1, -1). This enables learning for negative inputs and avoids the bias shift effect seen in ReLU and batch normalization. In a comparative study, DReLU outperformed SReLU by adding the parameter \u03b4, extending the identity function into the third quadrant. DReLU exhibits a noise-robust deactivation state for very negative inputs, a feature not found in LReLU and PReLU. DReLU is a activation function that is less computationally complex than other functions like LReLU, PReLU, and ELU. The study conducted 30 experiments across two datasets and three models, totaling 320 executions of training deep neural networks. In some cases, 20 runs were conducted for statistical significance. The study conducted 30 experiments across two datasets and three models, totaling 320 executions of training deep neural networks. The models were trained for 100 epochs, with evaluations at epochs 40 and 70 to assess learning speed based on activation functions. The best test accuracy was observed with a nonlinearity that learned faster, as detailed in Appendix C. No dropout was used in the experiments. Recent studies have shown that dropout, despite improving training time, provides unclear contributions to overall deep model performance. Dropout has become restricted to fully-connected layers, which are being replaced by average pooling layers in modern architectures. As fully connected layers are rarely used in modern CNNs, dropout usage is becoming unusual. Recent CNN models are opting for batch normalization over dropout, with results showing that using only batch normalization, particularly in DenseNets, yields significantly better outcomes. This trend of designing modern deep networks with only batch normalization and avoiding dropout is becoming more prevalent. The comparative study in the paper focuses on the use of batch normalization without dropout in modern CNNs, emphasizing the importance of avoiding misleading comparisons with experiments using different setups. Results from studies using activation functions like LReLU, PReLU, and ELU with dropout should not be directly compared to the findings presented in this study. The mentioned studies did not use batch normalization and dropout together, unlike modern CNNs. They also did not use standardized models like VGG or ResNet, and lacked statistical tests for their results. This raises concerns about the reliability of their conclusions. The paper provides a systematic statistical comparative study on activation functions in the context of using batch normalization without dropout. It analyzes learning speed and test accuracy performance of activation functions in standardized models after 100 epochs. The study analyzed the test accuracy performance of different activation functions in standardized models after 100 epochs. The null hypotheses were rejected, indicating that at least one activation function had a different test accuracy performance. Conover-Iman post-hoc tests were used for pairwise comparisons, with the best results highlighted in bold in the tables. DReLU outperformed other activation functions in reducing bias shift effect during training, producing faster learning and higher test accuracy in ResNet models. It surpassed ReLU in test accuracy and showed no statistical significance compared to LReLU. In CIFAR-100, DReLU demonstrated faster learning and higher test accuracy compared to ReLU and other activation functions. It consistently outperformed ReLU in test accuracy across all models evaluated. Additionally, in the CIFAR-10 dataset, DReLU was more effective in reducing bias shift during training compared to ReLU, as shown in the averaged nonlinearities layers mean activations. In VGG-19 and ResNet models, DReLU outperformed ReLU in terms of faster learning and higher test accuracy across multiple epochs. DReLU consistently showed superior performance in various scenarios, surpassing ReLU in both learning speed and test accuracy. The results indicated that DReLU outperformed ReLU in test accuracy across all scenarios and models analyzed. The study emphasized the use of different models in previous research, making direct comparisons challenging. Additionally, statistical tests were conducted to support the findings of this study. In experiments with CIFAR-100 and CIFAR-10 datasets, DReLU consistently showed faster learning and higher accuracy compared to ReLU across all models tested. DReLU outperformed ReLU in all scenarios, showing the highest accuracy in evaluated models. Batch normalization played a key role in preventing the \"dying ReLU\" problem, benefiting ReLU over other activation functions like LReLU, PReLU, and ELU. DReLU, a novel activation function for deep learning architectures, showed better learning speed and accuracy than ReLU and other alternative activation functions in all models and datasets. It significantly enhanced training speed during the first decades and outperformed test accuracy results of all investigated activation functions. DReLU outperformed other activation functions in test accuracy across various scenarios, including CIFAR-100 and CIFAR-10 datasets with VGG and Residual Networks models. It improved deep learning performance while being computationally less expensive. The gains were achieved by simply replacing the activation function without increasing model complexity. Batch normalization benefitted ReLU, while other proposed activation functions did not perform as well. Batch normalization benefitted ReLU, while other proposed activation functions did not perform as well. Skip connections in evaluated models, like ResNets, suggest results may generalize to other deep architectures. All major activation functions adopt identity transformation for positive inputs and a specific function for negative inputs.ReLU is the standard activation function in deep networks due to its simplicity and high performance. ReLU, or Rectified Linear Unit, is a widely used activation function in deep neural networks due to its simplicity and high performance. It was first introduced to improve the performance of Restricted Boltzmann Machines (RBM) and has since been utilized in various neural network architectures. ReLU has shown superior performance in the supervised training of convolutional neural network models by avoiding the vanishing gradient problem. However, a drawback is that ReLU generates positive mean outputs, leading to the bias shift effect. While the identity for positive inputs is widely accepted, there is no consensus on how to handle negative values. Promising approaches for negative values include LReLU, PReLU, and ELU activation functions. LReLU avoids slope zero for negative inputs, PReLU has a learnable slope for negatives, and ELU is inspired by natural gradient, all aiming to prevent producing only positive mean outputs. ELU activation function allows negative activation for negative inputs, but has higher computational complexity compared to ReLU, LReLU, and DReLU. Normalizing input data distributions improves training time and test accuracy in machine learning. Deep architectures require more sophisticated normalization techniques due to the cascading effect of layer outputs becoming inputs for the next layer. Each layer of a deep model has its own input data composed of the previous layer output. The stochastic gradient descent optimizes the parameters of the model. The outputs of each layer are fed into the next layer, producing an overall loss function. The transformation produced by each layer is combined with the loss function to map the data. The output of each layer in a deep neural network serves as the input data for the next layer. Training each layer involves updating parameters using stochastic gradient descent. Normalizing only the actual input data has limited impact on learning speed and accuracy, as the distribution of input data for each layer changes during training. Batch normalization is a method to address internal covariant shift in deep neural networks. It normalizes the inputs of each layer after mini-batch training, improving training speed and accuracy by synchronizing with parameter updates. The experiments were conducted on a machine with specific hardware and software configurations. Models were trained on CIFAR-100 and CIFAR-10 datasets, which contain different numbers of classes and images for training and testing. Regular mean-standard preprocessing was used in the experiments. The experiments involved training models on CIFAR-100 and CIFAR-10 datasets with regular mean-standard preprocessing. Features were normalized to zero mean and unit standard deviation. Data augmentation included horizontal flips, random crops, and image expansion. Activation function parameters from BID6 were used, with \u03b2 = 0.25 for LReLU and PReLU, and \u03b1 = 1.0 for ELU. We maintained \u03b2 = 0.25 for LReLU and PReLU, and \u03b1 = 1.0 for ELU, based on previous research. VGG with nineteen layers was used, along with ResNet-56 and ResNet-110. Kaiming initialization was employed, with an initial learning rate of 0.1 and decay of 0.2 at epochs 60, 80, and 90. The experiments used mini-batches of size 128 with stochastic gradient descent and Nesterov acceleration. Kruskal-Wallis tests were employed to assess activation functions' performance, considering different weight initializations and comparing five activation functions simultaneously. The study compared activation functions using Kruskal-Wallis tests and Conover-Iman post-hoc tests for pairwise comparisons. A compromise between normalization and nonlinearity was sought for the hyperparameter value of DReLU, with \u03b4 set to 0.05 based on experimental results. The performance of SReLU was notably lower than DReLU with \u03b4 = 0.05."
}