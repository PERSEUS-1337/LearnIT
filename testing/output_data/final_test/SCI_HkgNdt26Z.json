{
    "title": "HkgNdt26Z",
    "content": "One challenge in machine learning is the discrepancy between training data and real-world data. Language modeling faces the issue of users' language evolving over time. A novel technique for fine-tuning a general model on user private data improves prediction quality and communication efficiency. This approach reduces perplexity by almost 70% and increases keystroke saving rate by 8.7% in informal English. The experimental framework proposed evaluates the privacy of distributed training of language models, showing good privacy guarantees. Challenges arise post-deployment of machine learning models on user devices, such as data discrepancy and the need for regular updates. This poses a challenge for word prediction algorithms in software keyboard applications. The approach to improving customer experience in typing involves integrating a separate user language model trained on the device in an online fashion. A continuously learned personalized language model based on LSTM was proposed, but collecting data for training may be avoided by using a distributed fine-tuning approach on private data. This method, termed \"federated fine-tuning,\" starts with a language model trained on a large text corpus representing the general language. The text discusses the development of a distributed fine-tuning algorithm for language models to prevent overfitting on user data and ensure the model retains \"general English\" knowledge. Privacy analysis is crucial due to potential information leakage in distributed training algorithms. The proposed procedure aims to prevent catastrophic forgetting in language models. The research focuses on distributed fine-tuning of language models to prevent catastrophic forgetting. Experimental evaluation includes on-device training time, communication costs, and convergence rates. Two strategies for improving communication efficiency are compared, and an experimental framework for evaluating differential privacy is proposed. LSTM architecture is used for the language model, with a focus on improving keystroke saving rate for better customer typing experience. On-device training time is shown to be feasible. Our approach involves training an initial language model on standard English and continuously updating it on the user device to prevent overfitting. The goal is to provide a sorted list of input candidates for a software keyboard application based on a language model with low perplexity. The model should not overfit on texts written on the device, and the user can switch to general English if needed. The approach involves training a language model on standard English and updating it on the user device to prevent overfitting. The model is continuously fine-tuned on the device, and updates are sent back to the server for further model improvement. The goal is to provide a sorted list of input candidates for a software keyboard application with low perplexity, ensuring the model does not overfit on device texts. The goal is to update parameters with new dataset Ts*, while avoiding sending user data to the server. LwF involves fine-tuning a general language model on the user device, minimizing loss by minimizing Kullback-Leibler divergence. Minimizing loss in FORMULA1 - FORMULA2 is achieved by minimizing Kullback-Leibler divergence with respect to parameters of P u. Random rehearsal involves adding data from Tr to Tr* to improve efficiency in fine-tuning without the need for soft labels. The server-side solution aggregates models from users to update the general model efficiently. Transfer learning is used as an alternative approach, optimizing cross-entropy function with predictions from aggregated models. Keystroke saving rate is defined as a decrease in characters user has to type with software keyboard suggestions. In experiments, Keystroke Saving Rate (KSS) is shown to be better for customer experience assessment compared to perplexity, which underestimates out-of-vocabulary words. Removing OOV words from the test set is necessary for perplexity measurement. Direct comparison of models with different vocabularies is impractical due to this limitation. A small decrease in perplexity may not necessarily lead to KSS improvement. The goal of the experiments was to find an efficient pipeline for distributed fine-tuning of language models, comparing client-side and server-side model update approaches. In experiments, different approaches for client-side and server-side model updates were compared using Twitter and Wikipedia corpora. The models were trained on datasets with varying token counts and tested on subsets of the corpora. LSTM architecture with specific parameters was used for the experiments. The LSTM model with specific parameters was trained on a vocabulary size of 30k, dropout 0.5, minibatch size 20, and BPTT steps 35. The initial general English model was trained in 39 epochs and achieved a perplexity of 100.1 and 67.9% KSS rate on the standard English test set, while on the user data test set, it had a perplexity of 336.0 and 49.7% KSS rate, showing an 18.2% drop in performance. Experiment results with on-device model update algorithms showed that the performance gap between the two test sets can be reduced at the cost of performance degradation on the first dataset, with the best average perplexity achieved using the random rehearsal method and \u03bb = 0.5. The LwF method showed inferior performance due to poor approximation of true word distribution. An experiment with model averaging and transfer learning for server-side update was conducted on a mobile phone with specific parameters. Training took 140 seconds on 20 kilobytes of text using a quad-core mobile CPU. Computation time may be reduced by using a mobile GPU. Transfer learning and model averaging were tested for server-side model update on a mobile phone. Training was done using texts generated by the model's previous update. Results showed no significant difference between transfer learning on real and generated data, with model averaging being more computationally efficient. After 300 rounds of model updates with 3000 nodes, there was an 8.7 absolute gain in KSS on user data test with only a 0.6 absolute KSS drop on standard English data test. The model performed well after 100 rounds, highlighting the importance of rehearsal to prevent catastrophic forgetting. Strategies for efficient distributed learning communication include increasing computation on nodes and transmitting only some data to the server in a single round. Deep Gradient Compression achieved impressive results by transmitting only a part of data to the server. The Deep Gradient Compression method (DGC) sends only important weight updates during on-device training, accumulating the rest to send later. It allows for sending a small portion of updates without loss in model quality. DGC achieved a gradient compression ratio of 462x for language modeling. In experiments, models were trained on devices for one epoch instead of using DGC-style strategy, resulting in 1.7Gb of data. In experiments, models were trained on devices for one epoch, resulting in 1.7Gb of data transmitted to the server. A 2-layer LSTM model was used, with potential for improvement using models with fewer parameters. The competitiveness of the approach was proven by comparing communication efficiency strategies in a language modeling benchmark. The model was trained for 28 rounds using a specific strategy to improve model averaging performance. Training started on the first node and progressed sequentially to the fourth node before updating all nodes. The number of training epochs and learning rate decreased over rounds. The strategy achieved better perplexity with data sent from nodes to the server. The strategy achieved better communication efficiency with fewer communications compared to DGC. Increasing computation on nodes performs better in terms of communication efficiency. Data on a device is used only once in our approach, unlike DGC where data on each device is used multiple times. The two classes of strategies for improving communication efficiency are not mutually exclusive. The curr_chunk discusses differential privacy analysis based on experimental evaluation. Differential privacy measures the impact of individual input data on total output in mechanisms. Randomness is introduced to achieve differential privacy. The curr_chunk discusses the analysis of differential privacy in randomized mechanisms generating texts based on a language model trained on user data subsets. The focus is on ensuring privacy by considering sets of texts containing 10 words, as this length is deemed sufficient for an adversary to extract important information. Random variables are introduced to measure the impact of individual texts on the output. The curr_chunk introduces a random variable c(s) defined for any sequence of words in a language model. Parameter \u03b4 represents the probability of differences between probabilities P(s|\u03b8) and P(s|\u03b8 ). The tail of the distribution of c(s) is crucial for estimating privacy parameters, with empirical evidence suggesting a Pareto distribution due to Zipf's law in human language. Twenty pairs of adjacent user sets are considered for confident estimation of privacy parameters. The random variable c(s) is analyzed for privacy parameters using 20 pairs of adjacent user sets. The Lilliefors test is applied to a composite null hypothesis with a Pareto distribution assumption. The estimation of parameters \u03b5 and \u03b4 is based on quantiles of the Pareto distribution. The Lilliefors test fails to reject the null hypothesis in 19 out of 20 cases, indicating the validity of the model. The random variable c(s) has tails that decrease like the Pareto distribution with a big shape parameter. KS statistics and Hill's estimators are provided for different user pairs in Table 4. Results for different values of \u03b4 are shown in Table 6, with \u03b5 being the largest value. Choosing \u03b4 = 10^-4 gives \u03b5 = 0.67, offering reasonable privacy guarantees. Smaller values of \u03b4 also provide good privacy protection. The aim is to provide an empirical estimation of differential privacy. Our approach aims to provide an empirical estimation of differential privacy with high probability, similar to random differential privacy. In machine learning algorithms, noise is added to ensure differential privacy, but our mechanism uses randomness within the neural network. This limits us to only providing empirical estimations of privacy parameters. Our focus is on distributed fine-tuning of neural language models to prevent catastrophic forgetting. Our experiments showed that distributed fine-tuning of neural language models on user devices can significantly improve performance without degrading standard English training data. On-device training with random rehearsal and server-side model averaging yielded the best results, reducing communication costs and training time to less than 3 minutes. An experimental evaluation of differential privacy showed a reasonable level compared to other solutions, with an empirical estimation that holds with high probability. The Lilliefors test does not converge to the Kolmogorov distribution, instead converging to a distribution with smaller critical values due to overfitting on sample data. Results show that the random variable c(s) has Pareto distribution tails, supporting the hypothesis with a 5% significance level. The Lilliefors test does not converge to the Kolmogorov distribution, instead converging to a distribution with smaller critical values due to overfitting on sample data. Results show that the random variable c(s) has Pareto distribution tails, supporting the hypothesis with a 5% significance level. For all x > x 0, F(x 0) is just the ratio k/n, and C can be estimated using values from TAB4. (\u03b5, \u03b4)-differential privacy is provided by values \u03b5, \u03b4 that satisfy a certain formula."
}