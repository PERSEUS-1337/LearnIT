{
    "title": "Syxt5oC5YQ",
    "content": "Momentum is a widely used optimization trick that relies on a damping coefficient for performance. Aggregated Momentum (AggMo) is a variant that combines multiple velocity vectors with different damping coefficients to reduce oscillations and instability. It can handle aggressive damping coefficients like 0.999 while maintaining stability. AggMo can outperform other momentum methods in terms of convergence speed without requiring much tuning. Momentum methods, like classical momentum, use a damping coefficient to control the decay of the momentum vector. The choice of the damping coefficient, denoted as \u03b2, affects the trade-off between speed and stability in optimization. A \u03b2 slightly less than 1 can improve performance by allowing the optimizer to pick up speed along low curvature directions. However, large \u03b2 values can lead to oscillations and instability, requiring a smaller learning rate and slower convergence. Finding a balance to dampen oscillations while preserving speed is crucial in optimization. Aggregated Momentum (AggMo) is introduced as a variant of classical momentum in optimization. It maintains multiple velocity vectors with different \u03b2 parameters, combining the advantages of small and large \u03b2 values. This approach aims to dampen oscillations while preserving high terminal velocity, potentially speeding up optimization processes. AggMo combines small and large \u03b2 values to build velocity and dampen oscillations, stabilizing the algorithm. Inspired by passive damping in physics, it prevents resonance by using multiple momentum velocities. This method prevents oscillations and improves convergence rates on quadratic functions. In this paper, the authors analyze convergence rates on quadratic functions and provide theoretical convergence analysis for AggMo in online convex programming. Empirical evaluations show that AggMo can outperform classical momentum and Nesterov momentum in deep learning architectures like deep autoencoders, convolutional networks, and LSTM. AggMo works as a drop-in replacement for classical momentum but offers faster convergence at higher \u03b2 values. Classical momentum minimizes a function iteratively with a learning rate schedule and a damping coefficient. Nesterov momentum is a modified version that improves convergence and stability by correcting errors after moving in the direction of velocity. Nesterov momentum adapts to curvature by rescaling damping coefficients with eigenvalues of a quadratic function. Convergence on quadratic functions is studied as a test case for optimization behavior near a local minimum. Analysis of optimizers along eigenvectors of a quadratic function shows the impact of damping coefficients on finding optimal solutions. Increasing damping coefficient can cause oscillations that prevent convergence. In practice, critical damping is sought when using CM. Aggregated Momentum (AggMo) is a variant of gradient descent that aims to improve stability and convergence benefits by using multiple velocity vectors with their own damping coefficients. These velocities are updated and averaged at each optimization step to produce the final velocity for parameter updates. AggMo optimizes well over illconditioned curvature by using multiple damping coefficients. It converges faster than CM and Nesterov, dampening oscillations quickly for all eigenvalues. The velocities align towards the minima at point (1) and begin to oscillate at point (2), with different damping velocities contributing to the updates. AggMo achieves fast convergence by combining velocities and damping the system. The damping vector is chosen exponentially to reduce oscillations, with a default choice of K = 3 corresponding to \u03b2 = [0, 0.9, 0.99]. This setting is stable and effective in experiments, with minimal computational overhead compared to CM. AggMo achieves fast convergence by combining velocities and damping the system with a default choice of K = 3. The memory overhead for storing velocity vectors is small compared to activations in modern deep learning applications. By introducing separate learning rates for each velocity, Nesterov Momentum can be recovered using a simple generalization of Aggregated Momentum. The AggMo update rule can be written as DISPLAYFORM2. Similarly, the Nesterov momentum update with constant learning rate \u03b3 t = \u03b3 can be expressed as DISPLAYFORM3. When using the reparameterization \u03c6 t = \u03b8 t + \u03b3\u03b2v t, the update to \u03c6 from Nesterov is identical to the AggMo update to \u03b8, with \u03c6 0 = \u03b8 0. This allows us to reinterpret Nesterov momentum as a weighted average of a gradient update and a momentum update, leading to theoretical convergence results similar to Nesterov momentum. The convergence behavior of momentum optimizers on quadratic functions with fixed hyperparameters is analyzed. The convergence rate is determined by the eigenvalues of a linear dynamical system model for each optimizer. The convergence rate for quadratics with condition numbers ranging from 10^1 to 10^7 is displayed in FIG2, showing the optimal convergence rate with the blue dashed line. AggMo's convergence rate interpolates smoothly between CM with \u03b2 = 0.9 and \u03b2 = 0.99 as the condition number varies. It achieves approximately three-times faster convergence than Nesterov momentum in the under-damped regime without sacrificing performance on larger condition numbers. The evaluation of AggMo's convergence rate is done in the setting of online convex programming as proposed in BID37. AggMo's convergence rate in online convex programming, equivalent to stochastic convex optimization, is analyzed. The algorithm aims to predict the parameter minimizing regret, with a regret bounded by O( \u221a T ). The method shows comparable results to the best-known bound and adopts definitions from previous work to simplify notation. AggMo achieves a regret bound of O( \u221a T ) in online convex programming. The average regret converges as R(T )/T \u2192 0. The convergence of momentum methods has been extensively studied, with AggMo showing non-trivial results compared to Adam. BID29 explored the effect of momentum on neural network optimization, introducing the momentum view of Nesterov's accelerated gradient. They focused on creating good momentum schedules to adapt to ill-conditioned curvature. Despite evidence of its effectiveness, practitioners still often use fixed momentum schedules. AggMo evolves as a (K+1)-th order finite difference equation, allowing for greater expressiveness over the gradient history. BID14 introduced dependence on a larger gradient history with lagged momentum terms, but this introduces more hyperparameters to tune. Adaptive gradient methods have been developed to address ill-conditioned curvature in deep learning. Several adaptive methods in deep learning aim to adapt to the geometry of the data and detect oscillations during optimization. Natural gradient descent preconditions by the Fisher information matrix, approximating the Hessian. Methods have been proposed to reduce computational costs, while others focus on detecting and removing oscillations during optimization. The AggMo optimizer, with its passive damping approach, addresses oscillation issues without needing to detect them. It was evaluated on various deep learning architectures using datasets like MNIST, CIFAR-10, CIFAR-100, and Penn Treebank. Comparisons were made with classical momentum, Nesterov momentum, and Adam, which are widely used optimizers in deep learning tasks. The experimental setup involved performing a grid search over learning rate and damping coefficient for different optimization methods. Generalization results were included to show the impact of optimizer choice on network performance. Fully-connected autoencoders were trained on the MNIST dataset with a focus on momentum and learning rate decay schedules. Damping coefficients were evaluated for CM and Nesterov optimizers. The study involved evaluating damping coefficients for different optimization methods, including CM, Nesterov, Adam, and AggMo. Each model was trained for 1000 epochs, and results showed that AggMo outperformed Adam on validation/test sets. Optimal damping coefficients were found to be \u03b2 = 0.99 for CM and Nesterov, and \u03b2 = [0.0, 0.9, 0.99, 0.999] for AggMo. During experiments, AggMo showed stability with larger learning rates compared to CM and Nesterov. Increasing the maximum damping coefficient of AggMo improved performance, especially with a coefficient of 0.999. AggMo outperformed Nesterov with a damping coefficient of 0.9999. AggMo, with a damping coefficient of 0.999, achieved the fastest convergence. It was evaluated using CNN-5 and ResNet-32 architectures with data augmentation and regularization. Models were trained for 400 epochs. Results showed AggMo outperformed other methods on CIFAR-10 and CIFAR-100 datasets. AggMo significantly outperformed other methods, showing the best validation accuracy with CM for ResNet-32 experiments. AggMo had the fastest convergence and using default hyperparameters (a = 0.1, K = 3) led to faster convergence compared to CM and Nesterov. Training loss and validation accuracy for each optimizer on ResNet-32 are shown in FIG5. AggMo converged quickly without sacrificing final validation performance and was able to train ResNet-32 on CIFAR-100 without regularization. Using AggMo, ResNet-32 was trained on CIFAR-100 without batch normalization, achieving 69.32% test error compared to 67.26% with CM. Optimization with AggMo remained stable at larger learning rates than with CM even without batch normalization. Outperforming CM, AggMo and Nesterov optimizers were able to achieve better results without additional tuning of hyperparameters. LSTM Language Models were trained on the Penn Treebank dataset following the experimental setup of BID19. The authors used optimal hyperparameter settings and varied learning rate, momentum, and gradient clipping. They trained models for 750 epochs and found that momentum-based optimizers outperformed SGD without momentum in language modeling tasks. Adam performed surprisingly well in their experiments. In language modeling tasks, Adam and AggMo outperformed SGD without momentum. Heavy regularization during training makes Adam a good choice. Results for hyperparameter settings are in Table 3, with different optimization schemes compared in Figure 7. Momentum methods converge faster than momentum-free methods. SGD worked best without learning rate decay. AggMo outperformed SGD without momentum in language modeling tasks. Adam converged quickly and achieved a validation perplexity comparable to AggMo. Gradient clipping is critical for SGD without momentum, but all momentum methods perform better without it. Using other momentum methods may significantly improve convergence rates and final performance. AggMo worked well over a range of damping coefficients and learning rates, showing faster convergence rates and stability even with large damping coefficients. AggMo, a special case of Nesterov momentum, demonstrated faster convergence rates and stability with large damping coefficients. It could be used as a drop-in replacement for existing optimizers without additional tuning. In comparison to classical and Nesterov momentum, AggMo showed substantially faster convergence rates at higher \u03b2 values. Two toy problems were used to demonstrate this equivalence, with both optimizers following similar paths. AggMo, a variant of Nesterov momentum, displayed faster convergence rates and stability with large damping coefficients. It could be seamlessly integrated as a replacement for existing optimizers without the need for additional adjustments. Two toy problems were utilized to showcase this equivalence, with both optimizers tracing similar paths. The optimization trajectories of AggMo and another algorithm were compared, showing initial similarities but diverging as they approached the origin. Convergence rate computations and additional results were presented, highlighting the effectiveness of AggMo in optimizing quadratic functions. The spectral norm of matrix B determines the convergence rate of a linear dynamical system. In the special case of K = 1, the system exhibits over-damped or under-damped behavior. The critical damping coefficient yields optimal convergence. This analysis can be extended to Nesterov momentum for similar convergence bounds. In the study, the authors analyze the convergence bounds for Nesterov momentum by computing eigenvalues from matrix B for matrices A with varying condition numbers. They perform a grid search to approximate the optimal learning rate for AggMo. The study also illustrates how using multiple velocities can prevent oscillations during optimization, as shown in FIG0 for CM, Nesterov, and AggMo. The study analyzes convergence bounds for Nesterov momentum and explores the use of multiple velocities to prevent oscillations during optimization. The proof of Theorem 5.1 introduces notation and lemma for gradient descent algorithms. The proof involves applying various inequalities such as Cauchy-Schwarz and Young's inequality to bound the expressions. The update equations are manipulated to focus on the j th dimension, leading to a telescoping sum and the use of convexity. The final inequality is derived from the harmonic sum bound. In studying the convergence properties of AggMo, interesting observations were made regarding theoretical challenges and key differences from existing momentum methods. By reducing the matrix B to block diagonal form, a closed-form solution for eigenvalues can potentially be derived to analyze the quadratic convergence properties of AggMo. The goal is to find conditions for complex eigenvalues indicating under-damped systems. The dynamics of AggMo can be represented as a (K + 1)-th order finite difference equation, distinct from other momentum methods. Convergence proof techniques are challenging to apply to AggMo due to this uniqueness. Assuming a fixed learning rate \u03b3, the special case K = 2 is tackled, resulting in a finite difference equation. For K \u2265 2, adjustments to the equations lead to a (K + 1)-th order difference equation. The dynamics of AggMo can be represented as a (K + 1)-th order finite difference equation, distinct from other momentum methods. Existing momentum methods can generally be rewritten as a second order difference equation, inducing a second order ODE. AggMo does not easily fit into analytical tools developed for momentum methods. Experiments are conducted using the pytorch library, with early stopping used for determining the best validation performance in autoencoder training. In training autoencoders, fully connected networks with specific architecture are used, along with relu activations. The training process involves 1000 epochs with a multiplicative learning rate decay at specific intervals. Different optimizers are tested with varying learning rates. For classification tasks, training is done over 400 epochs with a similar learning rate decay approach. The CNN-5 model is trained with various learning rates and specific architecture details. It includes relu activations, max pooling layers, and convolutional kernels of different sizes. The ResNet-32 architecture is used for CIFAR-10 and CIFAR-100 datasets with a weight decay of 0.0005. The model is trained with weight decay of 0.0005, batch normalization, data augmentation, and dropout on LSTM layers. L2 regularization and weight drop method are applied, along with variable sequence lengths and batch sizes of 80. The model is trained with weight decay of 0.0005, batch normalization, data augmentation, and dropout on LSTM layers with variable sequence lengths and batch sizes of 80. Validation loss is monitored during training, and the learning rate is decreased if the loss does not improve for 15 epochs. Different optimizers require different learning rate decay strategies, with Adam needing smaller rates. Experimental results and the effectiveness of AggMo in non-convex settings are explored. The effectiveness of AggMo in non-convex settings is explored by comparing its performance with classical momentum on a non-convex toy problem. The function to optimize features flat regions and non-convex funnels with varied curvature, with an optimal value at (0, 0). AggMo outperforms classical momentum by quickly traversing both flat regions and funnels while remaining stable. AggMo outperforms CM and Nesterov with default settings, showing less sensitivity to hyperparameter tuning. For LSTM experiments, all methods performed best with default damping coefficients except for Nesterov momentum with \u03b2 = 0.99. AggMo achieved better perplexity results in training, validation, and test compared to other methods. The AggMo update rule, a continuous analog of AggMo, integrates velocities over a space instead of summing them. The update rule involves a mapping from beta values to velocities, approximating Equation 14 via Monte Carlo Integration. The ability to compute this integral in closed form is uncertain. The update rule for x t can be understood by expanding v t recursively. For the special case where \u03c0 is the density function of a Beta distribution, closed form solutions for the raw moments of b \u223c Beta(\u03b1, \u03b2) exist. This allows for a closed form solution to compute \u03b8 t given \u03b8 t\u22121 and the history of all previous gradients, known as Beta-Averaged Momentum. Each update requires the history of all previous gradients to be computed, but approximations can be made such as keeping only the T most recently computed gradients. The optimization of 1D quadratics using Beta-Averaged Momentum involves trajectories similar to those achieved with the original AggMo formulation, by keeping only the T most recently computed gradients."
}