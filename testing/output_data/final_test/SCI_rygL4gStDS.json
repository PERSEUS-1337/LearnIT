{
    "title": "rygL4gStDS",
    "content": "In this paper, the focus is on deep diagonal circulant neural networks, where weight matrices are a combination of diagonal and circulant ones. The study includes a theoretical analysis of their expressivity and introduces techniques for training these models effectively, such as an initialization scheme and a strategic use of non-linear functions. Deep diagonal circulant networks outperform other structured deep networks, requiring fewer weights for better accuracy. These models are trained on a real-world video classification dataset with millions of examples, aiming to create compact and accurate neural networks with practical applications in embedded systems and distributed learning. Structured matrices are essential in compact networks, replacing dense weight matrices with structured ones like low rank, Toeplitz, and circulant matrices. Despite efforts, compact models still struggle to achieve acceptable accuracy, raising questions about their effectiveness in real-world scenarios. In this paper, the effectiveness of deep neural networks with structured layers, specifically diagonal and circulant matrices, is investigated. The use of these matrices is inspired by linear algebra results and the AFDF structured layer design. The AFDF structured layer design was inspired by decomposition but was not initially successful in training deep neural networks. A theoretically sound initialization procedure for DCNNs was developed to address this issue, allowing signals to propagate effectively. Empirical insights were provided to explain DCNN behavior, showing the impact of non-linearities on convergence rate and accuracy. By combining these insights, large and deep DCNNs were successfully trained for the first time, demonstrating good performance on the YouTube-8M video classification problem. Additionally, an analysis of DCNN expressivity was proposed, introducing a new bound on the number of diagonal-circulant matrices needed for approximation. The paper introduces a new bound on the number of diagonal-circulant matrices needed to approximate a matrix based on its rank. It shows that a DCNN with bounded width and small depth can approximate dense networks with ReLU activations. The paper also discusses related work on structured neural networks, circulant matrices, theoretical analysis on DCNN expressivity, efficient training techniques, and experimental comparisons with other approaches. Structured matrices have properties beneficial for deep learning. Structured matrices are utilized in deep learning to compress large neural network architectures. Circulant matrices can approximate the Johnson-Lindenstrauss transform for dimensionality reduction. Cheng et al. (2015) proposed replacing weight matrices with circulant matrices, resulting in networks with good accuracy at a fraction of the original size. Comparison with ACDC's Structured Efficient Linear Layers (SELL) AFDF and ACDC is discussed. The AFDF structured layer, based on theoretical results by Huhtanen & Per\u00e4m\u00e4ki, serves as a building block for DCNNs. However, the ACDC layer, which does not involve circulant matrices, does not benefit from the same theoretical guarantees. The ACDC paper only trains large neural networks with ACDC layers, making it challenging to assess the true impact of these layers. In comparison, Thomas et al. (2018) have introduced neural networks with low-displacement rank matrices (LDR) as an alternative approach. The LDR framework, representing structured matrices using displacement operators and low-rank residuals, encompasses various matrix types like Toeplitz-like and Vandermonde-like matrices. However, it faces limitations in training complexity, parameter efficiency, and practical use in deep neural networks. Other compression techniques have also been proposed in addition to structured matrices. Circulant networks offer good performance in various contexts due to their compact representation using only n coefficients. This simplifies matrix-vector products from O(n^2) to O(n log(n)) using the convolution theorem. Circulant matrices have strong expressive power, allowing for efficient representation of linear transforms. They can be combined with diagonal matrices to represent any linear transform with arbitrary precision. The number of factors needed to approximate any matrix A can be bounded. The relationship between diagonal circulant matrices and low rank matrices is explored. Theorem 2 enhances the result by Huhtanen & Per\u00e4m\u00e4ki by expressing the number of factors required to approximate a matrix A as a function of its rank. This is particularly useful for dealing with low-rank matrices common in machine learning problems. Theorem 2 states that a sequence of structured matrices can represent a large class of matrices with fewer than 2n diagonal-circulant matrices. This result is useful for analyzing the expressivity of neural networks based on diagonal and circulant matrices. Zhao et al. (2017) showed that circulant networks with 2 layers and unbounded width are universal approximators, but questions remain about bounded-width circulant networks and their ability to approximate any function. In this section, the authors address the questions of whether any function can be approximated with a bounded-width circulant network and what functions can be approximated with a circulant network of bounded width and small depth. They introduce necessary definitions regarding neural networks and provide a theoretical analysis of their approximation capabilities. The authors show that bounded-width DCNNs can approximate any Deep ReLU Network, making them universal approximators. The authors demonstrate that bounded-width DCNNs can approximate any Deep ReLU Network, establishing them as universal approximators. Additionally, they introduce a theorem that examines the approximation properties of small depth networks with ReLU activation functions. The theorem shows that a DCNN of bounded width and small depth can approximate a Deep ReLU network of low total rank. This result refines Lemma 1 and highlights the expressivity of DCNNs in representing functions with specific properties. The properties of deep diagonal-circulant networks (DCNN) are illustrated in Figure 1, showing that DCNN are more expressive than networks with low total rank. These networks can efficiently approximate standard deep neural networks with most singular values close to zero. Additionally, neural networks can be trained to have low-rank weight matrices. DCNNs can be trained to have low-rank weight matrices, allowing for compact and accurate networks. Two techniques are proposed to aid in training deep DCNNs: an initialization procedure to prevent signal vanishing/exploding and studying different non-linearity functions for optimal parameters. The initialization scheme involves randomly drawing values for circulant matrices and biases. The covariance matrix in a DCNN is constant regardless of depth. Empirical findings suggest that reducing non-linearities in networks simplifies training. Experiments show replacing some ReLU activations with the identity function aids in training deep neural networks. In experiments, ReLU activations were replaced with Leaky-ReLU activations with varying slopes. Results showed that reducing non-linearity can help train deeper networks without facing difficulties. Maximum accuracy of 0.56 was achieved with one ReLU every three layers and leaky-ReLUs with a slope of 0.5. This setting was relied upon in the experimental section. The experimental section compares DCNNs with neural networks using ACDC layers. Linear networks were used for comparison, showing good performance with DCNNs having a better convergence rate. ReLU activations were then compared on CIFAR-10 dataset, with results presented in Figure 3. The experiment involved regression on a linear problem with X, Y, and W drawn from specific distributions. Networks based solely on ACDC layers were challenging to train and had low accuracy on CIFAR. Adding a single dense layer improved the convergence rate of ACDC in linear networks. The true contribution of ACDC layers in networks with many other layers remains difficult to quantify. Deep DCNNs can model complex relations at a low cost and offer good performance without additional dense layers. They are compared with Dense networks, Toeplitz networks, and Low Rank networks for accuracy on the CIFAR-10 dataset. Toeplitz networks consist of stacked Toeplitz matrices interleaved with ReLU activations. In experiments, different network structures like DCNNs, Toeplitz networks, and Low Rank networks were compared for accuracy on CIFAR-10 dataset. Various network depths and matrix ranks were tested, with a focus on the number of weights used in each network. Deep low-rank networks were found to be difficult to train. The size of networks correlates positively with accuracy, demonstrating successful training. DCNNs achieve 56% accuracy with 20 layers, comparable to dense networks with significantly fewer weights. Comparison with LDR networks shows DCNNs outperforming in accuracy. In comparison to LDR networks, DCNNs outperform in accuracy and size, especially when exploiting image features like translation invariance. By incorporating fixed transforms like the scattering transform, the accuracy of general purpose architectures in image classification can be improved without significantly increasing trained parameters. Our test architecture involves 2 depth scattering on RGB images with batch norm and LDR or DC layer. Increasing the rank of the matrix varies the number of parameters in the Scattering+LDR architecture. Results show that DCNN benefits from scattering transform, reaching over 78% accuracy. Scattering followed by DC layer outperforms other configurations with fewer parameters. Comparison with HashNet, Dark Knowledge, and Fast Food Transform is provided in Table 3. The test error of DCNN on MNIST datasets outperforms HashNet and Dark Knowledge with fewer parameters. The architecture with Fast Food achieves better performance with convolutional layers and 1 Fast Food Layer. Experiments on aggregated dataset show DCNN compared to a dense baseline with 5.7 million weights. Results on YouTube-8M dataset show performance in terms of number of weights, compression rate, and GAP. The paper discusses the training of diagonal circulant neural networks, which offer high compression ratios but may slightly decrease the GAP measure. The 32-layer DCNN is significantly smaller than the original model in terms of parameters while maintaining close performance. The study shows that stacking diagonal circulant layers with nonlinearities improves convergence rate and final accuracy, outperforming other structured alternatives. Theoretical guarantees are provided, enriching previous literature on the topic. The future directions of this work include proving that linearities improve convergence rate and final accuracy of the network. Generalizing good results of DCNNs to convolutional neural networks and focusing on circulant matrices in deep learning due to their ties with convolutions are also key areas of interest. The study aims to extend results to deep convolutional neural networks. The rectified linear unit on the complex domain is defined by ReLU (z) = max (0, R(z)) + i max (0, I(z)). Define the cyclic shift matrix S 2 R n\u21e5n as follows. The depth and width of the network are denoted by L and n respectively. Total rank k is the sum of the ranks of the matrices. Theorem 1 states that for any given matrix A 2 C n\u21e5n, there exists a sequence of matrices B 1 . . . B 2n 1 where B i is a circulant matrix if i is odd, and a diagonal matrix otherwise. Theorem 2 presents a rank-based circulant decomposition for a matrix A of rank at most k. It states that for any \u03b5 > 0, there exists a sequence of 4k + 1 matrices B1, ..., B4k+1, where Bi is a circulant matrix if i is odd, and a diagonal matrix otherwise, such that kB1B2...B4k+1Ak < \u03b5. The proof involves decomposing the SVD of matrix M into circulant and diagonal matrices. The curr_chunk discusses decomposing matrix U into a product of circulant and diagonal matrices, and then applying the same technique to matrix V. The goal is to obtain a product of 4k + 2 matrices alternating between diagonal and circulant. The values of d ij are unknown initially but will be computed. The process involves solving a linear equation system to find the values of d ij such that the product of matrices equals U. The curr_chunk discusses the block-diagonal matrix structure when k = 2, showing how each block is a permutation of the identity matrix, making the entire matrix invertible. By solving linear equations, the values of d 1,1 . . . d k,n are found. This technique is then applied to factorize matrix V as well. Ultimately, matrix A can be represented as a product of 4k + 2 matrices, alternating between diagonal and circulant matrices. The curr_chunk discusses the properties of a deep ReLU network and its convergence. It shows how the network can be transformed into a DCNN with increased depth while maintaining accuracy. The ReLU function's continuity and bounded input set ensure convergence to zero. The curr_chunk discusses the construction of a dense neural network N with specific properties for approximating a deep ReLU network. It involves the decomposition of matrices into diagonal-circulant matrices and the use of a DCNN for approximation. The text discusses the construction of a dense neural network N using diagonal-circulant matrices for approximating a deep ReLU network. It explores the impact of replacing matrices with low-rank approximators in neural networks and provides a formula for bounding errors in the approximation process. The text discusses constructing a dense neural network N using diagonal-circulant matrices to approximate a deep ReLU network. It provides a formula for bounding errors in the approximation process and discusses the impact of replacing matrices with low-rank approximators in neural networks. For experiments on CIFAR-10, neural networks were trained with specific hyper-parameters for 200 epochs using Leaky ReLU activation and Adam optimizer. Learning rate was adjusted at different steps. For YouTube-8M dataset, a neural network based on a state-of-the-art architecture was used, without convolution layers, processing input vectors from video frames. The models were trained on video frames using convolutional neural networks with specific parameters. DBoF, NetVLAD, and NetFV had different cluster sizes for video frames. Batch normalization and gradient clipping were used for stabilization during training."
}