{
    "title": "ByOExmWAb",
    "content": "Neural text generation models, such as autoregressive language models and seq2seq models, are state-of-the-art for machine translation and summarization benchmarks. These models generate text by sampling words sequentially, with each word conditioned on the previous one. While language models are typically trained via maximum likelihood with teacher forcing, which can optimize perplexity but result in poor sample quality, we propose using Generative Adversarial Networks (GANs) to improve sample quality. GANs train the generator to produce high-quality samples and have shown success in image generation. We introduce an actor-critic conditional GAN for text generation, filling in missing text based on context. This approach produces more realistic text samples compared to maximum likelihood trained models. RNNs are commonly used for language modeling, machine translation, and text classification, generating text by sampling from a conditioned distribution. Generative Adversarial Networks (GANs) are used to train models for text generation by providing a cost function that encourages high sample quality, addressing issues with teacher forcing and unpredictable dynamics in RNN hidden states. Generative Adversarial Networks (GANs) are effective for training generative models in an adversarial setup, where a generator creates images to deceive a discriminator. GANs have been successful in producing realistic images but face challenges with text sequences due to their discrete nature. To address this, Reinforcement Learning (RL) is used to train the generator while the discriminator is trained using maximum likelihood and stochastic gradient descent. GANs in text generation also encounter issues like training instability and mode dropping, where certain modalities are rarely generated by the generator. Training stability and mode dropping are challenges in text generation with GANs. To address this, a model is trained on a text fill-in-the-blank task, where missing portions of text are filled in to match the original data. This approach helps mitigate issues with autoregressive generation of text. The model operates autoregressively over filled-in tokens, conditioning on true context. Error attribution per time step is important in natural language GAN research. The discriminator evaluates each token, providing supervision to the generator. An outlier token can affect the entire sequence, so a discriminative model that penalizes outliers can improve generation quality. Research into extending GAN training to discrete spaces and sequences has been active. Training in a continuous setting allows for gradients to be passed through the discriminator to the generator. SeqGAN trains a language model using RL methods. The model introduces MaskGAN for text generation and considers actor-critic architecture in large action spaces. New evaluation metrics and synthetic training data generation are also explored. SeqGAN and Professor Forcing are two methods for training language models using RL techniques. SeqGAN trains a generator to fool a discriminator that distinguishes between real and synthetic text, while Professor Forcing uses a discriminator to differentiate hidden states of a generator RNN conditioned on real and synthetic samples. Both approaches have shown success in text generation tasks. Recent advancements in GANs for dialogue generation include the application of REINFORCE with Monte Carlo sampling on the generator. Techniques like WGAN-GP and boundary-seeking GAN objective with importance sampling are being explored to improve text generation. In BID24, the discriminator operates on the generator's output to optimize a lower-variance objective. Reinforcement learning methods like REINFORCE and actor-critic have been successful in natural language tasks, improving BLEU scores. Conditional text generation via GAN training has been explored, but our work stands out by using an actor-critic training procedure on a task designed for text generation. Our work employs an actor-critic training procedure on a task designed to provide rewards at every time step, aiming to mitigate severe mode-collapse. The task challenges the discriminator, reducing the risk of the generator facing a near-perfect discriminator. The critic helps the generator converge faster by reducing gradient update variance in a high action-space environment at the word-level in natural language. The MaskGAN architecture imputes missing tokens by conditioning on past and future information, using a seq2seq model. The MaskGAN architecture utilizes a seq2seq BID26 model to impute missing tokens by conditioning on past and future information. The generator consists of an encoding and decoding module, where a binary mask is used to select tokens to remain in the sequence. The encoder reads the masked sequence, providing future context for decoding. The decoder fills in missing tokens based on the masked text and previously generated tokens. The MaskGAN architecture uses a seq2seq model to impute missing tokens by conditioning on past and future information. The generator decomposes the distribution over the sequence into an ordered conditional sequence. The discriminator evaluates the filled-in sequence and is given the original context to avoid critical failure. The discriminator in the MaskGAN architecture evaluates the filled-in sequence by computing the probability of each token being real given the true context of the masked sequence. This is crucial to avoid critical failure modes where the discriminator fails to reliably identify fake text due to ambiguous contexts. The MaskGAN architecture uses a critic network to estimate the value function of the filled-in sequence, which is essential for training the generator through policy gradients in reinforcement learning. The generator aims to maximize the cumulative total reward by optimizing its parameters. The generator in the MaskGAN architecture optimizes parameters to maximize cumulative total reward by performing gradient ascent on E G(\u03b8) [R]. Using the REINFORCE algorithm, an unbiased estimator is found with the variance reduced by using a learned value function as a baseline. This approach follows an actor-critic architecture where the generator determines the policy and the baseline is provided by the critic. The critic in the MaskGAN architecture aids with credit assignment by designing rewards for each token generated at different time steps. The generator's gradient is updated to maximize the discounted total return, penalizing greedy token selection. The discriminator is updated following conventional GAN training methods. The discriminator in GAN training is updated based on the gradient. To address challenges with long sequences and large vocabularies, the core algorithm is modified with a dynamic task. The model incrementally increases the maximum sequence length to capture dependencies over shorter sequences before moving to longer ones. Additionally, a simple modification is made to alleviate variance issues with REINFORCE methods in large vocabulary sizes. Instead of generating rewards based on sampled tokens, the generator uses the full information of the distribution over all tokens before sampling. This incurs a computational penalty but can reduce variance. Pretraining involves training a language model first, then using those weights for the seq2seq model. The model with the lowest validation perplexity on the masked task is selected. The model with the lowest validation perplexity on the masked task is selected from a hyperparameter sweep over 500 runs in BID16. The inclusion of a critic in initial algorithms decreased gradient estimate variance by an order of magnitude, improving training. Evaluation of generative models remains an open research question, with a focus on heuristic metrics correlated with human evaluation. BLEU score BID21 is used for comparing candidate translations in machine translation. Unique n-grams produced by the generator in the validation corpus are computed, and a geometric average is taken for performance evaluation. The generator in GAN-trained NLP models can produce realistic language samples without being penalized for not producing the single correct sequence. Instead of focusing on reducing validation perplexity, the emphasis is on improving heuristic evaluation metrics. Both conditional and alternative options are explored for better sample quality. MaskGAN and MaskMLE are variants of GAN-trained NLP models that generate samples on PTB and IMDB datasets. The Penn Treebank dataset has 10,000 unique words, with training, validation, and test sets containing different word counts. The models are pre-trained with a variational LSTM language model before further training with a masking rate to improve sample quality. MaskGAN is pre-trained on PTB and IMDB datasets, with a validation perplexity of 55.3. The discriminator is then pre-trained on samples from the generator and real text. MaskGAN can also run in an unconditional mode, equivalent to a language model. The IMDB dataset consists of 100,000 movie reviews divided into training, test, and unlabeled instances. Each review indicates positive or negative sentiment. The MaskGAN model is pre-trained on PTB and IMDB datasets, achieving a validation perplexity of 55.3. The generator is further pre-trained with a masking rate of 0.5, reaching a perplexity of 87.1. The discriminator is then pre-trained on generated samples and real text. A comparison is made between MaskGAN and MaskMLE for conditional language generation on the IMDB dataset. In 1979, MaskMLE Black was a surprise to me in 1969. Conditional samples from IMDB for MaskGAN and MaskMLE models are presented. Positive reviews for the movie \"Vacation\" on IMDB. MaskGAN has a lower perplexity compared to MaskMLE on IMDB samples. The perplexity of IMDB samples under a pretrained LM is compared between MaskMLE (273.1 \u00b1 3.5) and MaskGAN (108.3 \u00b1 3.5). The study focuses on the quality of sample generation and highlights the issue of 'off-manifold' sequences in free-running mode. The evaluation is done quantitatively to understand this dynamic during sampling. The study compares the perplexity of IMDB samples generated by MaskMLE and MaskGAN. MaskGAN produces samples more likely under the initial model than MaskMLE. MaskMLE generates improbable sentences due to compounding sampling errors. The MaskGAN model operates in a free-running mode during training, making it more robust to sampling perturbations. Mode collapse in image generation can be measured by unique n-grams, with some evidence of mode collapse in MaskGAN. However, all complete samples from different models are still unique. The MaskGAN model shows mode collapse in image generation, measured by unique n-grams. Despite this, all complete samples from different models remain unique. RL training resulted in an initial drop in perplexity on the validation set, followed by a steady increase. Sample quality remained consistent, with a final perplexity of 400. Mode dropping near the end of sequences may affect sample quality. Validation perplexity does not always correlate with sample quality, which is best evaluated through unbiased human assessment. Generative models were compared using Amazon Mechanical Turk. Amazon Mechanical Turk was used to compare the quality of samples generated by different models. Results showed that MaskGAN outperformed MaskMLE on the IMDB dataset, while results were closer on the PTB dataset. Additionally, MaskGAN produced superior samples compared to SeqGAN. Our work supports matching training and inference procedures to improve language sample quality. MaskGAN algorithm, through GAN-training, enhances generated samples. Training with contiguous word blocks masked produced better samples, allowing the generator to explore longer sequences. Policy gradient methods with a learned critic were effective, but research on training with discrete nodes may offer more stable procedures. Attention was found to be crucial in the process. The use of attention was crucial for in-filled words to be conditioned on input context, improving stability in training for textual GANs. Considering GAN-training with attention-only models may further enhance performance. MaskGAN samples on a larger dataset were significantly better than MaskMLE model, despite higher perplexity. The model was trained using the Adam optimization method. Our model was trained using the Adam optimization method with default Tensorflow exponential decay rates. It consists of 2-layer 650 unit LSTMs for both the generator and discriminator, 650 dimensional word embeddings, variational dropout, and Bayesian hyperparameter tuning. The discriminator undergoes 3 gradient descent steps for every step on the generator and critic. We share embedding and softmax weights of the generator and improve convergence speed by sharing embeddings between the generator and discriminator. The critic shares all discriminator parameters except for the separate output head. Both generator and discriminator use variational recurrent dropout. The MaskGAN model explores various failure modes, including mode collapse across n-gram levels, under certain bad hyperparameter settings. This failure is a common issue in GAN-training. The MaskGAN model struggles with mode collapse at different n-gram levels, leading to repetitive and grammatically incorrect phrases. The generator can move between common modes in the text, making it challenging to produce syntactically correct sequences. This issue is also difficult for humans, as the filled-in text must be both contextual and match syntactically at the boundary. The MaskGAN model struggles with mode collapse at different n-gram levels, leading to repetitive and grammatically incorrect phrases. The intersection between the filled-in text and the present text is non-grammatical, similar to failure modes in GAN image generation. A larger capacity model may help mitigate these issues. The quality of produced samples is assessed using n-gram language statistics, but these are crude proxies and may devolve to a generator of low quality. The extreme expense of validation perplexity in the MaskGAN model could lead to low sample diversity despite improved 4-gram metric. Several samples from this model show a lack of diversity in the generated text. The model struggles to capture the complexities of natural language with metrics alone."
}