{
    "title": "HyB9Np6WG",
    "content": "Existing methods for prepositional representation in computational linguistics either treat prepositions as content words or rely on external linguistic resources. This paper introduces a new approach using word-triple counts to capture prepositions' interactions. Prepositional embeddings are derived through tensor decompositions on a large corpus, revealing a new geometry that proves useful in paraphrasing phrasal verbs. These embeddings are also applied to preposition selection and prepositional attachment disambiguation tasks with comparable or better results. The curr_chunk discusses the importance of prepositions in NLP tasks and the use of word embeddings like word2vec and GloVe to represent prepositions. It highlights the challenges in understanding prepositions due to their polysemous nature and flexible usage patterns. The paper achieves comparable or better results in prepositional attachment disambiguation tasks. The curr_chunk discusses the lack of specific properties of word embeddings for prepositions despite the success of using them in NLP tasks. It suggests that standard word embedding algorithms treat prepositions similarly to other content words, which may diminish their distinguishing features. The curr_chunk discusses the unique treatment of prepositions in linguistic theory, emphasizing the importance of considering their interactions with neighboring words. It proposes using a tensor of triples to capture these interactions within a specific context window, resulting in low-rank tensors and extracting embeddings for prepositions. The curr_chunk presents a method to extract embeddings for prepositions using tensor decompositions and standard word representation techniques. It demonstrates that the resulting preposition representation captures core linguistic properties and achieves state-of-the-art results on NLP tasks involving prepositions. Intrinsic evaluations show that the Hadamard product of verb and preposition embeddings approximates phrasal verb paraphrases. The curr_chunk discusses the significance of prepositions in context and their impact on semantics. It also highlights the challenges faced by second language English speakers in selecting the correct prepositions. The study introduces a new geometry for preposition embeddings and demonstrates their effectiveness in NLP classification tasks, outperforming state-of-the-art methods. The curr_chunk focuses on prepositional attachment disambiguation, a common cause of structural ambiguity in natural language. Despite extensive research, prepositional attachment remains a major source of parsing errors. The approach discussed in the curr_chunk achieves significant improvements in preposition choice tasks. Our approach for prepositional attachment disambiguation achieves 89% accuracy on the BID2 dataset, matching state-of-the-art performance without relying on extensive linguistic resources like syntactic parsers and WordNet. We highlight the importance of word-triple counts, especially when a preposition is involved, as they offer valuable insights into sentence representations. The benefits of tensor representations for prepositions are highlighted, with a focus on the simplicity of word and preposition representations via tensor decomposition. The approach achieves state-of-the-art results without relying on syntactic parsing or handcrafted features, showcasing the strength of prepositional representations found through tensor decompositions. The tensor with triples (word, word, preposition) is formed and its slices are shown to be low-rank. Low dimensional vector representations for words and prepositions are derived through tensor decomposition methods. A third order tensor is generated from WikiCorpus Al-BID0, where words co-occur within a certain distance in sentences. The tensor creation process involves prepositions and words co-occurring in sentences across a large WikiCorpus. In a large WikiCorpus, a tensor X is created with triples (word, word, preposition) and its slices are low-rank. The tensor is very sparse with only 1% non-zero elements. Each slice log(1 + X[:, :, k]) is low rank. The interaction between prepositions and neighboring words weakens more sharply with distance compared to content words. The tensor log(1 + X) is decomposed into three modes using the CANDECOMP/PARAFAC (CP) decomposition method. The singular values of slices corresponding to prepositions decay dramatically, indicating a low-rank structure. The tensor log(1 + X) is decomposed into three modes using the CANDECOMP/PARAFAC (CP) decomposition method. The columns of U represent word representations and the columns of Q represent preposition representations, each of dimension d (200 in this paper). Orth-ALS is an algorithm that periodically orthogonalizes decomposed components while fixing two modes and updating the remaining one, outperforming standard ALS method in various applications. Bias terms are learned to minimize the loss function. Bias terms are learned to minimize the loss function by assigning weights to tensor elements. The optimization problem is solved using gradient descent to obtain word and tensor representations. These representations help interpret phrases by reflecting the co-occurrence and cohesion of words in context. Additionally, a slice accounts for word co-occurrences outside the preposition window. The verb phrase divide something can be paraphrased as split off something, with the relation captured by well-trained embeddings. Paraphrases of verb phrases were generated using weighted tensor decomposition, comparing results with regular word embeddings and different operations. In this work, tensor representations and Hadamard product operations are used for vastly superior paraphrasing. Tensor-based preposition embeddings are evaluated for preposition selection and attachment disambiguation tasks. Tensor embeddings are trained with Orth-ALS and weighted decomposition methods using WikiCorpus as the training corpus. Baseline models like word2vec's CBOW and GloVe are included for comparison. Hyperparameters are set accordingly for each model. The study compares word2vec and GloVe models with tensor embeddings for preposition selection. Prepositions are challenging due to their polysemous nature. Evaluating lexical interactions is crucial for accurate preposition usage. The study focuses on preposition selection in English sentences, aiming to replace incorrect prepositions with the correct ones from a set of candidates. Evaluation metrics include precision, recall, and F1 score. The algorithm preprocesses the dataset, identifies errors, and corrects them in two steps. The study focuses on preposition selection in English sentences, aiming to replace incorrect prepositions with the correct ones from a set of candidates. The task is divided into error identification and error correction steps. In the identification step, a decision tree classifier using cosine similarity, rank, and probability features achieves high F1 scores. Error correction focuses on identified errors, using confusion probability and a two-layer feedforward neural network for replacement suggestions. The study focuses on preposition selection in English sentences. A two-layer feedforward neural network is trained with features to score prepositions, with the highest score suggesting the edit. Baseline methods use n-gram statistics and supervised scoring systems. Evaluation includes comparing tensor embedding-based features with word2vec and GloVe embeddings. Results are compared against baselines in a table. The study compares tensor embeddings with word2vec and GloVe embeddings for preposition selection. Tensor embeddings outperform other approaches, with weighted decomposition achieving the highest F1 score on the CoNLL dataset and ALS decomposition performing best on the SE dataset. Ablation analysis shows the importance of features, with left context being crucial for the CoNLL dataset. The study compares tensor embeddings with word2vec and GloVe embeddings for preposition selection. Tensor embeddings outperform other approaches, with weighted decomposition achieving the highest F1 score on the CoNLL dataset and ALS decomposition performing best on the SE dataset. Pair and triple similarity features are less important due to the neural network learning lexical similarity from embedding features. Limited context window and the need for more context are reasons for wrong preposition selection in some sentences. In the study, tensor embeddings are compared with word2vec and GloVe embeddings for preposition selection. The context window may not always be sufficient to choose the correct preposition, as seen in examples like \"on purpose\" versus \"for purpose\". Prepositional phrase attachment disambiguation is a challenging task in syntactic parsing, requiring accurate description of interactions among head words, prepositions, and child words. The study compares tensor embeddings with word2vec and GloVe for preposition selection. Prepositional phrase attachment disambiguation is a challenging task in syntactic parsing, requiring accurate description of interactions among head words, prepositions, and child words. The English dataset used in the work is collected from a linguistic treebank by BID2, with statistics enumerated in TAB5. The algorithm involves using embeddings for head candidate, preposition, and child, along with features like triple similarity and part-of-speech tags. A basic neural network is used for the task. The study utilizes a two-layer feedforward neural network to predict head candidates based on input features. State-of-the-art approaches for preposition attachment disambiguation are included, such as the HPCD Model, LRFR, and OntoLSTM. These methods incorporate word representations, tensor embeddings, and LSTM training for head selection. The study compares different approaches for head selection using LSTMs and tensor representations. Results show that their simple classifier based on tensor representations is close to the state of the art, outperforming word2vec and GloVe. Ablation analysis indicates that head vector feature has the most significant impact on performance. In experiments, the head vector feature is found to have the most impact on performance, followed by POS tag. Similarity features are less important due to access to lexical relatedness via embedding features. Distance feature, previously considered important, becomes less significant compared to embedding features. The lack of broader context in features contributes to attachment disambiguation errors, especially in cases like \"worked\" and \"system\" as head candidates for \"for trades\". More context is needed to determine the correct head in such instances. Tensor decomposition methods like ALS and SD capture relations in higher order interactions among different modes. The approach in identifying head verbs outperforms head nouns, with an accuracy of 0.897 for verbs and 0.887 for nouns. Tensors' geometry aids in paraphrasing verb phrases, showcasing the interaction between verbs and prepositions. Orthogonalized Alternating Least Square (Orth-ALS) adds component orthogonalization to ALS method. It is the algorithm of choice in this paper for preposition selection, a practical topic in grammar correction and second language learning. Prior works use heuristic rules and lexical n-gram features for preposition correction. Syntactic information like POS tags and dependency parsing enrich features for generic tasks involving prepositions. Prepositional Attachment Disambiguation is a key aspect of syntactic parsing, with recent works using word embeddings to improve accuracy. Different models, such as the HPCD Model and tensor decompositions, have been proposed to enhance word representations. Co-occurrence counts of word pairs and triples have revolutionized NLP research. In this work, the focus is on using tensor decompositions for word vector representations by considering co-occurrence counts of word triples. The tensor size and dynamic range have posed challenges, but restricting word triples to scenarios involving prepositions shows promise. Prepositions are crucial for modeling interactions between words and result in a tensor with similar sparsity and dynamic range as the original matrix of pairwise co-occurrence counts. The study focuses on using tensor decompositions for word vector representations, specifically for prepositions. The vector representations are expected to be useful in NLP tasks where prepositional role is important. The list of most frequent prepositions is provided, and a method for generating paraphrases of phrasal verbs is discussed. The study discusses a linear algebraic method for generating paraphrases of compositional phrasal verbs. It approximates the paraphrase representation and filters out candidate words that are the same as component words in the phrase. The method selects the most similar verb among candidates and uses Python NLTK tools for filtering. Results on a new dataset of 60 compositional phrases are detailed, considering word2vec, GloVe, and tensor embeddings, along with composition methods like addition and Hadamard product. The study explores composition methods for generating paraphrases of phrasal verbs using tensor embeddings and different composition methods. Tensor embeddings combined with the Hadamard product composition method outperform other approaches like word2vec and GloVe with additive composition."
}