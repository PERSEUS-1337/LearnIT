{
    "title": "B16_iGWCW",
    "content": "A deep boosting algorithm is developed to combine base deep CNNs with diverse capabilities to create a more discriminative ensemble classifier. The base experts are trained sequentially to recognize object classes in an easy-to-hard way. Experimental results show significant improvement with this algorithm. Boosting has achieved success in visual recognition by embedding multiple weak learners to construct an ensemble classifier. It sequentially trains these learners with respect to a weighted error function, improving performance by assigning larger weights to misclassified samples. It is worth investigating whether boosting can be integrated with deep learning for even better accuracy rates on large-scale visual recognition. Boosting has been successful in visual recognition by combining weak learners to create an ensemble classifier. There is interest in integrating boosting with deep learning to improve accuracy rates on large-scale visual recognition. A new deep boosting algorithm has been developed to use different weights over various object classes, aiming to generate a more discriminative ensemble classifier by combining a set of base deep CNNs with diverse capabilities. The paper introduces a discriminative ensemble classifier by combining base deep CNNs with diverse capabilities. It reviews related work, presents a deep boosting algorithm, reports experimental results, and discusses the need for improvement in deep learning for large-scale visual recognition. Boosting BID15 BID1 BID16 offers an easy-to-hard approach to training a set of weak learners sequentially, by assigning different weights to training samples adaptively. This method aims to organize object classes in an easy-to-hard way based on their learning complexities, allowing for more effective learning of deep CNNs. Leveraging boosting to learn a set of base deep CNNs could be a promising approach for recognizing large numbers of object classes with diverse learning complexities. Boosting algorithms have been developed to integrate boosting with deep neural networks for improved performance. Various methods have been proposed, such as integrating Adaboost with neural networks for online character recognition and credit scoring. Additionally, there are methods like deep incremental boosting and integrating residual networks with boosting to build ensembles for better training. These methods combine the strengths of boosting and neural networks to train effectively. Various methods have been proposed to integrate boosting with neural networks for improved performance. BID14 introduced a margin enforcing loss for multi-class boosting and presented two ways to minimize risk. BID0 designed an ensemble learning algorithm for binary-class classification using deep decision trees as base classifiers. BID7 extended it to a multi-class version, while BID9 developed an algorithm using directional derivative descent. Our deep boosting algorithm, developed by BID9, focuses on combining a set of base deep CNNs with diverse capabilities. It organizes object classes in an easy-to-hard way, sequentially learns to recognize different subsets of object classes, and seamlessly combines base deep CNNs with diverse capabilities. In this paper, a deep boosting algorithm is developed by seamlessly combining a set of base deep CNNs with various capabilities, such as AlexNet BID6, VGG BID19, ResNet BID3, and huang2016densely. These base deep CNNs are sequentially trained to recognize different subsets of object classes in an easy-to-hard way. The algorithm optimizes their structures and node weights jointly for recognizing the same set of object classes. The deep boosting algorithm combines base deep CNNs like AlexNet, VGG, ResNet, and huang2016densely to recognize subsets of object classes in an easy-to-hard manner. The algorithm optimizes structures and weights for the same set of object classes, focusing on achieving higher accuracy. The deep boosting algorithm combines base deep CNNs like AlexNet, VGG, ResNet, and huang2016densely to recognize subsets of object classes in an easy-to-hard manner. It focuses on achieving higher accuracy rates for specific object classes through iterative training processes that update importances and stop based on accuracy levels or maximum iterations. The base expert uses deep CNNs to map inputs into separable feature spaces and employs fully connected layers for classification. The base expert predicts multi-class distribution using probability scores for object classes. Model parameters are learned by maximizing a weighted margin objective function based on labeled samples. Importance scores for each class are normalized to train the base expert. Our deep boosting algorithm utilizes importance scores to focus on hard-to-classify object classes, supporting easy-to-hard solutions for visual recognition. The margin \u03be lt measures the difference in confidence between correctly and incorrectly classified examples for each object class. Maximizing the objective function is equivalent to maximizing the weighted likelihood. The classification error rate is used to update category weight and the loss function. The error rate is used to update category weight and the loss function of the next weak learner, encouraging predictors with large margin to improve discrimination between correct and incorrect classes. Error rate can be calculated in soft or hard decision, with a hyperparameter \u03bb controlling the threshold. The weighted error rate \u03b5 t is computed over all classes for f t (x) to focus on hard object classes. The distribution of importances is initialized equally for all object classes and updated iteratively to emphasize the object. The distribution of importances is updated iteratively to emphasize hard object classes misclassified by previous base deep CNNs. The updated importances distribution is normalized to separate hard object classes from easy ones and estimate the ratio of heavily misclassified categories. The deep boosting algorithm iteratively trains base deep CNNs to recognize different subsets of object classes in an easy-to-hard way. These base experts are combined to create a more discriminative ensemble classifier for recognizing object classes. The importances distribution is updated to focus on hard object classes misclassified by previous base deep CNNs. Our deep boosting algorithm iteratively trains base deep CNNs to recognize object classes in an easy-to-hard way. The ensemble classifier combines these base experts to improve accuracy rates on large-scale visual recognition. The selection of \u03b2 t in the algorithm is based on an increasing function of error rate \u03b5 t, emphasizing hard object classes and assigning larger weights to base experts with low error rates. The algorithm trains base deep CNNs to recognize object classes in an easy-to-hard manner. It emphasizes hard object classes by assigning larger weights to experts with low error rates. The selection of \u03b2 t is crucial for minimizing the ratio of heavily misclassified categories to all classes. The algorithm trains base deep CNNs to recognize object classes in an easy-to-hard manner, emphasizing hard object classes by assigning larger weights to experts with low error rates. The selection of \u03b2 t is crucial for minimizing the ratio of heavily misclassified categories to all classes. The upper boundary of the ratio is influenced by the hyper-parameter \u03bb, with a range discussed based on the precision requirement. When \u03bb increases, the precision requirement increases, leading to more hard categories. The value of \u03bb should be set smaller to alleviate large error rates \u03b5 t. The procedure of learning the t th base expert adjusts the parameters of the deep network to maximize the objective function O t. The objective function O t is maximized by calculating gradients with respect to all parameters, including weights and model parameters. The probability score of x for object class l can be written as a composite function. Gradients are back-propagated through deep CNNs to fine-tune weights and model parameters simultaneously. Investigating the gap between generalization error and empirical error is crucial. The gap between generalization error and empirical error is crucial. The final ensembled classifier's generalization error rate can be measured by a probability formula. The base-classifier space F with VC-dimension d is important for prediction accuracy. The effective number of hypotheses for the base-classifier space F over a sample set S can be estimated using the number of neurons and weights in the deep network. A large margin \u03be over the training set leads to a narrow gap between generalization error and empirical error, resulting in a better upper bound of generalization error. In this section, the proposed algorithms are evaluated on real-world datasets MNIST, CIFAR-100, and ImageNet. Networks are trained from scratch in each AdaBoost iteration for MNIST and CIFAR-100, while a pretrained model is used for ImageNet. Weight initialization method by BID2 is adopted, and networks are trained using SGD with weight decay and momentum. MNIST dataset has 60,000 training and 10,000 test samples, with accuracy improvement shown via AdaBoost on MNIST dataset by updating sample weights based on classification errors. The proposed method in the paper updates weights based on class errors, while the traditional AdaBoost method updates weights based on sample errors. The experiment on MNIST dataset shows a decrease in top 1 error from 4.73% to 1.87% after three iterations using the proposed method. After three iterations, the proposed method updates weights based on class errors, leading to a quicker drop in the top 1 error compared to traditional methods. The method trains classifiers in an easy-to-hard manner, adjusting class APs for major and minor weighted classes. This approach improves accuracy for hard classes while maintaining accuracy for easy classes, coordinating weak learners with diverse capabilities. Our method coordinates weak learners with diversified capabilities to enhance the classification ability of boosting models. Experiments are conducted on the CIFAR-100 dataset, which includes 60,000 images from 100 classes. Data augmentation techniques such as padding, mirroring, and shifting are used, along with normalization. During training, 5,000 images are held out for validation, while 45,000 are used for training. The category distribution is updated based on class errors from the validation set. The initial learning rate is set to 0.1 and is divided by 0.1 at epoch [150, 225]. The network is trained for 300 epochs using ResNet56 and DenseNet-BC models on the CIFAR-100 dataset. Different values of lambda (\u03bb) are tested, with \u03bb=0.7 showing the best performance with 24.15% test error after four iterations. Models with \u03bb=0.7 converge to a better optimal compared to \u03bb=0.5 and \u03bb=0.1. Lambda controls weight differences among classes, with smaller values leading to larger weight differences. In experiments on the CIFAR-100 dataset, different values of lambda were tested, with lambda=0.1 causing fluctuations in top 1 error. It was concluded that lambda should be used to ensure beta is below 0.5 and may harm ensemble model performance if set too low. The method focuses on different learning complexities for classes, with classes with lower APs given more attention in subsequent iterations. In experiments on the ILSVRC2012 Classification dataset, ResNet50 networks were trained with initial learning rates set to 0.1 and divided by 0.1 at epoch [30, 60]. The network was trained for 90 epochs and the performances of the ensemble classifier with different numbers of base networks were evaluated. In this paper, a deep boosting algorithm is developed to create a more discriminative ensemble classifier by combining base ResNet networks with diverse capabilities. The base experts, which are deep CNNs, are sequentially trained to recognize object classes in an easy-to-hard manner. Future work will explore the performance of heterogeneous base deep networks from different families."
}