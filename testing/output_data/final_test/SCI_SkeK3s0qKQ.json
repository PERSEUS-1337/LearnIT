{
    "title": "SkeK3s0qKQ",
    "content": "Rewards in the real world are sparse, posing a challenge for reinforcement learning algorithms. One solution is to allow agents to create rewards for themselves, making learning more effective. A new curiosity method using episodic memory to form a novelty bonus is proposed. This bonus is based on comparing current observations with those in memory, incorporating rich information about environment dynamics and overcoming previous issues. Our approach in visually rich 3D environments outperforms state-of-the-art curiosity methods in navigational tasks. The code is available at https://github.com/google-research/episodic-curiosity/. Real-world tasks with sparse rewards pose challenges for standard reinforcement learning algorithms. Multiple approaches have been proposed to achieve better explorative policies, such as giving a reward bonus. Approaches were proposed to achieve better explorative policies by giving a reward bonus to facilitate exploration. This bonus is combined with the original task reward and optimized using standard RL algorithms, inspired by neuroscience studies of animals. The formation of this bonus remains an open question, with modern curiosity formulations aiming to maximize surprise. However, this approach is not perfect, as shown in a thought experiment involving an agent in a 3D maze with a valuable goal and a TV remote control. The proposed curiosity definition is based on the idea that if an agent can predict the outcome of an action, it is less likely to find it interesting. The definition distinguishes between actions that are easily reachable from memory and those that are far from memory, with the latter considered more novel. The concept of novelty is defined through reachability in a graph where nodes represent observations and edges represent possible transitions. A neural network is trained to predict the distance in steps between observations, rewarding only those that require effort to reach. This approach aims to identify novel observations that are further away from memory. The concept of novelty via reachability is illustrated in Figure 1. To determine novelty, episodic memory stores past instances for comparison with current observations. The agent rewards itself for novel observations that take more steps to reach and adds them to memory. This process continues throughout the episode. Our method is benchmarked on tasks in visually rich 3D environments like VizDoom, DMLab, and MuJoCo. Compared to the state-of-the-art curiosity method ICM BID19, our method shows faster convergence and more robustness to spurious behaviors in procedurally generated environments. Our method demonstrates improved explorative behavior and outperforms the baseline in procedurally generated test levels in DMLab. It covers more area and does not hinder performance in tasks with dense rewards. Additionally, it enables an ant in a MuJoCo environment to learn locomotion solely from curiosity rewards. The agent receives observations, takes actions, and receives rewards in each time step. The goal is to optimize the sum of discounted rewards. Sparse rewards pose a challenge for traditional RL algorithms, but an episodic curiosity module helps by providing a reward bonus. The episodic curiosity module provides a dense reward that accelerates learning and improves performance in reinforcement learning tasks. It consists of parametric components, including an embedding network and a comparator network, trained to predict reachability. The reachability network consists of parametric components trained to predict reachability, along with non-parametric components like an episodic memory buffer and a reward bonus estimation function. The system includes embedding and comparator networks designed to estimate within-k-step-reachability. The R-network is a classifier trained with logistic regression loss to predict the probability of two observations being reachable within k steps. The episodic memory buffer stores embeddings of past observations with limited capacity. When capacity is exceeded, a random element is substituted. The reward bonus estimation module assigns larger rewards if no reachable observations are found in memory. The episodic memory buffer stores embeddings of past observations with limited capacity. A comparator network checks for novelty by comparing current embeddings with stored ones. The episodic curiosity module computes a reward bonus for novel observations, enhancing training for an RL agent. The similarity score between the memory buffer and current embedding is computed using an aggregation function, with 90-th percentile found to be a robust substitute for maximum. Hyperparameters \u03b1 and \u03b2 determine the scale and sign of task rewards, affecting episode length in training an RL agent. After computing the similarity score between memory and current embedding, hyperparameters \u03b1 and \u03b2 determine task rewards and episode length in training an RL agent. For fixed-duration episodes, \u03b2 = 1 is preferred, while a novelty threshold b novelty is used to add observation embeddings to memory. This threshold induces discretization in the embedding space to store only \"distinct enough\" memories, reducing redundancy in the memory buffer. Visualization in video 1 shows the curiosity reward bonus and memory state. If the full transition graph in Figure 1 was available, reachability network and novelty computation could be done analytically. The procedure illustrated in Figure 2 involves training a network with logistic regression loss using a sequence of observations to determine reachability. Positive examples are pairs of observations close in sequence, while negative examples are pairs further apart. The hyperparameter \u03b3 creates a gap between positive and negative examples to train the network effectively. In training a reachability network, two settings are explored: using a random policy and online training with the task-solving policy. The agent interacts with the environment to fill the replay buffer and forms training pairs. The second version collects data on-policy and re-trains the network after a fixed number of interactions. The reachability network training involves exploring two settings: using a random policy and online training with the task-solving policy. Experiments are conducted in various environments like VizDoom, DMLab, and MuJoCo to test the method's generalization and effectiveness. VizDoom and DMLab provide maze-like 3D environments with image observations, while MuJoCo experiments demonstrate the method's generality. Additional details on MuJoCo experiments are available in the supplementary material. The agent operates in VizDoom and DMLab using 84 \u00d7 84 grayscale and RGB images respectively. The action sets include navigational actions like move forward, turn left/right. The rewards and episode durations vary between environments. The PPO algorithm is chosen as the basic RL algorithm for training. The study used the PPO algorithm as the basic RL algorithm, with CNNs representing policy and value functions to reduce hyperparameters. PPO was applied to task and bonus rewards from curiosity algorithms. Two sets of hyperparameters were used for VizDoom and DMLab environments. Baseline methods included basic RL for task reward and ICM curiosity method. ICM outperforms VIME, #Exploration, and EX 2 on curiosity tasks in 3D environments. A new baseline method called Grid Oracle is introduced, utilizing privileged information to reward the agent for visiting as many cells as possible. This baseline shows strong performance in DMLab environments. The Grid Oracle baseline in DMLab environments has two hyperparameters for reward combination and cell size, tuned on a validation set. VizDoom environments lack splits, so tuning is done on the same environment. Mean final reward of 10 runs is used for tuning without seed tuning. Hyperparameter values are in the supplementary material. Bonus scalar \u03b1 depends on task rewards. In this section, we describe the specific tasks and experimental results for different methods in VizDoom and DMLab environments. Four methods are reported: PPO, PPO + ICM, PPO + Grid Oracle, and PPO + EC. Tasks include static-maze goal reaching in VizDoom, procedurally generated mazes in DMLab, no-reward maze exploration in DMLab, and dense reward tasks in DMLab. The curiosity bonus does not significantly impact performance in the dense reward tasks. The experiments in DMLab show that the curiosity bonus does not affect performance in dense reward tasks. Additional experiments demonstrate the R-network's ability to generalize between environments and its stability to hyperparameters. The goal is to verify the correct implementation of the baseline method using the MyWayHome task from VizDoom. The task involves reaching a goal in a static 3D maze within a time limit of 525 4-repeated steps. The task involves three sub-tasks (\"Dense\", \"Sparse\", \"Very Sparse\") in a maze layout shown in FIG1. In the \"Dense\" subtask, the agent starts in random locations close to the goal, making it an easy task with relatively dense rewards. The task involves three sub-tasks in a maze layout: \"Dense\", \"Sparse\", and \"Very Sparse\". The agent starts in different distances from the goal in each sub-task. Results show that the method works on-par with the ICM baseline in terms of final performance and converges faster, reaching 100% success rate quicker. In this experiment, the training curves are shifted to ensure fair comparison of training speed. The goal is to evaluate maze goal reaching task generalization on a large scale by training and testing on hundreds of levels. Two types of levels, \"Sparse\" and \"Sparse + Doors\", are used in the DMLab simulator where the agent must reach the goal within a time limit of 1800 4-repeated steps. The study evaluates maze goal reaching task generalization on a large scale by training and testing on hundreds of levels. The standard task \"Sparse\" is relatively easy due to same-room initialization, but a harder version \"Very Sparse\" with a gap between starting point and goal is introduced. Results show adaptability to changing layouts in tasks \"Sparse\", \"Very Sparse\", and \"Sparse + Doors\". The study demonstrates adaptability to changing layouts in different maze environments, outperforming baseline methods in \"Sparse\", \"Very Sparse\", and \"Sparse + Doors\" settings. The ICM method shows a tendency to engage in repetitive actions like firing or switching channels, indicating a limitation in its learning behavior. The study shows adaptability in maze environments, outperforming baseline methods in sparse settings. The formulation is more robust than ICM's prediction error, suggesting a need to explore methods in extremely sparse reward scenarios. In a study on adaptability in maze environments, the experiment aims to quantify the effectiveness of a method in a scenario with no task reward. The agent's behavior in this no-reward world influences its likelihood of finding the task reward. The task is modified to eliminate the reward, named \"No Reward\", and success is measured by the reward from Grid Oracle. Results show that the method and Grid Oracle work, while the ICM baseline does not. After observing the behavior of ICM and Grid Oracle in the maze environment with no task reward, it is noted that ICM's performance deteriorates over time, leading to a disagreement between prediction-error-based bonus and area coverage metric. A new task \"No Reward - Fire\" is created to exclude the firing action, showing similar results to the full action set. The agent's exploration remains reasonable despite the changes in behavior. The ICM method shows a quick exhaustion of curiosity in an environment without rewards, leading to undesired behavior. Concerns arise about passing the peak before reaching the first task reward in real tasks, requiring careful tuning per game. Good exploration behavior may take time to reach the first reward, posing challenges for the ICM method but remaining possible for our method. A good curiosity bonus should avoid hindering performance in dense-reward scenarios. A good curiosity bonus should not hinder performance in dense-reward tasks. Testing in the DMLab simulator shows that our method does not significantly worsen performance in tasks with dense rewards, such as \"Rooms Keys Doors Puzzle\" and \"Rooms Collect Good Objects Train\". The results indicate that our method is effective in maintaining performance in these scenarios. Our method in the DMLab simulator maintains performance in dense-reward tasks like \"Rooms Keys Doors Puzzle\" and \"Rooms Collect Good Objects Train\". The training curves for \"Dense 1\" are shown in FIG2, while for \"Dense 2\" are in the supplementary material. The Grid Oracle in \"Dense 2\" task performs slightly worse due to excessive running and occasional failure to collect all good objects. Our method combines curiosity, episodic memory, and temporal distance prediction, relating to prior work on exploration behavior in visually rich 3D environments. In prior work, embedding is trained for predicting actions between observations, unlike an autoencoder. However, the perceptive prediction approach can lead to the agent becoming passive. Our method avoids this behavior, unlike the ICM baseline. Another approach trains a temporal distance predictor to establish novelty based on observation classification compared to previous ones, without using episodic memory. The curr_chunk discusses curiosity-based exploration for RL, focusing on prediction-error-based, count-based, and goal-generation-based approaches. The count-based approach suggests keeping visit counts for observations and prioritizing rarely visited states, similar to episodic memory usage. The extension to continuous observation spaces is challenging, with notable progress made by works BID3 and BID16. The notable step in this direction was taken by works BID3 and BID16, introducing a trained observation density model converted to a function similar to counts. Another approach, BID27, discretizes the continuous observation space by hashing and uses a count-based approach. However, experiments show it does not perform well in visually rich 3D environments. Novelty Search and its follow-up maintain an archive of behaviors for comparison, but their effectiveness is still being evaluated. The work proposes maintaining an archive of behaviors and comparing them using euclidean distance. Novelty is defined through reachability, similar to prior works like BID12, BID1, and BID20. The method implicitly defines goals that are a fixed number of steps away using a reachability network. The method proposed is easier to implement than other goal-generation methods and is quite general. Two recent works were inspired by episodic memory in animals and focused on repeating successful strategies. Another approach involves predicting the distance between video frames as an auxiliary task for solving other problems. The proposed method utilizes episodic memory and reachability for curiosity in 3D environments, outperforming previous state-of-the-art methods. It enables a MuJoCo ant to learn locomotion through first-person view curiosity. The future goal is to incorporate memory awareness into policies. The supplementary material includes MuJoCo locomotion experiments, training details for R-network, hyperparameter values, and experimental results showing R-network's ability to generalize between environments. The authors express gratitude to various individuals for valuable discussions on their work. The curr_chunk discusses the transferability of R-networks between environments, stability and ablation study results, robustness to stochastic environments, computational considerations, and training curves for the \"Dense 2\" task. It also mentions a MuJoCo ant learning to move out of curiosity. The ant in the MuJoCo environment is placed on a floor with random textures, and the episode ends if its z-coordinate goes above 1.0 or below 0.2. The observation space uses a first-person view camera, while the action space is standard continuous. The basic RL solver used is PPO. The ant in the MuJoCo environment has a standard continuous action space. The basic RL solver is PPO. Results show that our method outperforms baselines in a setting with no task reward after 10M training steps. Additionally, an experiment with an extremely sparse task reward called \"Escape Circle\" was conducted. Our method outperforms baselines in a setting with no task reward after 10M training steps, showing significant improvement compared to the best baseline by a factor of 10. The closest related work is BID6, which demonstrates slow motion of the ant learned from pixel-based curiosity. Other works focus on using state features for intrinsic reward, while our approach uses pixels. Grid Oracle rewards are reported for \"No reward\" and \"Escape Circle\" scenarios. Training R-network uses mini-batches of 64 observation pairs, run for 50K iterations for VizDoom and 200K iterations for DMLab. Adam optimizer with learning rate 10^-4 is used. R-network has a siamese architecture with two branches, each Resnet-18 with 512 outputs, and a fully-connected network with four hidden layers. For online training of the R-network, experience is collected and training is performed every 720K 4-repeated environment steps. 10 epochs of training are done on the collected experience, with data shuffled before each epoch. Hyperparameters for different methods are provided for VizDoom, DMLab, and MuJoCo Ant environments. PPO algorithm is used for implementation, with scaling of both bonus and task reward for convenience. The implementation involves scaling both the bonus and task reward for convenience. The R-network is trained using 10M environment interactions across 30 DMLab-30 tasks to generalize between tasks. Using a universal R-network slightly reduces performance compared to a specialized R-network but still outperforms plain PPO. The R-network is trained on 10M environment interactions across 30 DMLab-30 tasks to generalize between tasks. Table S7 shows that transferring the R-network between similar environments yields reasonable performance, except for one case (using the R-network trained on \"Dense 2\"). The environments have different characteristics, affecting performance. The experiments involve testing the R-network in different environments, including \"No Reward\" and \"Very Sparse\" environments. The \"No Reward\" environment helps to reveal behavioral differences without task rewards masking them. Training the R-network requires a threshold to separate negative from positive pairs, with the policy performance ideally not being too sensitive to this hyper-parameter. The EC-module's performance is robust to varying thresholds and memory buffer sizes. The sample complexity includes training the R-network and policy, with the total complexity being the sum of both in the worst case scenario. The total sample complexity is the sum of training the R-network and policy. Training the R-network with a low number of environment steps already shows good performance. The Embedding network is desired but not necessary, while the Comparator is essential for predicting reachability. The Embedding network is not necessary, but the Comparator is essential for predicting reachability. Future prediction errors can be caused by various factors such as partial observability, insufficient future prediction models, or randomized transitions. The EC method relies on comparisons to the past instead of future predictions, making it more robust to these factors. The goal is to provide additional evidence for this through experimentation. The experiment focuses on analyzing how different methods behave in environments with stochastic next states. Two versions of DMLab environments, \"Sparse\" and \"Very Sparse,\" are created with added stochasticity by displaying random images on the agent's TV screen. Different settings include \"Image Action k\" with animal images and \"Noise\" with random patterns displayed at each step. The experiment analyzes methods in environments with stochastic next states in DMLab tasks. Different settings include \"Image Action\" with animal images and \"Noise\" with random patterns displayed at each step. Results for various methods in the randomized-TV versions of the task are shown in a table. The experiment compares different methods in DMLab tasks with stochastic next states. Results for various methods in the randomized-TV versions of the task are presented in a table, showing performance metrics for each method. The experiment compares different methods in DMLab tasks with stochastic next states. Results for various methods in the randomized-TV versions of the task are presented in a table, showing performance metrics. Our method is robust to stochasticity and outperforms baselines, with videos demonstrating effective maze exploration. The algorithm efficiently computes memory reachability queries in parallel via mini-batching. The algorithm efficiently computes memory reachability queries in parallel via mini-batching. Memory consumption is modest, and speed comparisons show variations between different methods. The R-network has a higher number of trainable variables compared to other methods, but optimization for speed and parameters was not a focus in this study. It is suggested that a simpler model may work just as effectively as the resource-intensive Resnet-18 used in the R-network. In this paper, the setup for the R-network is based on prior work BID22, which has shown good performance. However, there is no evidence that this setup is necessary. Training curves for the DMLab task \"Dense 2\" are shown, with the comparison between different methods adjusted for fairness. Additional training curves are provided in Figure S2."
}