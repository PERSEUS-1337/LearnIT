{
    "title": "rkMhusC5Y7",
    "content": "The proposed method in reinforcement learning for query reformulation involves multiple specialized sub-agents and a meta-agent that aggregates answers for document retrieval and question answering tasks. Training sub-agents on disjoint data partitions and the meta-agent on the full set leads to faster learning and better generalization compared to strong baselines. The increased diversity of reformulation strategies contributes to the improved performance, as seen in various language processing tasks. In this work, a method is proposed for efficient parallelized exploration of diverse policies in reinforcement learning. The agent is structured into multiple sub-agents trained on different subsets of data, coordinated by a generalist aggregator. This approach aims to achieve better performance through faster learning and improved generalization compared to traditional methods. Training multiple sub-agents on different data subsets allows for faster learning and diverse policies. This method improves generalization and performance in reinforcement learning compared to training a single generalist agent. The sub-agents do not need to communicate during training, making it easier to parallelize and more efficient than existing distributed algorithms. The proposed method builds upon previous works and evaluates on query reformulation and question-answering tasks. It outperforms a strong baseline and shows a correlation between performance and reformulation diversity. The main contributions include a simple method for diverse strategies, easy parallelization in training, and the finding that specializing agents on similar data does not work as well as random partitioning. The proposed approach, inspired by the mixture of experts, involves training specialized agents to handle different tasks. Shazeer et al. (2017) showed strong performance in language modeling and machine translation tasks by exchanging output vectors between machines. BID0 later proposed a method to reduce communication overhead by exchanging probability distributions instead. Our method only requires exchanging scalars (rewards) and short. Our method for improving exploration in RL involves exchanging rewards and short strings between agents, reducing communication overhead. Previous works used specialized agents to achieve diversity of strategies and faster convergence. Experiments are often conducted in simulated environments with frequent rewards, low state diversity, and fast responses. The proposed method involves training an RL agent to reformulate queries for better retrieval results in a diverse natural language environment with slow response times. This approach differs from previous works that focused on fast responses and specialized agents for exploration in RL. The proposed method trains an RL agent to reformulate queries for improved retrieval results in a natural language environment with slow response times. The agent aims to maximize the expected returned reward by generating reformulations. The environment is treated as a black-box, and the new agent consists of N sub-agents that provide different reformulations for an input query. The reformulation process involves multiple reformulators generating query variations, which are then aggregated to select the best result for the original query. Each sub-agent is trained independently on different data partitions to increase reformulation variability. The training of a new agent begins with partitioning the dataset into subsets, with different partitioning methods explored for performance analysis. The sub-agents are implemented as sequence-to-sequence models. In the implementation, sub-agents are trained on dataset partitions to generate reformulated queries using beam search. REINFORCE is used for training by sampling reformulations instead of beam search. The rank score is calculated based on the rank of results for each reformulation, and the relevance score is predicted using weight matrices and biases. The function f CNN is a CNN encoder followed by average pooling over the sequence. The function f BOW is the average word embeddings of the result. The aggregator is trained with stochastic gradient descent to minimize cross-entropy loss. Experiments and results in a document retrieval task are presented. The goal is to rewrite a query to increase the number of relevant documents retrieved by a search engine. Architecture details and hyperparameters can be found in Appendix B. The environment receives a query, returns a list of documents, and computes a reward using ground truth documents. Lucene 2 with BM25 is used as the search engine. Three datasets are used for training and evaluation, including TREC-CAR with input queries as concatenated Wikipedia article titles and section titles. The dataset has predefined folds for training and validation sets. The dataset includes Jeopardy and MSA datasets introduced by Nogueira & Cho. Jeopardy dataset uses Jeopardy questions with Wikipedia articles as ground-truth answers. MSA dataset consists of academic papers with paper titles as queries and cited papers as ground-truth answers. Recall is used as the reward metric for query reformulation. The curr_chunk discusses the use of different metrics for reward in query optimization, with Recall@40 showing the best performance. Results are reported in MAP, with other metrics in Appendix A. Methods like PRF and RM3 are used for query expansion. The curr_chunk discusses query optimization using a language model with interpolation parameter \u03bb set to 0.65. The N terms with the highest P (t|q 0 ) are used in an expanded query. RL-RNN is a sequence-to-sequence model trained with reinforcement learning. The curr_chunk discusses training N RL-RNN agents with different initial weights on the full training set for query optimization. Two variants of the proposed method, RL-N-FULL and RL-N-BAGGING, are evaluated. RL-N-BAGGING involves constructing the training set of each RL-RNN agent by sampling with replacement from the full training set. The proposed agent RL-N-SUB is similar to RL-N-FULL but trains multiple sub-agents on random partitions of the dataset. BERT AGGREGATOR replaces the simple aggregator with BERT for better performance in textual tasks. The BERT model is used for binary classification, ranking documents based on their probability of being correct. The final list of documents is obtained by ranking them based on probabilities of being correct. Two variants of the system are compared against the best non-neural reformulation method. The document retrieval results are summarized in Table 1. The number of floating point operations used to train a model is estimated. Sub-agents are frozen during the training of the aggregator to pre-compute tuples from the training set. The proposed methods (RL-10-{Sub, Bagging, Full}) show 20-60% relative performance improvement over the standard ensemble (RL-10-Ensemble) while training ten times faster. RL-10-Sub performs better than the single-agent version (RL-RNN) with the same computational budget and trains faster. RL-10-Sub (Pretrained) achieves the best balance between performance and training cost across all datasets. An RL-10-Full with an ensemble of 10 aggregators yields a 20% relative performance improvement compared to the top-performing system in the TREC-CAR 2017 Track. By replacing the aggregator with BERT, performance improved by 50-100% in all datasets. Without reformulation agents, performance dropped by 3-10%. The system's performance remained stable with more than ten sub-agents. Additional experiments were conducted in a question-answering task to assess the method's effectiveness. In a question-answering task, experiments were conducted comparing an agent with active question answering proposed by Buck et al. The environment receives a question, returns an answer, and computes a reward against a ground truth answer using BiDAF or BERT. Training and evaluation were done on the SearchQA dataset containing Jeopardy! clues as questions with correct answers and snippets. The training, validation, and test sets contain a total of 99,820, 13,393, and 27,248 examples. The agent is compared against various baselines and benchmarks, including BIDAF/BERT and AQA models. AQA-N-{FULL, SUB} models use AQA reformulators as sub-agents followed by an aggregator to create different models. The AQA-N-Full and AQA-N-Sub models were created using sub-agents and an aggregator trained on different partitions of the dataset. The main results on the question-answering task were presented, showing improved F1 and oracle performances compared to BiDAF. The proposed AQA-10-{Full, Sub} method outperforms single-agent AQA in F1 and oracle performances, achieving state-of-the-art on SearchQA with BERT. However, the reformulation strategy (BERT + AQA-10-Sub) did not improve results. There is potential for improvement as the oracle performance is higher than BERT alone, but the answers lack sufficient information for the aggregator to distinguish good from bad answers. Giving context to the aggregator did not yield successful results in experiments. The study evaluates the relationship between query diversity and performance using four metrics. Multiple agents trained on partitions of the dataset produce more diverse queries, leading to higher performance. Training multiple sub-agents on data partitions using reinforcement learning improves query reformulation system. The study evaluates the effectiveness of training sub-agents on data partitions using reinforcement learning and an aggregator to combine their answers for document retrieval and question answering tasks. Results are reported on standard TREC evaluation measures, including RPrecision, Mean-average Precision, Reciprocal Rank, Normalize Discounted Cumulative Gain, and Recall@40. An interesting extension would be to introduce diversity in the beam search decoder to analyze the impact on system capacity and reformulation diversity. The encoder architectures and training details for TREC-CAR, Jeopardy, and MSA tasks are described. Different CNN configurations with specific filter sizes and kernels are used, along with optimizer settings like ADAM and SGD. The Aggregator is evaluated in combination with reformulators using beam search or sampling techniques for document re-ranking. The proposed method involves using multiple reformulators to produce diverse rewrites, which outperforms using a single agent with sampled rewrites. The effectiveness of the aggregation function is validated on the TREC-CAR dataset, showing a drop in performance when changing the rank or relevance score functions. The study experimented with using multiple reformulators to generate diverse rewrites, showing improved performance compared to using a single reformulator. The rank of a document from the reformulation phase was found to be a helpful signal for the re-ranking phase. Additionally, adding vectors to represent sub-agents did not result in performance improvement. The results on various metrics and the impact of multiple reformulators versus aggregator contribution were presented. Using a single reformulator with the aggregator slightly improved performance, while employing ten reformulators with the aggregator led to better results, indicating the importance of a diverse pool of reformulators. The study found that using multiple reformulators led to better performance, with the pool of diverse reformulators responsible for most of the gains. Comparing the proposed method to reinforcement learning algorithms using non-linear function approximators, the method showed reduced variance and higher performance. The method was viewed as an ensemble, with smaller variance and better performance than a single agent or ensemble. The study found that using multiple reformulators led to better performance due to higher stability from multiple agents. Metrics for query diversity analysis were defined, including PCOS, PBLEU, PINC, and LENGTH STD. Sub-agents were trained on random data partitioning throughout the paper. In this study, sub-agents were trained on random partitions of the dataset. Different data partitioning strategies were compared, including a mini-batch K-means clustering algorithm. Three types of features were experimented with for K-means clustering. A greedy cluster balancing algorithm was used to address the issue of unbalanced clusters. Evaluation metrics were defined to assess the effect of partitioning strategies on sub-agents trained on different partitions. These metrics include out-of-partition score and out-of-partition variance, which measure the generalization capability and performance variation of sub-agents across partitions. The out-of-partition error measures the generalization gap between sub-agents trained on different partitions. Results show that clustering-based strategy is sensitive to feature choice, while random strategy performs consistently well. Sub-agents are important for optimal performance in tasks like SearchQA and TREC-CAR. The importance of sub-agents that are not overly specialized in their partitions is highlighted in comparing Q and Q+A for SearchQA. The absolute performance of sub-agents alone may not reflect final performance accurately, as seen in TREC-CAR. The proposed method (AQA-10-Sub) outperforms other methods in some reformulation examples, but struggles in others. Despite diverse reformulations, BiDAF still returns correct answers, but the aggregator may not assign high scores to them."
}