{
    "title": "Hyg9anEFPS",
    "content": "We propose a learned image-guided rendering technique that combines image-based rendering and GAN-based image synthesis to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications. Our method focuses on handling view-dependent effects by training a deep neural network to synthesize the appearance of an object from different viewpoints. EffectsNet is a deep neural network that predicts view-dependent effects in image-based rendering. It converts observed images to diffuse images, which can be projected into other views. A composition network is used to combine reprojected images for photo-realistic results in virtual and augmented reality applications. Our approach focuses on combining captured images for realistic 3D shape reconstruction. It addresses challenges like inaccuracies in surface models and view-dependent appearance effects. The goal is to improve re-rendering of objects in virtual environments. Diffuse textures are often used for novel viewpoint synthesis, lacking view-dependent appearance effects. Image-based rendering (IBR) techniques blend input images on the shape to approximate view-dependent effects, but may result in ghosting artifacts. Some algorithms combine view blending with optical flow correction or view-specific geometry to reduce these artifacts. Hedman et al. (2018) utilizes a deep neural network for per-pixel blending prediction. Our approach combines image-based rendering with deep learning advancements to output photo-realistic images and videos by handling view-dependent effects. Using a short video of an object, we reconstruct the geometry and predict view-dependent effects with our EffectsNet architecture. This allows us to subtract view-dependent effects from original images, reproject them into the target image space, estimate new view-dependent effects, and blend the images using an encoder-decoder network. EffectsNet is a neural network trained to estimate view-dependent effects like specular highlights or reflections, allowing for their removal from input images. The view-independent information obtained is then projected into a new view using reconstructed geometry. CompositionNet, another network, composites the projected images to generate photo-realistic output, resolving reprojection errors. Our algorithm combines classical image-based rendering with machine learning to handle view-dependent effects in source and target views. EffectsNet and CompositionNet are used to generate photo-realistic output images without hand-crafted blending schemes. Additionally, our approach utilizes multi-view 3D reconstruction based on COLMAP for geometric proxy generation. In the field of image-based 3D reconstruction, progress has been made in obtaining large-scale 3D models from internet images. Camera poses and calibration parameters are estimated using structure-from-motion and global bundle adjustment. A dense 3D point cloud is generated using multi-view stereo reconstruction, followed by surface mesh triangulation. Deep learning methods can enhance image quality, including reconstructing specular objects. Learning-based Image Synthesis involves using deep learning methods to improve image quality in various synthesis tasks. Generator networks like U-Net with skip connections and adversarially trained networks have shown promising results. Examples include generative CNN models for body appearance, articulation, pose, and face rendering. Approaches like DeepStereo and view synthesis by Tulsiani et al. have also been proposed. View synthesis is used as a proxy task to learn a layered scene representation from a large set of posed images. Appearance Flow utilizes image warping based on a dense flow field to map information from input to target views. CNNs trained for image-to-image translation can also be applied to novel view synthesis with the assistance of a shape proxy. This approach is related to image-based rendering algorithms that cross-project input views to the target via a geometry proxy and blend the re-projected views. Many IBR approaches aim to reduce artifacts by combining view blending and optical flow correction, using view-specific geometry proxies, or encoding uncertainty in geometry. Hedman et al. (2018) propose a hybrid approach between IBR and learning-based image synthesis, while our learned IBR method focuses on combining input views and separating view-dependent effects for better reproduction of appearance. Intrinsic decomposition addresses the challenge of splitting an image into layers representing physical properties like surface reflectance, diffuse shading, and specular shading. Various methods exist for decomposing monocular video, including hand-crafted priors, specular highlight estimation, and multi-view image-based separation. Wu et al. (2018) introduced a learning-based approach to convert multi-view images of specular objects into diffuse images. The text discusses a learning-based image-guided rendering approach for novel view synthesis of objects. It involves obtaining camera pose estimates and proxy geometry, rendering synthetic depth maps, and generating novel views based on a coverage-based look-up method. Our approach involves selecting reference images and predicting view-dependent effects for target views. The view-independent components are obtained through subtraction and warped to the target view using crossprojection. View-dependent effects are added to the warped views, and CompositionNet combines them for the final output. The training corpus includes images and depth maps per object with constant light, trained object-specifically from scratch each time. The training corpus consists of N images and depth maps per object with constant light. EffectsNet is trained in a self-supervised manner using a Siamese scheme to predict view-dependent effects. The network is optimized by minimizing the difference between diffuse images in the valid region. Photo-realistic synthetic imagery is generated using the Mitsuba Renderer for each of the N views with a resolution of 512 \u00d7 512. The training data consists of images captured with a Nikon D5300, recorded at 30Hz frame rate, and reconstructed using COLMAP. The target object is isolated and a Poisson reconstruction step is applied to extract the surface, generating synthetic depth maps and color images cropped to 512 \u00d7 512 resolution. The training corpus ranges from 1000 to 1800 frames. The main contribution is a convolutional neural network that learns view-dependent and view-independent effects in a self-supervised manner. The network extracts lighting effects based on geometric information from the proxy geometry using a Siamese network. The training data consists of images taken from different viewing directions, with a resolution of 512 \u00d7 512 pixels. The network extracts lighting effects based on geometric information from the proxy geometry using a Siamese network. It converts depth images to world space using camera parameters and generates normal maps and reflected viewing directions. The network architecture is an encoder-decoder network with skip connections, similar to U-Net, with 6 convolution layers outputting different dimensional feature maps. The network architecture consists of encoder-decoder layers with skip connections, outputting different dimensional feature maps. The network is trained in a self-supervised manner using a Siamese network to predict view-dependent effects based on constant illumination assumptions. The network architecture consists of encoder-decoder layers with skip connections, outputting different dimensional feature maps. The network is trained in a self-supervised manner using a Siamese network to predict view-dependent effects based on constant illumination assumptions. The difference of the diffuse aligned images is minimized by aligning pairs of input images and training the network to minimize differences in the overlap region. The self-supervised loss for a training sample is expressed with a binary mask and a regularizer to estimate view-dependent effects. In our experiments, the regularizer is weighted at 0.01. The crossprojection W p q between images p and q is determined by geometric proxy. To create a new target view, we choose a subset of K = 4 images through coverage-based nearest neighbor search from a set of reference views (n = 20). EffectsNet is used to estimate view-dependent effects and compute diffuse images, which are then crossprojected to the target view using depth maps. The view-dependent effects in the target image space are predicted based on the known depth map. The reprojected diffuse images with new effects are inputted into CompositionNet, which merges information from nearest neighbor images into a single output image. The EffectsNet is utilized for robust re-projection of view-dependent effects by selecting K nearest neighbor frames based on surface coverage for maximum texture information. View selection is iterative, maximizing surface coverage by cross-projecting samples from the target view to captured images. The selection of views for robust re-projection is based on geometry and camera parameters. A sample point is considered covered if visible from another viewpoint, determined by occlusion check. The process repeats until K best views are chosen, limited to a subset of 20 reference images from the training corpus. Views are selected for maximum coverage in an iterative manner, independent of the test phase. Cross-projection is modeled based on geometric proxy and camera parameters. The process involves mapping screen space points from one view to another using geometric proxy and camera parameters. Color information is transferred by cross-projecting valid pixels with depth estimates and sampling color based on bilinear interpolation. Occluded points are invalidated based on depth tests. Multiple images matching the target view are obtained by applying cross-projection to nearest neighbor images. In the image-based rendering process, view-dependent effects are addressed using EffectsNet to estimate and subtract them before re-projection. CompositionNet is then used to fuse the warped nearest views, creating a final image output. The encoder-decoder architecture uses convolution layers with different dimensional feature maps and activation functions. Transposed convolutions are employed in the decoder to mirror the encoder. A final convolution layer with a Sigmoid activation function outputs the final image. Adversarial loss is used to measure the difference between predicted and ground truth images. The main contribution of our work is combining IBR and 2D GANs to generate temporally-stable view changes. We analyze our approach qualitatively and quantitatively, comparing it to IBR and 2D GAN-based methods. Experiments used 4 views per frame from 20 reference views, with image reconstruction errors detailed in Tab. 1 in the appendix. The advantages of our approach are highlighted in the supplemental video, showcasing temporal coherence. Synthetic data is used for quantitative analysis of our image-based rendering approach, with a detailed ablation study in Appendix A.2.1. Comparisons are made with classical and learned image-based rendering techniques, particularly focusing on the EffectsNet component. Results show smoother specular highlights and sharper details, with a MSE improvement when EffectsNet is utilized. Real data experiments demonstrate the effectiveness of EffectsNet in estimating specular effects in images. The curr_chunk discusses the removal of specular highlights from an image using a neural network approach, comparing it to Pix2Pix and a state-of-the-art image-based rendering technique by Hedman et al. Results show higher quality with the proposed method. The MSE for DeepBlending is 45.07, InsideOut is 51.17, and the proposed method achieves an error of 25.24. The approach discussed in the curr_chunk uses EffectsNet to remove view-dependent effects in source views and add new effects in target views, resulting in improved rendering quality. The method is trained in an object-specific manner, ensuring optimal results. Training object-specific networks like EffectsNet and CompositionNet is feasible within a similar timeframe as multi-view stereo reconstruction. The approach utilizes EffectsNet and CompositionNet to enhance rendering quality by removing view-dependent effects and adding new effects. Inference time is 50Hz for EffectsNet and 10Hz for CompositionNet on an Nvidia 1080Ti. The method relies on a large training dataset and gracefully degrades when stereo reconstruction fails. Comparison with other methods shows a more uniform error distribution. The paper proposes a novel image-guided rendering approach that produces photo-realistic images of objects, demonstrating its effectiveness in various experiments. Comparisons with other methods show comparable or superior results, particularly in handling view-dependent effects with EffectsNet. The training corpus includes synthetic objects with diverse material properties and shapes. In this study, synthetic data is used to analyze the performance of an image-based rendering approach. The ground truth depth maps are included in the synthetic data, while real data relies on reconstructed geometry, leading to higher photo-metric error. Various rendering methods are compared, with the proposed approach showing promising results in producing photo-realistic images. Our image-based rendering approach is compared to two baseline methods, including a na\u00efve IBR method and the IBR method of Debevec et al. Our method outperforms the classical IBR techniques in reproducing view-dependent effects realistically and smoothly. Additionally, our method is able to in-paint occluded regions, as shown in the qualitative comparison of our predicted diffuse texture to the ground truth. The training set contained 4900 images. Our method outperforms classical IBR techniques in reproducing view-dependent effects realistically and smoothly. It can also in-paint occluded regions. Comparison to Pix2Pix shows our method has a lower MSE of 1.12 on a test set of 190 images, while Pix2Pix results in a higher MSE of 18.66. Our method outperforms Pix2Pix in image generation with a lower MSE of 1.12 compared to Pix2Pix's MSE of 18.66. Additionally, our technique handles reduction in training data size better, leading to higher quality results. Pix2Pix struggles with detailed sequences like the vase, resulting in significantly higher errors. Our method outperforms Pix2Pix in image generation with a lower MSE of 1.12 compared to Pix2Pix's MSE of 18.66. It handles reduction in training data size better, leading to higher quality results. Comparison to Texture-based Rendering shows that our approach captures view-dependent effects more effectively than methods using static vertex colors or textures. Our method outperforms Pix2Pix in image generation with a lower MSE of 1.12 compared to Pix2Pix's MSE of 18.66. It captures view-dependent effects effectively by reproducing specular effects and producing sharper results than the ground truth. The network reproduces the appearance of the training corpus, resulting in images with minimal motion blur. Image synthesis on real data: a comparison to the IBR technique of Debevec et al. (1998) shows reconstructed geometry, IBR result, our result, and ground truth. The network minimizes motion blur in generated images."
}