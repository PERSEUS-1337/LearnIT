{
    "title": "r1VVsebAZ",
    "content": "Using Generative Adversarial Networks (GANs), we simulated neural activity patterns to study information processing. The Wasserstein-GAN variant was adapted to generate diverse neural population activity while sharing parameters in the temporal domain. Spike-GAN, a GAN model, accurately generates spike trains matching first- and second-order statistics of neuron datasets. It performs comparably to existing methods on real salamander retina data without needing predefined statistics. Spike-GAN can create 'importance maps' to identify key statistical structures in spike trains. Spike-GAN is a powerful tool for generating realistic spiking neural activity and describing key features of neural population recordings in systems neuroscience. Generating synthetic spike trains is crucial for systems neuroscience, with uses in computational neuroscience including creating biologically consistent inputs for neural network simulations, generating datasets for analysis techniques, and estimating neural response probabilities. In experimental systems neuroscience, realistic neural population patterns help understand encoding strategies and design closed-loop experiments. Generating realistic spike trains is essential for systems neuroscience, used in computational neuroscience for neural network simulations and analysis techniques. Methods include renewal processes, latent variable models, and maximum entropy approaches to model spiking activity. Other approaches focus on linear stimulus selectivity and generating single trial spike trains using simple neural models. Deep-learning methods can be used to model neural activity in response to stimuli without explicitly specifying spike train statistics. This approach offers advantages over traditional models that may not accurately represent neural variability across different cortical areas. Generative Adversarial Networks (GANs) are explored for modeling neural activity without the need for labeled data. GANs can fit distributions with multiple modes, crucial for capturing the variability in spatio-temporal patterns of population activity in response to stimuli. This method generates realistic samples of neural data. In this work, the authors extend the GAN framework to synthesize realistic neural activity by adapting the Wasserstein-GAN (WGAN) and modifying the network architecture to model invariance in the temporal dimension while maintaining dense connectivity across modeled neurons. The proposed Spike-GAN aims to generate sharp, realistic samples of neural data, leveraging the scalability of deep neural networks to handle large datasets from experimental methods like chronic multi-electrode and optical recording techniques. The proposed Spike-GAN, based on the GAN framework, can generate realistic spike trains matching neuron statistics. It is applied to real data from the salamander retina and compared to other models. A new procedure is described to detect specific features in activity patterns. The adapted GANs simulate spiking activity of N neurons as binary vectors. In this work, the Wasserstein-GAN variant was used to improve the stability of GANs. The Earth-Mover distance between the original distribution and the generator distribution was minimized, ensuring a meaningful gradient for stability. A further enhancement was introduced to ensure the critic is Lipschitz, improving the architecture for simulating realistic neural population activity patterns. Our proposed GAN architecture utilizes a modified 1D-DCGAN to take advantage of temporal invariance in neural population activity patterns. The spatial dimension is transposed to correspond to the channel dimension, allowing for densely connected spatial dimensions with shared weights across the temporal dimension. The proposed GAN architecture improves training efficiency by utilizing densely connected spatial dimensions with shared weights across the temporal dimension. Key modifications include using LeakyReLU units, a specific architecture for the critic and generator, and addressing the checkerboard issue by dividing fractional-strided convolutions into upsampling and convolving steps. The Spike-GAN network was trained using mini-batch stochastic gradient descent with specific hyperparameters. The critic was updated 5 times for each generator update, and code and hyperparameters can be found on GitHub. Generated samples were compared to the ground truth dataset by discretizing them and drawing final values from a Bernoulli distribution. The Spike-GAN network was trained using specific hyperparameters and updated using mini-batch stochastic gradient descent. The model's performance was evaluated by measuring spike train statistics commonly used in neuroscience, including average spike count per neuron, average firing probability, covariance between neuron pairs, lag-covariance, and distribution of synchrony. The Spike-GAN network was trained with specific hyperparameters and evaluated using spike train statistics commonly used in neuroscience, including spike autocorrelogram and approximating second-order statistics with correlated neuron pairs. The Spike-GAN network was trained on 8192 samples for 500000 iterations, with correlations between neuron pairs and temporal correlations from neuronal biophysics. The resulting patterns were mostly binary, with a small fraction of intermediate values. Performance was evaluated using spike train statistics and second-order statistics with correlated neuron pairs. Spike-GAN was compared to a 4-layer MLP GAN in terms of spike train statistics commonly used in neuroscience. Spike-GAN showed better approximation of features involving time due to weight sharing along the temporal dimension, allowing it to learn temporally invariant features more easily. Spike-GAN effectively mimics the underlying distribution of training data and was tested on real recordings from retinal ganglion cells. The dataset included responses from 160 RGCs to natural stimuli, with 50 neurons randomly selected for analysis. The activity was partitioned into non-overlapping samples, yielding 8817 training samples. Results were almost identical for a different set of 50 neurons. In comparison to existing methods, Spike-GAN was tested on real retinal ganglion cell data. The dataset included responses from 160 RGCs, with 50 neurons randomly selected for analysis. Maximum entropy models were used to fit the dataset, including the k-pairwise model and a dichotomized Gaussian method. These models aim to constrain the neural population's activity by fitting specific statistics. The method developed by Lyamzin et al. (2010) is an extension of previous approaches in which signal and noise correlations are modeled separately. Unlike the k-pairwise model, the Dichotomized Gaussian (DG) model can fit temporal correlations and was tested for signs of overfitting by comparing generated samples with the training dataset. The generator in a GAN obtains information about the dataset through the critic, not directly from the training dataset. The closest samples in the training dataset are different from the generated ones. Different models provide a good approximation of firing rate, covariance, and k-statistics, with MaxEnt and DG models fitting tighter than Spike-GAN. Spike-GAN performs well without needing specific statistical structures to be manually specified. The k-pairwise model ignores temporal dynamics and neural features, failing to approximate autocorrelogram and lag-covariances. Spike-GAN and DG model perform well in fitting dataset statistics, with DG struggling with negative correlations. DG model matches positive correlations perfectly in the dataset. Spike-GAN generates samples comparable to state-of-the-art methods without defining important features of the dataset's probability distribution. Trained critic reveals population activity patterns in the original dataset involving stereotyped activation patterns called packets, fundamental for cortical coding. Spike-GAN was trained on neural patterns involving packets of 8 neurons each, demonstrating its capability to approximate complex statistical structures in neuroscience. Real neural population activity is challenging to interpret due to noise and non-sorted neurons within packets. Spike-GAN was trained on realistic neural patterns with noisy spikes and cluttered packets. The filters learned by the model suggest it can detect the specific structure of packets. Zeiler & Fergus (2014) proposed a method to analyze the relevance of different parts of input for neural networks. The study adapted the idea of analyzing the relevance of features in neural activity patterns by shuffling spikes emitted by neurons and computing the output differences. This process helps in determining the importance of the structure of spike trains. Importance maps are generated for each neuron to highlight the significance of specific spike trains. The study analyzed the relevance of features in neural activity patterns by shuffling spikes emitted by neurons to determine the importance of spike train structures. Importance maps were generated for each neuron to highlight the significance of specific spike trains, showing that the importance-map analysis is a useful tool. The importance-map analysis is a useful procedure to detect key aspects of neural population activity patterns. An application of Generative Adversarial Networks was explored to synthesize neural responses that mimic activity patterns. The study included examples of different packets highlighted in patterns and realistic neural population patterns. The Spike-GAN model was trained to generate importance maps showing the impact of disrupting specific spikes on the critic's output. These maps were obtained by selectively shuffling neuron activity at different time periods using a sliding window. The average importance maps across neurons and time dimensions were calculated, with error bars indicating standard error. The Spike-GAN model was adapted from the WGAN variant to allow for sharing in the population of neurons. Spike-GAN, a variant of WGAN, replicates neural activity statistics without handcrafted assumptions. LFADS is a deep learning method for modeling neuron activity dynamics, complementing Spike-GAN by inferring population dynamics. The GAN-based approach proposed in BID1 fits network models to experimental data of tuning curves from neurons. This work is the first to use GANs to simulate realistic neural patterns of neuron populations. Spike-GAN can visualize features of the training dataset, highlighting important spikes in generating activity motifs for unsupervised identification of neural representations. The Spike-GAN model aims to identify salient low-dimensional representations of neural activity for interpreting experimental results and designing realistic patterns of neural stimulation. It can generate realistic neural activity and identify key features, making it a valuable tool for designing perturbations in neural populations. Importance maps generated by Spike-GAN can infer the neurons involved in encoding information about stimuli and the spatio-temporal structure of neural activity packets. Spike-GAN is compared with MaxEnt and DG models for analyzing neural activity. Spike-GAN is more flexible, does not require prior assumptions, and benefits from deep neural network advancements. Spike-GAN benefits from engineering advances in neural activity analysis, enabling better use of large datasets. The study investigated how well Spike-GAN fits probability density functions of population activity patterns, using ground truth and numerical probabilities. Spike-GAN was trained using a small dataset from the same probability distribution as the ground truth and surrogate datasets. The generated dataset showed similar entropy to the ground truth and surrogate datasets, indicating no mode collapse. The study compared sample probabilities of the generated and surrogate datasets to the numerical probabilities, showing that Spike-GAN's deviation from the ground truth distribution was due to finite sampling effects. 44% of Spike-GAN samples were from the training dataset, with 56% generated de novo. 3.2% of generated samples had a numerical probability of 0, similar to the surrogate dataset. These results suggest Spike-GAN has effectively learned. Spike-GAN has learned the probability distribution of the training dataset, suggesting it can interpret neural activity features relevant for behavioral discrimination. A hypothetical experiment simulating a mouse discriminating stimuli was used to illustrate this approach. In a hypothetical experiment, two-photon calcium imaging records neural activity in V1 neurons of a mouse responding to stimuli associated with distinct behavioral outcomes. The mouse, previously trained on the task, shows activity patterns related to stimulus orientation. Calcium signals can be binarized for analysis despite not being binary-valued. The two stimuli in the experiment evoke distinct activity patterns in V1 neurons of a mouse. Spike-GAN is trained on recorded population responses to compute importance maps for each sample. The approximated packets and original ones show high correlation (r \u2265 0.8). The importance of neurons in encoding stimuli can be identified by averaging importance maps across time and trials. Neurons with the highest importance values participate in encoding specific stimuli, as shown in the bimodal distribution in FIG1. A threshold can be set to select these neurons, with recall and precision values shown in FIG1. Importance maps were also evaluated in noisier scenarios with noisy spike timing. The importance maps analysis provides valuable information about neurons encoding stimulus information and their timing, even in scenarios with noisy spike timing or limited training samples. These features identified by Spike-GAN may or may not be used by the animal for decision-making. To test this, one can replace sensory stimuli with optogenetic patterns during a discrimination task, comparing the behavioral responses when keeping or shuffling the highlighted features. The behavioral report of animals can be used to determine if specific features are consistently used in perception. Stimulating only part of neurons can test the brain's ability to fill in missing information. Altering the timing of neuron stimulation can study the role of spike timing in stimulus encoding. Probability distributions can be compared between ground truth and generated datasets for analysis. FIG1 shows the use of importance maps in a real experiment using two-photon microscopy to manipulate and record neuron activity in response to different stimuli. Simulated data presents two distinct neuron packets responding to the stimuli. Probability distributions are compared between ground truth and generated datasets. The presentation evokes a single packet in response to two different stimuli. Actual recorded patterns of activity and importance maps obtained after training Spike-GAN with 4096 samples are shown. Approximated packets are obtained by thresholding the importance maps and aligning them to the first spike above the threshold. The importance of each neuron can be inferred by averaging importance maps across time and trials. Neurons participating in the encoding of each stimulus are identified. The importance of neurons participating in encoding stimuli is more than three times that of non-participating neurons. Precision and recall values are computed based on a threshold for selecting participating neurons. Negative critic loss is shown during Spike-GAN training on samples from a population of 16 neurons. Sample similarity is measured using L1 distance, with non-matching spikes penalized. The Spike-GAN model was trained on a dataset with noise added to spike times, resulting in noisier inferred packets but still maintaining high correlation with ground truth packets. Correlation was computed with the average of all noisy packets in the training dataset to consider response uncertainty. The Spike-GAN model was trained on a dataset with noise added to spike times to account for response uncertainty. The structure of ground truth packets was shown with green pluses, while neurons' importance in encoding stimuli was indicated by red and blue markers. The precision and recall values for detecting relevant neurons were high, with a recall of 0.89 and precision \u2265 0.94."
}