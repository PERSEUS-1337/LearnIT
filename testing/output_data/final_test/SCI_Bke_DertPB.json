{
    "title": "Bke_DertPB",
    "content": "Generative adversarial networks (GANs) are popular for training generative models, with Wasserstein GANs being superior in stability and sample quality. They require a 1-Lipschitz critic, enforced through gradient penalization or weight normalization. Adversarial Lipschitz Regularization proposes an explicit Lipschitz penalty for improved performance in Wasserstein GANs. Wasserstein GANs are state-of-the-art in generative modeling tasks, replacing the discriminator with a critic to approximate the Wasserstein distance between real and generated distributions. This introduces a new challenge as Wasserstein distance estimation requires the function space of the critic. The Wasserstein GANs use a critic with 1-Lipschitz functions for distance estimation. Gradient Penalty replaced weight clipping for enforcing Lipschitz constraint. Variants of gradient norm penalization have been introduced. Virtual Adversarial Training improves robustness against perturbations by approximating adversarial directions. Adversarial Lipschitz Regularization (ALR) is a method proposed to train neural networks by penalizing the violation of the Lipschitz constraint explicitly. It focuses on enforcing Lipschitz continuity in complex models, particularly in Wasserstein GANs to improve training stability and reduce mode collapse. Source code for experiments is available at https://github.com/dterjek/adversarial_lipschitz_regularization. Adversarial Lipschitz Regularization (ALR) is proposed to enforce Lipschitz continuity in neural networks, particularly in Wasserstein GANs. ALR results in Adversarial Lipschitz Penalty (ALP), which when applied to the critic in WGAN (WGAN-ALP), shows state-of-the-art performance in Inception Score and Fr\u00e9chet Inception Distance on CIFAR-10. It also performs competitively in high-dimensional settings with Progressive Growing GAN on CelebA-HQ. Generative adversarial networks (GANs) use a generator network g to transform samples from a latent space Z to the data space X, trained alongside a discriminator network f to distinguish between fake and real samples. The training procedure of GANs involves a discriminator network trained to distinguish between fake and real samples, providing feedback to the generator network. Wasserstein GAN (WGAN) addresses distribution differences by using the Wasserstein metric, which offers better stability and theoretical properties compared to the Jensen-Shannon divergence. The Wasserstein-p distance offers better theoretical and practical properties compared to the Jensen-Shannon divergence. It provides a smooth distance even for distributions with non-overlapping supports, raising a metric from the space of supports to the space of probability distributions. The optimal coupling of two distributions is achieved through the optimal \u03c0, with an equivalent formulation known as the Kantorovich-Rubinstein formula. The Wasserstein-1 distance corresponds to the supremum over all 1-Lipschitz potential functions in the set of functions that are 1-Lipschitz with respect to the ground metric. In WGAN, the critic approximates this distance between the generator and real distributions, leading to more stable training and improved sample quality. Techniques to restrict the smallest Lipschitz constant of the critic have sparked the development of Lipschitz regularization methods for neural networks. Theoretical properties of neural networks with low Lipschitz constants were explored in various studies, showing improved generalization. Enforcing Lipschitz constraints in deep learning, particularly with WGAN, involved clipping the weights of the network to achieve better results. However, this approach sometimes led to poor quality samples or convergence issues. The Gradient Penalty (GP) method was introduced as a softened alternative to clipping weights in WGAN to enforce Lipschitz constraints. It aims to ensure unit gradient norm along lines connecting paired points from the induced distribution, improving critic regularization and other optimal solution properties. GP is applied on samples from the distribution by interpolating pairs of marginals, enhancing the WGAN-GP formulation. Theoretical arguments against GP were raised by Petzka et al. and Gemici et al., questioning the validity of unit gradient norm on interpolated samples. They highlighted that the regularizing effect of GP may be too strong due to the lack of optimal coupling and differentiability assumptions. A suggested solution is to penalize the violation of the Lipschitz constraint explicitly or implicitly. The first method has only shown success on toy datasets and performed poorly on larger datasets. The Lipschitz Penalty (LP) was proposed as an alternative to the Gradient Penalty (GP) for regularization in deep learning. LP penalizes the gradient norm only when it exceeds 1, unlike GP. Other methods like dropout in the critic and spectral normalization (SN) have also been suggested for Lipschitz regularization. Spectral normalization (SN) and other techniques like LP and dropout are proposed for Lipschitz regularization in neural networks. SN enforces a Lipschitz constraint with respect to the 2-norm per layer, while other approaches like LP and VAT aim to preserve the norm of the gradient during backpropagation. Virtual Adversarial Training (VAT) is a semi-supervised learning method that regularizes networks to be robust against local adversarial perturbations. It uses virtual adversarial perturbations to define a direction for each sample point, called the virtual adversarial direction. VAT utilizes unlabeled data with virtual labels assigned by the network being trained, and its regularization term is known as Local Distributional Smoothness (LDS). Adler and Lunz (2018) proposed adversarial Lipschitz regularization, arguing that penalizing the norm of the gradient is more effective than penalizing the Lipschitz quotient directly. This regularization method aims to make networks robust against adversarial perturbations by minimizing the divergence between distributions p and p. The virtual adversarial perturbation is defined by a hyperparameter and approximated through power iteration. Miyato et al. (2019) found that one iteration is often sufficient in practical scenarios. Adler and Lunz (2018) proposed adversarial Lipschitz regularization, arguing that penalizing the norm of the gradient is more effective than penalizing the Lipschitz quotient directly. They suggest that a carefully chosen sampling strategy can make the explicit penalty favorable over the implicit one. The network f is considered K-Lipschitz if the supremum over r in the given mapping results in a specific condition. Adversarial Lipschitz Regularization (ALR) involves adding a regularization term to penalize the violation of the Lipschitz constraint in the training objective. This term, called Adversarial Lipschitz Penalty (ALP), measures the deviation of the function from being K-Lipschitz around sample points, making the learned mapping approximately K-Lipschitz. ALR involves adding a regularization term to penalize the violation of the Lipschitz constraint in the training objective. The adversarial perturbation is a nonlinear optimization problem, with an approximation of the adversarial direction used to apply the penalty at different scales. The adversarial perturbation in ALR involves approximating the adversarial direction with a translation in the 2-norm, which may not always be accurate due to varying ratios. Different approximation schemes are being explored to improve performance, including methods from VAT, which could be combined with ALR for better results using various metrics. This is important as the Wasserstein distance can be defined with any metric, allowing for flexibility in defining adversarial examples. Adler and Lunz (2018) and Dukler et al. (2019) extended GP to work with metrics other than Euclidean distance by adding Monte Carlo approximations to the training objective. Different approaches were used for WGAN and Progressive GAN, with emphasis on Lipschitz penalties for more general metric spaces. In the semi-supervised setting, only the expectation was added to the training objective. The optimal choices for these scenarios were found through experimentation, but a definitive answer is not provided in the paper. The Lipschitz constant K can be adjusted manually or calculated from labeled data. Hyperparameters k, \u03be, and P need to be chosen carefully to balance the number of adversarial perturbations violating the Lipschitz constraint. Adjusting hyperparameters \u03bb and K can help maintain this balance. If the number of violations is too high, increase K or \u03bb; if too low, adjust the regularization strength or ALR parameterization. ALR is effective in WGANs compared to other methods like gradient norm penalization and weight normalization. Adjusting hyperparameters \u03bb, K, k, \u03be, and P is crucial for balancing Lipschitz constraint violations. ALR provides a softer regularization effect than weight normalization methods and is preferred when batch normalization is used in the network. ALR is preferable in WGANs over other methods like gradient norm penalization and weight normalization. It offers more control over regularization effects with additional hyperparameters to tune. The performance of ALR depends on the speed of approximating r adv, requiring 1 backpropagation step for each power iteration step. ALR has the potential to become computationally cheaper by adopting new techniques for obtaining adversarial examples. Specializing the ALP formula with a critic, it is applied to the WGAN objective to include an explicit penalty using adversarial perturbations as a sampling strategy. The WGAN-ALP formulation uses adversarial perturbations as a sampling strategy, resulting in a stable explicit Lipschitz penalty. It was trained on CIFAR-10 images using a residual architecture and the Adam optimizer with specific parameters. The WGAN-ALP formulation used adversarial perturbations for stability and explicit Lipschitz penalty. Training involved CIFAR-10 images, a residual architecture, and specific Adam optimizer parameters. The training process included 5 steps with a generator for 1 per iteration using minibatches of size 64. The loss function (17) optimized the critic, with K = 1 and \u03bb = 100 being optimal. Hyperparameters for r adv approximation were \u03be = 10, P as uniform distribution over [0.1, 10], and k = 1 power iteration. Inception Score and FID were used as evaluation metrics, monitoring every 1000 samples during training. The study monitored Inception Score and FID during training using 10000 samples every 1000 iterations and evaluated them at the end with 50000 samples. The training was repeated 10 times with different seeds, calculating mean and standard deviation of final scores. Results for WGAN-ALP and other GANs were compared in Table 1. Competing models were not evaluated directly, but values from relevant papers were included. The study compared the performance of WGAN-ALP with other GANs by monitoring Inception Score and FID during training. Results showed that ALR restricts the Lipschitz constant of the critic, with better performance compared to LP. Additionally, when the regularized network contains BN layers, ALR outperformed competing methods. The critic in WGAN-BN does not use BN layers, as argued by Gulrajani et al. (2017). When ALP was applied to WGAN-BN, it produced better results with a maximal Inception Score of 8.71. The question of how BN affects Lipschitz continuity is left for future work. Different penalties in WGAN were discussed by Gulrajani et al. (2017), Petzka et al. (2018), and Gemici et al. (2018). Gemici et al. (2018) proposed a method to approximate optimal coupling in the dual formulation of the optimal transport problem using a WGAN variant with a two-sided explicit penalty. The approach was found to be reasonable but less stable for certain values of \u03bb, as noted by Petzka et al. (2018) for the implicit penalty case. Gemici et al. (2018) proposed a method using a WGAN variant with a two-sided explicit penalty to approximate optimal coupling in the dual formulation of the optimal transport problem. Petzka et al. (2018) found this approach less stable for certain values of \u03bb in the implicit penalty case. To demonstrate ALR in a high-dimensional setting, a Progressive GAN was trained on the CelebA-HQ dataset, with the loss function of the critic replaced with ALP. The training objective was stable until the last stage of progressive growing, where the penalty was adjusted to the sum of absolute and squared values of the Lipschitz constraint violation. Optimal hyperparameters were determined, including \u03bb = 0.1, P as the uniform distribution over [0.1, 100], \u03be = 10, and k = 1 step of power iteration. The ALR method was tested in a high-dimensional setting using a Progressive GAN on the CelebA-HQ dataset. The best FID scores were 8.69 for the original GP version and 14.65 for the modified ALP version. While ALP did not outperform GP in this case, it showed effectiveness in high-dimensional settings. ALR is a powerful regularization method for learning Lipschitz constrained mappings with competitive performance in training WGANs. It draws parallels between Lipschitz regularization and adversarial training, suggesting potential for future research. In a study comparing ALR to VAT in a semi-supervised learning scenario, the ConvLarge architecture was trained on CIFAR-10 images. The training set was split into samples for classification loss, regularization, and validation. It was noted that assuming f(x) fixed during regularization and incorporating entropy minimization were crucial. The baseline VAT method was specialized with specific parameters. After experimenting with different parameters, the VAT method achieved a maximal validation performance of 85.3% and test performance of 83.54%. This slight improvement shows that ALR can be a competitive choice for semi-supervised learning. The VAT method was defined using neural networks for conditional distributions p(y|x), where the distribution over discrete labels y was conditioned on the input image x. The regularization term of VAT, known as LDS, can be viewed as a form of Lipschitz continuity in the context of a mapping from the space of images to the probability simplex. By metrizing the space of images and using metrics like the square root of JSD or Hellinger distance, the network f can be seen as a mapping between metric spaces. The network f is a mapping from metric space X to metric space Y. By setting K to 0, we aim to learn a mapping f with the smallest possible f L. To enforce the condition x + r \u2208 X, we bound the Euclidean norm of r from above. Assuming the supremum is achieved with an r of maximal norm, the formulas reduce to equivalent forms. VAT is a special case of Lipschitz regularization, with f and d Y assumed to be twice differentiable almost everywhere. The method described involves finding the direction of greatest curvature using the second-order Taylor approximation. The eigenvector corresponding to the largest eigenvalue of the Hessian matrix is the adversarial direction sought. Power iteration can be used to calculate this direction. The adversarial direction is approximated using the iterative scheme, with one iteration found to be sufficient and necessary. Weight normalization methods SN, LP, and ALR are compared using a toy example of approximating a real-valued mapping on a 2-dimensional interval. The optimal approximation is 1-Lipschitz, connecting to WGAN's optimal critic being 1-Lipschitz. In this example, the focus is on closely approximating the gradient of the optimal critic in connection to WGAN. Different Lipschitz regularization methods are compared using PyTorch implementation. The networks are trained for 214 iterations with specific architecture choices. The networks were trained for 214 iterations with specific architecture choices. Inputs were randomly drawn from [-4, 4] and outputs were defined accordingly. Heatmaps were used to visualize gradient norm surfaces of optimal and learned mappings. Different Lipschitz regularization methods were compared, showing their distinct competencies. The network learned to approximate the target function well without regularization, but its gradients differ from the optimal. Spectral Normalization (SN) applied to MLP layers results in a smooth mapping with low gradient norm. SN is not compatible with Batch Normalization (BN), providing only a slight improvement over unregularized case. SN regularizes globally instead of around data samples, affecting the entire input space. For WGANs trained on CIFAR-10, the mapping is regularized on a larger space than just the pixel values. The mapping in the network is regularized on a larger space than just the pixel values, with control over the regularization strength by tuning the value of \u03bb. Training with different values of \u03bb and k, both with and without Batch Normalization, shows varying results in the mapping's irregularity. Additional control over regularization is provided by the hyperparameters of the approximation scheme of r adv. The regularization of the mapping in the network is controlled by tuning \u03bb and k values, with and without Batch Normalization. Results show that \u03bb = 1 and k = 5 case is the best choice, with smoother mappings when BN is introduced compared to LP."
}