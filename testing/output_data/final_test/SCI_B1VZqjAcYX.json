{
    "title": "B1VZqjAcYX",
    "content": "Pruning large neural networks at initialization based on connection sensitivity, without the need for iterative optimization or complex pruning schedules. Achieves sparse networks with high accuracy on various classification tasks. Our approach to network pruning focuses on creating smaller subnetworks that maintain performance similar to the original large network. This method is effective on tasks like MNIST, CIFAR-10, and Tiny-ImageNet classification, across different network architectures. The goal is to reduce computational costs and memory requirements without sacrificing performance, making it ideal for real-time applications and resource-limited devices. In this work, a new saliency criterion is introduced to identify important connections in a network before training, aiming to create a smaller subnetwork that maintains performance. This method avoids the need for expensive prune-retrain cycles and heuristic design choices, making it easier to extend to new architectures and tasks. Our approach, called connection sensitivity, prunes redundant connections before training to achieve the desired sparsity level. It offers simplicity, versatility, and interpretability by selecting structurally important connections without the need for pretraining or complex schedules. This method can be applied to various network architectures without modifications. Our method, connection sensitivity, prunes redundant connections before training to achieve sparsity. It is evaluated on MNIST, CIFAR-10, and Tiny-ImageNet datasets, obtaining sparse networks with high accuracy. The relevance of retained connections and the impact of network initialization and dataset on saliency score are investigated. Early works in network pruning can be categorized into those using sparsity penalties and those based on saliency criteria. The loss function incorporates sparsity enforcing penalty terms to penalize weight magnitudes during training. Classical saliency criteria focus on sensitivity of loss with respect to neurons and weights, but are slow due to iterative pruning and learning steps. Our approach identifies and prunes redundant weights before training, addressing complexities and overfitting in deep neural networks. The risk of overfitting in deep neural networks has led to increased investigation in network pruning. Magnitude-based approaches have shown impressive results in achieving extreme sparsity without loss in accuracy, making them the standard method for network pruning. However, these approaches rely on pretraining and expensive pruning cycles, making them less adaptable to new architectures and tasks. Bayesian methods are also being explored for network pruning. Our approach to network pruning is simple and adaptable to any architecture or task without modifying the procedure. Other methods include structured simplification like pruning filters, structured sparsity, low-rank approximation, and sparsification using different techniques. There is also work on compressing weight representations through quantization and reduced precision. Neural network pruning aims to reduce the size of overparametrized networks while maintaining performance. Different methods like quantization and reduced precision are used for weight compression. The objective is to learn a sparse network without compromising accuracy, formulated as an optimization problem with a desired sparsity level. The conventional approach to optimize neural network pruning involves adding sparsity enforcing penalty terms to minimize a constrained optimization problem. However, saliency based methods are more effective in achieving sparsity by selectively removing redundant parameters in the network. Popular criteria for identifying redundant connections include the magnitude of weights. The conventional approach to optimize neural network pruning involves adding sparsity enforcing penalty terms to minimize a constrained optimization problem. Saliency based methods are more effective in achieving sparsity by selectively removing redundant parameters in the network. Criteria for identifying redundant connections include magnitude of weights and Hessian matrix values. Hessian based methods are complex and sensitive to architectural choices. In this work, a new criterion is proposed to measure connection importance in a data-dependent manner, reducing the need for expensive prune-retrain cycles in neural network pruning. This method allows for pruning the network once at the beginning and training on the sparse pruned network, potentially making training an order of magnitude faster than standard methods. The method proposed involves selectively pruning redundant connections in a neural network before training by introducing a criterion to identify important connections. This approach aims to measure connection importance independently of weight, using auxiliary indicator variables to modify the network's sparsity level. The method increases the number of learnable parameters but simplifies optimization by separating connection weight from presence. The method proposed involves selectively pruning redundant connections in a neural network before training by introducing a criterion to identify important connections. By separating the weight of the connection from its presence, the importance of each connection can be determined by measuring its effect on the loss function. This is done by comparing the loss when a connection is active versus pruned, but it is computationally expensive as it requires multiple forward passes over the dataset. By relaxing the binary constraint on indicator variables, the influence of connection j on the loss function can be approximated by the derivative of L with respect to c j. This allows for efficient computation of the rate of change of L with respect to c j, measuring the effect of connection j on the loss. This method perturbs the weight w j by a multiplicative factor \u03b4 to measure the change in loss, similar to BID19's approach of measuring datapoint influence on the loss function. The influence of connections on the loss function is measured by the derivative of L with respect to c j. The magnitude of the derivatives g j is used as the saliency criterion to identify important connections for pruning. High magnitude indicates significant impact on the loss, necessitating preservation for learning on weight w j. Connection sensitivity is defined as the normalized magnitude of the derivatives. Connection sensitivity is determined by the normalized magnitude of derivatives. The top-\u03ba connections are retained based on this sensitivity, with ties broken arbitrarily. This criterion differs from previous works. Pruning involves choosing these top connections for training, aiming to identify elements that least degrade performance when removed. Our saliency criterion, unlike magnitude and Hessian based methods, measures the sensitivity of elements on the loss function regardless of its value. This eliminates the need for pre-training and allows for single-shot pruning before training. The saliency measure depends on weights, dataset, and loss function. The saliency measure depends on weights, dataset, and loss function, and can be used to prune neural networks in a single shot with carefully chosen weights to minimize impact on derivatives. It is important to ensure weights are within a sensible range to avoid uninformative gradients. The saliency measure can be robust to architecture variations and is not dependent on pre-training. The saliency measure in neural networks can be made robust to architecture variations by using variance scaling methods to initialize weights. This ensures that the saliency measure remains consistent throughout the network. By relying on the dataset and loss function, the saliency criterion can identify important connections in the network for the given task. The practitioner can choose to compute the saliency measure using the whole training set, a mini-batch, or the validation set. In contrast to previous approaches, the method SNIP for Single-shot Network Pruning offers a simple criterion based on connection sensitivity to identify and prune redundant connections before training. Using just one mini-batch of training examples can effectively lead to pruning, even in cases of memory limitations. The algorithm for SNIP is provided in Algorithm 1, with Figure 1 showing test errors of pruned LeNets at varying sparsity levels. Our approach, SNIP, prunes LeNets at varying sparsity levels with minimal loss in accuracy across different network architectures. The method prunes explainable connections and is simpler than other alternatives. The sparsity level is defined as (m - \u03ba)/m * 100 (%), where m is the total number of parameters and \u03ba is the desired number of non-zero weights. Sensitivity scores are computed using a batch of examples for MNIST and CIFAR experiments. After pruning, the pruned network is trained using SGD with momentum of 0.9, batch size of 100 for MNIST and 128 for CIFAR experiments, weight decay rate of 0.0005, and initial learning rate of 0.1. The learning rate is decayed at every 25k or 30k iterations for MNIST and CIFAR, respectively. Our algorithm requires no other hyperparameters or complex learning/pruning schedules. 10% of the training data is spared for validation. For CIFAR experiments, standard data augmentation is used. The code can be found at https://github.com/namhoonlee/snip-public. We tested our approach on LeNet-300-100 and LeNet-5-Caffe networks for pruning, achieving similar performance to the reference with negligible loss at high sparsity levels. The saliency measure used does not require pre-training and prunes the networks quickly and effectively. Our approach prunes LeNets effectively at extreme sparsity levels, up to 98% for LeNet-300-100 and 99% for LeNet-5-Caffe with minimal accuracy loss. It outperforms other pruning algorithms in simplicity and performance, requiring no pretraining or additional hyperparameters. Our approach efficiently prunes LeNets at extreme sparsity levels, achieving errors comparable to the reference model with minimal accuracy loss. SNIP outperforms other pruning algorithms in simplicity and performance, requiring no pretraining or additional hyperparameters. Our approach stands out for its simplicity and performance compared to other pruning algorithms. It requires no pretraining or additional hyperparameters, making it cost-effective and efficient. Our approach stands out for its simplicity and performance compared to other pruning algorithms. It requires no pretraining or additional hyperparameters, making it cost-effective and efficient. SNIP prunes the network at initialization, allowing for faster training by focusing on the survived parameters. This approach is applicable to various modern network architectures like AlexNet and VGG models. The VGG models, including VGG-C, VGG-D, and VGG-like, are popular variants adapted for CIFAR-10 with reduced fc layers. The proposed pruning approach shows significant parameter reduction across various modern architectures like AlexNets, VGGs, and WRNs, with minimal impact on performance. Additionally, LSTM and GRU networks are evaluated on the sequential MNIST classification task. The model utilizes one layer RNN networks with LSTM BID42 or GRU BID4 cells of two unit sizes, 128 and 256. It is adapted for sequential MNIST classification task, processing row-by-row instead of pixel-by-pixel. Results show significant parameter reduction in various network models with minimal loss in accuracy. The pruning procedure is versatile and scalable, not requiring modifications for specific architectural variations. The model demonstrates significant parameter reduction in various network models without requiring additional hyperparameters or modifying the pruning procedure. It achieves extreme sparsities on convolutional, residual, and recurrent networks, with no substantial loss in accuracy. The approach can prune deep neural network architectures for extreme sparsities without losing accuracy. It aims to understand which connections are being pruned by visualizing binary connectivity masks. The method computes these masks using a minibatch of examples from the same class. The study visualizes pruned parameters in a neural network by curating mini-batches of examples from the same class. As sparsity increases, irrelevant parts of the image are pruned while discriminative parts for classification are retained. The results are significant for understanding connectivity in deep neural networks. The study demonstrates that pruning unimportant connections in neural networks improves classification accuracy. By analyzing results from MNIST and Fashion-MNIST datasets, it is evident that the method retains important features while removing irrelevant ones. This approach contrasts with traditional pruning methods, making it easier to identify key information for classification tasks. In this section, the study focuses on the effect of data and initialization methods on connection saliency in neural networks. The batch size and initialization methods such as random normal and variance scaling are varied to analyze pruning effects on network performance. Results are shown for LeNets and RNNs on MNIST dataset. The study examines the impact of different initialization methods on connection saliency in neural networks, specifically focusing on LeNets and RNNs on the MNIST dataset. Variance scaling method VS-H outperforms other initializers, especially crucial for complex RNN models like GRU. The results highlight the importance of using variance scaling methods for reliable and model-agnostic saliency measures. The study explores the impact of different batch sizes on the pruning of neural networks, showing how the survived parameters in the first layer of LeNet-300-100 change with increasing batch size. The experiment involves training LeNet-5-Caffe on MNIST with true or randomly shuffled labels to assess the network's ability to memorize the dataset. The results are visualized in Figure 3 and show that as batch size increases, the error decreases, indicating the effectiveness of the pruning method. The study presents the results plotted in FIG2, showing that neural networks can memorize random data but fail when pruned by SNIP. The pruned model can classify MNIST with true labels, highlighting the importance of the saliency criterion. Further analysis on varying \u03ba is provided in Appendix B. The simplicity of SNIP makes it easier for exploration compared to other pruning methods like BID27. In this work, a new approach called SNIP is introduced, which prunes irrelevant connections in neural networks before training. The connection sensitivity measure of SNIP identifies important connections in untrained networks, leading to sparse models. This opens up possibilities for understanding neural network architectures, multi-task transfer learning, and structural regularization. Future research will focus on exploring the generalization capabilities of sparse networks and comparing results with using SNIP on original (Fashion-)MNIST data. The results of using SNIP on original (Fashion-)MNIST show different pruning patterns compared to the original results. Varying sparsity levels affect training loss, indicating that networks with more parameters are more prone to fitting random labels. Pruned models can still perform classification tasks effectively. Tiny-ImageNet 2 is a subset of ImageNet with 200 classes, each having 500 training and 50 validation images. The classification task is more complex than MNIST or CIFAR-10. SNIP can prune parameters with minimal loss in performance, but AlexNet models lose more accuracy than VGGs due to a large first convolution stride."
}