{
    "title": "ByeUBANtvB",
    "content": "Backpropagation is the driving force behind artificial neural networks, but it's unclear if the brain uses this algorithm. Neuroscientists consider reinforcement learning as a realistic alternative, where neurons introduce changes and observe feedback to approximate gradients. A hybrid learning approach is proposed, where each neuron uses RL to learn gradient approximation. The approach converges to the true gradient for certain networks and matches the performance of gradient-based learning in feedforward and convolutional networks. Learning feedback weights offers a biologically plausible solution to the credit assignment problem in the brain, utilizing reinforcement learning algorithms and reward-modulated STDP. This approach involves a globally distributed reward signal providing feedback to all neurons in a network, allowing for a stochastic approximation of the gradient. Learning in the brain may rely on structures beyond a global reward signal. Artificial neural networks use gradient-based methods like backpropagation for credit assignment, outperforming RL-based algorithms in efficiency and performance. However, implementing backpropagation in biological systems poses challenges. Backpropagation is the primary method for solving supervised and reinforcement learning problems at scale, despite challenges in implementing it in biologically realistic neural networks. Efforts to modify or approximate backpropagation have gained significant attention recently, with synthetic gradients showing promise in learning based on approximate gradients. In small feedforward networks, fixed random feedback matrices can suffice for learning, known as feedback alignment. However, this method does not work in CNNs, very deep networks, or networks with tight bottleneck layers. Despite its limitations, rough approximations of a gradient signal can still enable learning. A proposed RL algorithm aims to train a feedback system for learning, building on similar ideas explored in recent work. We propose using a REINFORCE-style perturbation approach to train feedback signals for approximating backpropagation. This two-learner system aligns with cortical neuron physiology, supporting supervised learning in feedforward networks and providing a solution to the credit assignment problem. The network learns to use feedback signals trained with reinforcement. By combining local and global feedback signals, the proposed network model learns as well as regular backpropagation in small models and overcomes limitations of feedback alignment in more complex feedforward networks. This method suggests plausible ways the brain could solve the credit assignment problem by using reinforcement learning to train feedback signals. The network is composed of layers with non-linear activations and weight matrices, with a loss function defined in terms of the network output. Backpropagation relies on error signals computed top-down, with a synthetic gradient replacing the loss gradient term. Node perturbation introduces noise in each layer to estimate the error current. The synthetic gradient is used to update feedback matrices for parameters \u03b8, distinguishing true loss gradients from their estimates. This setup accommodates top-down and bottom-up information, utilizing stochasticity in biological neural networks to learn synthetic gradients. Various biologically plausible learning rules exploit random perturbations in neural activity. The noisy response from each unit is used to estimate gradients for optimizing the baseline loss. The REINFORCE algorithm coincides with the node perturbation method for Gaussian white noise. Node perturbation linearizes the loss with expectation over the noise distribution, providing an estimator of the loss gradient. Different choices of parameterization for the gradient function are possible, with a simple function of each layer's activations being sufficient. The gradient estimation problem may be simplified by providing each layer with error information from the loss function in a top-down manner. Feedback weights alignment can be achieved by learning appropriate feedback weights B. Various choices of g(h i ,\u1ebd i+1 ; B i+1 ) are investigated, with parameters B i+1 estimated through solving a least squares problem using gradient-descent. Additional experimental details can be found in the supplementary material. The estimator (3) is consistent as the noise variance approaches 0. The final layer feedback matrix converges to the true feedback matrix. Theorem 1 establishes convergence in a shallow non-linear network. Consistency is also established for the 'direct feedback alignment' estimator. The DFA estimator converges to the true feedback matrix for non-linear shallow or deep linear networks. Convergence theory is established for updating B and W simultaneously. The method can solve simple supervised learning problems using node perturbation with a four-layer network and MSE loss. The system uses node perturbation with a four-layer network and MSE loss to solve MNIST. Co-adaptation between the feedforward and feedback networks is crucial for providing a useful error signal. The relative error between the feedback and feedforward matrices is lower than feedback alignment, indicating the benefits of co-adaptation. The optimal noise level in node perturbation balances bias in the estimate and the ability to adjust to changing feedforward weights. The alignment between estimated and true gradients is low in node perturbation, indicating better error signal communication between layers. Sign congruence of feedforward and feedback matrices is crucial for performance. Node perturbation shows comparable learning performance to backpropagation and outperforms feedback alignment slightly. Node perturbation provides error signals closely aligned with true gradients, showing comparable learning performance to backpropagation and slightly outperforming feedback alignment. Performance-wise, there is no clear advantage over feedback alignment or backpropagation, especially in very deep networks or autoencoding networks with tight bottleneck layers. Testing on a simple auto-encoding network with MNIST input data also compares the method to the 'matching' learning rule. The study compares different learning methods in neural networks, including feedback alignment, node perturbation, denoising autoencoder, and the ADAM optimizer. Node perturbation shows better performance than feedback alignment and backpropagation, with improved separation of digits in the latent space. Feedback alignment fails to separate digits effectively, resulting in scrambled output. In the bottleneck layer, node perturbation successfully communicates error signals through thin layers of a network. Testing on a convolutional neural network solving CIFAR, feedback weights are learned directly from the output layer to earlier layers. A test accuracy of 75% is achieved on CIFAR10, showing the advantage of learning feedback weights compared to fixed feedback weights and backpropagation. The method can be used in a CNN to solve challenging computer vision problems without weight transport. It utilizes adding noise and approximating true gradients to solve the credit assignment problem. The performance improvement over fixed weights is studied by varying where noise is added in the models. In the study, different models are tested on autoencoding and CIFAR10 tasks to understand the performance of node perturbation method. Noise can be added to activations or inputs, or used in obtaining an estimator of true gradients. The combination of noise and gradient approximations improves performance in both tasks for SGD optimization and ADAM. Our method shows improved performance by combining noise and gradient approximations in tasks like CIFAR10. Noise added to activations does not help feedback alignment, indicating our method learns useful error signal approximations. Noise does not always improve performance, but using a less-biased gradient estimator does. Our method outperforms others without requiring weight transport or true gradients as a supervisory signal. Implementing a perturbation-based synthetic gradient method to train neural networks shows promise in achieving biologically-plausible deep learning. This approach, which removes the symmetric feedforward/feedback weight requirement of backpropagation, demonstrates comparable performance to backpropagation on MNIST. It can handle larger problems than perturbation-only methods and works in cases where feedback alignment fails, offering learning without weight transport in various network architectures. Integrating local and global feedback signals presents a promising direction for biologically plausible learning algorithms. The method of implementing a perturbation-based synthetic gradient for training neural networks shows promise in achieving biologically plausible deep learning. While it does not solve all issues with gradient-based learning, it demonstrates advantages in learning feedback weights compared to fixed weights. However, it does not yet reach state-of-the-art performance on challenging datasets like CIFAR. Further research is needed to fully characterize its performance, but it offers computational advantages. The method offers better data-movement performance and does not rely on the environment for gradients. Theoretical results show similarities to previous research on denoising autoencoders. Feedback alignment remains somewhat mysterious and not applicable in some network architectures. Recent studies have shown that weaknesses in feedback alignment can be addressed by adjusting weights to provide a useful error signal. This approach differs from previous methods by combining global and local learning signals. The method was tested in an idealized setting and is consistent with neurobiology in important ways. The method involves separate learning of feedforward and feedback weights in cortical networks, with pyramidal cells having compartments for integration of signals. Apical dendrites receiving reward information is a recent finding. Models show how these ideas can be implemented in spiking neural networks, suggesting perturbation-based rules for better learning systems. Neurons measure perturbations to learn feedback weights through plausible mechanisms. Neurons can measure perturbations to learn feedback weights through various mechanisms, such as birdsong learning and learning rules that do not require knowing the noise. The model involves subtracting a baseline loss to reduce estimator variance without affecting the expected value. This allows for the implementation of separation of feedforward and feedback systems and perturbation-based estimators by neurons. The brain may use a feedback system trained on RL-based methods as exact gradient signals are infeasible. The brain may utilize a feedback system trained through reinforcement signals to approximate gradients when exact signals are not feasible. This approach can inform learning models in the brain and artificial networks. The model involves drawing data from a distribution, linearizing the loss function, and estimating parameters using backpropagated error signals. This method allows for efficient learning using feedback signals. The setup allows us to consider each term as a vector or matrices for multiple inputs. The question is how to show that B is deterministic without noise. By defining the synthetic gradient without noise, we can prove convergence to Wi+1 similar to linear least squares estimator consistency. Assumptions include subgaussian noise, analytic loss function, and well-behaved Taylor series approximation. The final layer feedback matrix, B N +1, converges to the true feedback matrix under certain assumptions. The conditional expectation of the estimator converges to the gradient L, and the remainder term approaches zero as c h \u2192 0, given well-behaved moment conditions. The problem is close to a linear least squares problem due to the definition of e N +1 in relation to the baseline loss. The final layer feedback matrix, B N +1, converges to the true feedback matrix under certain assumptions. The least squares estimator converges to the true feedback matrix, and the estimator converges in probability to the true feedback matrix in the remaining layers of the network. The least squares estimator converges to the true feedback matrix in deep linear networks, with the node perturbation estimator expressed as a key component. The node perturbation estimator in deep linear networks can be expressed as the true gradient plus remainder and noise terms. Assumptions include using Gaussian or subgaussian random variables, restrictions on non-linearities, and difficulty in establishing full rank conditions for certain cases. The method of proof used here is not immediately applicable to non-linear networks due to the complexity of sums over observations. Convergence of the method is demonstrated in a small non-linear network solving MNIST with different noise levels and layer widths. Feedback matrices are updated while feedforward weights are held fixed, resulting in accurate estimators. Different noise variance yields equally accurate estimators. The estimator accurately estimates the feedback matrix W2 with a relative error of 0.8%. Convergence is layer dependent, with the second hidden layer matrix, W2, accurately estimated. The angles between the estimated gradient and the true gradient are close to zero for both layers, indicating strong alignment. Significant sign congruence is achieved in both layers, despite differences in the matrices themselves. The number of neurons affects the estimation process. The method provides error signals for networks of various sizes, implemented in TensorFlow with specific parameters. The network architecture is 784-50-20-10 with MSE loss and sigmoid non-linearity. Synthetic gradient updates are used for B with learning rate \u03b7 = 0.0005, while W is updated with learning rate 0.0004. Feedback alignment, backpropagation, and node perturbation use the same step size. An initial warm-up period of 1000 iterations is applied. Network dimensions are 784-200-2-200-784 with specific activation functions. In this case, the feedback weights are adjusted in a network with dimensions 784-200-2-200-784. Activation functions used are tanh, identity, tanh, relu. MNIST input data with MSE reconstruction loss is utilized, with a batch size of 32. Values for W step size, noise variance, and B step size were determined through random hyperparameter search. The denoising autoencoder incorporates Gaussian noise with zero mean and standard deviation \u03c3 = 0.3. The network architecture is 784-50-20-10 or 784-N-50-10 for solving MNIST with an MSE loss function, using a sigmoid non-linearity. B is updated through online ridge regression least-squares solution for faster convergence. The CNN architecture used for CIFAR10 and CIFAR100 includes Conv and MaxPool layers with specific filter sizes. Hyperparameters were determined through random search, with ADAM optimizer and dropout utilized. The methods listed in Table 2 were implemented with a noise standard deviation of 0.02 for the autoencoding task. The noise standard deviation for optimal performance in the autoencoding task was found to be 0.02 through hyperparameter search. Different methods added Gaussian noise with the same standard deviation to activations, while DAE used a noise standard deviation of 0.3 for network inputs. The synthetic gradient method implemented here differs from previous work, as matrices B are trained using true gradients instead of noisy estimators. This approach, although not biologically plausible, serves as a useful baseline for performance evaluation. Another investigated baseline is the 'matching' rule, similar to previous studies. For CIFAR10 results, a noise standard deviation of 0.067 was found to be optimal through hyperparameter search. This noise was added to activations, and synthetic gradients followed a similar form as before."
}