{
    "title": "S1eWbkSFPS",
    "content": "Graph neural networks have shown promising results in analyzing diverse graph-structured data like social, citation, and protein interaction networks. A new model called GESM addresses oversmoothing issues by considering both edge-based neighborhood relationships and node-based entity features. GESM uses random walk to alleviate oversmoothing and attention to explicitly use node information for weighted neighborhood aggregation. The proposed GESM model achieves state-of-the-art performance on various graph datasets for transductive and inductive learning tasks. Global information is shown to be significant, and the source code will be made publicly available soon. Graph neural networks have been successful in various real-world applications, thanks to advancements in deep learning. Recent research has focused on effectively aggregating feature representations of neighbor nodes in graph structures. Edge-based methods, which rely on relational inductive bias, have limitations in generalizing to new graphs. To improve neighborhood aggregation, some studies have incorporated node information to reduce the reliance on edge information. Graph attention networks (GAT) utilize the attention mechanism to adjust weights for neighborhood aggregation based on node features, showing promise in enhancing generalization for unseen graphs. However, most methods, including graph convolutional networks (GCNs), suffer from oversmoothing, limiting access to global information and leading to poor performance on datasets with only a small portion of data closely connected. GESM is a novel method that addresses oversmoothing in graph networks by incorporating random walk and attention mechanisms. It can effectively handle sparsely labeled datasets and diverse graph structures, as validated on benchmark datasets like Cora, Citeseer, and Pubmed. The GESM model demonstrates superior performance on both inductive and transductive learning tasks on benchmark datasets like Cora, Citeseer, and Pubmed. It adaptsively considers local and global information through random walk and achieves enhanced accuracy even with reduced label rates. The key contributions include the use of Graph Entities with Step Mixture via random walk for improved learning outcomes. The GESM model incorporates attention and utilizes nodes and edges for neighborhood aggregation in transductive and inductive learning tasks. It emphasizes the importance of propagation steps for performance analysis. Random walk is used to model how node information spreads in the graph. Random walk in graph theory models how node information propagates through a graph. The transition matrix P describes probabilities of moving to neighbor nodes from the starting node. The distribution of the random walk at step t is obtained by multiplying the transition matrix t times. The transition matrix is a matrix form of the Markov chain with steady-state. The attention mechanism, introduced in sequence-to-sequence modeling, allows the model to focus on important features by examining the hidden layer. GATs achieved state-of-the-art performance in GNNs by using the attention mechanism to consider the importance of neighboring nodes. This emphasis on node features over structural information during propagation is advantageous for training and testing graphs. Incorporating the attention mechanism in graph models allows for adaptive highlighting of salient features globally. Nodes in a graph are represented by a feature matrix X, with a label matrix Y for classes and a learnable weight matrix W. The adjacency matrix A of the graph includes self-loops for enhanced connectivity. Graph Step Mixture (GSM) addresses oversmoothing and localized aggregation issues in graph neural networks by separating node embedding and propagation processes through a mixture of random walk steps. GSM consists of three stages: input X passes through a fully connected layer, multiplied by a normalized adjacency matrix\u00c2 for each random walk step, and the results are concatenated and fed into the network. Graph Step Mixture (GSM) tackles oversmoothing and localized aggregation problems in graph neural networks by utilizing a mixture of random walk steps. The propagation process of GSM involves concatenating results from each step and feeding them into a fully connected layer. The adjacency matrix used in GSM is asymmetric, allowing for flexibility with arbitrary graphs, unlike prior methods that use a symmetric Laplacian adjacency matrix. This approach enables the neural network to combine localized sub-graphs with global graphs for improved performance. Our method, Graph Entity Step Mixture (GESM), enhances the base model by incorporating an attention mechanism to emphasize node information for aggregation. This involves using multi-head attention to compute attention coefficients based on concatenated features of nodes and their neighbors. Our method, Graph Entity Step Mixture (GESM), enhances the base model by incorporating attention to emphasize node information for aggregation. Attention helps avoid noisy parts of the graph and improves combinatorial generalization for inductive learning. Focusing on node features for aggregation provides more reliable results in inductive learning, with a time complexity of O(s \u00d7 l \u00d7 h) where s is the maximum number of steps, l is the number of non-zero entries in the adjacency matrix, and h is the hidden feature dimension. The model complexity is highly efficient with time complexity O(s \u00d7 l) for node classification on citation networks like Cora, Citeseer, and Pubmed. Only a small percentage of node labels are used for training, allowing for learning on datasets with public label rates. Experiments were conducted on datasets with low label rates for Cora, Citeseer, and Pubmed. The model's ability to propagate node information was tested by reducing label rates. The PPI dataset was used for inductive learning, consisting of 24 graphs with nodes multi-labeled with 121 labels and 50 features. The model was tested on datasets with low label rates for Cora, Citeseer, and Pubmed, and on the PPI dataset for inductive learning with multi-labeled nodes. It was compared with various state-of-the-art models for transductive learning. The study compared their model against various state-of-the-art models for transductive learning, including models utilizing teleport term, spectral filters, attention between nodes, and baseline models like GraphSAGE-LSTM and JK-LSTM. Different hyperparameters were used for datasets with public and random splits, with dropout probabilities set at 0.3 and 0.6 respectively. The study compared their model against state-of-the-art models for transductive learning. Different hyperparameters were used for datasets with public and random splits. The models were trained with specific settings for multi-head, hidden layer size, aggregation steps, regularization, and learning rate. Inductive learning utilized specific settings for hidden layer size, steps, and regularization. Models were evaluated based on micro-F1 score. Our models were initialized using Glorot initialization and trained to minimize cross-entropy loss using Adam. Results on benchmark datasets show our method ranked in the top-3 for every task, indicating strong predictive power for both transductive and inductive learning tasks. Our method, applicable to tasks with high predictive power, outperforms other methods in transductive learning tasks. The base model GSM excels in considering global and local information, while GESM shows the importance of node information. In inductive learning, GESM improves upon GSM by learning neighborhood node importance, surpassing GAT despite fewer attention layers. Our method, surpassing GAT, shows remarkable performance on datasets with low label rates, demonstrating the ability to consider global information. The use of a mixture of random walks played a key role in enhancing accuracy. The improved results in the experiments can be attributed to adaptively selecting node information from local and global neighborhoods, allowing peripheral nodes to receive information. GCN, SGC, and GAT suffer from oversmoothing, with GCN and GAT showing severe accuracy degradation after the 8th step. SGC's accuracy gradually decreases with step size. In contrast, GSM maintains performance without degradation, overcoming oversmoothing with step mixture. JK-Net also maintains training accuracy by using GGN blocks with multiple steps. Test accuracy comparison between GSM and JK-Net was conducted. In comparison to JK-Net, GSM outperforms in adaptability to larger steps, maintaining steady performance even with increased step sizes. Test accuracy results show GSM's superiority over JK-Net, confirming GSM's resilience to collapsing even with large step sizes. The impact of increasing the number of steps on accuracy for GSM and GESM is also observed, highlighting the contribution of considering remote nodes. Considering remote nodes can increase accuracy in models like GESM, as shown in Figure 5. The model benefits from more data in a larger neighborhood, leading to reliable decisions and improved performance. Accuracy converges faster with higher label rates, and the addition of attention improves performance. In terms of inference time, GSM is faster than GCN with larger step sizes, while GESM outperforms GAT in both accuracy and stability, as seen in Figure 6. Our methods, utilizing a mixture of random walk steps, offer fast and accurate results. The t-SNE plot in Figure 7 illustrates the difference between edge-based and node-based aggregation in our models. Results show that the attention mechanism helps models ignore noisy information in graphs. Top-3 results for average test set accuracy and standard deviation are presented in Table 5. The table presents results of various models on different datasets, with top-3 results highlighted. The experiments were extended to new node classification datasets to verify overfitting. Coauthor Physics and Amazon Computers/Amazon Photo are datasets used in the KDD Cup 2016 challenge and consist of co-authorship and co-purchase graphs, respectively. The node features and class labels differ between the two datasets. The experimental setup followed previous research, using specific hyperparameter values without tuning. Results show that the proposed method does not overfit and outperforms GAT, with GESM being more accurate and stable. Attention vectors were visualized to further analyze the performance. The distribution of attention vectors was visualized to analyze the performance of GESM, which showed slight adjustments in weight values leading to improved accuracy and stability."
}