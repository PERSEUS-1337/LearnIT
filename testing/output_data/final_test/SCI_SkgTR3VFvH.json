{
    "title": "SkgTR3VFvH",
    "content": "The growth in complexity of Convolutional Neural Networks (CNNs) has led to interest in partitioning networks across multiple accelerators during training and pipelining backpropagation computations. Techniques like micro-batching and weight stashing are used to address stale weights, but they underutilize accelerators or increase memory footprint. Our study explores the impact of stale weights on statistical efficiency and performance in a pipelined backpropagation scheme. Using 4 CNNs, we found that training with stale weights in early layers results in models with comparable inference accuracies to non-pipelined training on MNIST and CIFAR-10 datasets, with a slight drop in accuracy for each network. Our study explores combining pipelined and non-pipelined training in a hybrid scheme to address accuracy drops in deep pipelining. We demonstrate the implementation of pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving up to 1.8X speedups over a 1-GPU baseline with a small drop in accuracy. The growth in complexity of CNNs has led to partitioning networks across multiple accelerators during training, as shown in Figure 1. The study explores combining pipelined and non-pipelined training in a hybrid scheme to address accuracy drops in deep pipelining. The growth in complexity of CNNs has led to partitioning networks across multiple accelerators during training, with each accelerator responsible for computations of specific layers. However, the nature of the backpropagation algorithm leads to underutilization of accelerators due to dependencies between layers and batches of input data. The underutilization of accelerators can be alleviated by pipelining computations of the backpropagation algorithm over multiple accelerators. However, using stale weights can negatively affect the statistical efficiency of the network, hindering training convergence or lowering inference accuracy. Strategies to address this issue include avoiding stale weights with microbatches, ensuring consistency with stashing, or limiting their use. In this paper, the authors question common approaches to accelerator utilization and propose a pipelining scheme that allows for full accelerator utilization while using stale weights. This scheme is simpler to implement, fully utilizes accelerators, and has lower memory overhead. The authors evaluate the scheme using 4 CNNs and show that limiting pipelining to early layers in the network still results in comparable model quality to non-pipelined models. The study proposes a pipelining scheme for accelerator utilization in CNN training. The drop in accuracy for 4 networks ranges from 0.4% to 4% with deeper pipelining. A hybrid scheme combining pipelined and non-pipelined training maintains accuracy while improving performance. Speedup of up to 1.8X on a 2-GPU system is achieved. The paper outlines backpropagation, the pipelining scheme, and implementation details. The backpropagation algorithm involves two passes: a forward pass calculates output error, and a backward pass updates weights. Mini-batches are used for input data. In the forward pass, data propagates through the network from the first to the last layer. Activations and weights are computed at each layer. The output error is used with the true label to calculate training error. In the backward pass, the error is propagated back through the network. In the backpropagation algorithm, the error is propagated from the last to the first layer in the backward pass. Error gradients with respect to pre-activations and weights of each layer are calculated and used to update the weights. Dependencies exist between the calculations of activations and error gradients in each layer, ensuring proper weight updates. The backward pass cannot start until the forward pass is completed and the error is determined. The backpropagation algorithm ensures proper weight updates by calculating error gradients and activations in each layer. Pipelining addresses limitations in parallelism by using conceptual pipeline registers to update weights in a pipelined manner. The backpropagation algorithm utilizes pipelining with 4-stage registers for forward and backward passes. The stages are executed on 3 accelerators in a pipelined fashion for efficient computation. The backpropagation algorithm uses pipelining with 4-stage registers for forward and backward passes on 3 accelerators. The activations of layer l x are forwarded to FS 2, while mini-batch 2 is fed to FS 1. This pipelined execution is illustrated in a space-time diagram for 5 mini-batches, showing all accelerators active at steady state. The pipelining scheme in FS 1 uses weights that are 2 cycles stale, updated by errors from FS 2 and BKS 1. The weights are indicated by subscripts for staleness and delay. The backpropagation algorithm uses pipelining with 4-stage registers for forward and backward passes on 3 accelerators. The activations of layer l x are forwarded to FS 2, while mini-batch 2 is fed to FS 1. In the general case, K pairs of pipeline registers are inserted between the layers of the network, creating forward stages (FS i) and backward stages (BKS i). The updates of the weights by BKS 2 require activations calculated for the same mini-batch in FS 1 for all layers in the stage. The backpropagation algorithm uses pipelining with 4-stage registers for forward and backward passes on 3 accelerators. The forward and backward stages correspond to the same set of layers, with each stage assigned to an accelerator. Weight staleness is quantified based on cycles. Pipelined execution allows for a potential speedup, but stale weights may impact training convergence and model accuracy. Hybrid training combines pipelined and non-pipelined training to assess benefits and downsides. It addresses drops in inference accuracy due to weight staleness but reduces performance benefits by under-utilizing accelerators. Speedup in hybrid training depends on the number of iterations used for each training method. In hybrid training, a combination of pipelined and non-pipelined iterations is used to achieve the same inference accuracy as non-pipelined training. The speedup of hybrid training compared to non-pipelined training with accelerators is calculated using Amdahl's law. Pipelined training is implemented in both simulated and actual executions to analyze convergence, accuracy, and performance. PyTorch is used for the actual execution due to its support for collective communication. PyTorch is utilized instead of Caffe for its support of collective communication protocols and network partitioning across multiple accelerators. Custom Python Caffe layer, Pipeline Manager Layer (PML), enables simulated pipelining by registering inputs, passing activations, and updating weights during forward and backward passes. Hardware-accelerated pipelined training involves partitioning the network for efficient execution. To implement hardware-accelerated pipelined training in PyTorch, the network is partitioned onto different GPUs, each running its own process. Asynchronous data transfers are used, but communication must go through the host CPU, increasing overhead. Activations are copied between GPUs via the CPU during the forward pass, and error gradients are sent in reverse. Simulated pipelining is tested on a machine with one Nvidia GTX1060 GPU and an Intel i9-7940X CPU, while actual pipelining is evaluated on a machine with two Nvidia GTX1060 GPUs and an Intel i7-9700K CPU. In our evaluation, we use four CNNs including LeNet-5 with variations in hyperparameters. We assess the effectiveness of pipelined training in terms of convergence and inference accuracy compared to non-pipelined training. Speedup is used to measure performance improvements. Training convergence and inference accuracy improvements are shown for both pipelined and non-pipelined training based on the number of training iterations. Pipelined training is conducted with 4, 6, 8, and 10 stages. The evaluation compares the effectiveness of pipelined training with non-pipelined training using 4, 6, 8, and 10 stages. Results show similar convergence patterns for both methods, with slight drops in inference accuracy for some networks. The evaluation compares the effectiveness of pipelined training with non-pipelined training using different stages. The accuracy drop is within 3.5%, making the model quality comparable to non-pipelined training. However, with deeper pipelining (8 and 10 stages), there is a significant drop in inference accuracy (12% for VGG-16 and 8.5% for ResNet-20). This is attributed to the use of stale weights, as confirmed by previous literature. Further exploration focuses on the impact of the number and location of pipeline stages on inference accuracy, particularly with ResNet-20 due to its structure. The study evaluates the impact of increasing pipeline stages on model quality. Results show that as the number of stages increases, the model quality decreases. The inference accuracy is affected by the number of stale weights used in pipelined training. The study examines the effect of increasing pipeline stages on model quality, showing a decrease in accuracy with more stale weights. The inference accuracy drops as the percentage of stale weights increases, regardless of the degree of staleness. The position of the last pair of pipeline registers in the network determines the percentage of stale weight, impacting inference accuracy. Placing these registers early minimizes accuracy drop, especially in the initial convolutional layers of a CNN. Our profiling of the runtime of ResNet-20 reveals that the first three residual functions consume over 50% of the training time, suggesting more pipeline stages at the network's beginning. Hybrid training with ResNet-20 shows comparable convergence to both pipelined and non-pipelined training methods. The resulting inference accuracies from hybrid training are similar to those from traditional training methods. The 20K+10K hybrid training produces a model with comparable accuracy to the non-pipelined model. With an additional 10K iterations of non-pipelined training, the model quality slightly surpasses that of the non-pipelined model, demonstrating the effectiveness of hybrid training. Implementing 4-stage pipelined training on a 2-GPU system, each GPU handles one forward and one backward stage, achieving a maximum speedup of 2. Training every ResNet for 200 epochs, the inference accuracies with and without pipelining are compared, showing comparable model quality and speedups for all networks. Speedup for ResNet-362 is 1.82X. The table shows that as networks get larger, the speedup improves due to higher computation to communication overhead ratio. Hybrid training combines pipelined and non-pipelined training, resulting in a maximum speedup of 1.33. Table 5 validates that hybrid training can produce comparable model quality for each ResNet. Hybrid training combines pipelined and non-pipelined training for ResNet models, achieving a speedup of 1.29X as network size grows. The memory usage for 4-stage pipelined ResNet training is modest, with additional memory required for copies of activations. The results in Table 6 show a 60% increase in size for most models with a batch size of 128. Comparing our pipelined training scheme with PipeDream and GPipe, we focus on pipelining scheme, performance, and memory usage. Our approach is simpler as it does not require weight stashing or dividing mini-batches into micro-batches, reducing communication overhead. This makes it easier to implement in frameworks like PyTorch or hardware like Xilinx's xDNN FPGA accelerators. Our pipelining scheme, PipeDream, improves performance by eliminating pipeline bubbles. It achieves a speedup of 1.7X for ResNet-110 with 2 GPUs, comparable to GPipe's 1.3X speedup for ResNet-101 with 2 TPUs. Increasing pipeline stages negatively impacts performance, as seen in GPipe. PipeDream uses less memory than GPipe but more than PipeDream, saving intermediate activations and multiple copies of network weights. The memory footprint increase due to weight stashing depends on network architecture and mini-batch size. Our scheme uses less memory compared to PipeDream for VGG-16 on CIFAR-10 and ImageNet. Various approaches explore parallelism in deep neural network training, including data parallelism. Various approaches explore parallelism in deep neural network training, including data parallelism where each accelerator processes different mini-batches simultaneously and model parallelism where a model is partitioned onto different accelerators. Communication overhead can be significant in data parallelism, while model parallelism aims to exploit parallel processing efficiently. Pipelined parallelism addresses under-utilization of accelerator resources in training large models by allowing multiple accelerators to work on different parts of the model simultaneously. This approach reduces the overhead of communicating inter-layer activations and gradients during training. Studies have explored pipelined parallelism to improve efficiency in deep neural network training. PipeDream and GPipe are implementations of pipelined training for large neural networks across multiple GPUs. PipeDream uses weight stashing to limit the usage of stale weights, while GPipe pipelines micro-batches within each mini-batch to keep gradients consistently accumulated. GPipe eliminates the use of stale weights but may experience \"pipeline bubbles\" at steady state. Our work focuses on pipelining both forward and backward passes during training iterations, unlike GPipe which utilizes \"pipeline bubbles\" to reduce memory footprint by re-computing forward activations. Huo et al. introduced decoupled backpropagation (DDG) with delayed gradient updates, increasing memory usage. They also proposed feature replay (FR) to re-compute activations during backward pass, similar to GPipe, resulting in less memory usage and improved accuracy. Our work contrasts with previous approaches by utilizing pipelining with unconstrained stale weights, resulting in full pipeline utilization with a modest increase in memory usage. We study the impact of weights staleness on model quality, showing effectiveness in using stale weights in early layers. Additionally, we introduce hybrid training combining pipelined and non-pipelined methods, comparing performance and memory footprint to existing work. Our evaluation focuses on pipelined backpropagation for CNN training to fully utilize accelerators. Our work focuses on utilizing pipelining with stale weights for training CNNs, achieving a speedup of 1.82X on a 2-GPU system without significantly increasing memory usage. We found that pipelining with stale weights converges and inference accuracies are comparable to traditional backpropagation when implemented in early layers. However, implementing pipelining deeper in the network results in a significant drop in accuracy, which can be mitigated by combining pipelined and non-pipelined training for slightly lower performance gains. The study explores utilizing pipelining with stale weights for training CNNs, achieving a 1.82X speedup on a 2-GPU system without increasing memory usage significantly. The approach yields an average of 0.19% better inference accuracies for ResNets compared to the baseline. Future directions include evaluating the method with more accelerators, larger datasets like ImageNet, and exploring hardware implementation using FPGA or ASIC accelerators. LeNet-5 is trained on the MNIST dataset with specific parameters, while AlexNet's training progression is monitored with 300 tests. The progression of inference accuracy during training is recorded with 300 tests for AlexNet, VGG-16, and ResNet on the CIFAR-10 dataset using specific training parameters such as learning rate, momentum, weight decay, and mini-batch size. Tests are performed every epoch to monitor accuracy. VGG-16 utilizes batch normalization and dropout due to its difficulty in training. ResNet is trained on CIFAR-10 dataset with specific hyperparameters including learning rate, momentum, weight decay, and mini-batch size. Batch normalization is used throughout the network. Tests are conducted every 100 iterations to track accuracy progression. For non-pipelined training, ResNet models are trained for 200 epochs with SGD. The hyperparameters for 4-stage pipelined training are the same. For 4-stage pipelined training of ResNet on CIFAR-10, hyperparameters are the same as the non-pipelined baseline, except for the BKS 2 learning rate."
}