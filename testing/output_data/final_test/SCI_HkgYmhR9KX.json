{
    "title": "HkgYmhR9KX",
    "content": "Visual Active Tracking (VAT) involves autonomously controlling a tracker's motion system to follow a target object based on visual observations. A novel adversarial RL method called AD-VAT is proposed in this paper, using an Asymmetric Dueling mechanism where the tracker and target are trained in a competitive manner. The tracker aims to lock onto the target, while the target tries to evade capture, leading to a robust tracker for VAT. The proposed AD-VAT method uses an Asymmetric Dueling mechanism where the target learns to predict the tracker's reward, resulting in a stronger target and a more robust tracker. A novel partial zero-sum reward is also introduced to stabilize training. Experimental results show faster convergence and more robust tracking behaviors in various scenarios. The code for Visual Active Tracking (VAT) can be found at https://github.com/zfw1226/active_tracking_rl. VAT involves autonomously controlling a tracker to follow a target object using visual observations, which is crucial in applications like autonomous vehicle fleets and drones. Unlike conventional visual tracking, VAT considers camera control and is more practical and challenging. Deep reinforcement learning is used for training in VAT. Training an end-to-end deep neural network via reinforcement learning for Visual Active Tracking (VAT) is feasible with the advancement of deep reinforcement learning. The approach saves effort by mapping raw-pixel observations to control signals directly with a Conv-LSTM network, outperforming conventional methods. However, the performance is limited by training methods, requiring virtual environments for data generation. Deploying the trained tracker in the real world poses challenges. Building high-fidelity environments for Visual Active Tracking (VAT) is expensive and tedious. BID21 attempted to bridge the virtual-to-real gap by diversifying visual appearance, but neglected the motion of the target. In this work, a novel adversarial RL method called AD-VAT is proposed for learning Visual Active Tracking. The tracker and target object are treated as opponents in the mechanism, enhancing each other during competition. The training environments naturally form a curriculum, allowing the tracker to compete with targets of appropriate difficulty levels as both agents become stronger. In AD-VAT, a novel adversarial RL method is introduced for Visual Active Tracking. The target generates trajectories to train the tracker, encouraging the discovery of tracker weaknesses. Two components, PZR and TAM, are derived to address stability and convergence issues. PZR creates a zero-sum competition in the near range and penalizes the target for straying too far. The AD-VAT method introduces a reward structure inspired by the target's behavior to escape from the tracker. A \"tracker-aware network\" is used to model the target, incorporating the tracker's observations and actions. An auxiliary task is added for the target to predict the tracker's immediate reward. This asymmetric dueling mechanism strengthens the target and improves the tracker's robustness. Experiments in 2D and 3D environments validate the effectiveness of AD-VAT, with the 2D environment featuring randomly placed obstacles. The novel Adversarial Reinforcement Learning method for VAT task, AD-VAT, includes the Asymmetric Dueling mechanism. It is evaluated in 2D and 3D environments, with the 3D environments built on Unreal Engine 4. The tracker trained in AD-VAT shows generalization to high-fidelity environments. The Adversarial Reinforcement Learning method for VAT task, AD-VAT, introduces the Asymmetric Dueling mechanism to generate diverse trajectories for a more robust tracker. Two techniques are provided for efficient AD-VAT: a partial zero-sum reward structure and a tracker-aware network for the target. Active Object Tracking involves object tracking and camera control simultaneously, which can be addressed in a two-step or end-to-end manner. Great progress has been made in traditional object tracking, allowing for the use of mature visual tracking algorithms to passively track objects. Camera control can then be developed based on tracking results to actively follow a target. Various methods, such as motion detection and tracking, as well as perception and control policy modules, have been proposed for object tracking and camera control. However, challenges still exist in perfecting tracking algorithms and camera control based on tracking results. Camera control based on tracking results is challenging due to unknown correspondence between image space and camera parameter space. Joint tuning of visual tracking and camera control is expensive and involves trial-and-error. End-to-end methods establish direct mapping between raw input frame and camera action, eliminating the need for intermediate visual tracking results. Reinforcement learning can control the camera based on signal output from a Conv-LSTM network. Environment augmentation is used to improve generalization potential, but fixed targets in augmented environments may limit performance. In this paper, the idea of dueling is explored to improve performance in challenging environments. Adversarial Reinforcement Learning is used to enhance the robustness of RL agents by introducing adversarial noise and forces during training. Various approaches such as creating difficult challenges for agents and optimizing generator networks through adversarial training are discussed. In BID8, a generator network is optimized through adversarial training to ensure tasks are always at the appropriate difficulty level for the agent. A two-agent non-cooperative game is designed for VAT tasks, where one agent tracks and the other tries to escape. Unlike previous work, the adversary in AD-VAT is a physical player that can fully control its movements, creating a \"dueling\" scenario. The proposed approach in AD-VAT involves a fully controllable opponent referred to as \"dueling\" to provide more challenges during training, leading to a more robust visual tracker. Unlike standard self-play, the players in AD-VAT have asymmetric observation and task, with the target having more information and an additional auxiliary task for its adversarial policy. This asymmetry requires separate models for the target and tracker, making learning potentially unstable. The proposed method in AD-VAT introduces the Asymmetric Dueling mechanism for stable learning in Visual Active Tracking. It is formulated as a two-player game with partial zero-sum reward structure and a tracker-aware model for the target. The game is governed by a tuple representing state space, observation space, action space, reward function, and environment state transition probability. The curr_chunk discusses the observation and state transition probabilities in a two-player game scenario. It mentions the actions taken by the agents, the updated state, rewards received, and the policy of the tracker. Reinforcement Learning is used to learn the tracker's policy. The curr_chunk discusses the policy learning in a two-player game scenario, where the tracker aims to maximize its return by learning parameter \u03b8 1, while the target aims to maximize its return by learning parameter \u03b8 2. Conventional adversarial methods usually treat policy learning as a zero-sum game, but this formulation may not be suitable for certain scenarios in partial observable games. The curr_chunk introduces a partial zero-sum reward structure for visual active tracking, focusing on improving learning efficiency by constraining competition within the observable range. The reward for the tracker includes a positive constant and an error penalty term, measured based on relative position error in a polar coordinate system. The reward function for visual active tracking is based on a polar coordinate system with the tracker at the origin. Parameters like distance to the tracker and view angle are considered, with rewards clipped to avoid over-punishment. The reward function for visual active tracking is based on a polar coordinate system with the tracker at the origin. The target object's reward is related to the tracker's reward, with penalty terms controlled by tuning parameters. The target should stay within the observable range to avoid penalties, playing a zero-sum game with the tracker. If the target goes out of range, penalties increase based on distance. The optimal policy for the target is to escape and stay close to the edge of the range. The proposed \"tracker-aware\" model enhances the target's performance by incorporating the tracker's observation and action. This approach aims to make the target \"stronger\" than the tracker, following the concept of knowing the enemy to achieve victory. The conventional adversary typically relies solely on its own observation, leading to degraded performance when faced with imperfect or partial observations. The target's policy is adjusted to consider both its own observation and the tracker's input, improving its decision-making process. The proposed \"tracker-aware\" model enhances the target's performance by incorporating the tracker's observation and action. An auxiliary task predicts the tracker's immediate reward, improving the target's decision-making process. This approach yields a more diversified escaping policy and a more robust tracker. The experiments explore the VAT task from 2D to 3D environments. In 2D environments, maps are represented by an 80 \u00d7 80 matrix with different elements denoting free space, obstacles, tracker, and target. The tracker's goal is to place the target as close to the center of the observed matrix as possible. In 3D environments, high fidelity simulations are created using Unreal Engine to mimic real-world tracking scenarios. UnrealCV BID28 provides APIs for interactions between RL algorithms and the environment, with observations being first-person view images and a discrete action space with seven motions. In training, a Domain Randomized Room (DR Room) is used with two controllable players to improve feature representation. Testing focuses on the tracker's ability to transfer to different environments like Urban City, Snow Village, and Parking Lot. The DR Room has randomized textures and illumination conditions for better learning. The environments for testing the tracker's ability to transfer include Urban City with well-modeled buildings and streets, Snow Village with bumpy snowfields and cabins, and Parking Lot with complex illumination conditions and occluding pillars. Two base target agents, Rambler and Navigator, are provided for comparison. Agent Ram randomly generates trajectories by sampling actions from the action space and walking around a local area, while Agent Nav plans the shortest path to a goal and explores the map globally. Trajectories from Ram are considered easier cases, while trajectories from Nav are more difficult. Both agents are trained using the A3C reinforcement learning algorithm. The A3C reinforcement learning algorithm is used with multiple workers in parallel for training. The network architecture includes a Conv-LSTM structure without a fully-connected layer. Different CNN layers are used for 2D and 3D experiments, with input transformations for the latter. Shared Adam optimizer updates network parameters, and specific hyperparameters are set for different environments. The tracker uses learning rates \u03b4 1 = 0.001 in 2D and \u03b4 1 = 0.0001 in 3D, with \u03b3 = 0.9, \u03c4 = 1.00, and \u03bb 1 = 0.01. Parameter updating frequency n is 20, with a maximum global iteration of 150K. A higher regularizer factor \u03bb 2 = 0.2 in 2D and \u03bb 2 = 0.05 in 3D encourages target exploration for diverse trajectories. Validation is done in parallel using a Nav agent for testing. The Nav agent is challenging and suitable for validation. Two metrics, Accumulated Reward (AR) and Episode Length (EL), are used for quantitative evaluation. AR represents precision and robustness, affected by immediate reward and episode length. The performance is compared to baselines, and an ablation study shows the effectiveness of partial zero-sum reward and tracker-aware model. Testing is done with different target agents in four settings. The effectiveness of AD-VAT is demonstrated through testing the active tracker with various target agents in different settings. Results show mean and standard deviation of AR and EL, with a max episode length of 500. The tracker performs well when EL is 500, indicating successful target tracking. Initially, the adversarial target behaves predictably, making it easier for the tracker to warm up. As the tracker improves, the target explores different patterns, enhancing the learning process similar to curriculum learning. The curriculum learning is automatically produced by the target via adversarial reinforcement learning. The learning curve shows the advantage of the proposed AD-VAT. Two components, PZR and TAM, are introduced to implement AD-VAT, influencing the natural curriculum for the tracker. An ablation study result demonstrates the effectiveness of these components in improving sample-efficiency. In 3D experiments, the combination of PZR and TAM significantly boosts sample-efficiency and tracking performance. The AD-VAT tracker outperforms others in unseen scenarios, demonstrating transfer potential in real-world settings. The results show successful transfer to realistic environments. The three models successfully transfer to realistic environments, thanks to domain randomization and Conv-LSTM network. Target behavior during training, especially in complex environments, greatly impacts tracker performance. Adversarial behavior of the target improves tracker capability in challenging environments like Snow Village and Parking Lot. The AD-VAT tracker allows the target to explore actively, making the tracker stronger. The tracker's performance is evaluated in real-world video clips in Appendix C. The paper introduces an asymmetric dueling mechanism for visual active tracking (AD-VAT) that outperforms baseline methods. Experiments in 2D and 3D environments confirm its effectiveness. Future work includes exploring Multi-Agent RL methods for solving Partially Observable Markov Games and adapting the mechanism to more complex environments. The paper introduces an asymmetric dueling mechanism for visual active tracking (AD-VAT) that outperforms baseline methods in 2D and 3D experiments. The reward parameters and penalty structure are detailed, with visualizations provided to aid understanding. The paper introduces an asymmetric dueling mechanism for visual active tracking (AD-VAT) that outperforms baseline methods in 2D and 3D experiments. The zero-sum area is a sector area that fits the Field of View (FoV) of the tracker's camera. The penalty term in r2 decreases like a divergence sector based on the relative angle \u03b8 and distance \u03c1. Trajectories of the target and tracker are recorded during different training stages, with 100 episodes per stage. Target position distribution is plotted in Fig. 7 using a relative coordinate system for visualization. During early training stages, AD-VAT and Ram show similar random walking trajectories around the start point, while Nav method has the target moving straight to the goal, causing the tracker to lose track quickly. Random walking helps the tracker observe the target in various positions, leading to better exploration. As training progresses, the target seeks more challenging scenarios to defeat the tracker. During training, the target gradually seeks more difficult cases to defeat the tracker. The Nav explores the map globally, while the Ram focuses on local exploration. The AD-VAT method allows the reinforced target to adapt to the tracker's capability, resulting in different direction choosing patterns at different stages. The target balances between exploring the map and dueling with the tracker, finding the tracker's weaknesses more effectively. During training, the tracker enhances efficiency with importance sampling. A qualitative evaluation shows the tracker's sensitivity to target position and scale, adjusting camera movement accordingly. The tracker demonstrates the ability to transfer tracking to real-world scenarios by plotting \"Action\" maps based on target position and scale. The horizontal axis represents the target's x-axis position, while the vertical axis indicates the target's size. Results for other VOT videos can be found at: https://youtu.be/jv-5HVg_Sf4."
}