{
    "title": "rkgwuiA9F7",
    "content": "Assessing distance between true and sample distribution is crucial for generative models like Wasserstein Autoencoder (WAE), inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE). The Cramer-Wold AutoEncoder (CWAE) introduces a new generative model with a cost function based on Cramer-Wold distance. It simplifies optimization by eliminating the need for sampling in the training loop, matching the performance of WAE-MMD and often outperforming SWAE. The construction of generative models involves computing and minimizing the distance between true and model distribution, originally done using variational methods in VAE. The introduction of Wasserstein metric and models like WAE-GAN and WAE-MMD have improved this process. The construction of generative models has evolved with models like WAE-GAN and WAE-MMD, which relax the need for variational methods. SWAE introduced the sliced-Wasserstein distance for faster distribution comparison, but lacks an analytic formula for computing distances from the standard normal distribution. The Cramer-Wold distance is introduced in SWAE for sampling from the prior distribution and one-dimensional projections. It simplifies optimization of generative models using a characteristic kernel and closed-form formula, leading to the creation of CWAE. In this section, the Cramer-Wold distance is introduced for computing normality of high-dimensional samples using kernel-based density estimation. The approach leverages the Cramer-Wold Theorem and Radon Transform to reduce distance calculations between distributions to one-dimensional calculations. The Cramer-Wold theorem states that multivariate distributions can be uniquely identified by their one-dimensional projections. To compute the sliced-Wasserstein distance between two samples X, Y \u2208 R D, the mean Wasserstein distance between all one-dimensional projections is calculated. This method is effective as it only requires sampling over the projections. To compare samples or distributions in a single dimensional space, a Gaussian kernel is used to smoothen the distributions. The text discusses using a Gaussian kernel to smoothen distributions for kernel density estimation, particularly in low-dimensional spaces. The optimal choice of smoothing parameter is determined by Silverman's rule of thumb. This method is effective for computing distances on single-dimensional projections of data. The Cramer-Wold distance is introduced as a way to compute the squared distance between two samples by considering the mean squared L2 distance between their smoothed projections over single-dimensional subspaces. This distance allows for a closed-form solution in the case of normal distribution and is defined by a specific algorithm. The Cramer-Wold distance simplifies the computation to a closed form solution for the distance between two samples without the need for optimal transport or sampling over projections. Formulas for sample distances and distance from standard normal density are provided. The squared Cramer-Wold distance is formally defined, including the Kummer's confluent hypergeometric function. Asymptotic formulas and technical propositions are presented to prove Theorem 3.1. Theorem 3.1 relies on a crucial technical proposition involving a specific formula for slice integration of functions on spheres. By simplifying certain expressions and applying relevant equalities, the proof of Theorem 3.1 is completed. The main theoretical result of the paper is a theorem that eliminates the need for sampling when estimating the distance between a sample and a prior distribution. The formula for the distance between mixtures of radial distributions can be easily obtained using the Cramer-Wold distance. The section discusses the construction of CWAE, a generative model based on AutoEncoder, aimed at minimizing reconstruction error in a latent space. CWAE enforces a modified cost function to ensure data in the latent space originates from a prior distribution. The data in the latent space comes from a prior distribution, formalized by constructing an encoder and decoder. The generative model should have small reconstruction error and resemble the prior distribution in the latent space. The CWAE model adds a measure of distance from normal distribution to its cost function to ensure data in the latent space follows a standard normal density. The formula for Cramer-Wold distance is simplified for practical use in experiments, with a comparison to WAE and SWAE models. The CWAE model differs from WAE and SWAE by not requiring sampling from normal distribution or slices for cost evaluation. It also does not need a separately trained neural network for optimal transport function like WAE-GAN. Future work may introduce an adversarial version of CWAE for comparison. The CWAE model is validated on standard benchmarks like CelebA, Cifar-10, and MNIST, comparing with WAE-MMD and SWAE. Results show improvement over SWAE with a simpler cost function. Section 5.2 discusses qualitative tests and visual investigations of the latent space, while Section 5.3 focuses on quantitative tests using Fr\u00e9chet Inception Distance and other metrics. Two basic architecture types were used in the experiments on MNIST. The CWAE model was evaluated on MNIST using a feedforward network for both encoder and decoder, and a 20 neuron latent layer. For CIFAR-10 and CelebA datasets, convolution-deconvolution architectures were used. The comparison between CWAE and WAE-MMD was based on samples, interpolations, and reconstructions. The experiment showed no perceptual differences between the two generative distributions. In the experiment, CWAE was compared with AE, VAE, WAE, and SWAE on MNIST data using a 2-dimensional latent space and Gaussian prior distribution. Results showed that CWAE's latent distribution closely matched a normal distribution, unlike AE which did not optimize for normality. CWAE performed well in terms of perceptual quality and normality objective, similar to WAE-MMD. CWAE matches WAE-MMD in terms of perceptual quality and normality objective. Quantitative studies using FID and sharpness scores show similarities between CWAE and WAE. The latent distribution of CWAE closely resembles a normal distribution. Training time comparisons show CWAE's efficiency for batch-sizes up to 1024. CWAE is faster than other models for batch-sizes up to 1024, approximately 2\u00d7 faster up to 256 batch-size. A novel method for quantitative assessment of models based on comparison to standard normal distribution in the latent is proposed using Mardia tests. The normalized Mardia's kurtosis value is considered, which is zero for the standard normal density. Results in Figure 4 and TAB1 show FID score, Mardia's skewness, and kurtosis for different models on the CelebA dataset. WAE, SWAE, and CWAE models perform best in terms of reconstruction error, with CWAE showing strong generative capabilities. The VAE model has a slightly worse reconstruction error but output closer to a normal distribution. The VAE model's output distribution is closest to a normal distribution, resulting in blurred reconstructions. The Cramer-Wold metric was tested as a Gaussian goodness of fit but did not yield satisfactory results. WAE-MMD and CWAE perform similarly in terms of FID score and sharpness, with CWAE outperforming SWAE. The newly introduced CWAE matches the results of WAE-MMD. The CWAE model matches the results of WAE-MMD using a cost function with a closed analytic formula. It utilizes the Cramer-Wold metric for Gaussian mixtures to measure divergence from normality. Future work could explore using the Cramer-Wold distance in other settings, particularly in adversarial models. The Cramer-Wold metric is formally defined and shown to be given by a characteristic kernel for spherical Gaussians. For more information on kernels and kernel embedding of distributions, refer to BID12. The cw-metric is defined by generalizing the notion of smoothing for arbitrary measures \u00b5. It involves convolution operators, normal densities, and characteristic functions. The transport of density by projection is also defined for random vectors. If two measures have the same smoothing, then they coincide. The cw-metric is defined by generalizing the notion of smoothing for arbitrary measures \u00b5 using convolution operators and normal densities. The transport of density by projection for random vectors is also discussed. The cw-distance of two measures is formally defined using a kernel function, and it is proven that the function d cw is a metric. The Cramer-Wold kernel is a characteristic kernel with a closed-form scalar product of two radial Gaussians. It is the only kernel, besides the Gaussian kernel, that has a closed form for spherical Gaussians. This is important as Gaussian kernels cannot be effectively used in AutoEncoder based generative models due to the fast decrease in derivative. In this section, we compare CWAE model to WAE-MMD, showing that CWAE combines sliced-approach with MMD-based models. Both WAE and CWAE use kernels to distinguish between sample and normal density. The WAE cost function involves a characteristic kernel k, sample X from standard normal density, and kernel-based distance between probability distributions. The inverse multiquadratic kernel k is chosen with a default value, and the model has hyper-parameters. The CWAE model combines sliced-approach with MMD-based models, using the Cramer-Wold kernel with a regularizing hyperparameter \u03b3. Unlike WAE, CWAE has no hyperparameters and utilizes the sample estimation of d in the distance calculation. In the CWAE model, the logarithm of the divergence is used to balance terms in the learning process, eliminating the need for additional hyperparameters. The regularization hyperparameter is determined by Silverman's rule of thumb and depends on sample size, unlike WAE-MMD. This approach removes the noise in the learning process caused by random sample selection. In the CWAE model, the learning process is more stable with random sample selection from N(0, I). CWAE learns faster than WAE-MMD with smaller standard deviation of the cost-function. The estimation of values for the Cramer-Wold distance is crucial, with an approximate asymptotic formula provided for D \u2265 20 and a special case for D = 2. The Kummer's confluent hypergeometric function is used in the integral representation for a, b > 0. In the CWAE model, the learning process is stable with random sample selection from N(0, I). CWAE learns faster than WAE-MMD with smaller standard deviation of the cost-function. Estimation of values for the Cramer-Wold distance is crucial, with an approximate asymptotic formula provided for D \u2265 20 and a special case for D = 2. Kummer's confluent hypergeometric function is used in the integral representation for a, b > 0. For large D, iterative direct formulas for function \u03c6 D are of little numerical value, with a focus on the special case D = 2 for illustrative purposes in the latent for the MNIST data set. The approximation of I 0 is applied to practically implement \u03c6 2. The approximation of I 0 is used to practically implement \u03c6 2 in the CWAE model. Mean learning time comparison for different batch sizes shows that CWAE is faster for batch sizes within the range of [32, 512], but becomes slower for batch sizes larger than 1024 due to its quadratic complexity. Larger batch sizes, even exceeding 512, are rarely used in practice for training autoencoders like CelebA. The curr_chunk discusses the architecture of a convolutional-deconvolution network for the CIFAR-10 dataset, including details on the encoder and decoder layers with specific filter sizes, channel numbers, and activation functions. The network consists of convolution layers, dense layers, and transposed-convolution layers with various configurations. The architecture of the convolutional-deconvolution network includes layers with specific filter sizes, channel numbers, and activation functions. The network consists of convolution layers, dense layers, and transposed-convolution layers with various configurations. The models were trained with specific hyperparameters and for different numbers of epochs. Additionally, a comparison to the WAE-MMD model on CelebA was made using an identical architecture. The convolutional-deconvolution network architecture includes layers with 5 \u00d7 5 filters, batch normalization, ReLU activation, and transposed-convolution layers. Results for CWAE compared to VAE and WAE-MMD models are shown in TAB0. Models were trained using Adam for 55 epochs with the same optimizer parameters. Test reconstructions of VAE, WAE-MMD, SWAE, and CWAE models on CelebA dataset are displayed in Figure 9."
}