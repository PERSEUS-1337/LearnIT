{
    "title": "Hygv0sC5F7",
    "content": "The implicit bias of gradient descent methods in solving a binary classification problem with a nonlinear ReLU model and exponential loss function is studied. The loss function landscape can have spurious local minima, and gradient descent may converge to global or local max-margin directions. Stochastic gradient descent converges to global or local max-margin directions in expectation. The implicit bias of these algorithms in learning a multi-neuron network is explored, showing that the learned classifier maximizes margins under ReLU activation. The gradient descent algorithms converge to solutions with certain properties even without explicit regularization. For example, GD can converge to the solution with the minimum norm or the max-margin classifier. This implicit regularization has been studied in various machine learning problems recently. Given a set of training samples, the goal is to find a linear model by solving an empirical risk minimization problem. It has been shown that if the loss function is monotonically decreasing and the data is linearly separable, gradient descent converges to the solution with infinite norm and maximum margin direction. This implicit bias of gradient descent can explain experimental results even when the training error is zero. The focus of this paper is on the convergence of gradient descent (GD) in the context of linearly separable data. Existing studies have shown that when the training error reaches zero, the testing error continues to decrease due to improved model parameter margin. Gunasekar et al. (2018) generalized this concept to other gradient-based algorithms. Ji & Telgarsky (2018) analyzed GD convergence without data separability assumptions, highlighting implicit regularization in a subspace-based form. This paper addresses the convergence of GD for nonlinear leaky ReLU and ReLU models, questioning if it will still converge to the max-margin direction. Our study explores the iterationwise convergence of SGD under random sampling with replacement to the max-margin direction for nonlinear ReLU and leaky ReLU models, providing new insights and understanding. Our study focuses on the exponential loss function under the ReLU model, characterizing the landscape of the empirical risk function. We find that the risk function has asymptotic global minima and spurious local minima, unlike the linear model. The convergence of GD can lead to four cases: reaching the global minimum, a local minimum, a spurious local minimum, or oscillating between regions without convergence. The study focuses on the exponential loss function under the ReLU model, showing that the risk function has global and spurious local minima. Gradient descent can lead to convergence to the global minimum, a local minimum, a spurious local minimum, or oscillation between regions without convergence. SGD's implicit bias converges to the max-margin direction in linearly separable regions or local minima defined by data samples with positive labels. The analysis requires new technical developments different from traditional SGD analysis. The study focuses on the exponential loss function without attainable global/local minima and analyzes the implicit bias property of SGD. Gunasekar et al. (2018) studied the implicit bias of GD and SGD for minimizing the squared loss function, showing convergence to a global minimum closest to the initial point. Telgarsky (2013) showed AdaBoost converges to an approximate max-margin classifier. Soudry et al. (2017; 2018) studied GD convergence in logistic regression towards the solution of support vector machine. Our work studies the convergence of Gradient Descent (GD) and Stochastic Gradient Descent (SGD) under the nonlinear ReLU model with the exponential loss. Previous studies have focused on linear models with the same type of loss functions. Ji & Telgarsky (2018) analyzed the convergence rates of GD on arbitrary datasets, while Nacson et al. (2018a) improved the convergence rate under the exponential loss. Gunasekar et al. (2018) showed that steepest descent can lead to margin maximization under generic norms. Our work focuses on studying the convergence of Stochastic Gradient Descent (SGD) under the ReLU model with random sampling. We show that the averaged weight vector converges towards the max-margin classifier at a rate of O(1/ \u221a ln t). Previous studies have analyzed linear models, while our work specifically looks at the ReLU model. Various studies have explored the convergence and generalization performance of SGD under different models, characterizing convergence rates and generalization error bounds. The study focuses on the convergence of Stochastic Gradient Descent (SGD) under the ReLU model for binary classification. It considers linearly separable datasets and aims to train a ReLU model for the task. The model outputs \u03c3(w x i ), where \u03c3(v) = max{0, v} is the ReLU activation function. The ReLU activation function in the model outputs \u03c3(w x i), with the predicted label set as sgn(w x). The goal is to learn a classifier by minimizing empirical risk using the exponential loss. Understanding the nonconvex and nonsmooth nature of the loss function is crucial for characterizing the implicit bias of GD and SGD algorithms. The landscape of the loss function under the ReLU model differs significantly from that under the linear activation model, which is convex and achieves an asymptotic global minimum. The ReLU activation function in the model outputs \u03c3(w x i), with the predicted label set as sgn(w x). The goal is to learn a classifier by minimizing empirical risk using the exponential loss. The landscape of the loss function under the ReLU model differs significantly from that under the linear activation model. The linear activation is convex and achieves an asymptotic global minimum only if w * is in the linearly separable region. Under the ReLU model, the convergence property of GD can be very different from that under the linear model. Theorem 3.1 characterizes the landscape properties of problem (P) under the ReLU model. The ReLU activation function in the model outputs \u03c3(w x i), with the predicted label set as sgn(w x). The landscape under the ReLU model can be partitioned into different regions, where gradient descent algorithms can have different implicit bias. In this subsection, we analyze the convergence of GD in learning the ReLU model. The ReLU model's convergence in gradient descent is analyzed, focusing on the implicit bias property. Different notions of margin are introduced to characterize this bias, considering the presence of spurious local minima in the ReLU model's loss function. The global max-margin direction for samples in I+ is defined to understand the impact of the ReLU activation function. The ReLU activation function can affect the max-margin direction in gradient descent. The analysis shows that GD converges towards a max-margin direction only when w + is in the linearly separable region. Different cases of implicit bias in GD are characterized in Theorem 3.2, depending on the subset J + \u2286 I + and the termination conditions of GD. The ReLU activation function influences the convergence of gradient descent. Theorem 3.2 explains the implicit bias of GD in learning the ReLU model, where convergence can lead to either global or local minima. GD may oscillate between regions due to the ReLU function, affecting convergence. SGD is analyzed for solving problem (P) with random sampling at each iteration. SGD samples an index \u03be t \u2208 {1, . . . , n} uniformly at random with replacement for updates. The implicit bias of SGD when converging to global or local minima has not been studied in existing literature. Technical developments are needed for the convergence proof and bias characterization. Traditional analysis of SGD under convex functions requires bounded gradient variance. SGD samples an index \u03be t \u2208 {1, . . . , n} uniformly at random with replacement for updates. The implicit bias of SGD when converging to global or local minima has not been studied in existing literature. Technical developments are needed for the convergence proof and bias characterization. The variance of the gradient is bounded, and SGD enjoys a nearly constant bound on the variance up to a logarithmic factor of t in learning the ReLU model. Proposition 1 shows that the variance of the stochastic gradients is well-controlled, with the norm of the stochastic gradients growing logarithmically fast. The convergence rate of SGD for learning the ReLU model is established, with the stepsize set to diminish to compensate for variance. The expected risk of the averaged iterates generated by SGD converges at different loss values for global and local minima. The convergence rate is nearly O(ln 2 t/ \u221a t) when \u03b1 is close to 0.5, matching standard results in convex optimization up to a logarithmic order. The convergence rate of SGD for learning the ReLU model is established, with the stepsize set to diminish to compensate for variance. The analysis shows that the expected averaged iterate converges to the max-margin direction without an explicit regularizer in the objective function. The analysis focuses on the SGD update under the ReLU model, emphasizing the handling of stochastic gradient variance and exploiting classification properties. An example dataset is provided to show that SGD remains stable in the linearly separable region. The leaky ReLU activation function is discussed, with parameters defined for its form. Leaky ReLU takes the form \u03c3(v) = max (\u03b1v, v), with parameter (0 \u2264 \u03b1 \u2264 1). It behaves similarly to the linear model in terms of convergence during GD. The loss function under leaky ReLU has asymptotic global minima achieved in the separable region with infinite norm. The max-margin classifier is defined based on leaky ReLU for linearly separable data samples. The data samples with label -1 are scaled by the parameter \u03b1 of leaky ReLU. The max-margin direction of data X* converges to zero with its norm going to infinity. For SGD applied to solve problem (P), if wt stays in the linearly separable region for all t > T, then convergence is achieved. The study extends to training a one-hidden layer ReLU neural network for binary classification without assuming linear separability of the dataset. The output is determined by a network with hidden neurons and an output neuron, using the ReLU activation function. The parameter vector converges towards the max-margin classifier direction. The study focuses on characterizing the implicit bias of GD and SGD in learning weight parameters of a multi-neuron model. The model assumes activated neurons maintain their activation status and training error converges to zero. This differs from previous work and aims to achieve zero loss by setting the predicted label as the sign of f(x). The study characterizes the implicit bias of GD and SGD in learning weight parameters of a multi-neuron model by defining activation patterns and partitioning training samples based on ReLU activation patterns. The output of the network for samples in each partition is determined, and the max-margin direction is defined. The study characterizes the implicit bias of GD and SGD in learning weight parameters of a multi-neuron model by defining activation patterns and partitioning training samples based on ReLU activation patterns. The corresponding max-margin direction of the samples in B h as DISPLAYFORM5 is analyzed in Theorem 4.1, which characterizes the convergence of weight parameters and implicit bias in the original feature space. The study analyzes the convergence of weight parameters and implicit bias in the original feature space for ReLU neural networks. Theorem 4.2 establishes the implicit bias of SGD in optimizing the loss function, showing that neurons in the hidden layer maintain their activation status under certain conditions. This implies that samples with the same ReLU activation pattern have the same label, and the averaged SGD maximizes the margin for every sample partition. The study examines the generalization performance of ReLU neural networks through gradient descent methods. It shows that the network partitions data samples into subsets, maximizing the distance to the decision boundary for robustness. The convergence of GD and SGD can lead to global or local max-margin directions due to possible spurious asymptotic local minima. This differs from linear models in previous studies. The study explores the generalization performance of ReLU neural networks using gradient descent methods. It discusses extensions to leaky ReLU models and multi-neuron networks, emphasizing the need to investigate implicit bias in learning multilayer neural network models. The proof of Theorem 3.1 demonstrates the gradient behavior under different conditions, highlighting the non-negativity and smoothness of the loss function. The proof in Theorem 3.1 shows that gradient descent behavior diverges to infinity while converging towards the max-margin classifier. If the global max-margin classifier is not in a linearly separable region, gradient descent cannot remain in that region. The proof in Theorem 3.1 demonstrates that gradient descent diverges towards infinity while converging to the max-margin classifier. If the global max-margin classifier is not in a linearly separable region, gradient descent cannot stay in that region. In Example 1, initializing GD at the green classifier leads to convergence to the max-margin direction of the sample (x1, +1), which misclassifies the data sample (x2, +1). Example 2 involves a dataset with one sample labeled +1. The dataset in Example 2 consists of one sample with label +1 and one sample with label \u22121. When initializing gradient descent at the green classifier, it oscillates around the direction x2/x2 and does not converge. The sample z2 is always misclassified due to the ReLU activation, and there must exist a t such that wt x2 > 0. The linear classifier generated by gradient descent oscillates around the direction x2/x2 and does not converge. The sample z2 is always misclassified due to ReLU activation, and there must exist a t such that wt x2 > 0. The objective function reduces to a linear model depending on sample z1, and w t converges to the max-margin direction x1 as t \u2192 +\u221e. The linear classifier generated by gradient descent oscillates around the direction x2/x2 and does not converge. In the case that w t x 1 > 0 for all t, the sequence {w t x 2 } t is strictly decreasing with a constant gap, leading to w t x 2 \u2264 0 within finite steps. SGD stays in the linearly separable region eventually, reducing the original minimization problem to a linear model with samples in I +. The proof involves three main steps. By utilizing the update rule of SGD, the term E w t \u2212 u 2 is bounded. Through convexity, it is shown that \u2207L(w t\u22121 ), w t\u22121 \u2212 u \u2265 L(w t\u22121 ) \u2212 L(u). Expectation is taken on both sides, leading to further bounds. By setting u = (ln(t)/\u03b3)\u0175 + and considering certain conditions, conclusions are drawn regarding the upper bounds of the terms involved. The proof of Theorem 3.3 involves utilizing the update rule of SGD to bound E w t \u2212 u 2. Through convexity, it is shown that \u2207L(w t\u22121 ), w t\u22121 \u2212 u \u2265 L(w t\u22121 ) \u2212 L(u). Expectation is taken on both sides, leading to further bounds. By setting u = (ln(t)/\u03b3)\u0175 + and considering certain conditions, conclusions are drawn regarding the upper bounds of the terms involved. Substituting into the inequality and applying Jensen's inequality, a quadratic inequality is solved to establish the final result. The proof of Theorem 3.4 involves technical lemmas for the main theorem, including stepsize and data matrix considerations. The convergence rate is shown to decrease to 0 at a rate of O(ln 2 (t)/t 1\u2212\u03b1), with the best rate achievable at O(ln 2 (t)/ \u221a t) when \u03b1 is close to 0.5. The main theorem is proven using technical lemmas related to stepsize and data matrix considerations. The convergence rate decreases to O(ln^2(t)/t^(1-\u03b1)), with the best rate achieved at O(ln^2(t)/\u221at) when \u03b1 is close to 0.5. The proof involves showing the convergence of the direction of E[w_t] to the max-margin direction. The proof involves demonstrating the convergence of E[w_t] towards the max-margin direction, with a decreasing rate of at least O(ln(t)). By utilizing the Fenchel-Rockafellar duality, it is shown that strong duality holds under certain conditions. Additionally, the update of SGD is analyzed using Taylor's expansion, leading to insights on the linear separable region and the maximum of L(w). The proof demonstrates the convergence of E[w_t] towards the max-margin direction with a decreasing rate of at least O(ln(t)). Utilizing Fenchel-Rockafellar duality, strong duality is shown under specific conditions. The update of SGD is analyzed using Taylor's expansion, providing insights on the linear separable region and the maximum of L(w). The gradient \u2207L(w) in the linearly separable region is given by \u2207L(w) = -1/n \u2211 y_i 1 {w. The proof demonstrates the convergence of E[w_t] towards the max-margin direction with a decreasing rate of at least O(ln(t)). Utilizing Fenchel-Rockafellar duality, strong duality is shown under specific conditions. The gradient \u2207L(w) in the linearly separable region is given by \u2207L(w) = -1/n \u2211 y_i 1 {w xi>0} exp(\u2212y_i w x_i) x_i = -1/n i\u2208I + exp(\u2212w x_i) x_i. Only samples with positive classification output contribute to the SGD updates. If there exist misclassified samples, w_t < +\u221e. Based on the update rule of SGD, if there are misclassified samples, then w_t < +\u221e. When w_t < +\u221e, all samples are correctly classified within finite steps. The update rule of SGD implies that if there are misclassified samples, then w_t < +\u221e, ensuring all samples are correctly classified within finite steps."
}