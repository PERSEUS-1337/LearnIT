{
    "title": "SJOl4DlCZ",
    "content": "A deep classification model trained with private samples may be vulnerable to reverse engineering, known as the Classifier-to-Generator (C2G) Attack. This can occur when the model is embedded in mobile devices for tasks like object recognition in autonomous vehicles or face recognition for phone authentication. PreImageGAN is a novel GAN introduced for C2G attack, where the generator estimates the sample distribution conditioned by the preimage of a classification model. It successfully works with hand-written character recognition and face recognition, allowing adversaries to extract specific images without prior knowledge. Recent advances in deep learning technologies are expected to enhance online services like face recognition. Probabilistic classification models play a crucial role in predicting the association of face images with individuals. These models need to be deployed in cloud environments for online prediction services. The enterprise needs to deploy model f into the cloud for prediction with private information and offline prediction scenarios. In the case of mobile devices like automatic driving cars or laptops with face authentication, the full model f needs to be embedded in the device to work standalone. The possibility of an adversary obtaining training samples or their distribution from a classification model can lead to serious privacy violations, especially when the data is private or confidential. For example, in face authentication, if an adversary can estimate the data distribution of a target individual, it can result in privacy breaches as individual faces are considered private information. Releasing object recognition models, like those used in automatic driving cars, can lead to privacy breaches if adversaries reverse engineer the model to reveal training sample distributions. This can enable attacks like generating adversarial examples or exposing hidden vulnerabilities in the system. Careful consideration is necessary before making such models public to prevent serious problems. Releasing object recognition models, such as those for automatic driving cars, can pose privacy risks if adversaries reverse engineer the model to uncover training sample distributions. This could lead to attacks like generating adversarial examples or exposing system vulnerabilities. Careful consideration is crucial before making such models public to avoid potential serious issues. Generative Adversarial Networks (GANs) are used to estimate sample distribution for classification models. GANs can generate realistic samples not in training data. For the C2G attack, unlabeled auxiliary samples are used as background knowledge. This helps extract specific information like face images from models. PreImageGAN utilizes unlabeled auxiliary samples to enhance knowledge extracted from a classification model. The proposed Classifier-to-Generator (C2G) Attack estimates the training sample distribution using GANs, even when auxiliary samples are not from the same distribution. This method leverages the interpolation ability of GANs to generate intermediate images between different samples. Generative Adversarial Networks (GANs) estimate sample-generating distribution using deep architectures. C2G attack with PreImageGAN demonstrates performance on EMNIST and FaceScrub datasets. Adversary can estimate sample distribution without samples of target label. GANs can accurately estimate sample distribution in high dimensions. Generative Adversarial Networks (GANs) introduce the basic concept of GANs and its variants. The learning algorithm involves a generator G that creates fake samples and a discriminator D that distinguishes between real and fake samples. The goal is to train the generator to produce samples indistinguishable from real ones. The training of Generative Adversarial Networks (GANs) involves optimizing the generator G to minimize the distribution difference between generated samples G(Z) and real samples x \u223c d X. Different GAN variants like VanillaGAN and Wasserstein-GAN (WGAN) aim to address issues like gradient explosion and mode collapse. WGAN introduces Wasserstein distance minimization, and to stabilize convergence, a Gradient Penalty (GP) regularization term is added to the discriminator's loss function. The Auxiliary Classifier GAN (ACGAN) was introduced as a GAN to estimate sample distribution conditioned by label. The generator of ACGAN takes random noise and a label as input, while the discriminator predicts the label of the sample in addition to real or fake estimation. The generator can generate samples with a specified label, such as age or gender in face images. The proposed algorithm combines WGAN and ACGAN as building blocks. In the latter sections, WGAN and ACGAN are used as building blocks for a supervised learning setting. The label set T and sample domain X are defined, with \u03c1 t representing the distribution of samples in X with label t. In face recognition, x corresponds to a face image and t to an individual, with \u03c1 t representing the distribution of face images of individual t. The training dataset D tr is defined as {(x, t)|x \u2208 X, t \u2208 T tr }, where T tr is a subset of labels associated with the images. The goal is to prevent the leakage of training data and sample generating distribution \u03c1 t when training a classifier f. The adversary aims to learn the publisher's private distribution \u03c1 t * for any specified target label t * by using a model f and auxiliary samples D aux. The distribution of X tr is a mixture distribution where each label t has a corresponding weight \u03b1 t. A training sample consists of an individual t and their face image x, where x follows distribution \u03c1 t. A probabilistic discrimination model f assigns probabilities to each label in T tr based on the input x. The Classifier-to-Generator Attack (C2G Attack) involves the publisher training a model f on a private dataset D tr and sharing it with the adversary. The adversary aims to infer the distribution \u03c1 t * for a specified label t * using auxiliary dataset D aux. The background knowledge of the adversary in the Classifier-to-Generator Attack (C2G Attack) is determined by the relation between the training set T tr and the auxiliary set T aux. The adversary's advantage depends on the overlap between the two sets - the more overlap, the more advantageous to the adversary. If the target label t * is not in T aux, it is more advantageous to the publisher. The adversary's goal in the Classifier-to-Generator Attack is to learn the publisher's private distribution \u03c1 t * for any specified target label t * \u2208 T tr. The attack is formulated by the adversary's algorithm A, which outputs a distribution over X. In the example of face recognition, the adversary aims to estimate the distribution of face images of a specific target label. The success of the attack is evaluated based on the quasi-distance between the estimated generative model and the underlying distribution \u03c1 t *. The adversary's algorithm aims to estimate the private distribution \u03c1 t * for a specified target label t * \u2208 T tr. To evaluate the success of the attack, the quasi-distance between the estimated generative model and the underlying distribution is considered. Instead of directly evaluating the distance between the two distributions, an empirical evaluation of the attack algorithm is conducted. This involves training a classifier f \u2032 with D tr using a different learning algorithm from f, and then assessing the probability of correctly predicting samples drawn from the estimated distribution as t *. In the evaluation of the C2G Attack, the inception accuracy is used to estimate the distribution of a specific label. However, models with high inception accuracy may not always generate meaningful images due to artifacts. Therefore, the quality of generated images should be checked by humans to ensure validity. The evaluation criterion for the C2G Attack is similar to that of GANs, with the objective being to estimate distributions accurately. In the study, the evaluation criterion for the C2G attack is similar to GANs, focusing on estimating generative models. The performance is assessed using inception accuracy and subjective evaluation. The adversary's background knowledge impacts the attack's effectiveness, with three levels considered. The adversary's background knowledge impacts the attack's effectiveness, with three levels considered: exact same, partly same, and mutually exclusive. In the exact same setting, the adversary can obtain samples labeled with the target label. In the partly same setting, the adversary cannot obtain samples with the target label. In the mutually exclusive setting, the adversary cannot obtain any samples with labels used for training the model. In the \"partly same\" or \"mutually exclusive\" setting, the adversary's background knowledge is limited as they cannot obtain samples with the target label. The generative model of the target label can be approximated using auxiliary samples if their distribution is close to the true underlying distribution. The sample generating distribution of the auxiliary samples may not necessarily match the true distribution. PreImageGAN generates samples with the target label by interpolating latent variables of given auxiliary samples without having samples of the target label. Generator G of PreImageGAN generates fake samples using random draws of y and z, aiming for the generated fake samples to satisfy a specific condition. Discriminator D distinguishes between generated fake samples and real samples. PreImageGAN generates samples close to a target label by interpolating latent variables of images close to the target label. The objective function of G and D is formulated to maximize Wasserstein distance. The proposed method utilizes \u03b1-Lipschitz functions to minimize Wasserstein distance between the marginal of the generator and the generative distribution of auxiliary samples. By training G to satisfy f(G(z, y)) = y, the C2G attack can be performed successfully. The adversary can estimate \u03c1 t * given classifier f and unlabeled auxiliary samples D aux under certain conditions. The study utilized \u03b1-Lipschitz functions to minimize Wasserstein distance between the generator's marginal and generative distribution of auxiliary samples. Training involved using the Adam optimizer with specific parameters, a batch size of 64, and 1-Lipschitz continuity enforcement for the discriminator. EMNIST dataset with 62 alphanumeric characters was used for evaluation. The study evaluated the C2G attack on grayscale 28x28 pixel images of 62 alphanumeric characters. Different levels of adversary's background knowledge richness were tested to see how it affects results. Two types of target labels were tested: lower-case and numeric targets. The adversary was given an alphanumeric classifier in one setting and a numeric classifier in another. The classifier achieved a test accuracy of 0.8443 after ten epochs. In the training process of PreImageGAN, the generator was trained for 20k iterations with \u03b3 incrementing by 0.001 per iteration. The C2G attack targeted lower-case characters in alphanumeric classification models, showing successful results. Some generated images were disfigured in partly same settings. The C2G attack successfully reconstructed most target labels in alphanumeric classifiers, with some disfigured images in certain settings. However, in a mutually exclusive setting, some samples were disfigured, especially when the target label was set as numeric characters. In the mutually exclusive setting, a numeric classifier (10 labels, test accuracy 0.9911) was given to the adversary, achieving a test accuracy of 0.9202 after ten epochs. PreImageGAN was trained in the same setting and successfully reconstructed most target labels in alphanumeric classifiers. However, some images targeting numeric characters were disfigured, particularly \"3\" and \"7\" in the partly same setting. In the mutually exclusive setting, images targeting \"0\" and \"1\" resembled the target characters, while others were disfigured or resembled different alphabets. In the mutually exclusive setting, the C2G attack against alphabets succeeds but fails when targeting numeric characters due to the classifier's incompleteness. The classifier recognizing non-target labels as target labels causes the attack to fail. For example, images of \"T\" are generated as \"7\" due to misclassification. Many alphabets are falsely recognized as numeric characters. The attack is effective when targeting lowercase letters with a changing background knowledge. The C2G attack fails to generate images of target labels correctly when non-target characters are recognized as target characters. This failure is due to the image manifold recognized by the classifier not exactly fitting the target character's image manifold. The disfigured images of \"h,\" \"i,\" and \"q\" serve as adversarial examples, hinting at the need for defense methods against the C2G attack. The C2G attack fails to generate images of target labels correctly due to the background knowledge of the adversary affecting the results significantly. The inception scores drop as the adversary's background knowledge becomes poorer. The attack was evaluated using a ResNet-based network architecture and the FaceScrub dataset, consisting of color face images resized to 64x64 pixels. The C2G attack successfully reconstructs training samples by generating face images of 100 people in T tr using model f. The attack uses D tr with 12k images and D aux with 53k images. The generator in PreImageGAN is trained for 130k iterations with gamma incrementing by 0.0001 per iteration. The results show that the generated face images capture the features of the training samples, confirming the success of the C2G attack. The C2G attack successfully reconstructs training samples from the face recognition model without having training samples. The results show that the generated face images capture the features of the training samples, revealing how the model recognizes specific features like Keanu Reeves' mustache. The PreImageGAN generates images by exploiting features extracted from auxiliary images, not just picking similar images. The PreImageGAN successfully generates images recognized as the target with high probability, indicating that the auxiliary dataset lacks similar images. The generated images closely resemble the target images, showcasing the model's ability to capture specific features like Keanu Reeves' mustache. The PreImageGAN can generate images recognized as the target with high probability, even when the background knowledge of the adversary targeting numeric letters is changed. The model can generate a wide variety of images by interpolating two targets. The PreImageGAN can generate face images using classifier f and features from an auxiliary dataset. Experiments were conducted on Intel(R) Xeon(R) CPU E5-2623 v3 and a single GTX TITAN X (Maxwell), taking about 35 hours for training. The computational capability is similar to Amazon Web Service's p2.xlarge instance, costing $45 for 50 hours. The C2G attack is a practical and low-cost attack. BID4 proposed a model inversion attack against machine learning algorithms for extracting private input attributes from predicted values. The BID3 model inversion attack reconstructs face images from a face recognition model by estimating private input attributes. Unlike the C2G attack, the adversary's goal in the model inversion attack is to obtain the private input itself. BID3's target model is a shallow neural network, while ours is deep neural networks, making it harder to extract input information as the network architecture deepens. The BID10 model discusses leakage of training samples in collaborative learning through a model inversion attack using IcGAN. Their demonstration is limited to small-scale datasets like MNIST and AT&T datasets. In contrast, our experiments involve larger datasets such as EMNIST and FaceScrub datasets. The C2G attack is evaluated against face recognition in a mutually exclusive setting with a model trained on 100 people including celebrities like Brad Pitt and Keanu Reeves. The C2G attack successfully generates face images of target labels without providing face images of the 100 people to the adversary. Membership inference attack against generative models like BEGAN and DCGAN is discussed, showing that images generated by PreImageGAN are recognized with high probability. The C2G attack generates face images of target labels without providing face images of the 100 people to the adversary. Song et al. discussed a malicious regularizer to memorize private training data, while BID11 and BID12 focus on understanding deep neural networks through image reconstruction. No attack algorithm has been presented to estimate private training sample distribution like the C2G attack achieves. The C2G attack estimates the training sample distribution \u03c1 t * from a classification model f and auxiliary dataset D tr using the PreImageGAN algorithm. Results show failure of the C2G attack when the auxiliary dataset lacks information, generating noise images instead of numeric letters. This failure occurs when the attacker lacks background knowledge of the training data distribution. The C2G attack generates images with target labels set as t * = 0, 1, 2 using noise images as an auxiliary dataset. The generated images are influenced by the classification model f. The experiment measures how often non-target characters are misclassified as the target character with high probability. The tables display the top-five falsely recognized characters for each target label by the classifier f. If fewer than five characters are misclassified, the fields are left blank. The alphanumeric classifier f trained in a similar setting represents the characters falsely recognized as the target label. The C2G attack successfully generates images with target labels using noise images. The alphanumeric classifier misclassifies certain characters as the target label with high probability, supporting the effectiveness of the attack. In Table 6, \"E\" and \"F\" are frequently misrecognized as \"e\" and \"f\", respectively. In Figure 3, the alphanumeric classifier generates images of \"i\" and \"q\" with some distortion in the mutually exclusive setting. The misclassification occurs because the model does not fully utilize the structure of these characters. Additionally, a numeric classifier trained in a similar setting also misidentifies characters as the target label with high probability. In the mutually exclusive setting, the C2G attack successfully generates images of numeric characters, while non-target characters are falsely recognized as the target label. This results in the failure of the attack, as PreImageGAN generates non-target images as target images. The C2G attack successfully generates images of numeric characters, but non-target characters are falsely recognized as the target label. This suggests that the model does not contain appropriate features to recognize the target characters. The attack can generate images similar to the target characters for \"0\", \"1\", \"5\", and \"9\", but images for other characters are disfigured."
}