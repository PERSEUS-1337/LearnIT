{
    "title": "HkeFQgrFDr",
    "content": "Unsupervised image-to-image translation using unpaired training pairs has shown success but faces limitations like model collapse and independent two-domain mappings. To address this, a method called GMM-UNIT is proposed, utilizing a content-attribute disentangled representation with a GMM fitting the attribute space. This approach allows for continuous domain encoding, interpolation between domains, and extrapolation to unseen domains. GMM-UNIT is a unifying framework for unsupervised image-to-image translation, allowing for continuous domain encoding, interpolation between domains, and extrapolation to unseen domains. It addresses limitations like model collapse and independent two-domain mappings, offering a solution for mapping images to multiple domains with visual diversity. The existing models for image-to-image translation struggle with mapping images to multiple domains due to limitations in GAN and VAE approaches. This paper introduces a novel model that separates visual content from domain, enabling stochastic translation of images to multiple domains. The GMM-UNIT is a novel image-to-image translation model that disentangles visual content from domain attributes. It utilizes a Gaussian mixture model for the attribute latent space, enabling mode-diversity, multi-domain translation, scalability, and few/zero-shot generation. The code and models will be publicly available. The GMM-UNIT model utilizes a disentangled attribute-content latent representation for image-to-image translation. It leverages Gaussian mixture modeling for attribute encoding and aims to generate realistic images in different visual domains. The model is situated in the image translation literature, addressing the challenge of translating images between different visual domains, with impressive results achieved using GANs. The GMM-UNIT model uses a disentangled attribute-content latent representation for image-to-image translation, leveraging GANs to generate realistic images in different visual domains.GANs aim to synthesize new images similar to real data through an adversarial approach between a Discriminator and a Generator, conditioning on specific classes and latent vectors extracted from images. Unsupervised domain translation techniques involve translating images from one domain to another without paired supervision. Various methods have been proposed, such as minimizing feature-level distance, creating a shared latent space, using cycle consistency loss, and applying reconstruction loss. These approaches help narrow down feasible mappings between images. Recent advancements in domain translation have focused on translating images without paired supervision. Mo et al. (2019) extended previous approaches to translate multiple object instances in the same image. Choi et al. (2018) introduced StarGAN, a unified framework for multi-domain image translation using a single GAN model. Unlike deterministic mappings, style transfer aims to change the style of an image without altering its content, as seen in works by Gatys et al. (2015) and others. In contrast to deterministic image-to-image translation methods, BicycleGAN encourages multi-modality in paired settings through GANs and VAEs. This approach addresses the limitation of deterministic mappings by allowing for diverse outputs in domain translation tasks. Almahairi et al. (2018) augmented CycleGAN with latent variables to increase diversity. MUNIT assumes common content space but different style spaces for diverse outputs. Mode seeking loss encourages GANs to explore modes and avoid collapse. Models in literature are either multi-modal or multi-domain, requiring a choice between diversity and training efficiency. In response to the limitations of existing models, a unified model is proposed to address the trade-off between generating diverse results and training a single model for multiple domains. This model utilizes a content-attribute disentangled representation with a GMM distribution in the attribute space. By enforcing a variational loss on the latent representation to follow this GMM, each component is associated with a domain, enabling multi-modal and multi-domain translation. Additionally, GMM-UNIT introduces a continuous encoding of domains, allowing for domain interpolation and extrapolation with minimal data. GMM-UNIT is an image-to-image translation model that utilizes Gaussian Mixture Models (GMMs) to model the attribute latent space. It maps an image to multiple domains in a stochastic manner, allowing for domain interpolation and extrapolation with minimal data. In the proposed representation, Gaussian components in a mixture model the domains. This model allows for encoding many more domains than the latent attribute space dimension. Continuous encoding of domains enables navigation in the attribute latent space, generating images for unseen domains and interpolating between them. State of the art models can be seen as a special case of GMMs. The proposed GMM-UNIT model allows for multi-modal and multi-domain mappings, as well as few/zero-shot image generation. It is a generalization of existing state-of-the-art approaches, enabling the encoding of multiple domains in a latent space for image generation. GMM-UNIT is a generative-discriminative model for multi-domain and multi-modal image generation. It involves a generator that takes content and attribute latent codes to produce images, which are then evaluated by a discriminator. The model aims to disentangle content and attribute representations through content and attribute extractors. The GMM-UNIT model aims to disentangle content and attribute representations through content and attribute extractors. The model requires the encoders and generator to satisfy properties like consistency, fit, and realism. Various losses are used to enforce these properties, including self-reconstruction. The GMM-UNIT model enforces properties like consistency, fit, and realism through various losses, including self-reconstruction and cycle consistency. It also includes domain classification of generated and original images. The GMM-UNIT model enforces properties like consistency, fit, and realism through various losses, including self-reconstruction and cycle consistency. It also involves domain classification of generated and original images, with different losses applied to each. The network's objective function includes hyper-parameters for weight balancing. The GMM-UNIT model undergoes extensive quantitative and qualitative analysis in real-world tasks such as edgesshoes, digits, and faces. It is tested on one-to-one domain translation, multi-domain translation, and multi-domain translation with different combinations of attributes. The model is evaluated on a dataset with over 40 facial attribute labels, showcasing its ability to interpolate attributes and generate images. The GMM-UNIT model demonstrates few-shot generation in image-to-image translation and is applied to the Style transfer task. It is compared to state-of-the-art models in multi-modal and multi-domain image translation, showcasing its performance in image quality and diversity. The GMM-UNIT model is evaluated for image quality and diversity using FID, IS, LPIPS, NDB, and JSD metrics. FID measures distribution distance, IS evaluates image quality, and LPIPS calculates L2 distance between features. The LPIPS distance measures the similarity between features extracted by a deep learning model of two images, while NDB and JSD assess the similarity between distributions of real and generated images. Higher LPIPS distance indicates better diversity among generated images, while lower NDB and JSD suggest the generated data distribution approaches the real data distribution. The model is first evaluated on two-domain translation before multi-domain translation. Our model is evaluated on two-domain translation using a dataset of shoes and their edge maps. A single model is trained for edges \u2194 shoes without paired information, producing high-quality images with diversity. The model is then tested on a multi-domain translation problem with different digit domains. The Digits-Five dataset includes three domains: MNIST, MNIST-M, and SVHN. The model is compared with state-of-the-art on multi-domain translation, showing GMM-UNIT generates higher quality and diversity images than StarGAN*. StarGAN* fails at generating diversity, while GMM-UNIT achieves a higher IS due to solving a simpler task. The StarGAN*-like GMM-UNIT produces deterministic outputs with low LPIPS scores. GMM-UNIT is evaluated on multi-domain facial attribute translation using the CelebA dataset. The dataset contains 202,599 face images annotated with 40 binary attributes, with some attributes being mutually exclusive and others inclusive. Images are resized to 128x128, with 2,000 images for testing and the rest for training. GMM-UNIT models each attribute as a different component, allowing for generation of images with various attribute combinations. Unlike StarGAN and DRIT++, GMM-UNIT can handle mutually inclusive attributes. In an experiment with five binary attributes (hair color, gender, age), GMM-UNIT can generate 32 domains. Results show the model can translate images to simple attributes or combinations like blonde hair and male, demonstrating its stochastic approach. The GMM-UNIT model demonstrates a stochastic approach in generating images with various attribute combinations. It outperforms StarGAN* in quality and diversity of generated images. The model is also effective in style transfer tasks, extracting style from reference images to create sharp and realistic results. The GMM-UNIT model can generate new images with rare attributes not present in the training dataset. It is able to create images in previously unseen combinations of attributes, making it useful for imbalanced tasks. The model can also interpolate attributes within and across domains. The GMM-UNIT model can generate new images with rare attributes not present in the training dataset, allowing for interpolation within and across domains. Ablation studies show the importance of various loss functions in maintaining image quality and diversity. The GMM-UNIT model allows for image-to-image translation across multiple domains with a stochastic approach. It disentangles image content from attributes using a GMM, enabling continuous encoding of domains. This results in superior quality and diversity compared to state-of-the-art models, with fewer parameters. Future work includes learning mean vectors of the GMM from data and expanding its capabilities. Our deep neural models, based on MUNIT, BicycleGAN, and StarGAN, utilize Instance Normalization for the content encoder and Adaptive Instance Normalization and Layer Normalization for the decoder. The discriminator network employs Leaky ReLU. Notations include D for number of domains, N for output channels, K for kernel size, S for stride size, P for padding size, CONV for convolutional layer, GAP for global average pooling layer, and UPCONV for upsampling layer. The discriminator's layer count is reduced. In experiments, the discriminator's layer count is reduced. The Adam optimizer is used with specific parameters and learning rate schedule. Different batch sizes and loss weights are set for different datasets. GMM is simplified with specific properties for experiments. Random mirroring is applied during training. The covariance matrices in the GMM are diagonal with the same value on all components, making each Gaussian component spherical. Additional results for one-to-one domain translation are presented, comparing GMM-UNIT with state-of-the-art methods. DRIT++ shows issues with realism, while other methods achieve acceptable diversity. Different models are compared visually, with some being one-to-one domain translation models and others being multi-domain models. In Table 9, quantitative results on the CelebA dataset are shown per domain, along with generated images in comparison with StarGAN*. Attribute interpolation within a domain is demonstrated in Figure 12. Table 10 displays additional ablation results on the Digits dataset. Figure 13 visualizes attribute vectors in a 2D space using the t-SNE method, showing attributes sampled from the distribution and those extracted by the encoder E z."
}