{
    "title": "rklz9iAcKQ",
    "content": "Deep Graph Infomax (DGI) is a novel approach for unsupervised learning of node representations in graph data. It maximizes mutual information between patch representations and high-level graph summaries, without relying on random walk objectives. DGI shows competitive performance on node classification benchmarks, sometimes outperforming supervised learning methods. In this work, an alternative objective for unsupervised graph learning based on mutual information is proposed, instead of random walks. Deep InfoMax (DIM) is introduced for learning representations of high-dimensional data by maximizing mutual information between global and local input parts. This approach encourages the encoder to capture specific information types. Deep Graph Infomax (DGI) is introduced as a method for unsupervised graph learning, adapting ideas from Deep InfoMax (DIM) to the graph domain. DGI consistently outperforms strong baselines in transductive and inductive classification tasks. Contrastive methods, such as DGI, are essential for unsupervised learning of graph representations. These methods train an encoder to distinguish between representations capturing statistical dependencies and those that do not. The scoring function in contrastive approaches aims to increase scores on \"real\" input and decrease scores on \"fake\" input. Various scoring techniques, including classification, are used in graph literature. DGI's objective involves classifying local-global pairs and negative-sampled counterparts. Sampling strategies play a crucial role in these methods. Recent work on unsupervised graph representation learning focuses on classifying local-global pairs and negative-sampled counterparts. Sampling strategies, such as node-anchored sampling and curriculum-based negative sampling, are key implementation details in contrastive methods. Positive samples are pairs of nodes that appear together in short random walks, while negative samples are selected through random pairs or a curriculum-based approach. Predictive coding, like Contrastive Predictive Coding (CPC), is also utilized in these methods. Our approach contrasts global and local parts of a graph simultaneously, unlike CPC and other predictive models that focus on structurally-specified parts of the input. The sole prior works that contrast \"global\" and \"local\" representations on graphs do so via (auto-)encoding objectives on the adjacency matrix. The Deep Graph Infomax method presents an unsupervised learning setup for graphs, focusing on node features and relational information in the form of an adjacency matrix. Unlike previous methods, it offers a scalable approach without relying on matrix factorization-style losses. The objective is to learn an encoder for each node in a graph using graph convolutional encoders. These encoders generate node representations by aggregating over local node neighborhoods, producing patch representations that summarize a patch of the graph centered around the node. The approach focuses on maximizing local mutual information. The approach to learning the encoder for each node in a graph involves maximizing local mutual information. This is achieved by obtaining graph-level summary vectors using a readout function and a discriminator to assign probability scores to patch-summary pairs. Negative samples are generated by pairing the summary with patch representations from an alternative graph. The approach involves maximizing local mutual information by using a noise-contrastive objective with binary cross-entropy loss. This maximizes mutual information between nodes and summary vectors based on JensenShannon divergence. The approach maximizes mutual information between nodes and summary vectors by preserving similarities on the patch-level, aiming for patches to establish links to similar patches across the graph. This is connected to classification error of the discriminator and mutual information maximization on graph representations. The probability distribution of graphs, p(X), with a finite number of elements, |X|, is discussed in relation to a deterministic readout function on graphs and summary vectors. The optimal classifier between joint distribution p(X, s) and product of marginals p(X)p(s) has an error rate upper bounded by the set of all graphs mapped to summary vectors. Samples from the joint are drawn from the product of marginals, with a probability ratio that is maximized at 1. The probability of drawing any sample from the joint is bounded above, leading to lower classification error. The error rate of classifying samples from the joint distribution as coming from the product of marginals can be bounded by a certain probability. This bound is achieved when the readout function is injective for all elements. Assuming an injective readout function and a sufficient number of allowable states, the error rate can be further constrained. The optimal summary under classification error is when the number of allowable states is equal to the number of elements in X. This is proven by the injectivity of the readout function and the minimization of error when p(s(k)) is uniform. Mutual information is maximized when s has the same number of states as X, as it is invariant under invertible transforms. The optimal summary under classification error is achieved when the number of allowable states equals the number of elements in X. Mutual information is maximized by minimizing classification error in the discriminator, but this alone is not sufficient for learning useful representations. Discrimination between global and local representations is necessary for effective learning. The Deep Graph Infomax procedure involves sampling negative examples, obtaining patch representations through an encoder, and using binary cross-entropy loss for optimization. This process aims to maximize mutual information between neighborhood and high-level features for effective learning. The Deep Graph Infomax algorithm involves obtaining patch representations for the input graph through an encoder and summarizing them using a readout function. The learned representations are then used for node classification tasks in a fully unsupervised manner, showing competitive results. The experimental setup involves using a Graph Convolutional Network (GCN) model for transductive learning tasks on benchmark datasets like Cora, Citeseer, and Pubmed. Different encoders and corruption functions are employed for transductive learning, inductive learning on large graphs, and multiple graphs. The experimental setup involves using a Graph Convolutional Network (GCN) model for transductive learning tasks on benchmark datasets like Cora, Citeseer, and Pubmed. The propagation rule BID23 is applied with a parametric ReLU function BID17 and a learnable linear transformation \u0398. The corruption function encourages proper encoding of structural similarities in the graph by preserving the original adjacency matrix while shuffling the features row-wise to create a corrupted graph. DGI is shown to be stable to other choices. For inductive learning on large graphs, the GCN update rule is no longer used in the encoder. Instead, the mean-pooling propagation rule is applied, as demonstrated by GraphSAGE-GCN. The encoder for Reddit consists of a three-layer mean-pooling model with skip connections. Different corruption functions may be used, but those preserving the graph structure yield the strongest features. The encoder for Reddit is a three-layer mean-pooling model with skip connections. It computes 512 features in each layer with PReLU activation. Due to the dataset's size, a subsampling approach is used to select minibatches of nodes and obtain subgraphs around them. Each subsampled patch has 2611 nodes, and only computations for the central node's patch representation are performed. These representations are used to derive a summary vector for the minibatch. The encoder for the PPI dataset is a three-layer mean-pooling model with dense skip connections. It computes 512 features in each layer using a learnable projection matrix and PReLU activation. The corruption function is defined by row-wise shuffling feature matrices within subsampled patches to encourage diversity in negative samples. The patch representation is then submitted to the discriminator for inductive learning on multiple graphs. In a multiple-graph setting, randomly sampled training graphs are used as negative examples, with dropout applied to input features for further diversity. Standardizing embeddings across the training set before logistic regression is found beneficial. Identical readout functions and discriminator architectures are employed across all experimental settings, with a simple averaging of nodes' features used for the readout function. The curr_chunk discusses the use of a simple averaging of nodes' features for readout, with a mention that more sophisticated readout architectures may be needed for larger graphs. It also mentions the use of a bilinear scoring function for discriminator scores. Models are initialized using Glorot initialization and trained to maximize mutual information. The curr_chunk discusses training details such as optimizer, learning rate, early stopping strategy, and evaluation metrics for transductive and inductive datasets. Results of comparative evaluation experiments are summarized in TAB1, including mean classification accuracy after training and logistic regression. The results show strong performance in training logistic regression on raw input features and DeepWalk with concatenated features. The DGI approach competes well with GCN models, even outperforming on certain datasets like Cora and Citeseer. The DGI approach allows nodes access to graph properties, outperforming supervised encoders but not transductive methods like GraphSGAN BID7. DGI excels over unsupervised GraphSAGE on Reddit and PPI datasets, showing potential in inductive node classification. Competitive results on Reddit, but PPI performance gap attributed to sparse node data. The extreme sparsity of node features hinders encoder performance. Randomly initialized graph convolutional networks serve as a strong baseline. DGI improves upon this baseline, indicating previous negative sampling methods may have been ineffective. Deeper encoders yield more significant results. The DGI algorithm improves encoder performance by reducing variability in positive/negative examples' pool. Shallower architectures performed better on some datasets. Wider models are beneficial with the DGI loss function. Analysis focused on Cora dataset, showing discernible clustering in 2D projections of embeddings. The Deep Graph Infomax (DGI) algorithm improves encoder performance by reducing variability in positive/negative examples' pool. It exhibits discernible clustering in 2D projections, respecting the seven topic classes of Cora. The Silhouette score is 0.234, showing competitive performance even after removing half the dimensions from patch representations. Insights into DGI's learning mechanism reveal biased embedding dimensions for negative examples and encoding useful information for positive examples. Further qualitative and ablation studies can be found in Appendix B. The new approach for learning unsupervised representations on graph-structured data leverages local mutual information maximization across patch representations obtained by graph convolutional architectures. This results in node embeddings that consider the global structural properties of the graph, leading to competitive performance in transductive and inductive classification tasks. The approach is evaluated on standard citation network benchmark datasets like Cora, Citeseer, and Pubmed, where nodes represent documents and edges represent citations. The unsupervised learning algorithm allows access to all nodes' feature vectors for predictive evaluation on 1000 test nodes. Using a large Reddit post dataset, the objective is to predict the community based on GloVe embeddings and metrics. Inductive learning is done on multiple graphs with training on posts from the first 20 days of the month. The dataset used for inductive learning consists of graphs representing different human tissues, with 20 graphs for training, 2 for validation, and 2 for testing. Each node has 50 features including gene sets and immunological signatures. The discriminator scores attached to nodes were visualized using t-SNE visualizations. The discriminator scores of nodes in the graphs representing human tissues were visualized using t-SNE. In the positive Cora graph, only a few \"hot\" nodes received high discriminator scores, indicating a distinction between embedding dimensions for discrimination and classification. Negative examples showed no strong structure, but some achieved high discriminator scores due to certain factors. The analysis of the learnt DGI embeddings revealed distinct dimensions biased towards positive and negative examples. The presence of strong biases in certain dimensions was necessary to push negative examples down in the discriminator, while positive examples countered this bias and encoded patch similarity. The embeddings were visualized for top-scoring positive and negative examples to support this claim. The analysis of the learnt DGI embeddings revealed biased dimensions towards positive and negative examples, with positive examples encoding patch similarity. By removing biased dimensions first, classification performance remains competitive even after removing over half of the embedding dimensions. Alternative corruption functions for producing negative graphs were also considered. For the node classification task, DGI is stable and robust to different strategies. However, designing appropriate corruption strategies for other tasks is still an open research area. The corruption function described preserves the original adjacency matrix but corrupts the features by shuffling them. An alternative corruption function is considered, which preserves the features but adds or removes edges from the adjacency matrix by sampling a switch parameter. The corruption rate parameter \u03a3 ij determines whether to corrupt the adjacency matrix at position (i, j) by performing XOR operations. At a corruption rate of \u03c1 = 0, the adjacency matrix remains unchanged. DGI struggles to learn in this scenario but performs well at higher noise rates. Simultaneous feature shuffling and adjacency matrix perturbation are also considered, with DGI still learning useful features under this compound corruption strategy. The positive graph perturbation rate affects node embeddings for classification on Cora. Features learned for classification tasks are most effective when the negative graph has similar connectivity levels to the positive graph. Classification performance peaks at a high but sparse graph perturbation level, with diverse negative examples. Performance is only slightly impacted at higher corruption rates, still outperforming unsupervised baselines. DGI is stable and robust under corruption functions that modify both feature and adjacency matrices on the Cora dataset. Feature shuffling provides diverse negative examples without significant computational costs, leading to improved classification accuracy."
}