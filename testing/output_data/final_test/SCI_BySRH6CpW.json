{
    "title": "BySRH6CpW",
    "content": "Recent breakthroughs in computer vision involve using large deep neural networks with GPUs for speed. To address real-time processing challenges on limited hardware, training networks with binary or ternary weights can reduce memory size. LR-nets introduce a new method for training neural networks with discrete weights using stochastic parameters. By modifying the local reparameterization trick, binary and ternary models were tested on various benchmarks, achieving state-of-the-art results. Deep Neural Networks have driven advancements in machine learning, particularly in computer vision. Training networks with binary or ternary weights can reduce memory usage and improve inference speed on low-power hardware. However, backpropagating errors with discrete weights remains a challenge. One suggested heuristic is to use stochastic weights for forward pass and gradient computation. In an attempt to train neural networks with discrete weights, a more principled approach is proposed. Instead of using stochastic weights for forward pass and gradient computation, a smooth approximation is found to train the network. This method is based on modeling pre-activation using a smooth distribution and utilizing the reparameterization trick for computation. The pre-activation is modeled using a smooth distribution and the reparameterization trick BID9 is used to compute derivatives. This method can be used to train discrete weights, not just continuous Gaussian distributions. The approximation of pre-activation using Lyapunov CLT closely matches the actual pre-activation when sampling weights in a feed-forward setting. In this work, a novel method for training neural networks with discrete weights is presented. Binary and ternary weights were experimented with, with ternary weights proving easier to train. The method achieved state-of-the-art results on various datasets, including ResNet-18 on ImageNet. The approach allows for training binary and ternary networks to match the performance of the original models on MNIST and CIFAR-10 datasets. The training algorithm presented in this work achieves state-of-the-art results on various datasets, including MNIST and CIFAR-10, by using discrete weights. Compared to related works like expectation-backpropagation BID16 and BID18, this algorithm is different as it utilizes a stochastic forward pass. The method focuses on optimizing discrete distributions using a Gaussian approximation for the pre-activation distribution. The curr_chunk discusses the use of stochastic weights for discrete weight training, comparing different approaches to training binary or ternary networks. The method involves sampling binary weights for the forward pass and gradient computation, updating the stochastic weights, and discretizing during the forward pass. This approach differs from other methods that use the \"straight-through\" estimator for gradient back-propagation. The authors proposed training a ternary quantization network with learned discrete weights, achieving good performance and memory savings. However, using learned discrete weights adds computational overhead compared to binary or ternary networks. Weight compression can reduce memory requirements without significant loss of precision, but binary or ternary weight networks offer higher speedup capabilities, especially on dedicated hardware. The algorithm uses a stochastic network model with independently sampled weights from a multinomial distribution. The algorithm utilizes independently sampled weights from a multinomial distribution to train a ternary quantization network. To minimize the loss function, the reparameterization trick is suggested for optimizing continuous distributions. Sampling weights and running the model with different weights is inefficient for learning Bayesian networks. The reparameterization trick involves sampling pre-activations instead of weights, reducing run time significantly. It allows for optimization of networks with discrete weights by approximating them well with a Gaussian distribution. The reparameterization trick involves approximating discrete weights with a Gaussian distribution for efficient optimization. The algorithm computes gradients using smooth approximations and updates parameters accordingly. During forward pass, weights means and variances are computed, and sampling is done to return the final output. Backwards phase involves computing gradients for backpropagation and optimization using first-order methods like Adam. In this section, we discuss the finer implementation details needed to achieve state-of-the-art performance by initializing distributions over discrete weights from pretrained continuous deterministic weights. The aim is to have the mean value as close as possible to achieve optimal results. To achieve optimal results, the aim is to initialize distributions over discrete weights from pretrained continuous deterministic weights. The threefold goal is to have the mean value close to the original weight, low variance, and avoid deterministic initial distributions. The initialization modifies the probability of weights being zero to avoid being too deterministic. The initialization proposed in eq. 6 adjusts the probability of weights being zero to prevent deterministic distributions. This modification aims to maintain the mean value close to the original weight and avoid vanishing gradients. Adding L2 regularization on distribution parameters can prevent violations and low entropy distributions. This regularization, referred to as probability decay, was found to significantly improve performance. In some cases, the opposite effect was observed in binary settings. Further details on the impact of regularization can be found in the appendix. In binary settings, weight probabilities tend to get stuck at mid-values around 0.5, leading to undesired outcomes. To address this, a beta density regularizer is added to encourage probabilities closer to 0 or 1. Unlike ternary weights, binary weights require more precise fine-tuning for optimal performance. At the end of training, neurons in the hidden layer show deterministic weight distributions. An experiment with probability decay shows better approximation. Sampling weights multiple times for evaluation leads to minor performance improvement due to low entropy in trained distributions. Extensive experiments were conducted on MNIST, CIFAR-10, and ImageNet benchmarks. In experiments on MNIST, CIFAR-10, and ImageNet benchmarks, results are presented in binary and ternary settings. The first layer is binary or ternary, leading to memory and energy savings. MNIST dataset contains 60K training images and 10K test images of digits 0-9. No preprocessing or augmentation is used for this dataset. The architecture used follows a specific format with a 5x5 ReLU convolution. In the architecture for BID12, a 5x5 ReLU convolution layer and a 2x2 max-pooling layer are used. Batch Normalization is incorporated after every convolution layer. The loss is minimized with Adam and dropout is applied with a drop rate of 0.5. The batch size is 256, initial learning rate is 0.01, and is reduced by 10 after 100 epochs. Test error rate is reported after 190 training epochs on the CIFAR-10 dataset. The architecture for BID12 uses a 5x5 ReLU convolution layer and a 2x2 max-pooling layer. Batch Normalization is applied after each convolution layer, and Adam is used to minimize the loss with a dropout rate of 0.5. The images are preprocessed by subtracting the mean and dividing by the standard deviation, with padding during training and random cropping. The VGG inspired architecture includes convolution, max-pooling, and fully connected layers, with Batch Normalization after each convolution layer. The ResNet-18 architecture is used for image classification on the ImageNet dataset. Preprocessing involves subtracting the mean pixel and dividing by the standard deviation. Training includes random cropping and flipping of images. Test error rate is reported after 290 epochs of training. The ResNet-18 architecture is used for image classification on the ImageNet dataset. Preprocessing involves random cropping and flipping of images. The loss is minimized with Adam, using a batch size of 256 and initial learning rate of 0.01. Results are reported after 65 training epochs for the binary setting and after 55 training epochs for the ternary setting. Visualizations of network kernels are shown in FIG4, 3b, and 3c. In experiments with binary and ternary networks, the ternary network performs better from the start compared to the binary network, especially on harder tasks like ImageNet classification. Ternary networks are considered more reasonable as they do not force each neuron to affect all next-layer neurons. In this work, a novel algorithm was presented to train neural networks with discrete weights, specifically focusing on ternary weights. Results on image classification datasets showed state-of-the-art performance in both binary and ternary settings, highlighting the potential for more efficient training and inference of low-power consuming neural networks. Ternary weights were advocated as a more reasonable model than binary weights, with potential for further research into sparse ternary networks to reduce computation and memory overhead. Our proposed method for training neural networks with ternary weights outperforms using Gumbel-softmax on weights due to lower variance and shared randomness. The experiments on CIFAR-10 show significant improvement over existing methods. Our proposed regularization for training neural networks with ternary weights avoids the issues of activation saturation and vanishing gradients seen in Gumbel-softmax. By comparing average entropy with and without probability decay, we ensure the correctness of our approach. Experimental results on CIFAR-10 demonstrate the effectiveness of our regularization method. The proposed regularization for training neural networks with ternary weights causes a slight increase in entropy, especially in later training stages. The 'kink' in the graph is due to lowering the learning rate after 170 epochs. The regularization has a more significant effect on some layers, as shown in FIG6 for the last convolutional layer. The entropy at the end of training is higher than at initialization, and analyzing the probabilities helps explain this behavior."
}