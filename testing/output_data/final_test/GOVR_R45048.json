{
    "title": "R45048",
    "content": "Federal education legislation, specifically the Elementary and Secondary Education Act (ESEA) amended by the Every Student Succeeds Act (ESSA), requires states to use test-based accountability systems for schools receiving Title I-A funding. This funding provides aid to local educational agencies for disadvantaged students, with a budget of $15.5 billion in FY2017. States must assess students annually in reading, math, and science to receive Title I-A funds. Assessment results are used for educational accountability and to inform parents and stakeholders about school performance. The emphasis on educational assessment within federal education policies has led to debates about the time spent on tests in schools and the fit between assessments and their intended uses. This report provides Congress with an overview of different types of assessments and their uses in support of federal policies. The report provides an overview of assessments in relation to federal policies, explaining basic concepts and considerations. It aims to clarify how assessments required under federal programs fit into the broader landscape of educational assessments. This information can assist readers in understanding the uses of educational assessment in conjunction with policies and programs. The report discusses various types of assessments in elementary and secondary education, including the concept of balanced assessment systems. It also covers technical considerations like validity, reliability, and fairness in assessments to draw appropriate conclusions. The information can be applied broadly to all assessments used in schools, including those required by federal legislation. The report covers various types of assessments in elementary and secondary education, including those required by federal legislation like the ESEA and IDEA. It highlights assessment concepts applied in actual school assessments, ranging from small-scale classroom assessments to large-scale international assessments. Assessment results can be reported at individual or group levels, with some having high-stakes consequences. Achievement testing is the most common type of assessment used in educational settings. The report discusses state assessments required by the ESEA, including annual reading and math assessments for grades 3-8 and high school, as well as science assessments in three grade spans. Results are used for state accountability systems. States may also require exit exams for high school graduation, which can cover various subjects. The National Assessment of Educational Progress (NAEP) is a series of assessments used since 1969 for students in grades 4, 8, and 12 covering various subjects. States receiving Title I-A funding must participate in reading and math assessments for grades 4 and 8. Some students also participate in international assessments. The Programme for International Student Achievement (PISA), the Progress in International Reading Literacy Study (PIRLS), and the Trends in International Mathematics and Science Study (TIMSS) are international assessments that countries can choose to participate in. Participation is voluntary, and a sample of students is selected to take the assessment. States must provide special services to students with disabilities and English learners based on eligibility determined by specific assessments. Educational assessment is crucial for determining eligibility for special services for students with disabilities and English learners. It involves assessing students in various domains like listening, speaking, reading, and writing. While multiple assessments may seem excessive, each serves a specific purpose in evaluating student performance. Policymakers use this information to make decisions on the number and type of assessments needed in elementary and secondary education. The framework provided in the report addresses various purposes of assessment, balanced assessment systems, and scoring. It also discusses technical considerations and drawing conclusions from assessment results. A glossary at the end offers definitions of assessment terms for additional technical information. Educational assessments are designed for specific purposes, and results should be used accordingly. The report discusses the various purposes of assessment, including instructional, diagnostic, predictive, and evaluative. These assessments are used to modify instruction, identify student needs, predict outcomes, and evaluate performance. The issue of \"over-purposing\" tests can undermine their validity in education. Instructional assessments, such as pretests and progress monitoring, help teachers analyze student knowledge and adjust teaching methods. Standardized tests may not align well with classroom content, and diagnostic assessments are crucial for identifying student needs. Diagnostic assessments provide a comprehensive picture of a student's strengths and weaknesses beyond academic achievement. They are used to identify eligibility for special education services based on cognitive functioning, behavior, social competence, language ability, and academic achievement. States develop criteria for eligibility and select assessments consistent with IDEA requirements. Special education eligibility assessments include cognitive functioning tests like WISC for intellectual disabilities, screenings for speech-language impairments, and rating scales for emotional disturbances. Informal measures like parent interviews and classroom observations may also be used. The ESEA requires diagnostic assessments to determine limited English proficiency in students. States must assess ELs annually in speaking, listening, reading, and writing aligned with state standards. Most states use the WIDA consortium for linguistically diverse students, offering the ACCESS 2.0 test. Predictive assessments like benchmark tests are used to track progress towards predetermined goals. The ESEA requires diagnostic assessments for limited English proficiency students, using the WIDA consortium for linguistically diverse students. States can offer more intensive instruction or special services to students not on-track for meeting end-of-year achievement goals. Common assessments like PARCC and SBAC are aligned with the Common Core State Standards to meet federal assessment requirements. Interim assessments are predictive of end-of-year performance. Evaluative assessments determine the outcome of a curriculum, program, or policy by comparing results to predetermined goals. Unlike other types of assessments, they do not provide actionable information on students or schools. State accountability assessments serve an evaluative purpose, such as determining proficiency levels in reading and math under the ESEA. States must conduct annual assessments in reading and mathematics for students in grades 3 through 8 and once in high school. Results are used in the state accountability system to differentiate schools based on student performance. Some states use common assessments like PARCC and SBAC, while others use state-specific assessments. The assessment indicators required by the accountability system are based on evaluative assessments, making it challenging to use them for instructional or predictive purposes. A balanced assessment system is necessary to cover all educational assessment purposes. A balanced assessment system is essential to cover all educational assessment purposes, including evaluative, instructional, diagnostic, and predictive assessments at the state and local levels. This system may include a combination of formative and summative assessments to guide and improve instruction. The purposes of assessment are embedded within formative and summative assessments. Formative assessments are used during learning to improve instruction, while summative assessments are used at the end to summarize what students have learned. The distinction between formative and summative assessments lies in how the results are used to inform decision-making processes in curriculum, instruction, or policy. Formative assessments are used during learning to improve instruction, while summative assessments are used at the end to summarize what students have learned. A teacher may give a pretest to plan instruction (formative assessment) and a posttest for grading (summative assessment) in a balanced assessment system. Formative assessments are varied and can range from small-scale classroom assessments to large-scale interim assessments used to track student progress towards policy goals. The lack of universal agreement on what constitutes a formative assessment has led to confusion in the education industry, with some opting to avoid the term altogether. Various types of assessments, including classroom-based and interim assessments, will be discussed in this section. Formative assessments in the classroom can be informal or formal, used for instructional and predictive purposes. Results help identify student knowledge gaps and guide instruction adjustments. Teachers may change pace, delivery method, or repeat content. Feedback and re-assessment are key in the formative assessment process. Some testing experts question the accuracy of calling interim assessments \"formative\" as they may not provide timely information for instructional guidance. However, others believe these assessments can inform changes at the school or district level to improve student achievement. Interim assessments track progress towards policy goals. Interim assessments track progress towards policy goals for students, schools, and LEAs. Unlike classroom-based formative assessments, interim assessments are less flexible as they are determined by the state, not the teacher. Interim assessments are less flexible as they are determined by the state, not the teacher. Results may not be reported quickly enough for classroom instruction. They can predict school/district goals and identify areas needing support. Summative assessments are given at the end of a lesson or school year to evaluate learning. Most reported test results are based on summative assessments. State exit exams are high-stakes summative assessments with consequences for students. Under the ESEA, assessments in reading and math are used to evaluate schools. Not all summative assessments have consequences, like end-of-unit tests. NAEP and international assessments provide a broader view of achievement without major consequences. Test scores are reported in various ways, comparing individuals to peers or indicating proficiency. Misinterpreting scores can lead to inaccurate conclusions about academic performance. Common methods of score reporting include norm-referenced tests, criterion-referenced tests, scaled scores, and performance standards. Understanding the reported score is essential to avoid misunderstandings. Norm-referenced tests (NRTs) compare individual student performance to a normative sample using mean and standard deviation. Standard scores, often reported as percentiles, show how a student performed relative to peers. Commercial cognitive and achievement tests are typically norm-referenced. Norm-referenced tests (NRTs) compare individual student performance to a normative sample using mean and standard deviation. They are useful for making comparisons across schools, districts, or states. NRTs are criticized for measuring only superficial learning through multiple choice and short-answer formats. Norm-referenced tests (NRTs) are faulted for measuring superficial learning through limited formats and lacking instructional utility. They are criticized for not being linked to standards or curriculum, making interpretation difficult for educators. In contrast, Criterion-referenced tests (CRTs) compare individual performance to a predetermined standard, focusing on mastery of specific curriculum and content skills. Unlike NRTs, CRTs report scores of \"absolute standing\" against a criterion, not against a normative sample. Mastery of curriculum and content skills is determined through a collaborative process involving policymakers, educators, and measurement professionals. Different levels of mastery are set through various measurement techniques and professional judgment. Mastery can be defined as answering a certain percentage of items correctly or demonstrating proficiency in a content area. Unlike NRTs, CRTs do not compare students to a normative group but focus on individual performance against a predetermined standard. CRT results can be reported in various formats such as grades, pass/fail, or scaled scores. Criterion-referenced tests (CRTs) are versatile assessments that can be designed in various formats to meet educational needs. They are directly linked to standards and curriculum, allowing for planning and modifying instruction. While CRTs are cost-effective and time-efficient, they do not facilitate comparisons across schools, LEAs, and states due to the lack of a normative sample for standardized comparisons. CRTs do not have a normative sample for standardized comparisons, making it challenging to compare results across schools, LEAs, and states. To enable comparisons, consistent standards and definitions of \"mastery\" are needed. Test designers use scaled scores and performance standards to create a common metric for comparisons in elementary and secondary education. Scaled scores represent mastery levels on a scale, while performance standards describe the level of mastery achieved based on the student's grade level. Scaled scores are standardized scores used for accountability in state assessments. They are adjusted based on form difficulty and allow for comparisons across students and subgroups. Scaled scores are typically three- or four-digit scores with cut points indicating mastery levels, sometimes accompanied by a grade level. Comparisons can be made directly between students and subgroups using scaled scores. Scaled scores are used for accountability in state assessments and can be vertically scaled for comparison across time. They are independent of grade level but the descriptors attached to the scores change with grade level. Performance standards are another way to report results from assessments. Performance standards, also known as achievement levels, define levels of performance in a content area using cut scores. These cut scores indicate mastery or proficiency, with varying levels established. For example, NAEP has three achievement levels, while SBAC and PARCC use four and five-level systems, respectively, to assess student performance. Performance standards, or achievement levels, define levels of performance using cut scores in a content area. They can align with state standards and curricula, providing context to scores. For example, a score of 242 out of 500 may not mean much without context, but if the cut score to meet expectations is 240, then the student would be considered to have \"met expectations.\" Performance standards help in planning, modifying, and adapting instruction based on results. Performance standards are criticized for being imprecise and unable to measure student growth effectively. Cut scores may create inappropriate distinctions between students with similar abilities, as seen in the example where a score of 238 may not meet expectations while a score of 242 does. This insensitivity to student abilities is a major drawback of performance standards. Performance standards are criticized for being insensitive to student growth, as cut scores may not accurately reflect progress. Technical considerations like validity, reliability, and fairness are important for test developers to investigate and report statistical information to users. Test users must interpret this information to assess results accurately. The importance of validity, reliability, and fairness in interpreting test results is crucial for making appropriate inferences. Understanding these concepts helps in avoiding inappropriate conclusions from educational assessments, especially in high-stakes situations. Validity is key when making decisions based on assessments, as it determines the accuracy of the test. The validity of a test is not inherent to the test itself but rather the appropriateness and meaningfulness of the inferences drawn from it. In educational assessments, it is crucial to ensure that conclusions are valid, especially in high-stakes situations. For example, the SAT is used to measure critical thinking skills for college success, and drawing valid inferences from test results is essential. The validity of inferences drawn from test results, such as the SAT, is crucial for assessing college success. Validity is tied to the test's purpose, requiring a process of validation to support score interpretation based on the test construct. The process of validation involves investigating construct underrepresentation and construct irrelevance in assessment instruments. Construct underrepresentation refers to important aspects of a concept not being measured, while construct irrelevance can affect the validity of inferences drawn from test scores. Construct irrelevance in assessment refers to how test scores can be affected by content not related to the intended construct. For example, if an assessment is meant to measure addition and subtraction skills, including multiplication or division questions would create construct irrelevance. This can impact the accuracy of inferences drawn from assessment scores. Statistical procedures are used to investigate whether the assessment adequately covers all skills within the construct and if any skills are outside the intended scope. The properties of assessments can interact with individuals taking the test, especially in terms of construct irrelevance. This can lead to certain subgroups, like those from higher socioeconomic backgrounds, performing better due to exposure to irrelevant material. This can result in invalid inferences about subgroup performance. Validation involves collecting various types of evidence to ensure the accuracy and effectiveness of assessments. This includes comparing student scores with existing measures and examining how well the assessment predicts future outcomes. Reliability, on the other hand, refers to the consistency and precision of measurement when tests are repeated on different groups. Reliability in assessments is a measure of certainty in the accuracy of results. It assumes each student has a true score, which is the average score from multiple test administrations. The observed score differs from the true score due to measurement error, influenced by student and environmental factors. Reliability increases as measurement error decreases, ensuring a closer match between observed and true scores. Reliability in educational assessment is crucial for ensuring that a student's observed score closely aligns with their true score. This can be measured using a reliability coefficient, which ranges from 0 to 1. A coefficient of 1 indicates perfect consistency, while 0 implies the score is solely due to measurement error. Most assessments aim for coefficients above 0.8, with many exceeding 0.9. Reliability coefficients above 0.9 are crucial in educational assessment. Common types include alternate-form, test-retest, inter-scorer agreement, and internal consistency coefficients. Alternate-form coefficients ensure consistency between different forms of an assessment, like the SAT. Test-retest coefficients measure score stability over time. The reliability coefficients in educational assessment, such as test-retest and inter-scorer agreement coefficients, provide certainty in score consistency over time and between scorers. Internal consistency coefficients measure item correlation within an assessment, ensuring consistent performance on related items. The student should score well on multiplication but poorly on division in the mathematics assessment. The type of reliability coefficients reported depends on the assessment's purpose and format. For example, test-retest reliability may not be relevant for a test measuring short-term student growth. Test format also influences the choice of reliability coefficients to report. Test developers consider the format of the test, with multiple-choice tests having less concern for inter-scorer agreement compared to constructed response tests like essays. Reliability in assessment ensures accurate results, often reported with a confidence interval to estimate the likelihood of a student's true score falling within a range. A confidence interval is calculated using an estimated true score, standard error of measurement (SEM), and desired level of confidence. It is reported as a range of scores with a lower and upper limit, commonly seen in education at 90%, 95%, or 99%. The size of the interval changes with the degree of confidence, with a 90% confidence interval for a student with an estimated true score of 100 and SEM of 10 being 84 to 116. The confidence interval ranges from 84 to 116, with a 5% chance of the true score being lower than 84 or higher than 116. Increasing the confidence level to 95% widens the interval to 80 to 120, containing the true score 95% of the time. A 99% confidence interval extends the range to 74 to 126, capturing the true score 99% of the time. The range of scores in a confidence interval increases as the desired level of confidence rises. The consistency of classification in educational assessments is important, especially for high-stakes decisions. It ensures that students with similar abilities are consistently placed in the same performance standard category. If inconsistencies occur, it may indicate a reliability issue with the assessment. Consistency of classification is crucial in educational assessments to ensure students with similar abilities are consistently placed in the same performance standard category. This is particularly important for high-stakes decisions such as placing students in achievement levels based on state assessments or awarding high school diplomas. Inconsistencies in classification can lead to unreliable accountability systems and diploma awarding processes. Modeling shows that classification can vary based on assessment reliability and cut scores, impacting decisions like eligibility for special education services. Students suspected of disabilities undergo various assessments, with results interpreted according to state IDEA categories. Students may be \"declassified\" over time if they improve academically or if definitions change, influenced by assessment reliability. Fairness in educational assessments is influenced by the reliability of assessments and cut scores used for determining eligibility for special education services. The concept of fairness includes lack of bias, equitable treatment in testing, equality in testing outcomes, and opportunity to learn. Test bias is a common criticism in educational assessments, but not well documented or understood. Bias in educational assessments can occur based on subgroup membership, even when true scores are the same. Factors like culture, language, or disabilities can influence test scores, leading to controversy and difficulty in addressing bias. While statistical techniques like differential item functioning can detect bias in specific test items, they may not address bias in result interpretation. Test bias undermines result validity, but a simple score difference between subgroups doesn't always indicate bias. The issue of bias in educational assessments can arise based on subgroup differences, even when true scores are equal. Fairness in testing, ensuring equitable treatment for all students, is emphasized over bias detection. It is crucial to provide all students with equal opportunities and appropriate testing conditions. Equitable treatment in testing requires providing appropriate conditions for all students, including a comfortable environment, equal response time, and accommodations for disabilities and ELs. Discrepancies in test preparation services outside the classroom can undermine assessment validity. When high-stakes decisions are based on test results, fairness issues can arise if certain groups are disadvantaged. For example, if advantaged students are more likely to pass state exit exams than disadvantaged students, it can impact their ability to graduate, pursue higher education, and secure employment. This inequality in outcomes can further disadvantage the less privileged students. \"Fairness in educational assessment is crucial, especially when high-stakes assessments like state exams can impact outcomes for students. The concern lies in whether all students have had equal opportunities to learn the material being tested. If disadvantaged students have not had the same access to education, it may be unfair to assess them against the same standards as their more privileged peers.\" The difficulty in providing equal opportunity to learn is a systemic issue, not specific to individuals or schools. It raises questions about whether exposure to the same curriculum is enough for students to learn and how the school environment and teacher quality impact learning opportunities. Test users must assess the validity, reliability, and fairness of assessments to make appropriate inferences about student achievement. Conducting a thoughtful analysis of the assessment in terms of construct, purpose, type of scores, and evidence is crucial. When assessing student achievement, it is crucial to consider the construct, purpose, type of scores, and evidence of validity, reliability, and fairness. Inappropriate inferences can lead to unintended consequences if these factors are not carefully examined. Sample questions can help determine the appropriateness of conclusions drawn from assessments, such as identifying the content area being assessed and the specific construct being measured within that area. Understanding the construct of an assessment is crucial when comparing test results. For example, PISA measures general \"mathematical literacy,\" while TIMSS is curriculum-based. The difference in constructs can lead to varying performance results, highlighting the importance of how mathematics is taught in different countries. The United States scored above the international average on a TIMSS assessment and below on a subsequent PISA assessment. It is important to consider that TIMSS and PISA measure different constructs, so it is not appropriate to conclude that math achievement in the US is declining based on these results. Understanding the original purpose of assessments, such as whether they are for instructional, predictive, diagnostic, or evaluative purposes, can help determine how the results should be interpreted and used by teachers, administrators, and policymakers. For example, state assessments designed for evaluative purposes may not be suitable for modifying instruction for individual students. Interim assessments are designed for predictive purposes and provide timely results for teachers to target instruction to students who scored poorly. They are often aligned with state summative assessments but may not be definitive indicators of state assessment scores. Some summative assessments do not have associated interim assessments, and the alignment between interim and summative assessments may vary. The interim assessment may not align well with the summative assessment. Timing issues can arise if the content is not covered before the interim assessment. Questions about scores include comparing performance to others or to a standard. The score reported compares a student's performance to a criterion or standard, determining proficiency or meeting expectations in a content area. Misinterpreting scores is common, so understanding the assessment scale and score reporting is crucial. NRTs compare scores to peers but cannot determine proficiency, while CRTs in scaled scores or performance standards can. When using scaled scores from CRTs, meaningful comparisons between students and subgroups can be made. Vertically scaled scores allow for measuring student growth over time. Performance standards are used to align with state content standards but can be difficult to interpret, categorizing students based on their performance. Performance standards can be challenging to interpret as students within the same category may not score equally well. It is also difficult to measure a student's growth based on these standards. Test users should be cautious about equating student performance within the same category and making assumptions about growth based on movement through categories. The text discusses the importance of evaluating the technical quality of assessments, including validity, reliability, fairness, and bias. It mentions that commercially available assessments usually come with a manual detailing this information, while locally developed assessments may have evidence available upon request. Evaluating the quality of evidence provided is a complex task. It is important to evaluate the technical quality of assessments, including validity and reliability, especially for subgroups like students with disabilities and ELs to prevent bias. Different types of assessments require specific reliability evidence, such as inter-scorer reliability for essay tests. Inter-scorer reliability is crucial for assessments like the SAT with multiple forms to prevent bias. All assessments have some degree of error and bias, impacting the validity of scores. High-stakes assessments require careful consideration of reliability to ensure fair outcomes. In high-stakes assessments, reliability and validity evidence must be strong for fair outcomes. Context of assessment affects criticality of inference from test scores. Low-stakes assessments like classroom formative tests may not require exhaustive review of evidence. For high-stakes assessments like state exit exams, it is crucial to ensure validity and reliability to defend the inferences made. Poor validity can lead to testing on irrelevant content, affecting graduation outcomes. Similarly, poor reliability can result in students passing or failing due to measurement errors. Protections like allowing multiple exam attempts are sometimes implemented to safeguard against unfair outcomes. When making high-stakes decisions like state exit exams for graduation, using multiple measures of achievement can lead to more valid inferences. States may collect additional data or use various indicators to ensure fairness and accuracy in the assessment process. This approach helps protect against making invalid inferences based on a single measure with weaker evidence of validity and reliability. Educational assessments play a crucial role in elementary and secondary education, guiding instructional decisions and education policy. Multiple measures are recommended to avoid bias and ensure validity. Assessments track student academic achievement and are used in high-stakes decisions, emphasizing the importance of understanding their purpose. Educational assessments are crucial for guiding decisions in education and policy. It is important to understand their purpose and consider the appropriateness of inferences based on assessment results."
}