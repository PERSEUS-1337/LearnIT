{
    "title": "HkinqfbAb",
    "content": "Recently, there has been a surge in interest in neural network compression techniques that aim to reduce network size without sacrificing performance. Most current methods involve post-processing steps like parameter tying via quantization or pruning irrelevant edges. In this paper, a new algorithm is proposed that simultaneously learns and compresses a neural network by adding Gaussian priors and a sparsity penalty to the optimization criteria. This approach is easy to implement, generalizes L1 and L2 regularization, and enforces parameter tying and pruning constraints effectively. Experimental results demonstrate the effectiveness of this method. The new algorithm proposed in this paper enforces parameter tying and pruning constraints to compress neural networks effectively. It achieves state-of-the-art compression on standard benchmarks with minimal loss in accuracy and requires little hyperparameter tuning. Neural networks have shown remarkable performance in various domains but their large storage requirements make them impractical for certain applications. Additionally, they can potentially overfit due to being trained on small datasets compared to their number of parameters. Recent work has shown that a large proportion of neural network parameters are not necessary for generalization performance, leading to increased interest in model compression. Various methods such as pruning, quantization, low-rank approximation, group lasso, variational dropout, and teacher-student training have been proposed. Automatic parameter tying has been a focus, with a soft parameter tying scheme based on mixtures of Gaussians prior being suggested. Various compression methods have been proposed for neural networks, including a random parameter tying scheme based on hashing functions and a compression pipeline involving thresholding and k-means clustering. Other approaches include imposing a Gaussian mixture prior on parameters and a full Bayesian approach using scale mixture priors. These methods aim to achieve high compression rates without significant loss in accuracy. Recent work in neural network compression includes the use of scale mixture priors to estimate the significance of learned weights. BID1 introduced a soft-to-hard quantization approach for compression, while BID24 and K. Ullrich proposed a Gaussian mixture model approach, which can be computationally expensive. These methods aim to achieve high compression rates while preserving accuracy. The approach of BID13 for neural network compression uses separate pruning and parameter tying stages, which may limit compression efficiency. The parameter tying approach is only applied layerwise and requires more clusters before random weight sharing is effective. The BID13 approach for neural network compression involves separate pruning and parameter tying stages. Parameter tying can be inefficient with a small number of distinct parameters. The Bayesian approach for compression includes additional parameters to tune and requires sampling for prediction. The compression method in this work uses quantization and sparsity inducing priors, specifically an independent Gaussian prior for quantization. The compression method in this work involves non-probabilistic assignment to Gaussian distributions with a prior penalizing weights based on distance to the mean. It allows tying weights together, reduces hyperparameters compared to probabilistic methods, and includes pruning with a penalty for compression. This approach yields state-of-the-art results on benchmark datasets for neural network compression. In this work, a neural network is learned by minimizing a regularized loss function where parameters are partitioned into sets and constrained to be equal within each set. This alternative form of regularization achieves quantization and improves generalization performance. Parameter tying is a key component in neural networks, particularly in CNNs, where weights are shared across a layer. The goal is to discover parameter tying without prior knowledge, optimizing both parameters and cluster assignments. The problem of finding the optimal partition of parameters into clusters is intractable, so a relaxed version is considered where parameters are softly constrained to be close to their average cluster values. The regularizer function R is chosen to be a clustering distortion penalty on the parameters, specifically the k-means loss J(W , \u00b5), which is the sum of the distance between each parameter and its corresponding cluster center. This approach incorporates k-means directly into the objective as a prior probability over the weights, with the hope of improving quantization of the weights after training. The k-means prior is proposed to guide training towards good parameter tying, inducing quantization effectively with fewer parameters compared to a GMM prior. As clusters move closer together, the GMM prior leads to significant overlap, while the k-means prior performs comparably. The GMM prior in training can lead to poor practical performance as weights may not commit to a single cluster, resulting in lower accuracy. Issues with variances approaching zero can be mitigated by adjusting learning rates, annealing the GMM objective, or imposing hyperpriors on Gaussian parameters. Significant tuning may still be necessary for optimal solutions. Following the approach of BID13, storing model parameters using quantization with K distinct values can achieve a high compression rate. However, quantization alone is not sufficient for state-of-the-art compression. Post-processing with entropy coding can improve compression, but the k-means prior does not take advantage of this. In addition to quantization for compression, network pruning can result in sparse parameters that are efficiently stored and transmitted. By encouraging a zero cluster, weights close to zero can be dropped, leading to further compression. This approach complements entropy coding and k-means prior for improved compression techniques. In network pruning, a sparsity-inducing penalty is added to the learning objective, resulting in increased model sparsity without significant loss in accuracy. A two-stage approach is proposed: soft-tying minimizes the objective using gradient descent, while hard-tying replaces the penalty with a constraint forcing parameters in each cluster to be equal. This method enhances compression techniques by dropping weights close to zero. In network pruning, a sparsity-inducing penalty is added to the learning objective to increase model sparsity without accuracy loss. The optimization of the (sparse) APT objective L (3) is proposed using a block coordinate descent algorithm that alternately optimizes with respect to W and \u00b5. The method involves gradient descent on L using backpropagation for W optimization. The K-means problem can be solved exactly in polynomial time in the one-dimensional case using dynamic programming. The selection of K is done using a validation set, and nonparametric Bayesian methods could be employed for automatic selection in practice. The optimization of the sparse APT objective involves gradient descent on L using backpropagation for W optimization. The k-means algorithm precisely solves the problem of optimizing with respect to \u00b5 given fixed W. The standard EM-style k-means algorithm performs coordinate descent with respect to auxiliary variables A and \u00b5. The k-means algorithm optimizes cluster means under assignments A, using specialized 1-D techniques to speed up the process. Only full k-means is run occasionally for an approximate solution, with minimal impact on results. The k-means algorithm optimizes cluster means under assignments A using specialized techniques. The process involves redrawing cluster boundaries and updating partition means. Once optimized, soft-tying is replaced with hard-tying, where clustering is fixed, and parameters are updated subject to tying constraints. In hard-tying, data loss is optimized via projected gradient descent. In hard-tying, the gradient update involves calculating partial derivatives using backpropagation and setting the gradient components corresponding to parameters in a cluster to their average. This method allows weight tying across layers and incurs a linear time overhead per training iteration based on the number of network parameters. Soft-tying adds constant time per training iteration and has a memory requirement of O(N). It can be implemented using Tensorflow BID0 for optimization. Hard-tying involves updating parameters and performing assignments. Neural network parameters are initialized using a method proposed by BID9. Cluster centers are initialized heuristically. The experiments involve using APT on LeNet-300-100 to analyze the impact of k-means prior, number of clusters, and frequency of updates on accuracy. Another set of experiments focuses on the effect of APT on generalization performance of neural networks. Lastly, performance of sparse APT is compared with other methods under compression metrics. A typical parameter distribution produced by APT is shown using LeNet-300-100 on MNIST dataset. APT leads to clear parameter clusters, with K=8 sufficient for preserving the solution from soft-tying. Soft-tying does not significantly affect convergence speed or final model performance. Hard-tying can result in accuracy loss for small K, but recovers with larger K. APT leads to clear parameter clusters, with K=8 sufficient for preserving the solution from soft-tying. The effect of coordinate switching frequency on learning outcome was explored, showing that APT is generally not sensitive to k-means frequency except for very small K. Random tying is disastrous for small K, inducing significant quantization loss. Specialized training methods exist for networks with K=2 or 3, as APT cannot effectively quantize with such a small number of clusters. APT generally holds for sparse APT as well, with \u03bb 1 and \u03bb 2 impacting model sparsity and cluster convergence. Traditional regularization methods may not significantly affect the generalization capability of deep networks. The paper discusses a fundamental phase change in deep networks' generalization capability, emphasizing the importance of model architecture over regularization tuning. It explores a new model complexity concept based on the number of free parameters in parameter-tied networks. A comparison between APT and GMM prior performance is conducted on a toy problem involving bit-string shift detection. In a comparison between APT and GMM prior performance on a toy problem involving bit-string shift detection, various methods were evaluated with a common set of SGD step sizes and maximum training budget. The regularization parameter \u03bb was tuned for 2 penalty, APT, and GMM, with GMM using four individual learning rates for network parameters. 1280 experiments were launched for all possible combinations of parameter steps. The study compared APT and GMM regularization methods on various datasets with different network structures. Results showed that the choice of regularization had a mild effect on test error, while changing the network structure had a stronger impact on performance. Automatic parameter tying or norm restriction did not significantly improve regularization performance. In experiments comparing sparse APT with other compression methods on neural networks like LeNet and VGG-16, sparse APT was performed by softtying and then hard-tying for a set number of iterations. The parameter K was found to be sufficient for different network sizes to achieve \u2264 1% accuracy loss. Lambda values were tuned within a specific range for compression. For compressing LeNets and VGG-16, different training methods were used such as Adadelta and SGD with specific parameters. Training from scratch did not achieve the same accuracy as from a pre-trained model. Lambda values were tuned within a specific range for compression. The budgets for iterations were 80000/20000, starting with a pre-trained model with 7.3% error. Results are presented in TAB1, showing error rates, non-zero weights fraction, pruning scores, and maximum compression rates. Different compression criteria were evaluated separately for each variation of BC. Maximum compression scores were obtained by clustering final weights into 32 clusters. SWS used K=17 for LeNets, while sparse APT used K=17 for LeNets and K=33 for VGG-16. Sparse APT outperforms competitors on various datasets, except for BC methods in terms of max compression on LeNet-5 and VGG-16. Sparse APT achieves sparser solutions than BC variants due to the use of Huffman coding for compression. The primary difference lies in the clustering approach, with BC solutions not returning many equal-sized clusters. Sparse APT can achieve sparsity with a small number of parameters by tuning variances of the Gaussian prior. It allows for a trade-off between accuracy and sparsity, with the ability to select the desired performance level using a validation set. The sparsity/accuracy trade-off curve shows that selecting the smallest value of K can provide good accuracy and compression. The experiment with LeNet-300-100 showed that sparsity and quantization had little impact on generalization performance. Cluster centers in the experiment tended to oppose each other due to k-means loss J and independent Gaussian priors. Different values of K were tested, and overfitting was not observed with soft-tying or hard-tying. The experiment showed that different values of K did not significantly affect solution quality with soft-tying. However, switching to hard-tying resulted in significant accuracy loss for small K values, which decreased to zero for K = 32. Another set of experiments examined the impact of k-means frequency on model performance for various K values, with different numbers of gradient iterations between k-means runs. The best error rates were considered after hyperparameter search. The experiment showed that different values of K did not significantly affect solution quality with soft-tying. However, switching to hard-tying resulted in significant accuracy loss for small K values, which decreased to zero for K = 32. Model performance degrades with large t, particularly for smaller K, after hyperparameter search. Visualizations show structured sparsity in weights, resulting in entire units being pruned away. The experiment demonstrated that different K values did not impact solution quality with soft-tying, but hard-tying led to accuracy loss for small K values. Model performance worsened with large t, especially for smaller K values. Visualizations revealed structured sparsity in weights, leading to entire units being pruned. The first layer weights of LeNet-300-100 learned with sparse APT showed a column-sparsity of 48.6%, pruning away 403 of the 784 input units. The layer weight matrix of LeNet-300-100 has 784 input connections reshaped as a 28 \u00d7 28 cell. Sparse APT results in 76.3% row-sparsity. Comparing the number of input units pruned by 2, 1, and sparse APT on LeNet-300-100."
}