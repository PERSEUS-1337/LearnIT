{
    "title": "SJlWyerFPS",
    "content": "The DeepXML algorithm addresses limitations in deep extreme multi-label learning by introducing a novel architecture that splits training of head and tail labels. It increases accuracy by learning word embeddings on head labels, transferring them to tail labels, increasing negative training data, and re-ranking predicted labels. DeepXML algorithm efficiently scales to problems with millions of labels, outperforming XML-CNN and AttentionXML in training speed and accuracy. It is designed for deep extreme multi-label learning on short text documents like search engine queries. DeepXML is a deep extreme multi-label learning algorithm that can efficiently train on problems with millions of labels, surpassing XML-CNN and AttentionXML in speed and accuracy. It significantly improves prediction accuracy in matching advertiser bid phrases to user queries on web search engines. DeepXML reformulates the problem of matching queries to bid phrases as an extreme multi-label learning task, treating each of the top 3 Million advertiser bid phrases as a separate label. DeepXML reformulates the problem of matching queries to bid phrases by treating the top 3 Million advertiser bid phrases as separate labels. Extreme multi-label learning is used to predict relevant subsets of bid phrases given an input query, with applications in recommending Bing queries and predicting webpage clicks from titles alone. State-of-the-art extreme classifiers struggle with scalability and accuracy for short text documents. While feature engineering can help, deep learning methods like XML-CNN and AttentionXML outperform them by learning task-specific features. DeepXML aims to improve accuracy for short text documents by specializing architectures for head and tail labels, which are challenging due to data scarcity. This approach leads to accuracy gains in predicting rare tail labels, which are more rewarding than common head labels. DeepXML improved accuracy and scalability by partitioning labels into head and tail labels. It used tf-idf weighted linear combination for document representation and learned word-vector embeddings on head labels. A novel residual connection further boosted accuracy. DeepXML achieved state-of-the-art accuracies by fine-tuning the residual connection based document representation for tail labels, without sacrificing scalability. Modifications were made to the Slice classifier for pre-trained embeddings to train the tail residual connection efficiently. DeepXML improved performance by re-ranking predicted labels to eliminate hardest negatives for base classifier with minimal increase in training time. Experiments showed accuracy gains over XML-CNN and AttentionXML, with DeepXML being significantly faster to train on medium scale datasets. Additionally, XML-CNN and AttentionXML struggled to scale to a proprietary dataset for matching queries to bid phrases. DeepXML outperformed Slice, Parabel, and other techniques in accuracy by at least 19 percentage points on a dataset with 3 million labels and 21 million training points. It proposed a more accurate architecture for short text documents and an efficient training algorithm, making it significantly better at matching user queries to advertiser bid phrases compared to leading techniques in production. Source code for DeepXML is available. The paper discusses extreme multi-label classification techniques, categorized into learning with pre-computed features and jointly learning feature representation with the classifier. Various studies and researchers are mentioned in both categories. The paper explores extreme multi-label classification techniques, mentioning studies by Bhatia et al., Tagami, You et al., Krichene et al., and Barezi et al. Traditional approaches used sparse BoW features, but for short text documents, deep learning representations are more effective. Existing deep learning approaches for extreme classification are either not scalable or do not achieve state-of-the-art accuracy. XML-CNN is accurate on short text documents but not scalable beyond a million labels, while AttentionXML is slightly more scalable. The scalability of deep learning techniques in extreme multi-label classification degrades due to the linear cost in the number of labels. Negative sampling is traditionally used to address this issue, but at an extreme scale, it needs to be applied more aggressively. Various approaches like tree-based and hashing-based methods have been proposed to eliminate this problem. In extreme multi-label classification, scalability of deep learning techniques is hindered by the linear cost in the number of labels. Approaches such as approximate nearest neighbor sub-sampling and Maximum Inner Product Search aim to speed up predictions, but may lead to accuracy loss. Tail labels are harder to predict due to data scarcity but can be more informative. Propensity based precision and nDCG are the main focus, with trimming tail labels showing only marginal performance decay. Approaches to improve performance on tail labels include directly optimizing propensity scored metrics and treating tail labels as outliers. These methods have shown to boost performance on tail labels but are not suitable for short text documents due to limitations in feature support and long training times. Different approaches for matching user queries to bid phrases include embeddings based, sequence-to-sequence models, and query graph based models. State-of-the-art deep extreme classifiers are not scalable or accurate for short text documents due to limitations in trigger coverage, suggestion density, and recommendation quality. Query graph based models have limited trigger coverage, while Sequence-to-sequence models have expensive training and prediction costs. Efficient structures like trie have been used to reduce output complexity but at the cost of limited bid phrase coverage. DeepXML addresses limitations in accuracy and scalability for short text documents by using a feature representation inspired by FastText and an output layer inspired by Slice. Combining FastText and Slice required design modifications to achieve accuracy and scalability. DeepXML improves accuracy and scalability for short text documents by combining FastText and Slice. Previous attempts at supervised learning with FastText led to lower accuracies than other methods. DeepXML addresses these limitations by adding a new output layer inspired by Slice. DeepXML improves accuracy and scalability for short text documents by combining FastText and Slice. It adds a nonlinearity and residual block to enhance FastText's expressive power. The word-vector embeddings are trained on a small number of head labels and then transferred to the tail for fine-tuning, increasing accuracy and scalability. Model parameters are learned with Binary Cross Entropy loss and Adam optimizer. DeepXML-h can be efficiently trained with a fully connected final layer on a single GPU. The label set L h contains only a small subset of labels, with a size that does not grow beyond 0.2M even for datasets with millions of labels. An approximate nearest neighbour search (ANNS) structure is trained over label centroids L, with a shortlist size of 300 during prediction. However, ANNS trained over label centroids may lead to poor recall values for labels with highly different contexts. For example, in the WikiTitle-500K dataset, 280K articles are tagged with the 'living people' tag. DeepXML-h improves recall and precision by clustering documents using the KMeans algorithm into c clusters for each label in the set L h. This leads to a 5% increase in recall@300 and 6% precision@300 with a shortlist of size 300. DeepXML-t still relies on Slice for fine-tuning the residual block in the tail network and learning weights in the fully connected output layer for millions of tail labels. Slice reduces time for forward pass and backpropagation by representing labels with normalized mean of feature vectors and using ANNS data structure to determine likely labels for data points. Efficient with fixed feature representation but slows down when feature representation is being learnt and requires constant updating of ANNS data structure. DeepXML speeds up training by redefining label representation to be the unit normalized mean of document representation before the residual block, leading to a loss in training accuracy. This can be compensated by requiring Slice to generate 3x more nearest labels, but it significantly increases training time. The training process in DeepXML was optimized by adding randomly sampled negative labels to the shortlist, reducing training time without loss in accuracy. Classifier and ANNS scores are merged into a single vector for computing the final DeepXML score. The average cost of prediction involves computing dense feature representation and applying a sparse-sigmoid function. The training process in DeepXML was optimized by adding randomly sampled negative labels to the shortlist, reducing training time without loss in accuracy. DeepXML-RE, on the other hand, learns a re-ranker with training cost logarithmic in the number of labels by training over a shortlist of negative labels. This approach leads to only a 10-20% increase in training time compared to DeepXML. DeepXML-RE utilizes the same architecture as DeepXML-t, with a word embedding layer, residual block, and classifier. Model parameters are learned using binary cross entropy loss and SparseAdam optimizer. During prediction, DeepXML-RE evaluates on labels shortlisted by DeepXML, incurring a prediction cost of O(d|s|). Experiments were conducted on the Q2B-3M dataset, with 3 million labels, comparing DeepXML to Slice and Parabel. Additionally, comparisons were made with Simrank++ and a BERT-based sequence-to-sequence model for query keyword prediction. Experiments were also carried out on four moderate size datasets. The study compared DeepXML to various deep learning and BoW methods on moderate size datasets for tasks like tagging Wikipedia pages, suggesting relevant articles, and item-to-item recommendation of Amazon products. DeepXML was evaluated against XML-CNN, Attention-XML, Slice, AnnexML, PfastreXML, Parabel, XT, and DiSMEC. The implementation of algorithms was provided by the authors, and DeepXML has 7 hyperparameters. The study compared DeepXML to various deep learning and BoW methods on moderate size datasets for tasks like tagging Wikipedia pages, suggesting relevant articles, and item-to-item recommendation of Amazon products. DeepXML outperformed state-of-the-art BoW-based approaches and showed accurate and diverse predictions. The label threshold was chosen via cross-validation, and DeepXML was evaluated against XML-CNN, Attention-XML, Slice, AnnexML, PfastreXML, Parabel, XT, and DiSMEC. DeepXML outperformed BoW methods on various tasks, showing accurate predictions with different feature representations and classifiers. Sub-word features and pre-trained features were also utilized for improved precision. DeepXML-fr, DeepXML-NS, and DeepXML-ANNS classifiers were compared, with DeepXML proving to be more accurate and scalable. DeepXML is an algorithm that improves accuracy and scalability for extreme multilabel learning on text data compared to existing approaches like Slice, AttentionXML, and XML-CNN. It outperforms pre-trained representations like FastText, BERT, and SIF, showing up to 10% more accuracy. DeepXML can lead to a 1.0-4.3 percentage point gain in performance and is 33-42\u00d7 faster at training than AttentionXML. Additionally, DeepXML is up to 15 percentage points more accurate than leading techniques for matching search engine queries to advertiser bid phrases. DeepXML's gains are mainly in predicting tail labels and short documents, showing improvement in learning word representations for richer associations between words. The method allows for well-clustered words in a semantic space, extracting useful information about document labels even with limited word co-occurrences. Parameter settings for different data sets are listed in Table 5. Experiments were conducted with specific hardware and software configurations. Parameter settings for DeepXML on different datasets were detailed in Table 5, including the use of a random-seed of 22 on a P40 GPU card with specific software versions. DeepXML-h and DeepXML-t may have different values for certain parameters, with DeepXML-t utilizing a shortlist of size 500 during training and a shortlist of size 300 queried from ANNS at prediction time. Labels were divided into two sets based on frequency, with a splitting threshold \u03b3 chosen accordingly. The splitting threshold \u03b3 is chosen to cover most features in documents with at least one instance of label in set L h and |L h | < 0.2M. DeepXML has two components, DeepXML-h and DeepXML-t, trained on L h and L t. DeepXML-RE shows 3-4% better accuracy on propensity scored metrics and up to 2% more accuracy on vanilla metrics compared to DeepXML. PfastreXML outperforms DeepXML and DeepXML-RE on AmazonTitles-3M in propensity scored metrics but suffers a 10% loss on vanilla precision and nDCG. Performance evaluation in extreme multi-label settings has shown a substantial 10% loss in vanilla precision and nDCG, which is deemed unacceptable for real-world applications. Propensity scored precision@k and nDCG@k were used as unbiased metrics, along with vanilla precision@k and nDCG@k for extreme classification. Deep architectures like CNN, MLP, and LSTM with Attention have been utilized to learn rich features, but except for AttentionXML, these methods suffer from low accuracy. AttentionXML is a method that shows low accuracy on tail labels and degrades for short text documents. Parabel learns a hierarchy over labels to reduce training cost, but performs poorly on low dimensional features. Slice uses negative sampling to reduce training complexity in extreme classification and has been shown to scale to 100 million labels. DeepXML experiments were conducted with various configurations including DeepXML-f, DeepXML-fr, DeepXML-NS, and DeepXML-SW, each utilizing different layers and techniques such as word embeddings, ReLU non-linearity, dropout, and sub-word features. These variations aimed to improve accuracy and scalability in extreme classification tasks. DeepXML experiments included variations like DeepXML-ANNS and DeepXML-P, utilizing character tri-grams and word embeddings with different classifiers. DeepXML-ANNS trained with hardest negatives labels selected via ANNS, requiring multiple training of the ANNS graph. DeepXML-P used a shallow tree-based classifier proposed in AttentionXML."
}