{
    "title": "Bkln2a4tPB",
    "content": "Hierarchical multi-task dynamical systems (MTDSs) allow direct user control over sequence generation by using a latent code z for customization. Hierarchical multi-task dynamical systems (MTDS) enable style transfer, interpolation, and morphing in generated sequences, improving predictions through latent code interpolation. Time series data often exhibit characteristic differences, such as handwriting style, patient response to anesthesia, or motion capture data. Such variations can be effectively modeled and controlled by an end user, allowing for more personalized and accurate predictions compared to standard RNN approaches. Hierarchical multi-task dynamical systems (MTDS) enable style transfer, interpolation, and morphing in generated sequences, improving predictions through latent code interpolation. This approach considers latent variable models where a latent variable z characterizes each sequence, moving beyond the limitations of standard RNN approaches. In this paper, a more powerful form of customization is investigated for hierarchical multi-task dynamical systems (MTDS). The approach involves modulating all system and emission parameters based on a learned low dimensional manifold indexed by the latent variable z. This customization results in improved performance, greater data efficiency, and robustness to unfamiliar test inputs. The MTDS proposed in this paper allows full adaptation of all parameters of general dynamical systems via a learned nonlinear manifold. Experimental studies demonstrate data efficiency, user control, and robustness to dataset shift, surpassing existing approaches to time series modeling. The paper introduces a new model for time series modeling, allowing user control and style morphing in mocap data. It discusses the model, challenges in learning and inference, relation to existing work, experimental setup, and results. The model adapts parameters of dynamical systems via a learned nonlinear manifold, demonstrating data efficiency and robustness. The paper introduces a Multi-Task Dynamical System model for time series modeling, with parameters adapted via a learned nonlinear manifold. It describes two general choices of the base model, including a deep latent Gaussian model. The model is further detailed with a multi-task linear dynamical system for a given z. The paper introduces a Multi-Task Dynamical System model for time series modeling, with parameters adapted via a learned nonlinear manifold. It describes two general choices of the base model, including a deep latent Gaussian model. The model is further detailed with a multi-task linear dynamical system for a given z. The parameterization of \u03b8 z must satisfy constraints for stability and positive definiteness. An alternative formulation of the LDS is proposed, allowing for stability enforcement through a diagonal matrix \u03a3 z and orthogonal matrix Q z. The dynamics of a multi-task RNN (MT-RNN) are described, along with the emission model. The Multi-Task Dynamical System model introduces a deep latent Gaussian model and a multi-task linear dynamical system. The Orthogonal RNN (ORNN) is created with an orthogonal transition matrix for long-term dependencies. Parameters \u03c6 of an MTDS can be learned from a dataset, and approximate learning is done via the ELBO. The ELBO optimization involves maximizing a lower bound of the marginal likelihood using reparameterization for unbiased gradients. Learning a sensible latent representation can be challenging with a powerful RNN, but KL annealing can help avoid suboptimal solutions. Initial optimization without the KL penalty and initializing values can be beneficial in experiments. The posterior predictive distribution for unseen test sequences is estimated via Monte Carlo. Inference networks may perform poorly for novel test sequences, and standard approximate inference techniques may be preferred. The latent code can be varied during state rollout to simulate tasks that vary over time. The MTDS model performs full parameter customization on both dynamics and emission distributions, different from other proposed dynamical models. Specialized applications include using small deterministic nonlinear dynamical systems or RNNs as base models with parameters dependent on latent variables. The matrix depends on z via multilinear decomposition. Various methods have been proposed for controlling and customizing sequence prediction in video data, including disentangling time-varying and static features using GAN architecture and parts-based decomposition. However, customization of dynamic evolution remains a challenge in these approaches. Hierarchical approaches for dynamical systems with time-varying parameters are proposed in Luttinen et al. (2014) and Karl et al. (2017) for local LDS approximation. Rangapuram et al. (2018) predict parameters of time-varying LDS models directly via an RNN. Multi-task GPs are commonly used for sequence prediction, but MTGPs have limitations in terms of linearity and integration of inputs. Hierarchical approaches for dynamical systems with time-varying parameters are proposed in Luttinen et al. (2014) and Karl et al. (2017) for local LDS approximation. Rangapuram et al. (2018) predict parameters of time-varying LDS models directly via an RNN. Multi-task GPs are commonly used for sequence prediction, but MTGPs have limitations in terms of linearity and integration of inputs. The performance of the MTDS is investigated on synthetic data generated by linear superposition of damped harmonic oscillation (DHO) and real-world mocap data for human locomotion. The generative model for J oscillators with constant amplitudes \u03b3 and variable frequency and decay factors is considered. The DHO data is modeled using an MTLDS with deterministic state X = R 4 and a k = 4 latent variable z. All LDS parameters were adapted via the latent z except D := 0 and the emission variance s 2, which was learned using the MCO algorithm. The MCO algorithm is used for optimization to obtain a tighter bound than the ELBO. Different approaches like bias customization and training a Pooled LDS are compared for model convergence. The models are assessed for adaptation to test sequences with varying training set sizes. The MTLDS method outperforms single-task and pooled approaches in predictive posterior estimation. Results show significant advantages for MTLDS over Pooled-LDS for all training sizes. Bias customization alone is insufficient for improved performance. Generalization is effective even with a small number of training examples. The MTLDS method outperforms single-task and pooled approaches in predictive posterior estimation, even with a small number of training examples. The STL approach does not significantly outperform MTLDS, and has a much longer runtime due to higher dimensionality and conditioning issues. The dataset consists of 31 sequences in 8 styles, each representing a 21-joint skeleton in a Lagrangian frame. The model used is a recurrent 2-layer base model with a GRU and RNN, along with a linear decoding layer. The parameters include a matrix inducing a bottleneck between layers. The model consists of a 2-layer GRU network with 1024 units in the first layer and a non-gated architecture in the second layer. Sequences are split into segments of length 64 for learning, without appending predictions to inputs. This approach aims to force the model to recover from mistakes without the need for additional input data. The model utilizes a 2-layer GRU network with 1024 units in the first layer and a non-gated architecture in the second layer. It optimizes using variational procedures and compares different inference methods for better performance. Additionally, bias customization models and ablation tests are implemented for comparison on a new dataset. The study evaluates the performance of the MTDS, MTBias, and Pooled models using closed-loop and open-loop settings. Data efficiency is tested by training models on subsets of the original dataset. Single-task GRU models are also trained on different style data sets. Results are shown in Figure 3a, with the MTDS, MTBias, and Pooled models performing as expected. The MTDS model demonstrates greater data efficiency, achieving close to minimum error with only 7% of the dataset. MTBias and Pooled models require more data to achieve similar performance. Generalization to novel sequence styles is tested using a leave-one-out setup, showing competitive results. The MTDS, MTBias, and Pooled models are compared based on their performance on test sequences. The multi-task models show better customization and memory retention over long intervals compared to the competitor models. Customization to arms is challenging due to novel test motions, while legs and trunk are easier. Style transfer is explored by manipulating the latent z for different input styles. The MTDS model uses target style s2 encoded by z(s2) for style transfer. A classifier is used to test if the target style can be recognized from the data generated by MTDS. Successful style transfer results in the classifier assigning a high probability to the target style. The MTDS demonstrates good control over prediction style, while MTBias shows reduced control for some (source, target) pairs. Visualizations of the latent space show a sensible manifold of walking styles learned by MTDS. The MTDS model demonstrates good control over prediction style and uses a k = 2 latent embedding for style transfer. The latent space visualization shows a nuanced representation of different styles, highlighting the limitations of relying solely on task labels. Smooth style interpolation is possible in the latent space, allowing for dynamic morphing of styles in animations. The MTDS framework extends dynamical systems with a hierarchical structure for multi-task learning. The MTDS framework allows customization of all parameters for multi-task learning, enabling fine-grained sequence variation embedding and modulating predictions. Explicit task inference enhances prediction control, aids generalization from small datasets, and improves robustness against distribution changes at test time. The choice of a unit Gaussian distribution for the model allows for simple sampling schemes, straightforward posterior approximations, and continuous deformation of outputs. Alternative choices, such as a uniform distribution over a compact set, present challenges in posterior approximation. Default choices for model parameters include affine operators and multilayer perceptrons (MLPs), but for large parameter spaces, using an RNN with low rank matrix transformations may be more practical. The MLP parameterization can shift the density in parameter space to more appropriate regions via nonlinear transformation, yielding highly non-linear changes even to simple dynamical systems. It might be advantageous to curve the manifold to avoid such phenomena and make more expressive choices. The hidden dynamics of a linear dynamical system can be transformed by an invertible matrix while maintaining the same distribution over emissions. Parameterizations of the matrices A, B, R, and S in the MTLDS can benefit from specific choices to avoid degeneracy. The stability constraint for matrix A involves ensuring its singular values lie within the unit hypercube. The singular value decomposition (SVD) of a matrix A can be transformed using an orthogonal matrix without imposing additional constraints on other parameters in the linear dynamical system. Orthogonal matrices, like the one obtained through the Cayley transform, allow for flexibility in handling negative eigenvalues. To allow for negative eigenvalues, pre-multiply by a diagonal matrix E with elements in {+1, \u22121}. This leads to A = \u03a3Q, where \u03a3 is a diagonal matrix with elements in [\u22121, +1] and Q is a Cayley-transform of a skew-symmetric matrix. The stability constraint can be satisfied with A = \u03a3Q. The scale \u03ba of the latent system can be chosen arbitrarily without affecting A, but fixing the scale of B can prevent degeneracies in a hierarchical model. The text discusses parameterization techniques for matrices B, R, and S. For matrix B, a transformation using tanh can be applied for sparsity, while matrices R and S must be in the positive definite cone. Various parameterization methods are available for enforcing positivity, such as exponentiation or Cholesky decomposition. An alternative learning algorithm is proposed for the matrix R = LL^T, ensuring uniqueness by enforcing a positive diagonal. This approach, useful for unsupervised cases, constructs a lower bound for marginal likelihoods using Monte Carlo Objectives. Jensen's inequality is used to derive the lower bound on the log marginal likelihood. The lower bound on the log marginal likelihood can be increased by adding more samples. The gradient can be easily calculated if p(z) is re-parameterized. Importance sampling from the prior may help attract posterior distributions towards the prior. Sampling from the prior can be amortized over each iteration. Sampling from the prior can be amortized over each sequence to calculate likelihood inexpensively. Low-discrepancy random variates can reduce variance. Resampling a small number of particles for each sequence can reduce backpropagation cost. The dynamics forward from a particle z m can no longer be amortized over all {Y (i) }. Algorithm 1 is primarily restricted to unsupervised problems. Inference at test time can be performed by variational or Monte Carlo approaches. For stochastic state models, additional reasoning will be required. The No U-Turn Sampler (NUTS) may be a gold standard for inference over z. Sequential Monte Carlo methods face challenges in incorporating previous information efficiently, as the evaluation of posterior at time t+1 scales linearly with time t. However, by sequentially integrating past information, the process can be optimized to reduce inference runtime at each stage. Sequential Monte Carlo methods encounter difficulties in maintaining particle diversity over time due to severe particle depletion. To address this issue, Chopin (2002) suggests using 'rejuvenation steps' involving a Markov transition kernel applied to each particle, although this approach requires running until convergence and additional computation for diagnosis. Sequential Monte Carlo methods face challenges in maintaining particle diversity over time due to sample impoverishment. To combat this, Chopin (2002) proposes using 'rejuvenation steps' with a Markov transition kernel on each particle, requiring additional computation for diagnosis. Another alternative is the assumed density filtering (ADF) approach, which involves Bayesian updates and projecting the posterior into a parametric family Q using reverse KL Divergence. This method aims to address the issue of sample impoverishment by carefully choosing forward and backward Markov kernels. The ADF method involves projecting the posterior into a parametric family Q using reverse KL Divergence to avoid mode seeking behavior. The performance of ADF depends on the choice of Q, with challenges in optimization when Q is expressive. Tomasetti et al. (2019) suggest re-using previous gradient evaluations for efficiency, and variance reduction techniques can improve convergence in the inner loop. In experiments, sampling approaches were found faster and more reliable for updates, using iterated importance sampling to update the posterior. The proposal distribution is improved using a mixture of Gaussians for complex multimodal posteriors. The AdaIS procedure, using adaptive importance sampling, is efficient and robust for updating the posterior distribution. It provides a good parametric approximation of the true posterior, avoiding sample impoverishment issues. AdaIS is faster than MCMC moves and converges quickly without using stochastic gradients. The scheme benefits from the fast convergence rates of the EM algorithm, allowing for early stopping of iterations. In practice, one may not wish to calculate a posterior at every t, but instead intervals of length \u03c4. In DHO experiments, \u03c4 = 5 is used with ESS > 0.6M after 4 inner iterations. The MoG parameterization is used for multimodal posteriors, capturing target distribution well. Sobol sequences can reduce sampling variance. A conference paper at ICLR 2020 discusses the parameterization of A. The parameterization of A, B, and C in the STLDS model involves using various functions and matrices to learn a sparse representation. The prior distribution p(z) is a unit Gaussian, and the neural network h \u03c6 has 2 hidden layers. A fixed feature extractor T is used in the first layer to encode a rectangular support within a spherically symmetric distribution. The second layer consists of a fully-connected 300 unit layer with sigmoid activations. The output of an MTLDS is sensitive to parameter A, stabilized by diagonal-orthogonal parameterization. Learning rate reduced by 10x for A. Prior log s \u223c N (\u22121.5, 0.05) used to prevent overfitting. Learning rate schedule annealed from m = \u22121.0 to m = \u22121.5. \"Momentum\" parameter \u03b2 1 reduced at end of optimization. The latent z are inferred online using the adaptive IS scheme. Inference over log s is also performed, with an informative prior close to the learned value. Hyperparameters are given in Table 3 and did not require tuning. These parameters seem to work well without tuning for other experiments such as the Mocap data. The No U-Turn Sampler was used for the STL experiments due to poor conditioning and higher complexity of the posterior. Tuning was performed using ideas from Hoffman & Gelman (2014) and the warmup phase lasted 1000 samples. Each sampler was initialized from a MAP value obtained via optimization with 10 random restarts. Low standard deviation was enforced for both MAP optimization and sampling. The effective sample size typically exceeds 100 for each parameter. The average results for the MTLDS model show significant improvement over pooled and single-task models, except for MTLDS-4 at t = 40. Convergence of MTLDS to the true model with increasing N is also considered. The log marginal likelihood of test sequences is averaged using 10,000 samples from the prior, with Bayes Factor interpretations used to assess differences. Results show convergence towards the true value with increasing N, with MTLDS-128 showing a minimal difference. Boxplots in Figure 7 illustrate the interquartile range over 10 repetitions. In the Lagrangian representation, joint positions are relative to the root joint, avoiding confusion between overall trajectory and local joint motions. Joint positions can be represented by spatial position or joint angle. The text discusses the use of forward kinematics to recover joint positions based on angles, ensuring constant bone length in skeletons. However, this method increases sensitivity in internal joints. The authors propose modeling joint positions to avoid sensitivity issues, even if it may lead to violations of bone length. They also mention encoding joint positions via velocity for smoother predictions, but avoid it for local joint motion due to potential errors. The per-frame representation in mocap models includes velocity and position of joints. Inputs reflect controls for animators, with trajectory provided over the next second. Hyper-parameters for mocap models are also discussed, including learning rate. The mocap models include hyper-parameters such as learning rate and gait frequency control through a phasor. Ambiguity exists in trajectory at corners, with a boolean indicator provided for identification. The root trajectory is extracted from 12 inputs for the Eulerian trajectory, 2 inputs for gait phase, and 6 inputs for turning indicators. A cubic B-spline fit is used to find a smoothed version of the trajectory by selecting control points with the Ramer-Douglas-Peucker algorithm. The gait phase is extracted using a semi-automatic process with foot contacts calculated based on vertical position and velocity. Outliers are manually corrected, and the gait phase is calculated by interpolation. The experimental setup, learning, and inference details are discussed, with specific experiment details in the following section. The MTDS architecture is described, with both linear and nonlinear h \u03c6 tested in preliminary experiments. The nonlinear version of the model used a one hidden layer MLP with 300 hidden units and tanh activations. A rank 30 matrix was chosen for the final affine layer as a trade-off between flexibility and parameter count. The linear approach was ultimately chosen due to faster optimization on new data and robustness to initialization. Benchmark models use an encoding length of \u03c4 = 64 frames and predict the difference from the previous frame via a residual architecture. The model used a residual architecture for better performance. Hyperparameter selection for the MTDS focused on achieving good style-content separation and smooth interpolation between sequences. Learning rates were split between shared and multi-task networks, with L2 regularization applied. Standard variational inference with Gaussian diagonal covariance was used for the variational posterior over each z. The model used a residual architecture for better performance and hyperparameter selection focused on style-content separation and smooth interpolation. L2 regularization was applied, and standard variational inference with Gaussian diagonal covariance was used for the variational posterior over each z. The model was implemented in PyTorch and trained on GPUs with a small max sequence length, eliminating the need for truncated backpropagation through time. The ELBO criterion was optimized with specific constraints for the initial iterations before removing them for the remaining training duration. The benchmark models' hyperparameters were determined through a grid search over learning rate and regularization, using Adam and SGD optimizers. Data was standardized and represented in Lagrangian form. Drifts in predicted trajectory were not heavily penalized. At test time, amortized inference may not perform optimally, so standard inference techniques are considered. Posterior distributions were unimodal and Gaussian, with small variation in sequence space. Posterior predictive mean performed similarly to point estimate. Inference for k = 3 experiments took approx. 24 seconds per observation. Standard optimization techniques may perform similarly at reduced computational cost. Experiment 1 - MTL involved training data for each style using carefully chosen subsequences to represent inter-style variation. Training sets increased in size with each subsequence being a superset of the previous one. The test set comprised 4 sequences from each style, each of length 64, and a length-64 seed sequence was used for inference. Models were trained as described above. For inference, all models used a test sequence. The STL models have the same architecture as the pooled 1-layer GRU models but are trained only on their style data. They are trained for a maximum of 5,000 iterations due to less data availability. 2-layer GRUs are not trained due to limited data. Results show the MTDS outperforms a pooled RNN model in sample efficiency. The MTDS outperforms a pooled RNN model in sample efficiency, with significant improvements for various training set sizes. The k = 7 MTDS performs at least as well as the pooled GRUs for the first four training set sizes, with some styles being more challenging than others. Experiment results are provided in Table 7 and visualized in Figures 10, 11, and 12. The MTDS shows better performance compared to benchmark models on various styles, except for style 5. Competitor results have better short-term performance, but the MTDS can be improved by interpolating from the zero-velocity baseline for early predictions. The classifier distinguishes between 8 styles using a 512-unit GRU to encode observation sequences. The model is trained on all styles using a 300-unit hidden layer MLP with sigmoid activations. Gait frequency is standardized across all styles, and the mean frequency is applied to all sequences. A k = 8 latent code is used for the MTDS, optimizing for each target style over 20 candidate values. The model is trained on all styles using a 300-unit hidden layer MLP with sigmoid activations. Gait frequency is standardized across all styles, and the mean frequency is applied to all sequences. A k = 8 latent code is used for the MTDS, optimizing for each target style over 20 candidate values. The success of style transfer is measured by the classifier probability for the target style, shown in Figure 13a for the model with multi-task bias and Figure 13b for the full MTDS. Table 3c in the main text provides the marginal results wrt. the target style. Successful style transfer is achieved by the MTDS model, outperforming the MTBias model in most cases. However, both models struggle with extreme styles like 'childlike', 'angry', and 'old'. Despite ignoring useful correlations in the input distribution, improvements could be made with adversarial loss or domain knowledge integration. The MTDS model outperforms the MTBias model in style transfer, especially with extreme styles like 'childlike', 'angry', and 'old'. Improvements could be made with adversarial loss or domain knowledge integration. In a secondary experiment, style transfer is successful for almost all (source, target) pairs with MTDS, except for the angry style. The MTBias model still has notable failures. The goal is to showcase model performance by predicting from inputs in the training set. Examples from Experiment 1 compare animations and fit to ground truth for two training set sizes. Novel test examples show model adaptions to new sequences. Style morphing animation demonstrates changing latent code effects. The animation at https://vimeo.com/361910646 showcases the effect of changing the latent code over time, demonstrating style transfer and interpolation. To achieve smooth style morphing, it was necessary to fix the dynamical bias of the second layer to avoid 'jumps' during interpolation. Shifting the bias may induce bifurcations in the state space, while adapting the transition matrix allows for seamless interpolation."
}