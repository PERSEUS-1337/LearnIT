{
    "title": "S1e_xM7_iQ",
    "content": "Parameter pruning is a method for compressing and accelerating CNNs by removing redundant model parameters while maintaining performance. A new regularization-based pruning method called IncReg assigns different regularization factors to weight groups based on importance, proving effective on popular CNNs compared to existing methods. CNNs have been successful in computer vision tasks but require significant computation. Many research works focus on compressing CNNs to reduce computation and storage consumption for deployment on mobile devices. Structured pruning is proposed to accelerate CNNs by eliminating redundant model parameters, avoiding hardware-unfriendly irregular sparsity. There are two types of structured sparsity: row sparsity and column sparsity. Structured pruning methods include importance-based pruning and group pruning based on established criteria. Existing group regularization approaches mainly focus on the regularization form (e.g. Group LASSO) to learn structured sparsity, while ignoring the influence of regularization factor. They tend to use a large and constant regularization factor for all weight groups in the network, which assumes all weights are equally important, leading to problems during pruning due to the fragility of CNNs. In this paper, a new regularization-based method named IncReg is proposed to incrementally learn structured sparsity in CNNs. The method addresses the issues of large penalty terms during pruning and the fragility of compact networks like ResNet BID7. The objective function for regularization involves non-structured regularization, weight decay, and structured sparsity regularization on weight groups in each layer. In this work, the authors focus on the key problem of group regularization in CNNs. They use weight decay as the regularization form for different weight groups and iterations, with \u03bb g varying for each group. A piece-wise linear punishment function is proposed to determine the regularization increment, aiming to penalize unimportant weights more. The method prunes all conv layers simultaneously and independently, with \u03bb g initialized to zero and increased at each iteration. The authors focus on group regularization in CNNs, using weight decay as the regularization form for different weight groups. They propose a piece-wise linear punishment function to determine the regularization increment, aiming to penalize unimportant weights more. The method prunes all conv layers simultaneously and independently, with regularization factors adjusted based on a ranking criterion. The authors propose a method for structured sparsity in CNNs through gradual weight pruning. Regularization factors increase during training to push weights towards zero, leading to increased sparsity. Once a weight group's magnitude falls below a threshold, the weights are permanently removed. When a layer reaches its pruning ratio, regularization stops. Comparison with other methods shows higher speedups with the proposed IncReg approach. Results in Tab.1 show that IncReg achieves higher speedups and accuracies compared to other regularization schemes. Even with similar performance to AFP at small speedups, IncReg outperforms AFP significantly at larger speedup ratios. The incremental regularization approach allows the network more time to adapt during pruning, crucial for large pruning ratios. Comparison of methods on CIFAR-10 and ImageNet datasets further validates the effectiveness of IncReg. The proposed structured pruning method based on incremental regularization helps CNNs transfer expressiveness during pruning by increasing regularization factors gradually. It is effective on popular CNNs, especially for large pruning ratios and compact networks."
}