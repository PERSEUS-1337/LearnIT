{
    "title": "ryg48p4tPH",
    "content": "In multiagent systems (MASs), agents make individual decisions contributing globally to system evolution. Learning in MASs is challenging due to interactions with other agents, environmental stochasticity, and uncertainties. Previous works integrated multiagent coordination mechanisms into deep learning but lacked explicit consideration of action semantics between agents. A novel network architecture, Action Semantics Network (ASN), is proposed to represent action semantics between agents using neural networks. This allows for a better understanding of how different actions influence other agents. ASN can enhance deep reinforcement learning algorithms by incorporating action semantics between agents. Experimental results demonstrate significant performance improvements compared to other network architectures in complex multiagent systems. Deep reinforcement learning has been successful in addressing single-agent tasks but faces challenges in multiagent systems due to interactions and uncertainties. Various deep multiagent reinforcement learning approaches have been developed to tackle these complex problems. Various deep multiagent reinforcement learning approaches, such as MARL, have been proposed to address complex multiagent problems like coordinating robot swarm systems and autonomous cars. These approaches incorporate coordination mechanisms and architectures like centralized actor-critic and Counterfactual Multi-Agent Policy Gradients to facilitate multiagent coordination and credit assignment. Mean-field theory has also been applied to solve large-scale multiagent learning problems. Recent works have focused on deep multiagent reinforcement learning to address complex coordination problems. Palmer et al. (2018) introduced the retroactive temperature decay schedule for stochastic rewards. Sunehag et al. (2018) and Rashid et al. (2018) proposed network structures like the value-decomposition network (VDN) for multiagent learning. Recently, Rashid et al. (2018) introduced QMIX, a non-linear approach to VDN that considers individual and global Q-values. Tacchetti et al. (2019) proposed RFM for predictive modeling in multiagent learning, focusing on semantic state descriptions. However, RFM lacks consideration of action influence between agents. OpenAI designed network structures for multiagent learning in Dota2, using a scaled-up version of PPO and an attention mechanism. However, the influence of each action on other agents is not considered in their selection process. Previous works have focused on multiagent communication but have not explicitly addressed the fact that an agent's actions may have different impacts on other agents, a crucial aspect in decision-making in MASs. The curr_chunk discusses the importance of considering action semantics in multiagent learning to improve coordination. It introduces the Action Semantics Network (ASN) as a novel network architecture to characterize action semantics for more efficient multiagent coordination. The main contribution is the explicit consideration of action semantics in decision-making in MASs. The Action Semantics Network (ASN) is a novel network designed to extract action semantics for learning in MASs. It can enhance the performance of existing DRL algorithms and has shown better results in StarCraft II micromanagement and Neural MMO compared to state-of-the-art approaches. Stochastic games are a multiagent extension of Markov decision processes, and multiagent learning problems are modeled as partially observable stochastic games (POSGs) to account for incomplete environmental information. A partially observable stochastic game (POSG) is defined by a tuple N, S, A i, T, R i, and O i, where N is the set of agents, S is the set of states, A i is the set of actions for agent i, T is the transition function, R i is the reward function, and O i is the set of observations for agent i. Each agent draws a private observation correlated with the state, including private information and relative distances between agents. The goal of agent i in a partially observable stochastic game is to learn a policy that maximizes expected return. Deep Multiagent Reinforcement Learning approaches address complexities in multi-agent systems by incorporating various strategies to learn optimal policies. In MASs, strategies like deep multiagent learning or specialized network structures are used for coordination. Extracting action semantics is crucial for coordination, classifying actions into those affecting the environment or other agents. The value of an action depends on the agent's observation and information. In multi-agent systems, coordination strategies like deep multiagent learning and specialized network structures are utilized. Extracting action semantics is vital for categorizing actions based on their impact on the environment or other agents. The proposed Action Semantics Network (ASN) considers action semantics between agents to enhance estimation accuracy for different actions. Instead of inputting an agent's total observation into one network, ASN consists of sub-modules that take different parts of the observation based on action semantics. This approach helps avoid negative influences and inefficiencies in estimating action values. The proposed Action Semantics Network (ASN) categorizes an agent's action set into subsets based on their impact on the environment or other agents. A i in includes actions that affect environmental information or private properties, while A i out includes actions that directly influence other agents. This classification helps improve the accuracy of estimating action values in deep multiagent learning frameworks. The proposed Action Semantics Network (ASN) categorizes an agent's actions based on their impact on the environment or other agents. ASN divides the network into sub-modules to consider the influence of an agent's actions on other agents. Each sub-module takes different parts of the agent's observation as input according to the semantics of actions. This helps improve the accuracy of estimating action values in deep multiagent learning frameworks. The proposed Action Semantics Network (ASN) categorizes an agent's actions based on their impact on the environment or other agents. ASN divides the network into sub-modules to consider the influence of an agent's actions on other agents. Each sub-module takes different parts of the agent's observation as input according to the semantics of actions. This helps improve the accuracy of estimating action values in deep multiagent learning frameworks. ASN can be incorporated into existing deep MARL, classified into Independent Learner (IL) and Joint Action Learner (JAL) paradigms. The proposed Action Semantics Network (ASN) categorizes an agent's actions based on their impact on the environment or other agents. ASN divides the network into sub-modules to consider the influence of an agent's actions on other agents. Two classes of ASN-based MARL are proposed: ASN-IL and ASN-JAL. ASN-IL combines ASN with PPO for single-agent RL, while ASN-JAL integrates ASN with existing deep MARL approaches like QMIX and VDN. ASN-PPO replaces the vanilla policy network architecture with ASN for each agent i, optimizing the policy following PPO. ASN-PPO optimizes the expected return for each agent using the policy gradient theorem and advantage estimation. It reformulates the optimization problem with constraints and maximizes the objective during each iteration. ASN-QMIX combines ASN with deep MARL algorithms, replacing the vanilla Q-network architecture with ASN for each agent. The mixing network then combines the output of all agents' networks to produce the joint action-value. The network mixes the output of all agents' networks to produce the joint action-value function Q tot (s t , a t). The weights of the mixing network are restricted to be non-negative and produced by separate hypernetworks. ASN-QMIX is trained to minimize the loss function. Multi-action ASN extends the basic ASN to handle agents with multiple actions that can directly influence each other. Multi-action ASN is a generalized version that produces multiple embeddings for actions influencing agent j. It calculates action estimations using a pairwise interaction function. Parameter-sharing mechanism is used to reduce training complexity in MARL, enabling sharing between sub-modules of ASN. The basic ASN for agent i consists of sub-modules O2A i,j, each taking o i,j as input. In a homogeneous MAS, all influencing agents can share one sub-module, while in a MAS with different agent types, each type can share one sub-module. The performance of ASN is evaluated against various network structures. The performance of ASN is evaluated against different network structures in various DRL approaches, including vanilla, dueling, attention, and entity-attention networks. Test domains include StarCraft II micromanagement and Neural MMO. Other network architectures are not comparable to ASN. In Neural MMO, agents control individual army entities in a decentralized multiagent setting. They observe local game state information and take actions such as moving or attacking enemies. Joint rewards are given for damaging enemy units and killing enemies, with a bonus for killing all enemies. The game in Neural MMO awards points for damaging and killing enemies, with a bonus for killing all enemies. Previous works aimed to reduce learning complexity by manually adding rules to prevent invalid actions, but the study aims to see if these rules can be learned automatically. The results show that allowing agents to make invalid actions can still lead to successful performance. In StarCraft II, the study compares the performance of different network architectures. Homogeneous ASN allows agents to learn optimal timing for attacking opponents, outperforming other architectures. ASN helps agents distinguish between actions' effects on opponents, leading to higher performance. Attention network combined with IQL performs better than vanilla and dueling networks, while QMIX and VDN show similar performance with vanilla network. ASN-QMIX and vanilla-QMIX show similar performance when combined with QMIX and VDN, with entity-attention performing the worst. Mixed ASN-QMIX outperforms vanilla-QMIX by efficiently identifying action semantics between agents. ASN-QMIX performs well on a large-scale agent space, quickly learning and achieving higher win rates compared to vanilla-QMIX. ASN-QMIX quickly learns average win rates of approximately 80%, outperforming vanilla-QMIX which only achieves around 20%. As the agent number increases, the margin between the two methods also grows larger. ASN enables agents to consider more information from other agents, leading to higher win rates compared to vanilla-QMIX. An interesting observation is that vanilla-QMIX agents learn to run away to avoid being killed. ASN-QMIX achieves an average percentage of approximately 71.9% for choosing a valid action, while vanilla-QMIX only achieves around 44.3%. This shows that ASN effectively exploits action semantics between agents, enabling more robust learning in large-scale MASs. The model is tested on a 15m map in scenarios like one-on-one combat and one Marine vs two Marines to demonstrate improved estimation accuracy and multiagent coordination facilitated by ASN. The ASN agent demonstrates dynamic changes in attack action Q-values based on distance and opponent HP differences, allowing for automatic learning of valid actions without manual rules. This contrasts with the vanilla agent, which struggles to adapt without predefined guidelines. The ASN agent adjusts attack action Q-values based on opponent HP differences, while the vanilla agent consistently prioritizes attacking one opponent. This shows that ASN effectively utilizes action semantics and improves estimation accuracy, promoting robust learning among agents. In an ablation study, ASN design exploits 0-padding information and outperforms other network architectures in terms of convergence speed and final win rates. This indicates that ASN effectively extracts action semantics between agents. The Neural MMO environment features combat systems for a large number of agents. In Neural MMO, a group of 3 agents with 100 HP each can move or attack with different options. Invalid actions result in a penalty, and the game ends when all agents in one group die. Agents receive a joint reward based on the total HP difference between groups. In Neural MMO, agents can attack opponents with three different options to evaluate multi-action ASN efficiency. Two types of multi-action ASN are used: ASN-M1 shares parameters across attack actions, and A2C is compared. ASN performs best under all IL approaches due to its ability to choose appropriate actions to maximize damage on opponents. Vanilla network struggles to identify action semantics, resulting in lower performance than ASN. ASN outperforms other methods in identifying the best attack option in Neural MMO, particularly \"Melee\" which causes the most damage within a certain distance range. Both ASN-M1 and ASN-M agents achieve higher total damage compared to other methods. The ASN-M1 agent and ASN-M cause higher total damage than other methods, with ASN-M1 agent causing the highest total damage on average. The attention network only causes average total damage of approximately 1.5, while the entity-attention and vanilla network only cause average total damage of approximately 1.0 due to lower probability of selecting the best attack action \"Melee\". ASN has a larger probability to select \"Melee\" than other networks, resulting in larger total damage. Similar results are found in other distance ranges, showing that ASN consistently causes higher total damage than other networks. The proposed network architecture, ASN, aims to enhance multiagent learning by explicitly investigating action semantics between agents, making it the first to do so in MASs. In this paper, ASN is the first to characterize action semantics in MASs, improving DRL methods' performance. Future work includes modeling action semantics among more than two agents and in continuous action spaces. Hyperparameters for StarCraft II are provided, showing the performance of ASN-QMIX and vanilla-QMIX under different maps with manual rules. In a 10x10 tile, two teams of agents (green and red) with 3 agents each start on random tiles. Agents have a 43-dimensional observation vector, including info on teammates and opponents. Agents choose from 14 actions like stop, move left/right/up/down. Agents in a game choose from 14 actions including different attacks with penalties for failure. The game ends when agents die or time runs out, with rewards based on HP difference. Neural MMO uses vanilla, attention, and entity-attention networks for actor and critic networks. In Neural MMO, different ASN variants are tested for multiple agent actions influencing each other. Hyperparameters are provided in Table 3, showing average attack damage under different distance ranges between agents and opponents."
}