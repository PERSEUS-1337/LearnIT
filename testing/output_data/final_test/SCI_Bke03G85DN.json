{
    "title": "Bke03G85DN",
    "content": "Autonomous driving is a challenging problem due to its variability, with issues in vehicle control and scene recognition. While reinforcement learning has shown success in games and robotics, it has not been widely applied to real-world tasks like autonomous driving. A deep reinforcement learning algorithm with actor-critic architecture and multi-step returns was proposed for better agent robustness in complex environments. The experiment in the Carla simulator demonstrated superior performance compared to standard deep RL agents, showing promise for goal-oriented optimization. The performance of standard deep RL agents is influenced by reinforcement learning (RL), where an agent learns through interaction with its environment driven by a reward signal. RL algorithms have shown success in games and robotics, but face challenges in high-dimensional environments. Autonomous driving remains a highly challenging task, with standard RL strategies struggling to efficiently navigate complex environments. In this work, an advantage actor-critic approach with multi-step returns is proposed for autonomous driving, aiming to address the variability and complexity of the driving task. The approach utilizes temporal difference updates to enhance agent learning strategy and guide exploration. The training and evaluation are conducted using the CARLA simulator, known for its ability to simulate physical autonomous urban driving. CARLA is a valuable tool for autonomous driving due to its realistic environment with varying properties like weather conditions and density of cars and pedestrians. The proposed method in this study utilizes actor-critic RL, which involves updating policy parameters to improve performance. Various types of RL algorithms are classified into actor, critic, or actor-critic categories based on their approach to predicting actions. The actor-critic RL method involves updating policy parameters for improvement. Policy gradients offer convergence guarantees but may have high variance, leading to slow learning. Critic-only methods use TD learning for lower variance in estimated returns but lack reliable convergence guarantees. Actor-critic methods combine these approaches for a repetitive cycle of policy evaluation and improvement. Various algorithms have been developed with different directions of improvement, such as the Fuzzy Actor-Critic Reinforcement Learning Network (FACRLN) and others. The Consolidated Actor-Critic Model (CACM) and other actor-critic algorithms like Deterministic Policy Gradient (DPG) and Asynchronous Advantage Actor-Critic (A3C) have been developed for reinforcement learning. These methods are typically tested on standard RL benchmarks such as cart-pole balancing, maze problems, multi-armed bandit, Atari games, and OpenAI Gym tasks. Our work extends actor-critic RL to urban autonomous driving, a challenging task with intricate dynamics. The driving agent must navigate changing conditions, interact with various actors, and adhere to traffic rules. Our approach incorporates an actor and a multi-step TD critic to enhance RL stability in this Markov Decision Process setting. In value-based RL algorithms like Q-learning, a state value function V(s) and a state-action value function Q(s, a) are used to estimate returns under a policy \u03c0. The goal is to learn a policy that maximizes the return of a trajectory by selecting the best action based on the maximum value for each state-action pair. Policy-based methods optimize a parameterized policy directly without using a value function. They update the policy parameters \u03b8 using gradient descents like in the family of REINFORCE algorithms. The actor-critic framework replaces the reward function with the action value function to enable the agent to learn the long-term value of a state and improve decision-making. A critic is trained to approximate this value function parameterized with \u03c9 and update the model accordingly. An efficient way to derive optimal control policies is by evaluating them using approximated value functions. Building accurate estimators results in better policy evaluation and faster learning. T D learning combines Monte Carlo method and dynamic programming to calculate good approximations of value functions. The actor-critic algorithm still suffers from high variance. To reduce variance in the actor-critic algorithm, a baseline function like the state value function can be subtracted from the policy gradient. The advantage function A(s t , a t ) measures the improvement in predicting an action compared to the average V (s t ). By approximating the advantage function using the expected future reward and actual reward, we can calculate the actor policy gradient. The T D error is a suitable estimator for the advantage function, leading to the final actor policy gradient. The final actor policy gradient is deduced by estimating the advantage function using a generalized version of TD learning with multi-step returns. This approach improves learning performance by allowing the agent to gather more information before updating the policy. Experiments in the Carla simulator will demonstrate the effectiveness of this approach. In this study, the performance of an advantage actor-critic algorithm with multi-step TD target updates is evaluated in urban autonomous driving. The incorporation of a multi-step returns critic component aims to enhance the agent's learning strategy and improve overall performance. Experiments are conducted in the CARLA simulator, providing a realistic environment for the RL agent to interact with dynamic urban driving conditions. The CARLA simulator offers realistic urban driving conditions with advanced features for controlling the vehicle and gathering environment feedback. It operates as a server-client system with the server in Unreal Engine 4 running simulation commands and the client in Python sending driving commands. The 3D environment includes static objects like buildings and dynamic nonplayer characters. Training allows for varying traffic density and visual effects. The agent interacts with the environment through actions and observations in a driving simulator. The action space includes steering, throttle, and brake, while the observation space includes color images, depth, and vehicle measurements. Rewards play a crucial role in shaping driving policies by guiding the agent's predictions. The reward in the driving simulator is calculated based on various measurements from the observation space. It involves comparing current and previous observations to adjust the reward positively or negatively. Positive weights are given to distance traveled and speed, while negative weights are assigned to collision damage and certain intersections. The agent's training involves goal-directed navigation on straight roads, with episodes ending when the destination is reached or a collision occurs. The A2C networks are used for training. The A2C networks are trained with 10 million steps for 72 hours using CNN to approximate the value function and actor policy. The CNN architecture includes 4 convolutional layers, 3 max-pooling layers, and one fully connected layer. Discount factor is set at 0.9 with 10-step rollouts and a decreasing learning rate. Stochastic gradient descent is operated every 10 time steps, storing the resulting policy model only if its performance exceeds the last model. The final stored model is used in the test phase. In the test phase, the final stored model from training is used for evaluation. Comparative evaluation is done between two versions of the algorithm: one with the MSRC policy-evaluator and one without. The evaluation focuses on episodic average and cumulative rewards metrics to assess the MSRC contribution in complex tasks like autonomous driving. The n-step A2C approach outperformed the standard deep RL in training, with 100 retained models in later episodes compared to just 10. This indicates early exploration and efficient exploitation of the RL strategy controlled by the MSRC. The n-step A2C approach outperformed the standard deep RL in training, showing more efficient RL models with reduced variance in predictions. Testing in different environments also demonstrated the superiority of the n-step A2C method. The n-step A2C algorithm outperformed standard deep RL in training, demonstrating superior generalization capabilities in new environments. However, it showed decreased performance in a second test scenario, indicating some fragility to changing environments. The proposed approach combined actor and critic methods for continuous policy assessment and improvement using multi-step TD learning, achieving higher performance and faster learning in autonomous driving tasks compared to standard deep RL. The results showed vulnerability of the approach in unseen testing conditions. Future work will examine performance of other RL methods like deep Q-learning and Trust Region Policy Optimization on similar tasks. Additionally, addressing non-stationary environments impact on RL methods robustness as a multi-task learning problem will be explored using adaptive dynamic programming, context-aware, and meta-learning strategies. These strategies aim to design generalizable and fast adapting RL algorithms for autonomous driving. In future work, the aim is to design generalizable and fast adapting RL algorithms for autonomous driving tasks, increasing complexity and making conclusive comparisons with state-of-the-art experiments on the CARLA simulator."
}