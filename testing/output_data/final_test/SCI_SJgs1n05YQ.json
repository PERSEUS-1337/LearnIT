{
    "title": "SJgs1n05YQ",
    "content": "Building deep reinforcement learning agents that can generalize and adapt to unseen environments is a challenge in AI. The paper introduces LEAPS, a hybrid model-based and model-free approach for visual navigation tasks in diverse indoor scenes. LEAPS outperforms baselines by planning with a semantic model to make high-level decisions. LEAPS outperforms strong baselines by planning with a semantic model for visual navigation tasks in diverse indoor scenes. Deep reinforcement learning has shown strong achievements, but generalization ability remains a challenge. Model-free RL trains agents reactively, limiting generalization, especially on tasks requiring planning. Model-based RL learns a dynamics model for sequential decision-making via planning, but complex tasks with high-dimensional observations pose challenges. In this work, the aim is to improve the generalization of RL agents in domains with high-dimensional observations by focusing on semantic structures and shared properties in real-world environments, rather than building pixel-accurate models of dynamics. Realistic settings often exhibit functional arrangements, such as rooms in indoor scenes, and practical object placements, which can aid in high-level decision-making without the need for detailed models. The proposed hybrid framework, LEArning and Planning with Semantics (LEAPS), combines a model-based component that operates on the semantic level to pursue high-level targets, with a model-free component that acts on pixel-level inputs to execute the target. This approach involves training model-free multi-target subpolicies using neural networks and building a semantic model in the form of a latent variable model. The proposed hybrid framework, LEArning and Planning with Semantics (LEAPS), combines a model-based component that operates on semantic signals to plan high-level targets efficiently. The semantic model is dynamically updated through posterior inference, capturing semantic consistency among environments. This lightweight and interpretable approach requires observations with pixel-level data and semantic properties of the scene. In this work, the focus is on navigating complex indoor scenes in environments like House3D with strong object detectors. The challenge lies in reaching distant targets by understanding the overall structure of the scenario. The approach, LEArning and Planning with Semantics (LEAPS), utilizes semantic signals for efficient high-level target planning, emphasizing the general applicability of the method beyond navigation tasks. Our LEAPS framework utilizes semantic signals for efficient high-level target planning, outperforming model-free RL approaches. The effectiveness of planning on the learned semantic model is highlighted, especially in novel unseen environments where generalization is crucial. Meta-learning has shown promising results for fast adaptation to novel environments by learning a Bayesian model over the semantic level and inferring the posterior structure via the Bayes rule. This approach is interpretable, can work without exploration steps, and is potentially combinable with any graph-based planning algorithm. It is a special case of hierarchical reinforcement learning where high-level planning is based on semantic signals, making it easier to learn compared to recurrent controllers. Our approach, LEAPS, utilizes a discrete semantic signal along with the continuous state. This concept is similar to BID13 and BID5, which use discrete signals for planning and tackling sparse reward problems. The schema network assumes that even visual signals can be represented in binary form for logical reasoning. In evaluating our approach, we focus on visual navigation, a problem extensively studied in classical approaches like SLAM for 3D mapping and end-to-end approaches in various domains. In recent years, end-to-end approaches have been used for navigation in various domains, with a focus on improving performance as the distance from the target increases. Auxiliary tasks are often introduced during training to aid navigation. Another direction involves using recurrent neural networks and 2D spatial maps for planning computations. This approach considers general graph structures beyond 2D grids and captures relationships between semantic signals for navigation in environments like House3D. In House3D, Savinov et al. construct a graph of nodes representing different locations in the environment. In contrast, LEAPS uses semantic knowledge to learn a prior over shared semantic structures in real-world scenes, eliminating the need for exploratory steps. The contextual Markov decision process E(c) is defined by objects, layouts, and other semantic information describing the environment, sampled from the distribution of possible semantic scenarios. The curr_chunk discusses semantic scenarios where an agent observes high-dimensional images and low-dimensional semantic signals to encode information. The signals, provided by an oracle function, convey important details for tasks like robotic manipulation and visual navigation. LEAPS aims to train a policy on training environments to maximize reward on a test set by capturing structural similarities between environments as a probabilistic graphical model over semantic information. The agent learns a Bayesian model to capture semantic properties from exploration experiences and computes the posterior for unknown contexts in new environments. The agent learns a Bayesian model to capture semantic properties from exploration experiences and computes the posterior for unknown contexts in new environments, allowing effective planning to reach goals. Learning an accurate Bayesian model can be challenging, so an approximate latent variable model is used. The agent learns a Bayesian model to capture semantic properties for effective planning in new environments. Navigation to a target can be guided by planning on semantic signals, using binary variables to denote direct reachability between states. The agent learns a Bayesian model to capture semantic properties for effective planning in new environments. It holds a prior belief about a parameter to be learned, receives noisy observations of the unknown environment, and learns multi-target sub-policies based on its experiences. The semantic model aids in planning by selecting intermediate sub-targets to reach the final target with the highest probability. The agent explores the environment for a short horizon, receives semantic signals, and computes the bit-OR operation over binary vectors. Through posterior inference, it computes beliefs of latent variables and searches for an optimal plan to reach the final target with the highest probability. The agent computes beliefs of latent variables and searches for an optimal plan to reach the final target with the highest probability by maximizing joint belief along the path. The model parameters include \u03c8 prior for the prior of z and \u03c8 obs for noisy observation y, with \u03c8 obs related to the performance of sub-policies \u00b5(\u03b8). Learning \u03c8 prior is done from E train by running random explorations to determine reachable semantic signals. The agent learns \u03c8 prior from E train through random explorations to identify reachable semantic signals. To optimize \u03c8 obs, policy evaluation is conducted on E valid to maximize accumulative reward with the semantic model M(\u03c8). The LEAPS agent consists of a multi-target sub-policy \u00b5(T i , \u03b8) and the semantic model M(\u03c8), with learning achieved through standard deep RL methods on E train. The agent learns semantic signals through random explorations to optimize policy evaluation and maximize reward with the semantic model. The agent's trajectory is visualized with successful sub-target executions in different rooms. RoomNav is a navigation task in the House3D environment where the agent navigates to find a target room based on concept targets. It uses semantic signals to determine the current room type and successfully executes sub-policies to reach the goal. The task includes a training set of 200 houses, a testing set of 50 houses, and a validation set of 20 houses. The RoomNav task in the House3D environment involves navigating to a target room using semantic signals and sub-policies. The reachability variable z i,j represents connectivity between room types, and LSTM policies are learned using A3C with shaped rewards. Experimental details are provided in the section. In this section, the RoomNav task is experimented on to evaluate the performance of the LEAPS agent compared to model-free RL agents. The comparison includes success rates with different horizons and metrics such as distance to target room, planning distance, and relative improvement over the baseline. LEAPS agent outperforms baselines as planning computations increase, with higher relative improvements for targets requiring more semantic planning. Semantic signals are inputted to LEAPS, and comparisons are made with other model-free RL approaches. Semantic signals are extracted by a CNN room type detector for the semantic model at test time. The CNN room type detector extracts semantic signals for the LEAPS agent, which is trained on oracle information from House3D. The learned prior captures relationships between room types. Testing success rates are measured on E test, comparing LEAPS with baselines. The LEAPS agent, trained on oracle information from House3D, uses a semantic model to improve success rates. By updating the model every 30 steps, the agent outperforms baselines as the number of planning computations increases. Targets 3 or 4 plan steps away show the best relative improvements, with a small increase in success rate for targets 5 steps away due to rare occurrences in the semantic model. The LEAPS agent utilizes a semantic model to enhance success rates by updating every 30 steps. Targets 3 or 4 plan steps away exhibit the most significant improvements, with a slight success rate increase for targets 5 steps away. The agent employs two semantic-aware agents that consider semantic signals as input, training new sub-policies accordingly. The LSTM controller with 50 hidden units on E train generates a sub-target every N steps based on necessary semantic information. The LEAPS agent, with a semantic model, outperforms baselines and a HRL agent with an LSTM controller. The HRL agent shows higher relative improvements for targets requiring more planning computations. The LSTM controller has more parameters than the semantic model M, which can adapt to new sub-policies efficiently. The LEAPS agent, with a semantic model, outperforms baselines and a HRL agent with an LSTM controller. The LSTM controller needs re-training to adapt to new sub-policies efficiently. Evaluation metrics include success rate under different horizons and episode length, with a new metric called Success weighted by Path Length (SPL) capturing both factors. The LEAPS agent outperforms baselines and a HRL agent with an LSTM controller in success rate and SPL metric. The SPL margin of LEAPS over other agents increases with more planning computations allowed. The semantic augmented model-free agent and the HRL agent with an LSTM controller are also evaluated. The LEAPS agent outperforms baselines and a HRL agent with an LSTM controller in success rate for targets requiring planning computations. It shows significant improvements over the baselines, especially for faraway targets with longer horizons. Our LEAPS agent outperforms baselines in success rate for targets requiring planning computations, especially for faraway targets with longer horizons. The agent updates the semantic model every fixed N = 30 steps, potentially increasing episode length for goals requiring more planning computations. In longer horizons, LEAPS significantly outperforms all baselines in SPL metric. Future work could focus on the agent learning to update the semantic model dynamically. LEAPS is proposed to enhance RL agents' generalization in diverse environments with various layouts and object arrangements. The success rates of LEAPS agents are the highest for tasks involving planning computations, outperforming all baselines in different scenarios. LEAPS agents have the highest success rates for cases requiring planning computations, with the highest overall SPL value compared to baseline methods. They outperform best baselines as the horizon increases, but require a longer horizon for optimal performance due to updating the semantic model every fixed N = 30 steps. The agent plans on a model, explores the environment, and updates the semantic model with new information. Sub-policies focus on multiple targets for exploration. LEAPS agents excel in planning computations, achieving high success rates and outperforming baselines as the horizon expands. They update a lightweight, interpretable semantic model every N = 30 steps, which can be dynamically adjusted with minimal exploration. While effective in environments with semantic consistencies like House3D, LEAPS struggles in random environments like mazes. The approach is versatile and applicable to tasks like robotics manipulations and video games, with plans based on semantic signals. Future work will explore models for more complex semantic structures in contextual Markov Decision Processes. The context discusses LEAPS agents excelling in planning computations and updating a semantic model every N = 30 steps. The curr_chunk explains E(c) as a representation of the environment with state space S, action space A, reward function r(s, a; c), and transition probability P (s |s, a; c). It also mentions the agent's observation tuple (s o , s s ) consisting of high-dimensional observation s o and low-dimensional semantic signal s s. This signal encodes semantic information for tasks like robotic manipulation. In AI, low-dimensional discrete signals like s s are used for various tasks such as robotic manipulation, game status, and visual navigation. The objective is to find the best policy that maximizes the expected accumulative reward. Training and testing sets are sampled from C for practical implementation. The training and testing sets are sampled from C for practical implementation. In RoomNav, the agent needs to see specific objects for a certain duration and stay in the target area for a set number of time steps. The House3D environment originally supports 13 discrete actions, but this is reduced to 9 actions for simplicity. In RoomNav, the agent performs various actions like right-forward, large left rotate, large right rotate, left rotate, right rotate, and stay still. The evaluation setup is based on BID12, with a success rate measured on E test over 5750 episodes. 750 episodes are specialized for faraway targets to ensure a diverse range of evaluations. Each test episode has a fixed configuration for fair comparison, with the target always connected to the agent's birthplace but never within the target room. A CNN detector is used to extract semantic signals at test time, evaluating the performance of different approaches. The performance of LEAPS and baselines agents is evaluated using ground truth oracle semantic signals provided by the House3D environment. Results show that using the CNN detector or ground truth signal yield comparable overall performances in success rate and SPL metrics. LEAPS agents outperform baselines in both metrics, with LEAPS-CNN agents performing similarly to LEAPS-true agents, indicating the semantic model can tolerate practical errors in CNN detectors. The results in FIG3 illustrate the ground truth shortest distance information and average episode length for all approaches. The average ground truth shortest path is around 46.86 steps, indicating the challenging nature of the benchmark semantic navigation task. The confidence interval of the success rate was computed using a binomial distribution. Optimal plan steps involve extracting room locations, constructing a graph, and computing the shortest path from the agent's birthplace to the target. Incorporating hyperparameters like policy architecture from BID12, using segmentation mask + depth signals for better performance, training individual policies for each target, running A3C with specific parameters, reward shaping, and normalizing advantages for mean 0 and standard deviation 1. The text chunk discusses reward shaping, curriculum learning, and training details for the LSTM controller using A2C with specific hyperparameters. The text chunk discusses the input dimensions and features for the LSTM controller in RoomNav, including reward shaping, curriculum learning, and training details with specific hyperparameters. The LSTM controller input consists of various features, and for the semantic augmented LSTM policy, visual features are extracted using CNN. The text chunk discusses using panoramic views as input for the agent in RoomNav, with 4 images representing different first person view angles. For the \"outdoor\" target, recent 4 frames in the trajectory are used instead of a panoramic view. CNN layers with specific parameters are used to extract visual features, followed by computing attention weights and a weighted average of the frames for input to a single layer perceptron. The text chunk discusses training a single layer perceptron with 32 hidden units using positive and negative training data generated from semantic signals. The model is optimized using Adam optimizer with specific parameters, and a hard threshold and filtering process is applied during testing for smooth prediction."
}