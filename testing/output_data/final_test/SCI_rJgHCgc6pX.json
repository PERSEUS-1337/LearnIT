{
    "title": "rJgHCgc6pX",
    "content": "Creating a knowledge base that is accurate, up-to-date, and complete is a challenge in automated knowledge base construction. Alexandria is a system for unsupervised knowledge base construction using a probabilistic program to convert facts into text. This allows for retrieving facts, schemas, and entities from web text with increased accuracy. Alexandria does not require labelled training data, making it efficient for constructing knowledge bases with minimal manual input. Automated knowledge base construction faces challenges in maintaining accuracy, completeness, and up-to-date information. Various systems like KnowledgeVault, NELL, YAGO2, and DIG aim to automate KB construction or enhance existing ones. Despite efforts, there are ongoing challenges in keeping KBs accurate and complete, requiring manual input for tasks like entity linking and schema definition. Alexandria is a system for unsupervised, high-precision knowledge base construction. It uses a probabilistic program to generate text from a knowledge base and reason in the inverse direction to retrieve facts, schemas, and entities from web text. The goal is to create a system that can learn and update its own schema, discover new entities, and extract facts with high precision without human intervention. Alexandria uses a probabilistic program to extract facts, schemas, and entities from web text. It does not require labeled data and can be applied to new domains with little manual effort. The model is task-neutral and can be used for various tasks such as schema learning, fact retrieval, entity discovery, and entity linking. The entire system is defined by one coherent probabilistic model, eliminating the need to create multiple models. The Alexandria system uses a single probabilistic model to extract facts, schemas, and entities from web text, eliminating the need for multiple components. This approach ensures consistent propagation of uncertainty throughout the system and improves accuracy. The system is task-neutral and can be applied to various tasks such as schema learning, fact retrieval, entity discovery, and entity linking. Open IE systems like Reverb BID5 and OLLIE BID8 aim to discover information across domains without labeled data. They store information in a lexical form, leading to duplication or conflicting facts. Alexandria infers schemas of new domains and extracts facts consistently. Biperpedia BID6 is the closest work, using search engine logs and text to discover attributes. Alexandria's unsupervised approach integrates schema learning into a probabilistic model for fact extraction. KnowledgeVault, NELL, and DeepDive are systems that extract facts against a known schema, with KnowledgeVault completing over 250M facts. DeepDive, similar to Alexandria, uses a probabilistic model for fact extraction from text. The Alexandria probabilistic program uses a generative model of text to generate text from facts with high accuracy. It is the largest and most complex probabilistic program deployed in any application today, allowing for precise modeling of text generation. The Alexandria probabilistic program uses a generative model of text to accurately generate text from facts. It includes a knowledge base with typed entities and text extracts describing these entities in natural language. The program utilizes the Infer.NET inference engine to infer a schema and knowledge base from a large number of web text extracts. The probabilistic program is implemented in C# and uses random distributions to generate uncertain values. This work demonstrates that large-scale inference can be performed in complex probabilistic programs. The probabilistic program generates a schema for entity types with properties and types drawn from priors. Entity types are compound types constructed from primitive types learned from data. The program also generates an Alexandria knowledge base with typed entities and values for each property. The probabilistic program creates entities with multiple alternative values for properties to account for disagreements. Each entity has values for each property, with alternative values representing conflicting possibilities. This is different from sets of values where multiple values can be true for the same entity. The probabilistic program creates entities with multiple alternative values for properties to account for disagreements. Each entity has values for each property, with alternative values representing conflicting possibilities. Sets are modeled differently to alternatives, using a specific set type. Generating text from KB values involves selecting an entity at random from the knowledge base, converting its property values into string values using a specific format, and embedding these values into a natural sentence using a template selected randomly from a set of templates. The template is filled by replacing property placeholders with corresponding values, allowing it to appear in a larger text section. The final variable with observed values from the web enables learning from web text. A probabilistic program handles uncertainty in natural text, representing values as probability distributions. The core Alexandria probabilistic program includes three sections and extensions to improve precision and recall. The property list model allows parsing lists of property values, enhancing the system's capabilities. The property list template in the Alexandria probabilistic program represents (name, value) pairs, allowing property names to be learned during schema learning. The page model associates text extracts from the same HTML page more strongly by assuming a smaller number of entities on a single page compared to the entire web. The value noise model adds type-specific noise to property values to account for variations across pages. The Alexandria probabilistic program defines entity types with built-in property types like Date, PersonName, Place, Hierarchy, and Quantity. Noise is added to Quantity properties to account for variations in numeric values reported on different pages. Different types of noise could be modeled for other property types like Date. The Alexandria probabilistic program defines entity types with built-in property types such as Date, PersonName, Place, Hierarchy, and Quantity. These types have characteristics summarized in TAB3 and detailed in the following subsections. Each type has type parameters and a TypePrior distribution over instances, with a Prior distribution over values that can be learned from data. Additionally, each type has a ToString method for converting values. The Alexandria probabilistic program defines entity types like Date, PersonName, Place, Hierarchy, and Quantity. Each type has a TypePrior distribution over instances and a Prior distribution over values. The Hierarchy type has a prior over format strings and assumes known hierarchies for occupations, nationalities, star signs, genders, religions, causes of death, and hair & eye colors. In the future, hierarchies may be learned using a type prior. Object types have hand-specified priors over properties and require manual specification of format parts for the ToString() method. The ToString() method for object types requires manual specification of format parts, such as date and month formats for Date type and name formats for PersonName type. The method uses GetParts() to compute string values for each format part. The format prior is a uniform distribution over valid formats. Hierarchy type is used for properties that take on hierarchies like occupations and nationalities. The Hierarchy type is used for properties that take on a set of values, allowing for varying specificity through a hierarchy. Each node in the hierarchy has associated strings, serving as synonyms in natural language. Ambiguous terms can be handled with uncertainty in the probabilistic model. The Hierarchy type allows for varying specificity through a hierarchy, with nodes having associated strings as synonyms in natural language. Ambiguous terms can be handled with uncertainty in the probabilistic model, and the ToString() method converts values into strings based on a specified depth. Each leaf node also has a prior probability, learned as part of schema learning. The Place type, a subtype of Hierarchy, has a modified ToString() method for selecting nodes. The Hierarchy type includes a modified ToString() method for selecting nodes along a path from leaf node to root, representing values in list form. Quantity type represents quantities like lengths, weights, etc., with predefined sets of units and conversion factors. Gaussian prior is used for values with type parameters for mean, variance, and a flag for value or logarithm. The ToString() method handles unit conversion and sub-units for Quantity values. The ToString() method in the Quantity type handles unit conversion and sub-units by extracting unit and subunit formats from the string, converting the value into the target unit, and inserting the value strings into the format string to give the final result. The unit names in ToString() can be learned from web text, and sets of values are represented using the Set<T> type. The Set<T> type represents sets of values with a primary type parameter T for element types. The set size is defined by a Poisson distribution, and each element has a 'renown' probability. Beta distribution is used for renown probabilities. The ToString() method creates a sample of elements based on renown probabilities. The author chooses whether to mention all elements or just one from a set. The placeholder count in the format matches the mentioned elements, converted into string values. The Set<T> type allows up to ten elements with suitable separators. Explicitly modeling the set's cardinality can be beneficial for understanding data relationships. Infer.NET allows probabilistic programs to be compiled into efficient C# code for inferring posterior distributions of specific variables. The generated code applies standard inference algorithms using a library of distributions and operations. Inference over string values uses weighted automata. Large scale inference queries on the probabilistic program include template learning. Large scale inference queries on the probabilistic program involve template learning, schema learning, and fact retrieval. These queries infer variables such as templates, schema properties, and entities from web texts and known information. The process is applied to billions of documents and millions of entities to achieve efficient inference. To scale up inference queries on a probabilistic program for template learning, schema learning, and fact retrieval from web texts and known information, a distributed and optimized version of the inference algorithm was developed. This algorithm, written partially in SCOPE BID1, allows for large-scale execution on Microsoft's Cosmos distributed computing platform. It uses a manually defined, message-passing schedule optimized for rapid convergence, with distributions representing uncertain values of structured types like strings, objects, hierarchies, and sets. To speed up the inference process, additional approximations were applied to the EP messages by collapsing or removing uncertainty without affecting precision. Uncertainty was preserved in template matching, property correspondence, and extracted values. Uncertainty in entity references could be collapsed conservatively at each iteration. Texts were assigned to existing entities based on probability thresholds or to new entities. Caching and other optimizations further improved speed. The processing pipelines for Alexandria template learning and fact retrieval queries run on Microsoft's COSMOS distributed computing and Azure Batch cloud computing platforms, achieving speed ups through caching, re-using results, and executing searches at scale across the document corpus. Alexandria runs on Microsoft's COSMOS distributed computing and Azure Batch cloud computing platforms. Execution times vary based on the task and data size. Template learning is more computationally intensive but can be done at a lower frequency than fact retrieval. Alexandria can infer schema and property values from web scale text data with minimal supervision. It was given a single labelled example for bootstrapping and a set of names for schema learning and property retrieval. Alexandria has access to 8 billion English HTML pages from the web, processed to remove HTML tags and non-visible text. Duplicate text sections are treated as single observations. A small-scale query with Barack Obama as the known entity was used to bootstrap the system, resulting in 2-property templates. The bootstrapping process involved extracting name and date of birth information to create 2-property templates and a schema. Large scale inference queries were then conducted to infer a full schema and property values for a dataset of 4000 names. This process included fact retrieval, template learning, and schema learning using different entity counts and property thresholds. The process involved retrieving texts for 4000 people from 8bn documents, taking 25K compute hours. Running distributed inference on the texts took 900 compute hours. The largest retrieval for 2 million people took 250K compute hours for text retrieval and 200K hours for inference. Results of schema learning showed top properties discovered. The schema learning process identified top properties based on the number of web domains referring to them. Niche properties with fewer than 20 domain references were excluded to focus on generally relevant properties. Some properties discovered did not fit into existing types and included descriptive text properties, times, and a few rarer types. The schema learning process identified top properties based on web domain references, excluding niche properties with fewer than 20 references. Some properties, like descriptive text, times, and rarer types, were not easily categorized. Evaluating fact retrieval involves comparing retrieved value distributions to ground truth values, a challenging task due to variations like \"scientist\" vs \"physicist\" or \"4/5/1980\" vs \"May 4th 1980\". The evaluation uses a probabilistic program to infer property values and determine correctness based on probability distributions. The prediction accuracy is determined by comparing the ground truth value to the predicted value using a specific equation. High precision of around 97-99% is aimed for, with evaluation against Microsoft's knowledge graph data. Challenges include uncertainty in property values and variations in data formats. In situations where there is uncertainty about property values, such as conflicting dates of birth, a fair approach is to consider both alternatives as correct. Satori already supports alternative values, accounting for inherent quantitative uncertainty in numeric properties. This uncertainty is factored into the evaluation using a noise model. During evaluation, a noise model is used to consider values correct if they are close to the ground truth. Set membership uncertainty is addressed by evaluating a set value as correct if any predicted element matches a ground truth element. Element-wise precision and recall metrics are monitored to understand accuracy within each set. Alexandria outputs a set of entities for each name in the test set, focusing on the entity corresponding to the test set or none at all if not found. During evaluation, Alexandria outputs a set of entities for each name in the test set, focusing on the entity corresponding to the test set or none at all if not found. The fact retrieval process evaluates the discovered properties, considering multiple alternative value distributions for a particular name and property. In 1977, alternatives are ranked by web page support. Precision@1 and Precision@2 metrics measure correct alternatives retrieved. Recall is the percentage of entities with predictions. Results in Table 3 show high precisions without ground truth values. Prec@1 values are mostly above 97%, with some above 98%. Children and hair/eye color properties have lower precisions. The precision of predictions for various properties is high, with most above 97% and some above 98%. However, children and hair/eye color properties have lower precisions. Date of birth and date of death have around 30-40% predictions with two alternatives, increasing precision by about 1%. Recall varies widely, from 16.4% for siblings to 95.3% for date of birth, with an average of 59.7%. The precision of predictions for various properties is high, with most above 97% and some above 98%. However, children and hair/eye color properties have lower precisions. Date of birth and date of death have around 30-40% predictions with two alternatives, increasing precision by about 1%. Recall varies widely, from 16.4% for siblings to 95.3% for date of birth, with an average of 59.7%. Fact retrieval metrics for discovered properties show comparisons to previous results, with Alexandria's accuracy being at least as good as existing supervised systems and superior to others. Alexandria's accuracy is comparable to existing supervised systems and better than unsupervised systems. Errors in retrieval were analyzed, with main causes including bad template matches, confusion between individuals with the same name, and page semantics not preserved during HTML processing. The system can perform schema learning and high-precision fact extraction unsupervised, except for a single seed example. The results focus on people entities, but the system is versatile. The Alexandria model aims to be generally applicable to various entity types, not just people. It can learn seed examples for different classes like places, enabling schema learning and fact extraction across domains. The focus for future work is to use Alexandria as an Open IE system for automatic schema learning, entity discovery, and fact extraction with high accuracy and strong typing to prevent drift. Unlike BID4, Alexandria does not use joint priors across property values, but incorporating such priors could potentially increase precision. Alexandria's template-based language model is simpler compared to other NLP systems. However, its model of types and values is more sophisticated, leading to high precision. The system can achieve good recall by processing the entire web, even with a simple language model. This simplicity allows for easy application to text in different languages and the system can learn to extract data from non-English pages. The Alexandria system can extract data from non-English pages by making types multi-lingual. This would improve recall and understanding of facts expressed differently in various locales. The system aims for automatic KB construction and maintenance, with potential for unsupervised learning in different domains. The Alexandria system aims to extract data from non-English pages by making types multi-lingual, improving recall and understanding of facts expressed differently in various locales. The system focuses on automatic KB construction and maintenance, with potential for unsupervised learning in different domains. Implementation details include ToString() methods for object and hierarchy types, with examples of templates learned during experiments. The Alexandria system focuses on automatic KB construction and maintenance, with potential for unsupervised learning in different domains. TAB9 includes examples of nested templates, treated separately by the model for lower uncertainty in template variable during inference."
}