{
    "title": "SJxstlHFPH",
    "content": "DrKIT is a neural module designed for answering complex multi-hop questions using a virtual knowledge base. It utilizes sparse-matrix TFIDF indices and maximum inner product search on contextual representations to traverse textual data. The module is differentiable and can be trained end-to-end from natural language inputs. Pretraining includes generating hard negative examples from existing knowledge bases. DrKIT improves accuracy on 3-hop questions in the MetaQA dataset by 9 points and is 70% more efficient than existing systems. DrKIT is a highly efficient neural module for answering complex multi-hop questions using a virtual knowledge base. It outperforms existing QA systems by 70% and can process up to 10x more queries per second. Large knowledge bases like FreeBase and Wikidata organize information around entities, making it easier to extract answers. However, these KBs are often incomplete, leading to errors in relation extraction methods. Open-domain QA suggests using a large corpus as a virtual KB to answer queries, avoiding the need for relation extraction. Using a large corpus as a virtual knowledge base for answering queries can help preserve facts and avoid the need for relation extraction. However, it can be costly and challenging to answer questions, especially complex ones involving sets of entities or paths of relations. This paper addresses the limitation of phrase-indexed question answering by introducing a method to answer complex queries that require information aggregation from multiple documents. This paper introduces an efficient framework for complex question answering over a large text corpus using a neural module that approximates multi-hop queries. The module takes a sparse vector as input representing entities and a dense feature vector as the relation, producing another sparse vector as output. The system introduced, DrKIT, utilizes a neural network for complex question answering by aggregating entities from top-k spans retrieved from an index. It allows for end-to-end learning without intermediate supervision and achieves faster inference compared to existing QA systems. DrKIT is a system for multi-hop and open-domain QA, outperforming prior text-based systems on complex questions. It improves by 5 points on 2-hop and 9 points on 3-hop questions, reducing the gap between text-based and KB-based systems. Additionally, DrKIT surpasses DrQA and PIQA on a new dataset for multi-hop slot-filling over Wikipedia articles. The system aims to answer questions using a text corpus as a KB, expanding entities to co-occurring mentions to simulate following relevant relation edges. The system DrKIT aims to answer multi-hop and open-domain questions by expanding entities to co-occurring mentions in the text corpus. A neural network filters mentions based on relevance to the question, resulting in an ordered set of answer candidates. This process can be repeated for questions requiring multiple hops. The approach is formalized in a probabilistic framework, with efficient mention expansion and filtering using sparse matrix products and MIPS algorithms. A pretraining scheme is discussed for constructing mention representations. The given corpus is processed by an entity linker to identify mentions of entities. In a weakly supervised setting, the final answer entities for a T-hop question are known during training. The probability of an intermediate answer is recursively calculated based on entity linking system output and single-hop models. When answering questions over a text corpus, reasoning is done over entity mentions rather than entities themselves. The relevance of a mention is determined by a scoring function based on TF-IDF vectors of the mention and the entity from the previous hop. This scoring function is different for each hop in the model. When answering questions over a text corpus, reasoning is done over entity mentions rather than entities themselves. The relevance of a mention is determined by a scoring function based on TF-IDF vectors of the mention and the entity from the previous hop. This scoring function is different for each hop in the model. The model also considers the joint modeling of co-occurrence and relevance, which is crucial for good performance. The mention scoring needs to be evaluated for all latent entity and mention pairs, but by using an inner product, this can be done efficiently. The proposed scheme efficiently implements computation using matrix operations. TFIDF terms are pre-computed into a sparse matrix for entities and mentions. Entity expansion to co-occurring mentions involves sparse-matrix multiplication. Relevance scores are determined by top-K relevant mentions encoded as a sparse vector. Mention aggregation to entities is done through multiplication with another sparse matrix encoding coreference. Overall, the process involves elementwise product and defines Z t as the probability distribution of entities given the query. The proposed scheme efficiently implements computation using matrix operations, with Z t representing the probability distribution of entities given the query. The number of non-zero entries in Z t is bounded by K, preventing explosion across hops. Z t\u22121 and Z t are viewed as weighted multisets of entities, with Eq. 4 mimicking graph traversal in a traditional KB. The model is trained end-to-end by optimizing crossentropy loss between Z T, the set of entities after T hops, and the ground truth answer set A. A temperature coefficient \u03bb is used in the softmax computation to prevent peaked distributions of Z t. Taking a maximum over the mention set of an entity works better than taking a sum, optimizing over the most confident mention. Sparse TF-IDF Mention Encoding is used for computation. Sparse TF-IDF Mention Encoding is utilized for entity-mention expansion, with TF-IDF vectors constructed over unigrams and bigrams. The expansion is computed efficiently using a sparse matrix by sparse vector product. The lower bound for this operation is independent of the matrix size. The lower bound for sparse matrix multiplication with k nonzeros is independent of matrix size. To achieve this, a vector-driven multiplication algorithm is needed, slicing out relevant rows from the matrix. The sparse matrix is represented as two row-wise lists for easy slicing, resulting in k sparse vectors with at most \u00b5 non-zero elements each. These vectors can be added weighted by values from another vector efficiently. This implementation is feasible with deep learning frameworks like TensorFlow. Efficient top-k mention relevance filtering is crucial for computation feasibility. By using dense encodings of mentions and questions, a parallel computation can be approximated, enabling efficient retrieval of relevant mentions for a given question without exhaustive enumeration. This approach is compatible with deep learning frameworks like TensorFlow. The scoring function for the t-th hop in question q is computed by multiplying a matrix with g t (q, z t\u22121), using an approximate algorithm for Maximum Inner Product Search (MIPS) to find top-k values. Mentions are encoded using a BERT-large model, with the complexity of filtering step using MIPS being O(kp polylog|M|). The query is encoded with a smaller BERT-like model and passed through a Transformer network to produce an output sequence. Two additional Transformer layers are added to produce MIPS queries for start and end tokens. Entity embeddings are used to condition on current progress. The model uses entity embeddings to construct an average embedding of the set, reducing computational complexity. Staged training approach is adopted to pre-train a mention encoder before computing and indexing. The training approach involves pre-training a mention encoder to better understand mentions, using distant supervision from an open-domain KB instead of fine-tuning BERT on Squad. This helps capture entity-and relation-centric questions more effectively. The training approach involves pre-training a mention encoder using distant supervision from an open-domain KB to capture entity-and relation-centric questions more effectively. This involves identifying tuples in entity-linked text passages and learning to answer slot-filling queries in a reading comprehension setup. Negative instances are also provided during pretraining to improve transfer to the full corpus setting. The curr_chunk discusses the types of hard negatives considered in the experiments, the use of Wikidata and Wikipedia for data collection, and the methodology for multi-hop question answering with text. The prev_chunk introduces the training approach involving pre-training a mention encoder using distant supervision from an open-domain KB. The curr_chunk discusses evaluating DrKIT on the MetaQA benchmark for multi-hop question answering. METAQA dataset includes questions with relation paths from a movies KB, covering 8 relations, 43K entities, and paired with Wikipedia passages. Results show accuracy of top-most retrieved entity for sub-tasks ranging from 1-3 hops. DrKIT outperforms prior state-of-the-art systems in 2-hop and 3-hop cases on the MetaQA benchmark for multi-hop question answering. Unlike PullNet, DrKIT does not require supervision from MetaQA KB during training, showcasing its end-to-end learning capability. Adding intermediate supervision to DrKIT only marginally improves performance on 1-and 2-hop questions, with no impact on 3-hop questions. DrKIT's architecture prioritizes efficiency, aiming to answer questions with minimal processing at query time. Comparing DrKIT with PullNet, runtime gains range between 5x-15x as the number of dense nearest neighbors retrieved varies. Ablations on DrKIT for MetaQA data show that certain modifications, such as taking a sum instead of max over entity mentions, removing softmax temperature, or the TFIDF component, significantly impact performance for 2-hop and 3-hop questions. Pretraining methods introduced in \u00a72.3 are also crucial for performance. The results emphasize the importance of pretraining methods for DrKIT's performance. Even with incomplete pre-training data, DrKIT outperforms PullNet and other KB-only methods. Analysis of correctly answered questions shows that the model often answers 2-hop questions in 1 hop and copies the answer for the second hop. Mistakes in answers were evenly distributed across questions. The intermediate accuracy was only 47%, evenly distributed mistakes across two hops. MetaQA dataset limitations due to small KB. Introducing a new task on a larger scale with more relations, entities, and text passages. New dataset allows evaluation with unseen test set entities, important for real-world QA systems. Sampling two subsets of Wikipedia articles for pre-training and testing. Consider WikiData entities mentioned in articles and sample paths of 1-3. The curr_chunk discusses the construction of a semi-structured query using WikiData entities mentioned in articles, with paths of 1-3 hop relations. The task involves extracting the tail entity from Wikipedia articles in a dynamic setting. Two open-domain QA systems, DrQA and PIQA, are adapted for this task. The dataset includes 10K articles, 120K passages, >200K entities, and 1.5M mentions. The curr_chunk discusses adapting open-domain QA systems DrQA and PIQA for a semi-structured query task involving WikiData entities. A cascaded architecture is considered, tuning the number of intermediate answers retained in each step. Templates are used to convert intermediate questions into natural text. The text discusses testing open-domain QA systems for a semi-structured query task involving WikiData entities. Results show that re-training PIQA on slot-filling data improves performance, but DrKIT trained on the same data performs even better. End-to-end training further enhances performance, showcasing the effectiveness of the differentiable operation introduced in the study. The study compares the performance of PIQA and DrKIT on slot-filling queries, with DrKIT showing significant accuracy gains. End-to-end training further improves DrKIT's performance, highlighting the effectiveness of the proposed scheme. DrKIT, when using entity linking, shows a significant improvement in accuracy to 66% in answering queries about rare relations. Probing experiments reveal that the slot-filling version is better at encoding entity co-occurrence information compared to vanilla BERT. DrKIT's performance is reported when the index is pretrained using WikiData KB, HotpotQA training questions, or both. The HotpotQA dataset consists of crowd-sourced multi-hop questions and answers over Wikipedia passages. The focus is on retrieving passages to answer questions in an open-domain setting. The model selects entities, but the answers are free-form text spans, making entity selection not directly applicable. The task involves retrieving passages from a pool of 5.2M for answering questions, which is a multi-hop IR task. DrKIT is used to identify entities in the passages that contain the answer information. The top 10 passages are then passed to a reading comprehension architecture to select the answer span. The text corpus used is Wikipedia abstracts, with hyperlinks considered as mentions of entities. For pretraining mention representations, the text uses WikiData KB or HotpotQA training questions with TF-IDF retrieved passages as negative examples. Entity linking over questions involves matching bigram TF-IDF vectors with entity surface forms. Gold entities are retrieved within 2 hops for 87% dev examples. Unlike MetaQA and WikiData, HotpotQA does not specify the number of hops in advance. DrKIT is run for 2 hops. The text discusses the process of running DrKIT for 2 hops to retrieve entities for answering questions, using a weighted average of entity distributions. The mixing weights are determined by a classifier trained on the retrieval task. This method allows for soft mixing of different templates for answering questions. The results are compared to a previous study in terms of retrieval accuracy. DrKIT improves accuracy significantly, especially at @2 and @5, compared to the entity-centric IR baseline. It also shows over 10x improvement in queries per second during inference. Pretraining the index with Wikidata KB or HotpotQA questions doesn't make much difference, but a combination of both yields the best performance. Neural Query Language (NQL) introduces differentiable templates for accessing a symbolic KB with implicit relations between entities. Despite DrKIT's improved accuracy over baseline models, there is only a small enhancement in performance when using passages retrieved by DrKIT compared to entity-centric IR baseline. Some questions lack the necessary supporting passage, suggesting that the downstream model can answer them without it. Recent research has explored using text corpora as knowledge bases for answering compositional questions, with approaches such as Knowledge Graph embeddings and graph neural networks. Talmor & Berant (2018) treated the entire web as a KB, while recent papers have looked at complex QA using graph neural networks or identifying paths of entities in text. Recent research has explored using text corpora as knowledge bases for answering compositional questions, with approaches such as Knowledge Graph embeddings and graph neural networks. In contrast, Sun et al. (2019) and Ding et al. (2019) incorporate a dynamic retrieval process to add text about relevant entities in a multi-step QA model. DrKIT is a differentiable module designed to answer multi-hop questions directly using a large entity-linked text corpus. DrKIT is a module designed to answer multi-hop questions using a large entity-linked text corpus. It achieves state-of-the-art results on the MetaQA dataset and has an efficient implementation using sparse operations and inner product search. The model uses embeddings for mentions and queries, as well as start and end positions, resulting in a 750MB index size. The entity to mention co-occurrence matrix only retains mentions in the top 50 paragraphs matched with an entity to ensure sparsity. The question encoder is initialized with a Transformer network from pre-training. The model DrKIT answers multi-hop questions using a large entity-linked text corpus. It pretrains the index on MetaQA and Wikidata corpus. Levy et al. (2017) dataset was used for training single-hop questions. The open version requires answers to be extracted from a corpus. The corpus is constructed by entity-linking paragraphs from Wikipedia articles of 8K subject entities. TFIDF and coreference matrices are built for the corpus to answer test set queries using a pre-trained index. DrKit-entities show high Hits@1 performance on Rare relations subset. Probing experiments compare BERT model representations finetuned on Wikidata slot-filling task. In probing experiments, BERT's performance is evaluated on encoding fine-grained entity-type and entity co-occurrence information. Results show BERT performs well on entity-type but poorly on entity co-occurrence. DrKIT model's good performance suggests fine-tuning on slot-filling task helps encode entity co-occurrence information. The DrKIT model's good performance suggests that fine-tuning on slot-filling task helps encode entity co-occurrence information."
}