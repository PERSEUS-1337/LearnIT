{
    "title": "SkBYYyZRZ",
    "content": "The choice of activation functions in deep networks significantly impacts training dynamics and task performance. The most widely-used activation function currently is ReLU, but new activation functions are being discovered through automatic search techniques. One such function, named Swish, has shown to outperform ReLU on deeper models across various datasets. Swish, a new activation function, has been found to improve classification accuracy on challenging datasets like ImageNet when replacing ReLUs. Its simplicity and similarity to ReLU make it easy for practitioners to make the switch in any neural network. Activation functions are crucial in deep networks, with ReLU being the most successful and widely-used currently. ReLUs enabled the successful training of state-of-the-art deep networks due to their optimization advantages over sigmoid or tanh units. The ReLU activation function is widely used in deep learning due to its simplicity and effectiveness. While other activation functions have been proposed, none have gained the same level of adoption as ReLU. Recent advancements in using search techniques to automate the discovery of components have shown promising results. Automated search techniques are used to discover novel activation functions, specifically focusing on scalar activation functions to replace ReLU without altering network architecture. The best discovered activation function, named Swish, is f(x) = x \u00b7 sigmoid(\u03b2x), showing promising performance compared to ReLU in deep networks. Experiments demonstrate that Swish consistently outperforms ReLU in deep networks for tasks like image classification. Replacing ReLUs with Swish units on ImageNet improves classification accuracy by 0.9% on Mobile NASNet-A and 0.6% on Inception-ResNet-v2 BID40. Designing a search space for novel activation functions involves balancing size and expressivity to effectively utilize search techniques. The activation function structure is designed with unary and binary functions to construct the activation function. It consists of a \"core unit\" with two inputs, two unary functions, and one binary function. Unary functions operate on a single scalar input, while binary functions operate on two scalar inputs. The activation function is constructed by repeatedly composing the \"core unit\" defined as b(u1(x1), u2(x2)). The activation function structure consists of a \"core unit\" with two unary functions and one binary function, aiming to transform a single scalar input into a single scalar output. The search algorithm aims to find effective choices for these functions within the search space, which can be exhaustively enumerated for small spaces but requires an RNN controller for larger spaces. The controller predicts components of the activation function, which are used to construct the function. A \"child network\" is trained with the predicted function for tasks like image classification. The validation accuracy of the child network updates the search algorithm. The RNN controller is trained with reinforcement learning to maximize validation accuracy, pushing it to generate effective activation functions. The controller uses reinforcement learning to generate activation functions with high validation accuracies. A distributed training scheme is employed to parallelize the training of child networks, where worker machines train and report back validation accuracies. The search algorithm is updated based on aggregated validation accuracies. The ResNet-20 BID14 architecture is used for training on CIFAR-10 for 10K steps. The study uses reinforcement learning to generate activation functions for neural networks. An RNN controller is trained with Proximal Policy Optimization to explore different unary and binary functions. The search algorithm varies the number of core units and functions available to construct the activation function. The study uses reinforcement learning to generate activation functions for neural networks by varying unary and binary functions. The top performing activation functions consist of 1 or 2 core units and utilize the raw preactivation x as input to the final binary function. Some functions also incorporate periodic functions like sin and cos. The study explores activation functions for neural networks using periodic functions like sin and cos. Functions involving division tend to perform poorly unless the denominator is bounded away from 0. The discovered activation functions may not generalize well to larger models. Additional experiments were conducted using the preactivation ResNet-164 to test the robustness of the top performing functions. In additional experiments with different architectures like ResNet-164, Wide ResNet 28-10, and DenseNet 100-12, novel activation functions were tested. Results showed that six out of eight activation functions successfully generalized, with two consistently outperforming ReLU across all models. The study tested novel activation functions on various architectures and found that six out of eight functions generalized well. The focus is on evaluating the effectiveness of the Swish activation function compared to ReLU and other candidates on large models across different tasks. Swish, defined as x \u00b7 \u03c3(\u03b2x), is smooth and nonmonotonic, unlike ReLU. Swish activation function, unlike ReLU, is smooth and nonmonotonic. Its non-monotonicity distinguishes it from common activation functions. The derivative of Swish can be controlled by the scale of \u03b2, affecting how fast it asymptotes to 0 and 1. The non-monotonic \"bump\" of Swish when x < 0 is a significant aspect, with a large percentage of preactivations falling within this domain. The non-monotonic bump in Swish can be controlled by changing the \u03b2 parameter, with trained values spread between 0 and 1.5. Implementing Swish in deep learning libraries is simple with a single line of code change. When using BatchNorm, the scale parameter should be set to optimize performance. Swish activation function requires adjusting the scale parameter for optimal performance, unlike ReLU. Lowering the learning rate for training Swish networks is recommended. Swish outperforms other activation functions on various datasets. Experimental settings and results are detailed in Table 3, comparing Swish to different baselines. The Swish activation function is compared against various baseline activation functions like Leaky ReLU, Parametric ReLU, and Softplus on different models and datasets. The comparison shows Swish outperforming other functions in statistical significance. Softplus, ELU, SELU, and GELU are different activation functions compared to Swish. Swish is evaluated against baseline functions on CIFAR-10 and CIFAR-100 datasets. Swish and Swish-1 consistently outperform ReLU on CIFAR-10 and CIFAR-100 datasets, matching or exceeding baseline performance on various models. Softplus also shows strong performance similar to Swish. Swish is then benchmarked against baseline activation functions on the ImageNet 2012 classification dataset. The study evaluates different activation functions on various architectures designed for the ImageNet dataset, including Inception-ResNet-v2 and Mobile NASNet-A. Replacing ReLU with different activation functions, they train for a fixed number of steps and test with different learning rates using RMSProp. All networks are initialized with He initialization for reproducible performance evaluation. The study compares the performance of different activation functions on various models for the ImageNet dataset. Swish shows strong performance, outperforming ReLU on Inception-ResNet-v2 and Mobile NASNet-A. It performs well on mobile-sized models, with a boost over ReLU. Softplus achieves comparable accuracies to Swish on larger models but performs worse on mobile-sized models. Overall, switching to Swish improves performance with little additional cost. Switching to Swish improves performance with little additional tuning, as shown in machine translation benchmarks. Swish outperforms other baselines on newstest sets, with Swish-1 excelling on newstest2016. Swish outperforms other baselines on newstest2016 by 0.6 BLEU points. Softplus shows inconsistency in performance across domains, while Swish consistently performs well. Automated search techniques were used to discover Swish, a technique also applied in finding convolutional and recurrent architectures and optimizers. Meta-learning, a subfield that allows for flexible solutions with minimal assumptions, has been utilized in various areas such as one-shot learning and reinforcement learning. This work focuses on finding scalar activation functions with strong empirical performance. Different types of activation functions, such as many-to-one, one-to-many, and many-to-many functions, play a crucial role in deep networks. Prior research has mainly focused on proposing new activation functions, with few studies exploring other aspects. This study compares scalar activation functions across challenging datasets, showing that Swish outperforms ReLU on deep models. The importance of the gradient preserving property of ReLU is challenged by the performance of Swish. Architectural improvements reduce the need for individual components to preserve gradients. In this work, novel activation function Swish (f(x) = x \u00b7 sigmoid(\u03b2x)) was discovered using automatic search techniques. Empirical validation showed Swish consistently outperformed ReLU and other functions, even with simple model modifications. Swish's simplicity and similarity to ReLU make it easy to replace ReLUs in any network with just one line of code."
}