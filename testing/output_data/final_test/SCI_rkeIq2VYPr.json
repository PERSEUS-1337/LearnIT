{
    "title": "rkeIq2VYPr",
    "content": "Determinantal point processes (DPPs) are used in machine learning and computer vision for diversity. Optimizing DPP directly in deep learning is challenging due to computational instability. A new algorithm is proposed to optimize DPP term directly using L-ensemble in the spectral domain over the gram matrix, enhancing flexibility. Geometric constraints are considered to generate valid sub-gradients of the DPP term. Our algorithm generates valid sub-gradients of the DPP term when the gram matrix is not invertible, making it easily applicable to various deep learning tasks. Experimental results demonstrate its effectiveness and promising performance for practical learning problems in machine learning and computer vision applications. DPP is a diversity-oriented technique that shows superiority by incorporating a single metric and delivering genuine diversity in a bounded space. It is utilized in various tasks where sample points tend to distribute diversely. The probability of a discrete point set under a DPP with a kernel function can be characterized by a matrix called L-ensemble. DPP ensures that mapped points are not similar or linearly dependent, as a larger determinant implies a larger spanned volume in the associated Hilbert space. In learning DPP, the term det(L) is a diversity measurement extended to continuous space. Two strategies for learning DPPs are approximation methods, like low-rank approximation, and conversion to simpler formats for computational ease. The gram matrix is factorized as L = BB where B \u2208 n\u00d7m with m n, reducing complexity to cubic time of |L|. Kernel expressed as \u03ba(x, y) = \u03c3 1 \u03c3 2 \u03b4(x) \u03b4(y), calculating pairwise similarity in Euclidean feature space with cosine distance. Approximating eigenvalues of DPP for computation ease and stability. DPP applied in visual tasks like video summarization, ranking, and image classification, but approximation may not fully deliver diversity property. Direct optimization methods have been proposed to optimize DPP directly, updating the gram matrix L for more flexible feature mapping functions. Various algorithms such as Expectation-Maximization and K-Ascent have been introduced by researchers like Gillenwater et al. (2014) and Mariet & Sra (2015). Bardenet & Titsias (2015) suggested optimizing DPP using a lower bound in variational inference. The main challenge lies in the computational complexity of these methods. The computation in deep learning frameworks is hindered by the non-differentiable nature of DPP. Previous methods have not integrated DPP as a diversity metric in deep learning due to the unstable gradient calculation involving matrix inversion. Alternative methods like global pairwise orthogonality constraints and statistical moments are less effective in characterizing diversity compared to DPP. In this paper, the focus is on integrating DPP as a diversity metric in deep learning by proposing a sub-gradient generation method. This method introduces a \"differentiable direct optimization\" procedure to produce genuinely diverse features in a continuous bounded space. It is stable and scalable for large datasets with specific mini-batch sampling strategies, as verified by experiments on various tasks. Notations: Bold lower case x and bold upper case K represent vector and matrix, respectively. det(\u00b7) and Tr(\u00b7) calculate the determinant and trace of a matrix. The text discusses determinantal point processes (DPP) in deep learning, focusing on sub-gradient generation for diversity. It introduces methods for producing diverse features in a continuous space, with stable and scalable results for large datasets. Notations for vectors and matrices are defined, along with operations like determinant and trace calculations. The conversion from a positive semi-definite (PSD) matrix to a kernel is explained, highlighting the normalization process. In this section, the text discusses optimizing ensemble L directly and briefly introduces the Gaussian kernel. It mentions Mercer's theorem for constructing new kernels and the procedure used in multiple kernel learning paradigms. The focus is on the difficulty of deep networks due to the conversion from K to L and the need to carefully adjust gradients to ensure validity. The text discusses the Gaussian kernel and its properties in relation to an L-ensemble matrix. It highlights the bounded determinant value with the Gaussian kernel and its implications for algorithm development. The goal is to learn a mapping function that spreads features within a bounded Euclidean space. The text discusses the calculation of the gradient involving DPP in a bounded Euclidean space. It focuses on maximizing det(L) as an objective term for diverse feature selection using a DPP regularization term with a kernel \u03ba. The objective is to maximize det(L) for diverse feature selection using a DPP regularization term with a kernel \u03ba. The gradient of the determinant equipped with a Gaussian kernel is discussed, with the derivative expressed in a compact form. To maximize feature diversity, a DPP regularization term with a kernel \u03ba is used. The gradient of the determinant with a Gaussian kernel is discussed, focusing on the calculation challenges when the matrix is not invertible due to identical features caused by functions like Relu. The issue of numerical precision in GPU calculations can lead to errors, even when all features are distinct. Pseudo-inverse can be used on singular matrices but will maintain zero eigenvalues, affecting gradient updates. To address this, maximizing the determinant in DPP can provide a valid direction for back-propagation in deep learning. In deep learning, the back-propagation procedure can calculate using the eigen-decomposed matrix L. By sorting eigenvalues and amplifying smaller ones, a modified matrix with a small positive determinant can be obtained. This procedure ensures continuity and provides a valid direction for back-propagation in deep learning. The DPPSG algorithm enhances diversity by separating identical or close feature pairs in deep learning. It utilizes a modified matrix with a small positive determinant to provide a valid direction for back-propagation. Inspired by geometric inequality, an improved version considers the Gaussian kernel property. The function \u03c3i is concave in the feasible set, reaching its maximal objective when \u03c3i = 1. The DPPSG* algorithm introduces a proper sub-gradient based back-propagation method for deep learning. It utilizes geometric constraints to generate sub-gradients and determines whether to use normal back-propagation or sub-gradient based on the irregularity of the matrix L. This method can be integrated into deep learning frameworks for objectives involving matrix determinant, distinguishing itself from gradient-projection based methods like K-Ascent. Our method utilizes proper sub-gradients directly for deep learning, avoiding matrix inversion. A balanced sampling strategy is employed for mini-batches, ensuring diversity in training samples. Features are constrained to a bounded space for better control in applications. The method utilizes balanced sampling for diverse training samples and constrains features to a bounded space for better control. To enforce features mapped to a specific distribution, Wasserstein GAN is employed. Random sampling of points from the distribution is done for positive samples. The WGAN loss for discriminator is calculated based on the discriminator h(\u00b7) mapping input to feature. Two experiments are conducted to validate the method's effectiveness: metric learning and image hashing on MNIST and CIFAR, and local descriptor retrieval based on HardNet. For the MNIST test, a simple network structure with convolutional and fully connected layers is used, along with batch normalization on each layer. The performance of the network with batch normalization and specific filter sizes is evaluated in Table 1 and Figure 1. Adding DPP and WGAN regularization terms enhances retrieval task performance by avoiding feature concentration. DPP term ensures smoother learned maps around separating boundaries, which is more preferable for top-k inter-class samples. Feature distribution is visualized in Figure 1. In Figure 1 (c), feature points fall into the pre-defined space [-1, 1]. DPPSG* is slightly superior to DPPSG. Image hashing on CIFAR-10 produces binary codes using a Sigmoid function. DPP regularization enhances utility in binary code space. Testing is done with 12 and 16-bit binary codes. Feature distribution is visualized using TSNE in Fig. 2 (a) and (b). The binary code generation and classification method significantly enhances binary space utility while maintaining performance. VGG-19 convolutional layers are used as the base with 3 fully connected layers added. Contrastive loss is applied in a 20-dimensional space, and the network is trained from scratch. Additional metrics such as Precision-k and Wasserstein distance are also reported. The DPP+WGAN model outperforms the baseline in both coarse and fine classification levels. The DPP term acts as a regularization for feature smoothness. Larger batch sizes lead to better performance in CIFAR-100 retrieval. The computational efficiency of DPP sub-gradients is high. The DPP regularization term adds slight overhead to back-propagation under contrastive loss. Image hashing on CIFAR-10 is tested using the UBC Phototour dataset. The DPP regularization term is added to the state-of-the-art algorithm HardNet. The authors compared the performance of different baselines including SIFT, MatchNet, TFeat-M, L2Net, HardNet, and HardNet+ under the DPPSG* setting. They introduced their method, HardDPP, which showed improved performance with DPP regularization. HardNet does not integrate WGAN and focuses on embedding behavior near the margin, while DPP regularization enhances global feature distribution. In this study, the authors investigated learning diverse features using a determinantal point process in deep learning. They developed a method called proper spectral sub-gradient generation to address gradient instability. By incorporating Wasserstein GAN, they constrained features into a bounded space, resulting in improved performance on various criteria. DPP+WGAN framework showed significant performance on common criteria and feature space utility. Parameters for training and testing stages were specified for both MNIST and CIFAR-10 image hashing experiments. The WGAN model parameters include a loss term of 10 for the discriminator and 1 for the generator. The batch size is 500, learning rate starts at 0.01 and changes by 0.1 every 150 epochs. The total number of epochs is 350, using the Adam optimizer. Additional settings include \u03b1 = 1, \u03bb1 = 10^3, \u03bb2 = 10^3, margin \u00b5 = 0.8, variance for Gaussian kernel \u03c3 = 0.2, and \u2206 = 10^-6. Evaluation criteria for image retrieval tasks include mAP-k and Precision-k. The overhead of DPP involves time-consuming calculations for SVD or matrix inversion on a large number of features. In our setting, we used mini-batch strategy in deep learning to reduce computational cost, making the overhead of DPP dependent only on batch size. The complexity of our method is O(n^3), where n corresponds to batch size. We compared overhead on CIFAR-10 hashing task with varying batch sizes on a GTX 1080 GPU. In practice, the overhead of DPP computation is small compared to other computations, even in a simple network like CIFAR-10 hashing. A batch size of up to 500 is sufficient for most applications. Standard Pytorch functions were used without any tricks to reduce overhead. For the MNIST verification test, a simple backbone structure was employed. DPP and WGAN regularization were added to the features at the \"fully con2\" layer for better visualization. The same network structure was used for the CIFAR-10 image hashing task. For CIFAR-10 image hashing, DPP and WGAN loss are applied on the second last fully connected layer. For CIFAR-100 metric learning, VGG19 convolutional layers are used with 3 fully connected layers. DPP, WGAN, and contrastive loss are applied on the final fully connected layer. Performance degradation with DPP on hashing task is illustrated in Figure 2 (c). Original DCH features focus on specific digits, while DPP features spread across the discrete space. DPP features diffuse across the discrete space, leading to a larger searching radius for the k-th closest code. This can cause degradation as it may reach a code in another class. The conflict between \"utility vs mAP\" requires a trade-off."
}