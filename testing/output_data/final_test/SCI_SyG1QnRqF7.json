{
    "title": "SyG1QnRqF7",
    "content": "In this paper, a principled approach is proposed to train networks with improved resistance to variations between training and testing data by embedding a learnable transformation module into introspective networks. This approach involves synthesizing pseudo-negative samples with learned transformations to enhance the classifier. The approach involves training networks with improved resistance to variations between training and testing data by embedding a learnable transformation module. Experimental results show significant improvements in classification accuracy on benchmark datasets like MNIST, affNIST, SVHN, and CIFAR-10. In order to address the challenge of variations between training and testing data for image classifiers, a common strategy is to use data augmentation. This involves applying various transformations such as translations, rotations, flips, and noise injection to the training data. However, this brute-force approach is inefficient and not theoretically formulated. An alternative approach is to generate synthetic training samples using generative models. By embedding a learnable transformation module into introspective networks, the robustness of CNNs to large variations between training and testing data can be improved. The approach involves embedding a learnable transformation module into introspective networks, creating an introspective transformation network (ITN) that uses a reclassification-by-synthesis algorithm for training. The ITN synthesizes samples with learned transformations and enhances the classifier by retraining it with these synthesized samples. Transformation modules are learned jointly with the CNN classifier to augment training data intelligently and narrow down the search space for variations. This approach can be applied to models with generative capabilities. Our approach involves embedding a learnable transformation module into introspective networks, creating an introspective transformation network (ITN) that uses a reclassification-by-synthesis algorithm for training. The ITN synthesizes samples with learned transformations to enhance the classifier. Introspective networks have advantages over GANs, such as maintaining a single CNN discriminator that is also a generator. They are easier to train with gradient descent algorithms and help classifiers resist larger variations between training and testing data efficiently. Our approach involves embedding a learnable transformation module into introspective networks to create an introspective transformation network (ITN) for better performance in classification and cross-dataset generalization. It outperforms standard data augmentation methods and shows great abilities in resisting variations between training and testing data. Previous works have focused on data augmentation, but our method is more efficient and stable by connecting generative models with discriminative classifiers. Generative Adversarial Networks (GANs) like DCGAN, WGAN, and WGAN-GP have revolutionized image generation by training generators to produce realistic images that fool discriminators. Various techniques have been developed to enhance adversarial learning and improve generative models for complex image generation. Introspective networks offer an alternative approach to generating samples, closely related to GANs but with a single model for both generative and discriminative tasks. Unlike GANs, introspective networks focus on introspective learning and directly model image statistics for efficient sampling. Our approach involves introspective learning, focusing on generating unseen examples that complement the training dataset. We review the framework proposed by BID28 and provide a detailed mathematical explanation. Introspective learning aims to model positive samples by learning a generative model. The generative model for positives can be obtained from the discriminative model. The approach involves introspective learning to generate unseen examples that complement the training dataset. A solution from BID28 involves learning a generative model for positives from a discriminative model and a generative model for negatives. The iterative process updates the initial reference distribution of negatives to learn p(x|y = \u22121). Introspective Convolutional Networks (ICN) and Wasserstein Introspective Neural Networks (WINN) from BID19 use introspective learning to strengthen classifiers with a reclassification-by-synthesis algorithm. However, they struggle to capture large data variations between training and testing data. Test data often contain unseen variations not present in training data, such as objects viewed from different angles or with shape deformation. Our approach builds upon introspective learning to address the issue of large data variations between training and test data. We aim to quickly generate extra training samples with unseen variations not covered by the training data to improve classifier robustness. This is achieved by applying a transformation function to the original training samples, generating samples that still belong to the same category but with altered geometric properties. Our approach builds upon introspective learning to quickly generate extra training samples with unseen variations not covered by the training data. This is achieved by applying a transformation function to the original samples, altering their geometric properties while preserving the high-level properties. The training procedure of ITN involves positive and transformed positive sample sets, pseudonegatives, and a discriminative model to build robust classifiers. The approach involves building S \u2212 0 by sampling pseudo-negatives from p0(x|y = \u22121), initializing parameters \u03b8, \u03c9, and \u03c8, and iterating to compute transformation parameters and update samples. The model parameters are updated using pseudo-negatives and transformed positive samples to resist unseen variations. The approach involves minimizing the Wasserstein distance to approximate the distribution of pseudo negatives. The discriminative model aims to correctly classify x + , x T, and x \u2212 by training on extra samples. For multi-class classification, the reclassification-by-synthesis scheme needs to be adapted. To adapt the reclassification-by-synthesis scheme for multi-class problems, strategies like using one-vs-all classifiers or a single CNN classifier can be followed. The challenge lies in learning the unknown variable \u03c3 t in a principled manner to build robust classifiers, which is addressed by formulating a min-max problem. This approach provides a unified perspective on prior work in building robust classifiers. The inner maximization part aims to find the transformation parameter \u03c3 that achieves high loss values, while the outer minimization aims to find model parameters \u03b8 that enable correct classification. Solving Eqn. 7 directly is difficult, so a \u03c3 * is first found. Empirically, the first term in Eqn. 8 dominates, allowing the focus on learning more robust classifiers by dropping other terms. The goal is to find \u03c3 * that makes x T hard to classify correctly. The transformation parameter \u03c3 is efficiently found to increase the robustness of classifiers. The function parameter \u03c8 is learned to allow gradients to flow through the transformation function T during backpropagation. The discriminative model p(y|x) is updated and used to compute the generative model. The generative model is computed using Eqn.(3) in section 3.1 by maximizing the likelihood function p(x). Learning the generative model directly is cumbersome, so samples are updated to maximize the log likelihood of p \u2212 n (x). The update step involves random Gaussian noise \u03b7 and a step size \u03bb that is annealed in the sampling process. In practice, updates from previous samples reduce time and memory complexity. An update threshold ensures quality of negative samples by tracking f t (x; \u03b8 t ) in each iteration. A set D records E[f t (x; \u03b8 t )] to compute a normal distribution N (a, b) for a stop threshold. This ensures generated negative images are close to majority of positive images in feature space. The algorithm demonstrates resistance to large variations between training and testing. Our algorithm demonstrates outstanding classification performance on benchmark datasets and shows resistance to large variations between training and testing data. It outperforms CNNs, DCGAN, WGAN-GP, ICN, and WINN in challenging classification tasks. The architecture is flexible in addressing different types of unseen variations. The training phase involves generating negative samples to augment the original training set. The training phase involves generating negative samples to augment the original training set using a simple CNN architecture with specific filter sizes and activation functions. The method relies on a transformation function to convert original samples to unseen variations. The training phase involves generating negative samples to augment the original training set using a simple CNN architecture. The transformation function T(\u00b7) is used to convert original samples to unseen variations, demonstrating the effectiveness of ITN in resisting large variations with spatial transformers. The algorithm is evaluated on 4 benchmark datasets: MNIST, affNIST, SVHN, and CIFAR-10. In the experiments, various affine transformations were applied to the MNIST dataset, with the training, validation, and testing sets reduced in size. SVHN and CIFAR-10 datasets were introduced to further test the performance of ITN on real-world datasets. Standard data augmentation techniques were used, including rotation, translation, scaling, and shear. The results showed that the method achieved the best performance on all four datasets. Our method achieves the best performance on all datasets by generating novel negative images that strengthen classifiers. Performance improvements are more significant on complex datasets, indicating our method's ability to resist unseen variations. It outperforms standard data augmentation techniques on all datasets. Our approach, ITN, outperforms standard data augmentation on all datasets, confirming its effectiveness. When integrated with data augmentation, ITN shows even greater performance, as the explored space between the two methods is not overlapped, leading to the discovery of more unseen variations. Images generated by our method on various datasets demonstrate substantial performance improvements. Further exploration shows our method's ability to resist large variations across datasets. Our method, ITN, demonstrates superior performance in a challenging cross-dataset classification task between MNIST and affNIST datasets. It shows clear improvements over CNN, WGAN-GP, and WINN, even outperforming CNN with data augmentation. The key challenge lies in overcoming significant data variations between the training and testing sets. The experiment aims to showcase the effectiveness of ITN in handling unseen variations by reducing the training samples in the MNIST dataset to 0.1%, 1%, 10%, and 25%. Data augmentation improves classification performance by generating unseen samples, but it lacks efficiency and precision. Reducing training samples increases the discrepancy between training and testing sets, highlighting ITN's potential in resisting variations. Our method, ITN, demonstrates superior performance in handling unseen variations in the testing set compared to data augmentation. Experiments on both MNIST and CIFAR-10 datasets show consistent results, confirming ITN's efficiency. Testing errors for classification tasks are lower with ITN, showcasing its ability to resist data variations. Our algorithm can generalize to different types of transformations and utilize multiple transformations simultaneously to enhance resistance to variations. By replacing ST modules with DDT modules and combining both in our model, we verify performance using the MNIST dataset. The experiments involve testing sets with MNIST dataset transformations. Two types of testing sets are introduced: one with random DDT transformation and the other with random DDT and affine transformations. Parameters for DDT transformation are drawn from N(0, 0.7 \u00d7 Id) and images are placed in 42 \u00d7 42 images. Similar experiments are conducted on the CIFAR-10 dataset. Results show that ITN can integrate with DDT or DDT + ST to resist variations and partial unseen transformations in testing data without degrading performance. Our method, ITN, allows for the application of multiple transformation functions without knowing the types of variations in the testing data, maintaining good performance. It strengthens classifiers by generating unseen variations with learned transformations, showing consistent performance improvements across classification tasks and dataset generalization. ITN is more effective and efficient than traditional data augmentation methods. Future work includes applying ITN to large-scale datasets. Our approach, ITN, is more effective and efficient than traditional data augmentation methods. Future work involves applying ITN to large-scale datasets and expanding it to generate samples with more variations."
}