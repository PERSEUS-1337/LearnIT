{
    "title": "B1e3OlStPB",
    "content": "Designing a convolution for a spherical neural network involves balancing efficiency and rotation equivariance. DeepSphere, a method using a graph representation of the sphere, achieves this balance. The study explores how graph structure affects equivariance and evaluates DeepSphere's performance on various problems, showing efficiency and flexibility. Anisotropic filters may not be necessary. Spherical data is common in applications like planetary and brain activity analysis, as well as in observing the universe and LIDAR scans. Spherical data, such as observations of the universe and 3D objects, require inference of variables. Graphs efficiently represent spherical data for neural network inference. Full-sphere sampling is not ideal due to limitations like scalp measurements of brain activity and variable resolutions desired by climate scientists. Neural networks have been adapted to handle spherical data effectively. DeepSphere leverages graph convolutions to efficiently handle spherical data, ensuring computational efficiency, sampling flexibility, and rotation equivariance. The approach models the sampled sphere as a graph of connected pixels, approximating geodesic distances using a graph CNN formulation and hierarchical sampling pooling strategy. DeepSphere leverages graph convolutions to efficiently handle spherical data by constructing a weighted undirected graph from a sampling scheme on the sphere. Different samplings like equiangular, HEALPix, and icosahedral are used depending on the application. The graph Laplacian is defined to capture similarity measures between vertices, enabling efficient analysis of signals on the sphere. The graph Laplacian L is defined as L = D \u2212 A, where A is the weighted adjacency matrix and D is the diagonal degree matrix. The freedom in constructing the graph lies in setting the weights w to minimize equivariance error. Convolutions on the sphere are typically performed in the spectral domain through a spherical harmonic transform (SHT), with a computational cost of O(n 3/2 ) on isolatitude samplings. Graph convolutions can be defined with polynomial order P and coefficients \u03b1 i to be optimized during training. Khasanova & Frossard use these convolutions, which cost O(n) operations through recursive application of L. Pooling involves summarizing data on sub-pixels with permutation invariant functions like maximum or average, while unpooling copies data from a pixel to its sub-pixels. NNs are fully convolutional with global average pooling for rotation invariant tasks. Graph convolutions with batch normalization and ReLU activation are used for rotation invariant tasks. The ability of the graph framework to represent the underlying sphere depends on sampling locations and graph construction. The existence of a suitable subspace of continuous functions for invertibility is a common problem in signal processing. The text discusses the existence of F V and invertibility of T V in the context of equiangular sampling. It also explores rotation equivariance in graph convolution and the relationship between rotation operator and Laplacian. The text concludes with defining normalized equivariance error for empirical evaluation. The normalized equivariance error for signals and rotations is defined, with a tradeoff between equivariance and computational cost. A weighting scheme was designed to minimize equivariance error for longitudinal and latitudinal rotations. The resulting convolution is not equivariant to the whole of SO(3) but is sufficient for omnidirectional imaging. The weighting scheme designed minimizes equivariance error for longitudinal and latitudinal rotations, making the convolution sufficient for omnidirectional imaging. Inspired by Belkin & Niyogi (2008), the Laplacian converges to the Laplace-Beltrami operator for random uniform sampling, allowing for commutation with rotation. An approximation is made by considering only the k nearest neighbors to limit convolution cost. Optimal kernel widths are found for various resolutions of the HEALPix sampling. The optimal kernel widths for various resolutions of the HEALPix sampling are shown in Figure 3. The weights are computed based on the number of neighbors and the average squared Euclidean distance between connected vertices. However, a heuristic overestimation of the weights leads to increased equivariance error. The proposed weighting scheme is analyzed both theoretically and empirically, extending the work of Belkin & Niyogi (2008) to a deterministic sampling. The text discusses a weighting scheme applied to points on a sphere, adapting the radius of the kernel based on the number of samples. Theorem 3.1 establishes equi-area sampling on the sphere, leading to equivariance with the Laplace-Beltrami operator. Theorem 3.2 shows that the scaled graph Laplacian commutes with rotation in the limit of infinite sampling, ensuring equivariance of the discrete graph Laplacian as n approaches infinity. The proof of Theorem 3.1 inspires the construction of the graph Laplacian, indicating that scaling should be proportional to n^\u03b2. Theorems 3.1 and 3.2 provide asymptotic results, but in practical applications, finite samplings are used. Spectral convergence of the graph Laplacian is preferred, and the HEALPix sampling method is believed to meet the necessary conditions. The HEALPix sampling method is believed to satisfy the necessary conditions for spectral convergence of the graph Laplacian. Empirical results support this hypothesis, as shown in Figure 2, which compares equivariance error for different parameter sets of DeepSphere using HEALPix and equiangular sampling. The error is analyzed based on sampling resolution and signal frequency, with specific settings for each sampling method. The error in the measured equivariance is mainly due to imperfections in the Laplace-Beltrami operator approximation, not the sampling method. The weighting scheme does not achieve equivariance to all rotations. Optimal kernel width selection for 8 neighbors improves performance without added cost. Increasing resolution reduces equivariance error in high frequencies, likely due to sampling effects. More neighbors lead to decreased equivariance error, allowing precise control over cost and equivariance tradeoff in 3D shape recognition. Representing 3D shapes as spherical maps allows for rotation invariant treatment in tasks like shape retrieval contests. Objects are represented by 6 spherical maps with depth and normal maps generated for each pixel. Sampling is done using equiangular or HEALPix methods with specific parameters. The equiangular and HEALPix graphs are constructed with specific parameters for shape classification. The neural network consists of graph convolutional layers, max pooling layers, a fully connected layer, and polynomials of order P = 3. The model is optimized with a cross-entropy plus triplet loss, trained for 30 epochs with Adam optimizer. Results are compared with Cohen et al., showing similar performance in shape classification and retrieval tasks. 10 DeepSphere achieves similar performance to previous studies at a lower cost, suggesting anisotropic filters are unnecessary. The spherical maps' information mainly resides in low frequencies, so reducing equivariance error did not improve performance. Changes in sampling or resolution did not impact performance either. The convolution is equivariant to rotations, but using the convex hull did not show significant improvement. The tradeoff between cost and accuracy is highlighted in estimating cosmological parameters. The study focuses on estimating cosmological parameters using likelihood-free inference and benchmarking prediction methods on the classification of spherical maps. The classification task involves 720 partial convergence maps from two cosmological models with different parameters and noise levels. Convergence maps represent mass distribution in the universe through over-and under-densities. Graphs are built with different numbers of neighbors, and the study highlights the tradeoff between cost and accuracy in parameter estimation. The study uses graphs with different numbers of neighbors and optimal kernel widths to classify spherical maps. It employs graph convolutional layers, max pooling, and fully connected layers with softmax. Results show lower equivariance error leads to higher performance, likely due to high frequency content in the maps. There is a tradeoff between cost and accuracy controlled by the number of neighbors. The experiment demonstrates DeepSphere's flexibility and scalability. The study evaluates DeepSphere's flexibility and scalability in classifying spherical maps for the segmentation of extreme climate events. The dataset consists of 16 channels from a 20-year run of the Community Atmospheric Model v5, with heavily unbalanced labels. The graph is built with 6 neighbors and a kernel width set to the average of distances. The NN model uses an encoder-decoder with skip connections and polynomials of order P = 3. Cross-entropy loss is optimized with Adam for 30 epochs, with a learning rate of 1 \u00b7 10\u22123 and batch size of 64. Results are shown in table 3, with mean and standard deviation computed over 5 runs. Larger architecture improves performance, with more feature maps and depth leading to higher performance. The study demonstrated the flexibility of modeling spherical data using a graph by collecting historical measurements from 10,000 weather stations worldwide. The data was heavily non-uniformly sampled, with a higher density of stations over North America. Two artificial tasks were devised: dense regression to predict temperature based on previous days and global regression to predict the day from temperature or precipitation. The graph was built with 5 neighbors and a kernel width set to the average distance. The equivariance property of the resulting graph was not tested due to the non-uniform sampling. The NN model consists of 3 graph convolutional layers with polynomials of order P = 0 or 4 and 50, 100, 100 channels per layer. Global regression includes a GAP and fully connected layer, while dense regression includes a graph convolutional layer. MSE loss is optimized with RMSprop for 250 epochs. Results in table 4 show that using spherical geometry improves performance. DeepSphere achieves a balance for a spherical CNN with the number of neighbors k as a key parameter. DeepSphere, a spherical CNN, utilizes a graph representation for scalability and flexibility in handling large spherical maps. The number of neighbors k controls the tradeoff between cost and performance. Graph convolution implementation is straightforward, with potential drawbacks in isotropy of graph filters. The ubiquity of graph neural networks will further enhance efficiency in implementations. Experiments show that DeepSphere's isotropic filters do not hinder performance on 3D shapes and climate tasks. Possible reasons for this include NNs compensating for the lack of anisotropic filters or tasks being solvable with isotropic filters. Developing graph convolutions on irregular samplings respecting sphere geometry is an important research direction. The proof of theorem 3.1 is inspired by the work of Belkin & Niyogi (2008) on sampling a manifold M and defining a smooth function f. The proof leverages the extended graph Laplacian operator and involves the uniform probability measure on M. The Laplace-Beltrami operator \u2206 M is defined as the divergence of the gradient of a differentiable function f : M \u2192 R on a manifold M. Belkin & Niyogi (2008; 2007) have proven convergence of the extended graph Laplacian towards the Laplace-Beltrami operator on any compact, closed, and infinitely differentiable manifold M with random sampling. Their results are interpreted in a probabilistic sense, showing convergence in probability as n \u2192 \u221e and as t \u2192 0. The Laplace-Beltrami operator \u2206 M is defined as the divergence of the gradient of a differentiable function f : M \u2192 R on a manifold M. Belkin & Niyogi (2008) have proven convergence of the extended graph Laplacian towards the Laplace-Beltrami operator on any compact, closed, and infinitely differentiable manifold M with random sampling. This convergence occurs in probability as n \u2192 \u221e and as t \u2192 0. Proposition 1 by Belkin & Niyogi (2008) states that for a k-dimensional compact smooth manifold M embedded in Euclidean space R N, the extended graph Laplacian L t n converges towards its continuous counterpart L t as the sampling size increases. The Heat Kernel Graph Laplacian operator L t n converges pointwise to the Laplace Beltrami operator on a Lipschitz function defined on a sphere. This convergence is proven by utilizing Proposition 1 and finding a sequence of t n that decays faster than a given rate. The Heat Kernel Graph Laplacian operator L t n converges pointwise to the Laplace Beltrami operator on a Lipschitz function defined on a sphere. This convergence is proven by utilizing Proposition 1 and finding a sequence of t n that decays faster than a given rate. For a Lipschitz function f and a point y \u2208 S 2, there exists a sequence t n = n \u03b2 such that for t = n \u03b2 with \u03b2 \u2208 (-1/5, 0), the proof is concluded. Theorem 3.1 is then an immediate consequence of Proposition 3 and 1, showing that for all y \u2208 S, the sampling is drawn from a uniform random distribution. The proof relies on the uniformity properties of the distribution for sampling on the sphere. The result obtained is similar to a previous study by Belkin & Niyogi (2008). Convergence is proven for deterministic sampling with specific conditions on the kernel density. Rotation isometries and Laplacian operators are discussed for a Riemanniann manifold. Results on climate event segmentation: average precision for tropical cyclones (TC) and atmospheric rivers (AR) as positive classes. Different neural network architectures were tested, with the DeepSphere original architecture yielding the best results. The experiment with the model from Jiang et al. (2019) was rerun with a smaller batch size due to GPU memory limitations. The original DeepSphere architecture outperformed Jiang's architecture with more depth and feature maps. A wider architecture with double the feature maps was also tested. Weighted loss was implemented using scikit-learn's compute class weight function on the training set."
}