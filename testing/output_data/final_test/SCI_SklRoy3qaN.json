{
    "title": "SklRoy3qaN",
    "content": "We introduce a framework for quantifying classifier robustness to natural image perturbations in videos. We create ImageNet-Vid-Robust dataset with 22,668 images. Evaluating various classifiers trained on ImageNet, we find a median accuracy drop of 16%. Natural perturbations cause classification and localization errors, leading to a median drop in detection mAP of 14 points. Current CNNs face challenges in real-world environments requiring reliable performance. Convolutional neural networks (CNNs) face challenges in safety-critical environments due to unreliable predictions. Adversarial examples can significantly reduce accuracy without noticeable changes to input images. Researchers are exploring realistic failure modes by studying benign image perturbations, but it remains unclear if these challenges reflect real-world robustness issues. Recent research has focused on using videos as a source of natural image perturbations to improve the robustness of convolutional neural networks (CNNs). While some studies have shown only a small drop in accuracy when using video-perturbed images, the extent of the challenge posed by these perturbations remains unclear. This study aims to conduct a thorough evaluation of CNN robustness to video perturbations. In a study evaluating CNN robustness to natural perturbations in videos, ImageNet-Vid-Robust was introduced, containing 22,668 images grouped into 1,145 sets of visually similar images. Over 40 different model types were tested, revealing a median 16% accuracy drop due to small, natural perturbations. Our study on CNN robustness to natural perturbations in videos using ImageNet-Vid-Robust showed a median 16% accuracy drop for classification tasks and a median 14% drop in mAP for detection tasks. This poses a significant challenge for current CNNs, especially in safety-critical environments like autonomous vehicles. The dataset was created by extracting neighboring sets of naturally perturbed frames from ImageNet-Vid. Neighboring sets of naturally perturbed frames from ImageNet-Vid were used to create ImageNet-Vid-Robust. A dataset was curated with the help of human annotators to ensure accuracy. The curation step involves human annotators ensuring correct labeling and visual similarity between anchor and nearby frames. An expert annotator labels pairs for correctness and similarity, with steps taken to mitigate subjectivity. Reviewers mark frames as dissimilar for significant changes like motion, background, or blur. The curation step involves human annotators ensuring correct labeling and visual similarity between anchor and nearby frames. An expert annotator labels pairs for correctness and similarity, with steps taken to mitigate subjectivity. Reviewers mark frames as dissimilar for significant changes like motion, background, or blur. To increase consistency in annotation, human annotators proceed using two rounds of review, presenting only a single pair of frames at a time to reviewers. Annotators were instructed to mark a pair of images as dissimilar if a distinctive feature of the object is only visible in one of the frames. After two rounds of joint review, annotators discarded dissimilar or incorrectly labeled frames. The final dataset includes 1,145 anchor frames with varying numbers of similar frames. The study aims to assess a model's robustness to natural perturbations using valid anchor frames and their labeled sets. The study evaluates classification and detection models on ImageNet-Vid-Robust using pm-k analogues of standard metrics. Various classification models are tested with pm-k metric, analyzing per-class accuracies. Detection models are evaluated using bounding box annotations, studying errors made on adversarial examples. The study evaluates classification and detection models on ImageNet-Vid-Robust using pm-k analogues of standard metrics. In the analysis, a linear relationship between accuracy orig and accuracy pmk is observed across all models in the test bed. Frames with multiple correct classification labels are handled by considering a prediction correct if any of the labels are predicted. Each data point corresponds to a model in the test bed with confidence intervals shown. Frames for evaluation are taken from a neighborhood of adjacent frames in a 30 FPS video, allowing for scene changes of approximately 0.3s. Human review confirms visual similarity to the original frames. The study evaluates classification and detection models on ImageNet-Vid-Robust using pm-k analogues of standard metrics. A linear relationship between accuracy orig and accuracy pmk is observed across all models in the test bed. Frames with multiple correct classification labels are handled by considering a prediction correct if any of the labels are predicted. Different models types were considered, and the full table of classification accuracies is presented in the supplementary material. Leveraging the WordNet hierarchy enables evaluating models trained on the 1000 class ILSVRC challenge on images in ImageNet-Vid-Robust directly. This test set represents a substantial distribution shift from the original ILSVRC validation set. The study evaluates models on ImageNet-Vid-Robust using pm-k metrics. Models are tested with perturbations like Gaussian noise and JPEG compression to understand accuracy drops. The worst-case nature of the pm-k metric may bias evaluation towards corrupt frames. The study evaluates models on ImageNet-Vid-Robust using pm-k metrics, testing with perturbations like Gaussian noise and JPEG compression. Noise augmentation did little to improve robustness against perturbations, with the best performing robust model showing slightly smaller performance drop. Finetuning on ImageNet-VID was done to adapt to the 30 class problem and video domain. After training models on ImageNet-Vid-Robust, improvements in absolute accuracy were observed, but natural perturbation accuracy drop remained. Further analysis on whether bounding box annotations improve robustness was conducted by training a Faster R-CNN detection model with a ResNet 50 backbone on ImageNet Vid. Object detection involves classifying an object and regressing its bounding box coordinates. Object localization focuses solely on regressing the bounding box without classifying the object, which is crucial for practical and analytical purposes. This analysis aims to understand the impact of natural perturbations on object localization and detection tasks. The study analyzes the impact of natural perturbations on object detection and localization tasks using Faster R-CNN and R-FCN architectures. It shows a significant drop in mAP for object detection due to perturbed frames, while localization is easier with an increase in mAP. However, switching to the localization task does not improve the error delta, indicating that natural perturbations induce both classification and localization errors. The study shows that natural perturbations lead to errors in both classification and localization tasks. The ImageNet-Vid dataset is used, and the impact on object detection is analyzed, with a drop in mAP observed. The standard metric for detection is mean average precision, and a pm-k analog of mAP is defined. The pm-k analog of mAP is defined by replacing anchor frames with nearby frames to minimize per-image average precision. The pm-k mAP is determined by minimizing average precision across categories for each frame. The relationship between perturbed accuracy and distance is plotted, studying the effect on 30 classes in ImageNet-Vid-Robust. Recent work has proposed more realistic modifications to images for adversarial attacks, including minor rotations and translations of the input. Hyperparameters for detection models are also detailed."
}