{
    "title": "H1xAH2RqK7",
    "content": "Generative Adversarial Privacy and Fairness (GAPF) is a framework for learning private and fair representations of data by decoupling sensitive attributes. It uses adversarial learning in a minimax game to achieve privacy guarantees and demographic parity. GAPF's performance has been evaluated on various models and datasets, showing its effectiveness against complex adversaries in deep learning algorithms for data analytics. The success of deep learning algorithms for data analytics relies on large datasets containing sensitive information, leading to privacy and fairness concerns. Research is focused on developing techniques with privacy or fairness guarantees, including randomizing schemes and approaches with provable statistical privacy or fairness guarantees. In the context of privacy and fairness, challenges arise in preserving utility while providing provable guarantees. Differential privacy offers strong privacy guarantees but may reduce utility, while mutual information privacy achieves a better tradeoff but requires access to dataset statistics. Fairness concerns in machine learning stem from societal biases in datasets, necessitating the decorrelation of sensitive and non-sensitive data when publishing datasets. In the context of privacy and fairness, challenges arise in preserving utility while providing provable guarantees. Fairness concerns in machine learning stem from societal biases in datasets, requiring the decorrelation of sensitive and non-sensitive data when publishing datasets. Adversarial learning approaches leverage generative adversarial networks to achieve this decorrelation, ensuring maximal utility while approximating fairness definitions. This work introduces a general framework called generative adversarial privacy and fairness (GAPF) for creating private and fair representations of data. It provides connections to information-theoretic privacy and fairness formulations and derives optimal decorrelation schemes. The framework can be generalized to learn arbitrary representations using an encoder-decoder structure, but the focus of this paper is on learning private and fair representations of data. GAPF is a framework for creating private and fair data representations using an adversarially trained generative model. It can generate representations useful for various classification tasks without task modeling during training. Through experiments on different datasets, GAPF shows the ability to capture statistical and information-theoretic adversaries and enforce demographic parity. Comparisons are made between data-driven privacy/fairness methods and the minimax game-theoretic GAPF. The text discusses the comparison between data-driven privacy/fairness methods and the minimax game-theoretic GAPF formulation. Optimal decorrelation schemes for Gaussian mixture data are derived, and mutual information estimators are proposed to prevent adversaries from inferring sensitive attributes from the learned representation. Related work on privacy and utility guarantees in data publishing is briefly reviewed. Our approach utilizes generative models to tailor noise for privacy protection, inspired by adversarial neural cryptography and privacy-preserving adversarial networks. It aims to prevent malicious entities from accessing sensitive features while allowing non-malicious entities to learn public features from filtered data. Our model uses a GANs-like approach to learn decorrelation schemes for privacy protection, going beyond traditional methods. It formulates a game-theoretic setting with a distortion constraint to achieve private/fair representation in various learning tasks. The framework can handle different adversaries by adjusting the loss function and relies on the Penalty method or Augmented Lagrangian method for training. The approach focuses on data-driven privacy and fairness methods using adversarial models to learn representations directly from data. Unlike in-processing approaches, this method ensures fairness for various learning tasks through a pre-processing approach. GANs are used to generate synthetic non-sensitive attributes and labels to maintain fairness while preserving data utility. The study focuses on creating fair and private representations of original data by using a generative adversarial model to learn nonlinear compression and noise adding schemes. The dataset consists of entries denoted by sensitive (S), public (X), and target (Y) variables, with a focus on privacy and fairness without requiring knowledge of Y. Research on context-aware privacy focuses on how well an adversary can infer sensitive features from public data. Recent results on fairness in learning applications guarantee accurate predictions with respect to the target variable, while being unbiased towards the sensitive variable. Fairness measures include demographic parity, equalized odds, and equal opportunity. Demographic parity requires complete independence between the prediction and sensitive variable, while equalized odds ensures independence conditioned on certain factors. The decorrelation scheme allows for a randomized mapping of variables X, ensuring independence between the prediction and sensitive variable. Decision rules are used to infer the sensitive variable from the representation of the data. The generative decorrelator aims to minimize the adversary's ability to learn sensitive information from the published data representation. This is achieved by introducing a distortion function to measure the loss incurred by perturbing the original data. The data holder aims to find a decorrelation scheme that preserves privacy and utility by minimizing distortion in the original data. This leads to a minimax game between the generative decorrelator and the adversary, where the adversary seeks to minimize its expected loss. The GAPF framework allows for various adversarial models under different loss functions and decision rules. The optimal adversary decision rule under the \u03b1-loss is a '\u03b1-tilted' conditional distribution. For hard-decision adversaries, the optimal strategy is to use a MAP decision rule, while for soft-decision adversaries, the optimal strategy is to maximize the conditional distribution. The optimal adversarial strategy in the GAPF framework is to minimize mutual information between the generated data and sensitive attributes, subject to a distortion constraint. Using log-loss allows for a continuous interpolation between hard and soft decision adversaries, enforcing fairness as distortion increases. The proposed log-loss in GAPF serves as a proxy for ensuring fairness in data-driven optimization. In the context of the GAPF framework, log-loss is proposed as a means to enforce fairness by minimizing mutual information between generated data and sensitive attributes. A data-driven version of GAPF allows for learning decorrelation schemes directly from a dataset, using a generative model parameterized by \u03b8 p. The data holder competes against a computational adversary, represented by a neural network classifier, to optimize the generative model's parameters. In the GAPF framework, a neural network adversary is used to evaluate a decorrelation scheme. The generative decorrelator and adversary are modeled as deep neural networks. The adversary's empirical loss is quantified using cross entropy, which can be generalized to the multi-class case with the softmax function. The optimal model parameters are determined through minimax optimization in a two-player non-cooperative game. The minimax optimization in equation 2 involves a two-player non-cooperative game between the generative decorrelator and the adversary. An iterative algorithm is used to learn the equilibrium of the game, where the parameters of h and g are computed alternately by maximizing the adversary's loss function and minimizing the decorrelator's loss function. The distortion constraint in equation 2 sets this minimax problem apart from previous works, and the penalty method BID25 is used to incorporate the constraint by adding a penalty to the objective function. The penalty parameter \u03c1 t is multiplied by the constraint violation at each iteration in the generative decorrelator's optimization problem. The algorithm and penalty method details are in Appendix C. Performance is evaluated against a MAP adversary, deriving optimal decorrelation schemes and comparing them to data-driven schemes. The game-theoretically optimal decorrelation scheme is learned from a synthetic dataset by competing against a computational adversary. The performance is quantified by computing the accuracy of inferring S under a MAP adversary. The GAPF effectively decorrelates sensitive variables from the data, as shown by a mutual information estimator. The details of the optimal and data-driven GAPF are included in Appendix D. The generative decorrelator and adversary both have access to the data. A linear GAPF scheme is used for tractability, with additive Gaussian noise. The utility of the representation is quantified using a distortion constraint. The adversary's probability of detection is determined for GAPF schemes. The accuracy of the MAP adversary is evaluated by substituting parameters into equation 3. Numerical results show the performance of a learned GAPF scheme against a theoretical MAP adversary for a 32-dimensional Gaussian mixture model. The decorrelation scheme obtained through a data-driven approach performs well against the adversary, with a maximum accuracy difference of around 0.7% compared to the theoretical optimal. The data-driven version of GAPF can learn decorrelation schemes that perform as well as the theoretical version, with a decrease in mutual information as distortion increases. The GAPF framework is applied to real-world datasets like GENKI, which contains 16x16 greyscale face images with varying expressions, and HAR dataset with 561 motion sensor features. The HAR dataset includes 561 motion sensor features collected from 30 subjects performing six activities. Subject identity is the sensitive variable, and the dataset is split into training and test samples. The model is trained using a data-driven approach with TensorFlow. For the GENKI dataset, two decorrelator architectures are considered: FNND and TC-NND. FNND combines low-dimensional noise with the original image using a neural network, while TC-NND generates high-dimensional noise using transposed convolution neural networks. The processed image is generated by adding high-dimensional noise to each pixel of the original image using a multi-layer transposed convolution neural network. The gender classification accuracy of the adversary decreases as distortion increases, with FNND achieving lower accuracy compared to TCNND. The FNND uses both the noise vector and original image to generate the processed image, while the TCNND generates an independent noise mask added to the original image. The learned GAPF schemes show lower gender classification accuracy compared to uniform or Laplace noise, with decreased mutual information as distortion increases. Another CNN is trained for facial expression classification on datasets processed by different decorrelation schemes. Facial expression classification accuracy decreases gradually with increasing distortion. Error rates for classifiers trained on data processed by different decorrelation schemes show less bias towards sensitive variables as distortion increases. The FNND architecture performs better in enforcing fairness but has a higher error rate. The adversary's sensitive variable classification accuracy decreases as distortion increases, with a significant drop from 27% to 3.8% when distortion goes from 2 to 8. Activity classification accuracy only decreases by 18% at most even for a large distortion value of 8. The novel adversarial learning framework GAPF decreases mutual information as distortion increases. It allows for private/fair data representations with verifiable guarantees, where the decorrelation scheme is learned directly from the dataset. GAPF involves a game between a generative decorrelator and an adversary, providing guarantees against strong information-theoretic adversaries and enforcing fairness through demographic parity. Performance has been validated on Gaussian mixture models and real datasets. In this paper, the authors connect the objective function in GAPF with demographic parity to address fundamental questions about benchmarking data-driven results for large datasets. They also discuss investigating the robustness and convergence speed of decorrelation schemes learned in a data-driven fashion. Additionally, they mention designing objective functions that link to other fairness metrics such as equalized odds and equal opportunity. The authors briefly review existing approaches for publishing datasets with privacy and utility guarantees and clarify the differences in their work. The author in BID18 and BID26 propose novel approaches using minimax filters and deep auto-encoders to add differentially private noise to data. These methods aim to enhance privacy-utility tradeoffs while preventing malicious entities from accessing sensitive features. However, differential privacy (DP) may still result in a significant utility loss due to worst-case dataset statistics. Our approach introduces a generative model for randomization-based schemes to address this issue. Our work introduces a generative model for randomization-based schemes that use adversarial neural networks to learn decorrelation schemes, preventing adversaries from inferring sensitive information. Unlike previous approaches using non-generative auto-encoders, we employ a GANs-like approach to tailor noise to the dataset. Our work introduces a generative model using adversarial neural networks to learn decorrelation schemes, preventing adversaries from inferring sensitive information. The approach involves a weighted combination of loss functions to balance privacy and utility, with a game-theoretic setting subject to a distortion constraint. New methods are required to enforce the constraint during training, limiting distortion and preserving utility in learned representations. Performance comparisons are made between decorrelation schemes learned adversarially and game-theoretically optimal ones for synthetic data models. Our work introduces a generative model using adversarial neural networks to learn decorrelation schemes, preventing adversaries from inferring sensitive information. The approach involves using mutual information as a criterion to certify representations learned adversarially against attackers with varying architectures. This data-driven approach overcomes the need for dataset statistics and ensures fair representations through adversarial models. Our work focuses on using generative adversarial networks (GANs) to ensure fairness in learning tasks by generating synthetic non-sensitive attributes and labels. The goal is to develop a conditional GAN-based model that learns to create a fairer synthetic dataset through an unconstrained minimax game with carefully designed loss functions for fairness and utility. The synthetic data is generated by a conditional generative adversarial network (GAN) to preserve utility and ensure fairness by generating data that minimizes prediction of sensitive attributes. The focus is on creating fair representations of original data while maintaining utility for various learning tasks. More work is needed in this area, and the GAPF framework presented aids in achieving fairness. The GAPF framework allows for different adversarial models with various loss functions and decision rules to recover popular privacy notions. When the adversary uses hard decision rules, the estimate of the sensitive variable can be minimized using different approaches, such as squared loss functions. The GAPF framework considers squared loss functions to minimize the estimate of the sensitive variable under an adversary's optimal decision rule. For discrete variables, the adversary aims to maximize classification accuracy using a 0-1 loss function. The optimal decision rule in the GAPF framework is the maximum a posteriori probability (MAP) decision rule, providing privacy guarantees against a MAP adversary. Soft decision rules involve a distribution over S, analyzed under a log-loss function. The optimal adversarial decision rule is determined by the true conditional distribution P(s|g(X)). In the game-theoretic setting, the minimax optimization problem under the log-loss function reduces to maximizing I(g(X); S) \u2212 H(S) subject to a distortion constraint. The GAPF is equivalent to using mutual information as the privacy metric. Different loss functions capture different types of adversaries, with the \u03b1-loss function involving Arimoto conditional entropy. The minimax optimization objective can be expressed in terms of Arimoto mutual information and R\u00e9nyi entropy. The \u03b1-loss function allows for a continuous interpolation between a hard-decision adversary under 0-1 loss and a soft-decision adversary under log-loss function. It encourages probabilistic estimators by rewarding correct inferences of less likely outcomes. The objective is to train a model to predict a target variable Y from data X, forming a Markov chain S \u2192 X \u2192 X \u2192 \u0176. The Markov chain S \u2192 X \u2192 X \u2192 \u0176 is used to estimate Y in a machine learning model. The objective of the GAPF algorithm is to minimize I(S;X) under the log-loss function, ensuring fairness by minimizing an upperbound of I(S;\u0176) subject to a distortion constraint. The algorithm for learning the GAPF scheme from a dataset is presented as the alternating minimax privacy preserving algorithm in Algorithm 1. The algorithm incorporates penalty coefficients and updates lambda for convex optimization problems. The solution converges to the original constrained problem. The decorrelator's objective is to achieve optimal inference accuracy. The optimization problem is monotonically increasing in alpha. The optimization problem in equation 12 is equivalent to equation 16, with the objective function written as equation 7. The optimal scheme satisfies \u03b2 = (0, ..., 0) to minimize distortion. The Lagrangian of the optimization problem is given by equation 9, with derivatives taken to find the optimal solution \u03c3 * pi. The KKT conditions ensure dual feasibility with \u03bb * i \u2265 0. The dual feasible solution \u03bb * i \u2265 0 is obtained, leading to the conclusion that \u03bb * i > 0. The amount of noise added is proportional to |\u00b5 i |, indicating the distance between conditionally Gaussian distributions. The data-driven linear GAPF scheme involves learning the optimal decorrelation scheme through competition with a computational adversary. The learned GAPF scheme is evaluated by comparing it with a game-theoretic approach. The performance is quantified by computing the accuracy of inferring S under a strong MAP adversary. The adversary network's training loss is measured using an empirical log-loss function. The decorrelator and adversary learn optimal parameters by maximizing and minimizing specific equations, respectively. The decorrelator is modeled by a two-layer neural network with specific parameters. The neural network with parameters \u03b8 p includes mean and standard deviation values for each dimension. The adversary is modeled by a three-layer neural network classifier with leaky ReLU activations. A penalty term is added to the decorrelator's objective to incorporate the distortion constraint into the learning process. The training loss function of the decorrelator includes a penalty coefficient that increases with the number of iterations. The GAPF framework uses synthetic data from a Gaussian mixture model to evaluate performance. Two categories of datasets with different probabilities are considered. Training is done on Tensorflow with specific parameters. The FNND architecture for datasets like GENKI and HAR involves a four-layer neural network. Each image is reshaped and concatenated with random noise for processing. The study utilizes a four-layer neural network with Gaussian random noise for data processing. The noise vector is concatenated with the input, passed through fully connected layers with leaky ReLU activation, and reshaped into an image. A Gaussian random vector is linearly projected to a feature tensor in the TCNND model, followed by transposed convolution layers to generate processed data. Batch normalization is applied to hidden layers to prevent covariance shift. The study utilizes a four-layer neural network with Gaussian random noise for data processing, applying batch normalization to prevent covariance shift. An adversary model using CNNs outperforms other models for image classification, with a specific architecture detailed in Figure 9. Neurons capture subject's gender belief. Original data concatenated with Gaussian noise vector. Feed Forward neural network with 3 hidden layers used for processing. Adversary model has 5 layers with specific architecture. GAPF framework finds equilibrium in constrained optimization. The framework achieves equilibrium in constrained optimization under attacks based on a neural network. The privatized data is protected from general attacks, reducing correlation with sensitive labels. Mutual information is used to certify data protection, calculated using the nearest k-th neighbor method. The framework achieves equilibrium in constrained optimization under attacks based on a neural network. Mutual information is used to certify data protection, calculated using the nearest k-th neighbor method. Empirical MI is simplified to estimate entropy using a neural network for dimension reduction. The MI between the learned representation X and the label Y is approximated by \u00ce(X f ; Y ) using a CNN with two conv blocks and two fully connected layers. The conv layers have filters of size 3 \u00d7 3, max-pooling with stride 2, and ReLU activation. The output is a 2-dimensional vector after passing through batch normalization and ReLU activation. The feature embedding vector Xf is obtained through a CNN with two conv blocks and two fully connected layers. To estimate mutual information for a HAR dataset with a large label size, 128 neurons are used before the final softmax output layer. Due to the curse of dimensionality, PCA is applied to reduce the dimensionality to 12 for calculating mutual information. The 12-dimensional vector obtained through PCA encapsulates major information of the processed data."
}