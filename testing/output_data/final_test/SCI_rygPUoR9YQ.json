{
    "title": "rygPUoR9YQ",
    "content": "Generative Adversarial Networks (GANs) can create realistic images by capturing complex interactions between objects in a scene. A new model, called a self-consistent composition-decomposition network, is proposed to address this challenge. The model is conditioned on object images and can generate realistic joint scenes. Experimental results show the model's effectiveness in generating realistic images. The learned model captures interactions between object domains to generate new scenes at test time. Generative Adversarial Networks (GANs) can generate images based on input cues like images, text phrases, or class labels. The goal is to translate samples from a source distribution to samples from an output distribution, involving transformations of objects or image styles. However, these transformations may not fully capture the complexity of natural images. In this work, the focus is on exploring compositionality in learning a function that maps images of different objects into a combined sample that captures their joint distribution. The challenge lies in the complex interactions among objects in natural images, such as scaling, layout, occlusion, and viewpoint transformation. Previous work using spatial transformer networks within a GAN framework decomposes this problem by finding geometric modifications for foreground objects, but is limited to fixed backgrounds and does not consider more complex interactions. Our novel approach focuses on modeling object compositionality in images by generating joint interactions between two input object images. The model needs to understand occlusion ordering and spatial layout to accurately capture the composition without prior explicit information. Our approach aims to solve the problem of object composition in images by generating joint interactions between two object images without prior explicit information. We use a composition-decomposition network to create a combined image representing object interaction and then decompose it to obtain individual objects. In scenarios where paired examples are not available, we incorporate an inpainting network to handle unpaired cases. Our proposed Compositional-GAN approach addresses object composition in images by generating joint interactions between two objects. It is evaluated in paired and unpaired training scenarios, showing effectiveness in image generation and representation learning using GANs. Conditional GANs are used for image to image translation and inpainting missing regions. Image composition is a challenging problem in computer graphics, where objects from different images are overlayed in one single image. BID35 addressed this by training a discriminator network to distinguish realistic composite images from synthetic ones. BID27 developed a deep CNN for image harmonization to capture context and semantic information, outperforming previous models. BID14 used spatial transformer networks for image processing. In image composition, BID14 used spatial transformer networks for geometric corrections, while BID10 computed scene layouts from scene graphs. However, these approaches lack realism without explicit scene layout information. To address this, a generative network is proposed to handle scaling, layout, occlusion, and viewpoint transformation for composing realistic composite images. The proposed approach aims to generate realistic composite images by using a conditional generative adversarial network. It can handle scaling, layout, occlusion, and viewpoint transformation for composing images from different sets. The goal is to output the mode of the distribution, focusing on learning plausible compositions. The approach focuses on learning plausible compositions using a conditional generative adversarial network. It involves handling viewpoint and affine transformations to make the generator invariant. In conditional GAN models, auxiliary information is fed alongside the noise vector to generate images of a specific distribution. In a proposed compositional GAN model, two input images are concatenated channel-wise to generate an image from a target distribution. Real samples of these distributions are used during training, with dropout being the only source of randomness in the network. The relative appearance flow network BID34, GRAFN, is trained on paired or unpaired inputs to generate a new viewpoint of an object based on the viewpoint of another object encoded in its binary mask. The network is trained on images with different azimuth angles and their target images in new viewpoints, along with foreground masks. The network architecture and loss function are detailed in the appendix. The GRAFN network is an encoder-decoder network that predicts appearance flow vectors to generate a synthesized view. It shares weights with the encoder of the encoder-decoder mask generating network, GMRAFN, which predicts foreground masks. The model G composes two objects with corresponding real images in the training set. Additionally, a spatial transformer network is used to translate center-oriented input objects. Our model, BID9, utilizes a spatial transformer network to process two RGB images simultaneously for compositional GAN training. The model includes a RAFN step for synthesizing new viewpoints with paired data and inpainting for unpaired data. The backbone consists of a self-consistent composition-decomposition network for conditional generative adversarial networks. The model processes two RGB images together in a batch to generate a composite image, which is then decomposed using two different networks. The decomposition components share weights in the encoder but differ in the decoder. Ground-truth foreground masks are assumed available, and a GAN loss with gradient penalty is applied to ensure realistic generated images. Multiple L1 loss functions penalize deviations from the ground truth. The model utilizes multiple L1 loss functions to penalize deviations from ground truth images. A schematic of the full network is shown in FIG2, with a loss function summarized as DISPLAYFORM0. The self-consistency constraint penalizes deviation of decomposed images, and cross entropy loss is applied on predicted probability segmentation masks. Gradient penalty is added to improve GAN loss function convergence. Viewpoint transformation can be omitted if not needed. The benefit of decomposition networks is discussed in Section 3.5. In a broader object domain without paired inputs-outputs, segmentation masks are used to crop and resize object segments for training. A self-supervised inpainting network is added to the compositional GAN model to generate full objects from given segments, improving object occlusions and spatial layouts accuracy. In a broader object domain, segmentation masks are utilized to crop and resize object segments for training. A self-supervised inpainting network is integrated into the compositional GAN model to enhance object occlusions and spatial layouts accuracy. This involves applying random masks on images to fill in missing regions using conditional GANs. Training is done on sets X and Y to generate full objects from segments, with the use of foreground masks for guidance. Additionally, a spatial transformer network is trained for paired data, followed by composition-decomposition networks. The composition-decomposition networks are used to generate composite images and probability segmentation masks for paired data. The RAFN model is pre-trained and utilized at test time. Performance is evaluated on new images from sets X and Y, with foreground masks, to create natural-looking composite images. Network parameters are optimized to remove artifacts and improve results. Ground-truth for the composite image and object layout is not available at test time. The self-consistency cycle in the decomposition network provides supervision for penalizing deviation from original objects through an L1 loss. The weights of certain networks are frozen, and only the composition-decomposition layers are refined with GAN loss applied to real samples. Foreground masks of input instances are used, and predicted masks are multiplied with object foreground masks to eliminate artifacts outside the target region. Ground-truth masks can be obtained using pre-trained models. The refinement process involves using foreground masks to eliminate artifacts outside the target region for objects. Test results on composition tasks with paired or unpaired data are shown, with outputs before and after refinement. Generated decomposed images are compared with transposed inputs in the experiments. In this section, the performance of a compositional GAN model is studied for paired and unpaired scenarios using the Shapenet dataset BID2 and CelebA dataset BID17. The model generates images based on predicted segmentation masks and can handle tasks like placing a chair next to a table or a bottle in a basket. The training hyper-parameters are set to specific values for the experiments. The training hyper-parameters for the compositional GAN model are \u03bb 1 = 100, \u03bb 2 = 50, \u03bb 3 = 1, and \u03bb = 100. Composing a chair and a table involves challenges such as similar viewpoints and partial occlusion. The model learns to transform and compose the objects by feeding them simultaneously. A collection of 1K composite images from Shapenet chairs and tables is manually created for training. Paired training includes pairing information between composite images and their full chair and table, while unpaired training ignores pairing information. The model's performance is evaluated on unpaired examples by using different subsets of Shapenet chairs and tables. Chairs and tables in input-output sets can pose at random azimuth angles. The trained relative appearance flow network synthesizes chairs consistent with tables. Model outputs are visualized at different steps to evaluate network components. The study evaluates the network components by visualizing model outputs at different steps. The nearest neighbor composite example in the training set is found for a new input chair and table. The network is shown to not memorize its training data. The output of the network before and after the inference refinement step is illustrated, showing comparable results for both paired and unpaired training models. The study compares the performance of a model trained on unpaired data with paired data, showing successful resolution of composition challenges. Exemplar images and failure cases are presented, along with results from an Amazon Mechanical Turk evaluation. In a study comparing models trained on paired and unpaired data using 90 test images of chairs and tables, 57% preferred the composite images from the paired model. Results also showed that images generated after an inference refinement step were preferred 71.3% of the time, indicating the benefit of the refinement module in producing higher-quality images. Additionally, a compositional task involving putting a bottle in a basket was addressed. The study involved training models on paired and unpaired data using test images of chairs and tables. Results showed a preference for composite images from the paired model and the benefit of an inference refinement step. A compositional task with a bottle in a basket was also addressed, highlighting the importance of an inpainting network for handling occlusions. The study evaluated model performance on a face-sunglasses composition task using paired and unpaired training data. Results showed that paired training outputs were preferred 57% of the time, with the refinement module proving useful in 64% of examples. The study confirmed the benefit of the refinement module and comparable performance between paired and unpaired training scenarios. In a study on face-sunglasses composition tasks, experiments were conducted with model components removed to analyze their impact on composite images. Poor performance of CycleGAN and Pix2Pix models was demonstrated in a challenging composition task. Composite images of celebrity faces with sunglasses were created using the CelebA dataset, both in paired and unpaired training scenarios. In a study on face-sunglasses composition tasks, experiments were conducted with model components removed to analyze their impact on composite images. The results show that the model outperformed the ST-GAN model in terms of realism with only 180 paired or unpaired training images. Additional images are provided in Appendix C.5, and a study with 60 evaluators confirmed the superior performance of the model. Our model outperformed ST-GAN in face-sunglasses composition tasks, with 84% favoring our network predictions over ST-GAN. Additionally, 73% preferred our unpaired model over ST-GAN. These results confirm the generalization ability of our model and its superiority in composing faces with sunglasses compared to ST-GAN. Our Compositional GAN model addresses object composition in conditional image generation by capturing linear transformations and viewpoint adjustments for realistic joint image generation. The compositional GAN model successfully solves the compositionality problem without prior object layout information. It was evaluated through qualitative experiments and user evaluations with paired and unpaired training data. Future work aims to generate images with multiple non-rigid objects. The architecture includes an appearance flow network with encoder-decoder convolutional layers for predicting appearance flow vectors and synthesizing views. The model also generates foreground masks using batch normalization and ReLU activation layers. The relative spatial transformer network utilizes BID7 and ReLU activation layers in the decoders. The network generates affine transformations for two input images, learning their relative transformations simultaneously. Orange feature maps represent conv2d outputs, yellow maps are from max-pool2d with ReLU, and blue layers denote fully connected layers. The relative spatial transformer network uses BID7 and ReLU activation layers in the decoders to generate affine transformations for input images. Figure 6 and FIG8 show test results for paired and unpaired training models, focusing on viewpoint, linear transformations, and occluding object regions. The main challenge in the bottle-basket composition task is the relative scale of objects and partial occlusions. In Figure 8, test examples are visualized to study model performance before and after inference refinement in paired and unpaired scenarios. Nearest neighbor training examples are shown for new input pairs, highlighting the importance of the inpainting network in training with unpaired data. Experiments on composing a bottle with a basket reveal the effects of removing different components of the model. Qualitative results are illustrated in FIG9. In Figure 8, test results on the basket-bottle composition task are shown with different scenarios: no reconstruction loss leads to wrong color and faulty occlusion, no cross-entropy mask loss results in faded bottles, no GAN loss generates lower quality outputs, and missing bottles occur without decomposition generator. Paired and unpaired models are compared, with \"NN\" representing nearest neighbor images and \"NoInpaint\" showing unpaired model results. Outputs before and after inference refinement are displayed. The purpose of the model is to capture object interactions in 3D space projected onto a 2D image by handling spatial layout, scaling, occlusion, and viewpoint. It distinguishes from CycleGAN and Pix2Pix models by focusing on realistic image generation. The model is trained using a ResNet generator with adversarial loss and L1 regularizer on paired training data. The comparison is made with these two models using mean scaling and translating parameters from the training set. The ResNet model outperforms U-Net in generating images, but still produces unrealistic results. CycleGAN is used for unpaired data. Both Pix2Pix and CycleGAN struggle with transforming input samples to occluded outputs. Adding sunglasses to a face image requires proper alignment. Test examples show results of paired and unpaired training scenarios. ST-GAN model outputs are also included."
}