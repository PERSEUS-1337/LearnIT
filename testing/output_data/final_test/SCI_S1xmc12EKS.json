{
    "title": "S1xmc12EKS",
    "content": "The neural linear model is a Bayesian regression method that has been successful in various applications. It is tested on UCI datasets and ''gap'' datasets, showing competitive performance. Neural networks can be overconfident in predictions, but Bayesian neural networks aim to address this issue. The neural linear model is a compromise for Bayesian neural networks in regression settings, focusing on exact inference on the last layer for closed-form predictions. It has been applied in active learning, Bayesian optimization, reinforcement learning, and AutoML. In this work, the authors benchmark the neural linear model in a simple regression setting, using toy examples and UCI datasets. They train variations of the model where a neural network extracts features for Bayesian linear regression. The focus is on training the network effectively, with three different models proposed. The authors propose three different models for training neural networks, including the maximum a posteriori neural linear model (MAP-L NL). They use MAP estimation followed by Bayesian linear regression with slice sampling to prevent overfitting. Hyperparameters are tuned using Bayesian optimization. The authors propose a method to address uncertainty quantification issues in the MAP estimation by optimizing the marginal likelihood with respect to network weights. This approach, called regularized neural linear (Reg-L NL), reduces overfitting and is computationally efficient. The authors introduce the regularized neural linear (Reg-L NL) model, which marginalizes out noise and prior variances using slice sampling. They tune hyperparameters through Bayesian optimization. Another model, the Bayesian noise (BN) neural linear model, integrates noise variance marginalization into the model itself by using a normal-inverse-gamma prior on weights and noise variance. This approach acts as a regularizer and avoids the need for Bayesian optimization, but may risk overfitting. Training involves maximizing the marginal likelihood. The authors introduce the Bayesian noise (BN) neural linear model, which maximizes the marginal likelihood for all parameters and tunes prior parameters with Bayesian optimization. They compare this model with the regularized neural linear (Reg-L NL) model on various datasets. The authors present a synthetic dataset with 100 train and test pairs, where x is sampled in the range [-4, -2] \u222a [2, 4] and y is generated as y = x^3 + N(0, 9). Different models are compared on predictive distributions, with the BN(BO)-2 NL model showing promising results. Results on UCI datasets are also provided in the study. The study reports average test log likelihoods and RMSEs for various models, with BN(ML)-2 NL and BN(BO)-2 NL models showing the best performance. These models achieve state-of-the-art results on 'energy' and 'naval' datasets compared to other BNN-based methods. The Bayesian treatment of noise variance in BN-L NL models outperforms the Reg-L NL model. The study evaluates the performance of various models on UCI \"gap\" datasets, showing catastrophic failure in expressing uncertainty for some datasets. The BN(BO)-2 NL model performs the best among all models tested. The study used hyperparameter tuning for most models to obtain results, but it may not be desirable for practitioners seeking simplicity. The effect of hyperparameter tuning on model performance was investigated, showing significant differences in test log likelihoods and RMSEs for two-layer models on UCI datasets. The study found that omitting hyperparameter tuning worsened model performance, with significant differences in test log likelihoods and RMSEs for two-layer models on UCI datasets. Results extended to UCI gap datasets, showing statistically significant differences in performance for most models. MFVI and MCD achieved reasonable performance without tuning, competitive with tuned NL models but suffered from pathologies on gap datasets. Benchmark results for neural linear models in regression setting were presented. The neural linear model in the regression setting shows good performance despite simplicity, not as susceptible to gap uncertainty as MFVI or MCD. Extensive hyperparameter tuning is often required for reasonable performance. Exact inference on a subset of parameters may perform better than approximate inference on the entire set for BNNs. Further investigation into this issue is warranted. In the context of neural linear models in regression, exact inference on a subset of parameters may outperform approximate inference on the entire set for Bayesian Neural Networks (BNNs). This approach treats the network's parameters probabilistically, with the advantage of being tractable under certain assumptions. The model is described mathematically using training data and the outputs of the last hidden layer parameterized by weights and biases. Bayesian inference for neural linear models involves defining the model with weights and biases, using Bayesian linear regression for prediction, and learning the parameters through maximum a posteriori estimation. The regularization parameter \u03b3 is crucial for training the network effectively. To address the question of setting \u03b1, slice sampling is used to marginalize \u03b1 and \u03c3 2 based on the log marginal likelihood of the data. Bayesian optimization is employed to determine a suitable value of \u03b3, learning rates, and epochs. One drawback is the separation of feature learning from prediction, leading to potential issues with out-of-distribution prediction and uncertainty estimates. The neural linear model can be viewed as a Gaussian process model with a covariance kernel determined by basis functions \u03c6 \u03b8,i and hyperparameters \u03b8. The goal is to maximize the log marginal likelihood of the data with respect to \u03b8 and \u03c3. The computational complexity of maximizing the log marginal likelihood in an empirical Bayes approach is reduced to O(N + M^3) by applying the Woodbury identity. However, overfitting may occur due to a large number of hyperparameters, pushing the noise variance towards zero. Regularization schemes can help address this issue. Regularizing \u03b8 alone via L2 regularization is the most promising approach in addressing overfitting in Bayesian linear regression. This Type-2 MAP approach divides \u03b8 into weights \u03b8 W and biases \u03b8 b with regularization hyperparameters \u03b3 W and \u03b3 b. An alternative is to integrate the noise variance in a Bayesian manner, which is tractable with a normal-inverse-gamma prior. The posterior predictive is modeled using a Student's t-distribution. Empirical Bayes is used for training the network, with the marginal likelihood computed using the Woodbury identity. The marginal likelihood is computed using the Woodbury identity with neural networks tested having ReLU activation and one or two 50-unit hidden layers. A validation set is used, with its size set to one fifth of the training set size. For the MAP baseline, a batch size of 32 is selected, and Bayesian optimization is used to optimize hyperparameters including regularization parameter, learning rates, and number of epochs. The hyperparameters for the neural linear model are optimized in log space, including the prior variance, learning rates, and number of epochs. The network is optimized using ADAM. For the MAP neural linear model, slice samples are obtained for Bayesian linear regression. The regularized NL model tunes five hyperparameters via Bayesian optimization. The regularized NL model optimizes five hyperparameters through Bayesian optimization: \u03b3 W , \u03b3 b , learning rates for \u03b8 and \u03c3 2 , and number of epochs. Parameters are initialized and marginalized out after optimization. Bayesian noise NL (ML) optimizes \u03b8, a 0 , b 0 , \u03b1 W , and \u03b1 b via log marginal likelihood, with early stopping and grid search for learning rates. The Bayesian optimization algorithm used in the study optimizes hyperparameters by maximizing validation log likelihood over a grid of learning rates. Parameters are initialized and marginalized out using slice sampling. The approach follows a Gaussian process formulation and is based on previous work by Snoek et al. (2012). The study follows a Gaussian process formulation for Bayesian optimization, optimizing hyperparameters by maximizing validation log likelihood. Hyperparameters are learned using ADAM with 5000 iterations and slice sampling. Expected improvement acquisition function is used to select network hyperparameters. A total of 50 iterations of Bayesian optimization are conducted for each model, with 10 iterations of random search as initialization. In Table 1, models are summarized with optimization objectives and parameters tuned using Bayesian optimization and slice sampling. Results from the main text are provided, along with the effect of slice sampling on the models. Average test log likelihoods and RMSEs for UCI datasets are presented in subsequent tables. Average ranks of models across UCI datasets splits are also computed. The study evaluates different models using UCI datasets and follows the Friedman test procedure. The two-layer marginal likelihood-based methods generally outperform others, with the BN(BO)-2 NL model performing the best. Single-layer marginal-likelihood based methods follow, while MAP-based methods perform the worst. This confirms the expectation that more Bayesian models would perform better. The study evaluates various models using UCI datasets and follows the Friedman test procedure. Overfitting is observed in all models, especially on certain datasets. The trend suggests that increasing the number of layers may worsen overfitting, limiting the use of neural linear models to smaller networks. Test log likelihoods for UCI gap datasets are shown in Tables 6 and 7, focusing on capturing in-between uncertainty rather than overall performance. The study evaluates various models using UCI datasets and follows the Friedman test procedure. Overfitting is observed in all models, especially on certain datasets. The trend suggests that increasing the number of layers may worsen overfitting, limiting the use of neural linear models to smaller networks. Test log likelihoods for UCI gap datasets are shown in Tables 6 and 7, focusing on capturing in-between uncertainty rather than overall performance. The MAP-L, MAP-L NL, and Reg-L NL models fail catastrophically on the 'naval' and 'energy' dataset. MAP-1 performs poorly on 'yacht', while BN NL models perform poorly on 'naval' and 'energy' but do not fail catastrophically. The more fully Bayesian models perform better, highlighting the poor performance of Reg-L NL models. The setup for experiments on hyperparameter tuning includes selecting \"reasonable\" values such as batch size, prior variance, learning rates, and gradient steps for different models like MAP, MAP NL, and Regularized NL. The experiments on hyperparameter tuning involved setting specific values for parameters like batch size, prior variance, learning rates, and gradient steps for different models. The results showed that hyperparameter tuning is crucial, especially for two-layer cases, as models without tuning performed significantly worse. The results of models without hyperparameter tuning are significantly worse, especially for two-layer methods, with some datasets showing catastrophically bad results. However, the gap datasets did not show catastrophic differences. Models unable to represent in-between uncertainty failed catastrophically on more datasets. Statistical analysis confirmed the significant impact of hyperparameter tuning on model performance. The study shows statistically significant differences in model performance on standard and gap datasets, measured by log likelihoods and RMSEs. Results for models without hyperparameter tuning were significantly worse, especially for two-layer methods, with some datasets showing catastrophic results. The impact of hyperparameter tuning on model performance was confirmed through statistical analysis. For mean field variational inference (MFVI) and Monte Carlo dropout (MCD) without hyperparameter tuning, MFVI is implemented with a unity prior variance and ADAM optimizer. Approximately 25000 gradient steps are used with a batch size of 32. Gradients are estimated using 10 samples from the posterior. For MCD, a dropout rate of p = 0.05 is set with weight decay. ADAM optimizer is used with 25000 gradient steps and a batch size of 32. Testing is done with 100 samples. The test log likelihoods and RMSEs for one-and two-layer architectures in UCI datasets show reasonable values without hyperparameter tuning. Results for MFVI are competitive with neural linear models using hyperparameter tuning, indicating less overfitting. Regularizing weights is crucial for neural linear models, while being approximately Bayesian over all weights is less sensitive to hyperparameters. The results show that MFVI performs well in terms of test log likelihood and RMSE on UCI datasets, with less overfitting compared to MCD. Average ranks of single-run models for different datasets are provided, indicating MFVI's competitive performance. The text chunk discusses the test RMSEs on the UCI Gap Datasets for MFVI and Monte Carlo Dropout, highlighting that neural linear models fail on some datasets. However, MFVI and MCD are considered better for standard UCI datasets as they perform well without hyperparameter tuning. Average test log likelihoods and test RMSEs for MFVI and MCD on the UCI gap datasets are tabulated in Tables 15 and 16. The text discusses the failure of MFVI and MCD to express 'in-between' uncertainty on the 'energy' and 'naval' datasets. Neural linear models outperform MFVI and MCD in this aspect. The effect of slice sampling on model performance is briefly investigated with plots of predictive posterior distributions shown in Figure 10. The text investigates the impact of slice sampling on model performance by comparing different models using Figure 10. The differences in log likelihoods and RMSEs between models with and without slice sampling are plotted in Figures 11 and 12 for UCI datasets and UCI gap datasets, respectively. The results suggest that the effect of slice sampling on the toy problem is minimal. The study compares the impact of slice sampling on model performance, showing mixed results depending on the model and dataset. Majority of models benefit from slice sampling in terms of log likelihoods, but not always in terms of RMSE. Some models may even worsen with slice sampling. Overall, performance is mostly not worsened by slice sampling, with MAP-L NL models appearing to benefit the most. Slice sampling has mixed results on model performance. MAP-L NL models benefit from it, while Reg-L NL models are negatively affected. BN(BO)-2 NL models may also be harmed in cases of in-between uncertainty."
}