{
    "title": "B1xHUiC5tm",
    "content": "In the domain of time-series forecasting, predictive models utilize relations between past and future values. Exogenous features can also impact stationary time-series forecasting. A new approach involves learning these features in embedding vectors within recurrent neural networks. This method was applied to forecast smart cards tap-in logs in the Parisian subway network, showing improved forecasting performance compared to classical statistical methods. In time-series forecasting, models like ARIMA and deep learning methods like RNNs, ConvLSTM, and Graph Neural Networks are used to capture temporal dynamics. However, these models do not consider exogenous factors, such as road type or seasonality, which can significantly impact predictions. These factors, referred to as contextual information, are crucial for accurate forecasting. In time-series forecasting, models like ARIMA and deep learning methods like RNNs, ConvLSTM, and Graph Neural Networks are used to capture temporal dynamics. However, these models do not consider exogenous factors, such as road type or seasonality, which can significantly impact predictions. Contextual information, crucial for accurate forecasting, is utilized in constructing baselines based on past values and introducing context-embedded RNN for multivariate stationary time-series. This approach incorporates spatial context like subway stations and temporal context through day of the week and time of the day for forecasting. In time-series forecasting, contextual models outperform other recurrent models and historical average baselines, especially for stationary time-series. The experiments were conducted using data from Ile-de-France Mobilit\u00e9s, and the source code is available on GitHub. Contextual models excel in long-term forecasting by incorporating contextual information within RNNs. The paper proposes a new paradigm for learning contextual information within RNNs to improve forecasting performance. It discusses the background in time-series forecasting, introduces proposed models, and presents prediction experiments. Classical methods rely on ARMA models and their variants, while RNNs are used for modeling complex data like spatio-temporal data. The text discusses the use of Graph Neural Networks for traffic forecasting, focusing on learning peculiarities of each location instead of relations between them. Contextual features are used to improve forecasting performance, with examples like the KARIMA algorithm and the use of additional temporal features in LSTM and gradient boosting models. In recent studies, models like BID18 and BID5 incorporate temporal features in LSTM and gradient boosting decision trees. NARX models like BID6 and BID8 use exogenous features to forecast groundwater levels and building heat load. BID10 uses transition matrices for spatial and temporal information in predicting the next location. Public transportation data is less studied compared to traffic forecasting, with BID19 using SVM and Kalman filters for bus arrival time prediction and BID1 relying on historical averages for tap-in and tap-out. The data for the transportation problem involves a 3D tensor representing the number of passengers entering the transportation network over days, subway stations, and time-steps. The data shows discontinuity due to nightly subway closures, forming multivariate time-series observations. Recurrent neural networks are used for forecasting time-series data in transportation, encoding latent states for each time step to predict the next value. These models can adapt to anomalous behaviors and utilize past observations. The architectures include hidden states for each time step, embeddings for spatial location, day of the week, and time-step. Three recurrent architectures are proposed to handle spatial context in anomalous behaviors. Each architecture consists of a recurrent encoder E transforming observations into hidden states, decoded into predictions using a linear layer D. The models can incorporate temporal context. Univariate RNN trains distinct RNNs for each station, while Multivariate RNN considers each sample as a single day over the network. The proposed recurrent architectures capture the dynamics and correlation of the subway network by combining past and current information into a synthetic state. The model does not explicitly specify spatial context but includes it in the network weights, offering a natural way to handle multivariate series. The proposed hybrid architecture, Spatial RNN, combines univariate and multivariate models to address overfitting in learning relations between stations. It includes a shared couple (E, D) for station correlations, reducing the number of weights. Spatial context is explicitly learned through spatial embeddings, enhancing the model's performance. The Spatial RNN architecture combines univariate and multivariate models to address overfitting in learning station relations. It uses spatial embeddings to encode observations for each station, allowing for scalability to a network of thousands of stations while reducing dimensionality. The Spatial RNN architecture uses spatial embeddings to reduce dimensionality and improve performance by incorporating temporal context in the models. Temporal embeddings are shared across all networks to learn meaningful representations of temporal entities. The Univariate architecture focuses on designing multivariate and univariate architectures for days and time-steps, using embeddings for day of the week and time-step. The number of logs is influenced by the time of day, with morning and evening peak hours. Prediction in the Spatial case is shown in Equation 5. The Spatial model with day and time embeddings is used for computing predictions for a specific station s. The embeddings for station, day, and time are concatenated with the observed value at each time step. A recurrent encoder processes this vector to compute a hidden state, which is then decoded into a single prediction. The models are trained on a dataset from Ile-de-France Mobilites containing 256,028,548 logs. The dataset from Ile-de-France Mobilites contains 256,028,548 logs from October to December 2015 across 300 subway stations. 3 stations undergoing works and 15 days with traffic anomalies were removed. The remaining 297 stations and 77 days were split into 70% train, 15% validation, and 30% test samples, with stratification by day of the week. The transportation network has few hubs and many less crowded stations, complicating predictions. To tackle the prediction problem with the dataset from Ile-de-France Mobilites, each station is rescaled separately by calculating the 99.9th percentile and applying min-max scaling to values between -1 and 1. This approach removes outliers and prevents more important stations from overshadowing minor ones. Models like vanilla RNN and Gated Recurrent Units networks are trained using pytorch on GPU with the Adam optimizer. The models are trained with pytorch BID11 on GPU using Adam optimizer with a learning rate of 0.0001 and Mean Squared Error. Hyperparameters are presented in TAB1 with \u03bb s = 80, \u03bb d = 4 and \u03bb t = 30 for embeddings' sizes. Experiments are run with 5 different random seeds to compute standard deviation of the error. A strong baseline is constructed by averaging previous values given their context. BID12 propose a Bayesian network for tap-in logs forecasting but performs slightly worse than the average baseline due to the stationary nature of the series. The baseline model is a tensor of predictions of size 7\u00d7S \u00d7T, where the first dimension corresponds to each day of the week. The average baseline model, based on domain expert knowledge and contextual information, is not able to adapt to anomalies but is context aware. Recurrent models using RNN or GRU significantly outperform the baseline, learning the dynamics of the time-series and being robust to unseen values. An anomaly is observed on November 4th, where the recurrent models fit well to the unusually low traffic, indicating their ability to capture the time-series dynamics. The study combines recurrent models with temporal contextual information to improve predictions. Day and time embeddings are learned within the models, showing benefits for all models except for Multivariate and Univariate GRU. The combination of time and day embeddings outperforms time embeddings alone for some architectures. The Spatial model benefits more from day embeddings. Previous experiments focused on predicting one value per observation. The study combines recurrent models with temporal contextual information to improve predictions. Experiments focused on predicting one value per observation, but now aim to predict values in wider time windows. Adding temporal embeddings improves prediction quality, with augmented models showing slower degradation in predictions over time. Temporal context is especially useful for extending the horizon of accurate predictions. Temporal context is crucial for accurate long-term predictions, as shown by experiments with recurrent models. The Day & Time Spatial GRU model, incorporating spatial and temporal context, performs as well as the baseline with limited information. The historical mean emerges as the best estimator for long-term sequences, highlighting the importance of temporal context in prediction accuracy. The Day & Time Spatial GRU model outperforms the baseline in predicting disruptions in transportation logs, even with limited information. Contextual models can detect disruptions early on and provide competitive long-term predictions, showcasing the importance of temporal context in forecasting accuracy. The proposed framework for forecasting in the subway shows significant improvement in prediction error with spatial and temporal context. It performs well in one-step ahead prediction and remains competitive in long-term forecasting. Robust recurrent models can accurately predict traffic recovery in case of anomalies, helping users adapt their behavior. Predictions for the test set are computed using the first 16 values of each day until 8AM, showing the average RMSE difference between the baseline and proposed models for every time-step."
}