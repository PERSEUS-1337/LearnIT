{
    "title": "HJxyZkBKDr",
    "content": "Neural architecture search (NAS) has seen significant success in various applications in recent years. Neural architecture search (NAS) has seen significant success in various applications in recent years. It could be time to analyze the good and bad aspects in the field of NAS. NAS-Bench-201 is an extension to NAS-Bench-101 with a fixed search space, results on multiple datasets, and more diagnostic information. The search space design is inspired by cell-based searching algorithms, where a cell is represented as a directed acyclic graph with edges associated with predefined operations. NAS-Bench-201 provides a unified benchmark for up-to-date NAS algorithms. The NAS-Bench-201 provides a unified benchmark for NAS algorithms with a fixed search space containing 15,625 neural cell candidates. It includes training logs and performance data for architecture candidates on three datasets, saving time and improving efficiency for researchers. Additional diagnostic information like fine-grained loss and accuracy is also provided to inspire new NAS algorithm designs. The deep learning community is shifting from manual to automatic neural architecture design. Recent NAS algorithms are benchmarked against NAS-Bench-102, showing its effectiveness. Automated architectures found by NAS algorithms require less human interaction compared to manually designed ones like ResNet and VGGNet. Automated architectures found by NAS algorithms require less human interaction and expert effort. These architectures have shown promising results in various domains such as image recognition and sequence modeling. Different NAS methods utilize various search spaces and strategies for training and performance reporting. NAS-Bench-201 is proposed to address comparability issues in evaluating NAS algorithms, inspired by NAS-Bench-101 and NAS-HPO-Bench. It offers a fixed cell search space for better reproducibility of NAS methods. The search space in NAS-Bench-201 is based on neural cell-based searching algorithms, with each architecture consisting of a predefined skeleton and a searched cell represented as a DAG. The search space size is determined by the number of nodes and operation candidates, resulting in 15,625 cells/architectures. Each architecture is trained multiple times on different datasets for evaluation. NAS-Bench-201 offers a unified benchmark for NAS algorithms, including cell-based methods, providing training logs and performance metrics for each architecture. It aims to streamline the search process by eliminating repetitive training and hyper-parameter tuning, allowing researchers to focus on developing robust searching algorithms. NAS-Bench-201 provides a computational power friendly environment for NAS, decreases validation time, evaluates model transferability, and offers systematic analysis of the search space for various NAS algorithms. It includes a thorough evaluation of 10 recent advanced NAS algorithms, making it applicable to almost any up-to-date NAS algorithm. NAS-Bench-201 introduces a search space inspired by cell-based NAS algorithms, evaluating architectures on three datasets and providing diagnostic information for future NAS algorithm designs. The search space includes a macro skeleton with a 3x3 convolution and batch normalization layer, followed by three stacks of cells connected by a residual block. The search space in NAS-Bench-201 includes a macro skeleton with a 3x3 convolution and batch normalization layer, followed by three stacks of cells connected by a residual block. Each cell is stacked N = 5 times, with different output channels for each stage. The intermediate residual block downsamples the spatial size and doubles the channels of the input feature map. The shortcut path consists of average pooling and convolution layers. Classification involves a global average pooling layer and a fully connected layer with softmax for final prediction. Each cell in the search space is represented as a densely connected DAG. The DAG in NAS-Bench-201 represents operations transforming feature maps between nodes in a complete graph. The predefined operation set includes zeroize, skip connection, 1x1 convolution, 3x3 convolution, and 3x3 average pooling. Each node sums feature maps from incoming edges, allowing for basic residual block-like cells with 4 nodes. The search space is not restricted by a densely connected DAG topology. The search space in NAS algorithms is not limited by a densely connected DAG topology, allowing for various architectures to be explored. Architectures are trained and evaluated on popular datasets like CIFAR-10, CIFAR-100, and ImageNet-16-120, with a focus on consistent training and evaluation settings. The CIFAR-10 dataset consists of 60K 32\u00d732 color images in 10 classes, with 50K images in the training set and 10K images in the test set. To ensure fair comparisons in NAS algorithms, the training images are split into two groups of 25K images each for validation purposes. The CIFAR-10 dataset is split into two groups of 25K images each for validation. The CIFAR-100 dataset categorizes images into 100 classes. ImageNet-16-120 is built from the down-sampled variant of ImageNet. In this study, ImageNet-16-120 was created by down-sampling the original ImageNet to 16x16 pixels and selecting images with labels within the range of 1 to 120. The dataset consists of 151.7K training images, 3K validation images, and 3K test images across 120 classes. The architectures were trained using Nesterov momentum SGD with cross-entropy loss and a set of hyperparameters denoted as H. We train different architectures using Nesterov momentum SGD with cross-entropy loss for 200 epochs. Weight decay is set at 0.0005 and the learning rate decays from 0.1 to 0 using cosine annealing. Data augmentation varies slightly between datasets, with random flip, random crop, and RGB channel normalization on CIFAR, and similar strategies with different crop sizes on ImageNet-16-120. A different hyper-parameter set is used for CIFAR-10 with 12 training epochs. This provides options for bandit-based algorithms with short training budgets. NAS-Bench-201 allows researchers to speed up their searching algorithm on datasets by providing training and validation metrics for different architectures. The API can be used to query results with minimal computational costs. The table summarizes training/test loss/accuracies on four datasets. NASBench-201 is versatile for various NAS algorithms, unlike NAS-Bench-101. NAS-Bench-201 provides training and validation metrics for different architectures, allowing researchers to evaluate NAS algorithms efficiently. It offers insights for designing better algorithms and can be used on various datasets. Validation accuracy serves as a supervision signal, but it is sparse due to computational costs. The API is versatile for different NAS algorithms, unlike NAS-Bench-101. NAS-Bench-201 provides computational metrics for architectures, including parameters, FLOPs, and latency. This information can be used for designing algorithms targeting computational constraints, such as edge devices. Additionally, it offers fine-grained training and evaluation data to track changes in loss and accuracy. NAS-Bench-201 provides fine-grained training and evaluation information, showing architecture performance tendencies and attributes like convergence speed and stability. Some methods can predict final accuracy early on, allowing for faster training and evaluation. The trained parameters for each architecture are released, aiding hypernetwork-based NAS methods. NAS-Bench-201 and NAS-Bench-101 are large-scale architecture datasets that transform architecture search into searching neural cells represented as a DAG. NAS-Bench-201 provides fine-grained training and evaluation information, while NAS-Bench-101 defines operation candidates on the node. The main highlights of NAS-Bench-201 include the release of trained parameters for each architecture, aiding hypernetwork-based NAS methods. NAS-Bench-201 is algorithm-agnostic, unlike NAS-Bench-101, which is limited to selected algorithms. NAS-Bench-201 sacrifices the number of nodes to include all possible edges, making the search space algorithm-agnostic. NAS-HPO-Bench provides diagnostic information for efficient NAS algorithm designs. It includes a space for a 2-layer feed-forward network with 144 architectures. The performance of each architecture is displayed, showing how the number of parameters affects architecture performance. The performance of architectures in NAS is influenced by the choices of operations and how cells are connected. Comparisons with ResNet show competitive performance, with room for improvement. Architecture rankings on different datasets are displayed in Figure 3. The performance of architectures in NAS is consistent across different datasets, with correlations between validation and test accuracies varying. Directly transferring the best architecture between datasets does not guarantee optimal performance, highlighting the need for improved transferable NAS algorithms. Rankings of architecture performance at different time stamps are shown in Figure 5. In this section, 10 recent searching methods are evaluated on NAS-Bench-201 to serve as baselines for future NAS algorithms. Various NAS algorithms including Random Search, ES methods, RL algorithms, Differentiable algorithms, and HPO methods were experimented on a single GeForce GTX 1080 Ti GPU, training on the CIFAR-10 dataset. The NAS algorithms were tested on a single GeForce GTX 1080 Ti GPU using different training and evaluation sets. Results of 500 runs for RS, REA, REINFORCE, and BOHB were reported, along with 3 runs for RSPS, DARTS, GDAS, SETN, and ENAS. The validation and test accuracies of the searched architecture on CIFAR-10 were shown, highlighting the benefits for speed using NAS-Bench-201 for various NAS algorithms. NAS-Bench-201 allows NAS algorithms to quickly find the final architecture's performance, reducing searching time to seconds. Algorithms without parameter sharing outperform others, with REA, RS, REINFORCE, and BOHB showing the best results. Training on CIFAR-10 datasets, these algorithms achieve good performance with minimal GPU hours. In Figures 7 and 8, the performance of different algorithms' architectures per searching epoch is shown. DARTS-V1 tends to over-fit to architectures with skip-connection operations, while DARTS-V2 can alleviate this issue but still over-fits after more epochs. RSPS, GDAS, SETN, and ENAS are trained five times longer than DARTS due to optimizing fewer parameters. GDAS shows similar performance to DARTS after 50 searching epochs, while RSPS and SETN exhibit higher variance. Our NAS-Bench-201 aims to provide a fair environment for NAS algorithms, but some algorithms may not perform optimally due to hyper-parameter sensitivity. We welcome new algorithms to test on our dataset and are open to updating results based on better performance with different hyper-parameters. To prevent over-fitting, we propose rules for users to follow. We propose rules for users to follow in NAS-Bench-201 to ensure fair and efficient benchmarking. Avoid specific designs tailored to the best architecture and use the provided performance to prevent over-fitting. Stick to the performance provided in the benchmark for optimal results. The NAS-Bench-201 benchmark provides fair comparison with other algorithms by sticking to the performance provided, reporting results of multiple searching runs, and addressing limitations in hyper-parameter optimization. The use of the same hyper-parameter configuration for all architectures may introduce biases, but hyper-parameter optimization can help search for the optimal configuration. The optimal hyper-parameter configurations and architecture in one shot are computationally expensive. Potential designs using diagnostic information in NAS-Bench-201 could provide more insights for NAS. Parameter sharing is crucial for improving searching efficiency but sacrifices accuracy. Generalization ability of the search space needs to be tested on a larger scale. In NAS-Bench-201, experiments show rankings of RS, REA, and REINFORCE (REA > REINFORCE > RS). Results indicate GDAS \u2265 DARTS \u2265 ENAS for NAS methods with parameter sharing. The study extends reproducible NAS with evaluations of 15,625 architectures on three datasets, providing comprehensive analysis and insights for NAS algorithms. In NAS-Bench-201, various NAS algorithms are tested to establish baselines for future research. The study aims to combine HPO and NAS, explore a larger search space, and invites researchers to test their NAS algorithms on NAS-Bench-201. Table 6 compares the correlation of different training strategies using validation accuracy on various datasets. In NAS-Bench-201, architectures are encoded by a 6-dimensional vector, resulting in 12751 unique topology structures. With additional operations considered, there are only 6466 unique topology structures due to isomorphism. Numerical errors can lead to different outputs for architectures with isomorphic cells. The NAS-Bench-201 is built without considering isomorphism, and bandit-based algorithms are used for optimization. In NAS-Bench-201, bandit-based algorithms like Hyperband and BOHB are used to train models with a short time budget. Two options are provided for obtaining model performance on CIFAR-10: results from H \u2021 where cosine annealing converges at the 12-th epoch, and results from H \u2020 where cosine annealing converges at the 200-th epoch. The performance of converged networks correlates more with performance after a larger number of iterations. The first option is chosen for NAS algorithms without parameter sharing, and 10 NAS algorithms are re-implemented to search architectures on NAS-Bench-201. In NAS-Bench-201, various algorithms are implemented to search architectures. The searching time of different algorithms is compared to the baseline of first order DARTS. The total time budget for RS, REINFORCE, ENAS, and BOHB is set at 12000 seconds. Different algorithms have varying search procedure lengths, with configurations available at a specific GitHub link. Random search (RS) is conducted by randomly selecting architectures for evaluation. Regularized evolution for image classifier architecture search (REA) is used in NAS-Bench-201 to search for architectures. The sample size is chosen as 10 from [3, 5, 10] based on Figure 9, and the algorithm stops once the simulated training time reaches the time budget of 12000 seconds. The fitness is determined by the validation accuracy after 12 training epochs. We use the REINFORCE algorithm to evaluate different learning rates for architecture search. The training stops after 12000 seconds, with Nesterov momentum SGD used for shared parameter training. The architecture is trained using Adam with a learning rate of 0.0003 and weight decay of 0.001, batch size of 64, and data augmentation techniques. Random search with parameter sharing is conducted over 250 epochs, sampling one architecture per batch. Shared parameters are evaluated with 100 randomly selected architectures. The architecture is trained using Adam with a learning rate of 0.0003 and weight decay of 0.001, batch size of 64, and data augmentation techniques. Random search with parameter sharing is conducted over 250 epochs, sampling one architecture per batch. Shared parameters are evaluated with 100 randomly selected architectures. Gradient-based search using differentiable architecture sampler (GDAS) is employed, with a Gumbel-Softmax temperature linearly decayed from 10 to 0.1 over 250 epochs. After training the shared parameters for 250 epochs, 100 architectures with the highest probabilities are selected and evaluated using the same procedure as RSPS. A two-layer LSTM controller with a hidden size of 32 is used, along with a temperature of 5. The controller for the NAS-Bench-201 architecture search uses a two-layer LSTM with a hidden size of 32 and a temperature of 5. The sampling logits are adjusted with a tanh constant of 2.5. The controller's sample entropy is added to the reward with a weight of 0.0001. The controller is optimized with Adam using a learning rate of 0.001, while the network weights are optimized with SGD and a batch size of 128. BOHB is used as the hyperparameter optimization algorithm, with specific settings following previous work. The algorithm stops once the simulated training time reaches the time budget of 12000 seconds. The shared parameters in neural architecture search methods are optimized using DARTS, GDAS, and SETN on CIFAR-10. Each architecture candidate's probability of being good is calculated using SETN. Evaluating all candidates on the validation set is computationally expensive, so a one-shot validation accuracy is used instead. To accelerate the evaluation process, architectures are assessed on mini-batches of 2048, approximating one-shot validation accuracy. The correlation between proxy metrics and ground truth accuracy is analyzed, revealing the importance of BN layers on validation accuracy. In NAS-Bench-201 (version 1.0), architectures are trained multiple times with different random seeds to improve one-shot accuracy. Each architecture candidate recalculates BN layer mean and variance instead of using accumulated values from the training set. GDAS utilizes Gumbel-softmax sampling for architecture encoding optimization, leading to higher probability correlation. Uniform sampling strategy for shared parameter training increases correlation for one-shot accuracy compared to joint optimizing strategy."
}