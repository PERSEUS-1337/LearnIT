{
    "title": "BJeRg205Fm",
    "content": "We propose a method for quantifying uncertainty in neural network regression models for real values on a $d$-dimensional simplex, such as probabilities. Targets are modeled as samples from a Dirichlet distribution, with parameters provided by the neural network output. This approach allows for interpretable multidimensional predictions, confidence intervals, and risk quantification in decision making. The same method can also model targets as samples from a Dirichlet-multinomial compound distribution. Experimental results show these benefits without compromising point estimate prediction performance. The text discusses the use of neural networks to make point estimate predictions in various applications, such as distilling deep convolutional networks and predicting particle collisions. Neural networks typically maximize the likelihood of output targets given input features, with targets modeled as samples from different distributions. The majority of neural networks implicitly model targets as samples from binomial, categorical, Gaussian, or Laplacian distributions. The text discusses the use of neural networks in modeling target distributions, including binomial, categorical, Gaussian, and Laplacian distributions. Common loss functions like cross-entropy, mean squared error, and mean absolute error are used due to their simplicity in gradient calculations. Alternative approaches include using neural networks to parameterize more complex distributions like heteroskedastic Gaussian, mixture density, and Gamma output layers for modeling targets in different scenarios. The Gamma output layer is proposed to model targets in R >0. The approach simplifies the process compared to Conditional Variational Autoencoders (CVAEs) by restricting the output to Dirichlet or Dirichlet-multinomial compound distributions, allowing for exact likelihood calculations and gradients. In this work, gradients are derived for the Beta distribution, Dirichlet distribution, and Dirichlet-multinomial compound distribution to improve learning in neural network classifiers. Activation functions are proposed to stabilize numerical optimization with stochastic gradient descent. The approach presented in the current text chunk demonstrates the use of neural networks to model various types of targets, including targets over the multivariate simplex, real-valued scalar targets with bounds, and nonnegative integer-valued counts. Experimental results show that this method provides interpretable predictions with learned uncertainty while maintaining performance. The Gamma function generalizes the factorial function to real values in neural networks. The network is trained to minimize the negative log-likelihood of the training set using gradient descent. The digamma function is used for computing gradients, and accurate numerical approximations are available. The Dirichlet distribution is used for obtaining point estimates from the network output. The Dirichlet distribution can be parameterized by a vector \u00b5 and a scalar \u03b3. \u00b5 predicts the expectation of the Dirichlet, similar to a heteroskedastic Gaussian model. The Dirichlet output layer can model vectors of non-negative integer targets as samples from the Dirichlet-multinomial compound distribution. The text discusses modeling targets as samples from a Beta distribution using a neural network outputting two values \u03b1, \u03b2 > 0. This approach is applicable when the targets are real-valued scalars with lower and upper bounds, allowing for prediction of univariate targets y \u2208 [0, 1]. The Beta distribution can be parameterized by a neural network outputting two values \u03b1, \u03b2 > 0. Alternatively, it can be parameterized using \u00b5 \u2208 [0, 1] and \u03b3 > 0, where \u00b5 = E[z] and \u03b3 controls the width of the density function. The choice of activation function in the output layer of a neural network affects the shape of the objective function, with Inverse-Linear and Exponential-Linear functions proposed for stabilizing learning in a Dirichlet output layer. For models using stochastic gradient descent, choosing the right activation function is crucial. To prevent large gradients and stabilize learning, two piecewise activation functions - Inverse-Linear (IL) and Exponential-Linear (EL) - are proposed. Another activation function, Exponential-Tanh (ET), saturates at a hyper-parameter value \u03c4 > 1. These functions were tested in experiments with Dirichlet output parameterizations. In experiments with Dirichlet output parameterizations, using the IL activation function led to a more stable learning trajectory compared to the EL activation. Overflow errors were observed when some \u03b1 i values became very small, which could be mitigated with weight regularization or adding a stability factor to the output activation function. The model's increased flexibility could result in unexpected behavior, such as concentrating probability mass at a specific target value. Additional regularization and hyper-parameter tuning may be necessary for certain applications. The text discusses the evolution of a probability density function for a neural network with a Beta output trained to optimize the log-likelihood of a single data point with y = 0.1. The distribution shifts from uniform to concentrating probability mass at the target value after gradient descent updates to the parameters. The model quickly learns to concentrate probability mass at the target by updating \u03b8 1 and \u03b8 2 using gradient descent. Performance may improve when incorporating bounds for real-valued targets in a bounded interval. Tested on a simulated data set from the XENON1T dark matter detection experiment, the task is to predict x and y locations of particle collisions from detector data. Training data consists of 160,000 simulated collision events, with 20,000 events used for early stopping and validation. The detector has rotational symmetry accounted for by randomly rotating each collision. The neural network input consists of real-valued recordings from 248 detector elements for 1000 time steps, predicting x and y locations of events normalized to [0, 1]. A deep neural network regression model with an MSE objective and a linear output layer was optimized using Sherpa BID6, with a two-column siamese network design and specific layer shapes. The neural network model consisted of dense layers with specific units and activation functions. The optimization was done using the Adam algorithm on mini-batches with a dropout rate of 50%. The network's performance was evaluated on a regression task using different output layers. The neural network model used dense layers with specific units and activation functions, optimized with the Adam algorithm on mini-batches with a 50% dropout rate. The performance was evaluated on a regression task comparing different output layers, including a heteroskedastic Gaussian, Beta output layers, and alternative parameterizations. The MSE loss was used for comparison, showing that the more expressive output distributions resulted in better performance. Model compression, or network distillation, involves training a large model for a supervised learning task and transferring the learned information to a smaller model by predicting the probabilistic output of the large model. The smaller \"student\" model can train faster and generalize better by using the \"soft targets\" from the large model's imperfect predictions. This approach is beneficial when the targets are probabilities and the teacher model has a sigmoid or softmax output. The approach involves training a convolutional neural network on the CIFAR-100 dataset as a teacher model. The network architecture includes 18 layers with the selu transfer function, dropout rates, batch size of 100, and the Adam optimizer. Training samples were augmented, and student neural networks were trained to predict the teacher model's output. The student neural networks were trained to predict the teacher network's predictions with different output layers, including a standard softmax layer and a Dirichlet output. Gradients were clipped for stability, and a stability factor was added. Training with MSE objective had little effect on softmax output but hurt Dirichlet outputs. The Dirichlet output layers were compared to a softmax layer in training trajectories of neural networks. Using MSE as the objective had little effect on softmax output but hurt Dirichlet outputs. An autoencoder network was trained to learn a 2-dimensional embedding of simulated data. An autoencoder network was trained to learn a 2-dimensional embedding of data simulated from high-dimensional, semi-sparse multinomials for metagenomics analysis. The data set consisted of 10 clusters sampled from a Dirichlet distribution, with 1,000 samples from each cluster. Training was done on 80% of the examples with a neural network autoencoder model, and the output was a Dirichlet-multinomial distribution. The autoencoder network was trained to learn a 2-dimensional embedding of data simulated from high-dimensional, semi-sparse multinomials for metagenomics analysis. The model converged to an embedding with clearly separated clusters in the validation set. The neural network prediction is a distribution over possible target values, and the proposed model uses a Dirichlet-multinomial output layer. The Beta distribution and Dirichlet-multinomial compound distribution are proposed for modeling certain types of network targets. A neural network can parameterize these distributions and be trained using gradient descent on the negative log-likelihood of the training data targets. This approach provides an elegant way to model targets that lie on a simplex or bounded interval. The Dirichlet-multinomial model is used to predict distributions with correct support for decision making and confidence intervals. Experimental results show that the Dirichlet expectation serves as a good point estimate with similar mean squared error to optimizing MSE directly."
}