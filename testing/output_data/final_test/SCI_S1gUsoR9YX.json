{
    "title": "S1gUsoR9YX",
    "content": "Multilingual machine translation has gained attention for its efficiency in training and serving multiple languages with a single model. However, traditional methods often result in lower accuracy compared to individual models for each language pair. To address this, a distillation-based approach is proposed where individual models act as teachers to train a multilingual model through knowledge distillation. Experiments on various translation datasets show the effectiveness of this method, demonstrating the ability of one model to handle multiple languages with comparable or better accuracy. Neural Machine Translation (NMT) has rapidly developed, achieving human parity in accuracy. Multilingual NMT handles multiple languages in one model, reducing training and serving costs. Previous works focus on model architecture design through parameter sharing, achieving comparable accuracy with individual models. The text discusses the challenges of training a multilingual translation model supporting multiple language pairs while maintaining accuracy. It proposes transferring knowledge from individual models to the multilingual model using knowledge distillation. This approach aims to improve translation accuracy for a large number of language pairs. The paper proposes a new method for multilingual translation using knowledge distillation to bridge the accuracy gap between individual models and a multilingual model. Multiple individual models serve as teachers for separate language pairs, while the student model handles all language pairs in a single model. The proposed method uses knowledge distillation to train a multilingual model by matching outputs of individual models with ground-truth translations. Experiments show improved translation accuracy on various language pairs compared to individual models. The multilingual model with a fraction of the parameters can achieve similar or better accuracy than individual models on various language pairs. NMT models learn parameters by minimizing negative log-likelihood, using an encoder-decoder framework to model conditional probability. NMT has evolved from translating single language pairs to multilingual translation. NMT has evolved to handle multilingual translation by sharing components among multiple language pairs, such as using shared encoders and different decoders or combining multiple encoders and decoders for various source and target languages. This approach aims to bridge multiple language pairs with a neural interlingua, allowing for universal models to handle multiple languages efficiently. In BID17 and BID13, a universal model is used for multiple source and target languages with a special tag in the encoder. Multilingual translation in BID10, BID17, and BID27 boosts accuracy for low-resource language pairs. Multilingual NMT typically has lower accuracy with dozens of language pairs compared to individual models. A multilingual distillation framework is proposed to enhance multilingual NMT accuracy to match or surpass individual models. Knowledge distillation is used for model compression in BID2. In this paper, a multilingual distillation framework is developed for multilingual NMT. It differs from previous works by using multiple teacher models for knowledge distillation. In this multilingual distillation framework for NMT, multiple teacher models are used to train a single student model. The individual models for each language pair have higher accuracy than the multilingual model due to limited capacity. The proposal is to teach the multilingual model using the individual models as teachers, introducing knowledge distillation in a multilingual setting. The log-likelihood loss on corpus D for an NMT model can be formulated with cross-entropy. In knowledge distillation, the student matches outputs of the ground-truth label and the teacher model's probability outputs. The distillation loss is calculated using the cross entropy between the teacher model's output distribution and the student model's predictions. The multilingual distillation process involves assigning non-zero probabilities to multiple words, reducing variance in gradients. The total loss function includes log-likelihood loss and distillation loss, with a coefficient \u03bb to balance them. Algorithm 1 summarizes the process, matching outputs from the teacher model to train the student model. The distillation process outlined in Algorithm 1 involves using pretrained individual models for each language pair as inputs. The multilingual model learns from both ground-truth data and individual models with a loss function when its accuracy is below a certain threshold, otherwise it only learns from ground-truth data. Selective Distillation is used in the training process to prevent distillation from a bad teacher model, which could harm the student model's accuracy. When the multilingual model's accuracy exceeds the individual model for a specific language pair, distillation loss is removed, and the model is trained with the original negative log-likelihood loss instead. Selective distillation is a mechanism used in training to prevent distillation from a poor teacher model, which could negatively impact the student model's accuracy. In this process, when the multilingual model surpasses the individual model for a language pair, distillation loss is removed, and the original negative log-likelihood loss is used for training. Top-K Distillation is implemented to reduce memory usage by loading only the top-K probabilities of the teacher models' output distribution for distillation. In selective distillation, top-K distillation is used to reduce memory usage by loading only the top-K probabilities of the teacher models' output distribution for distillation. Experimental settings and results are reported for three public datasets: IWSLT, WMT, and Ted Talk translation tasks. The three datasets used in the experiments are described in the Appendix. Language codes according to ISO-639-1 standard are listed. Sentences are tokenized with moses tokenizer and segmented using Byte Pair Encoding (BPE) BID30. The Transformer BID34 is used as the NMT model structure. Model configurations for individual and multilingual models are kept the same. Parameters for different tasks vary in terms of hidden size, feed-forward hidden size, and number of layers. For the multilingual model training, data is upsampled for all languages to have the same size. Mini batch size is set to 8192 tokens. Individual models are trained with 4 NVIDIA Tesla V100 GPU cards, while multilingual models use 8. Default parameters of Adam optimizer and learning rate schedule are followed. Dropout is set at 0.2 for individual models and 0.1 for multilingual models based on validation performance. Knowledge distillation parameters include T check = 3000 steps, accuracy threshold \u03c4 = 1 BLEU score, distillation coefficient \u03bb = 0.5, and number of teacher's outputs K = 8. During inference, beam search is used with beam size set to 4 and length penalty \u03b1. During inference, beam search is used with a beam size of 4 and length penalty \u03b1 = 1.0 for all languages. Translation quality is evaluated using tokenized case sensitive BLEU BID28. Codes are implemented based on fairseq and will be released after publication. Results on IWSLT Multilingual NMT include many-to-one and one-to-many settings. Experiments are conducted on 12 languages\u2192English translations on the IWLST dataset. Three methods for comparison are considered: Individual, multi-distillation, and multi-baseline. Our method outperforms the multilingual baseline for all languages, achieving similar or better accuracy with significantly fewer model parameters. Individual models perform better on most languages, except for those with small training data that benefit from data augmentation in multilingual training. Our method performs well in the one-to-many setting, maintaining accuracy compared to individual models and improving over the multilingual baseline by nearly 1 BLEU score. Results for 6 languages\u2192English translations on the WMT dataset are shown in Table 3, demonstrating the effectiveness of our multi-distillation method. Our method outperforms the multi-baseline model and individual models on various languages, showing significant improvements in accuracy. Additionally, our method demonstrates superior performance in the one-to-many setting on the WMT dataset. The effectiveness of our method is further highlighted in the Ted talk dataset, where it shows substantial BLEU score improvements over individual models and the multibaseline model across 44 languages\u2192English translations. Our method shows significant improvements in accuracy over individual models and the multi-baseline for various languages, with larger improvements seen in languages with smaller data sizes. The number of parameters in our method is only 1/44 of the sum of 44 individual models, yet it can match or surpass them. Detailed experiment results are provided in the Appendix. Our proposed method for multilingual NMT, selective distillation, shows better performance on 13 out of 16 languages compared to distillation all the time, with significant BLEU score improvements. The student model matches the top-K output distribution of the teacher model to reduce memory cost. The study analyzes accuracy differences between top-K and full distribution on IWSLT dataset for De-En translation. Increasing K from 1 to 8 improves accuracy, but no gains are seen with larger K values. Back Distillation method is used to improve multilingual model performance. The study explores the use of back distillation to enhance a multilingual model's performance on the IWSLT dataset. Results show improved accuracy for 9 out of 12 languages, with word-level knowledge distillation outperforming sequence-level knowledge distillation. The study analyzes how distillation in a multilingual setting improves model generalization by comparing the generalization capability of two models through perturbing their parameters. The study compares the generalization capability of two models by perturbing their parameters. Results show that the proposed distillation-based approach improves accuracy in multilingual NMT, helping the model find wider local minima and generalize better. The study introduces a distillation-based approach to enhance the accuracy of multilingual NMT models. Experiments on various translation datasets show that the proposed method can match or even outperform individual models with significantly fewer parameters. Future research will focus on further analyzing how distillation aids in multilingual model training and expanding the method to larger datasets and more language pairs. Detailed descriptions of the IWSLT, WMT, and Ted Talk datasets used in the experiments are provided. The dataset used in the experiments includes sentence pairs ranging from 80K to 200K for various language pairs from IWSLT, WMT, and Ted Talk datasets. Training data sizes for each language pair are specified in the tables. The dataset used in the experiments includes sentence pairs ranging from 80K to 200K for various language pairs from IWSLT, WMT, and Ted Talk datasets. We use the official validation and test sets for each language pair, with data sizes of the training set listed in TAB2. Language names and their corresponding codes are also provided according to the ISO 639-1 standard."
}