{
    "title": "rkgKBhA5Y7",
    "content": "The most successful approaches to semi-supervised learning involve consistency regularization, which improves generalization performance by training models to be robust to small perturbations. However, SGD struggles to converge on the consistency loss, leading to changes in predictions on test data. To address this, training consistency-based methods with Stochastic Weight Averaging (SWA) is proposed, along with fast-SWA for faster convergence. Weight averaging helps achieve better results. Weight averaging, combined with a cyclical learning rate schedule, improves semi-supervised learning results on CIFAR-10 and CIFAR-100. This approach achieves 5.0% error on CIFAR-10 with only 4000 labels, outperforming previous methods. Semi-supervised learning is valuable in scenarios like medical imaging where labeled data is limited. Consistency-enforcing methods using unlabeled data are currently the most effective in semi-supervised learning. This paper focuses on improving consistency-based semi-supervised learning methods, such as Mean Teacher, by understanding loss geometry and optimization trajectories. The proposed approach aims to enhance SGD solutions through stochastic weight averaging (SWA). The BID12 method averages weights of networks from different training epochs to improve generalization. Empirical study shows it achieves best semi-supervised results. It regularizes the norm of the Jacobian of network outputs, encouraging flatter solutions related to generalization. Comparing training trajectories of BID12, Mean Teacher, and supervised models reveals differences. The training trajectories of consistency-based models show larger distances between weights at different epochs, wider error curves, and SGD iterates exploring multiple solutions. Averaging weights of SGD iterates stabilizes trajectories and centers solutions in flat regions of the loss. The training trajectories of consistency-based models show larger distances between weights at different epochs, wider error curves, and SGD iterates exploring multiple solutions. Averaging weights of SGD iterates stabilizes trajectories and centers solutions in flat regions of the loss. Weight averaging or ensembling models with diverse predictions improves generalization performance significantly, especially for \u21e7 and Mean Teacher models. Stochastic Weight Averaging (SWA) is proposed for further improvement. In Section 3, Stochastic Weight Averaging (SWA) is applied to the \u21e7 and Mean Teacher models. Modifications to SWA are proposed in Section 4, including fast-SWA with longer learning rate cycles and averaging weights of multiple networks. In Section 5, it is shown that fast-SWA converges faster to a good solution than SWA. Applying weight averaging to the models improves results on CIFAR-10 and CIFAR-100 with different numbers of labeled examples, achieving a 5.0% error on CIFAR-10 with only 4k labels. In the context of applying Stochastic Weight Averaging (SWA) to models in previous sections, the current chunk discusses improving results on CIFAR-10 with only 4k labels by 1.3%. Weight averaging is also applied to a domain adaptation technique related to the Mean Teacher model, enhancing results from 19.9% to 16.8% error on CIFAR-10 to STL domain adaptation. The text also briefly reviews semi-supervised learning with consistency-based models, emphasizing the importance of maintaining similar predictions under small perturbations. In semi-supervised learning, a student network measures consistency against a teacher network using labeled and unlabeled data. The consistency loss penalizes differences between predicted probabilities, with the total loss including cross entropy and a consistency term controlled by a parameter. The \u21e7 model uses the student model as its own teacher, with data perturbations to improve performance. The Mean Teacher model (MT) uses data and weight perturbations, with teacher weights being an exponential moving average of student weights. Other consistency-based models like Temporal Ensembling (TE) and Virtual Adversarial Training (VAT) also enforce consistency in different ways for training. In Section 3.1, a simplified version of the model penalizes the norm of the Jacobian of outputs and eigenvalues of the Hessian, related to generalization. Section 3.2 compares training trajectories of different models, showing consistency-based methods make large training steps. Section 3.3 demonstrates that averaging weights or ensembling predictions can lead to accuracy gains, especially for the proposed models. The text discusses penalizing the input-output Jacobian norm in a simplified version of the model. The consistency loss is calculated using small additive perturbations to the student inputs. An estimator is presented as an unbiased estimator of the Jacobian norm with variance controlled by the minibatch size. The quantity ||J x || F is related to generalization. The quantity ||J x || F is related to generalization in linear models, corresponding to weight decay. It is also related to graph-based regularization for nonlinear models, utilizing the manifold structure of unlabeled data. Consistency with respect to perturbations sampled from natural images can be enforced by penalizing the manifold Jacobian norm. Penalization of the Hessian's eigenvalues is also discussed. See Section A.5 for more details. Penalization of the Hessian's eigenvalues is discussed in Section A.6. The Hessian eigenvalues encode local information about the sharpness of the loss for a given solution. The trace of the Hessian is related to the notion of sharp and flat optima, which is a proxy for generalization performance. This analysis highlights the importance of minimizing the consistency loss for a simplified model. Based on the analysis, consistency loss in a simplified model leads to flatter solutions, promoting better generalization. The study explores SGD trajectories for consistency-based models compared to supervised training on CIFAR-10 with 4k labeled data for 180 epochs. The models use 46k unlabeled data points. Visualizing the evolution of gradient norms for cross-entropy and consistency terms shows the impact of consistency loss on model training. The study analyzes the impact of consistency loss on model training by comparing SGD trajectories for consistency-based models with supervised training on CIFAR-10. The consistency term dominates the gradient for these models, indicating larger steps and active exploration of solutions. The models exhibit flatter solutions, promoting better generalization. The study compares the impact of consistency loss on model training by analyzing SGD trajectories for consistency-based models versus supervised training on CIFAR-10. The distance between weight vectors in semi-supervised methods is larger, indicating larger steps during optimization. The train and test error surfaces are wider for consistency-based methods, possibly due to the effect of consistency loss on the network's Jacobian and the eigenvalues of the loss Hessian. The study compares the impact of consistency loss on model training by analyzing SGD trajectories for consistency-based models versus supervised training on CIFAR-10. The test errors of interpolated weights can be lower than errors of the two SGD solutions between which we interpolate. The error reduction is larger in the consistency models. The error surfaces along random and adversarial rays starting at the SGD solution are analyzed. The widths along random and adversarial rays are comparable for consistency models. In fast-SWA, the rays connecting SGD iterates serve as a proxy for the space being averaged over. Evaluating the width of solutions explored during training is expected to improve with consistency training. By analyzing SGD-SGD rays, meaningful changes to the model can be identified as individual SGD updates correspond to directions that impact predictions on the training set. Different SGD iterates produce significantly different predictions on the test data, indicating the resilience of neural networks to noise. Neural networks are resilient to noise but susceptible to targeted perturbations. Semi-supervised methods do not show improved flatness along adversarial rays due to non-adversarial input and weight perturbations. Larger optimization steps in certain models lead to higher diversity in predictions compared to supervised learning. The Diversity between models is 7.1% and 6.1% for specific models, higher than the 3.9% in supervised learning. The diversity in predictions of neural networks traversed by SGD is much higher for specific models compared to supervised learning. These models continue to actively explore plausible solutions until the end of training, leading to larger distances between weights and higher diversity. Using the last SGD iterate for prediction is not ideal as many equally accurate solutions produce different predictions. Ensembling can help achieve greater accuracy. Ensembling with diverse models can lead to greater benefits compared to purely supervised learning. By sampling pairs of weights from different epochs and measuring error reduction, we observe a strong correlation between model diversity and ensemble performance. Weight averaging can approximate ensembling when weights are close by. Weight averaging can approximate ensembling by averaging random pairs of weights at the end of training. The performance of the model is evaluated based on weight distances, with C avg serving as a proxy for convexity. The error surface of neural networks may be approximately convex late into training, as shown in FIG1. The distances between pairs of weights are larger for diverse models compared to supervised training. Weight averaging can approximate ensembling by averaging random pairs of weights at the end of training. The performance of the model is evaluated based on weight distances, with larger distances observed for diverse models compared to supervised training. For models like \u21e7 and Mean Teacher, SGD iterates at the periphery of a large flat region in weight space, leading to weight averaging achieving a larger gain. Averaging the SGD iterates can move towards the center of the flat region, stabilizing the trajectory and improving the width of the resulting model. Weight averaging can approximate ensembling by averaging random pairs of weights at the end of training, stabilizing the SGD trajectory and improving generalization. The improvement from weight averaging is on par or larger than prediction ensembling. The focus is on weight averaging due to its lower costs at test time and slightly higher performance compared to ensembling. The training trajectories of different models show active exploration of plausible solutions, even in late stages of training. In section 3.3, averaging weights leads to significant performance gains for the \u21e7 and MT models, surpassing supervised settings. Stochastic Weight Averaging (SWA) involves averaging weights traversed by SGD with a modified learning rate schedule, improving test accuracy. SWA enhances generalization performance in supervised learning, especially when applied to the \u21e7 and MT models. SWA starts from a pre-trained model and averages points in weight space traversed by SGD with a constant or cyclical learning rate. Stochastic Weight Averaging (SWA) involves averaging weights traversed by SGD with a modified learning rate schedule, improving test accuracy. SWA collects networks corresponding to minimum learning rate values and averages their weights to create a model for predictions. This technique can be applied to student networks in the \u21e7 and Mean Teacher models without interfering with training. Originally, SWA was proposed with small cycle lengths, but recent findings suggest using cyclical cosine learning rates for better performance. The proposed modification, fast-SWA, aims to improve upon SWA by averaging networks every k < c epochs starting from epoch c. This allows for more frequent updates and potentially better performance compared to SWA. Models included in fast-SWA may have higher errors due to being obtained at higher learning rates. Including more models in fast-SWA can compensate for larger errors, leading to faster convergence and lower performance variance compared to SWA. Experimental results show improved performance on CIFAR-10 and CIFAR-100 models, with fast-SWA outperforming SWA and achieving high performance faster. Additionally, fast-SWA improves results in semi-supervised literature and outperforms SWA in domain adaptation models. The experimental setup and results for adapting CIFAR-10 to STL are discussed, with the best reported test error rate improved from 19.9% to 16.8%. Various additional results and comparisons are provided, along with an analysis of train and test error surfaces of fast-SWA solutions. Weight averaging methods SWA and fast-SWA are evaluated on different network architectures, showing improvements in all settings. For training different methods, stochastic gradient descent (SGD) optimizer with cosine annealing learning rate is used. Two learning rate schedules are employed, a short schedule and a long schedule, with the long schedule showing improved performance. Each CNN experiment is repeated 3 times with different random seeds. Additional hyperparameters details can be found in the Appendix. The proposed fast-SWA method is evaluated on CIFAR-10 dataset using ResNet + Shake-Shake models with different amounts of labeled data. Results show significant improvement in test accuracy for both architectures. Comparison with other averaging strategies and popular semi-supervised learning approaches like VAT is also provided, with fast-SWA showing a reduction in error rates. In experiments with CIFAR-10 dataset, fast-SWA reduces error rates compared to VAT, improving performance with cyclical learning rate and Shake-Shake models. Fast-SWA improves performance and converges faster than SWA in training. It reduces error rates significantly earlier, especially for CIFAR-10 4k labels. The performance gains are higher for the \u21e7 model compared to the MT model, bridging the performance gap between them. Surprisingly, the \u21e7 model can even outperform MT after applying fast-SWA with a moderate to large number of labeled points. The \u21e7 model outperforms MT with fast-SWA on CIFAR-10 with 4k, 10k, and 50k labeled data points for the Shake-Shake architecture. Evaluation is done on CIFAR-100 with 10k and 50k labels using a 13-layer CNN, and the Tiny Images dataset BID32 is used as an additional source of unlabeled data. In experiments using CIFAR-100 images, additional unlabeled data from the Tiny Images dataset is utilized in two settings: 50k+500k and 50k+237k. Fast-SWA consistently enhances performance across various configurations of labeled and unlabeled data. Results are visualized in FIG3, showing significant error reduction compared to SWA. Detailed experimental results can be found in Table 3 of the Appendix. Fast-SWA significantly improves performance for both \u21e7 and MT models, as shown in Table 3 of the Appendix. Results are compared with previous best results in Table 1, using the 13-layer CNN and Shake-Shake architecture. The Shake-Shake architecture is also used in preliminary results in Table 5. Additional unlabeled data from the Tiny Images dataset is utilized in different settings to enhance performance. By utilizing additional unlabeled data from the Tiny Images dataset, semi-supervised learning has shown significant improvements in reducing the dependency on large labeled datasets. Analyzing solutions from successful consistency regularization models like \u21e7 and Mean Teacher reveals that SGD continues to explore diverse solutions late into training, leading to performance gains through averaging predictions or weights. Fast-SWA further enhances performance for these models, as demonstrated in Table 3 of the Appendix. Weight averaging, a variant of stochastic weight averaging (SWA), has shown promise in domain adaptation. Application-specific analysis of training objective properties can improve results in various areas such as reinforcement learning, generative adversarial networks, and semi-supervised natural language processing. Test error plots using a 13-layer CNN on CIFAR-10 with labeled and unlabeled data points demonstrate the effectiveness of weight averaging compared to standard fully supervised training. In this section, additional plots visualize the train and test error along different types of rays in the weight space. The test accuracy improves monotonically as more unlabeled data points are added for the model. Solutions become narrower along random rays, and the behavior of test error changes. In this section, the solution to one of the SGD iterates used to compute the average for different training methods is shown. Fast-SWA finds a centered solution, while the SGD solution lies near the boundary of a wide flat region. The train and test error surfaces are wider for certain models compared to supervised training. The behavior of train and test error surfaces along random rays, adversarial rays, and directions connecting the SGD solutions is also analyzed. In this section, detailed results for the \u21e7 and Mean Teacher models on CIFAR-10 and CIFAR-100 using the 13-layer CNN and Shake-Shake are provided. Variance of the gradient is higher for the \u21e7 and Mean Teacher models compared to supervised training. Diversity is found to be highly correlated with weight averaging improvement C. Tables 2 and 3 summarize results for CIFAR-10 and CIFAR-100 using the 13-layer CNN, while Tables 4 and 5 summarize results using Shake-Shake. EMA is used instead of SWA for the student in the \u21e7 EMA method. In this section, the performance of the \u21e7 and Mean Teacher models on CIFAR-10 and CIFAR-100 using EMA for student weights is discussed. Results show that applying EMA for the student network without using it as a teacher leads to a slight improvement in test error. Figures 8 and 9 display the model performance over training epochs for SWA and fast-SWA. Table 3 shows CIFAR-100 semi-supervised errors on the test set for models trained on a 13-layer CNN. The results include various models using different formulas and techniques such as EMA, SWA, fast-SWA, and VAT. Table 3 displays CIFAR-100 semi-supervised errors on the test set for models trained on a 13-layer CNN, using different formulas and techniques like EMA, SWA, fast-SWA, and VAT. The results show error rates for different label numbers and training epochs. Table 4 shows CIFAR-10 semi-supervised errors on the test set for models using Shake-Shake Regularization and ResNet-26 BID11. The errors are shown for different numbers of labels and training schedules, including SWA and fast-SWA formulas. Table 5 shows CIFAR-100 semi-supervised errors on the test set for models using Shake-Shake Regularization and ResNet-26 BID11. The errors are shown for different numbers of labels and training schedules, including SWA and fast-SWA formulas. The only hyperparameter for the fast-SWA setting is the cycle length c. The fast-SWA setting with a 13-layer CNN is not sensitive to the cycle length c. Using cyclical learning rates, fast-SWA generally converges faster due to a higher variety in collected weights. The MT model employs an exponential moving average (EMA) of student weights as a teacher for consistency regularization. Two potential effects of using EMA as a teacher are considered: improving teacher performance and subsequently enhancing student performance. Separating these effects, EMA is applied to the \u21e7 model. Applying EMA to the \u21e7 model is compared to using fast-SWA or Mean Teacher method. While EMA improves results over the baseline \u21e7 model, \u21e7-EMA still lags behind Mean Teacher, especially with limited labeled data. EMA averages network weights, focusing more on recent models, improving performance early in training when the student model changes rapidly. The student model changes rapidly, and EMA significantly improves performance early in training. However, once the model converges to the optimum, EMA offers little gain. SWA is a better way to average weights in this regime. Using SWA as a teacher performs on par with EMA as a teacher when applied to the model. Starting to use SWA as a teacher earlier in training may be possible, but EMA is more sensible during rapid model improvement in early epochs. During early epochs of rapid model improvement, EMA is more sensible than SWA. Estimator mean and variance are calculated using Taylor expansion in a simplified model with small additive data perturbations. The stochastic trace estimator for a symmetric matrix A is discussed, highlighting the importance of the ordering of the sum. Non-isotropic perturbations along data manifold can be understood as penalizing a Jacobian norm. Consistency regularization penalizes the Laplacian norm of the network on the manifold. Standard data augmentations like random translation are also applied. The text discusses the relationship between data augmentations, optimization, and the MSE loss function in machine learning models. It explains how smaller errors imply broader optima and how the Hessian matrix can be decomposed into different terms. The analysis also touches on the expected MSE loss along random rays and the improvement of solutions over time. In the experiments, two DNN architectures are used - a 13-layer CNN and Shake-Shake with different schedules. The architecture of the 13-layer CNN closely follows previous work and is re-implemented in PyTorch without Gaussian input noise for better generalization. Shake-Shake uses a 26-2x96d architecture with 12 residual blocks. Results are reported for both short and long schedules. In experiments, two DNN architectures are used - a 13-layer CNN and Shake-Shake with different schedules. The initial learning rate \u2318 0 and cycle length c were determined using a separate validation set. Stochastic gradient descent optimizer with Nesterov momentum is used. Fast-SWA averages weights of models every third epoch. In the \u21e7 model, gradients are back-propagated through the student side only. Mean Teacher uses \u21b5 = 0.97 decay. In Mean Teacher, a decay rate of 0.97 is used for the Exponential Moving Average of the student's weights. Consistency cost is ramped up over the first 5 epochs to a maximum value of 100. Cosine annealing learning rates are used with no ramp up, and SGD is used instead of Adam for optimization. The same hyperparameters are used for both the \u21e7 and Mean Teacher models in each experiment setting. For CNN experiments, a 13-layer CNN is used with a short learning rate schedule. The total batch size is 100 with a labeled batch size of 50. The maximum learning rate is 0.1. SGD is run for 180 epochs in Section 3.2, while in Section 3.3, 5 learning rate cycles are added. For ResNet experiments, a total batch size of 128 is used with a labeled batch size of 31. The maximum learning rate is 0.05 for CIFAR-10. Different labeled batch sizes are used for various label settings. For settings 50k+500k and 50k+237k, a labeled batch size of 64 is used with a limit of 100k unlabeled images per epoch. The maximum learning rate is 0.1. ResNet experiments use a total batch size of 128 with a labeled batch size of 31. Fast-SWA is applied to the best experiment setting MT+CT+TFA for CIFAR-10 to STL, involving confidence thresholding and augmentation. The optimizer is modified to use SGD with a cosine annealing schedule. In experiments with fast-SWA methods, averaging weights every iteration leads to faster convergence and better performance compared to averaging once per epoch. This approach resulted in improved test accuracy and faster convergence (600 epochs instead of 3000). The benefits of more frequent averaging are attributed to the specific geometry of loss surfaces and training trajectories in domain adaptation. Further analysis on applying fast-SWA to domain adaptation is suggested for future research. Implementation Details: Public code 3 of BID6 is used to train the model with stochastic gradient descent and cosine annealing learning rate. Maximum learning rate is 0.1 with momentum 0.9 and weight decay. Data augmentation setting MT+CF+TFA is applied, and fast-SWA is used, with results reported from epoch 4000."
}