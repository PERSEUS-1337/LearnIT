{
    "title": "S1xNEhR9KX",
    "content": "Neural networks are vulnerable to small adversarial perturbations. Adversarial training, a popular robust training method, shows sensitivity to input data distribution, unlike clean accuracy. Semantics-preserving transformations on data distribution can significantly affect the robustness of the trained model. This sensitivity was discovered through a study on the behaviors of clean and robust accuracy of the Bayes classifier. Empirical investigations confirmed the findings using semantically-identical variants of MNIST and CIFAR10 datasets. Standardly trained models achieve comparable clean accuracies on MNIST and CIFAR10 datasets, but adversarially trained models show significantly different robustness accuracies. This suggests that input data distribution can impact the adversarial robustness of neural networks. Recent research has focused on understanding this phenomenon and its implications for evaluating adversarial robustness. The relationship between adversarial robustness and input data distribution is explored in this paper. Adversarial training methods are sensitive to changes in data distribution, with even small shifts impacting robustness. This finding adds to the existing literature on the tradeoff between robustness and clean accuracy in neural networks. The paper explores the sensitivity of adversarially trained models to changes in data distribution, showing a significant difference in robustness compared to clean accuracy. The study is motivated by observations on the MNIST and CIFAR10 datasets, revealing that CIFAR10 has lower robustness despite similar clean accuracies. In practice, the paper analyzes the difference between regular Bayes error and robust error, showing that the former is invariant to data distribution transformations while the latter is not. A robust model for binarized MNIST is found with a 3% robust error guarantee. The study also designs augmented MNIST and CIFAR10 datasets to demonstrate the sensitivity of adversarial robustness to data distribution. The sensitivity of adversarial robustness to input data distribution raises concerns about evaluating and deploying robust learning algorithms. Performance estimations may be unreliable due to data shifts, impacting the accuracy of models like PGD trained on CIFAR10. The robust accuracy of PGD trained models is influenced by gamma values of gamma-corrected CIFAR10 images, indicating varying robustness properties in datasets under different light conditions. Fashion-MNIST BID27 and edge-fashion-MNIST show different robustness characteristics, highlighting the impact of dataset variations on algorithm evaluations. This new perspective complements existing research on data distribution influences on adversarial vulnerability, suggesting varying tradeoffs between clean accuracy and robustness based on input data characteristics. Different data distributions can have drastically different properties of adversarially robust generalization, not solely dependent on being from completely different distributions. Gradual transformations of data distribution can significantly impact datasets' achievable robustness. Investigating perturbable volume and inter-class distance as causes of sensitivity, with model capacity and sample complexity as potential remedies. The complexity of the problem in image classification has made it challenging to provide a definitive answer. The data distribution is denoted as a joint distribution P(x, y), where x is high dimensional and y is a discrete label. The support of x is the whole pixel space [0, 1] d. The influence of P(x) on adversarial robustness is discussed in the paper. In experiments, whitebox robustness is discussed using models trained with PGD training. The Bayes error is defined as the error of the best possible classifier without restrictions on the function space. The text discusses the concept of robust error in classifiers without restrictions on function space. It highlights the difference between clean accuracy and robust accuracy, especially in the context of distribution shifts. The focus is on the difficulty of achieving robust accuracy, particularly in the CIFAR10 dataset. The text explores the challenges of achieving robust accuracy in classifiers, emphasizing the impact of data distribution. It contrasts the inability to achieve good robustness under a uniform distribution with the achievability of provable adversarial robustness on a binarized MNIST-like dataset. The discussion also touches on the invariance of Bayes error under distribution shifts induced by injective maps, highlighting the difference in behavior between Bayes error and robust error. The Bayes error can behave differently from its robust error, as shown in two examples. Example 1 involves a uniform distribution in [0, 1] with different robust errors despite having 0 Bayes error. The robust error worsens with increasing dimensionality, showcasing the contrast between Bayes and robust errors. The Bayes classifier's accuracy grows with dimensionality, but achieving good robust accuracy is impossible with uniformly distributed data. Theorem 2.1 shows that no algorithm can achieve perfect clean accuracy and good robust accuracy simultaneously. Example 2 illustrates the vulnerability guarantee with uniformly distributed input data and 10 classes for the label. The Bayes classifier's accuracy increases with dimensionality, but achieving good robust accuracy is challenging with uniformly distributed data. Theorem 2.1 suggests that under adversarial attack, a classifier with perfect clean accuracy will misclassify or be vulnerable for at least 94% of samples. The distribution of x in CIFAR10 is closer to the uniform distribution on [0, 1] d, explaining why MNIST is more adversarially robust. However, achieving good robust accuracy remains inherently difficult due to the uniform distribution assumption in the theorems. Theorem 2.1 and Example 2 do not fully explain empirical observations. A provably robust model achieved 3.00% error on bMNIST test data. The role of data distribution in clean and robust accuracies is highlighted. The difference in distance distributions may explain these accuracies. The data distribution plays a crucial role in achieving robust accuracy. New datasets with different P(x)'s but fixed P(y|x) show stable clean accuracies under \"semantic-lossless\" shifts, while adversarial training struggles to achieve robustness. Adversarial training, a popular method for robust models, may lose effectiveness under non-uniform joint distributions. \"Provably\" means robust accuracy can be rigorously proven. New datasets are generated under \"semantic-lossless\" shifts, bridging the gap between MNIST and CIFAR10 datasets. To bridge the gap between MNIST and CIFAR10 datasets, two operations are proposed: smoothing and saturation. Smoothing is applied to MNIST images to make them less binary, while saturation is applied to CIFAR10 to create more binary datasets. The operations aim to maintain semantic information and are semantics-lossless. Smoothing involves applying an average filter of different kernel sizes to generate smoothed versions of MNIST images. In bridging MNIST and CIFAR10 datasets, smoothing and saturation operations are proposed. Smoothing involves applying average filters of varying kernel sizes to MNIST images, maintaining semantic information. Binarized MNIST data points are on unit cube corners, while smoothed versions push pixels off the corners. Saturation is denoted by x(p), pushing data to corners when p\u22652 and to center at 0.5 when p\u22642. In this section, the smoothing and saturation operations are used to manipulate the data distributions of MNIST and CIFAR10 datasets. The empirical results show how data distributions affect the robust accuracies of neural networks trained on them. The focus is on the intrinsic robustness of neural network models, without considering preprocessing methods. Standard neural network training on clean data is used to measure the classification task difficulty, while projected gradient descent (PGD) based adversarial training is used to measure the difficulty of achieving robustness. LeNet5 is used for MNIST variants, and wide residual networks with a widen factor of 4 are used for CIFAR10 variants. The PGD training settings follow those in a previous study. The evaluation of classification performance and robustness on MNIST and CIFAR10 variants follows the settings in a previous study. The test accuracy of standardly trained models on clean examples and the robust accuracy of PGD trained models are assessed. The tradeoff between accuracy and robustness is also considered. The strongest attack, \u221e untargeted PGD attacks, is used as the adversary. The PGD attack implementation from the AdverTorch toolbox was used to evaluate the sensitivity of robust accuracy to data transformations on MNIST variants. Results showed that while clean accuracy remained stable across different variants during standard training, robust accuracy and robustness w.r.t. predictions decreased significantly during PGD adversarial training. Binarized MNIST with adversarial training showed similar clean and robust accuracy, indicating that achieving high robust accuracy on binarized MNIST does not conflict with high clean accuracy. The results show that for PGD training on CIFAR10, the robust accuracy significantly increases from 43.2% to 79.7% before level 16, while the clean test accuracy drops slightly from 85.4% to 80.0%. After level 16, both clean and robust accuracy are almost the same, but robustness in predictions continues to increase, indicating instability. Lower saturation levels result in worse robust accuracy after PGD training, with a robust accuracy of 33.0% at saturation level 1. After saturation level 64, standard training accuracies drop significantly due to high saturation causing \"information loss\" in images. Models trained on highly saturated CIFAR10 are robust with a gap between robust accuracy and clean accuracy. In MNIST variants, drops in robust accuracy are due to adversarial vulnerability. Robust accuracy under PGD training is more sensitive to input data distribution than clean accuracy under standard training. Semantically-lossless data transformation does not introduce unexpected risks. The sensitivity of robust accuracy to input distribution shifts can lead to variations in performance, raising concerns about reliability in benchmarks. Factors like image acquisition conditions and preprocessing can impact robust accuracy, highlighting the need for careful dataset selection for benchmarking adversarial robustness. Gamma mapping is a simple operation used to adjust image exposure. Variants of CIFAR10 images under different gamma mappings demonstrate how input distribution shifts can affect performance measures in adversarial training. Despite changes in brightness, semantic information is preserved. The study explores how gamma values affect accuracy and robustness in adversarial training. Results show that while accuracies on clean data remain consistent, robust accuracy varies significantly based on gamma values. This highlights the importance of interpreting robustness benchmarks, as achieving good robustness on one image dataset may not translate to similar results on another dataset with slightly varied exposures. This raises questions about the accuracy of robustness measures across different datasets. In evaluating image classifier robustness, the choice of dataset is crucial. Fashion-MNIST and edge-fashion-MNIST are used as examples of \"harder\" datasets. The edge-fashion MNIST is created using a Canny edge detector on fashion MNIST images. Training experiments on both datasets show differences in robust accuracy compared to standard accuracy. This emphasizes the need for benchmark settings that align with real-world robustness requirements. The accuracy of image classifiers is influenced by PGD training, with a significant gap between robust accuracy and standard accuracy. In contrast, efMNIST is similar to binarized MNIST with minimal impact from PGD training and little difference between robust and standard accuracy. fMNIST and efMNIST are considered \"harder\" datasets in different ways, with fMNIST having richer semantics resembling natural images, making achieving adversarial robustness more challenging. On the other hand, efMNIST is seen as a set of more complex binary symbols, making classification harder. When introducing new datasets for adversarial robustness, it is important to consider if the dataset is \"harder in the right way\". Analyzing CIFAR10 and MNIST variants, saturation of pixel values towards 0 or 1 suggests factors affecting robustness: decrease in \"perturbable volume\" and increased distances between data examples. While these factors are correlated with robustness change, they do not fully explain the observed results. The perturbable volume is influenced by saturation, pushing data points to corners of the input domain. The volume changes drastically across saturation levels, impacting robustness. For example, the perturbable volume of CIFAR10 images differs significantly between original and saturated versions. The study compared robust accuracies of PGD attacks within and outside data domain boundaries on MNIST and CIFAR10 variants. Results showed that differences in perturbable volume did not cause variations in robustness. Saturation increases distances between data points and decision boundaries, impacting robustness. Inter-class distance is used as an approximation for robustness, with larger distances between classes indicating easier achievement of robustness. However, datasets with the same inter-class distance can exhibit different robust accuracies. Scaled variants of MNIST and binarized MNIST were constructed to have the same inter-class distances as other MNIST variants. The scaling operation pushes dimensions towards the center with a scaling coefficient \u03b1. Results are shown in TAB1. The smoothed MNIST dataset, despite having the same interclass distances as scaled binarized MNIST and original MNIST, is found to be less robust. This suggests that inter-class distance alone may not fully capture the robustness properties of datasets. To improve robustness, options include using better training algorithms, increasing model capacity, and training on more data. This study focuses on exploring the impact of larger model capacity by using differently sized LeNet5 models with varying widen factors. The study explores the impact of model capacity by using LeNet5 models with different widen factors on CIFAR10 variants. Results show that training and test accuracies are invariant to model sizes for MNIST, while PGD training accuracy increases with model capacity. Robust accuracy increases with model size until close to 100%, but test robust accuracy plateaus after widen factor 1. Increasing the training set size impacts the robust accuracies of PGD trained models for most MNIST variants. Training robust accuracy decreases while test robust accuracy increases as the training set size grows. This suggests that PGD training overfits to the training set when it is small, but the generalization gap decreases as the training set gets larger. Both training and test robust accuracies plateau after the training set size reaches 27000, indicating that further increasing the training set size may not be beneficial. For MNIST variants, increasing training set size and model capacity do not improve robustness beyond a certain point. CIFAR10 variants show similar trends, with training robust accuracy dropping as model size increases but test robust accuracy improving. Binarized MNIST and \u221e-saturated CIFAR10 have different sample complexity properties despite both being \"cornered\" datasets. The study found that binarized MNIST and \u221e-saturated CIFAR10 datasets have different sample complexity properties. The interaction between classification task and input data distribution in adversarial robustness is complex and requires further understanding. The research conducted theoretical analyses and systematic experiments on MNIST and CIFAR10 variants, revealing that robustness of adversarial trained models is sensitive to semantically-preserving transformations on data. This sensitivity questions the reliability of evaluating robust learning algorithms on specific datasets. Initial attempts were made to understand this sensitivity. The study analyzed the sensitivity of adversarial trained models to semantically-preserving transformations on data. Markov's inequality was applied, and the longest attacks vector to HP 2 was observed to be parallel to the normal vector 1 to HP 2. The attacks cover a set characterized by {x}. The proof for Theorem 2.1 involves Gaussian measures and a concentration inequality. The curr_chunk discusses Gaussian isoperimetric/concentration inequality and the LeNet5 model architecture for image classification without preprocessing MNIST data. The notion of modulus of convexity for a normed space is also defined, along with the important property related to it. The curr_chunk discusses training LeNet5 on MNIST and WideResNet on CIFAR10 variants with specific optimizer settings and preprocessing methods. Manual hyperparameter search was conducted, but no improvements were observed, so the same settings were used for all experiments in the paper. The paper used consistent settings for all experiments, including training a ConvNet with 2 convolutional layers and 2 fully connected layers. They used ReLUs as activation functions, no max pooling, and trained for 100 epochs with a batch size of 50. The first 50 epochs had a linear increase in epsilon from 0.01 to 0.3. Adam optimizer with a learning rate of 0.001 was used. The main body listed the exact number of experiments in tables 2, 3, 4, and 5. The improved robustness may be due to the effect of boundaries in the data distribution. The study examined the relationship between data distribution proximity to boundaries and robustness. It tested the correlation by analyzing the perturbation space volume across datasets. The size of the perturbation space is determined by the hyperrectangle volume at a given point x in high-dimensional space. The study analyzed perturbable volumes in datasets to understand the impact of data distribution proximity to boundaries on robustness. Different datasets showed significant variations in perturbable volumes, with differences in saturation levels affecting the perturbable space. Large saturation in CIFAR10 images led to substantial differences in perturbable volumes, highlighting the importance of considering data boundaries in robustness analysis. The study analyzed perturbable volumes in datasets to understand the impact of data distribution proximity to boundaries on robustness. Results showed that allowing perturbation outside of data domain boundaries did not significantly affect accuracy under PGD attack, indicating that PGD was unable to utilize the larger additional volumes for attacks. The study rejected the perturbable volume hypothesis as adversarial examples appear in certain directions instead of being distributed across space. Inter-class distances were calculated on MNIST and CI-FAR10 variants, with binarized MNIST showing significantly larger distances. The study analyzed inter-class distances on MNIST and CIFAR10 variants. Binarized MNIST had a larger distance, while CIFAR10 showed a monotonic increase with higher saturation levels. Inter-class distance had a strong positive correlation with robust accuracy. However, exceptions like original MNIST being more robust than smooth-2 MNIST suggest that distance alone cannot explain robustness variations. The relation between inter-class distance and robustness was explored through experiments with a ReLU network. In a synthetic experiment, a ReLU network is trained to separate 2 concentric spheres, where adversarial training is related to the inter-class distance of the data. Adversarial training pushes spheres closer together, requiring the network to separate them with smaller inter-class distance. A model should achieve good accuracy when the inter-class distance is large. This phenomenon was observed on original MNIST dataset. When the inter-class distance decreases, model capacity may not be enough for adversarial training, leading to a drop in accuracies under attack. This behavior was observed on original MNIST and saturated CIFAR10 datasets. The difficulty of achieving good accuracy under attack is related to the model capacity, as shown in a theoretical guarantee. The capacity of a ReLU network can be measured by counting its induced piece-wise linear regions, which relates to the number of facets of its decision boundary. A theorem states that a polytope in R^n with a certain distance from a ball has at least a certain number of facets. The relationship between inter-class distance and model capacity is illustrated, showing that a more complex nonlinear classifier may be needed when the distance is large. The capacity of a ReLU network can be measured by counting its induced piece-wise linear regions, which relates to the number of facets of its decision boundary. A more complex nonlinear classifier may be needed when the distance between data points is large, as shown in experiments. Further research is required for conclusive statements."
}