{
    "title": "Hyg1G2AqtQ",
    "content": "Reinforcement learning in input-driven environments involves exogenous stochastic input processes affecting system dynamics. State-dependent baselines in policy gradient methods lead to high variance during training. A bias-free, input-dependent baseline is derived to reduce variance, with analytical benefits shown over state-dependent baselines. A meta-learning approach is proposed to handle learning baselines dependent on long input sequences. Experimental results demonstrate improved training stability across various environments. In input-driven environments, deep reinforcement learning shows improved training stability and better policies. These environments are influenced by exogenous stochastic input processes, such as queuing systems and network control. Input-dependent baselines are found to enhance training stability and performance in robotics control and optimization. The focus is on model-free policy gradient RL algorithms, which face challenges due to high variance in gradient estimates. A common approach to reduce variance is to use a baseline, with the value function being the most common choice. The insight here is the use of a state-dependent baseline to improve learning efficiency. In input-driven environments, using a state-dependent baseline like the value function may not provide accurate information about action quality. A better approach is to use an input-dependent baseline that considers the specific input sequence. This baseline takes into account both the state and the future input values to estimate the policy gradient effectively. Input-driven Markov decision processes are defined, and it is proven that an input-dependent baseline does not introduce bias in standard policy gradient algorithms. Various scenarios are discussed, including adaptive bitrate video streaming and different environments with stochastic input processes. The optimal input-independent baseline is derived for environments where the input process is independent of states and actions. The optimal input-independent baseline is derived for environments where the input process is independent of states and actions. A simpler conditional value function is proposed for practical use, while input-dependent baselines are shown to be harder to learn efficiently. A meta-learning approach is suggested to specialize a \"meta baseline\" for specific input instantiations with a small number of training episodes. This method is applicable to scenarios where input sequences can be repeated during training, such as simulations or experiments with previously-collected input traces. Input-dependent baselines were compared to standard value function baselines for various tasks in queuing systems, computer networks, and continuous control RL benchmarks in the MuJoCo physics simulator. The tasks were made more challenging by adding a stochastic input element. Results showed that input-dependent baselines consistently improved training stability and eventual policies. Input-dependent baselines, applicable to various policy gradient methods, aim to optimize expected return in a Markov decision process. Stochastic policy \u03c0 is used to improve training stability and policies. Video demonstrations of experiments are available online. Policy gradient methods estimate the gradient of expected return with respect to the policy parameters. The input-dependent baseline results in lower policy gradient variance and higher test rewards compared to the standard, state-dependent baseline. The Policy Gradient Theorem states that using an input-dependent baseline yields a more precise policy than a state-dependent baseline. Estimating the policy gradient with a baseline can reduce variance in Monte Carlo estimation for the Q function. An optimal baseline is hard to estimate and is often replaced by the value function. The load balancing agent minimizes job completion time by sending jobs to servers with the shortest queue, using a Poisson process for job arrivals and a Pareto distribution for job sizes. The reward is based on elapsed time and number of enqueued jobs. The policy is to send jobs to servers with the shortest queue. However, a standard policy gradient algorithm struggles to learn this due to variance in reward signal caused by stochastic job arrivals. The difference in return is mainly due to the random job arrival process. The policy is to send jobs to servers with the shortest queue. Two A2C agents are trained, one with a standard baseline and the other with an input-dependent baseline tailored for each job arrival process. The input-dependent baseline reduces variance in policy gradient estimation effectively, leading to a policy closer to optimal. The variance of the standard baseline can be much worse than an input-dependent baseline. An input-driven MDP is formally defined as having an input process added to a standard MDP, influencing the next state based on the current state, action, and input. The goal is to learn policies maximizing cumulative rewards, with two cases focused on in the context of graphical models. In input-driven MDPs, two cases are considered: Case 1 where z t is a Markov process and \u03c9 t = (s t , z t ) is observed, and Case 2 where z t is a general process and only s t is observed. Case 1 corresponds to a fully-observable MDP, while Case 2 corresponds to a partially-observed MDP (POMDP). A proposed solution is to use an input-dependent baseline to reduce variance. An input-dependent baseline, using information not available to the policy, can be computed by knowing the input sequence at training time. This baseline is bias-free and optimal for variance reduction in input-driven MDPs. An input-dependent baseline can be computed using the input sequence at training time for variance reduction in input-driven MDPs. Two lemmas are stated for analysis, showing conditional independence and the policy gradient theorem. Theorems are provided in appendices for reference. The policy gradient theorem is generalized in Equation FORMULA3, with \u03c1 \u03c0 (\u03c9, z) representing a joint distribution over observations and input sequences. An input-dependent baseline is used for variance reduction in input-driven MDPs. The proof relies on the conditional independence of the input process and action given the observation. The policy gradient theorem is shown to be unbiased with input-dependent baselines, also applicable to TRPO. The optimal baseline for variance reduction is derived using the trace of the covariance matrix. The input-dependent baseline minimizes variance in policy gradient by using a simpler alternative to Equation FORMULA7. It provides the expected return given observation \u03c9 t and input sequence z t:\u221e. Input-dependent baselines are effective for reducing variance in policy gradient methods in input-driven environments and can be applied to A2C, TRPO, and PPO. Input-dependent baselines improve policy optimization in input-driven environments by minimizing variance in policy gradient. They are effective for reducing variance in methods like A2C, TRPO, and PPO. This approach is different from adversarial RL and meta-policy adaptation, focusing on learning a single policy without adaptation. However, input-dependent baselines can enhance policy optimization in adversarial RL and meta-policy adaptation methods. Input-dependent baselines can improve policy optimization in input-driven environments by reducing variance in policy gradient methods like A2C, TRPO, and PPO. While LSTM models were initially considered for training such baselines, they proved to be inefficient in high-dimensional spaces. However, in applications where input sequences can be repeated during training, learning the baseline becomes more efficient. Input-dependent baselines can enhance policy optimization in input-driven environments by reducing variance in policy gradient methods. When input sequences can be repeated during training, learning the baseline becomes more efficient. Two approaches are presented to exploit input-repeatability for efficient learning of input-dependent baselines. The text discusses using multiple value networks with independent parameters and a single policy network for training. The approach is not scalable for tasks requiring training over a large number of input variations. A meta-learning approach is presented to enable learning across different input sequences. The text discusses a meta-learning approach for training with an unbounded number of input sequences using a \"meta value network\" model. This method involves customizing the value network for specific input sequences and using it to train the policy network parameters. The implementation utilizes Model-Agnostic Meta-Learning (MAML) for training the meta input-dependent baseline for policy-based methods. The training algorithm involves generating input sequences and customizing the meta value network with k rollouts. The policy is updated using the customized value network, and the meta value network is updated accordingly. The process is repeated with different rollouts to adapt the meta value network. Our experiments show that input-dependent baselines consistently improve performance in various environments. Using different rollouts, we adapt the meta value network and compute baselines to update the policy network parameters. This approach outperforms standard state-dependent baselines, achieving 25%-3\u00d7 better testing rewards. Videos of our experiments can be found at https://sites.google.com/view/input-dependent-baseline/. In robotic control tasks, input-dependent baselines are evaluated using the MuJoCo physics engine in OpenAI Gym. Different external inputs are added to environments like Walker2d and HalfCheetah to improve performance. For example, a 2D walker deals with varying wind forces to adapt and walk forward while maintaining balance. Another scenario involves a half-cheetah running on floating tiles with random buoyancy. The half-cheetah runs on floating tiles with random buoyancy, affecting its dynamics. The robotic arm tracks a moving target with seven degrees of freedom. The HalfCheetah environment is a POMDP, while the Walker2d environment is fully observable. Results show that TRPO with a state-dependent baseline performs poorly in all environments, while the input-dependent baseline improves performance by up to 3\u00d7 in unseen testing environments. The agent learns to adapt to various challenges such as headwinds, different buoyancy levels, and tracking moving objects. The meta-baseline eventually outperforms 10-value networks by effectively learning from a large number of input processes. The input-dependent baseline technique improves policy performance in various environments, including load balancing and bitrate adaptation problems. Model-free reinforcement learning outperforms hand-tuned heuristics in these scenarios. In load balancing and bitrate adaptation environments, the input-dependent baselines improve policy performance compared to traditional heuristics. A2C implementation with standard value network as baseline shows unstable test rewards, while meta-baseline performs the best. Policy gradient methods can have high variance, but using a baseline has been effective in reducing it. Recent work has focused on variance reduction in MDPs, with techniques like state-action-dependent baselines and \u03bb-weighted return estimation improving performance in high-dimensional control tasks. Other approaches explore the bias-variance tradeoff in policy gradient methods using control variates. Recent work has focused on variance reduction in MDPs, with techniques like state-action-dependent baselines and \u03bb-weighted return estimation improving performance in high-dimensional control tasks. BID14 compare recent work on linear-quadratic-Gaussian tasks and complex robotic control tasks using control variates for policy gradient algorithms. Analysis of control variates for policy gradient methods is a well-studied topic, with potential for future work in input-driven MDP settings. Other approaches have proposed new RL training methodologies for environments with disturbances, such as adapting policies to different disturbance patterns through meta-learning. RARL BID3 improves policy robustness by co-training an adversary to generate a worst-case noise process. This work aims to enhance policy optimization in the presence of inputs like disturbances through input-driven Markov Decision Processes. In input-driven Markov Decision Processes, stochastic input processes affect state dynamics and rewards. Using an input-dependent baseline can reduce variance for policy gradient methods, improving training stability and policy quality. This approach is beneficial for various domains, including queuing networks, computer systems, and robotics control with disturbances or obstacles. Meta-learning is efficient for learning input-dependent baselines when input sequences can be repeated during training. Investigating architectures for cases where input processes cannot be repeated is essential. In input-driven Markov Decision Processes, using an input-dependent baseline can improve policy gradient methods by reducing variance. This approach is beneficial for various domains with stochastic input processes. Consider a 1D grid world scenario where a walker's position is perturbed by exogenous inputs, and the goal is to move forward. The policy is parametrized as \u03c0 \u03b8 (a t = +1|s t ) = e \u03b8 /(1+e \u03b8 ), with \u03b8 initialized to 0. The objective is to learn the optimal policy using policy gradient methods. In input-driven Markov Decision Processes, an input-dependent baseline can reduce variance in policy gradient methods. This is illustrated by evaluating the variance of the policy gradient estimate at the start of training under different baselines. The standard baseline results in a value function of 0 at all states, while an input-dependent baseline captures the average reward for a fixed input sequence. In input-driven Markov Decision Processes, an input-dependent baseline can reduce variance in policy gradient methods by capturing the average reward for a fixed input sequence. The variance reduction is proportional to the variance of the external input, which can dominate the overall variance in policy gradient estimation. An input-driven decision process in Markov Decision Processes can be fully observable or partially observable. The conditions in FIG3 determine the type of MDP, with states and actions defined accordingly. The variance reduction in policy gradient methods is influenced by the external input variance. The decision process in Markov Decision Processes can be fully observable or partially observable, depending on the conditions in FIG3. The variance reduction in policy gradient methods is affected by external input variance. The variance of the policy gradient estimate is minimized by setting the baseline to the minimizer of a quadratic equation. Input-dependent baselines are shown to be bias-free for Trust Region Policy Optimization (TRPO). An LSTM-based input-dependent baseline does not significantly improve performance over a standard state-dependent baseline for certain environments. Stochastic gradient descent may not guarantee consistent policy improvement in complex control problems, leading to the use of TRPO as an alternative approach. Trust Region Policy Optimization (TRPO) offers monotonic policy improvements with better sample efficiency and performance by maximizing a surrogate objective subject to a KL divergence constraint. Generalizing TRPO to input-driven environments involves discounted visitation frequency and input sequences, with an input-dependent baseline not altering the optimal solution of the optimization problem. The input-dependent baseline, such as the LSTM model, does not significantly improve policy performance in certain environments compared to a standard state-only baseline. In contrast, the MAML based input-dependent baseline reduces variance in policy gradient estimation more effectively and consistently achieves better policy performance. The input-dependent baseline, like the LSTM model, does not greatly enhance policy performance in some environments compared to a standard state-only baseline. However, the MAML-based input-dependent baseline reduces variance in policy gradient estimation more effectively and consistently achieves better policy performance. In a POMDP experiment with the Walker2d environment, removing the wind force from the agent's observation slightly decreases performance compared to the MDP case, where the agent can directly observe the wind. Input-dependent baselines improve policy performance and reduce variance in this POMDP environment. The MAML-based approach achieves the best performance by efficiently computing input-dependent baselines using multiple value networks. Algorithm 2 details training multi-value baselines for policy-based methods, balancing jobs over servers to minimize completion time. In a job scheduling scenario with heterogeneous processing rates, the goal is to minimize average job completion time by distributing jobs over servers. The system operates at 90% load, with jobs following a Pareto distribution and arriving in a Poisson process. The observed state includes job sizes and queue lengths, and the heuristic of joining the shortest queue is commonly used. Understanding workload patterns can lead to better policies, such as reserving servers for small jobs. The curr_chunk discusses the vector representing job size and work in each queue, with actions scheduling jobs and rewards based on active jobs and time elapsed. It also mentions bitrate adaptation for video streaming, where rewards are based on video resolution, rebuffering time, and bitrate changes. The observed state includes bandwidth history, video buffer size, and current bitrate. The observed state in the curr_chunk contains bandwidth history, current video buffer size, and current bitrate. State-of-the-art heuristics for BID7 achieve better testing rewards than PPO. The approach involves estimating network bandwidth and using model predictive control to select the optimal bitrate. In a discrete-action environment, 10-value networks and a meta-baseline are built using MAML on top of OpenAI A2C implementation BID7. The policy is trained with 16 parallel agents using ReLU activation function and Adam optimizer. The learning rate is set at 1e-3 and the entropy factor decays linearly from 1 to 0.001 over 10,000 iterations. Disturbance is introduced in the continuous-action robot control environments. For example, in the walker with wind scenario, a wind force is randomly sampled and Gaussian noise is added at each step. The episode terminates when the walker falls. The number of piers in the environment was increased from 10 to 50 to keep the agent on the pathway longer. The target in the 7-DoF robot arm environments is initialized randomly within specific coordinates and perturbed with Gaussian noise. The episode length is capped at 1,000 for all environments. Multi-value networks and meta-baseline were built on top of the TRPO implementation by OpenAI. GAE enhancement was turned off for fair comparison, with \u03bb = 1 showing minimal performance difference. The policy network uses \u03b3 = 0.99 for all environments. The policy network in the environment uses \u03b3 = 0.99. It has two hidden layers with 128 and 64 neurons each, using ReLU activation function. The KL divergence constraint \u03b4 is 0.01, and the learning rate for value functions is 1 \u22123. Input-dependent baselines with PPO show improved policy performance in MuJoCo environments, with the meta-learning approach achieving the best results. The meta-learning approach achieves the best performance by not being restricted to fixed input sequences during training. The input-dependent baseline technique, implemented through MAML, improves policy optimization for both TRPO and RARL. It is generally applicable to a range of policy gradient based methods. Our work improves policy optimization in the presence of noise, complementing adversarial reinforcement learning methods. Adversarial methods focus on generating worst-case noise, while we focus on enhancing policy optimization itself. Empirical results show that high-variance noise challenges controllers, highlighting the need for our approach. In the presence of noise, our approach enhances policy optimization, complementing adversarial reinforcement learning methods. Results show that our input-dependent baseline improves policy optimization, boosting the performance of TRPO and RARL. This approach is complementary to adversarial RL methods and helps improve policy adaptation. The Clavera et al. (2018a; b) propose a model-based meta-policy optimization approach (MB-MPO) for quick policy adaptation to new environments. In contrast, the goal of our approach is to learn a single policy for stochastic input processes without model discrepancy between training and testing. Our work focuses on learning a single policy for stochastic input processes without model discrepancy between training and testing. We implement an input-dependent baseline using MAML to improve policy optimization for TRPO and MPO. The input-dependent baseline enhances policy adaptation in specific input instances, boosting performance. In our study, we utilize the meta-policy adaptation technique proposed by Clavera et al. (2018b) to improve policy performance in a Walker2d environment with wind disturbance. By applying the MAML algorithm, we aim to adapt the policy to each unique wind instantiation, bypassing supervised learning for system dynamics and using the simulator for rollouts. The TRPO algorithm is used for policy optimization, with the meta-policy adaptation algorithm being MAML (Finn et al., 2017). The meta-policy adaptation algorithm MAML (Finn et al., 2017) is utilized to specialize the meta-policy for each input process instantiation through ten gradient steps. The experiment results show improved policy performance in the learning curve, with MPO outperforming TRPO. Using an input-dependent baseline improves performance for TRPO and MPO. MPO trained with the input-dependent baseline outperforms the single TRPO policy. The policy adaptation curve shows the testing performance of the adapted policy at each step, with the meta policy performing well even without adaptation. Specializing the meta-policy for each input instance further improves performance in the environment."
}