{
    "title": "Hkl8EILFdN",
    "content": "We introduce a tool called Interactive Visual Exploration of Latent Space (IVELS) for model selection, focusing on evaluating generative models of discrete sequences from a continuous latent space. Our model-selection pipeline compares and filters models using complex metrics, specifically targeting the variational auto-encoder family in a case study of modeling peptide sequences. The tool enables exploration of metrics, analysis of the learned latent space, and selection of the best model for a given task. Visual comparison can help evaluate how well unsupervised auto-encoders capture attributes in their latent space, particularly in text generation. Variational auto-encoders (VAE) and variations have successfully addressed this problem. This approach is also relevant in areas like drug discovery, where molecules can be represented as linear sequences. Our focus is on modeling peptide sequences using VAE, defining the sequence representation as a latent variable modeling problem. The text discusses the training objectives of auto-encoders, focusing on the reconstruction of input and regularization in the latent space. It highlights the trade-off between meaningful representations and the ability to sample new data points. Success measurement requires consideration of various metrics, detailed in Section 2.2, and incorporated in the IVELS tool (Section 5.1 and 5.2). Generating discrete sequences with user-specific attributes, like peptide sequences, requires careful consideration. When generating sequences with specific attributes, conditional generation is crucial. The IVELS tool allows for model filtering based on performance metrics and interactive exploration of attributes in a 2D latent space projection. Limiting the training set to desired attributes may not be feasible due to lack of labeled data. Conditional generation is essential when generating sequences with specific attributes. The IVELS tool facilitates model filtering and interactive exploration of attributes in a 2D latent space projection. Training models on labeled data may restrict them to a specific domain, while unlabeled sequence data is more freely available. A VAE can be trained on a large corpus without attribute labels, utilizing the latent space structure for conditional generation based on attributes introduced post-hoc. The encoding of data subsets corresponding to specific attributes is crucial for the IVELS tool. The paper presents a visual tool for interactive exploration and selection of auto-encoder models, allowing comparison of different models and visualization of key metrics in the latent space. It enables the discrimination of attributes in the latent space and facilitates visual exploration of attributes with 2D projections. The tool also allows the definition of new ad-hoc attributes to assist users in understanding the learned latent space. Using IVELS, we observed the application of auto-encoders in peptide modeling and various VAE models. Auto-encoders map input to a latent space for reconstruction, with regular AEs achieving high accuracy with sparsity constraints but lacking in data sampling. VAEs, on the other hand, incorporate a probabilistic framework that allows for easy sampling by constraining the encoding distribution to a simple prior distribution. The encoder in VAE specifies a diagonal Gaussian distribution for p(z). The standard VAE objective includes a Kullback-Leibler divergence term. To address posterior collapse, a multiplier \u03b2 is introduced in the weight of the second KL term. Tuning \u03b2 affects the trade-off between representation and reconstruction. The VAE family includes variations like Wasserstein auto-encoders (WAE) and adversarial auto-encoders (AAE). WAE optimizes an optimal transport plan through the encoder-decoder pair, while AAE uses a discriminator. Regularizing the encoder variance is beneficial in training with MMD or AAE. Default model architectures consist of a bidirectional GRU encoder and a GRU decoder with skip-connections between the latent code z and decoder. The model evaluation metrics for VAEs include reconstruction log likelihood, MMD between encodings and prior, encoder log variance, BLEU score, and perplexity. Different decoder options like deconvolutional decoder can be used to prevent latent variable collapse. The text discusses using sample-based metrics like BLEU and perplexity with beam search decoding in VAEs. It also explores the concept of activity and encoder variance to evaluate the usefulness of latent dimensions in encoding information. The total covariance in the usual parametrization of the encoder is calculated. The text discusses the total covariance in the parametrization of the encoder in VAE models, focusing on unconditionally trained models. It also mentions the challenges of incorporating attribute or label information during VAE training. The text discusses a strategy for dealing with missing labels in semi-supervised learning by training a sequence auto-encoder model unconditionally and defining attributes in the latent z-space. This approach allows for interactive specification of new attribute groups or subsets of data without the need for retraining the VAE. In semi-supervised learning, a strategy is discussed for handling missing labels by training a sequence auto-encoder model unconditionally and defining attributes in the latent z-space. This approach enables interactive specification of new attribute groups or data subsets without the need for VAE retraining. The subset marginal posterior distribution, p S (x), is crucial for analyzing how a specific attribute's distribution is encoded in the latent space and for sampling conditionally from q S \u03c6 (z). In semi-supervised learning, a strategy is discussed for handling missing labels by training a sequence auto-encoder model unconditionally and defining attributes in the latent z-space. To sample from q S \u03c6 (z) conditionally, it needs to be distinct from the background distribution q \u03c6 (z) and have sufficient regularity for simple density modeling. Discriminating between attribute labels within the same category is crucial for generating sequences with specific properties. The focus is on analyzing arbitrary subsets in an interactive exploration of q S \u03c6 (z) using specific metrics of interest. The metric of interest involves discriminating subsets in the latent space based on attribute availability and attribute labels. 2D projections of the marginal posterior distributions are used to analyze how attributes cluster visually in different models. The sequences considered are composed of twenty natural amino acids, with a maximum length restriction. The vocabulary consists of 20 characters, sequences are \u2264 25 in length. Peptides have various biological functions like antimicrobial and anticancer. VAE models have been used for capturing constraints in peptide sequences and designing chemicals. This study focuses on comparing latent spaces learned by modeling peptide sequences using VAE. Feasibility of using latent space learned from unlabeled data is investigated. In this study, the feasibility of using latent space learned from a large corpus of unlabeled sequences to track distinct functional families of peptides is investigated. The focus is on five biological functions of peptides: antimicrobial, anticancer, hormonal, toxic, and antihypertensive. These peptides are seen as promising alternatives to traditional drugs, especially in the face of antibiotic resistance. The dataset includes sequences with different attributes curated from various databases. The labeled dataset includes data from publicly available databases BID25, BID9, BID15, BID2, and BID11, augmented with the unlabeled dataset from Uniprot-SwissProt and UniprotTrembl (BID5), totaling approximately 1.7 million points. The data is split into train, valid, and test sets (80%, 10%, and 10% respectively) and attributes with data are upsampled 20x during training. The tool aims to support the role of a model trainer by visualizing high-level information captured during training and iteratively focusing down to medium-level evaluation of attributes. The three-level pipeline allows users to conduct cohort-analysis of trained models, filtering based on provided information. The first level presents a side by side view of metrics across multiple models, focusing on final epoch checkpoints. This tool supports model trainers in evaluating attributes at a medium-level. The IVELS tool simplifies model comparison by displaying multiple metrics at once and allowing sorting by different metrics for easy selection of promising models. It visually represents the balance between activity and noise dominance in latent dimensions, aiding in model analysis. In Section 6.2, different models successfully encode attributes in the latent space. The evaluation aims to linearly separate attributes within the learned space, a prerequisite for meaningful attribute relations in level 3. Logistic regression is used to differentiate between positive/negative labels and labeled/unlabeled samples on training, validation, and test sets. In Section 6.2, various models encode attributes in the latent space. Evaluation includes linearly separating attributes within the learned space using logistic regression to differentiate between positive/negative labels and labeled/unlabeled samples on training, validation, and test sets. The results show the ability to visualize attributes with color-coding and compare different values in the latent space. In stage 2, the unconditionally trained model shows salient attributes judged by a linear classifier on latent dimensions. High performance is seen across all models, with \u03b2-VAE and AAE models performing worse on the AMP attribute. Limited annotation for water-solubility leads to overfitting, indicating a need for more annotations. Toxicity proves more challenging than other attributes. tSNE projections of the latent space learned using three models are shown in Fig. 5, setting the stage for further investigation. The latent space learned using three different models (Level 3) reveals distinct attributes of positive antimicrobial and antihypertensive peptides. Antihypertensive peptides are longer and function through enzyme inhibition, while antimicrobial peptides disrupt cell membranes. VAE shows posterior collapse, while \u03b2-VAE addresses this issue. The \u03b2-VAE addresses collapse issues in the latent space, with about half of its dimensions being active. The WAE and AAE have little encoder variance, making them almost deterministic. The WAE shows good clustering of attributes like positive antimicrobial and antihypertensive in the latent space. The Interactive Visual Exploration of Latent Space (IVELS) tool is designed for model selection, specifically focusing on auto-encoder models for peptide sequences. It can be adapted for other models without a single metric for comparison, such as latent variable models for sequences, images, or speech synthesis. This tool allows model architects and trainers to efficiently explore and select different model variations, guiding the generation of samples with desired attributes."
}