{
    "title": "S1Dh8Tg0-",
    "content": "Neural networks are commonly used for classification tasks, with a learned affine transformation at the end for per-class values. The classifier can have a large number of parameters, growing with the number of classes, requiring more resources. In this work, we propose a method to fix a classifier with minimal loss of accuracy, offering memory and computational benefits. Initializing the classifier with a Hadamard matrix can also speed up inference. This has implications for our current understanding of neural network models. Deep neural networks are widely used for machine learning tasks, with convolutional neural networks (CNNs) being the standard for image classification. Research has focused on reducing the size of networks through weight sharing, mixed precision, low rank approximations, and quantization of weights, activations, and gradients. This work proposes a method to fix a classifier with minimal loss of accuracy, using a Hadamard matrix for initialization to speed up inference. In this study, activations and gradients were quantized to reduce computation during training, but extreme compression led to a loss of accuracy. The proposal suggests that common neural network models can learn useful representations without modifying the final output layer. Convolutional neural networks (CNNs) are commonly used for spatial and temporal tasks, composed of convolutional layers, pooling layers, and fully connected layers with non-linear activation functions. Earlier CNN architectures used fully-connected layers. Modern CNN architectures have moved away from using fully connected layers at later stages, as they have been found to have minimal impact on network performance. These layers can be easily compressed and reduced after training, leading to better generalization and accuracy with fewer trainable parameters. Numerous studies have shown that CNNs can be effectively trained without relying heavily on fully connected layers. Recent studies have shown that fully connected layers in CNNs are not essential for network performance, as they can be trained without explicit classification layers. This suggests that fully connected layers are redundant and play a minor role in learning and generalization. Despite this, they are still commonly used for classification, adding to the number of trainable parameters in a linear manner with the number of classes. Recent studies have shown that fully connected layers in CNNs are not essential for network performance, as they can be trained without explicit classification layers. This suggests that fully connected layers are redundant and play a minor role in learning and generalization. In this work, it is claimed that for common use-cases of convolutional networks, the parameters used for final classification are redundant and can be replaced with a predetermined linear transform. This can lead to a significant decrease in model parameters and computational benefits, allowing for easier deployment on devices with limited computation ability and memory capacity. The use of a fixed transform in CNNs can reduce communication costs in distributed systems and allow models to scale to a large number of outputs without a linear increase in parameters. The focus is on the final representation obtained by the network before the classifier, suggesting the importance of non-linear layers in learning and generalization. The final representation obtained by the network before the classifier is crucial, emphasizing the significance of non-linear layers in learning and generalization. The transformation involves training W and b through back-propagation, with W being a matrix of N \u00d7 C for input x of N length and C possible outputs. Training utilizes cross-entropy loss and softmax activation, minimizing negative log likelihood with respect to ground-truth target t. To evaluate the importance of the final classification transformation, W is replaced with a fixed orthonormal projection Q. Normalizing the representation of x on an n-dimensional sphere helps with faster training and convergence by avoiding scale changes in weights. However, the bounded range of q i \u00b7x between -1 and 1 causes convergence issues with the softmax function. To address this, a fixed scale parameter \u03b1 can be introduced to learn the softmax scale, acting as an inverse of the softmax temperature. This eliminates the need for a separate hyper-parameter for different networks and datasets. To address convergence issues with the softmax function, a fixed scale parameter \u03b1 is introduced to learn the softmax scale, functioning as an inverse of the softmax temperature. The classifier output is adjusted accordingly, and the behavior of the \u03b1 parameter over time shows a logarithmic growth pattern. This behavior is linked to the generalization of the network. The learned classifier, as explained by BID39, converges to a max margin classifier using a single parameter. Training the network with a cosine angle loss allows discarding the softmax function, resulting in a slight decrease in validation accuracy. The use of a Hadamard matrix as the final classification transform is suggested. The final classification layer utilizes a truncated Hadamard matrix for deterministic, low-memory classification without full matrix-matrix multiplication. The matrix allows for simple sign manipulation and addition for classification. Experimental results show accuracy percentages for different models on ImageNet dataset. We experimented with Cifar10 and Cifar100 datasets using a fixed classifier approach. The Cifar10 dataset consists of 50,000 training images and 10,000 test images with 10 classes, while Cifar100 has 100 classes. We trained a residual network on Cifar10 with a depth of 56 and compared models with learned and fixed classifiers. The results showed lower training error for the learned classifier model. The network with a learned classifier has lower error, achieving the same accuracy as the fixed classifier on the validation set. The fixed parameterization prevents the network from increasing the norm of a sample's representation, requiring more effort to learn its label. Comparing a fixed scale variable \u03b1 at different values versus a learned parameter, results show similar validation accuracy. Training the \u03b1 scale parameter instead of using a fixed value incurs an additional hyper-parameter to optimize. The study trained a model on the Cifar100 dataset using the DenseNet-BC model with 100 layers and k = 12. The higher number of classes led to about 4% increase in parameters. Validation accuracy remained high, and the training curve was consistent. To validate the results, the Imagenet dataset with 1000 visual classes was used. CNNs for Imagenet typically have a hidden representation of at least 1024 dimensions. The study evaluated a fixed classifier method on Imagenet using Resnet50 and Densenet169 models, removing approximately 8% to 12% of the model parameters. Similar convergence speed and final accuracy were observed on both validation and training sets. The Shufflenet architecture BID50 was chosen for further evaluation due to its challenging classifier parameters. The study evaluated a fixed classifier method on Imagenet using Resnet50 and Densenet169 models, removing 8% to 12% of parameters. The Shufflenet architecture BID50, with 1.8 million parameters, was chosen for its challenging classifier parameters. Fixing the classifier resulted in a model with 0.86 million parameters, allowing training in an under-specified regime with more samples than parameters. This unconventional approach led to similar validation accuracy as the original model. The study evaluated a fixed classifier method on Imagenet using Resnet50 and Densenet169 models, removing 8% to 12% of parameters. TAB0 summarizes fixed-classifier results on convolutional networks, offering a drop-in replacement for learned classifiers. Language modeling with fixed classifiers can save trainable parameters by using the same weights for word embedding and classifier. Removing both classifier and embedding weights can further reduce parameters. In the study, parameters were reduced by removing classifier and embedding weights and using a fixed transform. A language model was trained on the WikiText2 dataset with a recurrent model. Results showed that using a random orthogonal transform performed poorly compared to learned embedding. Pre-trained embeddings like word2vec or PMI factorization were used to initialize the embedding layer for better results. Using fixed word2vec embeddings, the study achieved better results with 89% fewer parameters than a fully learned model. This suggests a required structure in word embedding based on semantic relatedness between words and class imbalance. Cost-effective ways to train word embeddings can narrow the gap and avoid high training costs for larger models. The increasing number of classes in benchmark datasets like Cifar100, ImageNet1K, ImageNet22k, and language modeling will lead to higher computational demands for the final classifier, which should be considered alongside the chosen architecture. The work by BID41 was utilized in the study. The study by BID41 introduced JFT-300M, an internal Google dataset with over 18K classes. They used a Resnet50 model with 36M parameters, with 60% in the final classification layer. Distributing parameters between training servers was challenging, requiring splitting them into 50 sub-layers. Our work proposes using a fixed classifier to eliminate gradient synchronization for the final layer, and using a Hadamard matrix to make the process more efficient. Our method eliminates the need to save the transformation, making it more efficient and saving memory and computational resources. It works well when the ratio of learned features to classes is small (C > N), as seen in experiments with Imagenet classification using mobilenet-0.5 BID13 and a reduced version of ResNet. Even when C > N, a fixed classifier can achieve similar validation accuracy as a fully learned classifier. However, issues may arise when classes are highly correlated. When classes are highly correlated, a fixed orthogonal classifier may struggle to learn effectively in neural network models. Recent research suggests a connection between generalization capabilities and weight-related quantities in models. The use of fixed classifiers with a single scalar variable could simplify the model and offer new insights into training and understanding neural networks. In Binarized Neural Networks BID16, fixed classifiers are used with restricted activations and weights. The norm of the last hidden layer is constant for all samples, simplifying the model. Further exploration is planned for more efficient word embedding learning methods. Removing parameters from the classification layer in deep neural networks has shown little decline in classification performance. In Binarized Neural Networks BID16, fixed classifiers are used with restricted activations and weights to simplify the model. Keeping the last layer fixed can reduce computational complexity and communication costs in training. Using a Hadamard matrix as a classifier may offer computational benefits and save memory. As datasets become more complex, resource-intensive affine transformations should remain fixed during training. Efficient methods for creating pre-defined word embeddings should be explored to avoid unnecessary parameters. Future research should focus on representations learned by the non-linear part of neural networks up to the final classifier, as it appears to be highly redundant."
}