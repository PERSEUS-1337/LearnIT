{
    "title": "BklLVAEKvH",
    "content": "We present a novel end-to-end approach for unsupervised clustering using normalized cuts as the objective function. Our model directly assigns final clusters without the need for additional processing. It achieves state-of-the-art results on various datasets like MNIST, Reuters, CIFAR-10, and CIFAR-100, outperforming existing methods by up to 10.9%. Our approach also demonstrates superior generalization capabilities, surpassing recent top-performing methods by up to 21.9%. Spectral clustering is a recent top-performing approach for clustering unlabeled data, outperforming commonly used methods like k-means. It embeds data similarity in the Laplacian's eigenspace and uses k-means to generate clusters, allowing for direct minimization of pairwise distances between data points. In this work, a framework called CNC is introduced for Clustering by learning to optimize expected Normalized Cuts. By minimizing a continuous relaxation of the normalized cuts problem, CNC enables end-to-end learning that outperforms top clustering approaches like SpectralNet. This approach produces lower normalized cut values, leading to better clustering accuracy. The CNC framework introduces a new differentiable loss function for clustering without labeled datasets. It trains a deep learning model to minimize the proposed loss, which returns probabilities of belonging to each cluster. The optimal normalized cuts value is achieved by cutting the edge connecting two triangles, with the CNC loss converging to this value. The CNC model is trained to minimize expected normalized cuts in an unsupervised manner without labeled data. It directly outputs probabilities for each data point belonging to clusters. The approach is scalable through batch processing and outperforms other learning-based clustering methods on various datasets. The CNC model outperforms other clustering methods on datasets like MNIST, Reuters, CIFAR10, and CIFAR100, showing up to 10.9% improvement over baselines. It generalizes well to unseen data points, surpassing SpectralNet by up to 21.9%. Recent deep learning approaches focus on learning representations for clustering, with DCN proposing a joint dimensionality reduction and K-means clustering approach. Recent deep learning approaches focus on learning representations for clustering, with a joint dimensionality reduction and K-means clustering approach proposed by DEC (Xie et al., 2016). Other methods utilize a variational autoencoder with a Gaussian mixture prior for data augmentation and maximizing mutual information between inputs and predicted clusters. Different clustering objectives like self-balanced k-means and balanced min-cut have also been extensively studied. Spectral clustering, k-means, and balanced min-cut are extensively studied techniques in clustering. Regularized spectral clustering aims to create more balanced clusters. Generalizing clustering to unseen nodes and graphs is challenging, but SpectralNet uses a deep learning approach to address this issue. In contrast to SpectralNet, our approach uses Laplacian's eigenspace embeddings and k-means for clustering, with a focus on minimizing normalized cuts. Our method outperforms SpectralNet, improving clustering accuracy by up to 21.9% on unseen data points. The CNC objective optimizes normalized cuts for graph clustering, with a formal definition provided. A graph G can be clustered into g disjoint sets by removing edges connecting those sets. Normalized cuts (Ncuts) based on graph conductance have been studied, with the cost of a cut computed as the total weight of the edges removed to form disjoint sets. The Ncuts clustering problem is NP-complete, and an approximation is based on eigenvectors of the normalized graph Laplacian. CNC is a neural network framework that learns to cluster without labeled examples by minimizing the continuous relaxation of normalized cuts. The end-to-end training of CNC involves data points embedding and clustering steps. The new differentiable loss is introduced for the clustering step. In the clustering step of the CNC model, a new differentiable loss function is introduced to train the model. The goal is to learn assignment probabilities over clusters using embeddings. The CNC model is implemented with a neural network, and a loss function based on output probabilities is used to calculate expected normalized cuts. The CNC model uses an affinity graph to train a neural network for cluster assignments based on embeddings. The output probabilities determine the cluster assignment for each data point. The CNC model utilizes an affinity graph to train a neural network for cluster assignments based on embeddings. Equation 3 is rewritten to consider only adjacent nodes with a weight matrix (W). The total edge weights (w_ij) from nodes in a set (S_k) are calculated using vol(S_k, V). The expected normalized cuts are found by updating Equation 3 with a matrix representation in Equation 6. The CNC model is implemented using a neural network with network weights (\u03b8) optimized via backpropagation to minimize Equation 7. The CNC model uses an affinity graph to train a neural network for cluster assignments based on embeddings. The model optimizes Equation 7 via backpropagation and minimizes the loss on randomly sampled minibatches of data. The final assignment of a data point to a cluster is determined by the arg max of the model's output. In this section, the use of Siamese networks to learn embeddings for spectral clustering is discussed. The network is trained to capture affinities between data points by learning an adaptive nearest neighbor metric based on euclidean proximity. The goal is to generate embeddings where adjacent nodes are closer in the space and non-adjacent nodes are further apart. The performance of CNC is evaluated on real world datasets like MNIST, Reuters, CIFAR-10, and CIFAR-100. MNIST contains 70,000 gray-scale images of handwritten digits. Reuters dataset consists of English news labeled by category. The data is split randomly for evaluation and the impact of training data size is investigated. The generalization ability of CNC is evaluated on datasets like CIFAR-10 and CIFAR-100, with different training data size splits. CIFAR-10 has 60000 color images in 10 classes, while CIFAR-100 has 100 classes with 600 images each. The number of clusters is assumed in all runs, with comparisons made to other approaches like SpectralNet, DEC, DCN, VaDE, DEPICT, IMSAT, and IIC. The curr_chunk discusses various approaches, including the use of a variational autoencoder and deep learning for spectral clustering. Results for different methods on datasets like MNIST, Reuters, CIFAR-10, and CIFAR-100 are compared using clustering accuracy and normalized mutual information measures. The curr_chunk discusses the evaluation of clustering accuracy using ACC and NMI metrics, without using true labels in training or testing. ACC is calculated based on true labels and predicted clusters, while NMI measures mutual information and entropy between labels and clusters. A Siamese network is trained to learn embeddings representing data affinity based on k-nearest neighbors. In Table 1, clustering performance is compared across four benchmark datasets using a deep learning approach called CNC. CNC outperforms existing approaches on ACC and NMI metrics, showing significant improvements on CIFAR-10, Reuters, and CIFAR-100 datasets. The results suggest the effectiveness of using deep learning to optimize normalized cuts for clustering. In an ablation study, the impact of embeddings on clustering performance was evaluated using CNC. Results show that adding embeddings improves performance on MNIST and Reuters datasets. Even without embeddings, CNC outperforms SpectralNet. Generalization ability was tested by splitting data randomly and training on a 90%-10% split, showing CNC's superiority. Among seven methods in Table 1, only SpectralNet can generalize to unseen data points. CNC outperforms SpectralNet in most datasets by up to 21.9% on ACC and up to 10.7% on NMI. CNC shows better cuts than SpectralNet in clustering tasks, as seen in the numerical value of Normalized cuts in Table 3. The CNC outperforms SpectralNet in most datasets by up to 21.9% on ACC and up to 10.7% on NMI. The normalized cuts of CNC are slightly better for datasets with marginal improvement, while significantly smaller for datasets with improved accuracy like CIFAR-10 and CIFAR-100. The CNC loss function proves to be effective for clustering tasks, as shown by higher accuracy and smaller normalized cuts. Varying the size of the training dataset affects generalization, with only slight changes in accuracy when reducing training data to 90%. Experimentation on the Routers dataset was conducted by randomly dividing the data. The experiment on the Routers dataset involved dividing the data randomly into different splits (90%-10%, 70%-30%, 50%-50%, 20%-80%, and 10%-90%) to train the CNC model. The ACC and NMI of CNC increased as the size of the training data increased, with only a slight decrease in performance when using 10% training data compared to 90%. The CNC model details for the MNIST dataset include a Siamese network with 4 layers sized [1024, 1024, 512, 10] with ReLU. The Siamese network for different datasets includes various layers and sizes with specific activation functions and clustering modules. Parameters such as batch size, nearest neighbors, optimizer, learning rate, temperature, and minimum values are also specified for each dataset. The Siamese network has different configurations for CIFAR-100 and Reuters datasets, with specific layer sizes, activation functions, and clustering modules. Parameters such as batch size, nearest neighbors, optimizer, learning rate, temperature, and minimum values are adjusted accordingly for each dataset. Hyper-parameter sensitivity analysis is conducted on the Reuters dataset to study the impact of varying hyperparameters. The CNC model benefits from tuning hyperparameters like hidden layers, learning rate, batch size, and nearest neighbors. Other hyperparameters like decay rate, patience, Gumbel-Softmax temperature, and minimum temperature have less impact. Different configurations yield varying accuracies on the Reuters dataset. CNC (Clustering by learning to optimize Normalized Cuts) achieves state-of-the-art results on unsupervised clustering benchmarks (MNIST, Reuters, CIFAR-10, CIFAR-100) and outperforms baselines by up to 10.9%. It also enables generation, with up to 21.9% improvement over SpectralNet. The model is trained on VGG and validated on MNIST-conv, showing strong generalization to unseen TensorFlow graphs like ResNet, Inception-v3, and AlexNet. CNC (Clustering by learning to optimize Normalized Cuts) achieves state-of-the-art results on unsupervised clustering benchmarks and outperforms baselines. The model is trained on VGG and validated on MNIST-conv, showing strong generalization to unseen TensorFlow graphs like ResNet, Inception-v3, and AlexNet. Inference results show that GraphSAGE-on generalizes better than other models, with the importance of graph embeddings highlighted in Table 4. The node features are based on operation types in TensorFlow. The model CNC achieves state-of-the-art results on unsupervised clustering benchmarks by leveraging GCN and GraphSAGE to capture similarities across graphs. GraphSAGE-on achieves the best performance and generalizes better than other models when tested on different graphs like AlexNet, ResNet, and Inception-v3. The goal is not to beat existing graph partitioning algorithms but to show promise in generalization without heuristics. The model CNC achieves state-of-the-art results on unsupervised clustering benchmarks by leveraging GCN and GraphSAGE to capture similarities across graphs. The best performing model, GraphSAGE-on, has specific architecture details such as input feature dimension of 1518, 5 layers sized 512 with shared pooling, and a graph clustering module with 3 layers sized 64. ReLU, Xavier initialization, and Adam optimizer with lr = 7.5e-5 are used."
}