{
    "title": "HkeYCNgooQ",
    "content": "In a study on speech synthesis networks, researchers trained a network bilingually in English and Korean to analyze how it learns phoneme pronunciation across languages. Results showed that phoneme embedding vectors are closer if pronunciations are similar. This suggests the possibility of synthesizing speech in one language using a network trained in another language. The study trained a cross-lingual TTS model using English and Korean datasets to analyze phoneme embeddings across languages. Results showed phonemes with similar pronunciation are closer, indicating the potential for generalizing to resource-poor languages with abundant data. The study compared CER of generated speeches from models trained with varying data amounts for resource-scarce languages. The study successfully trained a cross-lingual multi-speaker TTS model using English and Korean data without bilingual speakers. They demonstrated the amount of data needed to train a TTS model when large amounts of data from another language are available. The spectrogram of generated speech using English and Korean phonemes was evaluated, with the average CER of transcriptions reported. Samples from each model were shared on a demo page, with IPA symbols used for phonemes."
}