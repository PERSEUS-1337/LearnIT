{
    "title": "rygwBRgYs7",
    "content": "Recent neural network designs have primarily focused on convolutional layers, but structured efficient linear layers (SELLs) have not been applied to this setting. A method using diagonal matrices, discrete cosine transforms (DCTs), and permutations has been introduced to optimize weight tensors in convolutional layers. Networks with structured efficient convolutional layers (SECL) outperform low-rank networks and show competitive computational efficiency. This approach reduces the size of linear layers and operations in convolutions while maintaining network structure. In this work, a Structured Efficient Convolutional Layer (SECL) is introduced, utilizing diagonal matrices, permutations, and discrete cosine transforms (DCTs) to optimize weight tensors in convolutions. The SECL approach reduces the size of linear layers and operations in convolutions while maintaining network structure, outperforming low-rank networks in terms of computational efficiency. The SECL approach introduces diagonal matrices, permutations, and DCTs to optimize weight tensors in convolutions. By using a riffle shuffle with a deck of cards, it can be evaluated faster than a fixed random permutation. Implementing this parameterization into convolutional layers poses a challenge due to non-square kernel matrices, except for pointwise convolutions where input channels match the output. To increase channels, the input is repeated along the channel dimension in integer steps, allowing for the implementation of almost all pointwise convolutions. Depthwise separable convolution, achieved by preceding pointwise with grouped convolution, serves as a substitute for convolution. The ACDC parameterization for each filter can be used as a substitute for convolution, but there is not much benefit. The motivation for ACDC layers comes from their complex equivalent using Fourier transforms. Machine learning systems typically operate using real numbers, leading to the decision to compress the strategy. The effectiveness of this compression strategy is demonstrated in experiments in Section 4.1."
}