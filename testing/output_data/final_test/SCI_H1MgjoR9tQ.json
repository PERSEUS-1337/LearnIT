{
    "title": "H1MgjoR9tQ",
    "content": "Continuous Bag of Words (CBOW) is a powerful text embedding method known for its ability to encode word content efficiently. However, it lacks the capability to capture word order due to the commutative nature of its word embeddings. To address this limitation, a new approach is proposed. The Continual Multiplication of Words (CMOW) algorithm is an adaptation of word2vec designed to capture linguistic properties. A hybrid CBOW-CMOW model combines the strengths of both approaches, improving word content memorization and linguistic information encoding by 8%. This hybrid model outperforms CBOW on 8 out of 11 supervised tasks with an average improvement of 1.2%. Word embeddings have been a significant contribution to natural language processing in recent years. Recent advancements in natural language processing have focused on universal sentence embeddings, with encoding architectures based on recurrent neural networks or the Transformer architecture. While these techniques are more expensive to train and apply compared to word embeddings, more efficient techniques like Continuous Bag of Words (CBOW) have shown strong results on various tasks. Despite its simplicity, CBOW remains effective with the use of sophisticated weighting schemes. The paper proposes a method to enhance aggregated word embeddings by incorporating word order awareness, addressing the limitation of CBOW approaches that solely rely on addition. This is crucial for tasks like sentiment analysis where word order plays a significant role. The paper introduces a Compositional Matrix Space Model (CMSM) to improve word embeddings by considering word order through matrix multiplication. This approach overcomes the limitations of addition-based techniques like CBOW, allowing for order-aware encodings in language modeling. The paper introduces a Compositional Matrix Space Model (CMSM) for word embeddings, overcoming limitations of vector-based models. A new unsupervised training scheme called Continual Multiplication Of Words (CMOW) is proposed, showing improvements over CBOW in capturing linguistic properties. The paper introduces a Compositional Matrix Space Model (CMSM) for word embeddings, proposing a hybrid CBOW-CMOW model that improves performance on linguistic probing tasks by 8%. The hybrid model outperforms CBOW on supervised downstream tasks, showing an average improvement of 1.2% on these tasks and 0.5% on unsupervised tasks. Key contributions include an unsupervised training scheme for the Compositional Matrix Space Model. The paper introduces the Compositional Matrix Space Model (CMSM) with an initialization strategy and training objective designed for training CMSMs. The resulting embedding model complements classical CBOW embeddings and a hybrid model combining both approaches is shown to be superior. The algorithm for learning the weights of the CMSM is presented, with only a few previous studies addressing this aspect. The paper introduces the Compositional Matrix Space Model (CMSM) for sentiment analysis. Other works have represented words as matrices, but not within the CMSM framework. Different approaches like BID11, BID29, and BID3 have been explored, with varying opinions on the effectiveness of CMSMs compared to traditional embeddings. Sentence embeddings are a current area of active research. Sentence embeddings are a key focus in current research, with a framework introduced by BID4 to evaluate their performance on downstream tasks and linguistic properties. Different methods include large RNNs, convolutional neural networks, and efficient aggregation of word embeddings. Sent2Vec stands out for incorporating bigrams in their approach. Incorporating bigrams, Sent2Vec introduces a unique approach to sentence embeddings. They use a composition function of addition, unlike our CBOW and CMOW encoders. Our study focuses on adapting the word2vec objective for the CMSM, discussing training objectives, initialization strategy, and the hybrid model. We utilize a lookup table for word matrices in our framework. The study introduces a novel approach to sentence embeddings using a composition function of addition. The model, called CMSM, employs a training objective similar to word2vec and utilizes a lookup table for word matrices. The encoding step, denoted as enc, aggregates word embeddings into a sentence embedding, making it a general-purpose text encoder. The model is well-defined for any non-zero sequence length and can be passed to the next layer for further processing. The text discusses a training objective similar to word2vec, focusing on maximizing the conditional probability of a word in a specific context. It involves selecting a target word from a window of tokens around a given word and using negative sampling to distinguish the target word from noise words. Logistic regression is used to predict context words and noise words, with a specific training objective in mind. The negative sampling training objective in word2vec is modified to select a random output word from the window, forcing the model to build a semantically rich representation of the whole sentence. This modification leads to a significant improvement in downstream tasks for CMOW but not for CBOW. The appendix discusses algorithms for learning parameters in CMSMs. BID34 and BID1 propose complex initialization strategies, while the proposed strategy suggests initializing matrices close to the identity matrix. This approach leverages modern optimizers like stochastic gradient descent to find optimal solutions in non-convex optimization problems. CMOW is a deep linear neural network with flexible layers, each corresponding to a word. The text discusses the issue of vanishing values in word embeddings when using random weight initialization in a deep linear neural network. It proposes a new initialization strategy to prevent this problem and ensure consistent representation of sentences regardless of length. The text proposes a new initialization strategy for word embedding matrices to prevent vanishing values in deep linear neural networks. This strategy shows a 2.8% improvement over Glorot initialization for training CMSMs. The approach of training CBOW and CMOW separately and concatenating sentence embeddings did not work well in preliminary experiments due to overlapping features learned by each model. The text discusses training CBOW and CMOW models jointly to improve performance on downstream tasks. Experiments were conducted to evaluate the proposed models for training CMSMs, with results presented on linguistic probing and downstream tasks. Each batch was created by selecting 1,024 sentences from the corpus and tokenizing them, with a maximum of 30 words selected from each sentence. The study involved training CBOW and CMOW models on the UMBC news corpus BID12, consisting of 134 million sentences and 3 billion tokens. Each sentence had an average of 24.8 words. To limit batch size, only 30 samples per sentence were used, with 0.1% of sentences reserved for validation. The study trained CBOW and CMOW models on the UMBC news corpus with 134 million sentences and 3 billion tokens. Validation was done on 0.1% of sentences, with training terminating after 1,000 updates and 10 consecutive validations of no improvement. The vocabulary was limited to 30,000 words, and optimization was carried out using Adam with an initial learning rate of 0.0003. Five different models were trained, including CBOW and CMOW with different dimensions, and a Hybrid CBOW-CMOW model. The study trained the Hybrid CBOW-CMOW model with 800 parameters per word in the lookup tables. Results of the 400-dimensional H-CBOW and H-CMOW models were compared to the 800-dimensional Hybrid method. The encoder of the model enc E \u2206 was retained for evaluation on linguistic probing tasks and downstream performance tasks. The logistic regression classifier is trained on top of embeddings. CBOW and CMOW show complementary results in linguistic probing tasks. CBOW excels at word content memorization, while CMOW outperforms CBOW in other tasks. The hybrid model performs close to or better than the individual models on all tasks, with an average improvement of 8%. The hybrid model shows small differences compared to CMOW, with the largest loss on the CoordInv task but an overall gain of 1.6%. The jointly trained H-CMOW and H-CBOW models, along with their separately trained 400-dimensional counterparts, show differences in word content memorization abilities. H-CMOW exhibits improvements in certain tasks, while CBOW outperforms CMOW in most supervised downstream tasks. The joint model performs closely to the individual models, with some tasks showing more than a 1-point improvement. The CMOW model shows improvements in certain tasks, but CBOW outperforms CMOW in most supervised downstream tasks. The hybrid model reduces the performance gap between CBOW and CMOW on unsupervised tasks, even outperforming CBOW on some tasks. The CMOW model produces sentence embeddings comparable to fastSent BID14 and benefits from an improved initialization strategy. Our novel training scheme for the Compositional Matrix Space Model of language improves downstream performance by 2.8% compared to Glorot initialization. Choosing the target word at random boosts CMOW performance on downstream tasks by 20.8% on average. CMOW embeddings encode linguistic properties better than CBOW, but CBOW excels at word content memorization. CMOW outperforms CBOW on the TREC question type classification task with an accuracy of 88.0 compared to 85.6. Our hybrid model, H-CMOW, shows an 8% improvement on average compared to CBOW, focusing on linguistic properties rather than word content. This results in a 1.2% overall performance improvement on 11 downstream tasks, with significant enhancements in sentiment analysis tasks. The hybrid model H-CMOW shows an 8% improvement on average compared to CBOW, focusing on linguistic properties rather than word content. This results in a 1.2% overall performance improvement on 11 downstream tasks, with significant enhancements in sentiment analysis tasks. The improvements are seen in tasks such as sentiment analysis (SST2, SST5), question classification (TREC), and sentence representation benchmark (STS-B), which rely on word order information. However, the model does not outperform CBOW on tasks that mainly depend on word content memorization. It is noted that the models in the study do not represent the state-of-the-art for sentence embeddings, as better scores are achieved by LSTMs and Transformer models trained on larger corpora and vocabularies. In the present study, we compare CBOW, CMOW, and the hybrid model in a controlled scenario. Our analysis provides insights on the models' learning when trained separately or jointly. We introduce an order-aware extension to bag-of-words embedding algorithms, with CMOW embeddings computed as efficiently as CBOW embeddings. The matrix embedding approach of CMOW allows for faster sentence encoding compared to CBOW, utilizing GPU matrix multiplication. This method offers theoretical advantages over RNNs, requiring fewer sequential steps for encoding sequences. The approach is well-suited for large-scale, time-sensitive text encoding applications and can be combined with other existing models in a hybrid approach. Our hybrid model combines CMOW with fastText BID21 for efficient unsupervised learning of word order aware Compositional Matrix Space Model. The resulting sentence embeddings capture linguistic features complementary to CBOW embeddings, improving downstream task performance, especially for tasks dependent on word order information. The model bridges the gap between simple word embedding based encoders and non-linear recurrent encoders. Code for the model is available at https://github.com/florianmai/word2mat. The text discusses a modified training objective for learning sentence embeddings that capture semantic information. The objective involves sampling the target word randomly from a uniform distribution, rather than setting it as the center word from a window of tokens. Experimental results on linguistic probing tasks are presented to evaluate the effectiveness of this modified objective. The results of linguistic probing tasks show that CMOW-R outperforms CMOW-C on most tasks, with a 20.8% relative improvement on downstream tasks. CBOW-R scores slightly lower than CBOW-C on average across all tasks. Additionally, a novel random initialization strategy is proposed for training CMSMs in Section 3.3. The experimental results show that a novel initialization strategy for training CMSMs outperforms Glorot initialization, leading to significant improvements in word content memorization and downstream task performance. On average, the relative improvement compared to Glorot initialization is 2.8%."
}