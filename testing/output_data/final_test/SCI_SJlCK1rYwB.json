{
    "title": "SJlCK1rYwB",
    "content": "Several studies have shown that strong natural language understanding models can be biased towards dataset biases, leading to poor generalization and performance in real-world scenarios. To address this, we propose learning strategies to train neural models that are more robust to biases and transfer better to out-of-domain datasets. Our approach includes an additional bias-only model to adjust the base model's loss and focus on hard examples, reducing the impact of biased data. These methods are model agnostic and simple to implement, with experiments conducted on large-scale natural language datasets for validation. Our debiased models significantly improve robustness on various datasets, gaining points on FEVER, HANS, and SNLI hard set. These datasets assess model robustness in out-of-domain settings without typical biases in training data. Recent neural models have achieved high performance on natural language understanding benchmarks, but they tend to rely on dataset biases and superficial correlations to perform well without truly learning the task. Large-scale NLI benchmarks have been shown to contain annotation artifacts. Large-scale NLI benchmarks have annotation artifacts, where certain words in the hypothesis can indicate the inference class, allowing models to perform well without truly understanding the relationship between premise and hypothesis. Models can rely on statistical cues, like linking negation words with the contradiction label. This leads to poor performance on out-of-domain datasets. Recent studies are creating new evaluation datasets to address these biases. Recent studies aim to create unbiased evaluation datasets by reducing reliance on biases during neural model training. Proposed debiasing techniques adjust cross-entropy loss to down-weight biased examples, focusing on learning hard examples instead. This approach prevents NLI models from predicting labels based on existing biases. Our strategy involves adding a bias-only branch on top of the base model during training to prevent NLI models from predicting labels based on existing biases. Three main debiasing strategies are proposed, including ensemble methods to combine predictions and adjust training loss to reduce bias influence. The proposed debiasing strategies involve adding a bias-only branch on top of the base model during training to prevent biased predictions. These strategies adjust the training loss to focus on hard examples and prevent learning biased examples. The approaches are simple, effective, model agnostic, and applicable to various datasets. Evaluation on textual entailment benchmarks shows promising results. The study proposes debiasing strategies to train neural models for improved out-of-domain performance on challenging datasets like SNLI, MultiNLI, and FEVER. Including these strategies in training baseline models, such as BERT, shows significant gains in performance. The contributions include proposing debiasing strategies and empirically evaluating their effectiveness. The study proposes debiasing strategies to improve out-of-domain performance on challenging datasets like SNLI, MultiNLI, and FEVER. Empirical evaluation shows substantial gains on HANS and SNLI hard set, as well as fact verification tasks. The goal is to optimize model parameters for resistance to benchmark biases and domain changes. The study proposes debiasing strategies to improve out-of-domain performance by identifying dataset biases and heuristic patterns. Several strategies are used to incorporate bias-only knowledge into training the base model for robustness. The bias-only model is trained to capture identified biases to avoid relying on shortcut patterns. The bias-only model is designed to capture identified biases in the dataset and is trained using biased features to improve the base model's robustness. Incorporating bias-only knowledge into training of the base model involves strategies to down-weight loss from biased examples, sharing parameters between models, and preventing the base model from learning biases. The bias-only model is trained alongside the base model to improve robustness. Our debiasing methods involve computing the loss of the combined classifier by combining bias-only and base model predictions using the product of experts ensemble method. This approach allows the models to make predictions based on different characteristics of the input, with the bias-only branch focusing on biases and the base model learning the actual task. The combined predictions are then used to compute the cross-entropy loss. The combined predictions of bias-only and base model are used to compute the cross-entropy loss. Cadene et al. (2019) proposed RUBI to alleviate biases in VQA benchmarks, but we evaluate its effectiveness in NLU contexts on various datasets. A sigmoid function is applied to bias-only model predictions to obtain importance weights for each label. The obtained mask adjusts the base model's predictions to prevent leveraging shortcuts, down-weighting biased examples and increasing the contribution of hard examples. Variants include RUBI + log space and normalizing the output. Debiased Focal Loss is a novel variant of Focal Loss that leverages bias-only branch predictions to reduce the importance of biased examples and focus on learning hard examples. The focusing parameter \u03b3 impacts the down-weighting rate, with higher values increasing the effect.\u03b3 = 2 is set for Debiased Focal Loss. The Debiased Focal Loss uses a scaling factor to down-weight biased examples based on their bias level. It is applied to update the base model parameters and has shown promising results on NLI and fact verification datasets. The Debiased Focal Loss uses a scaling factor to down-weight biased examples based on their bias level, showing promising results on NLI and fact verification datasets. The models' performance on challenging unbiased evaluation datasets is compared against BERT 4 as the baseline, with other baselines included for comparison. The FEVER dataset contains claim-evidence pairs from Wikipedia, with a new evaluation set created to address idiosyncrasies in the benchmark. The collected dataset is challenging, making classification based on claim cues equivalent to a random guess. The collected dataset is challenging, and models evaluated on it see a significant drop in performance. BERT is considered the baseline model, with debiasing methods showing improvements ranging from 1.11 to 9.76 absolute points. The Product of experts and Debiased Focal Loss are highly effective, surpassing prior work. Evaluation is done on hard SNLI and MNLI datasets where a hypothesis-only model struggles to predict labels accurately. The success of recent textual entailment models is attributed to biased examples, with performance substantially lower on hard sets. Base models like InferSent and BERT are considered, with the bias-only model using only the hypothesis to predict labels. Results on development sets and hard sets are shown, with improvements seen on MNLI matched hard dataset using debiasing methods. The Debiased Focal Loss and product of experts show gains on MNLI mismatched hard, while the product of experts and RUBI improve results on MNLI matched hard. The product of expert model enhances performance on MNLI hard sets while maintaining accuracy. NLI models can rely on superficial syntactic heuristics according to (2019) and introduce the HANS dataset. The study introduces the HANS dataset to highlight examples where models using syntactic heuristics fail. BERT is used as the base model trained on MNLI dataset with various features considered for the bias-only model, including syntactic heuristics and similarity features. The bias-only loss assigns equal weight to contradiction and neutral labels to improve model recognition of entailment. Debiased Focal Loss and Product of experts models significantly improve model performance by 5.45 and 3.89 points, respectively. Experimenting with different values of \u03b3 in Debiased Focal Loss shows that increasing \u03b3 focuses on learning hard examples and reduces attention on biased examples, leading to improved out-of-domain accuracy on the SNLI hard set. Overall, the methods proposed in the study enhance out-of-domain performance across different datasets. Debiased Focal Loss and Product of experts models show consistent high gains in various settings. RUBI variations perform differently across datasets, with RUBI+log space excelling on SNLI with BERT baseline and HANS dataset. RUBI+normalize outperforms others on FEVER experiment and MNLI matched hard set with BERT baseline. Trade-offs between in-domain and out-of-domain performance are observed, especially with Debiased Focal Loss. Models with BERT baseline maintain in-domain performance consistently. Recent studies have highlighted biases in NLU benchmarks, showing that models can rely on annotation artifacts and heuristic patterns to perform well. This phenomenon is observed in various tasks such as textual entailment, fact verification, argument reasoning comprehension, and reading comprehension. Biases in VQA datasets have also been confirmed, leading question-only models to ignore visual content. The common strategy to address biases is to augment datasets by balancing them. To address biases in datasets, strategies include augmenting datasets to balance existing cues and creating adversarial datasets. While collecting new datasets is costly, it is important to develop strategies to train models on biased datasets while improving their performance. One approach is to compute n-grams associated with each label and assign balancing weights to training samples. Alternatively, end-to-end debiasing strategies can be implemented. In contrast to previous approaches, the proposed method involves training a bias-only model to dynamically adapt the classification loss and reduce the impact of biased examples during training. This approach aims to improve performance on hard datasets while preserving necessary information for the NLI task. The study proposes novel ensemble-based debiasing techniques to reduce biases in neural models. They introduce a bias-only model to capture biases and adjust the cross-entropy loss to focus on hard examples. The debiasing techniques are model agnostic and significantly improve model robustness to domain-shift, with notable performance gains on various test sets. The study introduces ensemble-based debiasing techniques to reduce biases in neural models, including a bias-only model and adjusting the loss function to focus on hard examples. Different models like InferSent and BERT are trained and fine-tuned with specific configurations, showing improved performance on SNLI hard set. Results: Table 6 displays the performance of each label (entailment and non-entailment) on the HANS dataset and its individual heuristics."
}