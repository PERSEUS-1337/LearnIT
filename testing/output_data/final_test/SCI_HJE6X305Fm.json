{
    "title": "HJE6X305Fm",
    "content": "Generative Adversarial Networks are a key tool in generative modeling, image editing, and content creation. In this paper, the authors discuss the challenges of training generative adversarial networks (GANs) due to the delicate balance required between two deep networks. They highlight the benefits of minimizing a Wasserstein objective for smoother and more stable optimization. By making the discriminator robust to adversarial attacks, any GAN objective can be turned into a smooth and stable loss. The authors demonstrate that adversarial robustness improves the performance of GANs quantitatively and qualitatively, making training more robust to various factors. The authors discuss the challenges of training GANs and the benefits of minimizing a Wasserstein objective for stable optimization. They show that a robust discriminator leads to a robust minimax objective for the generator, making training smoother and more stable. This robustness does not need to be enforced for every input of the discriminator, but rather in expectation over generated samples. New regularization terms are introduced to enhance GAN training. The authors introduce two new regularization terms from adversarial training literature to ensure the robustness of the discriminator in GAN training. These terms improve visual quality and stabilize training across various architectures and hyper-parameters. The scope of generative modeling has evolved with the emergence of photo-realistic image generation techniques like GANs, Variational AutoEncoders, and Pixel Convolutional Networks. The authors focus on improving generative image processing, image editing, and image translation through advancements in architectures and loss functions in GANs. They emphasize the importance of using a robust discriminator for smoother training and better image quality. The focus is on improving GANs through advancements in loss functions, specifically discussing the Jensen-Shanon divergence, least squares GANs, and Wasserstein distance. The importance of a Lipschitz-1 constraint on the discriminator is highlighted, with methods such as weight clipping and gradient penalty discussed. The theoretical properties of Wasserstein objective are emphasized for stability in optimization. In this paper, it is shown that any GAN objective can be made smooth by ensuring the discriminator is robust to adversarial perturbations. The discriminator only needs to be robust in expectation over all generated samples. A simple penalty function can achieve the theoretical smoothness results of WGAN and other GAN objectives. Theoretical analysis shows that any GAN objective is Lipschitz and smooth when both the generator and discriminator are Lipschitz. Despite the success of deep networks, their differentiable nature makes them vulnerable to adversarial attacks. The initial work of BID27 sparked an arms race between novel attack methods (BID5, BID2, BID22, BID19) and defenses (BID11, BID23). Attackers aim to find perturbations that cause significant output changes in a network. Fast attacks perturb input in the gradient direction (BID5), while more complex ones optimize for an attack vector (BID2). Sampling-based generative modeling seeks to map noise distribution samples (P Z) to data distribution samples (P R) using methods like normalized gradient attack. Generative Adversarial Networks (GAN) involve a two-player game between a generator G and a discriminator D, where G maps noise to data samples and D judges if the generated data is close to the true distribution PR. GAN optimizes both G and D in a minimax game using a differentiable loss function f. G and D are deep neural networks with parameters \u03b8 and \u03c6, and the discriminator produces a scalar output passed through f to determine the model's performance. Different choices of f lead to various GAN models. Different choices of loss functions f lead to different GAN models. Examples include the original GAN objective using a sigmoid log likelihood loss, least squares GAN with a Euclidean loss, and Wasserstein distance in WGAN with a Lipschitz-1 discriminator. Adversarial attacks are defined as |h(x) \u2212 h(x + \u2206)| < \u03b5 for all \u2206 p < \u03b4, focusing on distance norms for simplicity. The text discusses the robustness of a function to adversarial perturbations in a generative distribution. It shows that the GAN objective remains robust to perturbations if the discriminator is robust, even as it adapts to the perturbation. The text analyzes the robustness of the discriminator to adversarial attacks in the context of a generative distribution. It focuses on the optimal discriminator for a generator and proves the robustness of the adversarial objective. The discussion extends to the robustness of discriminators, assuming concave loss functions for popular architectures like original GAN, LSGAN, and WGAN. Theorem 4.2 discusses the robustness of the discriminator to perturbations in popular GAN architectures like original GAN, LSGAN, and WGAN. It proves that a concave loss function and a robust discriminator lead to a robust and smooth objective. This extends the theoretical properties of WGANs to other GAN models. Theoretical properties of WGANs are extended to other GAN models by relaxing Lipschitz constraints for adversarial robustness. The continuity and Lipschitzness of any GAN objective can be analyzed using Theorem 4.2, showing that any GAN objective is continuous or Lipschitz as long as the discriminator and generator are. This contradicts previous findings that only WGAN is continuous, which was based on a discontinuous discriminator with infinitely large weights. Robust discriminators or losses can be challenging to train, but a regularized discriminator objective can lead to a robust discriminator without the need for constraints or specific architectures. The regularized discriminator objective aims to train a robust discriminator without constraints. Adversarial regularization, such as distillation or logit pairing, is used to ensure robustness to attacks. The attack vector is found using fast normalized gradient attack. The regularized discriminator objective maximizes robustness, as proven in Theorem 4.3. The optimal discriminator D* is robust and can be further improved by increasing the weight \u03bb of the robustness term. Robust feature matching (RFM) minimizes the attack direction v to enhance robustness in practice. Robust feature matching (RFM) minimizes DISPLAYFORM6 with attack direction v, weight w, and hyperparameter \u03b1. \u03b4 = 0.05 and \u03b1 = 10 \u22124 in experiments. Theorem 4.3 ensures robustness of optimal discriminator DISPLAYFORM7. Intuition on adversarial defense impact on GAN loss landscape shown with Dirac-GAN example. Practical optimization issues separated from loss function form. Dirac-GAN has true data distribution p D as Dirac distribution at zero, generator produces samples from Dirac distribution at \u03b8. The generator in Dirac-GAN produces samples from a Dirac distribution centered at \u03b8, with the discriminator being a linear function. The GAN objective reduces to a specific formula, and adding robustness through a robust loss or discriminator smooths out loss functions for better gradient signals during training. We conduct experiments on MNIST, CIFAR10, and CelebA datasets using a modified DCGAN architecture with residual blocks, batch normalization, and Leaky ReLU. Training details include weight decay, batch size, optimization with ADAM, and specific epochs for each dataset. Different loss functions like Jenson-Shannon divergence, least squares, and Wasserstein are compared for robust GAN training. In experiments on CIFAR10 and CelebA datasets, various loss functions like divergence loss, least squares loss, and Wasserstein distance were compared for GAN training. Additional regularization methods such as Instance Noise, Gradient Penalty, Adversarial Regularization, and Robust Feature Matching were also considered. The Fr\u00e9chet Inception Distance (FID) score was used as the primary quantitative measure of image quality, with lower scores indicating closer similarity between real and generated distributions. In experiments on CIFAR10 and CelebA datasets, various loss functions and regularization methods were compared for GAN training. The Fr\u00e9chet Inception Distance (FID) score was used as the primary measure of image quality. Without regularization, the original and least squares objectives performed best, while the linear (WGAN) objective had training issues due to unstable gradients. Instance noise worsened the problem for CIFAR-10 but slightly helped for Celeb-A. Gradient penalty improved stability for both datasets, especially for the linear (WGAN) model, but robust regularizations (AR and RFM) outperformed it in enhancing image quality. The study evaluated the robustness of regularization methods (AR and RFM) in improving image quality across different GAN objectives. Various experimental setups on CIFAR-10 were tested, including different losses, batch sizes, learning rates, network architectures, and normalizations. Models with RFM regularization consistently outperformed others in terms of FID score. Our study compared the performance of different regularization methods in GANs, ranking them based on FID scores. Robust feature matching (RFM) consistently outperformed other methods, showing better convergence in experimental setups. The connection between robust discriminators and optimization smoothness was highlighted, with RFM achieving the best results in over two thirds of the experiments. Further investigation into the theoretical implications of adversarial robustness on local convergence is needed. Our study compared different regularization methods in GANs, with robust feature matching (RFM) outperforming others in achieving smooth and robust loss functions. The results suggest that robust regularization leads to better training and visual outcomes compared to standard gradient penalties. Additionally, any GAN with a continuous generator and discriminator has a continuous objective, and Lipschitz functions play a role in the smoothness of the GAN objective. The GAN objective is (KLC)-Lipschitz in the generator parameters \u03b8, with a focus on robust feature matching. The optimal discriminator in robust feature matching is shown to be robust, enhancing the GAN objective function. The effects of robust loss or discriminator on the GAN objective are explored using the Dirac-GAN setup. In the Dirac-GAN setup, different GAN configurations were tested, including an unconstrained GAN, a GAN with hard robustness constraints, and one with robustness regularization. The unregularized GAN did not converge, while the hard constraint GAN approached the global optimum within a certain distance d, dependent on the robustness strength. Adversarial regularization led to a convergent Dirac-GAN objective, but further theoretical analysis is needed for a full understanding of its local convergence properties. Additional experiments were conducted using defenses against popular attacks such as Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and Carlini Wagner (CW) with robust feature matching (RFM) and adversarial regularization (AR). These defense techniques also improved the FID score. Additional generations were presented for various models, including a robust discriminator using WGAN+RFM and robust loss using GAN+DS."
}