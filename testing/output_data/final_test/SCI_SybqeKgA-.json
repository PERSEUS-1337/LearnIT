{
    "title": "SybqeKgA-",
    "content": "Mini-batch gradient descent is commonly used in deep learning to balance computation cost and gradient uncertainty. BA-SGD dynamically chooses batch size, while BA-Momentum extends this to momentum algorithm. Experiments show that batch adaptive methods achieve lower loss compared to mini-batch methods. BA-Momentum is more robust against larger step sizes. The code for the batch adaptive framework is now open source and applicable to any gradient-based optimization problems. Efficiency in training large neural networks is crucial as they have more parameters and require more data for state-of-the-art performance. Commonly used methods for training deep neural networks include stochastic gradient descent (SGD) with momentum, ADAGRAD, ADADELTA, and ADAM. SGD approximates the gradient by only using a subset of data. BID18 has developed a batch adaptive stochastic gradient descent (BA-SGD) that dynamically chooses a proper batch size during training. BA-SGD models the decrease of objective value as a Gaussian random walk game with rebound based on Taylor extension and central limit theorem. It updates parameters only when the expected decrease of objective value compared to the current batch size is significant. The BA-SGD algorithm dynamically adjusts batch size to approximate the gradient better and avoid frequent model updates. While it shows promise in simple tasks, its performance on complex neural networks like CNNs and RNNs remains unexplored. Empirical studies suggest that SGD struggles in deep and complex neural networks. In this paper, the batch adaptive framework is extended to the momentum algorithm, resulting in batch adaptive SGD (BA-SGD) and batch adaptive momentum (BA-Momentum). These methods are evaluated on deep learning tasks involving RNN and CNN models, showing that batch adaptive methods converge to lower loss values compared to fixed batch-size methods. BA-Momentum is particularly robust against large step sizes by dynamically adjusting the batch size to counteract noise. The batch adaptive framework is extended to the momentum algorithm, resulting in batch adaptive SGD (BA-SGD) and batch adaptive momentum (BA-Momentum). The batch size boom phenomenon is observed in the training of BA-Momentum, where the batch size increases to larger values, aiding in reaching lower loss levels. The code for the batch adaptive framework using Theano is now open source. The batch adaptive framework, including BA-SGD and BA-Momentum, is discussed in relation to Fashion-MNIST BID17 and relation extraction tasks. The robustness of BA-Momentum against large step sizes is highlighted, along with efficiency issues and potential applications. The framework involves using X and Y for training data and subsets, \u03b8 for model parameters, and F for the objective function. The gradient is computed over the whole data set, with \u03be j representing the difference between the approximate and real gradients. The approximate gradient on a batch Y is modeled as a normally distributed random variable. Using the Central Limit Theorem, it should follow a multi-dimensional normal distribution. By applying the first-order Taylor extension, the objective function F(\u03b8) can be approximated at any parameter configuration. When using the SGD optimization algorithm to update parameters, the decrease in objective value can be expressed as a function of the learning rate \u03b7 and a noise term \u03b5Y. This noise term can be seen as a weighted sum of each dimension of vector \u03b5Y, satisfying a one-dimensional Gaussian distribution. The decrease in objective value during parameter updates is modeled as a random walk game with a Gaussian dice. The objective value is considered a state, with the transition to the next state determined by a Gaussian dice. The game state domain is defined as a half closed set of real numbers. The game involves moving towards a minimum objective value using a random walk with a Gaussian dice. The moving step is determined by a Gaussian dice, and the best batch size balances gradient variance and computation cost. The main appeal of momentum in optimization algorithms is its ability to reduce oscillations and accelerate convergence by utilizing past parameter updates with an exponential decay. Recent work on momentum, such as YellowFin, has shown to converge in fewer iterations than ADAM on large models like ResNet and LSTM. Momentum utilizes past parameter updates with exponential decay, controlled by a rate \u03c1. Equation 11 can be expressed differently, and the decrease in objective value can be estimated analogically. The noise term \u03b5 t represents the difference between real and estimated gradients, with noise becoming constant for previous iterations but random for the current iteration. The noise term \u03b5 t represents the difference between real and estimated gradients, with noise becoming constant for previous iterations but random for the current iteration. \u03b5 t is normally distributed, and the decrease of objective value, \u2206F t, follows a one-dimensional Gaussian distribution. A recurrence formula is used to calculate the mean and variance of \u2206F t without recording all the f \u03b8\u03c4 from previous iterations. The Gaussian dice in Section 2 satisfies a specific formula. The expected value of the next state is calculated using Equation 7. The batch adaptive momentum algorithm determines the best batch size to maximize a utility function in Equation 8. Pseudo code is presented to implement the algorithm, calculating the optimal batch size for each update step. If the optimal size is larger than the current batch size, more instances are added to enlarge the batch. The Gaussian dice in Section 2 satisfies a specific formula for calculating the expected value of the next state. The batch adaptive momentum algorithm determines the optimal batch size to maximize a utility function. Pseudo code is presented to implement the algorithm, gradually increasing the batch size until it reaches the optimal size. Calculating the covariance matrix is time-consuming, and the learning efficiency of different algorithms on deep learning models is discussed. The study evaluates learning efficiency of algorithms on deep learning models. One model is a CNN for image classification on Fashion MNIST dataset, while the other is a complex RNN for relation extraction on financial documents. Fashion MNIST has 60000 training samples and 10000 test samples of 28x28 grayscale images. The CNN model has 3 convolutional layers. The RNN model uses 3855 training instances. In both experiments, 12 different optimization algorithms are used, including BA-Momentum, BA-SGD, mini-batch momentum, and mini-batch SGD with fixed sizes. Manual-Momentum and Manual-SGD methods increase batch sizes after a set number of epochs to reduce noise in estimated gradients. Batch sizes start at 32 and double every 25 epochs for image classification and every 75 epochs for relation extraction, reaching 256 eventually. The study compares different optimization algorithms, including BA-Momentum, BA-SGD, mini-batch momentum, and mini-batch SGD with fixed sizes ranging from 32 to 256. The results show that BA-Momentum achieves a lower loss compared to other methods in image classification tasks on Fashion MNIST. After comparing optimization algorithms, BA-Momentum achieves lower loss than other methods in image classification tasks on Fashion MNIST. It also shows fewer fluctuations in later training stages and has a batch size that increases to larger sizes. BA-SGD achieves the second lowest loss, with Mini-32-SGD achieving the lowest. The batch size for BA-SGD increases more frequently as iterations progress, with the largest possible batch size reaching almost 500. The relation extraction task results show that BA-Momentum maintains the lowest loss. BA-SGD and Mini-32-SGD have similar curves, but BA-SGD fluctuates less and eventually reaches a lower training loss. Batch sizes for BA-SGD change less frequently, with BA-Momentum staying constant at 32. Larger batch sizes lead to worse generalization in this task, so BA-SGD and BA-Momentum opt for smaller sizes. The model generalizes better with smaller batch sizes for BA-SGD and BAMomentum. There is a sudden rise in loss at the 224th epoch for BA-SGD, likely due to inaccurate gradient estimates. BA-SGD uses larger batch sizes than BA-Momentum, as momentum-based methods reduce uncertainty without needing large batches like SGD. The \"BA\" methods consistently outperform the \"Manual\" methods in terms of loss. Test accuracies are provided in Appendix D. The \"BA\" methods achieve the lowest loss in most cases due to their optimization approach. In the fine tuning process of BA-Momentum, it is observed that BA-Momentum is robust against large step sizes, maintaining low loss even with higher learning rates compared to other mini-batch methods. This is achieved by adopting a smaller batch size initially and gradually increasing it to reduce noise in estimated gradients, resulting in a more stable decrease in loss throughout training. In testing the robustness of BA-Momentum, four different learning rates (0.005, 0.01, 0.02, and 0.05) were chosen. Results showed that BA-Momentum had lower loss compared to Mini-32-Momentum as the learning rate increased from 0.005 to 0.02. Both methods suffered high loss at a learning rate of 0.05, confirming the robustness of BA-Momentum against larger step sizes within a reasonable range. When testing the robustness of BA-Momentum, higher learning rates lead to larger batch sizes to counteract uncertainty in parameter updates. A phenomenon observed is the batch size boom, where batch sizes increase significantly during learning, causing divergence from Mini-32-Momentum. The BA-Momentum algorithm, an extension of BA-SGD, shows improved performance in natural language processing and image classification tasks compared to Mini-32-Momentum. The batch size boom phenomenon occurs as learning rate increases, leading to lower loss and more accurate updates. In experiments, BA-Momentum achieves lower loss than mini-batch methods and is more robust against large step sizes. However, computing derivatives in BA-SGD and BA-Momentum is time-consuming compared to mini-batch gradient descent. A feasible approach to reduce computation cost may involve modifying the derivation process in Theano. A possible approach to reduce computation cost is to modify the derivation process in Theano for a batch of instances. This batch adaptive framework has various applications, such as accelerating distributed deep learning by considering both computation and communication costs when deciding on a proper batch size. The batch adaptive framework can help remedy generalization degradation when using large batches. BID5 provided evidence that using a larger batch degrades model quality and proposed strategies to mitigate this issue, such as warm-starting with small batches before transitioning to large batches during training. The batch adaptive framework addresses the issue of varying batch sizes for different datasets. It focuses on dynamically changing the batch size based on data characteristics. For the classification task on Fashion MNIST, 3 convolutional layers with different filter sizes are used. The relation extraction task involves extracting correctly matched quadruples from financial document sentences using a bi-directional LSTM RNN model. In the BA-Momentum method, a bi-directional LSTM RNN model with two layers is used to predict the probability of correct quadruples. The decrease of objective value is modeled as a one-dimensional Gaussian distribution, with mean and variance estimated using real data from Fashion-MNIST. The distribution is verified through computations on the dataset, showing close alignment with the estimated values. The decrease of objective value in the BA-M algorithm follows a Gaussian distribution, with mean and standard deviation values close to the real ones. The covariance matrix calculation in complex models can be reduced by assuming independence of model parameters, leading to a diagonal matrix and lower space and time costs. The covariance matrix in complex models can be simplified to a diagonal one, reducing space and time costs. A small dataset from UCI is used to test the impact on BA-Momentum performance. Two versions are compared, one with exact covariance matrix calculation and the other with estimated diagonal matrix. The task involves classification with 3 classes and 42 features, using around 60000 data instances for training. After 14 epochs, results show that computing the optimal batch size takes up a small percentage of total computing time for both BA-Momentum and BA-SGD. The optimal batch size computation for BAMomentum and BA-SGD involves means, variances, and a binary search to maximize the utility function. Test accuracies of different methods are shown for image classification and relation extraction tasks. The best test accuracies achieved by each method during training are recorded. The proposed \"BA\" methods achieve two best test accuracies in four cases. The \"BA\" methods achieve high test accuracies in various cases without the need for fine tuning, while \"Manual\" methods require tuning for satisfactory generalization."
}