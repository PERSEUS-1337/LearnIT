{
    "title": "rye7IMbAZ",
    "content": "In inductive transfer learning, fine-tuning pre-trained convolutional networks outperforms training from scratch by assuming the pre-trained model extracts relevant features for the target task. This paper explores regularization schemes to maintain learned features from the source task during fine-tuning. Regularization schemes are explored to maintain learned features from the source task during fine-tuning of pre-trained convolutional networks. The recommended approach involves a simple $L^2$ penalty using the pre-trained model as a reference, which outperforms the standard scheme of weight decay on a partially frozen network. Modern convolutional neural networks can achieve remarkable performance on large-scale image databases, but the vast amounts of data, computing time, and power consumption required for training deep networks are dissatisfying. Transfer learning can refine convolutional networks trained on a large database for related visual tasks by initializing the network with pre-trained parameters. During fine-tuning of pre-trained convolutional networks, using L2 regularization may lead to large deviations from initial values, potentially causing loss of general knowledge relevant to the target task. To address this, alternative parameter regularization methods are considered to preserve acquired knowledge. The standard L2 regularization, which drives parameters towards the origin, may not be suitable for transfer learning where initial values serve as a more sensible reference point. Modifying the regularization approach helps maintain control overfitting while constraining the search space effectively. The paper explores parameter regularization methods for transfer learning, aiming to improve it by requiring less labeled training data. Variants of L2 penalties, Lasso, and Group-Lasso are evaluated to retain knowledge acquired from the source problem during fine-tuning of pre-trained models. The paper discusses parameter regularization methods for transfer learning, focusing on L2-SP and Group-Lasso-SP approaches using pre-trained parameters. Experimental results show these methods outperform standard L2 weight decay. The study recommends using L2-SP for transfer learning tasks and references existing works on inductive transfer learning in convolutional networks. Similar regularization techniques have been applied to support vector machines in the past. In transfer learning, the focus is on improving performance on the target problem with limited labeled data by using parameter regularization methods. The target domain remains the same as the source domain, but the target task differs from the source task. The study explores L2-SP and Group-Lasso-SP approaches for transfer learning, which have shown better results than standard L2 weight decay. In transfer learning, the focus is on improving performance on the target problem through parameter regularization methods that encourage similarity between target and source problems. Various approaches using pre-trained models like AlexNet and VGG have shown significant improvements on image classification tasks. BID7 proposed a scheme for selecting a subset of images from the source problem with similar local features to the target problem and fine-tuned a pre-trained convolutional network to enhance performance. Transfer learning with convolutional networks relies on the generality of learned representations from databases like ImageNet. The learned representations in deep learning can be transferred to different domains, and network parameters are reusable for various tasks. Parameter regularization in deep learning includes L2 regularization, L1 regularization, and max-norm regularization. In lifelong learning, preserving knowledge from previous tasks is crucial to avoid catastrophic forgetting. BID16 proposed using outputs from the original network on source tasks to train on target tasks, achieving better performance. BID13 also successfully preserved network parameters to improve sensitivity and performance. In transfer learning scenarios, elastic weight consolidation may not be as effective as fine-tuning with stochastic gradient descent for learning new tasks. However, in domain adaptation where the target task is similar to the source task, elastic weight consolidation can still be beneficial even with limited labeled target data. In domain adaptation, approaches aim to find a common representation space for source and target domains to reduce domain shift. BID24 introduced parameter regularization to maintain similarity between pre-trained and fine-tuned models. The exponential term in the regularization showed improvement but not as much as L2-SP. Regularization has been used for shrinkage estimators, with shrinking towards adaptively chosen targets being a common practice. In domain adaptation, approaches aim to find a common representation space for source and target domains to reduce domain shift. Shrinkage towards adaptively chosen targets has been a common practice, with recent advancements like adaptive SVM and projective model transfer SVM showing promising results in improving classification performance with limited labeled data in the target task. These approaches differ from the present proposal in terms of their focus on binary classification problems and fixed representations. Transfer learning with convolutional networks involves learning similar representations for classification parameters from scratch, utilizing regularization functions to facilitate optimization and prevent overfitting. This approach differs from domain adaptation methods focused on fixed representations and binary classification tasks. Regularization in transfer learning with convolutional networks involves implicitly restricting the network's capacity to fine-tune a pre-trained model. This process provides an induction bias towards the initial solution and can be enhanced by incorporating a regularization term for better training outcomes. This approach outperforms standard methods like weight decay or freezing parts of the network to preserve low-level representations. Regularization in transfer learning with convolutional networks involves incorporating a regularizer \u2126(w) in the objective function to optimize network parameters for the target task. The regularizer acts as a trade-off between data-fitting and regularization terms, with the L2 penalty being a common choice for transfer learning. Regularization in transfer learning with convolutional networks involves incorporating a regularizer \u2126(w) in the objective function to optimize network parameters for the target task. The L2 penalty is commonly used for transfer learning. The L2-SP-Fisher Elastic weight consolidation was proposed to prevent catastrophic forgetting in lifelong learning setups. It involves using the initial parameter vector w0 and estimated Fisher information. The L1-SP penalty in transfer learning with convolutional networks uses the estimated Fisher information to penalize differences between the current parameter vector and the initial one. This penalty encourages some components of the parameter vector to remain frozen at their pre-trained values. In transfer learning with convolutional networks, the GL-SP penalty encourages freezing groups of parameters corresponding to channels of convolution kernels. It uses a group structure defined by a fixed partition of the index set, with predefined constants to balance different group cardinalities. In transfer learning with convolutional networks, the GL-SP penalty encourages freezing groups of parameters corresponding to channels of convolution kernels. It uses a group structure defined by a fixed partition of the index set, with predefined constants to balance different group cardinalities. In our experiments, we used \u03b1 g = p 1/2 g and implemented Group-Lasso-SP to freeze feature extractors at any depth of the convolutional network. The group G g of size p g = h g \u00d7 w g \u00d7 d g gathers all the parameters of a convolution kernel, and grouping is done at each layer for each output channel. The group index g corresponds to the layer index l and the output channel index at layer l, resulting in a total of G = l c l groups. GL-SP-Fisher is the Fisher version of GL-SP, evaluated on several pairs of source and target tasks using ResNet BID11 as the base network. In transfer learning, ResNet BID11 is used as the base network for tasks involving classification. Parameters are regularized in all layers except new ones, which are regularized by L2 penalty. Different source and target databases are used to compare the effect of similarity on transfer learning. The target databases used for generic object recognition include ImageNet, Stanford Dogs 120, and MIT Indoors 67. Each database is split into training and testing sets. Caltech 256 has two configurations with 30 or 60 examples per category for training. Stanford Dogs 120 has 100 examples per category for training. MIT Indoors 67 has 80 examples per category for training. Images are pre-processed by resizing to 256x256, subtracting mean activity, and applying random blur. The mean activity is computed over the training set from each channel, and data augmentation is done with random blur, mirror, and crop to 224x224. Cross-validation is used to find the best regularization hyperparameters \u03b1 and \u03b2, with \u03b2 consistently chosen as 0.01. Test accuracy varies smoothly with regularization strength, showing benefits in penalizing the last layer. Fisher information matrix is estimated on the source database (ImageNet or Places 365), yielding different estimates. Stochastic gradient descent with momentum 0.9 is used for optimization, running 9000 iterations and reducing the learning rate by 10 after 6000. Under the best configuration, the learning process is repeated five times to calculate average classification precision and standard deviation. Experiments are conducted using Tensorflow BID0, comparing fine-tuning with L2-SP and L2-SP-Fisher to baseline fine-tuning with L2 and selective joint fine-tuning BID7. The results show average accuracies and standard deviations from 5 different runs, with no additional images used during transfer learning. The results of experiments show that fine-tuning with L2-SP or L2-SP-Fisher outperforms standard L2 fine-tuning and BID7. The worst runs of L2-SP or L2-SP-Fisher fine-tuning even surpass the state-of-the-art results of BID7 on Caltech 256 setups. Experimental results are visually represented in Figure 2, comparing transfer learning accuracies using Places 365 and ImageNet as source databases. The results show that transfer learning with L2-SP or L2-SP-Fisher outperforms standard L2 fine-tuning and BID7. Fine-tuning strategies based on penalties using the starting point as a reference perform consistently better. There is a benefit in having an explicit bias towards the starting point, even when the target task is not similar to the source task. The strategies based on L1 and Group-Lasso penalties perform poorly compared to the simple L2-SP penalty, especially on Caltech 256-60 when the source problem is Places 365. The optimization algorithm used may not be suitable for these penalties due to discontinuities at the starting point. Implementing a smoothing technique did not improve results. Variants using the Fisher information matrix behave similarly to those using a Euclidean metric on parameters, indicating that the Fisher information matrix may be less relevant for the objective focused on Stanford Dogs 120. The performance drop is larger for L2 fine-tuning compared to L2-SP, with L2-SP-Fisher showing slight improvement. L2-SP-Fisher is considered a better approach for lifelong learning where accuracies on source tasks are important. Transfer learning can benefit from freezing the first layers of a network to induce a strong bias. However, this strategy can be costly and inefficient for L2 fine-tuning compared to L2-SP fine-tuning. ResNet-101 uses a combination of frozen and trained layers to achieve better accuracy on the target task. Analytical results are challenging in deep learning. The regularized objective function with L2-SP is a compromise between the unregularized objective function and the pre-trained parameter vector, along the directions of eigenvectors of the Hessian matrix. This differs from L2, which compromises between the unregularized objective function and the origin. The regularization procedures with L2-SP in deep learning involve a compromise with the pre-trained parameter vector, aiming for a solution closer to the \"true parameters\" for more effective fine-tuning. This approach is inspired by the concept of shrinkage estimation and the Stein shrinking effect. The connection with Stein shrinking effect may be inspiring by surveying the literature considering shrinkage towards other references, such as linear subspaces. Manifolds of parameters from the pre-trained network could provide a better reference than a single parameter value. Analysis of activations in the network shows dependence between pre-trained and fine-tuned activations. Activation similarities are easier to interpret than parameter similarities. R2 coefficients of determination with L2 and L2-SP regularizations for Stanford Dogs 120 training examples are shown in Figure 4 for ResNet-101. The ResNet-101 network begins with one convolutional layer and stacks 3-layer blocks. The R2 coefficients of fine-tuned activations decrease throughout the network, reaching low values for L2 regularization and staying high for L2-SP and L2-SP-Fisher at the greatest depth. This indicates that the roles of network units are retained with L2-SP and L2-SP-Fisher. The study shows that network unit roles are maintained with L2-SP and L2-SP-Fisher fine-tuning, even in the last high-level representations before classification. The computational efficiency of L2-SP penalties is minimal, with less than a 1% increase in floating point operations for ResNet-101. These regularization techniques offer 3-4% improvements in classification accuracy at little computational cost, demonstrating their relevance for inductive transfer learning with deep convolutional networks. The study demonstrates the effectiveness of L2-SP penalty in fine-tuning deep convolutional networks, showing improved performance compared to standard L2 penalty. Other penalties like L1 or Group-L1 norms do not show significant value in inductive transfer learning. Fisher information with L2-SP does not enhance accuracy on the target task. Additional approaches implementing implicit bias at the functional level remain to be explored. The effectiveness of L2-SP penalty in fine-tuning deep convolutional networks is demonstrated, showing improved performance compared to standard L2 penalty. Other penalties like L1 or Group-L1 norms do not offer significant value in inductive transfer learning. Approaches implementing implicit bias at the functional level, such as BID16, need further testing in the context of inductive transfer learning. The effect of L2 regularization can be analyzed by approximating the objective function around the optimum, showing rescaling of parameters along the eigenvectors of the Hessian matrix. The L2-SP regularization improves performance in fine-tuning deep convolutional networks compared to standard L2 penalty. The Hessian matrix of J w.r.t. w can be decomposed as H = Q\u039bQT, leading to a relationship between w and w* in the direction of eigenvectors."
}