{
    "title": "S1viikbCW",
    "content": "Despite the high performance of neural networks, the lack of interpretability has hindered their safe usage in practical applications, especially in high-stakes domains like medical diagnosis. Improving interpretability by explaining the importance of concepts in predictions is crucial for gaining trust and adoption. This work introduces concept activation vectors (CAV) to quantify the relative importance of concepts in neural network predictions. CAV allows non-experts to define concepts of interest and test hypotheses using examples, enabling the identification of biases and reasoning behind predictions. Neural networks' lack of interpretability hinders their safe usage in critical applications like medical diagnosis. Concept activation vectors (CAV) quantify the importance of concepts in predictions, allowing for bias identification and reasoning behind predictions without the need for retraining. Understanding and interpreting neural networks' behavior is crucial for gaining trust and adoption in practical applications. To establish acceptability and adoption for critical applications like in the medical domain, greater understanding of how neural networks function is crucial. Recent work suggests that linear combinations of neurons may encode meaningful information, but methods are lacking to identify these combinations and their role in concept understanding and classification decisions. Neural networks create rich internal representations beyond input features or output classes, raising the need to formalize and test hypotheses about these representations. Many machine learning interpretation methods focus on input features, but it is important to understand the rich internal representations created by neural networks beyond just input features or output classes. Model understanding and interpretation should not be limited to concepts explicit in training data, especially in cases like classification fairness where no input features may identify discriminated-against groups. For example, the Inception model BID24 has an output class for 'doctor' but no input features. This work introduces concept activation vectors (CAV) to identify linear combinations of neurons corresponding to semantic concepts in neural networks. CAV provides quantitative measures of user-provided concepts' importance for hypothesis testing. Testing with CAV (TCAV) aims for accessibility and customization in analyzing model predictions. TCAV allows for customization and quantification in analyzing model predictions by testing the importance of user-provided concepts without the need for retraining. It focuses on testing the relative importance of a small set of concepts rather than all possible features, promoting interpretability without requiring a full understanding of the entire network's behavior. TCAV enables quantitative relative importance testing for non-ML experts by allowing users to provide concepts without retraining the network. Users express concepts using examples, such as collecting pictures of women for the concept of gender. This approach facilitates communication between ML models and non-expert users. The paper discusses the TCAV method for interpretability in neural networks. It compares this method to existing interpretability techniques and highlights the importance of identifying semantically meaningful directions in a layer. The paper also addresses the challenges of plug-in readiness and mentions the popular saliency methods used for interpreting neural networks. Recent work has shown that existing saliency methods for interpreting neural networks may not meet the criteria of customization and quantification. Some methods can be sensitive to irrelevant data properties, leading to significant changes in explanations. Techniques like DeepDream can visualize patterns that activate specific neurons effectively. The DeepDream method offers insights into neuron activation and opportunities for AI-aided art, but it does not meet criteria for accessibility, customization, and quantification. It requires understanding of neurons and internal architecture, lacks a method to find semantically meaningful concepts, and lacks a way to quantify the relationship between pictures and network output. DeepDream can be combined with TCAV to identify and visualize interesting directions in a layer. Users can choose to restrict themselves to interpretable models or post-process models for insights. Building inherently interpretable machine learning models may result in decreased performance. Interpretation techniques for machine learning models are gaining importance, with a focus on providing explanations for ML decisions without the need for retraining or modifying the network. One challenge is ensuring that the explanation is consistent with the model's behavior to avoid inconsistencies. The plug-in readiness desiderata poses challenges for explanations to be consistent with the model's behavior. Recent work has shown that saliency methods can have inconsistencies, such as vulnerability to input shifts and being easily tricked. Perturbation-based interpretability methods, like TCAV, aim to improve consistency by using generated explanations as input for validation of the network's output. TCAV is a perturbation method that aims to provide globally truthful explanations for deep neural networks. It uses concepts provided by users to maintain consistency between explanations and the model's reasoning. TCAV uses user-provided concepts tied to real-world data to test the importance of concepts in classification. Neural networks gradually disentangle concepts layer by layer, with representations found in linear combinations of neurons. Linear structure in neuron activations can be uncovered by training a linear classifier. To uncover linear structure in neuron activations, a linear classifier can be trained using user-defined concepts in TCAV. An analyst selects positive (P C) and negative (N) data points for a given concept, representing examples like photos of striped objects versus random photos. The input to the network is represented as a vector in R n, with P C and N subsets of R n. Inference on an input example at layer l yields a function f l: R n \u2192 R m, where X \u2286 R n denotes a set of inputs. The concept activation vector in TCAV involves training a linear classifier on layer activations to distinguish between two sets of inputs, P C and N. This allows for flexibility in choosing the set P C, and a variation includes a relative concept activation vector trained on different concepts C and D. The technique of Concept Activation Vectors (CAVs) allows flexibility in choosing sets of concepts for interpretation in neural networks. It can be used to test the relative importance of different concepts by comparing them against each other or random concepts. The real value of CAVs lies in their ability to perform statistical significance tests to generate explanations. The Concept Activation Vectors (CAVs) technique allows for testing the importance of different concepts in neural networks using a two-tailed z-test. This method provides a faster and more general way to modify vector activations for interpretation. The Concept Activation Vectors (CAVs) technique allows for testing the importance of different concepts in neural networks by modifying vector activations. By adding or subtracting concept activation vectors from an image embedding, changes in classification results can indicate the relevance of specific concepts. This method provides a way to empirically assess the impact of concepts on classification outcomes. The addition of concept activation vectors to image embeddings can empirically assess the impact of concepts on classification outcomes. Future work should explore more principled methods to measure concept relevance. The influence of a concept C on class k can be quantitatively measured by a specific metric. Testing the importance of concepts can be done through z-testing on measured values. Concept activation vectors can help localize where a concept is disentangled in a network, challenging the common view of concepts represented at different layers in feedforward networks. Using concept activation vectors, we can detect specific concepts in a network. The experiment results show that the learned CAVs accurately identify the concepts of interest. Hypothesis testing using these CAVs provides valuable insights. The experiments are based on a model using publicly available parameters. Concept vectors are learned from a subset of pictures to reflect realistic scenarios. In experiments, CAVs accurately identify concepts of interest. Linearly learned CAVs align with intended concepts, with accurate predictions. The point in the network where concepts are learned aligns with general knowledge about representations. The accuracy of neural networks aligns with general knowledge about learned representations. Activation maximization techniques visualize Concept Activation Vectors (CAVs) to confirm patterns. Linear classifiers show high accuracy in distinguishing concepts, indicating linear separability. The experiment demonstrates that concepts in neural networks are linearly separable in many layers, with abstract concepts showing increased accuracy in higher layers. Activation maximization techniques are used to visualize Concept Activation Vectors (CAVs) in the direction of the learned representations. This experiment does not yet confirm semantic alignment with human concepts. The experiment uses activation maximization techniques to visualize Concept Activation Vectors (CAVs) in neural networks. Textures and colors are identifiable in mid-layers, becoming less recognizable in later layers. Color CAVs contain random textures. The experiment visualizes Concept Activation Vectors (CAVs) in neural networks using activation maximization techniques. Color CAVs in mid-layers contain random textures, with higher level concepts represented in the right image of FIG3. The zebra CAVs show textures related to water, trees, and grass, indicating a learned bias from the training data. DeepDream can be combined with TCAV to identify and visualize meaningful directions in a layer. The experiment visualizes Concept Activation Vectors (CAVs) in neural networks by aligning them with meaningful concepts using real data. The cosine similarity between pictures and CAVs confirms alignment with specific concepts, such as striped objects. CAVs for abstract concepts like CEO can also be constructed, showing that learned concepts are not necessarily classes the neural network learned to classify. Low cosine similarity indicates little relation between pictures and concepts, supporting the qualitative findings of the experiment. In this section, the experiment demonstrates how Concept Activation Vectors (CAVs) align with meaningful concepts through cosine similarity with pictures. The experiment suggests that CAVs impact classification by adding or subtracting v l C, as shown in FIG5 for eight classes. Adding or subtracting v l C impacts the classification task, with the v l C direction aligned with the probability of the target class. CAVdirection consistently increases the probability of class for semantically relevant concepts, while random directions are less consistent. This difference allows for a more rigorous hypothesis test to identify relationships between concepts and the target class that align with human intuition. In the next section, insights gained from tests are described, confirming common-sense intuitions about training images. Results show that the yellow color is influential to the cab class, and the 'women' concept is important to the 'bikini' class due to human presence in the images. Additionally, the 'model women' concept is significant for the 'bikini' class, leading to the realization that most pictures in this class feature women. The importance of the 'bikini' class pictures featuring professional models posing in bikinis was highlighted. The 'arms' concept was found to be crucial in predicting the 'dumbbell' class, consistent with previous qualitative findings. Despite a small number of examples, the TCAV method confirmed these findings quantitatively. The TCAV method can identify relationships between concepts and classes, such as 'baby' being important for the 'school bus' class and 'red' being important for the cucumber class. It can help discover insights for various applications, including improving model fairness. The choice of negative samples when constructing the CAV can lead to spurious suggestions of relevance. When testing unrelated concepts and classes, such as zebra and textures like honeycombed and lace-like, the lace-like concept showed high alignment. Statistical testing against random concepts can help filter out spurious results in the TCAV method. The concept of \"concept activation vectors\" (CAVs) is introduced as a way to probe the internal representation of a concept in a classification network. CAVs can be defined using example inputs, making them user-friendly. The Testing with CAVs (TCAV) technique quantifies the relationship between a CAV and a specific class, allowing for quantitative analysis. The TCAV technique quantifies the relationship between a concept activation vector (CAV) and a specific class, providing quantitative analysis. Experiments supported the intuition that certain features, like stripes on a zebra, are important for classification. DeepDream was used to create images reflecting CAVs, showing strong resemblance to original concepts. TCAV can also find associations between concepts, both obvious (like \"yellow\" and \"taxi\") and non-obvious (like \"red\" and \"cucumber\"). Comparing networks with TCAV can reveal differences in the importance of concepts. Users can use concept activation vectors (CAVs) to adjust network results during inference by adding a scalar multiple to intermediate layer activations. This can help deemphasize or enhance input conceptual aspects, potentially reducing bias learned from training data."
}