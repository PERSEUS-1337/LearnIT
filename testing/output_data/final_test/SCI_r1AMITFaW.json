{
    "title": "r1AMITFaW",
    "content": "In this work, a mathematical analysis was conducted on the memory of three RNN cells: SRN, LSTM, and GRU. A new design called ELSTM was proposed to extend memory length, and a multi-task RNN model called DBRNN was introduced for the SISO problem. The performance of the DBRNN model was evaluated. The LSTM and GRU are commonly used building cells in RNN models for NLP. Various RNN models like BRNN, encoder-decoder, and deep RNN have been proposed. Despite their design to enhance memory length and avoid gradient issues, understanding their memory length is still lacking. The memory of a RNN model is defined as a function mapping an element in a sequence to current output. The research aims to analyze the memory length of three RNN cells - SRN, LSTM, and GRU. This analysis is different from investigating gradient vanishing/exploding issues as it is done on trained RNN models. A new design called ELSTM is proposed to extend memory length. BRNN is a popular choice for macro RNN models but cannot solve dependent output sequence problems alone. Language tasks often involve dependent output sequences. The research proposes a new multitask model called the dependent bidirectional recurrent neural network (DBRNN) to address the limitations of the encoder-decoder system and BRNN. Experiments on part of speech tagging and dependency parsing demonstrate the performance of the DBRNN model with the ELSTM cell. Future research directions are also discussed. The memory capabilities of different types of recurrent neural networks (RNNs) are studied in this section. Specifically, the ability of RNNs to map input sequences into internal representations is explored. It was found that a Simple Recurrent Network (SRN) can only memorize sequences of length 3-5 units, while a Long Short-Term Memory (LSTM) network can memorize sequences longer than 1000 units. The memory of SRN, LSTM, and Gated Recurrent Unit (GRU) models are compared using Elman's SRN model with linear hidden state activation function and non-linear output activation function. The Simple Recurrent Network (SRN) is a mathematically tractable model with performance similar to other popular frameworks. It is described by equations involving weight matrices, input vectors, and non-linear functions. The model's memory decay is analyzed, focusing on cases where the largest singular value of the weight matrix is less than 1. The Simple Recurrent Network (SRN) has memory decay that is at least exponential with its memory length. A LSTM cell diagram is plotted with various functions and gates. The LSTM cell is mathematically defined with weight matrices and input vectors. The hidden state vector of the LSTM can be derived under certain assumptions. The LSTM hidden state vector can be derived by induction. Comparing outputs of SRN and LSTM, we see that certain components play the same memory role. The impact of input on output in LSTM lasts longer than in SRN due to the forget gate weight matrix. The GRU is an effective alternative to LSTM, with operations defined by four equations. The GRU system can be simplified by setting certain matrices to zero. The update gate in GRU and the forget gate in LSTM serve similar functions. Memory decay in GRU can be controlled by adjusting the weight matrix of the update gate. LSTM and GRU enhance memory retention through different mechanisms. In designing extended-long short-term memory (ELSTM) cells, two new models are proposed: ELSTM-I with trainable input weight vector and ELSTM-II without a forget gate. These models introduce a scaling factor to increase or decrease the impact of input in the sequence, aiming to enhance memory retention compared to traditional LSTM and GRU systems. The proposed ELSTM models, ELSTM-I and ELSTM-II, introduce a scaling factor to adjust the impact of input in the sequence, enhancing memory retention compared to traditional LSTM and GRU systems. ELSTM-I has longer memory than LSTM, proven by the closed form expression of h t and the role of s k. ELSTM-II, despite lacking a forget gate, can attend to or forget specific positions in a sequence through the scaling factor, using fewer parameters than ELSTM-I. In comparison to traditional LSTM and GRU systems, ELSTM models introduce a scaling factor to enhance memory retention. ELSTM-II requires fewer parameters than ELSTM-I and LSTM for typical sequence lengths. For example, to double the parameters compared to LSTM, the sentence length needs to be 4 times the size of word embedding and number of cells combined. In practice, most NLP problems have sentence lengths typically less than 100. In an experiment with sequence to sequence with attention BID16, ELSTM-I uses 75M of memory, ELSTM-II uses 69.1M, and LSTM uses 71.5M. The ELSTM-I, ELSTM-II, LSTM, and GRU models use different amounts of memory. They all have similar computational times on a GPU. A multitask model called dependent BRNN (DBRNN) is proposed to predict an output sequence. The model combines features of bidirectional RNN and encoder-decoder models. The BRNN model utilizes a bidirectional design to fully utilize information. The DBRNN model, a multitask RNN, combines features of bidirectional RNN and encoder-decoder models. It aims to address the issue of erroneous predictions in the forward path by introducing a new design. The model has three learning objectives for the forward and backward RNN predictions. The DBRNN model, a multitask RNN, combines features of bidirectional RNN and encoder-decoder models to address erroneous predictions. It consists of lower and upper BRNN branches, with the final bidirectional prediction being a pooling of forward and backward predictions. The design makes DBRNN robust to previous errors. The DBRNN model combines bidirectional RNN and encoder-decoder features to address prediction errors. It includes upper and lower BRNN branches, with errors back-propagated through time. Comparisons show DBRNN's robustness to errors. The DBRNN model combines bidirectional RNN and encoder-decoder features to address prediction errors. It includes upper and lower BRNN branches, with errors back-propagated through time. Comparisons show DBRNN's robustness to errors. The predicted distribution output is compared with the solution in BID5, highlighting three main differences: target sequences, attention mechanism, and encoder-decoder design. Experiments are conducted on part of speech tagging and dependency parsing tasks, with the latter requiring longer memory and more complex relations between input and output. In experiments comparing five RNN models, the training and test datasets are from Universal Dependency 2.0 English branch and CoNLL 2017 shared task data. Input and target sequence lengths are fixed, with padding for shorter sequences. Stemmed and lemmatized sequences are used for POS tagging and DP problems. The target sequences for POS tagging and DP include universal POS tags and dependency relations to the headword. The input is processed through an embedding layer before being fed into the network. The study compares LSTM, GRU, ELSTM-I, and ELSTM-II cells under various macro-models without fine-tuning network parameters for optimal performance. The results show performance metrics for different models in the POS tagging problem. The study compares different cell types like LSTM, GRU, ELSTM-I, and ELSTM-II under various macro-models without fine-tuning network parameters. The results show that ELSTM-I and ELSTM-II cells outperform LSTM and GRU cells. The sequence-to-sequence model with attention and ELSTM-I combination has the best performance with an accuracy of 60.19% and 66.72%. Basic RNN often outperforms BRNN for the DP problem, as it can access the entire input sequence when predicting the latter half of the output sequence. The two DBRNN models outperform both BRNN and sequence-to-sequence models in both POS tagging and DP problems. The study compares different cell types like LSTM, GRU, ELSTM-I, and ELSTM-II under various macro-models without fine-tuning network parameters. ELSTM-I and ELSTM-II outperform LSTM and GRU for complex language tasks, showing better memory retention and attention. However, for simpler tasks like POS tagging, ELSTM cells are over-parameterized and tend to overfit. ELSTM-I and ELSTM-II excel in sequence-to-sequence models with and without attention. The ELSTMs outperform LSTM and GRU for complex language tasks due to their more expressive hidden state. ELSTM-I and ELSTM-II show better memory retention and attention in sequence-to-sequence models. The memory decay behavior of LSTM and GRU was analyzed, leading to the proposal of ELSTM-I and ELSTM-II to enhance memory length. Additionally, a new RNN model called DBRNN was introduced. The DBRNN model, a combination of BRNN and encoder-decoder, outperforms other designs for complex language tasks. ELSTM-I and ELSTM-II show superior performance compared to other models. Future studies will explore the potential of ELSTM in deep RNNs and improving the DBRNN further."
}