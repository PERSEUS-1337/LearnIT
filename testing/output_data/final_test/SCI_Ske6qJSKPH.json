{
    "title": "Ske6qJSKPH",
    "content": "We study task-specific learning rate schedules for hyperparameter optimization, searching for schedules that generalize well. Our novel online algorithm interpolates between existing techniques for increased stability and faster convergence. Empirical results show our method outperforms baselines in final test accuracy. Learning rate adaptation for neural networks has been extensively studied, with recent work focusing on optimization methods. Recent work in connectionism has focused on developing complex schedules with a small number of hyperparameters and optimizing training loss. Hyperparameter optimization (HPO) is a distinct branch that aims to minimize generalization error by holding out data for validation. Research in this area includes model-based, model-free, and gradient-based approaches. The paper aims to automatically compute a learning rate schedule for stochastic optimization methods based on the given learning task, focusing on producing models with low validation error. It considers gradient-based hyperparameter optimization to find an optimal schedule. The paper discusses a constrained optimization problem to minimize the objective function E(w) with weight update dynamics \u03a6, where w represents initial model weights. The horizon T should be chosen sufficiently large to minimize training error effectively and avoid underfitting. The paper discusses a constrained optimization problem to minimize the objective function E(w) with weight update dynamics \u03a6. Most HPO techniques would not be suitable for determining LR schedules due to requiring multiple evaluations of f, defeating the goal of speed. Several researchers have investigated solutions for deriving greedy update rules for the learning rate. The paper discusses methods for updating learning rates in optimization algorithms without considering future information. Different approaches use various approximations to minimize performance after a single parameter update. The type of approximation and objective (training or validation loss) are separate issues, but comparative experiments are lacking in the literature. In this work, the authors analyze the structure of the true hypergradient for online gradient-based hyperparameter optimization techniques. They also study failure modes of previously proposed methods and the sensitivity of results to hyper-hyperparameters like the initial learning rate and hypergradient learning rate. In Section 3, the authors analyze failure modes of previous methods and discuss the type of approximations used. Section 4 introduces a new algorithm called MARTHE for producing competitive learning rate schedules with low validation error. Unlike previous proposals, MARTHE is almost parameter-free and automatically tunes its configuration parameters. Section 5 empirically compares different hypergradient approximations, while Section 6 presents real-world applications. In Section 6, real-world experiments validate the approach presented. Future applications and research directions are discussed in Section 7. The optimization problem is studied under the perspective of gradient-based hyperparameter optimization, treating the learning rate schedule as a vector of hyperparameters. The gradient of f can be computed iteratively using forward-mode algorithmic differentiation. The Jacobian matrices A t and B t in SGD depend on w t and \u03b7 t. A t = I \u2212 \u03b7 t H t (w t) and [B t] j = \u2212\u03b4 tj \u2207L t (w t). Forward-mode differentiation is used for easier interpretation and visualization. Stochastic approximations of Eq. (2) can be obtained with randomized telescoping sums or hyper-networks based stochastic approximations. Eq. 3 describes the tangent system. The tangent system in SGD measures parameter changes for learning rate variations. Translation matrices are sparse with only one non-zero column. The derivative for a learning rate \u03b7 t is a scalar, with the hypergradient affected only by future parameters trajectory. Eq. (4) shows the scalar product between gradients at the t-th step and the objective E at the final iterate, transformed by Jacobians. The hypergradient of \u03b7 t is influenced by the future trajectory, not \u03b7 t itself. Eq. (1) is highly nonlinear and challenging to solve with projected gradient descent due to inefficiencies in evaluating the gradient. Online updates for \u03b7 t are preferred due to the computational expense of multiple updates and the need to compute f T and the weight trajectory. The real-time hyperparameter optimization (RTHO) algorithm and Hypergradient descent (HD) are two previous methods for online learning rate schedule computation. RTHO uses forward-mode differentiation and accumulates partial hypergradients, while HD aims to minimize loss w.r.t. the learning rate after one optimization step. Both methods update based on trajectory information up to the current time. The update rules for RTHO and HD involve estimating hypergradients and adjusting learning rates. However, both methods have drawbacks: HD may underestimate the learning rate, while RTHO may be slow to adapt or unstable. These issues are illustrated using test functions in Fig. 1. In Fig. 1, two bidimensional test functions are used to compare RTHO and HD optimization methods. RTHO consistently outperforms HD due to its ability to accumulate partial hypergradients and exploit second order information for faster progress. The Beale function shows RTHO's superiority in flat regions, while a simplified Bukin function demonstrates RTHO's struggle with outdated information in valleys. Our proposed algorithm MARTHE smoothly interpolates between RTHO and HD optimization methods, maintaining an adaptive LR schedule online during training. It uses past trajectory and gradients to minimize computational overhead and improve performance in neural network optimization. MARTHE algorithm combines RTHO and HD optimization methods, utilizing past trajectory and gradients for efficient neural network optimization. It focuses on updating hyperparameters every K iterations, with a special case of K = 1 for a unified treatment. The Beale function and shorter horizon auxiliary objectives are used for evaluation. The MARTHE algorithm combines RTHO and HD optimization methods for efficient neural network optimization by updating hyperparameters every K iterations. It utilizes past trajectory and gradients, with a special case of K = 1 for a unified treatment. The Beale function and shorter horizon auxiliary objectives are used for evaluation, leading to cheaper approximations of future trajectories. The MARTHE algorithm combines RTHO and HD optimization methods for efficient neural network optimization by updating hyperparameters every K iterations. Using larger K's can capture longer horizon dependencies in the hypergradient structure, with a cost of O(c(\u03a6)) per step in time and memory. The parameter \u00b5 \u2208 [0, 1] controls the speed of past history accumulation. The MARTHE algorithm combines RTHO and HD optimization methods for efficient neural network optimization by updating hyperparameters every K iterations. The parameter \u00b5 \u2208 [0, 1] allows control over how quickly past history is forgotten. Values of \u00b5 < 1 help discount outdated information, while increasing \u00b5 extends the horizon of hypergradient approximations. The computational scheme is similar to forward-mode algorithmic differentiation, but the \"tangent system\" in Eq. 9 only tracks variations w.r.t the first component \u03be 0, reducing running time. Adapting \u00b5 and \u03b2 online may add overhead to the optimization procedure. The proposed update rule for MARTHE involves computing \u00b5 online to ensure descent direction w.r.t. the true hypergradient. The choice of h \u00b5 and adapting online the approximation horizons are discussed in the Appendix. Additionally, \u03b2 is adapted online in this work. The update rule for MARTHE involves adapting \u03b2 online to inject additional stability in the learning system. This adjustment allows for a broader range of good values for the hyper-learning rate, leading to improved optimization dynamics. Algorithm 1 MARTHE presents the pseudocode for optimizing dynamics. The runtime and memory requirements are dominated by the computation of variables Z, with a complexity up to four times that of \u03a6. Default values of \u03b2 0 = 0 and \u03b7 0 = 0 are suggested when no prior knowledge is available. A selection procedure for \u03b2 is recommended, starting high and decreasing until system stability is achieved. In this section, we compare LR schedules optimized by gradient descent (LRS-OPT) against other schedules like HD, RTHO & MARTHE. We trained neural networks on a subset of MNIST images using cross-entropy loss and SGD optimization with a mini-batch size of 100. The study compared LR schedules optimized by gradient descent (LRS-OPT) with other schedules like HD, RTHO & MARTHE. Neural networks were trained on a subset of MNIST images using cross-entropy loss and SGD optimization with a mini-batch size of 100. The validation set consisted of 700 images, and the validation loss E was defined after 512 optimization steps. The experiments were repeated for 20 random seeds, except for LRS-OPT, which was repeated for 4 seeds. Results were visualized in Figure 2, showing the behavior of the LR schedule found after 5000 iterations of gradient descent. The study compared LR schedules optimized by gradient descent (LRS-OPT) with other schedules like HD, RTHO & MARTHE. Figure 2 presents a qualitative comparison between offline and online schedules. HD schedules quickly decay, while RTHO schedules may cause instability. Fixing \u00b5 = 0.99 produces schedules that perform well. Average validation accuracy over 20 random seeds is shown for various values of \u03b2. The study compared LR schedules optimized by gradient descent (LRS-OPT) with other schedules like HD, RTHO & MARTHE. Figure 2 presents a qualitative comparison between offline and online schedules. HD schedules quickly decay, while RTHO schedules may cause instability. Fixing \u00b5 = 0.99 produces schedules that perform well. Average validation accuracy over 20 random seeds is shown for various values of \u03b2. The achieved average accuracy for different configurations is reported, with LRS-OPT attaining 96.2% accuracy. Using adaptation schemes for \u00b5 and \u03b2 allows for finding schedules that mimic the optimized one, but this is task-dependent. Fixing \u00b5 > 0 seems to have a beneficial impact for all tried values of the hyper-learning rate. Fixing \u00b5 > 0 has a beneficial impact for all hyper-learning rate values. Using adaptive mechanisms improves validation accuracy and reduces sensitivity to parameter choice. LRS-OPT takes over 2 hours on an NVIDIA GPU, while adaptive methods take less than a minute. Experimenting with various LR scheduling techniques, MARTHE outperforms fixed LR strategies like exponential decay and staircase decay. In comparing LR scheduling techniques, experiments were conducted using decay methods such as exponential decay, staircase decay, and stochastic gradient descent with restarts (SGDR). The experiments also included online strategies like HD and RTHO, with a fixed batch size and termination after 200 epochs. Different initial learning rates were set for SGDM and Adam, with specific decay factors and parameters for each method. We conducted experiments with LR scheduling techniques using decay methods like exponential decay, staircase decay, and SGDR. Different initial learning rates were set for SGDM and Adam, with specific parameters for each method. Experiments were run on CIFAR-10 with VGG-11 using SGDM and CIFAR-100 with ResNet-18 using Adam. Source code will be publicly available. In experiments with LR scheduling techniques on CIFAR-10 with VGG-11 and CIFAR-100 with ResNet-18, MARTHE produced competitive validation accuracy without requiring much tuning. MARTHE achieved 92.79% accuracy on CIFAR-10, on par with SGDR, and outperformed other adaptive algorithms. On CIFAR-100, MARTHE showed faster convergence compared to other methods. The MARTHE LR scheduling technique showed competitive validation accuracy on CIFAR-10 and faster convergence on CIFAR-100 compared to other methods. MARTHE produced aggressive schedules that improved convergence speed and final accuracy, reaching 76.68% accuracy in training. Additional experimental validation is provided in the appendix. The MARTHE LR scheduling technique offers competitive validation accuracy on CIFAR-10 and faster convergence on CIFAR-100. It uses an adaptive moving average over increasingly long hypergradient approximations, interpolating between HD and RTHO. The algorithm is simple to implement within deep learning environments, with moderate computational overhead. MARTHE is a general technique for finding online hyperparameter schedules and can be applied to other scenarios like tuning regularization parameters. Future work includes validating the method in different learning domains and automatically tuning other crucial hyperparameters. Future research directions include adapting the LR and tuning other hyperparameters. Learning adaptive rules for \u00b5 and \u03b2 in a meta learning fashion is also suggested. Table 1 provides a summary and notation used in the paper. A method for computing the dampening factor \u00b5 t based on a quantity is introduced in Section 4. If q(\u00b5 t ) is positive, the update \u2206\u03b7 t+1 is a descent direction for the objective f T. The heuristic rule proposed in the experiments involves setting a normalized, thresholded value for q(\u00b5 t) using a multiplicative counter c t. This heuristic is independent of the initialization of \u00b5 and helps optimize test functions successfully. For the Beale function, \u00b5 t = 1 is selected for all iterations, while for the smoothed Bukin function, \u00b5 t = 0 is chosen for around 40% of the iterations, reducing the optimality gap to 10^-6 for \u03b2 = 0.0005. Variants of the heuristic were also explored, including thresholding and penalizing updates larger than g 1. The experiments involved thresholding and penalizing updates larger than g 1 for h \u00b5. Randomly setting \u00b5 t to 0 was found to be unsuccessful, leading to the introduction of an undesirable configuration parameter. Further improvements for h \u00b5, such as adapting the hyper-learning rate, are suggested to capture long term dependencies. Meta-learning update rules could be a potential direction for future investigation. In this section, additional experimental results are presented for the CIFAR-100 dataset using a ResNet-18 model and SGDM as optimization methods. The initial learning rate is set to 0.1 for all methods, including MARTHE. MARTHE is able to achieve competitive results even when starting with an initial learning rate of 0. The schedules quickly reach high values and then sharply decrease within the first 40 epochs. Figure 6 shows a sample of a generated schedule. MARTHE is a parameterless method that can be used when prior knowledge of the task is lacking. High initial learning rates are not suitable for gradient-based adaptive strategies, as they can lead to instability. Different parameters were tried for SGDR to complete training after 200 epochs. In this section, the sensitivity analysis of inadaptive MARTHE with respect to \u03b7 0 and \u00b5 is studied, without applying the proposed online adaptive methodologies for \u00b5 and \u03b2. The impact of \u03b7 0, \u00b5, and \u03b2 for MARTHE is examined, emphasizing the importance of method sensitivity for effective performance in practical applications, especially in the context of hyperparameter optimization algorithms. Test accuracy results for VGG-11 on CIFAR-10 with SGDM as optimizer are presented, showcasing the sensitivity of inadaptive MARTHE with respect to \u03b7 0 and \u00b5 while fixing \u03b2 at different values. The sensitivity analysis of inadaptive MARTHE with respect to \u03b7 0 and \u00b5 shows a certain degree of sensitivity, especially with respect to the choice of \u00b5. Implementing adaptive strategies for computing the dampening factor \u00b5 and hyper-learning rate \u03b2 is essential for achieving competitive results. The plots in Figure 9 demonstrate the sensitivity of inadaptive MARTHE with respect to \u03b7 0 and \u00b5, with darker colors indicating lower final accuracy."
}