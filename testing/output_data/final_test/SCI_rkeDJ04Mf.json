{
    "title": "rkeDJ04Mf",
    "content": "In this work, a new method is proposed to enhance the robustness of convolutional neural networks (CNNs) against adversarial attacks. By using data dependent adaptive convolution kernels and a HyperNetwork to compute statistical adaptive maps, the CNN weights are filtered to generate dynamic kernels. This approach optimizes both weights and kernels for learning image classification models that are resilient to attacks. The proposed method enhances CNN robustness against adversarial attacks without additional algorithms. The proposed method enhances CNN robustness against various attacks, including Gaussian noise, fast gradient sign methods, and black-box attacks. Despite the power of deep convolutional neural networks in computer vision tasks, recent architectures have been found to be vulnerable to small image perturbations, leading to misclassification of images as adversarial examples. Adversarial examples pose a threat to safety-critical applications like autonomous driving systems using deep learning algorithms. Various attack methods for neural networks include unspecific statistical noise, gradient-based attacks for generating imperceptible misclassifications. Various attack methods for neural networks include white-box methods like (Iterative) Fast Gradient Sign Method, L-BFGS, Jacobian-based Saliency Map, and DeepFool, which require access to the full network architecture and weights. Black-box methods, on the other hand, only have access to the network outputs and may involve substitute networks and noisy pixel searches. Transferring adversarial examples from another network is not considered a genuine black-box method. The text discusses a new method to train CNNs by improving their robustness to adversarial perturbations using a HyperNetwork to adaptively filter convolution weights based on statistical properties of input data and features. The proposed method dynamically generates data-dependent convolution kernels and empirically verifies its effectiveness. The text discusses the robustness of proposed models using large scale vision dataset without using computationally complex defense methods. Several defense methods have been proposed in the last decade, such as training with adversarial examples and evolving uncertainty during training. The most intuitive approach is to employ adversarial examples during training phase. BID20 suggests ensemble training data with adversarial examples from pre-trained models to prevent degenerate minima. BID21 proposes adding a stability term to the objective function to maintain performance on the original task. BID13 introduces defensively distilling knowledge from a conjugate network with the same structure. BID9 introduces a method to augment neural networks with a conjugate network for detecting adversarial perturbations, which can be detected well despite being imperceptible to humans. However, these methods are cumbersome and resource-intensive, with longer training periods due to convergence issues caused by adversarial examples. Recently, De BID0 proposed a neural network architecture called Dynamic Filter, which uses dynamically generated weights conditioned on input to increase flexibility without significantly increasing model parameters. This architecture can learn various filtering operations such as spatial transformations and feature extraction. BID4 further generalized this with HyperNetworks, introducing static HyperNetworks as a weights factorization approach for deep convolutional networks. Static HyperNetworks are a weights factorization approach for deep convolutional networks, not widely used in large-scale vision tasks due to high weight dimensions. Adversarial examples are defined as instances where the prediction label remains the same while being close to the original example. Static HyperNetworks are a weights factorization approach for deep convolutional networks, not widely used in large-scale vision tasks due to high weight dimensions. Adversarial examples involve manipulating the input to make the network misclassify it or target a specific class with high confidence. Vanilla CNNs perform convolution between feature maps using stationary kernels. The main idea is to adaptively filter convolution weights of CNNs using a HyperNetwork. The network receives input channel-wise mean and standard deviation, outputs a map, computes Hadamard product over sub-kernels of the convolution weight, and employs adaptive convolution kernels for the operation. The network is a two-layer neural network with l2 regularization and ReLU activation function. The network utilizes ReLU in the hidden layer and sigmoid in the output layer. Parameters are updated using SGD with momentum. HyperNetworks back-propagate errors through the HyperNetwork route. Gaussian noise is commonly used as an attack type in the network. In this paper, a truncated version of Gaussian noise is used as the attack type. The method involves finding perturbations using a white-box approach and employing a targeted class for iterative changes. Tram\u00e8r et al. proposed a novel attack that applies random perturbations before optimizing under a first-order approximation. The algorithm proposed by DISPLAYFORM3 LocSerachAdv BID11 efficiently locates perturbed pixels to fool a deep neural network without using gradient information. It aims to push the true label below a certain threshold in the probability vector. The method generates perturbed versions of an image and selects pixels that deceive the network the most through a greedy search. This attack is effective as it does not rely on the target network's architecture or optimization. In the experimental evaluation, a new method called Relative Confidence Diminution (RCD) score is introduced to measure robustness to attacks in addition to classification accuracy. The RCD score computes the difference between Relative Confidence (RC) and variance of noisy labels, providing insight into the model's susceptibility to adversarial perturbations. In this section, different models like ResNet-18, ResNet-50, and a smaller Plain-10 network are evaluated using the ILSVRC-2012 dataset. The models are optimized for classifying 1000 classes and trained with specific hyper-parameters. Testing involves resizing validation images to 299x299 before evaluation. During testing, validation images are resized to 299x299 and a single center crop of 256x256 is fed into the networks. Three types of attacks are employed to generate adversarial examples, focusing on network robustness rather than perturbation perceivability. Gaussian noise is used to evaluate network robustness, with results showing improved performance when equipped with SHC. The robustness of networks equipped with SHC is enhanced in all cases, maintaining the same robustness towards Gaussian noise as the Plain-10 network, with a 2% improvement for ResNet-18 and ResNet-50. Evaluation of network robustness against Fast Gradient Sign Method and its variants is conducted using 5,000 validation images for each comparison pair. Adversarial examples are created using specific equations, with parameters adjusted for different attack scenarios. Additionally, model robustness against LSA attack is assessed using 500 images initially correctly classified with high confidence, with RCD values reported over attack steps. The proposed SHC networks show robustness to gradient-based attacks, with different attack methods affecting networks differently. For example, RAND+FGSM is more effective against ResNet-50 compared to I-FGSM, while ResNet-50-SHC shows similar effectiveness with both methods. Robustness in vanilla CNNs is related to architecture depth and training difficulty, indicating a connection to attack gradient back-propagation. The SHC based ResNet-50 performs worse than ResNet-18 in 3 of 4 situations. The LocSearchAdv algorithm is used to examine the robustness of the proposed method to black-box attacks by perturbing 1,500 pixels. The target is to decrease the confidence of the correct class, even if the image has been mis-classified. Results are provided in Table 3. The proposed method outperforms SHC models in the early stages of the LSA attack, taking about 60 more steps for SHC models to achieve similar results. Adversarial examples can be generated successfully with enough attempts, although it may take more tries to fool the proposed method. Changes in confidence and RCD scores are depicted in FIG2, with images correctly classified marked in green and mis-classified in red frames. The obtained M at the last convolution layer of ResNet-50-SHC is also provided. The proposed method maintains high confidence during attacks, correctly classifying adversarial examples at step 150. The robustness of a deep neural network is determined by the decision boundary, with the concept of quasi decision boundary used to examine SHC network robustness against black-box attacks like LocSearchAdv. The text discusses the use of LocSearchAdv algorithm to define a neighbourhood of an input image in an SHC network. It introduces the concept of a quasi decision boundary for sub-networks in classification tasks. The text discusses the use of LocSearchAdv algorithm to define a neighbourhood of an input image in an SHC network. It introduces the concept of a quasi decision boundary for sub-networks in classification tasks. The quasi decision boundary is a good approximation of the true decision boundary, but breaks once the target is out of the neighbourhood. The greedy search algorithm of LSA uses random perturbed inputs to estimate perturbations and generate an adversarial image. However, as the perturbations accumulate, the strength of perturbations becomes unreliable, leading to halted or reversed attack progress until the true decision boundary is broken. ResNet-50-SHC performs worse towards most gradient-based attacks. The ResNet-50-SHC performs worse against gradient-based attacks compared to ResNet-18-SHC, despite having better accuracy with clean and noisy images. To analyze the SHC model's robustness, experiments were conducted attacking different sub-networks. Gradients were blocked between the HyperNetworks route and the sub-network F H to generate adversarial examples for evaluation within the SHC-network. The proposed method using HyperNetworks helps weaken adversarial attack gradients in CNNs, improving robustness. The SHC model's performance against attacks was analyzed by blocking gradients between HyperNetworks and sub-networks for evaluation within the SHC-network. The use of HyperNetworks in training CNNs improves robustness against adversarial attacks without sacrificing performance on original tasks. However, the mechanism behind this robustness and the design of network architectures with even better adversarial robustness are still areas of uncertainty and open problems for future research."
}