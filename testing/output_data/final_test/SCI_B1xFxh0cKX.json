{
    "title": "B1xFxh0cKX",
    "content": "Many applications in machine learning require optimizing a function with unknown true gradient but available surrogate gradient information. Guided Evolutionary Strategies propose a method to optimally use surrogate gradient directions along with random search, allowing estimation of a descent direction for first-order optimization. Tradeoffs resulting from this approach are characterized analytically and numerically. Guided Evolutionary Strategies propose a method to optimally use surrogate gradient directions along with random search for first-order optimization. Analytical and numerical tradeoffs are characterized, leading to improved performance in various example problems. When gradient information is not available, zeroth-order optimization methods like evolutionary strategies can be used. However, when only partial gradient information is accessible, there are two extreme approaches: ignoring the surrogate gradient information entirely or directly feeding it to a first-order optimization algorithm. In this work, a method called guided evolutionary strategies (Guided ES) is proposed to combine unbiased descent direction estimated with evolutionary strategies and low-variance estimate given by surrogate gradients. The critical assumption is having access to surrogate gradient information when the true gradient is unknown or hard to compute in machine learning problems. In this work, guided evolutionary strategies (Guided ES) combine unbiased descent direction estimated with evolutionary strategies and low-variance estimate given by surrogate gradients. Models with discrete stochastic variables use methods like Concrete/Gumble-Softmax, while learned models in reinforcement learning utilize Q functions or value estimation. Examples of optimization methods include truncated backprop. Guided ES combines surrogate gradient information with random search to reduce variance in the search direction. The method focuses on a low-dimensional guiding subspace defined by recent surrogate gradients, improving optimization efficiency. Contributions include a new approach for combining surrogate gradients with random search and an analysis of the bias-variance tradeoff. The work introduces a method that combines surrogate gradient information with random search to improve optimization efficiency. It builds upon evolutionary strategies by augmenting the search distribution with surrogate gradients. The method also includes an analysis of the bias-variance tradeoff and a scheme for choosing optimal hyperparameters. Applications to example problems are discussed as well. The method combines surrogate gradient information with random search to enhance optimization efficiency. It augments evolutionary strategies by adapting the search distribution with surrogate gradients, avoiding the need for a full covariance matrix. The goal is to minimize a function over a parameter space in n-dimensions where gradient information is either unavailable or uninformative. The method combines surrogate gradient information with random search to enhance optimization efficiency by using antithetic sampling to reduce variance. The estimator relies on computing 2P function evaluations and is referred to as vanilla evolutionary strategies (or vanilla ES). It tends to have high variance, requiring a large number of samples to be practical and scaling poorly with the dimension n. The method combines surrogate gradient information with random search to enhance optimization efficiency by using antithetic sampling to reduce variance. It leverages additional information about f from prior knowledge or previous iterates during optimization. By assuming biased or corrupted gradients, a set of vectors is used to generate a subspace for optimization. This subspace is denoted by an orthonormal basis U, which changes the distribution of i in eq. (1) to N (0, \u03c3 2 \u03a3) with a hyperparameter \u03b1 that trades off variance between the full parameter space and the subspace. The hyperparameters in the optimization process include \u03b1, which trades off variance between the full parameter space and a subspace, and \u03b2, controlling the descent direction size. The parameter \u03c3^2 influences the variance scale and noise in gradient estimates. The estimator requires 2P function evaluations and can be parallelized for efficiency. The method involves using a surrogate gradient to guide a random search procedure in optimization. A second order Taylor approximation is used to estimate the function in the local neighborhood, leading to an estimate of the gradient. The Guided ES method uses antithetic sampling to cancel out terms in the Taylor expansion for parameter updates. There is a bias-variance tradeoff in the estimate g, where choosing \u03b1 close to 1 reduces bias but increases variance, while choosing \u03b1 close to 0 induces bias for reduced variance. The method always provides a descent direction in expectation. The Guided ES estimator ensures a descent direction in expectation by aligning the guiding subspace with the true gradient. The correlation coefficients \u03c1 quantify this alignment, ranging from zero to one. The normalized squared bias of the estimate g is evaluated, consisting of two terms: one independent of the search space and the other dependent on the uncentered correlation. The variance of the estimator is quantified using the total variance and normalized variance, which are dependent on the correlation and parameters of the distribution. Increasing the number of function evaluations decreases the variance linearly. Increasing the number of function evaluations decreases the variance linearly. The tradeoff between normalized bias and variance for different hyperparameter settings is explored in FIG1, helping to choose optimal values for the hyperparameters. The sum of normalized bias plus variance is shown as a function of the tradeoff and scale hyperparameters, with the global minimum indicating the optimal values. The expressions for bias and variance depend on the subspace and parameter dimensions, as well as the hyperparameters. The hyperparameters of the guiding distribution and the uncentered correlation between the true gradient and the subspace are key factors in minimizing bias and variance. Optimal hyperparameters are chosen to minimize the expected normalized square error in the gradient estimate, subject to specific constraints. The relationship between hyperparameters and the descent of the function is further explored, leading to the determination of optimal tradeoff and scale hyperparameters based on certain variables. The optimal value for the tradeoff hyperparameter (\u03b1) is determined based on the correlation (\u03c12) and ratio of subspace dimension to parameter dimension (kn). In regions where the subspace is high quality and small relative to the full space, \u03b1 is set to zero. Conversely, in regions where the subspace is large and low-quality, \u03b1 is set to one. An intermediate regime exists where \u03b1 is between 0 and 1. The optimal value for the tradeoff hyperparameter (\u03b1) is determined based on the correlation (\u03c12) and ratio of subspace dimension to parameter dimension (kn). A reparameterization technique is used to express the objective as a least squares problem subject to a non-negativity constraint. The point where the non-negativity constraint becomes tight is identified using complementary slackness conditions, yielding equations \u03c12 = k+4n+4 and \u03c12 = kn. The optimal hyperparameters are determined based on the correlation (\u03c12) and subspace dimension to parameter dimension ratio (kn). In practice, setting \u03b2 = 2 and \u03b1 = 1 2 works well. Future work could involve estimating the correlation \u03c12 online to choose hyperparameters. Initial testing was done on a toy problem with controlled bias in the surrogate gradient. Guided ES utilizes biased gradients for quick descent followed by random search, outperforming Vanilla ES and CMA-ES. Surrogate gradients are also beneficial in unrolled optimization. Further experimental details are available in Appendix E.1. Surrogate gradients are used in unrolled optimization, where derivatives are taken through an optimization process. This approach has been applied to optimize hyperparameters, stabilize training, and train neural networks to act as optimizers. However, recent research has shown that this method can yield biased gradients. To address this bias, Guided ES was used to train multi-layer perceptrons to predict learning rates based on the eigenvalues of the Hessian at the current iterate. In FIG3, bias induced by unrolled optimization is shown for different optimization steps in MLP. The absolute difference between optimal learning rate and MLP prediction is compared for various optimization algorithms. Guided ES is explored for training models to generate synthetic gradients when surrogate gradients are not provided. This approach is crucial in model-based and actor-critic methods in RL. The approach of using Guided ES to generate synthetic gradients is crucial in model-based and actor-critic methods in RL. A parametric model is defined to provide synthetic gradients for the target problem, and despite varying quality, Guided ES consistently makes progress. Experimental details are provided in Appendix E.3, and Guided ES is also applied to train neural networks with discrete variables. In training neural networks with discrete variables, Guided ES was applied using a surrogate gradient. The encoder weights were updated with Guided ES, leading to a small improvement in training loss compared to vanilla ES. This approach addresses biased gradients and suboptimal encoder weights. Guided ES combines first-order methods and random search, using surrogate gradients correlated with the true gradient. It addresses biased gradients and suboptimal encoder weights, demonstrating a high quality of surrogate gradients. The method applies Guided ES update to parameters with surrogate gradients and first-order methods to parameters with unbiased gradients. Further experimental details are provided in Appendix E.4. The technique is applied to unrolled optimization, synthetic gradients, and training neural networks with discrete variables. The squared bias norm is defined, and the expression for the normalized bias is derived. An identity is stated for a Gaussian distribution, and the total variance is calculated using this identity. The expression for the total variance is then simplified. The total variance expression is simplified by dividing by the norm of the gradient. Optimal hyperparameters are defined to minimize the sum of normalized bias and variance, with a reparameterization technique used to rewrite the problem. The constraints on the original parameters are expressed as non-negativity constraints in the new parameters. The optimal hyperparameters are defined to minimize bias and variance, with a reparameterization technique used. The Lagrangian and dual problem are formulated to ensure primal and dual optimality. The lagrange multipliers are calculated as the residuals, and complimentary slackness conditions are applied. The Lagrange multipliers are calculated as residuals, and complimentary slackness conditions are applied to find the transition points where constraints become tight. Two solutions are obtained, yielding upper and lower bounds for optimal hyperparameters. The equations for optimal hyperparameters in FIG2 show that choosing hyperparameters that descend the simple quadratic loss rapidly is equivalent to minimizing the expected square error in the estimated gradient. This supports the method used to choose hyperparameters in the main text. The expected loss after a single training step is derived, showing the equivalence between the descent direction and the gradient used for choosing hyperparameters in \u00a73.4. In this section, the computational and memory costs of Guided ES are compared to standard evolutionary strategies and gradient descent. The costs are outlined in terms of parameter dimension, number of function evaluation pairs, and cost of computing the full loss. The table provided shows the per-iteration compute and memory costs for gradient descent, standard evolutionary strategies, and Guided ES. The text discusses the comparison of computational and memory costs between gradient descent, standard evolutionary strategies, and guided evolutionary strategies. It includes details on the costs for each method and specifies the target problem of linear regression with specific parameters. The surrogate gradient for optimization was generated by adding a random bias and noise to the gradient, scaled to have the same norm. Plots in Figure 1b show loss suboptimality, with parameters initialized to zeros and optimized for 10,000 iterations. A coarse grid search over learning rates was performed for each optimization algorithm. The learning rates for different optimization methods were chosen after a grid search, with values of 5e-3 for gradient descent, 0.2 for guided and vanilla ES, and 1.0 for CMA-ES. Other parameters such as variance of perturbations and subspace dimension were also set. The target problem involved optimizing a quadratic loss function using a multilayer perceptron with specific architecture and inputs. The MLP used 10 eigenvalues of the Hessian as inputs and a softplus nonlinearity for the output. Surrogate gradients were generated through truncated backpropagation. FIG3 shows the progress of optimizing the MLP to predict the learning rate using different algorithms. The learning rates for gradient descent, guided ES, and vanilla ES were chosen as 0.3, 0.5, and 10 respectively. The target problem involved a mean squared error objective with a random sampled x*. Surrogate gradients were calculated using a multilayered perceptron with relu activations. The model was optimized online. The model M was optimized online by minimizing mean squared error with function observations. Data D was randomly sampled in batches from recent function evaluations. Evolutionary strategies algorithms generated function samples during optimization. Training data for model M was generated by sampling points around the current iterate. The model M was optimized online by minimizing mean squared error with function observations. Data D was randomly sampled in batches from recent function evaluations. Evolutionary strategies algorithms generated function samples during optimization. Training data for model M was generated by sampling points around the current iterate. ES (Normal with \u03c3 = 0.1) was used to ensure similar training data spread for M in the replay buffer when optimizing with Adam. Results of the three algorithms were compared over 10 random instances, with \u03c3 = 0.1 and P = 1 pair of samples per iteration. Guided ES used a subspace dimension of k = 1. The spread of data used to train M was controlled by \u03c3, which was tuned with a coarse grid search. A vector quantized variational autoencoder (VQ-VAE) was trained on MNIST with specific network configurations and a small codebook. For vector quantization, a small codebook with twelve vectors was used. The codebook and latent variables had a dimensionality of 16, with 10 latent variables. Training the encoder weights involved using a straight through estimator BID2 to bypass discretization. Guided ES was used with correct gradients for the decoder and embedding weights, while first-order methods were used for vanilla ES or Guided ES. Training iterations involved 10 pairs of function evaluations in parallel to reduce variance."
}