{
    "title": "BkM27IxR-",
    "content": "Learning to Optimize is a framework for learning optimization algorithms using reinforcement learning. The extension developed in this paper is tailored for training shallow neural nets, outperforming other algorithms on various tasks like MNIST, Toronto Faces Dataset, CIFAR-10, and CIFAR-100. The learned optimization algorithm is robust to changes in stochasticity of gradients and neural net architecture. Machine learning has revolutionized various application areas, but the design of tools for machine learning itself has been overlooked. Optimization algorithms are a crucial tool in machine learning, traditionally designed by human experts through theoretical analysis and empirical validation. Learning to Optimize introduces a framework for training optimization algorithms using reinforcement learning, showing superior performance on tasks like MNIST, Toronto Faces Dataset, CIFAR-10, and CIFAR-100. In the context of machine learning, BID20 and BID0 introduced frameworks for learning optimization algorithms. BID20 aims to develop a task-independent optimization algorithm, while BID0 focuses on task-specific optimization. This paper focuses on learning an optimization algorithm for high-dimensional stochastic optimization problems, like training shallow neural nets, under the \"Learning to Optimize\" framework proposed by BID20. The algorithm is formulated as a reinforcement learning problem. The algorithm in Algorithm 1 updates the current iterate x (i) using a step \u2206x in each iteration. Different choices of the update formula \u03c6 yield different optimization algorithms. An optimization algorithm can be seen as a Markov decision process, where the state includes the current iterate and the action is the step vector \u2206x. Learning \u03c6 reduces to a policy search problem. In this paper, an extension of a method proposed in BID20 is developed for learning optimization algorithms for high-dimensional stochastic problems. The optimization algorithm outperforms popular hand-engineered algorithms like ADAM, AdaGrad, and RMSprop. It also generalizes well to different datasets such as the Toronto Faces Dataset, CIFAR-10, and CIFAR-100. The work on learning optimization algorithms is recent, with BID20 being the first to propose task-independent optimization algorithms. The BID20 study focused on learning task-independent optimization algorithms using reinforcement learning, while BID0 explored task-dependent optimization algorithms using supervised learning. These methods can be applied for \"learning to learn\" or \"metalearning\" when the objective functions are loss functions for training other models. Different methods aim to learn meta-knowledge about learning, with some focusing on parameter values useful across related tasks. The meta-knowledge captures commonalities shared by tasks in a family, enabling quicker learning on new tasks. Methods aim to determine the best base-level learner for a task by capturing correlations between tasks and base-level learner performance. One challenge is to parameterize the space of base-level learners effectively for tractable search. BID8 proposes a nonparametric representation for storing examples of different base-level learners. BID8 proposes a nonparametric representation for storing examples of different base-level learners, while BID23 suggests representing base-level learners as general-purpose programs. BID16 views the training procedure of base-learners as a black box function modeled by a recurrent neural net, simplifying meta-training to training the recurrent net. Hyperparameter optimization is another method in this category, parameterizing the space of base-level learners with predefined hyperparameters. Multiple trials with different hyperparameter settings on the same task are also considered. Hyperparameter optimization methods like Bayesian optimization, random search, and gradient-based optimization aim to learn a good algorithm for training a base-level learner. The goal is to capture commonalities in learning algorithm behaviors that achieve good performance, with the learned algorithm needing to generalize across base-level learners and tasks. Learning a learning algorithm often reduces to learning an optimization algorithm, explored in various studies. Related work includes learning how to adjust hyperparameters of optimization algorithms and parameterizing operands of solvers for optimization problems. In the \"Learning to Optimize\" framework, optimization algorithms are trained using supervised learning on a set of objective functions. The goal is to learn an algorithm that minimizes a meta-loss function, penalizing undesirable behaviors like slow convergence. In the \"Learning to Optimize\" framework, optimization algorithms are trained using supervised learning on a set of objective functions. A good choice of meta-loss would be DISPLAYFORM2, equivalent to cumulative regret, representing the area under the curve of objective values over time. The objective functions correspond to loss functions for training base-level learners, with the algorithm learning the optimization algorithm acting as a meta-learner. The learned optimization algorithm is evaluated on unseen objective functions at test time. The optimization algorithm in the \"Learning to Optimize\" framework is trained on objective functions for base-learners, which are unrelated to the tasks used for training. The goal is to exploit the geometric structure of the error surface induced by the base-learners. The meta-learner should generalize across tasks, while the base-learner should be task-specific. The goal of reinforcement learning is to minimize cumulative costs in a partially observable Markov decision process (POMDP). The environment is defined by probabilities over states and actions, with unknown densities often given to the learning algorithm. A policy is a conditional probability over actions based on current observations. The reinforcement learning algorithm aims to learn a policy that minimizes total expected cost over time. The policy is often constrained to a parameterized family, with functions modeled using function approximators. The state consists of the current iterate and features that depend on the history of iterates. The reinforcement learning algorithm learns a policy to minimize total expected cost over time using a parameterized family of functions modeled with function approximators. The state includes the current iterate, gradients, and objective values, while the observation excludes the current iterate and consists of features that depend on recent iterations and the previous memory state of the optimization algorithm. The memory state is learned jointly with the policy and serves as a statistic of previous observations. The transition probability density captures how gradients and objective values are likely to change given the current step, encoding the local geometry. The reinforcement learning algorithm aims to minimize total expected cost over time by learning a policy using a parameterized family of functions. The policy is modeled with function approximators and includes the current iterate, gradients, and objective values. The observation features depend on recent iterations and the previous memory state of the optimization algorithm. The transition probability density captures how gradients and objective values are likely to change given the current step, encoding the local geometry of the training objective functions. Learning an optimization algorithm reduces to searching for the optimal policy, which is modeled as a recurrent neural net fragment that outputs the step to take. The reinforcement learning method used is guided policy search (GPS), a policy search method designed for searching over large classes of policies. Guided Policy Search (GPS) is a policy search method for searching over large classes of policies. It maintains two policies, \u03c8 and \u03c0, where \u03c8 is a time-varying linear policy and \u03c0 is a stationary non-linear policy. GPS solves a constrained optimization problem to optimize these policies, using the resulting policy from \u03c8 as supervision to train \u03c0. The problem is relaxed by enforcing equality on the mean actions taken by \u03c8 and \u03c0 at every time step. The problem is solved using Bregman ADMM, which updates in each iteration. The algorithm assumes \u03c8 (a|s,t;\u03b7) = N(Ks + k, G), where \u03b7:\u03b8:\u03c9,\u03a3\u03c0 and \u00b5\u03c0\u03c9(\u00b7) can be modelled using a neural net. At the start of each iteration, a model of the transition probability density p(st+1|st,at,t;\u03b6) = N(As + Bat + c, F) is constructed. The algorithm fits local quadratic approximations to c(st) around samples drawn from the trajectory induced by \u03c8. The subproblem to update \u03b7 involves solving a constrained optimization problem using a dynamic programming algorithm known as linear-quadratic-Gaussian (LQG) regulator. The constrained problem is solved using dual gradient descent, updating \u03b8 straightforwardly as \u03c0 is decoupled from the transition probability. The subproblem to update \u03b8 amounts to a standard supervised learning problem, with D t (\u03b8, \u03b7) computable analytically. The problem of learning high-dimensional optimization algorithms in reinforcement learning presents challenges due to the state and action space dimensionality. The running time of LQG is cubic in the state space dimension, making policy search expensive. However, many high-dimensional optimization problems have exploitable underlying structure. Neural nets parameters can be permuted without changing function. Optimization algorithm should be invariant to permutations of weight matrix rows and columns. Permutation invariance enforced on coordinate group. For optimization algorithms in neural nets, coordinate groups correspond to weight matrices or bias vectors. The parameters are structured in a block-diagonal form for updating, ensuring independence in transition probability densities. The parameters in neural nets are structured in a block-diagonal form for updating, ensuring independence in transition probability densities. This leads to a decomposition of the Bregman divergence penalty term into multiple independent subproblems, one for each coordinate group. In LQG, efficiency is improved by executing on each subproblem separately. Parameters are shared across coordinates in the same group for \u03c0. \u03a3 \u03c0 has a block-diagonal structure with shared entries in appropriate sub-matrices. State features \u03a6(\u00b7) and observation features \u03a8(\u00b7) at time step t are described. State features \u03a6(\u00b7) are defined in terms of summary statistics of history of iterates, gradients, and objective values. State features \u03a6(\u00b7) include the relative change in recent objective value, normalized gradient, and iterate changes. Observation features \u03a8(\u00b7) are used during training and testing, with a focus on efficiency and memory usage. The optimization algorithm, referred to as \"meta-training\", was trained on a two-layer neural net with specific dimensions and evaluated on its ability to generalize to different tasks/datasets. It was modeled using a recurrent neural net with specific parameters and evaluated on three datasets including the Toronto Faces Dataset (TFD). The optimization algorithm was evaluated on three datasets: Toronto Faces Dataset (TFD), CIFAR-10, and CIFAR-100, each with unique characteristics. Hand-engineered algorithms were compared to a meta-trained optimization algorithm on the same training objective function. The optimization algorithm meta-trained using Predicted Step Descent consistently descends to the optimum the fastest across all datasets, outperforming other algorithms on the same training objective function. The optimization algorithm meta-trained using Predicted Step Descent consistently outperforms other algorithms on various datasets, even when the neural net architecture is changed. Predicted Step Descent algorithm excels in optimizing neural nets, showing quick recovery and outperforming other algorithms on TFD and CIFAR-10. It adapts well to performance fluctuations and handles increased stochasticity effectively, unlike L2LBGDBGD which struggles and slowly diverges. Reducing mini-batch size from 64 to 10 shows Predicted Step Descent still outperforming other algorithms on both original and enlarged architectures. In contrast to other algorithms, Predicted Step Descent handles increased stochasticity well on TFD and CIFAR-10, showing significant oscillations but achieving better results. Hand-engineered algorithms faced greater challenges, with L2LBGDBGD quickly diverging on the datasets. Doubling the number of iterations shows Predicted Step Descent performing reasonably beyond its training limit. The paper introduces a new method for learning optimization algorithms for high-dimensional stochastic problems. The method was applied to training shallow neural nets and showed generalization to tasks like the Toronto Faces Dataset, CIFAR-10, and CIFAR-100. The learned optimization algorithm proved robust to changes in gradient stochasticity and neural net architecture."
}