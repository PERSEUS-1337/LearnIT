{
    "title": "Bye5SiAqKX",
    "content": "In a unified framework, two types of preconditioners and preconditioned stochastic gradient descent (SGD) methods are studied: the Newton type, closely related to the Newton method, and the Fisher type, with a preconditioner related to the inverse of Fisher information matrix. These preconditioners can be efficiently estimated on matrix Lie groups using natural or relative gradient descent. Various existing preconditioners and methods are either special cases or closely related to the Newton or Fisher types. Experimental results on large scale machine learning problems are presented to study performance acceleration with preconditioners. The use of preconditioner for accelerating gradient descent in large scale machine learning problems, including popular methods like SGD, momentum, RMSProp, Adagrad, and Adam. Convex optimization methods are well studied but may struggle with gradient noise and scaling up to problems with many parameters. Natural gradient with the Fisher information is proposed for a large family of machine learning problems. The Fisher information metric is used as a preconditioner in natural gradient methods for machine learning problems. Other preconditioners include equilibrated and momentum-based methods. These preconditioners can be combined to accelerate convergence, as seen in the Adam algorithm. Preconditioners are grouped into Newton type and Fisher type, with the former suitable for general purpose optimizations. The Fisher type preconditioner is related to the inverse of the Fisher information matrix and is used in a subclass of stochastic optimization problems. It can be derived from the same framework as other preconditioners and estimated on matrix Lie groups. The preconditioner is used in minimizing specific estimation criteria for the cost function in machine learning problems. The Fisher type preconditioner, derived from the same framework as other preconditioners, is used in a subclass of stochastic optimization problems. It is related to the inverse of the Fisher information matrix and can be estimated on matrix Lie groups. The preconditioner is utilized in minimizing specific estimation criteria for the cost function in machine learning. The quadratic surface in the trust region could be non-convex due to the assumption-free nature of the symmetric matrix H. Trust region could be non-convex. Simplifying notations, higher order approximation errors are not considered. Preconditioned SGD updates \u03b8 as DISPLAYFORM0 with step size \u00b5, f(\u03b8) as a quadratic function, and matrix P as a preconditioner. By letting \u03b8 = P^-0.5 \u03b8, (2) can be rewritten as DISPLAYFORM1. Preconditioned SGD is equivalent to SGD in a transformed parameter domain. Stochastic gradient is written explicitly as DISPLAYFORM2. Combining equations gives a linear system update for \u03b8. The linear system for updating \u03b8 within the trust region is given by equation (5), where a properly determined preconditioner P could accelerate convergence. The preconditioner is pursued by minimizing criterion c(P) as shown in equation (7), where expectations are taken over \u03b4\u03b8. The preconditioner P accelerates convergence by minimizing criterion c(P) in equation (7), determining a unique positive definite P that optimally preconditions the stochastic gradient. This preconditioner perfectly matches the amplitudes of parameter perturbations with the associated preconditioned gradient, regardless of gradient noise. The preconditioner P accelerates convergence by minimizing criterion c(P) in equation (7), determining a unique positive definite P that optimally preconditions the stochastic gradient. However, the optimal P and P^-1 given by (8) are not unbiased estimates of H^-1 and H, respectively. Handling small numbers with floating point arithmetic can lead to numerical errors, especially with single and half precision math in large scale neural network training. Luckily, (6) relates \u03b4\u011d to the Hessian-vector product, which can be efficiently evaluated with automatic differentiation software tools. The Hessian-vector product can be efficiently evaluated with automatic differentiation software tools. A new preconditioner estimation criterion is introduced, which only requires the Hessian-vector product and not the Hessian itself. This criterion is related to the Newton method and is used in machine learning problems where the empirical Fisher information matrix can be well defined. The curr_chunk discusses the optimization of a positive definite solution for a function using a preconditioner related to the Fisher information matrix. It also introduces a modification to obtain an unbiased estimation of the inverse Fisher information matrix. Additionally, it mentions the use of an exponential moving average in the optimization process. The curr_chunk discusses the unbiased estimation of the inverse Fisher information matrix using a modified exponential moving average. It highlights the importance of regularization for singular statistical models and the use of preconditioners for optimization. The curr_chunk discusses the unique positive definite preconditioner determined by criterion (10) when DISPLAYFORM0 has distinct eigenvalues. Other solutions of FORMULA0 are either indefinite or negative definite and are not relevant. The proof is not novel and is omitted. For scalar parameter \u03b8, optimal solutions minimizing (10) are found to be DISPLAYFORM1. Positive preconditioner is chosen for gradient descent, leading to eigenvalue DISPLAYFORM2 for the locally linear system in (5). The optimal preconditioner damps gradient noise and ensures convergence with a normalized step size. Eigenvalues of the locally linear system are normalized into the range [-1, 1]. The updating rule for the Newton type preconditioner is derived, with the Fisher type preconditioner using stochastic gradient. The preconditioner is represented as a symmetric and positive definite matrix, estimated as Q. Lie group always refers to the matrix Lie group. The preconditioner is represented as Q, which must be nonsingular. In practice, Lie groups with sparse representations are of interest. A small perturbation \u03b4Q is considered to ensure Q + \u03b4Q remains on the same Lie group. The distance between Q and Q + \u03b4Q is defined using a tensor metric. The natural gradient for optimizing Q is derived based on the group of invertible upper triangular matrices. The natural gradient for optimizing the preconditioner Q is derived from the group of invertible upper triangular matrices. The relative gradient can also be used for updating Q, with a normalized step size. The updating rule involves the norm of a matrix and the maximum absolute value. The natural gradient for model parameter learning should not be confused with the natural gradient on the Lie group derived from a tensor metric. The Newton type preconditioned SGD involves evaluating stochastic gradient and normalizing step sizes. The preconditioner Q is initialized as a scaled identity matrix, with R depending on the Lie group. The Fisher type preconditioned SGD can be obtained by replacing \u0124 with \u011d+\u03bbv. The step size for preconditioner updating is normalized. The preconditioned SGD methods involve updating step sizes and damping factors for the preconditioner Q. It is recommended to avoid explicit matrix inversion for large Q and to update the preconditioner less frequently to save computations. Parallel computing can be used to update the preconditioner and model parameters simultaneously and asynchronously. Preconditioned SGD methods involve updating step sizes and damping factors for the preconditioner Q. It is recommended to avoid explicit matrix inversion for large Q and update the preconditioner less frequently to save computations. Parallel computing can be used to update the preconditioner and model parameters simultaneously and asynchronously, potentially accelerating convergence. For recurrent neural network learning, clipping the norm of preconditioned gradients can prevent excessively large parameter updates. Preconditioned gradient clipping is related to the trust region method, where a clipping threshold \u2126 can be set proportional to the square root of the number of model parameters. Choosing different Lie groups for preconditioner estimation can achieve a good balance between performance and complexity, with sparse structures being more practical for large problem sizes. Lie groups with sparse structures are of interest for constructing sparse preconditioners. Simple rules involving Kronecker product and direct sum can be used to design useful Lie groups. Examples include invertible upper triangular matrices and diagonal matrices with positive diagonal entries. These matrices form Lie groups with reducible representations, useful for constructing diagonal preconditioners. Preconditioners learned on Lie groups with sparse structures, such as diagonal preconditioners, involve flattening the matrix parameter \u0398 into a vector and using a Kronecker product preconditioner. The role of Lie groups in learning affine transformations is explored, with a focus on coordinate transformations and the preconditioned SGD learning rule. The affine transformation is rewritten in terms of transformed input and output feature vectors. The preconditioned SGD involves transformed input and output feature vectors x and y. Feature whitening and normalization can speed up convergence. A special Kronecker product preconditioner with diagonal and sparse matrices is used for learning affine transformations. The transformed features y = Q^T1y and x = Q2x are scaled and normalized. The feature normalization operation forms a Lie group with four freedoms, where the scaling-and-normalization preconditioner does not require the last diagonal entry of Q2 to be 1. The group of feature normalization operation is a subgroup of Q2, which is a special Kronecker product preconditioner with Q1 as a diagonal matrix and Q2 as an upper triangular matrix with positive diagonal entries. The scaling-and-whitening preconditioner involves augmenting the input feature vector with 1 to form a Lie group represented by upper triangular matrices. Different Lie groups can be used for constructing preconditioners, such as the Kronecker product preconditioner.Normalization and whitening groups are special cases of these groups, with various choices available. In FORMULA0, various preconditioners with different sparsities can be efficiently learned using natural or relative gradient descent. Adagrad, RMSProp, and Adam use a Fisher type preconditioner on diagonal matrices. The optimal solution for the preconditioner has a closed-form solution, and a simple exponential moving average is used in practice. The equilibration preconditioner reduces to a vector with unit entries. The equilibration preconditioner in BID4 is obtained from an optimal solution reducing to a vector with unit entries. Our method provides an iterative solution to approximate the Fisher metric inverse, avoiding explicit matrix inversion by using back substitution with an auxiliary vector. This approach offers advantages over KFAC, especially on GPUs with large matrices. The method presented offers a faster alternative to matrix inversion by using back substitution with an auxiliary vector. It is derived from a unified framework and does not require different preconditioner learning rules when switching Lie group representations. Explicit input feature normalization may not always be beneficial, especially in scenarios like recurrent neural network learning. The Newton and Fisher type preconditioned SGD methods provide a more general and principled approach to finding the optimal preconditioner. The method presented offers a faster alternative to matrix inversion by using back substitution with an auxiliary vector. It is derived from a unified framework and does not require different preconditioner learning rules when switching Lie group representations. The Newton and Fisher type preconditioned SGD methods provide a general approach to finding the optimal preconditioner, with a focus on the square root Fisher type preconditioners for numerical robustness on large scale problems. The Pytorch implementation package shows that original Fisher type preconditioners may perform better on small scale problems like the MNIST handwritten digit recognition task. The minimization of the Rosenbrock function is considered as a benchmark problem for mathematical optimization, with methods using fixed step sizes for optimization. The best step sizes for various optimization methods are selected from a sequence. Gradient descent and momentum methods use a step size of 0.002, while Nesterov momentum uses 0.001. Preconditioned SGD initializes Q as 0.1I and operates on triangular matrices. The Newton method outperforms others, converging in about 200 iterations. The Fisher method performs poorly. The Newton type preconditioned SGD works well for mathematical optimization using the ImageNet ILSVRC2012 database for image classification with AlexNet. Modifications include not augmenting training data and using a modified LRN function. Approximations are made for Hessian-vector product evaluation due to LRN function limitations. Convolution is rewritten as correlation between input image patches and filter coefficients, resulting in eight matrices to optimize in AlexNet. The AlexNet optimization process involves training with various methods and settings, such as batch normalization and preconditioned SGD with specific parameters. The preconditioned SGD performs better with a scaling-and-normalization preconditioner, initialized with specific values and updated step size. The Q Q Q is initialized to 0.1I I I and updated with a step size of 0.01. For the Fisher type preconditioner, \u03bb is set to 0.001 with an initial step size of 0.00005. The Newton type preconditioner has an initial step size of 0.01. Batch normalization's training loss is only for reference due to conflicts with LRN and L2-regularization. The scaling-and-normalization preconditioner accelerates convergence but is super sparse. Newton type preconditioned SGD performs the best, achieving 56% top-1 validation accuracy with one crop for testing. The momentum method may require 90 epochs for similar performance. The world level language modeling problem is considered with a reference implementation available at https://github.com/pytorch/examples. The task involves predicting the next token from history observations using a network with six layers: encoding, LSTM, dropout, LSTM, dropout, and decoding. Matrices are optimized for each LSTM layer, with the encoding layer's weight matrix being the transpose of the decoding layer's. Three matrices are optimized with hidden layer size 200. The step size is reduced when the current perplexity on the validation set is higher than the best found. For SGD, the initial step size is 20 and the gradient is clipped with a threshold of 0.25. Momentum is set at 0.9 with an initial step size of 1 and clipping threshold of 0.25. Adam and sparse Adam have initial step sizes of 0.005 and a damping factor of 10^-12. Sparse Adam updates moments and model parameters only when stochastic gradients are non-zero. Different preconditioners were tried for matrices, with diagonal performing the worst. Preconditioned SGD has a clipping threshold of 100, initial step size of 0.1, and Q initialized to I. Fisher has \u03bb set to 0. The initial step size is 0.1, and Q is initialized to I. Results show that methods like Adam and sparse Adam perform poorly with a dropout rate of 0.35. Preconditioners preserve sparsity of gradients, saving computations by avoiding zero gradients. Both preconditioners accelerate convergence significantly despite high sparsity. Compared to SGD, Fisher type preconditioned SGD adds limited computational complexity with sparse preconditioners. Newton type preconditioned SGD requires Hessian-vector product, doubling computational complexity per iteration compared to SGD. Fisher type SGD has similar complexity to SGD. The preconditioned SGD's wall time per iteration highly depends on implementations, with the goal of achieving comparable performance to regular SGD. Experimental results comparing different preconditioners and optimization methods can be found on the GitHub page. In the ImageNet experiment, methods implemented in TensorFlow take two days and a few hours to finish 40 epochs on a GeForce GTX 1080 Ti GPU. The word level language modeling experiment in Pytorch enables second-order derivatives, showing similar performance between SGD and Fisher type preconditioned SGD in terms of wall time per iteration. The Newton type method requires 80% more wall time per iteration than SGD on the same GPU. Two types of preconditioners, Newton and Fisher, are studied for optimization. The Fisher type has lower computational complexity but may need more tuning for step size and damping. Both preconditioners can be learned efficiently using natural or relative gradient descent on matrix Lie groups. The Newton type preconditioned SGD has higher computational complexity per iteration but is more user-friendly with normalized step size and built-in gradient noise damping. Both preconditioners accelerate convergence on large-scale problems even with sparse representations."
}