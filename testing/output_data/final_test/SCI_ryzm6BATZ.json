{
    "title": "ryzm6BATZ",
    "content": "We propose a new energy function for GANs based on image quality assessment methods, addressing shortcomings of the original BEGAN model. The new function incorporates l1 score, Gradient Magnitude Similarity score, and chrominance score. Systematic experiments explore hyper-parameters, showing that each component represents different features. Models using the new energy function produce better image representations than BEGAN. The BEGAN model has been improved through modifications to the error signal and loss function, resulting in better image representations. A new loss function, the scaled BEGAN GMSM, further enhances the model's ability to handle a broader range of image features. Generative Adversarial Networks (GANs) use a discriminator to distinguish between real and generated data. Training involves a competition between the discriminator and generator to improve their respective abilities. GAN convergence occurs at a Nash equilibrium. The task is to learn the generator's distribution over data by defining a generator function that produces images from noise vectors. The discriminator in Generative Adversarial Networks (GANs) is a function D trained to distinguish between real and generated data, while the generator function G produces images from noise vectors. BID13 proposed using deep convolutional neural networks for G and D, with D constructed as an autoencoder to compare its output with G's using standard distance functions. BID13 proposed a new interpretation of the GAN architecture as an energy-based model, where the discriminator acts as an energy function assigning low values to high data density regions. The generator, on the other hand, produces samples in low energy regions identified by the discriminator. The discriminator in BID13's energy-based GAN model assigns low energy to high data density regions. The generator produces samples in these low energy regions. The loss functions for the discriminator and generator are defined using margin loss. The system aims to reach a Nash equilibrium where generated samples are indistinguishable from real data, but visual inspection can easily differentiate them. The BID13 model proposes two direct approaches to address the problem, focusing on changing Equations 1 and 2. The BEGAN model adopts the first approach, while the second approach involves modifying the original GAN to use an autoencoder's reconstruction error. The main contributions include an energy-based formulation of BEGAN's solution, addressing Equation 1 issues, exploring hyper-parameters of the new energy function, evaluating learned representations, and demonstrating improved image generation using BEGAN+GMSM on CelebA dataset. The BEGAN model introduces changes to Equation 2, replacing the hinge loss with the Wasserstein distance between autoencoder reconstruction loss distributions of G and D. New hyper-parameters k t , \u03bb k , and \u03b3 are added. Equation 4 emphasizes the importance of E(G(z)) during training, with k t \u2208 [0, 1], \u03bb k as the learning rate for k, and \u03b3 \u2208 [0, 1]. Both equations describe the discriminator's performance. Equation 4 in the BEGAN model emphasizes the importance of the generated output energy during training, with new hyper-parameters added. It aims to keep the energy of the generated output approaching the limit of the energy of the real images. The model adjusts how the discriminator achieves its goal by changing the boundaries and conditions in the equations. The boundary dynamically establishes an equilibrium between the energy state of real and generated input, showing improvements in the loss function. The correct selection of parameters is crucial, with a dynamically updated parameter being the best instantiation. The energy function was modified to improve the proposed loss function in the context of GANs. The energy function in GANs was initially defined using mean square error (MSE) for reconstruction loss. A more generalized equation was proposed, suggesting various distance functions could be used for reconstruction error. The field of image quality assessment (IQA) focuses on evaluating digital image quality, with full-reference IQA functions like \u03b4 being important. The full-reference IQA functions like \u03b4 are used to evaluate image quality, as MSE is not a reliable indicator. Different functions can better indicate image quality by comparing reference and distorted images through a distortion vector. FR-IQA evaluates distortion vectors to measure image quality, with MSE quantifying the visibility of errors. However, MSE only defines the length, not the type, of a distortion vector. Different image vectors can have the same MSE from a reference image, leading to various types of distortion. The MSE is limited for measuring image quality as different techniques have been developed to improve upon it, such as the Structural Similarity Index (SSIM) which measures similarity in luminance, contrast, and structure between reference and distorted images. Visual inspection may not always reveal the correspondence between real and generated images. The original proof by BID13 showed that the generator should produce samples indistinguishable from the dataset, but did not account for Equation 1. This means that the generated output may include a residual distortion vector described by \u03b4, which can lead to different internal representations of learned images even if systems reach a Nash equilibrium with the same energy distribution. The energy function is crucial for defining the data distribution, not just the loss function like MSE. The energy function, along with the loss function, is essential for defining the data distribution. A multi-component approach is used to define \u03b4, including the l1 norm, gradient magnitude similarity score (GMS), and chrominance similarity score (Chrom). GMS and chrom scores are derived from the color Quality Score (cQS) model, which evaluates local gradients and chrominance information in the YIQ color space model. The energy function, loss function, and multi-component approach define the data distribution. The GMS and Chrom scores are derived from the color Quality Score (cQS) model, evaluating local gradients and chrominance information in the YIQ color space model. The similarity between gradient magnitudes and color dimensions of reference and distorted images is computed using Equations 7. The GMSM and Chrom scores are then calculated to evaluate image representations. The energy function components were experimentally evaluated on the CelebA dataset of face images using 12 different models based on the BEGAN model architecture. Models were trained using Adam with specific parameters and decay rates. The study evaluated 12 models based on the BEGAN architecture using specific parameters and decay rates. Experiments were conducted on 64 \u00d7 64 and 128 \u00d7 128 pixel images with varying numbers of convolution layers. Increasing input sizes to 128 and 256 resolved modal collapse issues. Models were trained for different numbers of epochs, with some experiencing collapse at various points. The study evaluated 12 models based on the BEGAN architecture using specific parameters and decay rates. Experiments were conducted on 64 \u00d7 64 and 128 \u00d7 128 pixel images with varying numbers of convolution layers. Models collapsed around epoch 65,000. Evaluations compared error scores of distance functions. Images were upscaled for qualitative evaluations. The study evaluated 12 models based on the BEGAN architecture using specific parameters and decay rates. Experiments were conducted on 64 \u00d7 64 and 128 \u00d7 128 pixel images with varying numbers of convolution layers. Models collapsed around epoch 65,000. Evaluations compared error scores of distance functions. Images were upscaled for qualitative evaluations using cubic image sampling. Results suggest that model training should be customized to emphasize relevant components in GAN applications. The study evaluated 12 models based on the BEGAN architecture using specific parameters and decay rates. Experiments were conducted on 64 \u00d7 64 and 128 \u00d7 128 pixel images with varying numbers of convolution layers. Models collapsed around epoch 65,000. Evaluations compared error scores of distance functions. Results suggest that model training should be customized to emphasize relevant components in GAN applications. The results of the evaluations showed that different distance functions captured various features of the image representations, with models 1-3 using one function each and models 4-6 exploring different combinations. Model 5 had the lowest chrominance error score, and Model 6 had the lowest scores for l1 and GMSM using a \u03b3 of 0.5. Models 7-9 with \u03b3 set at 0.7 showed similar results to the previous scores. Model 8 (BEGAN+GMSM) had the lowest GMSM score overall, while Model 9 (BEGAN+GMSM+Chrom) scored the lowest chrominance score among models without modal collapse. Model 12 (scaled BE-GAN+GMSM) had the lowest error scores for l1 and GMSM, with Model 11 (BEGAN) scoring the lowest for chrominance. Visual comparison showed Model 12 had higher similarity between original and autoencoded images compared to Model 11. The generator output for models 11G and 12G shows greater similarity between original and autoencoded images compared to real images. Visual comparison of chrominance also weakly supported the hypotheses. Model 9 (BEGAN+GMSM+Chrom) accurately captures relevant information in the YIQ color space. Model 12 (scaled BEGAN+GMSM) has higher similarity between original and autoencoded images than Model 11. Model 9 (BEGAN+GMSM+Chrom) accurately captures relevant information in the Q dimension (green-purple) of the color space. Comparison with models 11 and 12 shows differences in chrominance similarity scores. Model 12Q exhibits increased gradient 'speckling', indicating an inverse relationship between GMSM and chrominance distance functions. The BEGAN model introduced energy function problems from BID13. A new multi-component energy function was proposed based on Image Quality Assessment research. The scaled BEGAN+GMSM model outperforms competitors in subjective evaluations. Future research will expand to other datasets and FR-IQA energy functions. Models 11 and 12 show smooth interpolations in gender, rotation, facial expression, hairstyle, and face angle. Convergence measure scores were compared across all 300,000 epochs. Model 11 demonstrated better convergence than Model 12 over 300,000 epochs, with lower convergence measure scores indicating improved image quality. The results were smoothed using Gaussian with \u03c3 = 0.9, and images were displayed in 40,000 epoch increments starting from epoch 20,000. The convergence measure correlated with better images as the models converged. The model shows improved image quality over 300,000 epochs, with finer details learned as training progresses. Earlier epochs display more youthful output, while later epochs show increased age."
}