{
    "title": "Hkl5aoR5tm",
    "content": "Training Generative Adversarial Networks (GANs) is challenging, but a new architectural modification called self-modulation improves GAN performance without the need for labeled data. In a large-scale study, a relative decrease of 5%-35% in FID was observed, and adding this modification led to improved performance in 86% of settings. Self-modulation is a simple change that can be readily applied to any GAN. Generative Adversarial Networks (GANs) are powerful models used for tasks like image generation, compression, super-resolution, inpainting, and domain transfer. Training GANs is challenging due to the high-dimensional parameter space and non-convex game nature. Stabilizing GAN training is a key research challenge, with various approaches proposed including divergence variations, regularization, optimization schedules, and specific neural architectures. Conditional GANs benefit from self-modulation layers in the generator, which condition on the generator's own input instead of external information like labels or embeddings. This approach is motivated by Feature-wise Linear Modulation in supervised learning. The self-modulation technique conditions the generator on its own input, improving sample quality by 5% to 35% across various settings. It outperforms the baseline in 86% of cases, even with label information available. The method is discussed in relation to other diagnostic tools for generative models. Conditioning generative models on side information has been shown to improve model performance. Two main approaches have emerged: concatenating side information with the noise vector or conditioning hidden layers directly on the side information. However, concerns arise regarding the availability of side information and the challenge of unsupervised learning. Self-modulating layers are proposed as a solution to these concerns. Self-modulating layers for the generator network modulate hidden activations based on latent vector z, re-weighting feature maps in a feature-wise fashion. Batch normalization, widely used in deep neural nets, provides a convenient entry point for self-modulation. The method is presented in the context of its application via batch normalization. In batch normalization, activations of a layer are transformed using estimated mean and variances of features, along with learnable scale and shift parameters. The proposed method replaces non-adaptive parameters with input-dependent ones, parametrized by a neural network applied to the generator's input. This approach allows for self-modulation in both unconditional and conditional generation scenarios. The use of side information in conditional generation has been shown to be beneficial. By incorporating side information (such as class labels) into the generator, performance can be improved. This is achieved by composing the information with the input through a learnable function. Despite its simplicity, this method outperforms standard conditional models. Recent techniques for generator conditioning are summarized in TAB0. The use of more complex modulation architectures does not significantly improve performance. Self-modulation has been shown to yield robust improvements in various settings. Recent techniques for generator conditioning are summarized in TAB0. The study explores various loss functions, architectures, discriminator regularization/normalization strategies, and hyperparameter settings from recent studies. Both unconditional and conditional generation are considered, with an analysis of the generator's Jacobian condition number and precision/recall metrics. Two loss functions are examined, including non-saturating loss and hinge loss. The importance of controlling the discriminator's Lipschitz constant is highlighted. In the GAN literature, the importance of normalization and regularization techniques is emphasized for model performance. Two state-of-the-art techniques, gradient penalty and spectral normalization, are considered. Different network architectures and optimization hyper-parameters are explored, including training with the Adam optimizer for 100k generator steps. Multiple discriminator steps per generator step are found to be beneficial in previous studies. In previous studies, multiple discriminator steps per generator step have been found to help training. Three sets of hyperparameters are considered: (0, 0.9, 1), (0, 0.9, 2), (0.5, 0.999, 1). Models are trained with a batch size of 64 on a single nVidia P100 GPU. Datasets include CIFAR10, CELEBA-HQ, LSUN-BEDROOM, and IMAGENET. LSUN-BEDROOM dataset contains around 3M images, partitioned into test and train sets. CELEBA-HQ contains 30k images. The dataset used includes 30k images from CIFAR10, 3000 examples for testing, and the rest for training. IMAGENET has 1.3M training images and 50K test images resized to 128 \u00d7 128 \u00d7 3. Evaluation metrics include Inception Score and Frechet Inception Distance for generative models. The Inception Score (IS) and Frechet Inception Distance (FID) are evaluation metrics for generative models. IS measures the entropy of conditional and marginal label distributions, while FID compares real and generated samples in a feature space using a multivariate Gaussian. FID is robust and sensitive to mode dropping. The proposed approach outperforms the baseline in 30 out of 32 settings, with a relative improvement detailed in TAB3. Robustness testing involved a Cartesian product of parameters resulting in 36 settings for each dataset. The study involved testing 3 hyperparameter settings for spectral normalization and 6 for gradient penalty, resulting in 1440 trained models. The performance comparison between self-modulation and baseline models across different settings is detailed in TAB1, with relative improvements in TAB3 and FIG1. The proposed method consistently outperformed the baseline in all considered settings when using the RESNET style architecture. The proposed method consistently outperforms the baseline in all settings, with a 33% reduction in FID and improved inception scores. Self-modulation is effective in most GANs without additional hyperparameter tuning, outperforming the baseline in 124 out of 144 settings. Self-modulation is effective for label-conditional generation, with optimal performance achieved by applying it to each layer. Two settings are compared: one with label-conditional Batch Norm in the generator only, and the other with projection-based conditioning in both generator and discriminator. The former setting is referred to as G-COND, while the latter is P-CGAN. Incorporating self-modulation in label-conditional generation leads to improved performance. Comparison between standard label-conditional batch normalization and self-modulation with additional labels shows significant performance enhancement. Training on IMAGENET for 500k generator steps further demonstrates the benefits of self-modulation. Incorporating self-modulation in label-conditional generation leads to improved performance, with significant enhancements shown in comparison to standard label-conditional batch normalization. Training for 500k steps on IMAGENET demonstrates the benefits of self-modulation, with the best results achieved by applying it to every layer. Self-modulation is a simple yet effective addition to label-conditional generation, showing significant improvements when applied to each layer. Conditional modulation, using side information to modulate neural network computation flow, has been applied in various contexts. Existing conditional modulations are usually instantiated via Batch Normalization, including multiplicative and additive modulation, which are closely related to Gating techniques used in neural networks. The proposed method of self-modulation improves GAN performance by adopting both additive and multiplicative modulation. This technique can be applied to popular GANs, enhancing their results. The impact of self-modulation on model improvement is not clearly explained, but diagnostic statistics are computed to analyze its effects. The method discussed involves computing the condition number of the generators Jacobian and estimating precision and recall for generative models. Different metrics are used to evaluate the performance of the generators, with a focus on regularization and penalization based on the generated points' probability under the true data distribution. The F1/8 score penalizes models for low recall in generating high probability points under the true data distribution. The correlation between FID and condition number is positive for poor models but inverse for the best models with self-modulation. Self-modulation provides training stability and yields models with a small range of generator condition numbers. Self-modulation stabilizes the generator towards favorable conditioning values, improving mode coverage and stability. Further analysis tools and theoretical study are needed to understand the effects of self-modulation and other techniques. The model structures used in experiments include SNDCGAN and ResNet architectures with self-modulated BN layers. Different resolutions of images in datasets require variations in spatial dimensions for both architectures. P-cGAN is adopted for conditional settings with label information available. The Projection Based Conditional GAN (P-cGAN) is utilized with conditioning in both generators and discriminators. Conditional batch norm is applied for the generator by conditioning on label information. For discriminator label conditioning, the dot product between final layer feature and label embedding is added back to the discriminator output logits. These conditioning strategies do not depend on specific architectures and can be applied universally. The conditioning strategies discussed are not architecture-dependent and can be applied universally with slight modifications. The same architectures and hyper-parameter settings as in Miyato & Koyama (2018) are used, specifically the ResNet architecture. Two settings are compared: one with only generator label conditioning and no projection-based conditioning in the discriminator, and the other with both generator and discriminator conditioning, known as the standard full P-cGAN."
}