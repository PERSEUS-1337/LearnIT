{
    "title": "ryeo27n9Ur",
    "content": "In this work, a memory-efficient learning procedure is proposed for large-scale computational imaging systems. The procedure exploits the reversibility of network layers to enable data-driven design. The practicality of the method is demonstrated on super-resolution optical microscopy and multi-channel magnetic resonance imaging systems. Imaging systems, such as tomographic systems and magnetic resonance imaging, use software and hardware to retrieve non-traditional information. The decoding process is iterative, alternating between data consistency and image prior knowledge. Recent work has optimized these systems by creating Physics-based Networks (PbNs) through iterative decoding and training on datasets to learn design parameters. PbNs use reconstruction operations like proximal gradient descent algorithms. Physics-based Networks (PbNs) are efficiently parameterized with a few learnable variables, enabling robustness in inverse problems. Training PbNs relies on gradient-based updates computed using backpropagation, which can be memory-intensive for modern imaging systems processing large amounts of data. Methods to save memory during backpropagation trade off spatial and temporal complexity. Temporal complexity in training Physics-based Networks (PbNs) can be optimized by various methods such as forward recalculation, forward checkpointing, and reverse recalculation. These techniques aim to balance temporal and spatial complexities to improve memory efficiency during backpropagation. In this work, a memory-efficient learning procedure for backpropagation in specialized network architectures is proposed. The method enables learning for large-scale computational imaging systems by utilizing invertibility and reverse recalculation concepts. Practical restrictions on layers are addressed, and a hybrid scheme combining reverse recalculation with checkpointing is introduced to reduce numerical error accumulation. The effectiveness of the method is demonstrated in designing superresolution optical microscopy and multi-channel magnetic resonance imaging systems. Imaging systems encode and decode information from measurements using a forward model and random noise. Decoding involves an inverse problem formulation with data and prior penalties. Proximal gradient descent is efficient for minimizing the objective in this process. The PbN structure involves alternating steps with gradient descent and proximal functions to enforce priors. It unrolls N iterations of the optimizer into N layers of a network. The learnable parameters are optimized using gradient-based methods with auto-differentiation functionalities from common machine learning toolboxes. The main contribution is improving the spatial complexity. Our main contribution is to improve the spatial complexity of backpropagation for PbNs by treating the larger single graph as a series of smaller graphs. When performing reverse-mode differentiation, our method treats a PbN of N layers as N separate smaller graphs, saving memory. The procedure involves recalculating the current layer's input and forming smaller graphs for each layer to compute gradients efficiently. The procedure involves computing the gradient of the loss for all N layers in reverse order. To efficiently perform reverse-mode differentiation, the inverse operation of each layer must be computed. The inverse of the gradient step layer can be solved iteratively using the backward Euler method. Convergence is guaranteed under certain conditions. The Lipschitz constant of an argument can be computed using Lip(\u00b7). The fixed point algorithm (Alg. 2) ensures exponential convergence rate. The proximal update layer is like a backward Euler step and its inverse can be a forward Euler step under certain conditions. When a function is not bijective, the inversion is not straightforward. To mitigate numerical errors in reverse recalculation, storing intermediate variables as checkpoints during forward calculation is recommended. While most PbNs cannot store all variables for reverse-mode differentiation, storing a few can help ensure accuracy. Standard bright-field microscopy is versatile for imaging biological samples. Fourier Ptychographic Microscopy (FPM) is a super resolution method that creates gigapixel-scale images by acquiring multiple measurements under various illumination settings and combining them via phase retrieval optimization. The system's dependence on many measurements limits its ability to image fast-moving biology, but reducing the number of measurements using linear multiplexing shows promise. However, current limitations in scale are due to GPU memory constraints. Our proposed memory-efficient learning framework reduces the required memory for Fourier Ptychographic Microscopy to only a few gigabytes, enabling the use of consumer-grade GPU hardware. Results show a reduction in the number of measurements by a factor of 10 using significantly less memory and a slight increase in processing time. Testing loss and reconstructions using our method are comparable to standard learning, demonstrating the effectiveness of our approach. MRI is a Fourier-based medical imaging modality that captures biophysical information without radiation. Multi-channel MRI with compressed sensing reduces scan times by using image prior knowledge. Memory-efficient learning reduces memory footprint during training for larger MRI problems, enabling state-of-the-art performance. Comparison with full backpropagation shows significant memory and time savings. Memory-efficient learning reduces memory footprint during training for larger MRI problems, enabling state-of-the-art performance. The training loss is consistent across epochs, and inference results show a normalized root mean-squared error of 0.03 between conventional and memory-efficient learning. This approach allows for applications previously limited by GPU memory constraints without a significant increase in training time. Specialized for PGD networks, similar methods can be applied to other PbNs with complex subroutines, but invertibility conditions must be met. Gradient descent blocks with evolving step sizes may not satisfy invertibility conditions, leading to numerical errors during reverse recalculation of unstored variables. In this communication, a memory-efficient learning method is presented for large-scale computational imaging problems, utilizing reversibility for reverse-mode differentiation. The method is demonstrated on SR optical microscopy and multi-channel MRI applications, with the expectation that other computational imaging systems can be integrated into the framework."
}