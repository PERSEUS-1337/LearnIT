{
    "title": "H1g-gk5EuQ",
    "content": "Neural language models (NLMs) are generative models trained on a large corpus to improve modeling accuracy. They have been applied to tasks like automatic speech recognition (ASR) by re-scoring the n-best list to reduce error rates. However, the generative nature of NLMs can lead to suboptimal performance. This work proposes adapting NLMs to a discriminative approach by enlarging the margin between \"good\" and \"bad\" sentences. The method is trained end-to-end and shows significant improvements in tasks involving re-scoring decoded text. The curr_chunk discusses the application of language models in ASR and SMT tasks, focusing on predicting the next symbol based on hidden states and joint probability. It highlights the need to consider the discrimination ability of language models in supervised learning tasks. The curr_chunk discusses improving the discrimination ability of language models in supervised learning tasks like ASR and SMT. Existing works aim to enhance ASR accuracy by discriminating between \"good\" and \"bad\" sentences using hand-crafted features. However, these methods may not utilize large unsupervised corpora effectively. The proposed method in this work focuses on enlarging the difference between the log-likelihoods of \"good\" and \"bad\" sentences. The proposed method in the curr_chunk does not rely on hand-crafted features like existing works BID23, BID10, and BID21. It is trained in an end-to-end manner and utilizes large external text corpora. The large margin language model improves ASR and SMT tasks by reducing word error rate (WER) and increasing bilingual evaluation understudy (BLEU) scores significantly. This method shows a notable advantage over alternative methods that rely on ad hoc choice of features. The proposed method in the current chunk utilizes a large margin language model to improve ASR and SMT tasks by reducing word error rate and increasing BLEU scores. It does not rely on hand-crafted features like existing works and is trained in an end-to-end manner. The method is based on comparisons between pairs of sentences and resembles a siamese network architecture. The current chunk discusses the application of siamese network architecture in learning similarities on sequences and draws inspiration from information retrieval methods. It also introduces the concept of LM score and highlights the use of negative examples in training NLM. The current chunk discusses the importance of utilizing negative examples in training language models to enhance discrimination in applications like automatic speech recognition and machine translation. Negative samples, such as noninformative replies or suboptimal beam candidates, help distinguish between positive and negative candidates. The current chunk discusses the use of negative examples in training language models for applications like automatic speech recognition. It presents an example of a CTC-based ASR model trained on the WSJ dataset and evaluates beam candidates against ground-truth text. The importance of utilizing negative samples to distinguish between positive and negative candidates is highlighted. The current chunk discusses optimizing language models by maximizing likelihood on positive samples and minimizing it on negative samples using stochastic gradient descent. The potential issue is that negative samples may dominate the optimization process. The training focuses on negative samples but fails to improve discrimination. An experiment using an ASR system extracts 256 beam candidates per training sample in the WSJ dataset. A conventional NLM is used as a baseline for beam rescoring, with training initiated from this model using SGD. The training loss is unstable, and the goal is for ground-truth sentences to score higher than beam candidates. The margin between ground-truth and candidate scores is analyzed, showing a symmetric distribution around zero. The Large Margin Language Model (LMLM) proposes a new objective function to improve discrimination ability in training. By setting a margin between ground-truth and negative candidates, the training loss becomes more stable and steadily decreases towards zero. This approach contrasts with the conventional NLM training, which showed poor discrimination ability. The Large Margin Language Model (LMLM) introduces a stable training method with improved discrimination ability compared to the baseline NLM. It ranks candidate sentences based on edit distance for ASR and BLEU score for SMT, enforcing a margin between better and worse quality sentences. The rank-LMLM formulation introduces more comparisons among candidates, leading to higher computational costs during training. Extensive experiments on ASR and SMT show that both LMLM and rank-LMLM outperform baseline language models and other domain adapted models. The baseline-LM architecture includes a 2048 dimensional embedding layer, two LSTM layers with 2048 nodes each, and a softmax layer with a 400K dimensional output. The large vocabulary size incurs high computational costs, addressed using the sampled softmax technique. The baseline-LM, trained on the common-crawl corpus with a vocabulary size of 400K, achieves a perplexity of 110 on a dev set with 400K sentences, outperforming a 5-gram model. The experimental setup involves training an ASR/SMT model, extracting B beam candidates for each training sample, and using them for training LMLM and rank-LMLM. The study involves training LMLM and rank-LMLM models using ASR/SMT and language model scores, optimizing weights for WER/BLEU on dev set, and reporting results on the test set. Two other approaches include refining the baseline NLM with task-specific text (refine-LM) and training a smaller NLM from scratch on task-specific data (interp-LM). LMLM and rank-LMLM experiments set \u03c4 = 1 and sample 20% of pairs for rank-LMLM due to the large number of pairs. These models can leverage unsupervised data by warm-starting with a conventional language model. Without warm-starting, LMLM training gets stuck in \"bad\" local minimal and struggles to generalize well. Previous studies have also recommended using unsupervised models to warm-start supervised training. An experiment using an ASR system extracted 64 beam candidates for each training utterance in the WSJ dataset. Training LMLM with warm-starting from the baseline-LM showed improved learning curves compared to training without warm-starting. The text discusses the benefits of warm-starting in training LMLM models, showing improved learning curves and stronger generalization ability. It also highlights the positive impact on discrimination and effective identification of erroneous sentences. The text discusses the benefits of warm-starting in training LMLM models, showing improved learning curves and stronger generalization ability. Additionally, it highlights the positive impact on discrimination and effective identification of erroneous sentences. The study compares LMLM and rank-LMLM, noting that rank-LMLM shows more selectivity in assigning scores to different beams. The scores from both models are smaller than those from baseline-LM, as they are not guided by the max-likelihood objective. Rank-LMLM scores are even smaller due to more pairwise constraints in training. The max-likelihood training is argued to not align well with beam re-scoring purpose, as shown in reported WERs and CERs on test sets for ASR tasks using the proposed methods on datasets like WSJ and Fisher. The ASR models for WSJ and Fisher have different architectures, with WSJ having one convolutional layer and 5 RNN layers, while Fisher has two convolutional layers and 6 GRU layers. The models are trained using in-domain data and benefit from using a language model during decoding. A beam search decoder with a beam width of 2000 is used, providing strong baseline WERs for tasks like WSJ. The top-1 beam candidates are re-scored using different language models like baseline-LM, refine-LM, interp-LM, LMLM, and rank-LMLM. Results on WSJ and Fisher tasks are compared in TAB3, with rank-LMLM achieving the best WER and CER. Adapted language models like LMLM and rank-LMLM outperform generative language models for re-scoring in supervised tasks. In a proof-of-concept example on an SMT task using the IWSLT15 Vietnamese-to-English dataset, LMLM and rank-LMLM significantly outperform other methods, as shown in TAB5 with BLEU scores on the test set. The dataset was simplified by decapitalizing words and removing punctuations, resulting in a vocabulary size of about 44K. The SMT model trained with an attention mechanism in BID17 had two layers of LSTMs for both encoder and decoder with 128 activations each. The generative language models in the study do not improve upon the SMT model without re-scoring. The decoder in a seq-to-seq model functions as a language model, resulting in grammatical beam candidates with varying semantic qualities. Generative language models act like grammar checkers and lack discrimination in the semantic field. The proposed approach aims to enhance the discrimination ability of language models by maximizing the margin between \"good\" and \"bad\" examples. The method discussed aims to improve language models by maximizing the margin between \"good\" and \"bad\" sentences, showing gains in tasks like ASR and SMT. Future applications include conversation generation to distinguish between boring and informative replies, as well as applying the method to lattices during decoding."
}