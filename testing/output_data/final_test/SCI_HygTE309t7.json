{
    "title": "HygTE309t7",
    "content": "Modern applications generate massive amounts of image data. A novel image outlier detection approach (IOD) leverages an image classifier to discover outliers without using labeled data. Confidence from a convolutional neural network (CNN) does not effectively detect outliers, so a Deep Neural Forest-based approach is proposed to accurately classify images and detect outliers. Our IOD approach effectively detects outlier images in benchmark datasets like MNIST, CIFAR-10, CIFAR-100, and SVHN, capturing over 90% of outliers while maintaining classification accuracy. The importance of outlier detection in image data for applications like autonomous vehicles and video surveillance is highlighted, with examples of potential risks. The success of deep learning in computer vision has led to the development of various outlier detection methods. In recent years, works have leveraged deep autoencoders or GANs for outlier detection in computer vision. These approaches use learned representations for outlier detection, but may not effectively distinguish outliers from inliers. Some recent works have incorporated outlier detection objectives actively into the learning process to address this issue. The proposed image outlier detection (IOD) strategy combines deep learning image classifiers with classical outlier detection techniques to effectively detect outliers in complex real datasets with multiple normal classes. This approach addresses the limitations of one-class techniques in handling datasets like CIFAR-10 and unifies the core principles of outlier detection within a single framework. The proposed image outlier detection (IOD) strategy combines deep learning image classifiers with classical outlier detection techniques to effectively detect outliers in complex real datasets with multiple normal classes. However, directly using the confidence produced by CNN to identify outliers is not particularly effective due to conflicting requirements of accurately classifying images and detecting outlier images. The proposed deep neural decision forest-based approach addresses the challenge of detecting outliers in image classification by leveraging information theory-based regularization and ensuring independence among decision trees to improve classification accuracy. Joint optimization of split and leaf nodes speeds up convergence. The deep neural forest-based approach improves outlier detection in image classification by optimizing split and leaf nodes for faster convergence. It demonstrates effectiveness with benchmark datasets like MNIST, CIFAR-10, CIFAR-100, and SVHN, achieving high accuracy in outlier detection while maintaining multi-class classification accuracy. In CNNs, the final FC layer computes a weighted sum score for each target class, with the maximum weighted sum being the largest score among all classes. The maximum weighted sum score is determined by the largest score among all classes in image classification. A softmax activation function generates class probabilities for each node, indicating the likelihood of an image belonging to a specific class. The maximum probability or weighted sum score can serve as a confidence measure for images, with outliers typically having lower probabilities compared to inliers. Our method uses the maximum weighted sum score as a confidence measure for images and establishes an outlierness threshold in the training process. Unlike previous approaches, we do not need to set a specific cutoff threshold, but instead use a hyperparameter k to determine outliers. Our method utilizes the maximum weighted sum score as a confidence measure for images and sets an outlierness threshold during training. Unlike previous methods, we use a hyperparameter k to determine outliers, which is more intuitive than setting a specific cutoff threshold. However, simply assuming a percentage of outliers in the training data does not work well, as outlier images can have a large maximum probability. This is illustrated through experiments on a CIFAR-10 model classifying CIFAR-10 and CIFAR-100 testing data, where ideally all CIFAR-100 images should be detected as outliers. The experimental evaluation shows that using maximum weighted sum performs better than maximum probability in detecting outliers. With a cutoff threshold set at k=5000, more than 30% of CIFAR-100 images are not correctly classified as outliers, resulting in an accuracy of less than 0.7. The low accuracy is attributed to the conflicting requirements of accurately classifying images and effectively detecting outliers. Regularization methods like data augmentation, random dropout, and weight decay are commonly used to improve the generalization capability of Convolutional Neural Networks (CNNs) by avoiding overfitting. However, these methods can compromise the model's ability to detect outliers. To address this issue, a deep neural decision forest-based approach is proposed to harmonize the conflicting requirements of accurately classifying images and effectively detecting outliers. The deep neural decision forest-based approach combines deep convolutional networks with decision trees to address the issue of accurately classifying images while effectively detecting outliers. The model, BID10, utilizes the maximum route of each tree to distinguish outliers from inliers, incorporating deep CNN, decision nodes, and prediction nodes in its structure. The deep neural decision forest model, BID10, integrates a deep CNN with decision nodes and prediction nodes. Decision nodes are internal nodes connected to the final FC layer, while prediction nodes are leaf nodes. Each decision node is assigned a probabilistic decision function to classify samples and route them through the tree. The connection between the neural network and the decision tree is facilitated by a linear output unit provided by the FC layer. The function connects the neural network and decision tree, turning it into a probabilistic routing decision. Each prediction node holds a probability distribution over the output space. Stochastic routing averages leaf predictions based on the probability of reaching the leaf. The final prediction for a sample is determined by the routing function and leaf probabilities. The function connects the neural network and decision tree, turning it into a probabilistic routing decision. Each decision tree in the forest produces a probability distribution over each route, where the probability of a route is computed as the product of the probabilities w.r.t. all split nodes on that route. The probability distribution of the routes is expected to be very skewed and biased to one particular route, with an extremely small probability for routes that do not fit the features represented by the split nodes. The deep CNN learns features through split nodes on routes, with the max route determining the class of the image x. The max route probability measures the classifier's confidence in its classification decision. For a decision tree T, confidence is measured as the max route probability. In a forest of decision trees, the final confidence of image x is measured as the max route probability. The max route probability in outlier detection is more effective than the maximum weighted sum of classes in CNN. The max route probability is limited by the product operation, while the maximum weighted sum falls off slowly due to linear combination of features. Regularization is introduced to prevent uniform probability distribution in routing decisions. In outlier detection, a penalty is applied to routing decisions with high entropy to ensure the max route probability stands out. A softmax function is used for normalization to address convergence issues during runtime. The revised entropy of the routing probability distribution is penalized by adding entropy w.r.t. each training sample to the log-loss term. The total log-loss for a random forest of decision trees is defined, with a max route probability cutoff threshold determined for outlier detection accuracy. Our deep neural decision forest-based approach overcomes skewness in the classification probability distribution, improving generalization performance by using ensemble nature of decision forest to prevent overfitting in individual trees. The deep neural decision forest architecture is enhanced to improve generalization capability by ensuring independence among trees in the forest, as opposed to sharing layers in a traditional CNN. This new architecture divides FC layers into independent components to achieve better generalization. The deep neural decision forest architecture divides FC layers into independent components to ensure independence among trees, improving classification accuracy and outlier detection. Training involves estimating decision node parametrizations and leaf predictions using a two-step optimization strategy. The new form of prediction nodes makes the deep neural forest fully differentiable. The deep neural decision forest architecture ensures independence among trees for improved classification accuracy and outlier detection. The new prediction nodes make the deep neural forest fully differentiable, allowing for joint optimization of parameters through back-propagation. The proposed image outlier detection strategy is empirically demonstrated on CIFAR-10 and MNIST datasets, considering examples from other datasets as outliers during testing. The text describes different methods for outlier detection, including IOD-IOD-Weighted-Sum, IOD-IOD-Max-Route-Shared-FC, IODMax-Route-Different-FC, IOD-Max-Route-Penalty, Deep SVDD, and AnoGAN. These methods are compared based on their confidence measures and architectures. Experimental Setup: Experiments were conducted on a GPU using Pytorch BID22 for IOD models. Deep SVDD and AnoGAN codes were reused from their authors. Outlier detection and classification accuracy were measured. Parameter settings were manually adjusted with specific details provided. Networks were trained with mini-batches of size 128, momentum set to 0.9, and weighted decays at 0.0001. No data augmentation was used, and images were pre-processed with global contrast normalization. When testing on different datasets, images were adjusted accordingly. The CIFAR-10 dataset consists of 10 classes of natural images with 50,000 training images and 10,000 testing images. A deep neural network with 10 convolutional layers and 2 fully connected layers is used for testing MNIST on models trained for other datasets. Different max route-based approaches are employed for final prediction. The IOD approach uses 20 depth-3 trees with FC layers, each set containing 2 FC layers with 1024 and 384 hidden units. The final FC layer connects to a tree, with a learning rate initialized at 0.1 and decaying to 0.001. For outlier detection, a testing image is considered an outlier if its maximum weighted sum score or max route probability is smaller than a threshold. In comparison to TAB0, the IOD-Max-Route-Penalty method shows significant improvement. Our IOD-Max-Route-Penalty method outperforms Deep SVDD and AnoGAN in outlier detection. Deep SVDD struggles with separating outliers from inliers in multiple classes, while AnoGAN only works well with MNIST images. Our max route-based method surpasses IOD-Weighted-Sum by using deep neural forest architecture for confidence computation. The IOD-Max-Route-Penalty method outperforms other methods in outlier detection by introducing regularization to penalize routing decisions with large entropy, leading to better detection of MNIST outliers. This method achieves higher classification accuracy but may introduce overfitting. The regularization in the new deep neural forest architecture may slightly decrease accuracy, but it effectively alleviates overfitting. The IOD-Max-Route-Different-FC consistently outperforms IOD-Max-Route-Shared-FC in outlier detection and classification, showcasing the effectiveness of the new architecture. The fully isolated decision trees in the forest enhance outlier detection and image classification capabilities. Refer to Appendix C for MNIST results. Classical outlier detection techniques share the principle that an object is an outlier if its outlierness score is high. In outlier detection, the outlierness score determines if an object is an outlier. Unlike previous works, the score is not measured in the original feature space to avoid high-dimensional data issues. One-Class Classification methods like OC-SVM and SVDD use normal training data to separate outliers based on kernel-based transformations. OC-SVM uses a hyperplane to separate target objects, while SVDD encloses data in a hypersphere in the transformed feature space. In outlier detection, methods like OC-SVM and SVDD enclose data in a hypersphere of radius R in the transformed feature space. However, these approaches are sensitive to specific choices of representation, kernel, and hyper-parameters, making them not robust. They assume all normal data belongs to one class, which doesn't fit scenarios with rich classes of images. An unsupervised image outlier method extracts features from raw pixels and transforms, then computes mean and variance values to identify outliers based on significant deviations. The authors proposed a probabilistic PCA model in BID8 focusing on accurate image reconstruction by extracting typical features and removing outliers. Recent research has focused on enhancing one-class classification with deep neural networks for outlier detection. Recently, \"fully deep\" outlier detection methods have been proposed that outperform mix methods by producing representations and boundaries separating outliers from inliers within one learning process. However, these methods still struggle with finding a single boundary between outliers and inliers, which is a limitation for multi-class datasets. In contrast, the IOD method significantly outperforms one-class approaches when handling complex image datasets like CIFAR-10. Deep autoencoder-based methods have also been widely used for outlier detection due to their ability to extract common factors. Deep autoencoders and GAN-based methods are used for outlier detection. Autoencoders extract common factors of variation from normal samples to detect outliers, but may not be effective in distinguishing outliers from inliers. GAN-based methods like AnoGAN train to detect outliers by producing representations and boundaries. The GAN-based method AnoGAN trains a GAN to generate samples from training data for outlier detection. Deep statistical methods like DAGMM and DSEBMs use deep structures such as autoencoders to model data distribution and detect outliers. The DSEBMs detect outliers by combining autoencoders and EBMs, while statistical methods detect outliers based on object density. The model may incorrectly assign high density to outliers if there are many nearby outliers, leading to false negatives. In contrast, the IOD framework can work with any image classifier, unlike the specific network architecture used in the Open Set Deep Network. Additionally, BID34 addresses training CNNs to be robust against noisy labels. In contrast to the DSEBMs and statistical methods for outlier detection, the proposed approach focuses on effectively detecting outliers from image data using a novel framework and outlierness measure based on deep neural decision forest. The method aims to enhance outlier detection capacity through new architecture and regularization techniques. The approach focuses on enhancing outlier detection capacity using a deep neural decision forest. It introduces a new form of prediction nodes for fully differentiable deep neural forests, enabling joint optimization of parameters through back-propagation. The prediction nodes in BID10 hold a probability distribution over Y and are optimized alternately with decision nodes. New forms of prediction nodes make the decision forest fully differentiable, allowing joint optimization with decision nodes. Each prediction node is parametrized using a k-dimensional parametric probability distribution. The softmax function converts real-valued scores to values between 0 and 1. The prediction nodes in BID10 hold a probability distribution over Y and are optimized alternately with decision nodes. New prediction nodes ensure the final loss function remains convex. Learning the random forest model involves minimizing the total log loss by finding optimal parameters \u0398 and \u03c0. By independently minimizing the penalized loss of each tree, the loss function becomes fully differentiable, allowing the use of Stochastic Gradient Descent for optimization. The gradient of the loss with respect to \u0398 can be computed using back-propagation, similar to traditional CNN modules. The curr_chunk discusses the computation of prediction nodes in a neural network model using the chain rule. It also presents experiment results on the MNIST dataset with a neural network architecture. The weighted-sum method and max route-based methods are compared for final predictions. The max route-based methods in the neural network model outperform the weighted-sum method in outlier detection and classification on the MNIST dataset. The IOD-Max-Route-Penalty approach shows the best results in outlier detection with minimal impact on classification accuracy. The new deep neural forest architecture consistently outperforms previous methods in outlier detection and classification. Deep SVDD and AnoGAN perform well on the MNIST dataset, while BID6 introduces a theoretical framework for modeling uncertainty with dropout neural networks. The uncertainty of softmax output can indicate the model's uncertainty in its prediction, with a larger intersection signifying more uncertainty. This uncertainty can be used to measure the outlierness of an image, evaluated as a baseline method on CIFAR-10 data using a specific network architecture. The method is applied on CIFAR-10 training data by forwarding each image in the model 100 times to record softmax input values. Outlierness is determined by the intersection between predict class and all other classes, using a cutoff threshold. The outlier detection scheme has an accuracy of 53%, lower than the maximum weighted sum baseline at 70%. Additionally, the Dropout method is applied on MNIST training images using the suggested network architecture, with each image forwarded 100 times in the model. The authors forward MNIST images multiple times in the model to compute outlierness scores. Increasing the parameter from 200 to 5000 improves outlier detection accuracy to 48.13%, but still lower than their proposed method. The input parameter k influences outlier detection accuracy in their IOD-based method. In the IOD-based method, outlier detection is based on the maximum weighted sum or route probability being smaller than a cutoff threshold. Comparatively, Deep SVDD and AnoGAN use outlierness scores to determine outliers. The IOD-Max-RoutePenalty method outperforms SVDD and AnoGAN in outlier detection on CIFAR-10. Smaller k values result in lower accuracy for all methods due to varying cutoff thresholds. In Isolation Forest, outliers are detected using average path lengths on isolation trees. Evaluation on CIFAR-10 data involves using a CNN to extract features for outlier detection. The IOD-Max-Route-Penalty method consistently outperforms other methods in detecting outliers, except for AnoGAN in the case of CIFAR-10 outliers from the MNIST model. In Isolation Forest, outliers are detected using average path lengths on isolation trees. The number of outliers in CIFAR-10 is set as 5000, similar to the parameter k used in IOD-based methods. Despite careful tuning of parameters and dimensionality reduction techniques, the outlier detection accuracy is poor - less than 2% in all cases for CIFAR-100, MNIST, and SVHN datasets. After applying dimensionality reduction techniques, Isolation Forests were used on the lower dimension space, resulting in slightly better outlier detection rates. Directly applying Isolation Forests on raw images yielded outlier detection accuracies of 9.73% for CIFAR-100, 13.14% for SVHN, and 8.3% for MNIST."
}