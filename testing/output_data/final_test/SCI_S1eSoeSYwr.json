{
    "title": "S1eSoeSYwr",
    "content": "Deterministic neural networks are used in safety critical domains, requiring robust measures of uncertainty. A novel method is proposed to train NNs to estimate the target and its associated evidence by placing evidential priors over a Gaussian likelihood function. The model penalizes incorrect predictions of evidence alignment with the output, allowing it to estimate the probabilistic mean, variance, and underlying confidence of the target. The model estimates the probabilistic mean, variance, and uncertainty of the target, showing well-calibrated measures on various benchmarks and robustness to adversarial input. Recent advances in deep supervised learning have led to superhuman performance in various domains such as computer vision and robotics. However, these models can be easily fooled by adversarial perturbations and struggle to understand when their predictions should not be trusted. Regression-based neural networks are now being used in safety-critical applications where the ability to infer model uncertainty is crucial for widespread adoption. Precise uncertainty estimates are valuable for human interpretation of confidence and anomaly detection. Existing approaches to uncertainty estimation focus on learning aleatoric uncertainty (uncertainty in the data) and epistemic uncertainty (uncertainty in the prediction). Challenges include computational expense of sampling during inference and selecting appropriate weight priors. A new approach formulates learning as an evidence acquisition process, allowing the model to acquire evidence during training to support its predictions. The model acquires evidence during training to support its predictions by placing priors over the likelihood function, allowing for the learning of epistemic and aleatoric uncertainty representations without sampling during inference. This novel method includes a regularizer for expressing lack of evidence on out-of-distribution examples and evaluation on benchmark regression tasks. In this work, the evaluation focuses on learned epistemic uncertainty in regression tasks and comparison with other uncertainty estimation techniques for neural networks. The aim is to learn a function that minimizes the sum of squared errors, without explicitly modeling noise or uncertainty in the data. The optimization problem can also be approached from a maximum likelihood perspective. In this paper, a novel approach is presented for estimating evidence in support of network predictions by directly learning both the inferred parameters \u03b8 = (\u00b5, \u03c3 2) that maximize the likelihood of observing the targets in regression tasks. The approach aims to model the uncertainty of the data, known as aleatoric uncertainty, while acknowledging the predictive model or epistemic uncertainty. The approach presented aims to estimate evidence supporting network predictions by learning aleatoric and epistemic uncertainty. Higher-order prior distributions are placed over parameters to probabilistically estimate unknown mean and variance in a Gaussian distribution. Sampling from an evidential distribution yields lower-order likelihoods over the data. The model aims to predict the target from an input with an evidential prior for uncertainty estimation. The posterior distribution is estimated using a factorized Gaussian conjugate prior, the Normal Inverse-Gamma (N.I.G.) distribution, with parameters representing \"virtual-observations\" supporting a property. The N.I.G. distribution is used to estimate the posterior distribution with parameters representing virtual-observations supporting a property. The total evidence of the evidential distributions is defined as the sum of inferred virtual-observations counts. The N.I.G. hyperparameters determine the location and dispersion concentrations of the likelihood function, visualized in Fig. 2A with varying model parameters. In Fig. 2A, different evidential N.I.G. distributions are visualized with varying model parameters. Increasing the evidential parameters results in a tightly concentrated p.d.f. around the inferred likelihood function. Neural networks are used to infer the hyperparameters of this higher-order distribution, allowing for simultaneous learning of regression tasks and uncertainty estimation. The evidential prior, a higher-order N.I.G. distribution, enables analytical computation of the maximum likelihood Gaussian. The N.I.G. distribution allows for analytical computation of the maximum likelihood Gaussian from expected parameter values. The approach involves learning a model to output the hyperparameters of the distribution, focusing on maximizing model evidence and inflating uncertainty when predictions are incorrect. The model enforces a prior to inflate uncertainty estimates by maximizing model fit through the model evidence, computed by marginalizing over likelihood parameters. An N.I.G. evidential prior allows for an analytical solution for the model evidence, which is minimized for computational reasons. Instead of using empirical Bayes, the sum-of-squared errors can be minimized between the evidential prior and the Gaussian likelihood function. In our experiments, using L SOS i (w) resulted in greater training stability and increased performance, compared to minimizing evidence on errors. Regularizing training by applying a lack of evidence prior aims to minimize evidence (or maximize uncertainty) everywhere except where there is training data, achieved by minimizing the KL-divergence between the inferred posterior, q(\u03b8), and a prior, p(\u03b8). The KL-divergence between the inferred posterior and a prior is crucial for regularization in training, especially in the categorical setting. However, in regression, defining the KL-divergence with a N.I.G. zero evidence prior is problematic. This prior is necessary where there is no data support, but unlike classification, penalizing evidence except for the point estimate is challenging due to the infinite and unbounded space. In regression, a novel evidence regularizer, L R i, is formulated based on prediction errors. The penalty imposed on evidence is determined by the value of p in the L-p norm of x. A value of p = 1 is found to provide optimal stability during training. This regularization loss penalizes errors in predictions that scale with the total evidence of the inferred posterior. Large amounts of predicted evidence are not penalized as long as the prediction is accurate. The combined loss function used during training includes terms for maximizing model evidence and regularizing evidence. Aleatoric uncertainty represents unknowns that vary with each experiment, while epistemic uncertainty reflects uncertainty in the learned model. Performance comparisons against benchmarks are qualitatively evaluated on a one-dimensional regression dataset. The study compares different regression techniques for uncertainty estimation on a one-dimensional dataset. Evidential regression accurately estimates uncertainty, while other methods like model-ensembles and Bayes-by-Backprop underestimate it. Performance is evaluated using RMSE and NLL metrics. The study compares regression techniques for uncertainty estimation, with evidential regression outperforming other methods in RMSE and NLL on 8 out of 9 datasets. Evidential models can also estimate aleatoric uncertainty. Refer to Sec. 7.3.3 for more details and results. Evidential regression outperforms other methods in RMSE and NLL on 8 out of 9 datasets, achieving top scores within statistical significance. In this subsection, the scalability of evidential learning is demonstrated by extending to the high-dimensional task of depth estimation using a U-Net style NN trained on over 27k RGB-to-depth pairs from the NYU Depth v2 dataset. The model outputs a single activation map for deterministic regression, dropout, ensembling, and BBB. The evidential models for depth estimation demonstrate efficiency in terms of trainable parameters, inference times, and predictive accuracy compared to ensembles and BBB. The models output four activation maps and show comparable performance through RMSE and NLL metrics. The size and speed of all models are summarized in Table 2, with evidential models requiring significantly fewer parameters than ensembles. Additionally, evidential regression models do not need sampling for uncertainty estimation, leading to more efficient inference times. The depth estimation problem posed challenges for the BBB baseline, leading to comparison with spatial dropout and ensembles. Evaluation focused on accuracy and predictive uncertainty on test data. Evidential uncertainty modeling effectively captured depth errors with clear confidence predictions, outperforming dropout and ensembling in uncertainty estimation. The study evaluates uncertainty calibration in depth estimation models by fitting ROC curves to error and uncertainty estimates. The model's ability to detect likely errors using predictive uncertainty is tested, with a focus on sensitivity and specificity. The evidential model shows concentrated uncertainty estimates where errors occur, indicating effective uncertainty estimation. Aleatoric uncertainty estimates are also considered in the evaluation. The study evaluates uncertainty calibration in depth estimation models by fitting ROC curves to error and uncertainty estimates. It compares evidential aleatoric uncertainty to Gaussian likelihood optimization in high data uncertainty domains. The results show agreement in identifying sources of high uncertainty. Uncertainty estimation helps understand out-of-distribution samples and model trustworthiness. The evidential uncertainties are well calibrated with the model's errors, and performance on out-of-distribution samples is investigated. The evidential model predicts increased uncertainty on out-of-distribution samples, including adversarially perturbed inputs. Adversarial noise leads to higher absolute error across all methods, highlighting the impact of noise on predictive uncertainty. The evidential uncertainty model shows increased uncertainty with adversarial noise, maintaining strong overall uncertainty despite calibration accuracy loss. Predictive uncertainty steadily increases with noise, spatially correlating with error in the image. Our work presents a scalable representation for inferring uncertainty distribution parameters while learning regression tasks via MLE. Unlike other methods that rely on multiple samples, we only need a single forward pass to evaluate predictions and uncertainty. Our approach of uncertainty estimation in Bayesian inference focuses on modeling higher-order distribution priors over neural network predictions to interpret uncertainty in regression tasks. This methodology has shown better calibration and the ability to predict model failures, extending prior works that focused on classification tasks. In this paper, a novel method for training deterministic neural networks is developed to estimate a desired target and evaluate evidence to generate robust metrics of model uncertainty. The approach involves learning evidential distributions and penalizing prediction errors based on available evidence for stable training. The method is validated on a benchmark regression task and successfully scales to computer vision tasks like depth estimation, showing increased predictive uncertainty with out-of-distribution perturbations. This framework aims to provide precise uncertainty metrics for safe deployment of neural networks in critical domains. The text discusses the transformation of Normal Inverse-Gamma distribution to Normal Gamma distribution for neural network deployment in safety-critical domains. It also mentions the computation of KL divergence between two distributions and the definition of evidence. The training set includes examples drawn from y = sin(3x)/(3x) + N(0, 0.02) in the region -3 \u2264 x \u2264 3, with models consisting of 100 neurons and 3 hidden layers. Fig. 3 shows estimated uncertainty and predicted mean across the test set. The importance of augmenting the training objective with an evidential regularizer L R is demonstrated in the experiment, with quantitative results in Fig. 8 for the same regression problem. Fig. 8 presents quantitative results on training a regression problem with and without an evidential regularization term. This term introduces uncertainty for out-of-distribution samples, improving epistemic uncertainty reliability. The training set includes examples drawn from y = sin(3x)/(3x) + (x), and the novel LR loss helps minimize evidence on OOD data for regression prediction problems. The approach is evaluated against a method for uncertainty estimation by inferring the mean and variance of a Gaussian likelihood. The paper discusses aleatoric uncertainty estimation using a Gaussian likelihood function. Training involves minimizing negative log-likelihood, with a comparison to Gaussian NLL network performance. Evidential modeling matches Gaussian likelihood optimization. Epistemic uncertainty estimation algorithms are compared, with the evidential method outperforming others in terms of parameters and inference speed. The evidential method outperforms other algorithms in terms of space and inference speed while maintaining competitive RMSE and NLL."
}