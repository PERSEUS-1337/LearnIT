{
    "title": "SJLlmG-AZ",
    "content": "Motion is a crucial signal for agents in dynamic environments, and learning to represent motion from unlabeled video is a challenging task. A model based on group properties of transformations is proposed to train a representation of image motion. This method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion without the need for labels. The deep neural network trained using this approach identifies image characteristics of motion in different sequence types, providing useful information for localization, tracking, and odometry in the context of vehicle motion. Motion perception is crucial for biological and computer vision. Understanding image motion helps agents make better decisions. Motion cues are used by organisms like flies to avoid threats. In computer vision, motion representation can be local or global, with optical flow being a common local approach. The motion subspace in image processing characterizes smooth changes in image motion, distinct from other image transformation subspaces like changes in human faces. Global representations like visual odometry compactly explain scene movement but are limited by rigid world assumptions. Smooth changes in the motion subspace correspond to realistic motion sequences in images. The representation of motion in image sequences should distinguish between realistic and unrealistic transformations. Learning global, nonrigid motion representations without labels is challenging but essential for understanding scene motion. This framework aims to capture observer motion and scene content without relying on pixel-level reconstruction or correspondence. Our method constrains the representation of motion by addressing key properties of the latent motion space. These properties include estimating metric properties of motion, ensuring consistent representation regardless of image content, and distinguishing natural motion sequences from those with unnatural transitions. A general model of visual motion is presented, utilizing group properties to constrain learning in the model. The model enforces group properties of associativity and invertibility during training using a metric learning approach on recomposed sequences. It trains a deep neural network to represent motion in image sequences globally without labels or explicit feature matching. The learned representation captures the global structure of motion in both 2D and 3D settings. The text discusses the construction of image sequences with equivalent motion to learn a group homomorphism \u03a6 \u2208 M between motion in the world and in an embedding. It compares global representations of motion like structure from motion (SfM) and simultaneous localization and mapping (SLAM) with local representations like optical flow, scene flow, and non-rigid structure from motion. These methods aim to represent different types of motion in image sequences. The text discusses methods for estimating 3D point trajectories and motion representation using spatio-temporal features (STFs) like convolutional neural nets (CNNs). Unlike other approaches, this work focuses on capturing overall motion rather than local regions. The text discusses various methods for training networks to estimate optical flow, depth, and motion from images or scenes. These methods utilize different techniques such as classifying image patches, using brightness constancy assumption, and exploiting the relationship between depth and disparity. The text discusses different approaches for learning representations of image content from sequences, including methods that capture single image properties correlated with temporal order rather than motion itself. These approaches differ from traditional methods that use labeled motion as a learning cue. The text discusses learning sequence embeddings using RNN outputs and embedding loss to adjust distances. Sequences are recomposed to enforce associativity and invertibility, capturing transformations in continuous image sequences. The method approximates a group homomorphism between latent motion and its representation, focusing on how motions relate. In the model, a latent motion space M is defined as a closed subgroup of homeomorphisms on a structure space S. This space respects associativity and invertibility, allowing for composition of motions. The model includes a hidden Markov model for a discrete set of images, with M being associative, containing the identity, and having unique inverses for all elements. A specific example is rigid image motion in 3D scenes. The latent structure of a scene can be modeled by a point cloud with a motion space in 3D. Different scenes have varying degrees of freedom, making it unclear which group effectively characterizes motion. The goal is to learn a function that maps pairs of images to a representation of the motion space and an operator that emulates the composition of elements in the motion group. The representation and operator for motion should adhere to the group properties of the latent motion. The sequence representation should exhibit properties such as equivalence of differently composed subsequences, existence of an identity element, and invertibility. An embedding loss is used to enforce associativity and invertibility. The embedding loss enforces associativity and invertibility among subsequences sampled from an image sequence. It encourages uniqueness of the identity representation by pushing loops away from non-identity sequences in the embedding space. This learning framework can be viewed as inference on a graphical model. Learning a representation of motion is an underconstrained problem, and the group learning rules introduced here constrain the problem with minimal restriction on scene changes. The framework encompasses flow inference with brightness constancy as a constraint on the projection operator. The model's assumptions about geometric properties of motion are valid even with large motions and changing illumination. The latent structure and motion of a scene are non-identifiable, implying multiple representations can adequately represent it. Our method aims to capture a representation of motion in a scene with stable structure, using a CNN and LSTM network. It may struggle with rapidly changing content or occlusion. The LSTM operates over the sequence of CNN outputs to produce an embedding sequence. The network is trained to minimize a hinge loss with respect to the embedding of pairs of sequences, using cosine distance. Six recomposed subsequences are included for each training sequence. During training, sequences with varying lengths are used to encourage generalization to motions of different temporal scales. Different sampling schemes are employed to discourage the network from focusing only on the beginning or end of a sequence. The network is exposed to sequences with the same start and end frames but different motion to encourage reliance on features in the motion domain. Additionally, a representation \u03a6 is explored using single images as CNN input to further enhance learning. The CNN in this configuration only has access to single images, so it cannot directly extract image motion. The representation learned from image pairs outperformed the one learned from single images in all tested domains. The learning procedure can discover motion structure in rigid scenes undergoing 2D translations and rotations. Features useful for representing motion were learned on the KITTI dataset of vehicle sequences. Networks were trained using Adam, with specific training parameters for MNIST and KITTI datasets. The CNN-LSTMs were implemented in Torch and used dilated convolutions to capture large-scale motion patterns. ReLU nonlinearities and batch normalization were applied after each convolutional layer. The networks were trained on sequences of 3-5 images and tested on sequences of up to 12 images from the MNIST dataset. The learning procedure was tested on image sequences with smooth motion from 2D translations and rotations for 20 frames. The CNN-LSTMs used dilated convolutions to capture motion patterns in Torch. The network learned a representation that clusters sequences based on translation and rotation, not image content. Saliency maps show positive and negative gradients of the network. The saliency maps depict gradients of the LSTM output over the sequence, resembling spatiotemporal energy filters for motion processing. They indicate how pixel values influence the representation, not just the shape of first layer filters. The network's functional mapping can adapt to image content, unlike standard energy model filters. The model's representation of motion in 3D scenes is tested using the KITTI dataset. The representation is evaluated for decoding camera motion on KITTI visual odometry sequences with ground truth camera poses. The study evaluates the model's ability to decode camera motion on KITTI visual odometry sequences by regressing from flow field PCA components to camera motion parameters using least squares. The method, referred to as Flow+PCA, achieves promising results on held-out test data. Despite not being trained on ground truth pose or data from the odometry dataset, the learned representation decodes pose better than chance, with significant improvements in X and Z translation. While not as competitive as Flow+PCA or state-of-the-art odometry methods, the method suggests a meaningful representation of motion. On KITTI visual odometry, the method distinguishes realistic from unrealistic motion, performing similarly to Flow+PCA regression with four to five principal components. The network's ability to capture typical motion of scenes is tested through an interpolation task, comparing distances between frames. Results on the KITTI tracking dataset are compared to Euclidean distance metrics. The network's ability to capture typical motion of scenes is tested through an interpolation task, comparing distances between frames. The embedding distance of the true frame is significantly lower than all other frames, unlike the Euclidean distance. Saliency maps on a KITTI dataset sequence highlight objects moving in the background and independent motions of the car, focusing on areas relevant to determining motion in 3D. The method presented in the study shows potential for learning features for independent motion detection and tracking. However, attempts to regress from the learned representation to action classes did not yield competitive performance. Future work will explore using group properties to encourage intermediate latents to represent motion alongside other tasks, aiming for an embedding that successfully maintains representations of spatial content and motion for tasks like action recognition. The study introduces a new model for learning motion representations by enforcing group properties of motion. The representation can generalize between scenes with different content and motion, useful for navigation and prediction tasks. The method performs similarly to the Flow+PCA method on KITTI dataset, showing potential for generating better global motion representations in various real-world tasks. The study introduces a new model for learning motion representations by enforcing group properties of motion. The representation can generalize between scenes with different content and motion, useful for navigation and prediction tasks. The marginal improvement in Flow+PCA appears to sharply drop off beginning around four to five principal components. The most dramatic increases in performance come from the Z component of translation and the Y component of rotation. Egomotion estimation benefits greatly from maintaining information about spatial position. Methods using flow fields explicitly represent local motion at each position of the image, while our method is global and does not. KITTI visual odometry is characterized by stereotyped depth and is reasonably modeled as rigid. Camera translation and rotation can be estimated from a full flow field nearly linearly. Flow principal components capture dominant motions exhibited by the vehicle on this dataset. The good performance of Flow+PCA in capturing dominant motions and depth configuration on the KITTI dataset highlights the advantage of domain-restricted models. Despite not making restrictive assumptions, our learning rule and model perform reasonably well in this setting."
}