{
    "title": "rygkk305YQ",
    "content": "This paper introduces a neural end-to-end text-to-speech model that can control latent attributes in generated speech, such as speaking style, accent, background noise, and recording conditions. The model uses a conditional generative model with hierarchical latent variables, including a categorical variable for attribute groups and a multivariate Gaussian variable for specific attribute configurations. Extensive evaluation shows its ability to control these attributes and consistently synthesize high-quality clean speech. The neural end-to-end text-to-speech model can control latent attributes in speech generation, such as speaking style, prosody, and noise levels. It uses a conditional generative model with hierarchical latent variables to achieve this. The neural end-to-end text-to-speech model can control latent attributes in speech generation, such as speaking style, prosody, and noise levels. It uses a conditional generative model with hierarchical latent variables to achieve this. Inputs include a vector inferred from the target speech to capture residual attributes not specified by other input streams, along with text and a speaker label. These models have shown convincing results in synthesizing speech resembling the prosody or noise conditions of reference speech, even if the text or speaker identity differs. However, in crowdsourced data like BID26, where prosody, speaker, and noise conditions vary simultaneously, simply copying latent attributes from a reference is insufficient for synthesizing speech that mimics the prosody of one reference while matching the noise condition of another. Disentangling the latent representation would allow for independent control of generating factors. The study aims to construct a systematic method for synthesizing speech with random latent attributes to facilitate data augmentation. They extend Tacotron 2 to model two separate latent spaces for labeled and unlabeled attributes using a variational autoencoding framework with Gaussian mixture priors. This approach allows for disentangled attribute representations and interpretable clusters in the training data. The proposed model is a probabilistic hierarchical generative model that improves sampling stability and attribute control compared to existing models. It factors the latent encoding using two mixture distributions to model supervised speaker attributes and latent attributes separately. The model is evaluated on four datasets with subjective and objective metrics, showing its capability to control speaker, noise, and style independently. The proposed model uses two mixture distributions to separately model supervised speaker attributes and latent attributes in a disentangled fashion. It can infer speaker attribute representation from noisy utterances and synthesize high-quality clean speech that approximates the voice of the speaker. This work is the first to train a controllable text-to-speech system on real data with variation in recording conditions, speaker identity, prosody, and style. The proposed model uses two mixture distributions to separately model supervised speaker attributes and latent attributes in a disentangled fashion. It can infer speaker attribute representation from noisy utterances and synthesize high-quality clean speech that approximates the voice of the speaker. Tacotron-like TTS systems predict acoustic features frame by frame to minimize reconstruction loss and integrate latent attributes for speech synthesis control. The proposed model utilizes a graphical model with hierarchical latent variables to control attributes for interpretability and disentanglement. It introduces two latent variables, y l and z l, alongside observed variables X, Y t, and y o. Sampling from priors and conditional distributions generates speech frames conditioned on text and attributes. The proposed model uses a GMM latent model with diagonal-covariance Gaussian priors to capture unseen attributes and achieve interpretability by assigning instances to different mixture components. The covariance matrix is constrained to be diagonal for statistical capture. The proposed model utilizes a GMM latent model with diagonal-covariance Gaussian priors to capture unseen attributes and ensure interpretability. The conditional output distribution is parameterized with a neural network, following the VAE framework. The model is trained by maximizing its evidence lower bound. The model is trained by maximizing its evidence lower bound (ELBO) using a GMM latent model with diagonal-covariance Gaussian priors. A continuous latent variable, z o, is introduced to model within-class variation and infer representations for one-shot learning. The model introduces a continuous latent variable z o to capture within-class variation and infer representations for one-shot learning. The model comprises a synthesizer, a latent encoder, and an observed encoder, with a diagonal-covariance Gaussian prior for the latent variable. The variances of the latent variable are initialized to be smaller than those of another latent variable to encourage disentanglement of observed and latent attributes. The model utilizes a synthesizer, latent encoder, and observed encoder to parameterize three distributions with neural networks. The synthesizer is based on the Tacotron 2 architecture, mapping text to text encodings and predicting speech. Latent variables are injected into the decoder for each step, representing phonemes in the sequence. The proposed GMVAE-Tacotron model utilizes a WaveRNN-based neural vocoder for faster inference and parameterizes two posteriors with a recurrent encoder. It is related to models that use reference embeddings to model prosody or noise. The GMVAE-Tacotron model incorporates a WaveRNN-based neural vocoder for quicker inference and utilizes reference embeddings to model prosody or noise. It introduces a mixture distribution for latent attributes, enabling automatic discovery of attribute clusters for easier interpretation of the latent space. The GMVAE-Tacotron model uses a mixture distribution for latent attributes to interpret the underlying latent space. It identifies distinctive dimensions and adds a second mixture distribution to model speaker attributes, allowing for disentangled speaker and latent attribute representations. This speaker model can approximate the voice of unseen speakers by controlling the output speaker identity using speaker embeddings. The proposed GMVAE-Tacotron model introduces a special case where the variance of z o is nearly zero, resulting in a fixed speaker representation. The model utilizes a mixture distribution for latent variables and provides flexibility for conditional generation scenarios. It was evaluated on various datasets with different speaker attributes and recording conditions. The study used y o as a categorical variable with the number of speakers in the training set, y l as a 10-way categorical variable, and z l and z o as 16-dimensional variables. Tacotron 2 with a speaker embedding table served as the baseline for experiments, trained for at least 200k steps using the Adam optimizer. Quantitative evaluations relied on mean opinion scores (MOS) from native speakers rating speech naturalness on a scale of 1 to 5. The study evaluated GMVAE-Tacotron's ability to model speaker variation using a dataset of 385 hours of English speech from 84 professional voice talents with various accents. Speaker labels were only used for evaluation, not during training. The model's interpretability was tested by analyzing the distribution of mixture components for utterances of specific accents or genders. The study analyzed the distribution of mixture components for utterances of specific accents or genders. Components were used to model speakers from one gender, with some representing a subset of accents. The assignment consistency was 92.9%, grouping utterances by speaker and speakers by gender or accent. Dimensions of z l controlled distinct characteristics of speakers. The study demonstrated the disentangled nature of learned latent attribute representation, showing control over F0, speaking rate, accent, and more. The model can synthesize clean speech from noisy data by separating background noise level, allowing independent control. Artificially generated training sets were used to add background noise and reverberation to clean speech for experimentation. The study showed the model's ability to synthesize clean speech from noisy data by separating background noise levels. Noise signals were added to a random selection of 50% of utterances, with two speakers having noise added to all their utterances. Speaker labels were provided as input to the decoder, expecting latent attributes to capture the acoustic condition of each utterance. The study demonstrated the model's capability to differentiate between clean and noisy speech components. By analyzing the Euclidean distance between component means, two distinct clusters were identified. Synthesized utterances showed that samples from one cluster were noisy, while samples from the other cluster were clean. Further analysis was conducted to determine if noise levels were influenced by a single latent dimension. The study identified the most discriminative latent dimension with a scattering ratio of 21.5, compared to 0.6 for the second largest. By analyzing the effect of this dimension on output values while keeping others fixed, the effective range was determined using a Gaussian approximation spanning four standard deviations. The study identified the most discriminative latent dimension with a scattering ratio of 21.5, compared to 0.6 for the second largest. The noise level was controlled by manipulating the 13th dimension, while other dimensions had minimal impact. Appendices contain spectrogram demonstrations of noise level control. The study evaluated synthesis quality for noisy speakers using subjective MOS ratings and an objective SNR metric. The proposed model outperformed a baseline, a 16-token GST, and a VAE variant. Different strategies were used to encourage clean audio synthesis under each model. The proposed model synthesized natural and high-quality speech. The proposed model demonstrated superior synthesis quality for noisy speakers, achieving the highest MOS and SNR. Evaluating prosody and speaking style, the model utilized a dataset of 147 hours of US English audiobook recordings for training. Results showed the model's ability to sample and control speaking styles, enhancing naturalness in speech synthesis. The proposed model showed superior synthesis quality for noisy speakers, utilizing a dataset of US English audiobook recordings. It demonstrated the ability to sample and control speaking styles, enhancing naturalness in speech synthesis. The prior captured a common prosody, allowing for more natural speech synthesis with lower variance compared to the baseline. The model supports random sampling of natural speech from the prior, providing wide variation in speaking rate, rhythm, and F0. Unlike the GST model, it does not require heuristically chosen weights for token normalization. The GMVAE-Tacotron model learns to disentangle attributes and control them independently, utilizing the GST weight simplex during training. Latent dimensions in the model are conditionally independent, allowing for manipulation of attributes like speed. This is demonstrated in Figure 7 (b) where the F0 contours are stretched horizontally to manipulate speed while maintaining the same shape. Unlike the GST model, GMVAE-Tacotron does not require heuristically chosen weights for token normalization. The GMVAE-Tacotron model can manipulate attributes like speed independently, as shown in Figure 7 (b). In contrast, the style control of GST is more complex, with changes in F0 while controlling speed. The model can synthesize speech resembling a reference utterance using an audiobook dataset sampled at 24kHz. Speaker identity is closely linked to recording conditions and speaking style in the dataset. The ability to disentangle and control attributes independently is crucial for synthesizing high-quality speech for all speakers. A continuous speaker representation was learned using a z o layer, with training data from 1,172 unique speakers. Different latent dimensions control various speech attributes, and modifying z l does not affect speaker identity if z o is fixed. One mixture component corresponds to a narrative speaking style in clean recordings. The GMVAE-Tacotron model demonstrates the ability to generate high-quality speech by conditioning on a clean output. Two approaches were considered: using the mean of the clean component or inferring a latent attribute representation from reference speech. Evaluation was done on sets of \"seen clean,\" \"seen noisy,\" and \"unseen noisy\" speakers, as well as \"unseen clean\" speakers. The study evaluated the effectiveness of using inferred latent features in the GMVAE-Tacotron model for generating high-quality speech. Results showed that the inferred latent features captured acoustic variations and denoising operations improved the quality of synthesized audio. The study compared the effectiveness of using denoising operations and inferred latent features in the GMVAE-Tacotron model for generating high-quality speech. Results showed that both methods outperformed the baseline, with raters preferring the proposed model in subjective tests. The MOS evaluation indicated that the proposed model delivered similar naturalness levels under all conditions. The study evaluated the synthesized speech for speaker similarity using denoised latent attribute representations compared to baseline and d-vector systems. The proposed model showed similar speaker similarity scores to the baseline on the seen clean speaker set but outperformed on the seen noisy speaker set. The proposed model and denoised baseline performed worse than the baseline on the seen noisy speaker set due to biased speaker similarity ratings from similar acoustic conditions. Ground truth utterances with varied conditions also had lower ratings, indicating the need for an unbiased speaker similarity test in noisy environments. The proposed model achieved better speaker similarity scores on clean speaker set compared to d-vector system, but worse than d-vector (large) system trained on more speakers. The GMVAE-Tacotron TTS model enables fine-grained control of latent attributes and systematic sampling. The proposed model can learn speaker attributes from unseen utterances and synthesize high-quality speech for a target speaker, even with low-quality data. It can control latent attributes independently and cluster them without supervision, making it effective for training controllable TTS systems on large-scale data with rich styles. The section provides a detailed derivation of the evidence lower bound (ELBO) estimation used for training controllable text-to-speech systems. It includes a differentiable Monte Carlo estimation of the posterior and an ELBO for graphical models with varying attribute representations. The posterior over latent attribute class is approximated using a Gaussian mixture model, with details on mean vector and covariance matrix elements. The text discusses the derivation of the evidence lower bound (ELBO) for training controllable text-to-speech systems. It involves a differentiable Monte Carlo estimation of the posterior and an ELBO for graphical models with varying attribute representations. The posterior over latent attribute class is approximated using a Gaussian mixture model. The ELBO for training controllable text-to-speech systems involves using an additional observed attribute representation and a Monte Carlo estimation. The synthesizer is an attention-based network following Tacotron 2 architecture, with input text encoded by convolutional layers and bidirectional LSTM. The Tacotron 2 architecture uses a bidirectional LSTM for text encoding and a location sensitive attention mechanism for decoding. The decoder is extended to consume additional attribute representations by concatenating them with the original input. The Tacotron 2 architecture uses a bidirectional LSTM for text encoding and a location sensitive attention mechanism for decoding. The decoder incorporates residual connections and linear projection to predict the mel spectrum and an end-of-sentence token. A post-net predicts a residual to enhance spectrogram detail. A WaveRNN-based neural vocoder is used for waveform inversion. Latent and observed encoders map mel spectrograms to vectors representing latent variables. The network architecture includes two symmetric encoders with the same design, using convolutional and bidirectional LSTM layers to disentangle latent and observed attributes. The Adam optimizer is used with an initial learning rate of 10^-3 and exponential decay. The network architecture includes two symmetric encoders with convolutional and bidirectional LSTM layers. The learning rate is adjusted every 12.5k steps after 50k steps. Parameters are initialized using Xavier initialization. A batch size of 256 is used. The number of samples for Monte Carlo estimate is set to 1. Prior hyperparameters for four datasets are detailed. A minimum value is set for the standard deviation of the conditional distribution. After adjusting the minimum standard deviation to e \u22122, the dimensionality of z l was increased from 16 to 32 to improve reconstruction quality. However, reducing the dimensionality too much can lead to insufficient modeling capacity for latent attributes. Empirically, a 16-dimensional z l was found to be appropriate for capturing salient attributes in the datasets. The majority of dimensions were interpretable, with varying numbers of dummy dimensions that did not affect the model output. The model trained on different datasets had varying numbers of dummy dimensions that did not affect the output. In the graphical model, there are two latent variables: the discrete latent attribute class y l and the continuous latent attribute representation z l. The continuous latent variable z l is used to condition the generation of X along with other observed variables. In experiments, the latent variable z l is always utilized without dropping to zero KL-divergence, even without KL-annealing tricks. Previous studies show that directly conditioned latent variables can collapse when using strong models like auto-regressive networks. This is due to the competition between improving reconstruction performance with the latent variable and decreasing KL-divergence by making it uninformative. Auto-regressive models tend to converge to the latter case during training, but not always. In experiments, the latent variable z l is always utilized without dropping to zero KL-divergence, even without KL-annealing tricks. The posterior-collapse does not occur in our experiments due to the complexity of the speech sequence distribution. Reconstruction performance can be significantly improved by utilizing information from z l, which indexes mixture components in the latent attribute representation space. No degenerate clusters were observed when training our model with specific hyperparameters. Our GMVAE differs from Dil-GMVAE, as explained below. In our model, the conditional distribution is Gaussian, parameterized by mean and covariance vector. In contrast, Dil-GMVAE uses neural networks to parameterize the conditional distribution of z l given y l, resulting in a mixture of infinitely many diagonal-covariance Gaussian distributions for more complex modeling. Dil-GMVAE has a stronger stochastic decoder compared to GMVAE, allowing for better mapping of y l. In Dil-GMVAE, the stochastic decoder maps y l to z l with a complex marginal distribution p*(z l). The choice of using the same distribution for all y l is preferred over modeling different distributions for each y l to minimize KL-divergence on y l. This approach maintains the expressiveness of z l in the ELBO objective. Our GMVAE formulation reduces to a single Gaussian when p(z l | y l ) is the same for all y l, leading to a trade-off between the expressiveness of p(z l ) and the KL-divergence on y l. Posterior-collapse in our model occurs when the conditional mean and variance for each mixture component are the same. The ELBO derived from our model includes terms related to p(z l | y l ), such as the expected KL-divergence on z l and the KL-divergence on y l. The second term encourages a uniform posterior q(y l | X), promoting posterior collapse by pulling the conditional distribution for each component closer together. The first term in the curr_chunk discusses how the closeness of p(z l |y l ) to q(z l |X) is influenced by the posterior distribution q(y l |X), which can either promote or prevent posterior collapse based on the entropy of q(y l |X). The variance in this distribution plays a key role in determining the level of collapse. The temperature parameter in softmax affects the posterior distribution over y. Setting a smaller initial variance helps prevent posterior collapse. Different components model US female speakers with varying F0 ranges. Dimension 0 controls the duration of the initial pause in speech. Dimension 0 controls the duration of the initial pause before speech begins, dimension 2 controls the overall speaking rate, and dimension 9 mainly controls the degree of F0 variation. Accent differences controlled by dimension 3 are easier to recognize through audio samples. Speaker attributes were evaluated using classifiers on latent features, with test utterances split in a 9:1 ratio for training and evaluation. Three LDA classifiers were trained. The GMVAE-Tacotron model was evaluated for controlling individual attributes using two metrics: average fundamental frequency (F0) and average speech duration. 10 samples of seed z l were drawn from the prior to synthesize 25 text sequences for each resulting value of z * l. The model's ability to predict speaker identity, gender, and accent was assessed using three LDA classifiers trained on latent attribute representations. The GMVAE-Tacotron model was evaluated for controlling individual attributes by computing average metrics over 250 synthesized utterances. The results showed that manipulating individual dimensions primarily controls the corresponding attribute, with a slight increase in pitch observed with speaking rate. The model's ability to synthesize speech resembling the prosody or style of a reference utterance was evaluated using two metrics for style transfer performance. The evaluation of the proposed model for style transfer performance included measuring phonetic and timbral distortion using MCD and F 0 frame error (FFE). The model with a 16-dimensional z l was better than the baseline but inferior to a GST model with 60 degrees of freedom. Increasing the dimension of z l to 32 reduced the gap to the GST model. The proposed model for style transfer performance was evaluated using MCD and F0 frame error. A 16-dimensional zl model performed better than the baseline but was inferior to a GST model with 60 degrees of freedom. Increasing zl to 32 narrowed the gap to the GST model. The ability to reconstruct speech frame-by-frame is correlated with latent space capacity. Increasing zl dimensionality could improve performance but may reduce interpretability and generalization of latent attribute control. The study evaluated style transfer performance using MCD and F0 frame error. A 16-dimensional zl model outperformed the baseline but was not as good as a GST model with 60 degrees of freedom. Increasing zl to 32 narrowed the performance gap with the GST model. The ability to reconstruct speech frame-by-frame is linked to latent space capacity. Increasing zl dimensionality could enhance performance but might impact interpretability and generalization of latent attribute control. The GMVAE-Tacotron can be used for non-parallel style transfer to generate speech with different text content. Five samples of zl encode different speaking styles, with variations in speaking rate and F0. The synthesized mel-spectrograms demonstrate independent control of speaking style and prosody. Synthesized mel-spectrograms show independent control of speaking style and prosody. F0 is controlled for different values, pause duration before a phrase is varied, and voice roughness is adjusted. The spectrograms demonstrate changes in F0 range, pause duration, and voice roughness. The synthesized mel-spectrograms demonstrate independent control of attributes related to style, recording channel, and noise-condition. F0 contours vary while traversing different dimensions, affecting energy distribution in low and high-frequency bands. This dimension may correspond to variation in microphone frequency response in the training data. The bottom row shows variation in speaking rate while other attributes remain constant. Background noise level varies in the third row, affecting energy distribution in low and high-frequency bands. Audio samples can be found at the provided link."
}