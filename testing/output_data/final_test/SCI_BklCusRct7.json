{
    "title": "BklCusRct7",
    "content": "Generative models like VAEs and GANs are trained on fixed prior distributions in the latent space. Sampling from the Generator can be done in various ways for exploration. Previous works have tried to reduce distribution mismatch between outputs and the prior distribution through heuristic modifications or changing the latent distribution. This paper proposes a framework to fully eliminate this mismatch in latent space operations. Our approach, based on optimal transport maps, eliminates distribution mismatch in latent space operations for generative models like VAEs and GANs. The discriminator in GANs classifies synthetic samples from real ones, while the generator maps samples from a fixed prior distribution to fool the discriminator. Generative models like VAEs and GANs use trained generators to map latent samples from a fixed prior distribution to output samples that match the data distribution. In the literature, exploring the behavior of the model often involves various arithmetic operations on the latent samples, causing a distribution mismatch. Our approach based on optimal transport maps aims to eliminate this issue in latent space operations for generative models. Our approach using distribution matching transport maps aims to address distribution mismatches in latent space operations for generative models, such as sampling and vector arithmetic. This ensures that the generator trained on a fixed prior can still produce outputs consistent with the prior distribution. Our proposed technique addresses distribution mismatches in latent space operations for generative models by modifying the linear trajectory to distribute points identically to the prior. This interpolation operator avoids distribution mismatch when interpolating between samples of a uniform distribution. Many papers exploring learned models have ignored this issue, but our approach minimizes deviations from the original trajectory to maintain consistency. The approach aims to address distribution mismatches in latent space operations for generative models by modifying linear interpolation to distribute points consistently with the prior distribution. BID20 experimentally observed a distribution mismatch between points drawn from uniform or Gaussian distributions and those obtained through linear interpolation, proposing the use of spherical linear interpolation (SLERP) to improve sample quality. While SLERP has shown better results than linear interpolation, its heuristic nature limits its full replacement. In high dimensions, SLERP tends to approximate distribution matching for interpolation, as shown in Section 2. BID6 analyze distribution mismatch for Gaussian priors and propose an alternative prior with dependent components. Our framework allows adaptation of operations while preserving the original prior distribution. The approach involves finding a warping function to match the distribution of the desired operation with the prior distribution. Optimal transform is used to modify the operation while staying faithful to the original. This is achieved by warping points of the operation to match a curve that closely follows the original trajectory. With implicit models like GANs and VAEs, data X is used to train a generator G to approximate x. Operations on latent samples result in random variables, and examples are shown in the literature. Linear interpolation is a common operation used in the literature to analyze how a trained generator changes when creating related samples from source samples. However, there is a distribution mismatch issue with linear interpolation and other operations, leading to a dilemma. Linear interpolation and other intuitive operations in TAB0 result in a distribution mismatch with the trained generator's distribution. This mismatch becomes significant in high dimensions, as shown in FIG1 with d = 100. BID6 quantify this mismatch using KL-Divergence, which grows linearly with the dimension d. This phenomenon occurs for all prior distributions with i.i.d. entries. The analysis shows that distribution mismatch occurs for all prior distributions with i.i.d. entries, not just Gaussian. A framework using optimal transport is proposed to construct distribution preserving operators. The proposed matched interpolation preserves the prior distribution for both priors by minimizing the modification needed to bring the distribution back to the prior. The problem of finding a minimal modification in optimal transport theory involves minimizing the cost function between probability distributions p x and p y with domains X and Y, respectively. This problem was first posed by Monge in 1781 and can be formally stated as minimizing the cost function. The optimal transport problem aims to minimize the cost function between probability distributions p x and p y with domains X and Y, respectively. A relaxed approach was introduced by Santambrogio in 2015, where a joint probability distribution p x,y is used to find the optimal transport plan from p x to p y with respect to the cost c. This approach allows for a probabilistic relationship between x and y, unlike the deterministic approach of the original problem posed by Monge in 1781. The optimal transport problem involves minimizing the cost function between probability distributions p x and p y. Problem 1 may have a minimization over the empty set, while Problem 2 allows for the construction of a joint density p x,y with marginals p x and p y. This leads to a stochastic function f (x) from X to Y, where f is a stochastic mapping. The optimal transport problem involves minimizing the cost function between probability distributions p x and p y. For constructing operators, it is desirable for the mapping f to be deterministic. By choosing a suitable cost function c, we can find an analytical or efficient numerical solution. The operators in TAB0 are pointwise, resulting in i.i.d. components for both input and output. The minimization problems (MP) and (KP) simplify into identical scalar problems. The minimization problems (MP) and (KP) simplify into identical scalar problems for distributions p X and p Y. An optimal transport map T for (MP-1-D) gives an optimal transport map f for (MP) by pointwise application of T. The scalar problems have a known solution under mild constraints, with a convex function h and cost C(x, y) = h(x \u2212 y). An optimal transport map from p X to p Y is defined by the Cumulative Distribution Function (CDF) of X and the pseudo-inverse. The Cumulative Distribution Function (CDF) of X and F \u2265 y} is the pseudo-inverse of F Y. The joint distribution of (X, T mon X\u2192Y (X)) defines an optimal transport plan for (KP-1-D). The mapping T mon X\u2192Y (x) is non-decreasing and known as the monotone transport map from X to Y. By combining Theorems 1 and 2, a concrete realization of Strategy 1 is obtained. The cost c is chosen to admit to Theorem 1, and the monotone transport map is computed. In FIG3, the monotone transport map is shown. In FIG3, the monotone transport map is displayed for linear interpolation y = tz1 + (1 \u2212 t)z2 for various t values. Detailed calculations and examples for different operations are provided in Appendix B for Uniform and Gaussian priors. Validation of matched operators is done through numerical simulations of distributions for toy examples and common prior distributions. In 2-D, midpoint distributions of linear interpolation, matched interpolation, and spherical interpolation are compared using a uniform prior, showing that the matched interpolation aligns with the prior distribution while linear interpolation condenses towards the origin. The linear interpolation condenses towards the origin, forming a pyramid-shaped distribution. The spherical interpolation of BID20 follows a great circle with varying radius between two points, resulting in a distribution with a \"hole\" circling around the origin. Sampling 1 million pairs of points in 100 dimensions shows a dramatic difference in vector lengths between the prior and midpoints of linear interpolation. Spherical interpolation matches the prior distribution's first moment but induces a distribution mismatch. Matched interpolation fully preserves the prior distribution. The study used DCGAN generative models trained on LSUN bedrooms, CelebA, and an icon dataset to evaluate image quality. Different output resolutions were tested, with models trained on uniform or Gaussian latent priors. The dimensionality of the latent space varied between datasets. Improved Wasserstein GAN with gradient penalty was used for training. The study evaluated image quality using DCGAN generative models trained on various datasets with different latent space dimensions. They used improved Wasserstein GAN with gradient penalty for training. The Inception score of trained models was compared with scores from linear interpolations, showing a significant decrease in score with linear interpolation. The study compared the Inception score of DCGAN generative models trained on different datasets with linear interpolation, showing a significant decrease in score with linear interpolation. The matching procedure used in the study produced samples from the same distribution as random samples, resulting in a perturbation range of 0.23 - 0.25 for 2-point interpolation on latent spaces. The study compared Inception scores of DCGAN generative models trained on different datasets with linear interpolation, showing a significant decrease in score. The matching procedure produced samples from the same distribution as random samples, with a perturbation range of 0.23 - 0.25 for 2-point interpolation on latent spaces. Visual comparisons between linear interpolation, SLERP, and matched interpolation showed that linear interpolation produced inferior results, while SLERP and matched interpolation were visually similar in quality. The study compared Inception scores of DCGAN generative models trained on different datasets with linear interpolation, showing a significant decrease in score. Linear interpolation produced inferior results visually compared to SLERP and matched interpolation, especially evident in 4-point interpolation. The divergence between interpolation methods is most pronounced at the midpoint of the interpolation. The study compared Inception scores of DCGAN generative models trained on different datasets with linear interpolation, showing a significant decrease in score. Linear interpolation produced inferior results visually compared to SLERP and matched interpolation, especially evident in 4-point interpolation. The divergence between interpolation methods is most pronounced at the midpoint of the interpolation. Vicinity sampling examples are provided in the Appendix, showcasing the differences in output under linear, spherical, and matched operators. A random walk in the latent space can be achieved through matched vicinity sampling, starting from a point drawn from the prior distribution. The study proposed a framework using optimal transport to eliminate distribution mismatch in common latent space operations for generative models. Analytical formulas for matched operations were provided, resulting in higher quality samples compared to the originals. This work was supported by ETH Zurich General Fund and Nvidia. Published at ICLR 2019. The analysis in this study focuses on the difference between the average norm of the midpoint of linear interpolation and the points of the prior in a latent space with a prior distribution. It is shown that almost all points lie on a thin spherical shell due to the properties of the distribution, particularly for i.i.d Gaussian entries. The distribution of the midpoint of linear interpolation lies mostly on a spherical shell, with a different radius than the prior points. The shells intersect at regions with low probability for large dimensions, making the midpoint a strong outlier. The KL-Divergence between the midpoint and the prior points grows linearly with the dimensions. The text discusses the Kantorovich problem and its relation to the Monge version. It explains how minimizing one problem will also minimize the other, and provides examples of computing matched operations for linear interpolation and vicinity sampling. The text discusses operations for linear interpolation and vicinity sampling using a uniform or Gaussian prior. It explains how to compute the monotone transport map and provides a helpful lemma for finding non-decreasing mappings. The text discusses operations for linear interpolation and vicinity sampling using a uniform or Gaussian prior. It explains how to compute the monotone transport map and provides a lemma for finding non-decreasing mappings. The transformation from X to Y involves finding a nondecreasing function g that transforms p X to p Y. Given a uniform distribution Z \u223c Uniform(\u22121, 1), the linear interpolation between two points z1 and z2 with component distribution p Yt is computed. The CDF F Yt is then obtained by a series of computations."
}