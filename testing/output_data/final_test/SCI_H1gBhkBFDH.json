{
    "title": "H1gBhkBFDH",
    "content": "Group convolutional neural networks (G-CNNs) enhance classical CNNs by incorporating geometric structure of groups. G-CNNs lift feature maps to higher dimensional disentangled representations, ensuring equivariance under geometric transformations. A modular framework is proposed for G-CNNs on arbitrary Lie groups, using B-splines defined on the Lie algebra to expand convolution kernels. The study introduces a flexible framework for localized, atrous, and deformable convolutions in G-CNNs using B-splines on the Lie algebra. The impact of this approach is evaluated on cancer detection and facial landmark localization datasets, showing superior performance compared to classical 2D CNNs. G-CNNs are equivariant to transformations described by the group, guaranteeing predictable behavior and insensitivity to local and global transformations. By considering larger groups, additional geometric structure can be utilized to improve performance. The success of G-CNNs is attributed to lifting feature maps to higher dimensional objects generated by matching kernels under a range of poses. The success of G-CNNs is attributed to lifting feature maps to higher dimensional objects generated by matching kernels under a range of poses. This disentangles features with respect to pose/transformation parameters, enabling flexible learning of high-level representations. This hierarchical composition resembles the recognition-by-components model and is also seen in capsule networks. The group theoretical connection is made explicit with equivariant capsules providing sparse index/value representation of feature maps on groups. The success of G-CNNs is attributed to disentangling features with respect to pose/transformation parameters, allowing for flexible learning of high-level representations. This is illustrated in the detection of faces using group structures to recognize patterns of relative transformations. Representing low-level features via feature maps on groups is motivated by the organization of orientation-sensitive cells in the primary visual cortex. This approach is mathematically modeled by sub-Riemannian geometry on Lie groups, leading to effective algorithms in image analysis. In recent work, advanced V1 modeling geometries emerge in specific CNN architectures, showing the relation between group structure and the organization of V1. The Log-map allows mapping elements from curved manifolds to a flat Euclidean tangent space, with Lie groups providing a flexible tool to define group convolution kernels. The 2-sphere is treated as a quotient group in this context, with technical details given in the text. The introduction of G-CNNs by Cohen & Welling (2016) has led to a growing body of literature demonstrating their superiority over classical CNNs. G-CNNs are locally equivariant to transformations in a Lie group H, with core definitions such as group product, inverse, Log, and action on R d needing implementation. The impact of G-CNNs is studied on datasets focusing on rotation and scale equivariance, showing superior performance compared to classical 2D counterparts. The added value of atrous and localized G-convolutions is also examined in detail. The introduction of G-CNNs by Cohen & Welling (2016) has shown their superiority over classical CNNs in being locally equivariant to transformations in a Lie group H. Different types of G-CNNs have been explored, including discrete, regular continuous, and steerable continuous G-CNNs. Steerable G-CNNs are specialized in handling 3D rotations by expanding kernels in circular/spherical harmonics. This approach is limited to unimodular groups. In this paper, scale-translation G-CNNs are experimented with to achieve scale equivariance beyond roto-translations. Scale equivariance has been addressed in various settings using scale-space theory and semi-group theory. The work differs in using non-learned lifting layers, discrete group convolutions via atrous kernels, and semi-group theory. The current work generalizes B-spline based SE(2) CNNs by incorporating scale invariance through shared weights among kernels with different dilation rates. This approach is a special case of B-spline G-CNNs and utilizes Lie group principles to construct convenient coordinate systems for handling equivariance. Unlike other methods, this work deals with local symmetries and is not translation equivariant. The current work generalizes B-spline based SE(2) CNNs by incorporating scale invariance through shared weights among kernels with different dilation rates. It utilizes Lie group principles to handle equivariance, where points relate via the logarithmic map. This approach differs from other methods that use the exponential map with respect to gauge frames. The essential tools for deriving a generic framework G-CNNs involve treating groups in an abstract setting, with examples provided for the rototranslation group. A group is defined by a set G with a group product operator satisfying closure, identity, and inverse axioms. A Lie group is a group with a differential manifold structure, smooth group product, and inverse. The differentiability of the group leads to infinitesimal generators. The Lie group induces infinitesimal generators in the Lie algebra, identified with the tangent space at the identity. Vectors in the Lie algebra can be expanded in a basis and mapped to a flat Euclidean space through the logarithmic map. Semi-direct product groups are specifically considered in this work. The curr_chunk discusses affine Lie groups of type G = R d H, where H is the semi-direct product of the translation group R d with a Lie group. An example is the special Euclidean motion group SE(2) with H = SO(2). The group product of G involves rotation matrices acting on vectors in R 2. Transformations that preserve the group structure are described, and equivariance to G is defined. An operator \u03a6 is said to be equivariant to a group G when it satisfies Equivariance of neural networks with respect to G ensures no information loss during transformations, just a shift in network locations. NNs must use layers with linear operators defined by group convolutions for equivariance. Traditional NN layers involve input vector x, linear map K w : X \u2192 Y, bias term b, and non-linearity \u03c6. Instead of Euclidean spaces, this work focuses on structured data with feature maps as functions on domain X. Group convolutional neural networks are formed by constraining the linear operator to be equivariant under transformations in a group G. This constraint leads to equivariant maps between homogeneous spaces, defined by a one-argument kernel for any g y \u2208 G. The kernel is constrained based on the quotient of G with H = Stab G (y 0 ). The kernel front-factor simplifies for homogeneous spaces of an affine Lie group G. Standard CNNs are a special case of G-CNNs equivariant to translations. Rotation invariance limits representation power for planar feature maps. The proposed G-CNNs aim to maximize representation power by lifting feature maps to the higher dimensional domain of the group itself. Three types of layers are suggested: group correlation layer, lifting correlations layer, and spatial cross-correlation layer. These layers utilize group correlations and transformations to enhance the convolutional kernel's effectiveness under all possible transformations in G. The G-CNNs aim to enhance convolutional kernels by transforming them under the action of group H. The kernels are sampled on a discrete grid and expressed in an analytic form for computations under arbitrary transformations in G. Bekkers et al. (2015a) proposed expanding group correlation kernels using shifted cardinal B-splines on Lie groups H to construct kernels on R^d H for G-correlations. Cardinal B-splines are localized polynomial functions with finite support, and can be used to construct kernels on SE(2) by identifying the group with positions and orientations. The new definition of B-splines on Lie groups enables the construction of required kernels for G-correlations. B-splines are functions expanded in a basis of shifted and scaled copies of cardinal B-splines. They are fully characterized by spline degree, scale, and a set of centers forming a uniform grid. B-splines on Lie groups are functions expanded in a basis of shifted and scaled copies of cardinal B-splines with additional parameters. The spline is uniform if certain conditions are met. Examples of B-splines on Lie groups are provided. B-splines on Lie groups are expanded as tensor products of B-splines on R d and H. Design choices include using trainable coefficients and fixed centers and scales. Global vs localized uniform B-splines are considered, with global uniform B-splines only possible for certain Lie groups. Constructing approximately uniform B-splines on compact groups involves creating a grid of centers that covers the group uniformly. Atrous B-splines, achieved through convolutions with sparse kernels, are commonly used to increase receptive field size in deep CNNs. Atrous convolution kernels can be constructed with B-splines by fixing scale factors and increasing distance between center points. Deformable CNNs treat centers as trainable parameters, leading to deformable convolution layers. G-correlation layers using B-spline kernels result in equivariant G-CNNs. The text discusses the use of B-splines in G-CNNs for roto-translation equivariance and invariance. It highlights the partition of unity on rotation groups, scaling groups, and spheres, as well as the multi-scale properties illustrated through texture reconstruction. Experiments compare the performance of roto-translation G-CNNs against a 2D baseline and explore different choices for achieving equivariance and invariance. The study compares the performance of roto-translation G-CNNs with a 2D baseline and explores different kernel choices for SO(2)-part of the network. Finer sampling of SO(2) improves results until N h = 12, after which there is a slight degradation possibly due to overfitting. Atrous kernels tend to outperform dense kernels, and sampling coarse kernels on a fine grid can be beneficial. The study compares the performance of roto-translation G-CNNs with a 2D baseline using different kernel choices for the network. Atrous kernels outperform dense kernels, and augmentations are crucial for the 2D model but have minimal impact on the SE(2) model. The SE(2) model without augmentation outperforms the 2D model with augmentation, confirming the theory that G-CNNs guarantee both local and global equivariance. The CelebA dataset contains 202,599 RGB images of celebrities with labels for attributes and facial landmarks. The dataset consists of 17,548 images of 8,774 celebrities with faces at varying scales, each labeled with 5 heatmaps representing facial landmarks. A scale-translation G-CNN was trained with different kernel choices for the network. The G-CNNs use dense networks with kernels defined over the whole discretization of H = R +, allowing interactions between features at all scales. Local networks consider interactions between neighboring scales via localized kernels or no scale interaction at all. Each G-CNN is a multi-scale network with kernels applied at various scales, compared to a 2D baseline with fixed-scale kernels. The discretization of H is not a closed group, but the group structure still applies locally, allowing information to leak out of the domain. The group structure still applies locally, allowing information to leak out of the domain. Using localized kernels of size N k = 1 along the H axis can avoid this information leak. Results show that scale augmentations beyond 1.4 did not improve results, with an optimal scale of h = 2 giving the best trade-off between scale variations in the data. The experiments show that G-CNNs outperform 2D CNNs by considering both small and large scale features simultaneously. Localized kernels on H axis outperform all-scale interactions. 2D CNNs benefit moderately from scale augmentations but cannot match the performance of G-CNNs, indicating the importance of a multi-scale approach. The paper introduces a flexible framework for building G-CNNs for arbitrary Lie groups. The proposed B-spline basis functions enable the construction of localized, atrous, and deformable convolution kernels for G-CNNs on arbitrary Lie groups. Experimental results demonstrate the benefits of localized and atrous group convolutions, especially in scale-translation G-CNNs. Future work will explore further applications of these tools for equivariance constraints. The Lie group B-splines provide tools for equivariance constraints on transformation groups. The left-equivariance constraint enforces bi-left-invariance of the kernel. Every homogeneous space of G can be identified with a quotient group G/H. When Y \u2261 G = G/{e}, the symmetry constraint vanishes, allowing for the construction of equivariant maps without constraints. To construct equivariant maps without kernel constraints, functions should be lifted to the group G. Examples of Lie groups H and their actions on R d are provided, along with tools for building B-spline based G-CNNs for groups of the form G = R d H. The group structure of semidirect product groups G = R d H is derived from the action of H on R d, defining representations. The group of translations is defined by translation vectors R d with group operations. The group structure of 2D translations combined with rotations in SO(2) forms the roto-translation group SE(2) = R2 SO(2), also known as the special Euclidean motion group. The group SE(2) is derived from the group SO(2) and consists of rotation matrices and translation vectors in R2. The group structure of SE(2) is derived from SO(2) and consists of rotation matrices and translation vectors in R2. The left-regular representation of SO(2) is given by \u03b8 .x , R \u03b8 \u2212\u03b8 ). The determinant of the Jacobian of the action of H on R d is | det h| = 1. The explicit forms for the lifting and group correlations can be written using the group structure. The group convolutional setting involves working with convolution kernels on SE(2) that assign weights to locations with locally oriented features. The B-spline basis, centered around each rotation matrix in H, is computed via the scaling group. The action of H on Rd is scalar multiplication. Combining 2D translations with scaling gives a semi-direct product group. The scale-translation group R2R+ is obtained by combining 2D translations with scaling. A smiley face can be described using group elements in R2R+, representing locations and scale of features. In a group convolutional setting, convolution kernels on R2\u00d7R+ assign weights to locally scaled features, transforming via the group representation. The scaling of a group convolution kernel involves planar scaling and a logarithmic shift along the scale axis. The determinant of the Jacobian of the action of SO(3) on R3 is | det h| = 1. The roto-translation group SE(3) = R3 SO(3) combines 3D translations with rotations. The group product and inverse in the 3D rotation group are given by matrix product and matrix inverse. The action of SO(3) on R3 is matrix-vector multiplication h x = R.x. The determinant of the Jacobian of the action of SO(3) on R3 is | det h| = 1. The explicit forms for the lifting and group correlations can be written out using a parameterization with ZYZ Euler angles. The Haar measure in terms of this parameterization is given by d\u00b5(R) = sin \u03b2d\u03b1d\u03b2d\u03b3. This parameterization will be used in the construction of the quotient group SO(3)/SO(2). The 2-sphere is defined as a group quotient of SO(3) where rotations around the z-axis form the sub-group SO(2). The group SO(3) acts transitively on S 2, allowing for multiple rotations mapping the reference vector z to the same point on the sphere. This parameterization is used to construct convolution kernels on the sphere for G-CNNs. To define B-splines on the 2-sphere, a logarithmic map from a point in S 2 to the tangent vector space at the origin is needed. This map is constructed using the Log defined for SO(3), where any rotation R \u03b1,\u03b2,\u03b3 with \u03b1 = \u2212\u03b3 results in a vector in T e (SO(3)) that generates torsion-free exponential curves. The Log of S 2 maps any point in S 2 to a 2-dimensional vector space in T e (SO(3)), allowing for the definition of a B-spline on S 2. A B-spline on the 2-sphere is defined using individual spline basis functions centered around specific points. The rotation matrices on S 2 generate different rotations based on varying parameters, but still map to the same point. The B-splines are approximately isotropic, so the effect of rotation is neglected by setting a parameter to 0. The superposition of shifted splines is not isotropic by design, which is useful for using the spline as a convolution kernel to lift functions to SO(3). When using G-CNNs to generate feature maps on S 2, the kernels are constrained to be isotropic. Alternatively, gauge-equivariant networks can be used on S 2, for which the proposed splines are well-suited. The proposed splines are highly suited for networks to move from the discrete setting. Images are lifted to a space of positions and scale parameters by constructing a scale space. The discrete scale space correlation is defined with a set of scales, and a 2D correlation kernel is used for scale interaction. The curr_chunk discusses the lifting correlation kernel in a B-spline basis and its approximation using a scaled B-spline. It also highlights similarities with previous work on equivariant neural networks using left-invariant vector fields. The curr_chunk discusses the group correlation between a kernel and function in a Lie group setting, defining gauge equivariant correlation on manifolds. It involves the exponential and logarithmic maps, convolution kernels, and tangent spaces identification. The exponential map in Lie group settings allows for the identification of tangent spaces at the origin. In gauge equivariant CNNs, exponential maps are dependent on the reference frame at each point. Group correlations with B-splines involve integration over the group, while gauge correlations involve integration on tangent spaces. The G-CNN architectures used in the experiments of Sec. 4 involve sequential GCNNs with lifting layers, group correlation layers, spatial max-pooling, projection over H, and 1 \u00d7 1 convolution or fully connected layers. Two different architectures are used for metastasis classification and landmark detection tasks, as summarized in Table 1 and 2. The PCam architecture for metastasis classification involves sequential layers without spatial padding, resulting in a 64-dimensional rotation invariant feature vector. The final layers act as a neural network classifier with a softmax output. The PCam architecture for metastasis classification involves sequential layers without spatial padding, resulting in a 64-dimensional rotation invariant feature vector. The final layers act as a neural network classifier with a softmax output. The network approximates the support of kernels with a disk instead of a rectangle, using splines with basis function centers within a radius. Data augmentations include rotations, flips, and color variations. The PCam architecture for metastasis classification involves sequential layers without spatial padding, resulting in a 64-dimensional rotation invariant feature vector. The final layers act as a neural network classifier with a softmax output. The network approximates the support of kernels with a disk instead of a rectangle, using splines with basis function centers within a radius. Data augmentations include rotations, flips, and color variations. The architecture for landmark detection in the CelebA dataset is given in Tab. 2. The input is part of the group correlation kernels with different basis sizes and spatial max pooling. The architecture for landmark detection in the CelebA dataset involves using zero padding in each layer to map input images to output heatmaps. Batch normalization and ReLU activation functions are used in each layer, with the final layer using a bias vector and logistic sigmoid activation. The success rate for localizing landmarks correctly is shown in Fig. 5, computed as the average fraction of successful detections for all landmarks in all images. Landmarks are localized via the argmax on each heatmap. The CelebA dataset uses zero padding to map input images to output heatmaps for landmark detection. The success rate for localizing landmarks correctly is computed as the average fraction of successful detections for all landmarks in all images, with a detection considered successful if the distance to the actual landmark is less than 10 pixels. Global and local kernels are generated on a fixed scale range, with equidistant centers for local kernels."
}