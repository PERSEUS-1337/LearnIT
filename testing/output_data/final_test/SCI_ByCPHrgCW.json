{
    "title": "ByCPHrgCW",
    "content": "Homomorphic encryption can address privacy issues when applying deep learning to sensitive data in industries like healthcare, finance, law, and government. This paper introduces a new method for efficiently implementing deep learning functions using bootstrapped homomorphic encryption. The implementation includes Single and Multi-Layer Neural Networks for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST. Results show promise for privacy-preserving representation learning and giving users control over their data. The healthcare, finance, law, and government industries require privacy and confidentiality. AI using deep learning can enhance efficiency, but current implementations lack privacy preservation. Homomorphic Encryption (HE) offers a solution by allowing computation on encrypted information without accessing plaintext. This approach combines deep learning and HE to improve privacy for server-side models. Our privacy-preserving method for deep learning involves using homomorphic encryption to improve privacy for server-side models. Encrypted inputs are processed by a hybrid model on the server-side, producing encrypted outputs. This allows organizations to protect their deep learning solutions from being reverse-engineered while preserving client privacy. Our approach to privacy-preserving deep learning involves Fully Homomorphic Encryption (FHE) to ensure client data confidentiality while allowing server-side processing. FHE enables processing of encrypted inputs without revealing client data to the server, supporting deep models with common operations like convolution and ReLU. Advances in both deep learning and FHE have made them practical for production use, with Partially Homomorphic Encryption (PHE) enabling operations between encrypted integers. The cryptosystem from BID13 allowed both multiplication and addition on encrypted integers, making it Turingcomplete. However, bootstrapping operations were slow, taking up to 30 minutes. Leveled FHE in BID4 eliminated bootstrapping but limited sequential operations. The bootstrapping procedure in FHEW and TFHE cryptosystems allows for quick operations by packing multiple messages into a single encrypted message. FHEW BID10 reduces bootstrapping time to under 0.5 seconds using Ciphertext Packing, while TFHE BID5 further reduces it to under 0.1 seconds with additional logic gates like AND, OR, and XOR. Both systems have open-source software implementations. The FHEW implementation has incorporated improvements and gates from BID5. Mohassel & Zhang (2017) and BID20 use garbled circuits and secret sharing instead of homomorphic encryption for online operations, requiring more communication and potentially revealing the model structure. BID27 enables multiple parties to collaboratively build a model, while BID23 trains a model with secure weights. BID24 implemented old deep learning models under PHE, compromising security by computing activation functions in plaintext. Our approach focuses on designing deep learning functions for a binary FHE system using bootstrapping to support activation functions like ReLU. Unlike previous systems that compute activation functions in plaintext, our method utilizes plaintext weights more directly by exploiting the properties of binary logic gates. This allows for the implementation of state-of-the-art models with the ability to perform arbitrarily many sequential operations efficiently. Our approach focuses on designing deep learning functions for a binary FHE system using bootstrapping to support activation functions like ReLU efficiently. We support TFHE and FHEW for boolean operations on binary inputs, abstracting shared concepts such as ciphertexts, logic gates, encryption, decryption, and keys. Additionally, we modified FHEW to implement XOR, crucial for our design's efficiency. This design, named Hybrid Homomorphic Encryption (HHE), greatly improves efficiency in applying homomorphic encryption to deep learning. Hybrid Homomorphic Encryption (HHE) improves efficiency by simplifying designs for binary FHE systems. Bootstrapping is not always necessary when combining encrypted and unencrypted inputs in logic gates. Operations like NOT can be performed instantly without bootstrapping in FHEW and TFHE. In Hybrid Homomorphic Encryption (HHE), operations like NOT can be performed instantly without bootstrapping. Model weights can be stored in plaintext for deep learning inferences, allowing for a hybrid approach. Building adder and multiplier circuits can largely be done using a Hybrid Full Adder (HFA) circuit. No bootstrapping homomorphic gates are needed for this implementation. The Hybrid Full Adder (HFA) circuit allows for performing addition operations without the need for bootstrapping homomorphic gates. By using a combination of XOR and OR gates, different combinations of plaintexts and ciphertexts can be processed efficiently. The circuit minimizes the number of gates required, making it suitable for software implementation in deep learning applications. In a software implementation, the Hybrid Full Adder (HFA) circuit enables variable bit-depths without overflow. By utilizing HFAs and implicit optimizations, the circuit simplifies the implementation process. Parallelizing sections of physical adder circuits may not be beneficial for performance improvement, as minimizing bootstrapping gates is key. The BID0 matrix is used to create an efficient multiplier for two's complement numbers by generating partial products through AND operations. The Hybrid Full Adder (HFA) circuit simplifies software implementation by allowing variable bit-depths without overflow. The BID0 matrix efficiently multiplies two's complement numbers using AND operations to generate partial products. This method minimizes bootstrapping operations and leads to a straightforward software implementation, especially when multiplying plaintext and ciphertext numbers. When multiplying two's complement numbers, the presence of 0s in the plaintext bit-string reduces the need for bootstrapping operations. To optimize the process, two's complement is used when there are more 1s than 0s in the plain bit-string. This approach also minimizes bootstrapping gates for cases of -2n. The speed of the multiplication operation is influenced by the input's bit-depth and the distribution of 0s and 1s. The worst-case scenario is when the input has an equal number of 0s and 1s in the bit-strings. The properties of the hybrid Baugh-Wooley multiplier are similar to a plaintext Booth Multiplier BID2, optimized for inputs with long runs of 1s or 0s. These properties could be beneficial in deep learning models where model weights are adjusted to increase bit-string purity. The HFA optimizations in the adder and multiplier circuits could be applied to other digital circuits efficiently. Different activation functions like ReLU, Sigmoid, leaky ReLU, or hyperbolic tangent can be used in deep learning models. BID8 uses the square activation in their FHE implementation. BID8 uses the square activation in their FHE implementation, resulting in less accurate models that may fail to converge. The square activation requires multiplying two ciphertexts, leading to slower operations. ReLU can be efficiently implemented without approximation in two's complement binary. A HybridText allows for mixed bit-strings. Piecewise approximation can be used for the sigmoid activation function in the final layer. Fast approximation methods are described in Appendix A. The main operations in neural networks involve weighted sums and convolutions. Efficient matrix multipliers and sophisticated convolution implementations are important for future work. Modifications to deep learning models can reduce bootstrapping operations. Our deep learning models can be optimized for embedded systems by using lower precision inputs and weights to reduce necessary operations. Variable bit-depth support allows for optimization by allocating more bits to weights as needed. Fixed-point arithmetic can be used instead of floating point values, taking advantage of existing digital circuit designs. Separable convolutions can enable large kernels at a comparable cost to traditional convolutions. Low-precision inputs can still work well with deep learning models. The software written in C++ implements functionality to normalize encrypted inputs for deep learning models, allowing for representation of 3-7 standard deviations. The software written in C++ implements functionality to normalize encrypted inputs for deep learning models, allowing for representation of 3-7 standard deviations. In addition, an interpreter was implemented to export and read models constructed in the Keras framework BID6. Various neural networks were built and tested on different datasets, including a single-layer Perceptron and Multi-Layer Neural Network on the Wisconsin Breast Cancer dataset, and a CNN on the MNIST database of handwritten digits. The dataset contains more benign samples than malignant, so malignant losses are weighted more during training to balance the process. 60,000 instances are used for training and 10,000 for testing in the handwritten digits dataset. Appendix C provides detailed information on model structure and hyperparameters. Performance of arithmetic and deep learning operations is measured, comparing speed and accuracy trade-offs for different model variants. Models and operations are executed on a single thread of an Ivy-Bridge i5-3570 CPU. Various operations and models are measured for efficiency, with a focus on plaintext exploitation whenever possible. Average time for logic gate execution is measured across 1,000 runs, specifically for multiply and add operations. The average time for logic gate execution across 1,000 runs is measured, with operations like multiply and add being averaged from 16 runs. Normalization operation time is averaged for 1 input out of 30 inputs for a Wisconsin breast cancer instance. Model execution times do not include time spent on normalization, encryption, decryption, and NOT operations, as they have a negligible impact. TFHE library performs operations around 4.1\u00d7 faster than FHEW according to measurements. Additional execution timings are provided in Appendix D. At the time of writing, the TFHE library is significantly faster than the FHEW library, especially on systems with FMA and AVX2 support. The hybrid multiplier in the study shows significant speed improvements compared to ciphertext-only multipliers, with the 4-bit hybrid multiplier being the fastest. Interestingly, the average normalize operation is faster than a multiply due to many inputs having small standard deviations and containing plaintext 0s. The study compares the speed of different versions of the breast cancer perceptron and MLP models. The 8-bit variant is almost 4.9\u00d7 faster than the 16-bit variant for the perceptron, and there is a 3.9\u00d7 speedup for the MLP. The ReLU activation function executes so quickly that its time is negligible compared to other operations. The system can handle deep models with little overhead but is slow with a large number of parameters. The study compares the speed of different versions of breast cancer models. The 8-bit variant is faster than the 16-bit variant, with little overhead for deep models but slow with many parameters. The proposed Hybrid Homomorphic Encryption system allows for encrypted input processing in deep learning models. The system is viable for production use depending on the problem value and model size. New HE libraries are frequently updated, allowing for potential performance gains as the HE paradigm evolves. Multi-threaded code could significantly speed up models, with the ability to run duplicate models on input instances for amortized cost. Efficient matrix multiplication and larger 3-bit circuits are being explored to improve batch-processing times and accelerate existing HE libraries with GPUs. Incorporating homomorphic logic gates on GPUs or FPGAs could accelerate existing HE libraries. Plaintext weights are \"encrypted\" without noise in BID8 for improved performance. It may be possible to create a cryptosystem like FHEW that integrates HHE efficiently. The latest deep learning models use additional functionality like residual blocks BID14 and self-normalizing activation function BID16, which can be feasibly extended to the implementation. Optimization for embedded hardware and utilizing available tricks in this space present straightforward opportunities. Using homomorphic logic gates on GPUs or FPGAs could accelerate existing HE libraries. Placing plaintext weights in BID8 for better performance, a potential cryptosystem like FHEW integrating HHE efficiently. Deep learning models incorporate residual blocks BID14 and self-normalizing activation function BID16, extendable to implementation. Optimization for embedded hardware and utilizing tricks in this space offer straightforward opportunities for building a privacy-centric software industry. Lookup tables can approximate functions using constants, linear equations, polynomials, or other functions, providing versatility in approach. Using a piecewise linear approximation of sigmoid with factors of 2n, achieving an error of less than 0.02 against the original function. Algorithm 3 computes linear components for sigmoid approximation, applied to a lookup table based on thresholds. Trade-off between number of components and complexity of calculation due to unknown input number. Lookup table in Algorithm 2 selects component based on input number threshold, isolating correct component through masking and ORing components. During testing, training the perceptron with sigmoid and \"fast sigmoid\" activations for 100 epochs resulted in similar accuracies and losses. The implementation is designed to be easily configured and built, with four main components: backend, arithmetic logic, deep learning, and testing. The backend abstracts shared functionality between FHE libraries and allows for easy support of new backends. The software will be released as open-source, with features to take advantage of new or updated HE libraries over time. The software includes a \"Fake\" backend for testing, arithmetic logic with adder and multiplier circuits, deep learning operations like weighted sum and convolution, and a testing component to ensure proper execution. Three different deep learning models were created in the Keras framework. In the Keras framework, three models were created and trained using the ADAM optimizer with Nesterov momentum. The perceptron and MLP were trained for 200 epochs with a learning rate of 0.001, while the CNN was trained for 100 epochs with a learning rate of 0.002. The 2-bit hybrid multiplier showed a 9.6\u00d7 speedup compared to the 4-bit, while the 16-bit hybrid adder was 2.1\u00d7 faster than the cipher adder, and the 8-bit hybrid adder was 2.8\u00d7 faster. The perceptron may require multiple attempts to converge successfully within the first 100 epochs. The 8-bit hybrid adder is 2.8\u00d7 faster than the 16-bit adder, while the 8-bit cipher multiplier is only 3.5\u00d7 faster than the 16-bit. Lower precision numbers are handled more efficiently by the hybrid multiplier due to their form."
}