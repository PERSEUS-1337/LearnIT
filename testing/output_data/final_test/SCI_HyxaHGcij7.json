{
    "title": "HyxaHGcij7",
    "content": "Multi-view learning provides self-supervision with different views of the same data. The distributional hypothesis offers self-supervision from adjacent sentences in unlabelled corpora. Two multi-view frameworks for learning sentence representations in an unsupervised manner are presented, one using a generative objective and the other a discriminative one. The final representation is an ensemble of two views, one encoded with a Recurrent Neural Network (RNN) and the other with a linear model. The vectors produced by these frameworks offer improved representations compared to single-view counterparts. Multi-view frameworks provide enhanced representations compared to single-view methods, improving transferability on downstream tasks. Minimizing disagreement among views helps learn rich feature representations, enabling self-supervised learning when annotated data is unavailable. Distributional hypothesis suggests words in similar contexts have similar meanings, with distributional similarity reinforcing this idea. The distributional hypothesis suggests that word meaning can be determined by the company it keeps, widely used in machine learning to learn language representations without human-annotated data. Unsupervised and self-supervised learning are important due to the difficulty and cost of obtaining annotated data. Proposed algorithms aim to learn from unlabelled data using multi-view learning and distributional hypothesis ideas, inspired by the lateralisation and asymmetry in information processing of the human brain. Our proposed multi-view frameworks aim to leverage both RNN-based models and linear/log-linear models for learning sentence representations. Previous work on unsupervised sentence representation learning can be categorized into generative objective models, hindered by scalability for very large corpora. The proposed multi-view framework combines generative and discriminative objectives for learning sentence representations efficiently. The framework utilizes an RNN as the encoder and an invertible linear projection as the decoder, reducing training time significantly. Additionally, a regularisation technique enforces invertibility on the linear decoder, allowing it to be used as a linear encoder after learning. Discriminative models are also employed to classify adjacent sentences. Our proposed multi-view framework combines generative and discriminative objectives for efficient learning of sentence representations. It utilizes RNN and linear encoders to maximize agreement among adjacent sentences, encoding different aspects of input sentences. This approach encourages better representation alignment and benefits future use of the learned representations. In proposed frameworks for learning sentence representations, a generative objective and a discriminative objective are utilized. Two encoding functions, an RNN and a linear model, are learned in both frameworks. Aligning representations from two views improves performance on evaluation tasks compared to single-view trained counterparts. Models trained under these frameworks achieve good performance on unsupervised tasks and outperform existing models. Our model aims to combine RNN-based and avg-on-word-vectors sentence encoders in a multi-view framework for improved performance on both supervised and unsupervised tasks. Previous studies have shown that models performing well on supervised tasks may fail on unsupervised tasks, but with a labelled training corpus, our model excels in both. Additionally, multi-task learning shows impressive results on downstream tasks without the need for labelled data. In a multi-view framework, two different sentence encoders are introduced to compose sentence representations. One encoder is a bi-directional Gated Recurrent Unit (GRU), while the other is a linear encoder. The GRU encoder processes sentences sequentially to capture complex syntactic interactions, while the linear encoder is good at capturing the coarse meaning of a sentence. The learning framework introduced consists of an RNN encoder and a linear decoder. It focuses on predicting words in the next sentence without the need for autoregressive or RNN decoders. Negative sampling is used to calculate the likelihood of generating words in the next sentence. The learning framework introduced includes an RNN encoder and a linear decoder for predicting words in the next sentence without autoregressive or RNN decoders. Negative sampling is used to calculate the likelihood of generating words in the next sentence, with a focus on word vectors and regularisation techniques for invertibility. The learning framework introduced includes an RNN encoder and a linear decoder for predicting words in the next sentence without autoregressive or RNN decoders. The update rule for regularisation involves setting \u03b2 to 0.01. The framework reuses the decoding function for building sentence representations, utilizing information encoded in the decoder. A multi-view framework with discriminative objective aims to maximize agreement between representations of sentence pairs. An RNN encoder and a linear avg-on-word-vectors produce a vector representation with a trainable temperature term \u03c4 for exaggerating differences between sentences. The choice of cosine similarity based loss in the learning framework is influenced by the correlation between vector length and word frequency. The model is unsupervised/self-supervised, learning the importance of similarity between neighboring sentences for meaning. A postprocessing step removes the top principal component of representations, followed by l2 normalization. In the multi-view framework with discriminative objective, the top principal component is estimated using the power iteration method to reduce training and testing discrepancy. In the learning framework, the top principal component is estimated using the power iteration method to reduce training and testing discrepancy. Three unlabelled corpora from different genres are used in experiments, with models trained separately on each corpus. Adam optimizer and gradient clipping are applied for stable training. Pretrained word vectors are used and fixed during learning. Summary statistics of the corpora can be found in a table. Representation pooling methods in the testing phase are also discussed. The experiments are conducted in PyTorch using modified SentEval package. Hyperparameters are tuned based on STS14 performance. Batch size and dimension are kept the same for fair comparison. Representation of sentence input is calculated as z = \u1e91 f + \u1e91 g /2, where \u1e91 is the post-processed and normalized vector. Models are compared with unsupervised, semi-supervised, and supervised learning methods, with results presented in TAB1. The results of six models trained with learning frameworks outperform other methods, with the model trained on the UMBC News Corpus showing the best performance. Evaluation involves learning a linear model on top of sentence representations. Concatenating various representations creates a richer feature vector for the linear model to explore. The linear model explores different aspects of encoder functions by forming a richer feature vector. The representation z f is calculated by pooling hidden states, while z g is calculated with three pooling functions. These representations are concatenated to form a final sentence representation for various tasks such as semantic relatedness, paraphrase detection, and sentiment analysis. The results of classification and opinion polarity are presented in TAB3, comparing various supervised and unsupervised models. Models trained on BookCorpus show promising results, with the Amazon Book Review model performing best on sentiment analysis tasks. RNN and linear encoders perform well across tasks, while orthonormal regularization in the linear decoder enforces invertibility in the multi-view framework. The multi-view framework enforces invertibility to align vector representations produced by different functions. Training under this framework improves performance, with an ensemble of both views providing stronger results. Results with and without the invertible constraint are compared in Table 6. The ensemble method in the multi-view framework aligns representations from different functions by applying an invertible constraint. This improves performance on unsupervised tasks, while on supervised tasks, the linear classifier can pick relevant features from both views for good predictions. There is no significant difference between the framework with and without the invertible constraint. The ensemble method in the multi-view framework aligns representations from different functions by applying an invertible constraint, improving performance on unsupervised tasks. Comparing the framework with discriminative objective to other variants, including multi-view and single-view models, shows that linear/log-linear models produce better representations for unsupervised tasks. Additionally, multi-view learning with both functions (f and g) improves performance on unsupervised tasks and supervised evaluation tasks. In multi-view frameworks, aligning representations from different encoding functions (f and g) improves performance on both supervised and unsupervised tasks. Simply averaging representations without alignment leads to poor performance. The ensemble of two views provides better results compared to using a single encoding function. Our multi-view framework with f and g improves performance on unsupervised tasks and shows similar results on supervised tasks compared to ensembles of single-view models. The model also demonstrates higher training efficiency and better matching between encoding functions. The proposed sentence representation learning frameworks combine RNN-based and linear encoders, can be trained efficiently on large unlabelled corpora, and show generalisation ability and transferability in experiments on three corpora. The experimental results support the finding that linear/log-linear models work better on unsupervised tasks, while RNN-based models perform better on supervised tasks. Multi-view learning aligns different models to produce better representations, and the ensemble of both views provides rich semantic information. Future work should explore various encoding architectures and learning under the multi-view framework. The experimental results support the finding that linear/log-linear models work better on unsupervised tasks, while RNN-based models perform better on supervised tasks. Multi-view learning aligns different models to produce better representations, and the ensemble of both views provides rich semantic information. The Power Iteration algorithm is used to estimate the top principal component from the representations produced from f and g separately. The Power Iteration method is used to estimate the top eigenvector of the covariance matrix from representations produced by f or g. The algorithm involves tuning hyperparameters like batch size, encoder dimension, context window, and number of negative samples. Experimental results are based on N=512, d=1024, with a context window of c=3 and K=5 negative samples. The model requires 8GB on a GTX 1080Ti GPU and uses an initial learning rate of 5 x 10^-4. The model uses K=5 negative samples and requires 8GB on a GTX 1080Ti GPU. The initial learning rate is 5 x 10^-4 and various weights and biases are initialized before training. Word vectors are fixed from FastText, with out-of-vocabulary words set to 0 vectors. The temperature term is tuned during training and used to convert agreement to a probability distribution. Another model with fixed temperature performed similarly in experiments. The post-processing step 'WR' improves model performance on unsupervised tasks and supervised sentence similarity tasks, but has minimal impact on single sentence classification tasks. Models with both generative and discriminative objectives are trained to combine RNN encoder, linear decoder, and linear encoder for further improvement. Combining generative and discriminative objectives does not show further improvement compared to models with only one objective. The number of parameters for each selected model are: 1. Ours: \u2248 8.8M 2. Quick-thought BID33: \u2248 19.8M 3. Skip-thought BID28: \u2248 57.7M."
}