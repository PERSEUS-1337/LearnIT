{
    "title": "SkxV7kHKvr",
    "content": "Graph Neural Networks combine Graph Signal Processing and Deep Convolutional Networks for pattern recognition in non-Euclidean domains. A new method is proposed to deploy two pipelines based on the duality of a graph to improve accuracy by exploring primal and dual graphs. This framework shows potential in both semi-supervised and unsupervised learning, unlike Convolutional Neural Networks which are more suited for Euclidean domains like computer vision tasks. Research on Geometric Deep Learning has gained attention for applying Convolutional Neural Networks to non-Euclidean domains. Methods like ChebNet and GCN have shown success in semi-supervised learning by leveraging inner connections between objects. Convolution operations in this field include spatial and spectral domains, with the latter involving graph Fourier transform. The spectral one further converts signals using graph Fourier transform into the spectral domain. Previous research has overlooked the interchangeable nature between nodes and edges, limiting the full utilization of the graph's duality. A new approach is proposed to transform the primal graph into its dual form, allowing for the exploitation of edge features. This method combines vertex and edge features to solve a wider range of problems and improve performance. The text discusses the development of two pipelines based on different graph forms to enhance accuracy and performance. A new framework is introduced for semi-supervised and unsupervised learning. Graph-based semi-supervised learning involves annotating data with limited label data on a graph using graph Laplacian regularizer and sample-based methods. Graph Convolutional Networks extend convolution operations to graph data. Graph Convolutional Networks generalize convolution from grid data to graph data. ChebNet approximates filters using Chebyshev polynomials based on Laplacian eigendecomposition. GCN simplifies ChebNet by introducing a first-order approximation for spatial-based perspective. MoNet defines convolution as a Gaussian mixture of candidates. GAT applies attention mechanism to graph networks. DGI proposes unsupervised representations learning on graph-structured data by maximizing mutual information. Our study focuses on classifying vertices based on the relationship between them (edges) and regularizing mutual information between classification on vertices and edges, following the path of previous dual approaches on graph networks. The curr_chunk discusses the preliminaries of graph theory, including the definition of a graph, adjacency matrix, Laplacian matrix, and eigendecomposition. It also mentions the use of the random walk Laplacian for directed graphs. The curr_chunk explains the use of graph Fourier transform and graph convolution with Chebyshev polynomials for computational efficiency in graph filters. ChebNet introduces Chebyshev polynomials into convolutional layers to approximate graph convolution with polynomial filters. The graph filter is defined using the scaled normalized Laplacian matrix and trainable parameters. The curr_chunk discusses the dual graph concept in graph theory, where a primal graph G is transformed into a dual graph \u011c with vertices for each edge of G. The dual graph has edges connecting vertices where two edges of G share a common vertex. The conversion from a primal graph to its dual counterpart involves considering vertices (i, j) and (j, i) as the same in the dual graph. Features of dual nodes are obtained by applying functions to corresponding primal nodes' features, such as calculating the distance between node features. In Twin Graph Convolutional Networks (TwinGCN), features of dual nodes are inherited from primal graph edges. The Twin Graph Convolutional Networks (TwinGCN) consist of two pipelines with the same architecture as GCN. The lower pipeline uses dual features derived from primal features to make predictions in the dual vertex domain, which are then aggregated back to the primal vertex domain. This dual pipeline allows for utilizing predictions on edges to affect predictions on nodes, propagating knowledge through edges. Training the dual pipeline requires labels for dual nodes. The Twin Graph Convolutional Networks (TwinGCN) utilize dual features from primal nodes to make predictions in the dual vertex domain. The convolution layers in the pipelines perform graph convolution operations with shared weights as learnable parameters. The loss function for semi-supervised node classification is defined based on node labels and predicted outcomes. The dual pipeline affects predictions on nodes by utilizing predictions on edges. The Twin Graph Convolutional Networks (TwinGCN) use dual features from primal nodes for predictions in the dual vertex domain. Kullback-Leibler Divergence is employed as a regularization term to incorporate the dual pipeline's impact on primal pipeline predictions. Joint probability matrices are calculated, and Kullback-Leibler Divergence is evaluated to derive primal predictions efficiently. Special incidence matrices are introduced to facilitate this process. The incidence matrices in TwinGCN use sparse matrix multiplication on GPUs for faster computation. The focus is on semi-supervised node classification, with plans to support unsupervised learning in the future. Experiments are conducted on three benchmark datasets. The study conducts experiments on three benchmark datasets: Cora, Citeseer, and Pubmed, collected from citation networks. The training process uses 20 labeled samples for each class. The proposed architecture utilizes graph convolution based on spectral graph theory, similar to ChebNet, GCN, and GWNN models. Unlike some methods, these models maintain the same graph Laplacian base structure. The FastGCN method applies Monte Carlo importance sampling on edges, ensuring convergence as sample size increases. The primal pipeline consists of two graph convolution layers with 16 hidden units and ReLU activations, evaluated with softmax loss. Dropout is set to p = 0.5, and Adam optimizer with lr = 0.01 is used. The dual graph is larger with quadratically more nodes and edges than the primal graph. To prevent overfitting on the dual pipeline, a higher dropout rate of over 70% is set. A sampling rate is introduced to extract a small fraction of dual node labels. The performance of TwinGCN, with two pipelines, outperforms GCN in node classification by aggregating knowledge propagated through edges. TwinGCN converges slower but achieves higher accuracy as epochs increase due to the dual pipeline. In this work, TwinGCN with parallel pipelines on primal and dual graphs achieves higher accuracy in node classification compared to GCN. The loss curve of TwinGCN is slightly above GCN due to the summation of primal and dual pipelines. Controlled experiments show TwinGCN outperforming GCN and GCN with double pipelines in various datasets. TwinGCN, with pipelines on primal and dual graphs, outperforms GCN in node classification accuracy. It can also be extended to unsupervised learning by modifying loss functions. Use unnumbered third level headings for acknowledgments."
}