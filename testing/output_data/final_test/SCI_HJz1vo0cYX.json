{
    "title": "HJz1vo0cYX",
    "content": "We propose a framework to calibrate prediction accuracy and confidence in deep neural networks through stochastic inferences. By analyzing the relation between model parameters and prediction scores, we design a variance-weighted confidence-integrated loss function. This function allows for learning networks that predict confidence-calibrated scores using a single inference, resulting in outstanding confidence calibration. Our algorithm improves classification accuracy and confidence calibration in deep neural networks by incorporating stochastic depth and dropout techniques. It addresses the issue of overconfidence in predictions by training networks to achieve proportional accuracy to confidence levels. This is crucial for applications like autonomous driving and medical diagnosis that require accurate uncertainty estimation. Our algorithm aims to improve classification accuracy and confidence calibration in deep neural networks by incorporating stochastic depth and dropout techniques. It addresses the issue of overconfidence in predictions by training networks to achieve proportional accuracy to confidence levels, crucial for applications like autonomous driving and medical diagnosis. The goal is to learn deep neural networks that can estimate accuracy and uncertainty of each prediction simultaneously, proposing a generic framework to calibrate prediction score with accuracy in deep neural networks. Our algorithm focuses on improving classification accuracy and confidence calibration in deep neural networks by utilizing stochastic regularization techniques like stochastic depth and dropout. It aims to estimate uncertainty of predictions based on stochastic inferences, enabling networks to predict confidence-calibrated scores with just a single prediction. This approach addresses the issue of overconfidence in predictions and establishes a relation between stochastic modeling and inferences in deep neural networks. The paper introduces a variance-weighted confidence-integrated loss function for deep neural networks to improve confidence calibration without hyper-parameters. It addresses the overconfidence issue and accurately estimates uncertainty in various architectures and datasets. The following sections discuss prior research, theoretical background, a confidence calibration algorithm, and experimental results in Bayesian interpretation. Neural networks are gaining attention in the machine learning community. Bayesian approach provides a mathematical framework for uncertainty estimation in deep neural networks. Approximate inference techniques like MCMC, Laplace approximation, variational inference, and deep ensembles are used due to the high computational cost of exact Bayesian inference. Post-processing methods require a hold-out validation set, and ensemble-based techniques use multiple models to estimate uncertainty. Stochastic regularization, such as randomly dropping hidden units, is a common technique to improve generalization performance in deep neural networks. Stochastic regularization methods like dropping weights or skipping layers are commonly used to improve generalization in deep neural networks. While most methods use stochastic inferences during training and deterministic inferences during testing, there is a need for diverse and reliable outputs during testing as well. Techniques such as label smoothing and penalizing entropy have been explored to encourage models to be less confident and improve calibration. BID15 suggests that blind label smoothing and penalizing entropy can enhance accuracy by integrating loss functions with the same concept as BID19, but the improvement is marginal in practice. This section discusses the Bayesian interpretation of stochastic regularization in deep neural networks and its relation to uncertainty modeling. Deep neural networks are prone to overfitting due to their large number of parameters, and various regularization techniques such as weight decay, dropout (BID18), and batch normalization (BID7) have been used to address this issue. Stochastic regularization, which introduces random noise to a network, is a popular class of techniques, with a focus on multiplicative binary noise injection. Stochastic regularization techniques, such as weight perturbation and dropout-binary noise injection, are widely used in deep neural networks to prevent overfitting. These methods introduce random noise to the network during training, which can be interpreted as weight perturbation. The objective of a classification network trained with stochastic regularization is defined by minimizing the cross entropy loss. At inference time, the network is parameterized by the expectation of the perturbed parameters. The Bayesian objective is to estimate the posterior distribution of the model parameter to predict a label for an input. Variational approximation minimizes Kullback-Leibler divergence with the true posterior. Monte Carlo method and mini-batch optimization are used to approximate the intractable integral and summation over the dataset. The text discusses the optimization of a loss function with 2 regularization using stochastic gradient descent. It involves approximating the posterior distribution as a Gaussian mixture and estimating predictive mean and uncertainty through Monte Carlo approximation. Stochastic inferences like dropout are realized through binary noise samples. Equation 8 allows for computing average prediction and variance directly from multiple stochastic inferences. A novel confidence calibration technique for deep neural network predictions is introduced, utilizing a variance-weighted confidence-integrated loss function. The relationship between variance of stochastic inferences, prediction accuracy, and confidence is highlighted, along with an end-to-end training framework for confidence self-calibration. Equation FORMULA9 indicates that model variation is linked to variance in stochastic predictions. The text discusses how variance of multiple stochastic inferences can estimate accuracy and uncertainty of predictions. Results from CIFAR-100 with ResNet-34 and VGGNet show a strong correlation between predicted variance and reliability-accuracy. It suggests disregarding examples based on prediction variances. The text discusses designing a loss function for accuracy-score calibration by integrating confidence into the standard cross-entropy loss term. The main idea is to regularize uncertain examples' score distributions with the uniform distribution while keeping confident examples' distributions intact. The proposed loss function integrates confidence into the standard cross-entropy loss term to regularize uncertain examples' score distributions. It introduces a global hyper-parameter \u03b2 to control the impact of the confidence-integrated loss term. Additionally, a more sophisticated confidence loss term is proposed by leveraging the variance of multiple stochastic inferences. The proposed framework introduces a variance-weighted confidence-integrated loss for training accuracy-score calibration networks. This approach aims to provide confidence-calibrated predictions by combining two complementary loss terms and balancing them with the variance measured through multiple stochastic inferences. The proposed framework introduces a variance-weighted confidence-integrated loss for training accuracy-score calibration networks. This loss combines two terms that push the network in opposite directions, encouraging high scores for ground truth labels and predicting the uniform distribution. The balancing coefficient \u03b1 i is the normalized variance of individual examples obtained through multiple stochastic inferences, measuring model uncertainty. The proposed framework introduces a variance-weighted confidence-integrated loss for training accuracy-score calibration networks. The balancing coefficient \u03b1 i measures model uncertainty for each training example, optimizing the loss function to force predictions towards a uniform distribution for high uncertainty examples. Prediction confidence is intensified for examples with low variance. Prediction scores are well-calibrated, representing confidence, and can be relied upon. Score calibration techniques adjust confidence scores post-training, with BID3 proposing a method using a global temperature \u03c4 to scale logits before applying softmax. The proposed framework introduces a variance-weighted confidence-integrated loss for training accuracy-score calibration networks. It involves scaling logits using a global temperature \u03c4 before applying the softmax function. This technique maximizes entropy of the output distribution and minimizes KL-divergence. The framework is closely related to temperature scaling in BID3 and utilizes deep neural network architectures like ResNet and VGGNet. ResNet employs residual connections in its architecture, while VGGNet uses dropout before every fully connected layer except for the classification layer. The proposed framework utilizes residual connections for feature representation and stochastic depth for regularization in ResNet. It interprets stochastic depth and dropout as noise injection into network weights. The framework is evaluated on Tiny ImageNet and CIFAR-100 benchmarks, with images resized to 32 \u00d7 32 for consistency. For two benchmarks, images in Tiny ImageNet are resized to 32 \u00d7 32. Networks are trained using stochastic gradient descent with a momentum of 0.9 for 300 epochs. The initial learning rate is set to 0.1 and exponentially decayed at specific epochs. Batch sizes are 64 for ResNet and 256 for VGG. The proposed variance-weighted confidence-integrated loss is trained using T samples per input image, with T set to 5. The normalized variance is estimated based on Bhattacharyya coefficients. Trained models with VWCI loss are compared to models with CI losses using different constant \u03b2's. Classification accuracy is measured. The study evaluates classification accuracy and expected calibration error (ECE) of models trained with confidence-integrated loss. ECE measures the discrepancy between confidence and accuracy, with results presented for ResNet-34 and VGG-16 on two datasets. Baseline methods with stochastic inferences reduce calibration error, with a more significant reduction seen with a higher number of samples. The study shows that models trained with VWCI loss outperform baselines and are competitive with CI loss in terms of classification accuracy and confidence calibration. Stochastic inference and variance-driven weight help measure uncertainty for each instance and balance opposing terms. The confidence loss term regularizes the network by enforcing predictions to a uniform distribution, leading to accuracy gain and confidence calibration. The CI loss with fixed coefficient \u03b2 impacts prediction confidence, with proper \u03b2 improving performance but sensitivity to its choice. No single \u03b2 is globally optimal across architectures and datasets, with examples showing varying accuracy. CI loss works well on CIFAR-100 due to high accuracy and minimal harm from overconfident deep neural networks. The VWCI loss with variance-driven weights maintains high accuracy and confidence in decision-making systems. It outperforms CI on Tiny ImageNet due to its challenging nature. VWCI provides better coverage of test examples compared to CI by adjusting confidence levels based on individual instances. The VWCI loss with variance-driven weights outperforms CI on Tiny ImageNet by adjusting confidence levels based on individual instances, leading to better coverage of test examples. Using predictive uncertainty for balancing terms is preferable over setting a constant coefficient in the loss function, as it balances accuracy and score based on uncertainty. The framework for uncertainty estimation in deep neural networks involves calibrating accuracy and score based on stochastic inferences, with variation of multiple stochastic inferences for a single example being crucial for estimating uncertainty. The proposed variance-weighted confidence-integrated loss aims to improve confidence calibration in networks by enabling uncertainty estimation through a single prediction. It also provides a unified understanding of existing calibration methods and compares different variations within the framework for analysis."
}