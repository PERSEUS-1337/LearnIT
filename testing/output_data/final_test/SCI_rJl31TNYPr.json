{
    "title": "rJl31TNYPr",
    "content": "Recent work in adversarial machine learning has focused on visual perception in autonomous driving, specifically studying Adversarial Examples (AEs) for object detection models. However, the process of Multiple Object Tracking (MOT) in the visual perception pipeline presents a challenge to existing attack techniques targeting object detection. A success rate of over 98% is required for attacks to affect tracking results, a threshold that current techniques cannot meet. This paper introduces a novel attack technique called tracker hijacking, which effectively fools MOT using AEs on object detection. The paper introduces a novel attack technique called tracker hijacking, using Adversarial Examples (AEs) on object detection to fool Multiple Object Tracking (MOT) in autonomous driving. Successful AEs on just one frame can manipulate objects to cause safety hazards for autonomous vehicles. Evaluation on the Berkeley Deep Drive dataset shows a nearly 100% success rate with attacks on 3 frames, compared to up to 25% with blind object detection attacks. Building upon prior work on adversarial examples in object detection, Zhao et al. improved the robustness of adversarial sticker attacks in outdoor settings, achieving a 72% success rate with a car running at 30 km/h. This research highlights the importance of tracking detected objects in autonomous driving systems. In Multiple Object Tracking (MOT) for autonomous driving, trackers are used to build moving trajectories of obstacles, which are crucial for driving decision making to avoid collisions. MOT requires high tracking accuracy and stability in object detection results to influence driving decisions. This poses a challenge to attack techniques targeting object detection in autonomous driving systems. In autonomous driving, attacking object detection is challenging due to the need for high success rates over consecutive frames to fool the tracking process. Existing attacks have not achieved the required success rate. This study introduces a novel attack technique called tracker hijacking, focusing on both object detection and tracking to deceive the Multiple Object Tracking (MOT) process. The study introduces a novel attack technique called tracker hijacking in autonomous driving to deceive the Multiple Object Tracking (MOT) process. Attackers can exploit the tracking error reduction process in MOT to alter tracking results, potentially causing safety hazards for autonomous vehicles. The attack can succeed with successful Adversarial Examples (AEs) on as few as one single frame. This paper introduces a novel attack technique called tracker hijacking in autonomous driving to deceive the Multiple Object Tracking (MOT) process. The attack can succeed with successful Adversarial Examples (AEs) on as few as one frame, and 2 to 3 consecutive frames on average. When attacking 3 consecutive frames, the success rate is nearly 100%, compared to attacks blindly targeting object detection with only up to 25% success rate. The paper also highlights the importance of considering MOT in adversarial machine learning attacks in autonomous driving. The novel attack technique, tracker hijacking, can deceive the Multiple Object Tracking (MOT) process in autonomous driving by using Adversarial Examples (AEs) on as few as one frame. The attack has a high success rate, nearly 100%, when targeting 3 consecutive frames, compared to attacks focusing solely on object detection. The code and evaluation data are available on GitHub. Several works in adversarial machine learning have focused on visual perception tasks in autonomous driving, particularly object detection models. The key challenge is designing robust attacks that can withstand real-world driving scenarios like different viewing angles, lighting conditions, and camera limitations. For example, Lu et al. found that adversarial examples against Faster-RCNN generalize well in digital space but fail in the physical world. Eykholt et al. created adversarial stickers that can deceive YOLOv2 object detector when attached to a stop sign. In recent advancements in adversarial machine learning, physical adversarial attacks against object detectors have shown success rates of 70% in road tests at fixed speeds. However, it is argued that existing attacks still cannot affect visual perception results without considering object tracking, specifically Multiple Object Tracking (MOT). In recent advancements in adversarial machine learning, physical adversarial attacks against object detectors have shown success rates of 70% in road tests at fixed speeds. However, even with a high attack success rate, it is still challenging to affect visual perception results without considering object tracking, particularly Multiple Object Tracking (MOT). MOT aims to identify objects and their trajectories in video frame sequences, with tracking-by-detection being the dominant paradigm. This paradigm is widely used in autonomous driving systems today. Object tracking in video frame sequences involves associating detected objects with dynamic state models using a per-track Kalman filter. This filter operates in a predict-update loop to estimate object states based on motion models and detection results. The association problem is solved through bipartite matching based on similarity costs, with spatial-based cost being a commonly used metric. The spatial-based cost metric measures overlapping between bounding boxes in object tracking. An accurate velocity estimation is crucial in Kalman filter prediction to reduce errors in association. MOT manages tracker creation and deletion with specific thresholds based on constant object detection. The hit count threshold, referred to as H, filters out false positives in object detection. A tracker is deleted if no object is associated with it for a reserved age of R frames. The values of R and H depend on detection accuracy and frame rate, with a suggested configuration of R = 60 frames and H = 6 frames for a 30 fps system. This prevents attacks targeting object detection by requiring constant deception for at least 60 frames, while a proposed tracker hijacking attack can create fake objects that last for R frames. The proposed tracker hijacking attack can fabricate objects that last for R frames and vanish target objects for H frames in the tracking result by attacking as few as one frame, and only 2~3 frames on average. This work focuses on the track-by-detection pipeline in MOT, using the IoU-based Hungarian matching algorithm as the target. The proposed tracker hijacking attack can manipulate object detection results by applying adversarial patches, causing the tracker to deviate from the target object in multiple frames. This attack was validated using the IoU-based Hungarian matching algorithm in the track-by-detection pipeline of MOT. The proposed tracker hijacking attack aims to deceive object detectors by erasing the target object's bounding box and creating a fake bounding box shifted towards a specified direction. This manipulation, known as tracker hijacking, can have lasting effects on the tracker's velocity and persistence in subsequent frames. The attack involves delaying the deletion of velocity until a reserved age has passed and not tracking the target object until a hit count has been reached. The success rate of the attack is nearly 100% when 3 consecutive frames are attacked. This can lead to severe safety consequences in self-driving scenarios, potentially causing emergency stops or rear-end crashes. Adversarial patches can be used to deceive autonomous vehicles by causing target objects to move in or out of their intended path. This can lead to emergency stops or rear-end crashes, with the potential for severe safety consequences in self-driving scenarios. Adversarial patches can deceive autonomous vehicles by manipulating target objects' movements, potentially causing collisions. The attack targets the first-order Kalman filter and uses Intersection over Union for data association. The methodology involves using Intersection over Union for data association and the Hungarian matching algorithm to associate bounding boxes in consecutive frames. Adversarial patches are generated to manipulate detection results and hijack a tracker in targeted video sequences. The algorithm finds an optimal position to place an adversarial bounding box to hijack the tracker of a target object. Adversarial frames are constructed to redirect the tracker using carefully chosen positions. The tracker is updated with the adversarial frame to deviate it from its original direction, and the attack is successful if the target object is not associated with its original tracker in the next frame. The algorithm aims to find the optimal position for an adversarial bounding box to disrupt the tracker of a target object. By shifting the bounding box in a specified direction, the attack maximizes the cost of Hungarian matching to maintain association with the original tracker while introducing an adversarial velocity. This process facilitates the generation of adversarial examples. Generating adversarial patches against object detection involves formulating the attack as an optimization problem. Existing attacks focus on minimizing the probability of the target class to erase the object from detection results, but this approach is ineffective in fooling multiple object trackers. A new tracker hijacking attack incorporates two optimization objectives to erase the bounding box of the target object. The tracker hijacking attack aims to minimize the target class probability to erase the bounding box of the target object and fabricate an adversarial bounding box at the attacker-desired location. The experiment methodology evaluates the effectiveness of the attack by measuring the minimum number of frames for the attack to succeed. In normal measurement noise covariance range, the tracker hijacking attack can successfully deviate the target tracker by fooling only 2~3 consecutive frames on average. The attack achieves a superior success rate compared to previous attacks against object detectors, even by fooling as few as 3 frames. The attack effectiveness depends on the difference between the direction vectors of the original tracker and the adversary's objective. The attacker can shift the tracker significantly with just one frame by choosing the opposite direction, but it is harder if the direction is the same as the target's. The number of frames needed for the attack was measured in two scenarios: target object move-in and move-out. 100 video clips were randomly sampled from the Berkeley Deep Drive dataset for the study. For the study, 10 video clips were selected for object move-in and another 10 for object move-out scenarios. Each clip was manually labeled with a target vehicle and annotated at its back. The visual perception pipeline was implemented using Python with YOLOv3 for object detection and a Kalman filter based on OpenCV for data association. The effectiveness of the attack depends on the Kalman filter's measurement noise covariance parameter. The measurement noise covariance parameter in the Kalman filter plays a crucial role in the effectiveness of the tracker hijacking attack. It is often tuned based on the performance of detection models, with configurations ranging from very small to very large values. The average number of frames needed for a successful track hijacking is evaluated under different covariance configurations. The recommended configuration for tracker hijacking attack involves R = 60 and H = 6, but real-world deployments often use smaller values. The attack can succeed with successful AEs on object detection in 2 to 3 consecutive frames. Even with a successful AE on one frame, the attack has 50% and 30% success rates with cov of 0.1 and 0.01 respectively. Object move-in attacks require fewer frames compared to move-out attacks due to differences in velocities between the attacker and the autonomous vehicle. The tracker hijacking attack achieves a superior success rate compared to detection attacks, even with as few as 3 frames. The attack aims to erase the target class from the detection result of each frame, making hijacking easier. The detection attack needs an adversarial patch to fool at least 60 consecutive frames, achieving over 98.3% success rate. The attack can still have up to 25% success rate before reaching 60 frames. In adversarial machine learning research targeting visual perception in autonomous driving, the success rate in MOT is significantly higher compared to object detection attacks. Future research should consider using MOT accuracy as an evaluation metric and studying weaknesses specific to MOT or interactions between MOT and object detection. This paper focuses on studying weaknesses specific to Multiple Object Tracking (MOT) and interactions between MOT and object detection. The research marks the first effort in these directions, aiming to improve practicality and generality. The evaluation is currently done digitally with video frames, but the proposed method can also be applied to generate physical patches. The approach of finding the location for adversarial bounding boxes is generally applicable to different association mechanisms. In this work, the researchers focus on studying weaknesses in Multiple Object Tracking (MOT) and its interaction with object detection. They introduce a novel attack technique called tracker hijacking, which exploits tracking errors in MOT to manipulate the movement of objects in the path of autonomous vehicles. The evaluation results demonstrate that their attack can successfully manipulate objects with just three frames, posing potential safety risks. The proposed adversarial bounding box generation algorithm against YOLOv3 can be adapted for other object detection models, paving the way for future research in adversarial machine learning in self-driving scenarios. The results show that attacks targeting Multiple Object Tracking (MOT) can have a nearly 100% success rate with just 3 frames, compared to attacks on object detection which only have up to 25% success rate. The source code and data are available on Github. This research suggests that MOT should be considered in adversarial machine learning for autonomous driving. The track hijacking attack iteratively finds frames to perturb for a successful hijack, generating adversarial patches. The FINDPOS algorithm aims to find the optimal position to place an adversarial bounding box in order to hijack the tracker of a target object. It iteratively moves the bounding box along a desired directional vector while maintaining certain invariants, such as association with the original tracker and overlap with the patch. The loop ends when the bounding box is shifted to the farthest position while still meeting the invariants. The algorithm FINDPOS finds the optimal position for an adversarial bounding box to hijack a target object's tracker. Once the target bounding box location is identified, an adversarial patch is generated against the object detection model using an optimization problem. The adversarial patch generation is approached as an optimization problem. Existing attacks focus on minimizing the target class probability to erase the target from detection results, but these are ineffective in fooling MOT. Our tracker hijacking attack incorporates two loss terms: one to erase the target bounding box and another to fabricate an adversarial bounding box to hijack the tracker. The algorithm FINDPOS identifies the optimal position for the adversarial bounding box. Our tracker hijacking attack uses the Adam optimizer to generate an adversarial patch that satisfies the requirements. The attack focuses on erasing the target bounding box and fabricating an adversarial bounding box to hijack the tracker. The code for the implementation can be found on Github. The algorithm formulates adversarial patch generation as an optimization problem, using the Adam optimizer to minimize loss L1 + \u03bbL2. L1 minimizes target class probability in the vanish area, while L2 controls fabrication of adversarial bounding box to hijack the tracker. Fabrication loss L2 is used only for the first adversarial frame in a sequence to give the tracker an attacker-desired velocity. The Kalman filter is used in the track-by-detection pipeline to improve estimation accuracy by combining a noise model. It helps correct errors in detection results and is implemented with a focus on erasing target bounding boxes. The attack stops after reaching the maximum iteration, returning the adversarial example with a patch applied. The implementation is available on Github. In the track-by-detection pipeline, the Kalman filter is used for state estimation by combining current measurements with previous estimations. The Kalman gain is updated by measurements, and first-order filters track bounding box properties. The tracker states are updated with time and measurement steps using state transition models. The Kalman filter is used for state estimation in the track-by-detection pipeline. The measurement update is performed using observation models and covariance values. The Kalman gain is related to variations in measurement noise and is updated by measurements. The Kalman gain is proportional to the covariance value, impacting object tracking response. A small cov value results in faster tracking, closely following detection boxes. A large cov value makes the Kalman filter rely more on its own estimation, reducing tracker responsiveness. Empirical validation showed that with cov values between 0.01 to 10, a track hijacking attack can achieve nearly 100% success rate by fooling 3 consecutive detection frames on average."
}