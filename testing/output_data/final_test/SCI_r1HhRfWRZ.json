{
    "title": "r1HhRfWRZ",
    "content": "We show that models trained to predict proprioceptive information about the agent's body come to represent objects in the external world. The dynamic body models learn holistic persistent representations of objects in the world through predicting their effects on the agent's body. The model successfully predicts sensor readings over 100 steps into the future and continues to represent object shapes even when the body is no longer in contact with them. Active data collection is done by maximizing the entropy of predictions about the body. Active data collection by maximizing the entropy of predictions about the body leads to learning dynamic models that show superior performance for control. The same models can be used to answer questions about properties of objects in the real world. Situation awareness involves perceiving elements in the environment and understanding their meaning and future status. As artificial intelligence expands into the virtual and real world, we are increasingly faced with various applications such as simulated creatures, virtual assistants, self-driving vehicles, and household robots. When building systems like virtual assistants, self-driving vehicles, and household robots, designers face the challenge of understanding and reasoning about the world around them. The physical world is divided into the platform, which is designed and known well, and everything else, which presents unpredictable situations. Designers have limited control over the external world, making it difficult to anticipate all possible scenarios. While the platform's state is easily accessible through sensors, the external world's state is not readily available to the system. The platform for systems like virtual assistants and self-driving cars consists of sensors and actuators that are consistent across different situations. This same partitioning of the world occurs naturally for living creatures as well, where the body serves as the platform. The hand neural network sensory model has learned persistent representations of the external world, enabling it to predict object shapes even without contact. This suggests an approach to building models for reasoning about the world based on the body as a consistent vehicle for interaction. In this paper, the authors use robotic hands in simulation and reality to build predictive models of proprioceptive signals when grasping target objects. The models can accurately predict the effects of external objects on the body and can be used for planning behaviors dependent on these objects. The study uses a simulated hand model with sensors and motors to predict sensor measurements, and a real robotic hand with sensors to validate the simulation results. The study involves a real robotic hand with sensors measuring tension and pressure on fingertips to make predictions about external objects. The research is related to intrinsic motivation and exploration, focusing on actively seeking information about the world. The notion of choosing actions to improve a model of the world, echoing earlier speculative work. Intrinsic motivation and curiosity-based objectives have been implemented in visual space through predicting interactions with objects or future summary statistics. Some works have used learned future predictions for control. Intrinsic motivation is often framed as learning to induce errors in a forward model, possibly with an additional inverse model. However, using planning for active control prevents direct adaptation of these objectives. Working with similar but different objectives in the planner is necessary due to the inability to roll forward objectives dependent on observed prediction errors. Our approach in exploring environments involves using simulation with deterministic distributed representations. We draw from a vast literature on haptics and touch sensing, as well as robotics work on anticipation of sensation and touch sensing for grasping tasks. Model-based planning is also a key aspect of our methodology. In this paper, the use of touch sensing to enhance grasping tasks is discussed, along with the success of model-based planning in related domains. The focus is on sequence-to-sequence modelling, particularly in predicting distributions and dynamics modelling in reinforcement learning. The paper introduces a sequence to sequence variant that shares weights between the encoder and decoder. Additionally, a method for learning control policies under unknown dynamics models is proposed, where the dynamics model parameters are considered unobserved and a system identification model is trained to predict these parameters from observations. The system identification model predicts parameters from observations to enhance a universal policy trained to act optimally under various dynamics models. The training procedure makes the two-stage modeling process robust, differing from other works that focus on memory-based tasks and use a two-stage training process requiring knowledge of system parameters. In contrast to other works, we use system parameters only for analysis, not training. We consider complex robot hands without explicit system dynamics parameterization. Another study fits dynamics models with neural networks, using a global model as a prior for simpler local models within each episode. In this work, the goal is to learn a predictive dynamics model of an agent's future observations based on its actions and past observations. The model encodes information in a hidden state to reason about the global state, even though no information about it is available during training. The paper discusses the use of diagnostic models to predict unobserved states in dynamics models, using privileged information for training but not influencing the dynamics model representations. The Predictor-Corrector (PreCo) dynamics model is introduced for long-horizon multi-step predictions over the observation space. It encodes observed trajectories into a hidden state to predict future observations, showing awareness of unobserved states in the environment. The Predictor-Corrector (PreCo) dynamics model separates the dynamics into predictor and corrector components for single-step and multi-step predictions. The predictor predicts the next hidden state after taking an action, the corrector corrects the current hidden state after receiving an observation, and the decoder maps the hidden state to a distribution over observations. This allows for action-conditional predictions using hidden states. The PreCo model separates dynamics into predictor and corrector components for single-step and multi-step predictions. The predictor predicts the next hidden state, while the corrector corrects the current hidden state. Training involves maximizing likelihood on a reference set of trajectories using single-step and multi-step predictions. The structure of the resulting graph shows only hidden states, with overshooting used for multi-step predictions. The components use single layer LSTM cores and predict independent mixtures of Gaussians. The paper discusses using a mixture density network to predict independent mixtures of Gaussians for controls and sensors. Model predictive control (MPC) is utilized based on predictions from dynamics models, where optimization problems are solved in a receding horizon fashion. The approach involves using separate MLPs for means, standard deviations, and mixture weights, with parameter optimization done using Adam. In experiments, constraints are added to control actions in a box and slew rate constraints are enforced. The MPC solver uses a shooting method with a modified Adam BID32 to find an optimal control sequence from an initial hidden state. The decoder maps hidden states to observations, and other models can map hidden states to unobservable quantities. During an episode, the optimal control sequence is iteratively found from an initial hidden state. The predictor is unrolled at each iteration to compute the objective and gradients using automatic differentiation. Control constraints are handled by projecting onto a feasible set after each Adam iteration. Two ways of collecting trajectories for training dynamics models are discussed: passive collection without input from the dynamics model and active collection to improve the model. The simplest data collection strategy involves designing behavior independent of the dynamics model. Passive data collection involves designing behavior independent of the dynamics model. The collected data is then used to fit the dynamics model using the PreCo training procedure. There is no feedback between the dynamics model and the data collection behavior. Active data collection, on the other hand, uses the dynamics model to guide exploration in areas where the model is poor. The method of active data collection described in the experiments involves maximizing uncertainty in rollout predictions to guide exploration in areas where the model is poor. The uncertainty maximization policy guides exploration by seeking actions with unknown outcomes, which are resolved through execution and observation. Using MPC over an objective that maximizes uncertainty in predictions, Mixtures of Gaussians are used with R\u00e9nyi entropy as a measure of uncertainty. The method extends the work of Wang et al. (Appendix A provides a more detailed derivation). The uncertainty maximization policy uses Mixtures of Gaussians with R\u00e9nyi entropy to guide exploration by seeking actions with unknown outcomes. The information seeking objective in MPC is expressed as the cost function C(h t , u t ) = \u2212 fi H 2 (f i ), where actors use MPC to plan actions that maximize the model's predicted uncertainty. The observations and actions generated are collected into a shared buffer for training data collection. The actors collect data and store it in a shared buffer for the learner to sample and train the PreCo model. The updated model is then used by the actors to plan. The simulated environment includes a hand with sensors and an object underneath. The hand has 13 motors and provides a 132-dimensional observation. The baseline software is available for download from the MuJoCo website. The hand in the simulated environment has sensors providing a 132-dimensional observation. The hand can move to grasp or manipulate a randomly chosen geometric object. A policy is manually designed for the hand to grasp and release the object, generating data for training a PreCo model. The PreCo model is trained using a dataset generated by the grasping policy. The model's awareness is evaluated by predicting the target shape at each timestep, especially when the hand is not in direct contact with the target. The diagnostic model used for prediction is trained after the dynamics model, with a focus on preserving information once contact is lost. The diagnostic model is trained after the dynamics model to predict the target shape when the hand is not in direct contact. The training of the diagnostic does not affect the dynamics model, ensuring no information leakage. Experimental results compare diagnostic predictions using dynamics model features to baselines without these features. The baseline models for the diagnostic include a lower bound using residual information from hand movements, an upper bound LSTM model with full temporal dependencies, and a middle ground RandLSTM model that does not train input or recurrent weights. The PreCo model successfully preserves information about the target identity over time without direct input or training. Different data collection strategies lead to models of varying quality, evaluated by using them in an MPC planner for a diagnostic control task of maximizing pressure on fingertip sensors. The models can anticipate the presence of the block and reason about its effect on the body through MPC planning. Different data collection policies were compared for a diagnostic task, with performances shown in FIG4.1. The IndNoise policy collects data through random actions, while the CorNoise policy collects data differently. The CorNoise policy collects data using random actions sampled from an Ornstein Uhlenbeck process with damping of 0.2 and independent normal noise source with standard deviation 0.2. The AxEnt policy uses the MPC planner to maximize total entropy of model predictions over a horizon of 100 steps, aiming to maximize R\u00e9nyi entropy. The AxTask policy also uses the MPC planner for data collection, with correlated noise added to actions chosen by the planner for better models. The planners' choice of adding correlated noise to actions leads to better models. Evaluation of models is done by running episodes to achieve maximum fingertip pressure, with results shown in FIG4. The AxEnt model's awareness is evaluated using a shape diagnostic task in FIG5, showing improved performance compared to passive awareness diagnostics. The AxEnt model is used for executing different tasks qualitatively. The AxEnt model is used for executing different tasks qualitatively, such as maximizing entropy of predictions to lead to exploratory behavior. Optimizing for fingertip pressure tends to lead to grasping behavior, while an alternative solution involves pushing fingertips into the palm. Examples in Figure 7 show the hand behaving to maximize uncertainty about the future or minimize uncertainty. The hand, when trained to maximize uncertainty, engages in playful behavior with objects. Body models learned with this objective can be re-used for novel objectives like minimizing uncertainty, resulting in behavior where the hand avoids contact to minimize uncertainty about future predictions. Minimizing entropy of predictions leads to behavior where the hand consistently pulls away from the target object, making future observations uninformative. The model is aware of how to interact with the target, as shown in qualitative results. The model demonstrates awareness of interacting with and avoiding the target object in both simulation and reality. A real-life setup using a Shadow Dexterous Hand 2 with a turntable for object orientation is utilized to test the model's effectiveness. Videos of the model in action can be viewed online. The object used for testing is a soft foam wedge fixed to a turntable. 1140 grasp trajectories were collected over two days, with 47 trajectories used for testing and 1093 for training. Each trajectory consists of two grasp-release cycles with the target object at a fixed orientation, measuring four proprioceptive features from the robot at each timestep. The robot's hand has joint ranges limited to prevent fingers from pushing each other, with actuator strengths also limited for safety. Grasped and released positions are sampled from distributions, and position targets are calculated by interpolating between these positions in 20 steps. The sensor model faces complexities such as mismatched actual and target positions when a finger touches an object, causing bending and deformation. The hand can occasionally cause the target object to rotate during the episode, creating an unrecorded source of error. A forward model is trained on collected data to predict the orientation of the block successfully. Learning a forward predictive model of proprioception allows for reasoning about external objects and planning to achieve objectives not seen during training. The study demonstrated successful application of modelling techniques on a real robotic platform to predict block orientation. Planned trajectories and model predictions were shown to optimize fingertip pressure and minimize predicted entropy. The MPL hand, actuated by 13 motors, can exert bidirectional force on hand joints for flexion actions. The MPL hand is actuated by 13 motors, controlling finger joints and thumb separately. The hand is also attached to the world by a fully actuated three-degree-of-freedom wrist joint, with sensors providing proprioceptive information on joint position and velocity. The MPL hand is actuated by 13 motors controlling finger joints and thumb separately, with sensors providing proprioceptive information on joint position, velocity, and force. IMUs in the fingers record rotational and translational acceleration, while pressure sensors measure contact forces. Touch sensors are located on each finger segment and the palm, with hyperparameters for models used in experiments detailed in Table 1. The meaning of each parameter in the random search is shown in FIG9. The depth parameter does not include the output or input layers. The output layers of the MLP parts are indicated separately in the diagrams and are attached together in various ways as shown in FIG1."
}