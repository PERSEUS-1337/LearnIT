{
    "title": "rkl85oRqYX",
    "content": "Modern neural networks often require deep compositions of high-dimensional nonlinear functions to achieve high test accuracy, leading to an overwhelming number of parameters. However, this can be problematic for devices with limited memory or computational power. To address this issue, an efficient mechanism called reshaped tensor decomposition is introduced to compress neural networks by exploiting periodicity, modulation, and low rank structures. This method reshapes the layers into higher-order tensors and applies higher order tensor decompositions to achieve better compression. It improves low rank approximation methods and can be used in conjunction with existing compression techniques for neural networks. Our reshaped tensor decomposition method improves compression for neural networks, outperforming state-of-the-art techniques with 5% test accuracy improvement on CIFAR10. This approach addresses the challenge of large model parameters in modern neural networks, which can be a bottleneck for constrained devices like smartphones and IoT cameras. Our proposed method for compressing neural networks leverages additional invariant structures to improve compression, surpassing existing techniques with a 5% test accuracy boost on CIFAR10. This addresses the issue of large model parameters in modern neural networks, crucial for constrained devices such as smartphones and IoT cameras. The proposed method for compressing neural networks utilizes additional invariant structures like periodicity, modulation, and low rank to reduce redundant parameters. Experiments on benchmark datasets confirmed the effectiveness of this approach, showing that reshaping vectors into higher order objects can significantly decrease the number of parameters needed while preserving information. The proposed method for compressing neural networks utilizes invariant structures like periodicity and modulation to reduce redundant parameters. A new framework called reshaped tensor decomposition (RTD) reshapes neural network layers into higher-order tensors and deploys tensor decomposition to exploit these structures. The technique described in section 3 focuses on tensorized layers to leverage periodic, modulated, and low-rank structures. By decomposing tensors, the number of parameters can be reduced, resulting in more efficient neural networks. However, decomposing higher-order tensors poses challenges as existing methods may not converge to the minimum error. Fine-tuning is necessary for achieving high performance in compressing neural network parameters. A novel data reconstruction-based sequential tuning method is used to minimize differences between uncompressed and compressed outputs. This approach tunes individual compressed blocks sequentially, reducing memory and complexity during compression. New reshaped tensor decomposition methods are proposed to exploit invariant structures for parameter compression in neural networks. Our method utilizes tensor decomposition to compress neural network parameters, exploiting invariant structures for better performance than existing methods. We introduce a computational framework based on tensor algebra for efficient training and inference. Sequential knowledge distillation, known as Seq tuning, transfers knowledge from uncompressed to compressed networks by minimizing data reconstruction error block by block. Our strategy for compressing neural networks involves loading one block of the network into the GPU at a time, leading to faster convergence compared to traditional methods. Extensive experiments demonstrate that our reshaped tensor decomposition outperforms existing low-rank approximation techniques, achieving higher accuracy on CIFAR10 and scaling to deep residual neural networks on ImageNet. Section 2 introduces tensor operations and decompositions, while Section 3 discusses convolutional layers and proposes new schemes for exploiting invariant structures. In Section 4 and Appendix B, the text demonstrates through experiments that their compression technique achieves higher accuracy than existing low-rank approximation methods. Appendix A reviews compression techniques and discusses the relationship with their proposed method. The use of tensor diagrams is emphasized throughout the text, with a detailed appendix defining tensor operations mathematically. Tensor diagrams represent arrays as nodes in a graph, with the number of edges indicating the array's order. Various tensor operations are illustrated using input matrices/tensors and output tensors. These operations are higher-order generalizations of matrix/vector operations. In tensor diagrams, operations are represented by linking edges from input tensors, with different line shapes indicating types of operations. Generalized tensor decomposition is introduced as the reverse mapping of these operations, recovering factors/components to approximate the original tensor. Various classical tensor decompositions are discussed in detail in the appendices. In Appendix F, various tensor decompositions are introduced, including Tucker (TK) and Tensor-train (TT). These decompositions are applied to the convolutional kernel in FIG4. A standard convolutional layer in neural networks is parameterized by a 4-order kernel K, mapping input tensor U to output tensor V with stride d. It takes O(HW ST XY) operations. The convolutional layer is compressed using tensor decompositions like CANDECOMP/PARAFAC (CP), Tucker (TK), and Tensor-train (TT). For example, a Tensor-train decomposition on the kernel K requires fewer parameters. Details of CP and TK are deferred to Appendix G. The convolutional layer is compressed using tensor decompositions like CANDECOMP/PARAFAC (CP), Tucker (TK), and Tensor-train (TT). The complexities of reshaped tensor decomposition (RTD) are illustrated in Figure 5, showing how a matrix can be reshaped into a higher-order tensor to reduce the number of parameters required. Reshaped Tensor Decomposition (RTD) tensorizes the convolutional kernel K into a higher-order tensor K \u2032. The tensorized kernel can be compressed using CP, Tucker, and Tensor-train decompositions, named as reshaped CP (r-CP), reshaped Tucker (r-TK), and reshaped Tensor-train (r-TT) respectively. Sequential tuning tensor decompositions provide weight estimates in tensorized convolutional layers. Fine tuning is necessary to achieve high performance as decomposing higher order tensors is challenging and known methods may not converge to the minimum error decompositions. The strategy of data reconstruction-based sequential tuning involves layer by layer parameter fine-tuning using backpropagation to minimize the difference between uncompressed layer outputs and tensorized compressed layer outputs. Reshaped tensor decomposition increases computational complexity by mapping a layer into multiple narrower layers with width R l. Sequential tuning tensor decompositions involve layer by layer parameter fine-tuning in tensorized convolutional layers. The reshaped tensor decomposition reduces the original width of the layer to create deeper, narrower layers. Efficient algorithms for forward/backward propagations are designed using tensor algebra, avoiding the need to explicitly reconstruct the original kernel. The key idea is to interact the input with each factor individually, reducing the computation steps required. The time complexity for the forward pass in tensor decompositions is analyzed in Appendices I, G, and I. Tensor algebra allows for parallel implementation of propagations, speeding up prediction. The parallel time complexity of prediction using RTD outperforms PD and original convolutional layers. The complexity of forward pass using tensor decompositions on convolutional layers is discussed. The optimal complexity of tensor algebra is NP complete, and the reshaped tensor decomposition method is evaluated on various networks for compression. The baseline comparison is with plain tensor decomposition methods. Our primary contribution is the introduction of reshaped tensor decomposition (RTD), a state-of-the-art low-rank approximation method that captures additional invariance structures like periodicity and modulation. RTD outperforms plain tensor decomposition (PD) in maintaining high accuracy even with highly compressed networks on CIFAR-10. We also introduce a new training approach, data reconstruction-based sequential (Seq) tuning, as an alternative to traditional end-to-end (E2E) tuning. Our algorithm achieves higher accuracy than baseline on ResNet-34 CIFAR10 by compressing the network to 10% of its original size. Using reshaped tensor decomposition with Seq tuning, the accuracy increases to 91.28% with 10% compression rate, showing a performance loss of 2% with fewer parameters. Further aggressive compression results in a performance loss of 6% with only 2% of the parameters. Tensor-train decomposition also shows higher compression and accuracy trends. However, Tucker decomposition is less effective with very high compression due to the network's reduced internal structure. Increasing the network size to 20% of the original provides reasonable performance on CIFAR-10 for Tucker as well. Seq tuning consistently outperforms end-to-end tuning in terms of accuracy and convergence speed. RTD restores test accuracy even at very high compression ratios, indicating the presence of extra invariant structure in deep neural networks. Our proposed approach, tensorization combined with low rank approximation (RTD), captures invariant structure in deep neural networks that baseline PD cannot. RTD with Seq tuning achieves high accuracy and compression rates on CIFAR10 and scales well to large networks like ResNet-50 on ImageNet 2012. Seq tuning of RTD is faster and more effective than plain tensor decomposition with E2E tuning. Our proposed efficient RTD compression method, utilizing tensorized decompositions, outperforms baseline PD compression in capturing invariance structures of ResNet. Seq tuning of RTD is faster and more effective, scaling well to state-of-the-art neural networks. The text discusses the use of tensorization for neural networks and higher order tensor decompositions. It also mentions a survey on compressing neural networks, categorizing methods into low-rank factorization, compact filters, knowledge distillation, and parameter pruning. The decomposition schemes focus on low-rank factorization and compact filter designs, with a strategy of sequential tuning for knowledge distillation. Our method involves compressing student network block by block, which can be further enhanced with parameters pruning, quantization, and encoding. Low-rank Factorization techniques have been used to reduce parameters in fully connected and convolutional layers, with methods like matricization and tensor decomposition. The groundbreaking work by Novikov et al. (2015) shows that parameters in fully connected layers can be compressed efficiently using tensor decomposition. This idea has been extended to compress LSTM and GRU layers in recurrent neural networks by Yang et al. (2017). Our paper, concurrent to Wang et al. (2018), extends this concept to convolutional layers by exploiting invariant structures among filters. We investigate and implement a broader range of decomposition schemes for compact filters, reducing parameters by imposing constraints on linear layers. Innovative research has shown that fully connected layer parameters can be compressed using tensor decomposition. This concept has been extended to compress convolutional layers by exploiting invariant structures among filters. Various decomposition schemes are explored to create compact filters and reduce parameters in linear layers. In distillation, information is transferred from a teacher network to a smaller student network. Different methods like supervised training with logits and matching outputs at each layer have been proposed. Seq tuning strategy is similar to previous work but uses identical mapping. Compression techniques like pruning, quantization, and encoding can further reduce parameters in the network. Our method of compressing sublayers is complementary to existing techniques in the pipeline. Seq tuning leads to faster and more stable convergence compared to end-to-end tuning. Experimental results show that Seq tuning has high error initially but drops significantly after tuning the final block, requiring fewer gradient updates than end-to-end tuning for stable performance. The reshaped tensor decomposition compression method achieves stable performance with quick convergence on fully-connected layers, losing only about 2% accuracy when compressed to 0.2% of their original size. The method can be applied flexibly to both fully-connected and convolutional layers of a neural network. The reshaped tensor decomposition compression method shows high efficacy on fully-connected layers, with only a 1% accuracy reduction when compressed to 1% of their original size. The uncompressed network achieves 99.31% accuracy on MNIST dataset. Symbols are used to denote vectors, matrices, and tensors in tensor operations. The tensor operations in this paper are denoted by symbols like \u2297 for outer product, * for convolution, and \u00d7 for contraction/multiplication. Subscripts and superscripts are used for these operators, with \u2022 representing compound operations. Nature indices start from 0, but reversed indices can start from -1. Functions are introduced to alter the layout of multi-dimensional arrays. The notation for tensor operations includes symbols like \u2297 for outer product, * for convolution, and \u00d7 for contraction/multiplication. Subscripts and superscripts are used for these operators, with \u2022 representing compound operations. Nature indices start from 0, but reversed indices can start from -1. Functions are introduced to alter the layout of multi-dimensional arrays, such as slicing with the symbol colon ':' and converting between arrays and vectors using big-endian notation. The function swapaxes(\u00b7) is used to permute the modes of a tensor as needed. The function swapaxes(\u00b7) permutes the modes of a tensor as needed. It is a basic tensor operation that can be combined with other operations in tensorial neural networks. Tensor contraction is a fundamental operation in tensorial neural networks, allowing for the combination of basic operations to create complex compound operators. The sequential time complexities of these operations are analyzed in detail, with considerations for implementation. The degree of parallelism in implementing these operations depends on software and hardware realizations. The sequential time complexity serves as an estimate of computational expense. Tensor contraction is a key operation in tensorial neural networks, with time complexity analysis for estimating computational expense. Parallel computing and memory layout optimization play crucial roles in speeding up tensor operations. Tensor multiplication is a special case of tensor contraction involving a matrix. It yields another tensor with entries computed according to multi-linear algebra conventions. The number of operations for tensor multiplication follows tensor contraction exactly. The tensor T can be computed using convolution operations, with the resulting dimension determined by the type of convolution chosen. Vector convolution is generally asymmetric, except for circular convolution. Equation D.3a can also be expressed as D.3b. Fast Fourier Transform (FFT) is crucial for reducing computational complexities in convolution. FFT may not always be necessary, especially in cases where computing convolution without FFT is faster due to certain conditions. Implementing FFT can be challenging in neural networks with fancy convolution definitions. Therefore, tensor convolutions are assumed to be computed without FFT unless specified otherwise. Tensor outer product is a generalization of the outer product for two tensors, but it requires significant computational and memory resources. It is rarely calculated alone in practice. Tensor partial outer product is a variant that is widely used in conjunction with other operations. The tensor partial outer product, similar to outer product, involves computing entries shared between two tensors. The computational complexity is O(( DISPLAYFORM20, making it equivalent to tensor contraction. Unlike matrix multiplication, tensor operations are not commutative or associative in general. Tensor operations can change mode locations but can be fixed by adjusting operators. Operations need to be evaluated left to right unless brackets are used. Compound operations combine basic tensor operations to perform multiple operations on multiple tensors simultaneously. In this section, two representative examples illustrate simultaneous multi-operations between tensors. For instance, operations like partial outer product, convolution, and contraction can be performed simultaneously on tensors, resulting in a new tensor. The number of operations required for computation varies depending on the method used, such as FFT or without FFT. Additionally, simultaneous operations between a tensor and multiple tensors can also be performed, combining operations like partial outer product on different tensors. Simultaneous multi-operations between tensors involve performing partial outer product, convolution, and contraction simultaneously, resulting in a new tensor. The time complexity of these compound operations can be optimized by breaking the evaluation into three steps. The time complexity of compound operations between tensors can be optimized by breaking the evaluation into three steps, resulting in a total time complexity of O(RXST + RXHP T + RX \u2032 P QT ). Determining the best order to evaluate these operations is an NP-hard problem, often solved through exhaustive search or heuristic strategies due to the complexity of the task. More complex compound operations involving multiple tensors can also be defined, as seen in the derivation of backpropagation equations. Compound operations over multiple tensors are often represented graphically as tensor networks. These operations, both basic and compound, are linear in their operands, making it easy to calculate derivatives. In the next section, we will derive the backpropagation equations for these operations, as detailed in the monograph by Cichocki et al. (2016). The backpropagation equations for compound tensor operations can be derived using the classic chain rule, which is crucial for feed-forward neural networks. These equations can be analyzed similarly to tensor operations, with the same computational complexities as in the forward pass. This observation is valuable for analyzing tensorial neural networks and allows for reusing the same numbers in both forward and backward propagation. In the analysis of backward propagation, the derivatives of the loss function with respect to its operants can be computed using tensor contraction and the classic chain rule. This simplifies the process and allows for easier computation using tensor notations. The backpropagation equations involve compound operations and have the same time complexity as the forward pass. Tensor multiplication derivatives can be computed using tensor contraction and the chain rule. The derivatives of T with respect to T(0) and T(1) can be computed using tensor convolution with circular matrices. The derivatives of the result tensor T with respect to T(0) and T(1) can be obtained using matrix calculus and chain rule, leading to simplified backpropagation equations for convolution operations. This method can be applied to general convolution by replacing circular convolution with its corresponding adjoint operator. The trick to start with circular convolution and generalize to general cases is useful for deriving backpropagation equations. The time complexities of backpropagation equations for tensor convolution are identical, with differences only in the number of operations. The asymmetry in time complexities for forward and backward passes can be utilized in neural networks. The number of operations required for both equations is O(( DISPLAYFORM5, which is identical to the forward pass. The backpropagation equations for tensor convolution are similar to those for tensor contraction, with differences in notation. The time complexity for these equations is O((, identical to the forward pass. In this section, we derive backpropagation equations for compound operations involving tensors. These equations are more complex but demonstrate techniques applicable to all compound operations. An example of simultaneous multi-operations between two tensors is discussed, showcasing the application of these derived equations. The text discusses deriving backpropagation equations for compound tensor operations, showcasing techniques applicable to all compound operations. It includes the use of circular convolution and tensor notations to obtain general backpropagation rules, with a focus on operations involving multiple tensors. The time complexities of forward and backward passes are also mentioned, highlighting the computational efficiency of computing derivatives with respect to different tensors. The text explains deriving backpropagation equations for compound tensor operations, focusing on simultaneous multi-operations between a tensor and multiple tensors. It showcases techniques using circular convolution and tensor notations to obtain general backpropagation rules efficiently. The text simplifies deriving backpropagation equations for compound tensor operations by using shorthand notations and reorganizing complex expressions for more efficient computation. It highlights the complicity of tensor operations and the ability to reduce them to basic forms. The text discusses the complexity of finding the optimal order for evaluating compound tensor operations, which is often solved using heuristics due to its NP-hard nature. It compares the efficiency of different sets of equations and encourages readers to find the most efficient way through combinatoric search. Tensor decompositions, extensions of matrix factorizations for multi-dimensional arrays, are reviewed, along with commonly used decomposition methods. The text also explores how backpropagation algorithms adapt to tensor decomposition schemes in neural networks. The text discusses how backpropagation algorithms adapt to tensor decomposition schemes in neural networks. It presents backpropagation equations directly in tensor notations for different decomposition methods, highlighting the computational expense of propagating gradients through the original tensor to its factors. It suggests avoiding explicit computation of these equations in practice due to their complexity. The text discusses deriving backpropagation rules for CP decomposition in neural networks, which factorizes tensors into factor matrices. It suggests isolating the factor of interest to simplify the computation process. Tucker decomposition provides a more general factorization than CP decomposition for an m-order tensor. It factors the tensor into factor matrices and a core tensor, with Tucker ranks required to be smaller or equal to the dimensions at their corresponding modes. This decomposition can help simplify the computation process. Tucker decomposition is a more general factorization than CP decomposition for an m-order tensor. When R 0 = \u00b7 \u00b7 \u00b7 = R m\u22121 = R and C is a super-diagonal tensor with all super-diagonal entries as ones, Tucker decomposition reduces to CP decomposition. The backpropagation equations for Tucker decomposition can be derived similarly to CP decomposition, with a focus on tensor notations and operations. The backpropagation rule of tensor multiplication in Appendix E leads to a more efficient equation, with the number of operations bounded by O(( l=0 I l )). Tensor-train decomposition factorizes a tensor into interconnected core tensors with Tensor-train ranks controlling the tradeoff between parameters and accuracy. Backpropagation equations for Tensor-train decomposition are derived from Novikov et al., reformatted in tensor notations. The Tensor-train decomposition is reformatted using auxiliary tensors {P (l)} and {Q (l)}. Backpropagation equations are obtained in tensor notations. Variants of standard decompositions are considered, aiming to recover input factors from tensor operations. The Tensor-train decomposition is reformatted using auxiliary tensors {P (l)} and {Q (l)}, and backpropagation equations are obtained in tensor notations. Variants of standard decompositions are considered to recover input factors from tensor operations, such as arbitrary ordering of modes, partial Tucker decomposition, and grouping multiple modes into supermode for decomposition. Tensor T can be factorized into three factors, denoted as r1,i2,j2. This allows for compression and acceleration of the standard convolutional layer in neural networks. The table summarizes different tensor decompositions, their parameters, and time complexities for backpropagation. The standard convolutional layer in neural networks can be compressed and accelerated by factorizing the tensor of parameters into smaller factors. This alternative strategy reduces computational complexity and avoids explicit reference to the original kernel. The use of FFT in computing convolutions can also speed up the process. In modern CNNs, a standard convolutional layer is defined by a 4-order kernel. It maps a 3-order tensor to another 3-order tensor using a specific equation. The number of parameters and operations in a convolutional layer are determined by the size of the kernel and input/output channels. The standard convolutional layer performs a compound operation of two tensor convolutions and one tensor contraction simultaneously between the input tensor U and the kernel of parameters K. Backpropagation equations in tensor notations are obtained following a standard procedure. Various decompositions and singular value decomposition on the kernel K are applied to evaluate Equation G.1, interacting the input with the factors individually. Tensor decomposition on the parameters decouples a layer in the original model into several sublayers in the compressed model. Decomposing a convolutional layer using singular value decomposition (SVD) is a common method for compressing neural networks. Different approaches exist for matricizing the tensor of parameters, resulting in seven distinct types of matricization. One popular method groups filter height and input channels as a supermode. SVD-convolutional layer decomposes filters into two steps, each involving one factor tensor. This results in a concatenation of two convolutional layers without nonlinearity in between. Backpropagation equations for these steps can be derived following a specific procedure. The backpropagation equations for the CP-convolutional layer involve decomposing the kernel using CP decomposition, resulting in three factor tensors. This process requires a three-step procedure to evaluate V, with intermediate tensors involved. After CP decomposition, tensor multiplications are performed in three steps, including 1x1 convolutional layers and depth-wise convolutional layers. The time complexity for the forward pass is faster than standard convolutional layers. Backpropagation equations for derivatives with respect to input tensors are then calculated. The use of Tucker decomposition to compress and accelerate convolutional layers is proposed in Kim et al. (2015). A three-step forward pass procedure is obtained by plugging Equation G.12a in G.1, resulting in a faster Tucker-convolutional layer compared to the standard convolutional layer. The Tensor-train-convolutional layer proposes applying Tensor-train decomposition to compress a convolutional layer. However, naive Tensor-train decomposition on the kernel K may yield inferior results. In this paper, the proposed method involves reordering the modes of the kernel and decomposing it into factors. The evaluation process is decoupled into four steps, with the tensor-train-convolutional layer consisting of four sub-layers. The Tensor-train-convolutional layer involves reordering kernel modes and decomposing it into factors. It can be interpreted as a Tucker-convolutional layer or a SVD-convolutional layer. Backpropagation equations are easily derived. Time complexities for input/intermediate tensors and factors can be calculated. The summary includes the number of parameters and operations required for various plain tensor decompositions on convolutional layers. The dense layer in a neural network is characterized by a matrix-vector multiplication, with a matrix parameterized by ST parameters. Singular value decomposition (SVD) can compress the matrix, reducing parameters from ST to ((S + T)R) and time complexity from O(ST) to O((S + T)R). This compression is inspired by exploiting invariant structures with tensor decompositions. In Section 3, tensor decompositions can exploit invariant structures. The matrix K is tensorized into a tensor K \u2208 R S0\u00d7\u00b7\u00b7\u00b7\u00d7Sm\u22121\u00d7T0\u00d7\u00b7\u00b7\u00b7\u00d7Tm\u22121. Input/output u, v are reshaped into U \u2208 R S0\u00d7\u00b7\u00b7\u00b7\u00d7Sm\u22121 , V \u2208 R T0\u00d7\u00b7\u00b7\u00b7\u00d7Tm\u22121. A tensorized dense layer maps U to V with a 2m-order tensor K. Backpropagation equations are obtained by reshaping those for a standard dense layer. The tensorized dense layer can be compressed by decomposing the kernel K into smaller factors. In Section 3, tensor decompositions can exploit invariant structures. The matrix K is tensorized into a tensor K \u2208 R S0\u00d7\u00b7\u00b7\u00b7\u00d7Sm\u22121\u00d7T0\u00d7\u00b7\u00b7\u00b7\u00d7Tm\u22121. Input/output u, v are reshaped into U \u2208 R S0\u00d7\u00b7\u00b7\u00b7\u00d7Sm\u22121 , V \u2208 R T0\u00d7\u00b7\u00b7\u00b7\u00d7Tm\u22121. A tensorized dense layer maps U to V with a 2m-order tensor K. Backpropagation equations are obtained by reshaping those for a standard dense layer. The tensorized dense layer can be compressed by decomposing the kernel K into smaller factors. The schemes of tensor decompositions used in this section are analyzed, with a focus on designing principles. Learning the factors through the gradient of the original tensor is costly, leading to a multi-step procedure for computing the output. Factorizing the kernel K by grouping (S l , T l )'s as supermodes is proposed instead of naive CP decomposition over all 2m modes without grouping any supermode. The tensorized dense layer can be compressed by decomposing the kernel K into smaller factors. By grouping (S l , T l )'s as supermodes, the total number of parameters of a r-CP-dense layer is significantly reduced. The backpropagation equations for this layer are obtained following a multi-step procedure. If a compound operation does not contain convolution, the time complexity of backpropagation is the same as the forward pass. The time complexity of backpropagation in a tensorized dense layer is bounded by O(mSR) due to the application of TK decomposition, which factors the tensor of parameters K. The order of contraction of factors does not affect the overall complexity, making the contraction with all m factors at most O(mSR). The time complexity for backpropagation in a tensorized dense layer is O(m(S + T)R + R2m). The procedure to derive backpropagation rules involves modifying variable names from standard Tucker decomposition and tensorized layer equations. Despite being technically complex, the process is straightforward. The backpropagation complexity for tensorized dense layers is bounded by O(m(S + T)R + R2m). The layer follows a procedure similar to compressing networks using tensor decompositions, with improved efficiency in the backward pass compared to the original design. The kernel is decomposed using Tensor-train decomposition. The backpropagation complexity for tensorized dense layers is bounded by O(m(S + T)R + R2m). The layer follows a procedure similar to compressing networks using tensor decompositions, with improved efficiency in the backward pass compared to the original design. The kernel is decomposed using Tensor-train decomposition, where factor tensors are involved and operations are done through tensor contractions. The time complexity for backpropagation is bounded by O(m max(S, T)). The authors propose a novel tensor contraction layer, which maps an m-order tensor to another m-order tensor. This layer is a special case of r-CP-dense layer with rank-1 kernels. A tensor regression layer is also introduced, reducing an m-order tensor to a scalar by contracting all modes with another tensor of the same size. The tensor regression layer is effectively parameterized by a set of matrices and a core tensor. It is a special case of r-Tuckerdense layer where input factors are contracted with output factors. The design of r-Tucker-dense layer differs from CP and Tensor-train decompositions, with a drawback in the grouping of supermodes before factorization. The r-Tucker-dense layer poses an \"information bottleneck\" due to the small size of the intermediate tensor, causing significant loss during the forward pass. This issue is avoided in r-CP-dense and r-Tensor-traindense layers by grouping supermodes, which maintains similar sizes for all intermediate tensors. The design of Tucker-dense layer initially does not group supermodes together, leading to problematic contractions between input and factors. The design of the Tucker-dense layer faces challenges with large intermediate tensors during interactions with factors, leading to memory issues. To maintain reasonable time complexity, this design is abandoned. However, the Tucker-dense layer has an advantage over r-CP-dense and r-Tensor-train-dense layers in terms of backward pass operations, making it faster at the same compression rate. The Tucker-dense layer is faster than r-CP-dense and r-Tensor-train-dense layers at the same compression rate. It is more desirable for speed over accuracy. In the appendix, parameters are tensorized to exploit invariance structures. Several additional convolutional layers are proposed based on this technique. The input and output tensors are folded, and parameters are reshaped into a tensor. Backpropagation equations are easily obtained for these layers. In this section, CP decomposition is used similarly to the r-CP dense layer. The tensor K is decomposed into supermodes, with (S l , T l )'s grouped and (H, W ) as an additional supermode. The tensor K takes a specific form, with Equation I.3a being a key difference. The CP decomposition is utilized in a similar manner to the r-CP dense layer, with Equation I.3a being a key difference. The output evaluation procedure now includes an extra step, resulting in a (m + 2)-steps algorithm presented at the entries level. The order to interact the factors is arbitrary, with the convolutional factor U (m) placed at the end for convenience during implementation. The 3D-convolutional layer with R input feature volumes and one output feature volume has a time complexity in the forward pass that can be easily obtained. The total number of operations for the r-CP-convolutional layer is O(m max(S, T) r-Tucker-convolutional layer. Partial Tucker decomposition is proposed on the tensorized kernel K, factorized with two extra modes for filter height and width. The core tensor now has two extra modes for filter height and width, increasing the number of parameters by a factor of HW. The evaluation of V can be sequentialized into three steps, with backpropagation rules derived similarly to the r-Tucker-dense layer. The time complexity for obtaining derivatives is magnified by XY in the first step, and scaled by HW X'Y' and X'Y' in the middle and last steps respectively. Total operations for derivatives with respect to input/intermediate results is O(mS r-Tensor-train-convolutional layer. The r-Tensor-train-convolutional layer applies Tensor-train decomposition to the tensorized kernel K, decomposing it into factor tensors. This layer has an additional factor K (m) with RHW parameters, leading to a total number of O((m(ST)1m + HW)R). The preprocessing steps involve adding a singleton mode R-1 = 1 to U and K (0), resulting in U (0) \u2208 RX\u00d7Y\u00d7S0\u00d7\u00b7\u00b7\u00b7\u00d7Sm-1\u00d7R-1 and K (0) \u2208 RR-1\u00d7S0\u00d7T0\u00d7R0. The evaluation of V now involves (m + 1) steps. The evaluation of V now involves (m + 1) steps, with the last step being a 3D-convolutional layer denoted as TAB2. The dense layers are numbered as DISPLAYFORM5 to DISPLAYFORM10."
}