{
    "title": "SyeOVTEFPH",
    "content": "Adversarial training is a successful strategy for improving neural network robustness to attacks, but it lacks generalization to unperturbed test sets. The proposed instance adaptive adversarial training technique enforces sample-specific perturbation margins, leading to improved test accuracy on unperturbed samples with a slight decrease in robustness. Extensive experiments on various datasets validate the effectiveness of this approach. Experiments on CIFAR-10, CIFAR-100, and Imagenet datasets show the effectiveness of adversarial training in improving neural network stability to input perturbations. Despite its ability to increase robustness, adversarial training can lead to a drop in accuracy on clean test inputs. The drop in clean accuracy can be significant on CIFAR-10 and Imagenet, making robust models undesirable in some industrial settings. Recent research suggests a fundamental trade-off between robustness and accuracy, with theoretical results characterizing this tradeoff. The objective is to improve clean accuracy of adversarial training while maintaining a chosen level of robustness. The method aims to optimize the tradeoff by addressing the constraints of adversarial training. Adversarial training faces challenges in achieving label consistency due to overlapping classes in the -ball around input images. The trade-off between stability and accuracy is evident, with varying levels of robustness for different images on CIFAR-10. Adversarial training encounters difficulties in maintaining label consistency because of overlapping classes in the -ball around input images. This results in varying levels of robustness for different images on CIFAR-10. Instance adaptive adversarial training assigns perturbation radii customized to each training image, ensuring high adversarial robustness where possible while minimizing constraints. Instance adaptive training significantly improves the tradeoff between accuracy and robustness by assigning perturbation radii customized to each training image, breaking through the pareto frontier achieved by standard adversarial training. The learned instance-specific perturbation radii are interpretable, with small radii indicating ambiguity and nearby images of another class, while large radii suggest unambiguous class labels that are difficult to manipulate. This approach differs from Ding et al.'s work on adaptive margins in a max-margin framework for adversarial training, as our goal is to understand and enhance the robustness-accuracy tradeoff. Defending against adversarial attacks involves crafting attacks using various methods like FGSM, PGD, C/W attack, DeepFool, etc., all utilizing the gradient of the loss function to create perturbations. Alternative attack metrics include spatial transformer attacks and those based on Wasserstein distance in pixel space. In machine learning, defending against adversarial attacks is a critical issue. Adversarial training is a defense strategy that remains resilient to many attacks. The focus is on classification problems, with neural network models trained by minimizing cross entropy loss. Adversarial training involves optimizing the network over perturbed training sets, presented as a min-max problem. Instance Adaptive Adversarial Training (IAAT) is proposed as a solution to the shortcomings of uniform perturbation radius in adversarial training. It involves solving an optimization problem by sampling mini-batches of images, crafting adversarial perturbations of varying sizes for each image, and updating the network model using the perturbed images. This approach aims to improve the robustness of neural network models against adversarial attacks. Instance Adaptive Adversarial Training (IAAT) updates the perturbation radius i for each image x i based on the success of crafting adversarial examples. The radius is adjusted after each epoch using a heuristic approach. A warmup period precedes the instance adaptive training, where uniform perturbations are used. This method aims to enhance the network's robustness against adversarial attacks. In instance adaptive adversarial training, a detailed algorithm is provided. The evaluation includes test accuracy, adversarial accuracy against PGD attacks, transfer attacks, and image corruptions. Experiments are conducted on CIFAR-10 and CIFAR-100 datasets using Resnet-18 and WideRenset-32-10 models trained on PGD-10 attacks. Models are evaluated on PGD-10 attacks with 5 random restarts in the whitebox setting. In adversarial training, models are evaluated on PGD attacks with varying strengths and random restarts, transfer attacks, and image corruptions. Training models with different perturbation radii results in a tradeoff between natural accuracy and adversarial robustness. Instance adaptive adversarial training is compared with a tradeoff curve in Fig. 3a, 3b, showing improvement over accuracy-robustness tradeoff. Two versions of IAAT are reported - with and without warmup phase. Warmup phase helps retain robustness with a drop in natural accuracy. Clean accuracy improves on CIFAR-10, with a 4.06% increase for Resnet-18 and 4.49% for WideResnet-32-10. The algorithm also improves robustness to unseen image corruptions, enhancing overall generalization ability of the network. The performance gain in natural test accuracy increases on CIFAR-100 for Resnet-18 and Wideresnet-32-10. Adversarial robustness is maintained over a range of test values. Adversarial training with different values shows varying performance at different regimes. The instance adaptive training algorithm achieves good performance across all regimes. The values chosen by the adaptive algorithm correlate well with human concepts. The instance adaptive algorithm shows good performance across all regimes, with values chosen correlating well with human concepts of class ambiguity. It achieves similar robustness to other attacks as adversarial training on gradient-based attacks, while improving natural accuracy. During training, adversarial attacks are generated using 30 steps of PGD with a fixed epsilon of 16/255. Distributed training with synchronized SGD on 64/128 GPUs is used for efficiency. Evaluation includes testing on clean samples and whitebox adversarial attacks with epsilon values ranging from 4 to 16. PGD-1000 attacks are employed, and mCE is calculated to assess robustness to image corruptions. Our experimental results in Table 4 show a significant drop in natural accuracy for adversarial training, with Adaptive adversarial training improving accuracy by 10+% on all models. IAAT outperforms the adversarial training baseline on whitebox attacks at low regimes but shows a 13% drop at high epsilon values. Our model consistently outperforms adversarial training on the corruption dataset. Instance adaptive training is used after warmup with uniform normbound constraints, as shown in Table 5 and 6 for adversarial robustness. When warmup is used, adversarial robustness improves with a slight decrease in natural accuracy, especially in CIFAR-100. Instance-specific perturbation radius is estimated to ensure consistent predictions within a chosen radius. A line search method is used for this estimation, showing marginal improvement compared to IAAT but being computationally expensive. In this work, the focus is on improving the robustness-accuracy tradeoff in adversarial training by developing instance adaptive adversarial training. The proposed algorithm imposes label consistency constraints within sample-specific perturbation radii, leading to improved performance on CIFAR-10, CIFAR-100, and Imagenet datasets. In a recent paper, mixup adversarial training was introduced as a method to improve natural accuracy without compromising adversarial robustness. However, when compared to IAAT on stronger attacks, mixup showed a higher drop in adversarial accuracy. Samples assigned low values by IAAT were visually confusing, while high values were distinctively belonging to one class. The visualization in Figure 5 shows samples with low values being visually confusing, while high values belong distinctly to one class. Figure 6 displays samples near the decision boundary within a fixed radius. The algorithm effectively retains label information within the chosen radius. Evolution of the perturbation radius over epochs in adaptive adversarial training is visualized in Figures 7a and 7b, showing convergence around 11, higher than the default setting of 8. Each sample has a different profile, with some exceeding the radius of 8 and others converging below it. In Figure 8, a histogram of 's at different training snapshots is shown, indicating an increase in spread as training progresses. Testing against a strong adversary is crucial for assessing model robustness. Models are attacked using PGD with up to 2000 iterations, showing saturation in robustness beyond 500 iterations. The IAAT algorithm has hyper-parameters \u03b2 and \u03b3, with a sensitivity analysis performed in this section. In a sensitivity analysis, the algorithm's performance is not highly affected by hyper-parameter choices, but the best results are seen with \u03b3 = 1.9 and \u03b2 = 0.1. Adversarial training on CIFAR-10 and CIFAR-100 datasets follows standard settings, while on Imagenet, PGD-30 attacks are used, making training computationally expensive. During distributed training on 64/128 GPUs, synchronized SGD updates are used. Adversarial attacks are generated with FP-16 precision during training, and FP-32 is used during testing. Two tricks are employed to speed up instance adaptive adversarial training: a weaker attacker (PGD-10) is used, and values are clipped with a lower bound of 4. Models were trained on PyTorch, with Resnet-50 on 64 Nvidia V100 GPUs, and Resnet-101 and Resnet-152 on 128 GPUs. Time taken for training is reported in Table 12."
}