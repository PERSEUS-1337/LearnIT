{
    "title": "HygHbTVYPB",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in modeling distributions over complicated manifolds like natural images. However, GANs often suffer from mode collapse, characterizing only a few modes of the data distribution. To address this, a novel framework called LDMGAN is proposed, introducing Latent Distribution Matching (LDM) constraint to align distribution of generated samples with real samples in latent space. A regularized AutoEncoder (AE) is used to map data distribution to prior distribution in encoded space. Extensive experiments show that LDMGAN significantly improves GAN\u2019s stability and diversity. Generative Adversarial Networks (GANs) are powerful tools for unsupervised learning of probability distributions over complex manifolds like natural images. GANs consist of a generator and a discriminator that compete with each other to generate sharp and compelling samples. They have gained success in image generation tasks and do not require explicit parametric specification of the model distribution. In practice, GANs are notoriously hard to train due to sensitivity to architectures and hyper-parameters, leading to convergence issues and mode collapse. The mode missing issue in GANs may be due to the lack of a regularization term for generating diverse samples. In this work, a regularization constraint called Latent Distribution Matching is proposed to address the mode collapse issue in GANs by aligning distributions in encoded space. A regularized autoencoder is introduced to map data distribution to a simple prior distribution, resulting in the LDMGAN framework. This approach stabilizes GAN training and improves diversity and stability compared to other methods on various datasets. The GAN framework consists of a generator and discriminator neural network that are optimized through a minmax game to minimize the Jensen Shannon divergence between the model distribution and data distribution. Stabilizing GAN training and alleviating mode collapse are key challenges addressed in various works. DCGAN, Improved GANs, Unrolled GAN, TTUR, and WGAN are works that aim to stabilize GAN training and reduce mode collapse. These techniques include deep convolutional architectures, feature matching, mini-batch discrimination, historical averaging, unrolling optimization of the discriminator objective, two time-scale update rule, and leveraging the Wasserstein distance for better convergence. WGAN requires the discriminator to lie on the space of 1-Lipschitz functions. Some works like WGAN-GP, SN-GAN, EBGAN, BEGAN, and MDGAN have integrated autoencoders into GANs to address mode collapse and improve training stability. These methods enforce Lipschitz constraints, use spectral normalization, and combine autoencoders with GANs to prevent mode missing problems. Several models have been developed to address issues in GANs, such as mode collapse and training stability. VAEGAN unified VAE and GAN, AAE used adversarial learning in its encoded space, BiGAN and ALI jointly trained an inference and generative model, VEEGAN employed a reconstructor network, and AGE aligned model distribution with data distribution in encoded space. AGE aligned model distribution with data distribution in encoded space by eliminating the discriminator and using an encoder. They compared real and generated samples in the encoded space to prevent mode collapse in GANs, differentiating from VAEGAN and VEEGAN. Our regularized autoencoder aligns aggregated posterior with the prior distribution using explicit divergence, avoiding mode collapse in GANs. The training objective involves an adversarial game between a discriminator and a generator in data space. The LDMGAN framework addresses the issue of generator outputs becoming too similar by introducing a regularized autoencoder. This autoencoder performs data dimensionality reduction to create a well-shaped manifold in encoded space, enforcing diversity in generated samples. The LDMGAN framework introduces Latent Distribution Matching (LDM) to ensure diverse sample generation. LDM involves an encoder function mapping true data distribution to a prior Gaussian, addressing mode collapse in GANs. The encoder maps data distribution to a prior Gaussian to prevent mode collapse in GANs. Mode collapse causes a distribution mismatch between the generated and true data distributions, evaluated by a divergence measure in the latent space. Comparing the two distributions is challenging, so an intermediate solution is introduced. The paper introduces Latent Distribution Matching (LDM) to combat mode collapse in GANs by regularizing the generator. This involves aligning the encoded data distribution with the prior distribution p(z) using a regularized autoencoder. The paper introduces Latent Distribution Matching (LDM) to combat mode collapse in GANs by aligning the encoded data distribution with the prior distribution using a regularized autoencoder. The model combines autoencoder with GANs to ensure learning from all training samples, regularize the generator, and map data distribution to a gaussian. The model combines GANs with reconstruction loss in data space to address mode collapse by aligning the encoded data distribution with a prior distribution using a regularized autoencoder. This ensures learning from all training samples and maps data distribution to a Gaussian, providing a way to push the generator towards missing modes. The regularized autoencoder combined with GANs aligns the encoded data distribution with a prior distribution, using a factorized Gaussian as the prior. The regularized autoencoder uses a factorized Gaussian as the prior distribution and a deterministic function for the encoding function. It minimizes the reconstruction error and imposes a regularization term on the encoded space. The autoencoder is trained in two phases: the reconstruction phase and the regularization phase. The regularized autoencoder, implemented by deep neural networks, guarantees matching the aggregated posterior with the prior distribution. The training objective includes a standard GAN loss and Latent Distribution Matching constraint on the generator. The regularized autoencoder is combined with a GAN, with hyper-parameters controlling loss weights. The LDM term is minimized only for the generator, with a divergence measure between empirical and prior distributions in the latent space. The training algorithm and architecture of the proposed model are outlined in Algorithm 1 and Figure 1. In this section, the model is compared with four different models on various datasets. Mode collapse in GANs is challenging to evaluate, especially with natural images, but can be accurately assessed with synthetic data. Five models are compared on two synthetic datasets with different Gaussian distributions. The study compares models on synthetic datasets with multiple Gaussian distributions, showcasing challenging tasks due to separated modes. All models share similar network architectures for fair comparison, with specific details on the encoder, generator, and discriminator networks. The VAEGAN used differs slightly from the original paper, employing shallow network architectures and L2 loss on low-dimensional data. The study compares models on synthetic datasets with multiple Gaussian distributions, showcasing challenging tasks due to separated modes. The network architectures are shallow, and L2 loss is used on low-dimensional data. Two metrics are used to quantify mode collapse behavior: number of captured modes and percentage of high quality samples. Results show that the model captures the greatest number of modes and generates sharper distributions compared to other models. The study compares models on synthetic datasets with multiple Gaussian distributions, showcasing challenging tasks due to separated modes. While vanilla GAN generates high-quality samples, it suffers from severe mode missing problems. VEEGAN captures most modes but fails to generalize well. LDMGAN, proposed in the study, effectively alleviates mode collapse in GANs, outperforming VAEGAN. A large-scale grid search on MNIST dataset validates the effectiveness of LDMGAN in capturing modes and generating high-quality samples. Our LDMGAN model uses the same hyper-parameters as vanilla GAN, with the encoder being the \"inverse\" of the generator. A grid search similar to previous studies is conducted to determine the search range. A regular CNN classifier is trained on MNIST digits to estimate missing modes and sample qualities, using the MODE Score metric. Training each architecture for 50 epochs, 10K samples are drawn for evaluation to assess the resulting MODE Score distribution. Our model improves sample quality and diversity compared to GANs. We evaluate it on the stacked MNIST dataset with 1000 modes, generating a dataset of 50000 images for training using the same architecture for all models. The generator and discriminator networks follow DCGAN, while the encoder network is a simple two-layer MLP network. For BiGAN and VEEGAN, we concatenate the last convolutional layer of the discriminator. Our model, trained for 50 epochs, combines the last discriminator layer with the latent code for joint space discrimination. The number of modes is estimated using a classifier, and 26000 samples are drawn for evaluation. The model shows the highest mode recovery and best data distribution match compared to other methods on Stacked-MNIST, as indicated in Table 3. Our model achieves the highest quality and captures the most modes on Stacked-MNIST. FID scores are used to measure performance on CIFAR-10 and CelebA datasets. The model is trained for 50 epochs on CIFAR-10 and 30 epochs on CelebA with a prior input dimension of 100. Experiments are conducted in an unsupervised setting, evaluating FID scores per epoch and selecting the best model per run. FID scores are not reported for VEEGAN and BiGAN models due to architecture differences. Our model achieves the highest quality and captures the most modes on Stacked-MNIST. FID scores are used to measure performance on CIFAR-10 and CelebA datasets. The reported results in Table 4 are averaged over 5 runs, showing that our model has the best FID scores on both datasets. Random samples generated from different models are shown in Appendix A, and interpolation results on CelebA dataset are also provided to demonstrate that our model is not just memorizing the training data. Our proposed LDM-GAN model, trained on CelebA dataset, introduces a Latent Distribution Matching constraint to address mode collapse effectively. It includes a novel regularization constraint and a new regularized autoencoder for mapping data distribution to a gaussian. Extensive experiments show stability and improved FID scores on CIFAR-10 and CelebA datasets compared to baseline models. Our proposed LDM-GAN model, trained on CelebA dataset, introduces a Latent Distribution Matching constraint to address mode collapse effectively. It improves GAN's stability and diversity, achieving better FID scores on CIFAR-10 and CelebA datasets compared to baseline models."
}