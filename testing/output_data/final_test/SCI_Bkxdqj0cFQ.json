{
    "title": "Bkxdqj0cFQ",
    "content": "This paper utilizes Background Check technique to help two-class neural networks detect adversarial examples by measuring the one-dimensional difference between logit values. The method achieves high recall on image sets with large perturbation vectors, unlike existing literature on adversarial attacks. Unlike other deep learning methods, this approach does not require knowledge of attack parameters during training. Adversarial attacks, introduced by BID23 in image classification, manipulate data to deceive neural networks, raising concerns for security critical applications. Various attack methods have been developed, like the fast gradient sign method (FGSM) by BID5. Adversarial attacks can be categorized as white box or black box, with examples leading to false positives or false negatives in image classification. Adversarial attacks in image classification can be nonsensical or clear cut, targeted or non-targeted. Defenses against these attacks include deep learning and distributional techniques. BID6 and BID0 have developed methods using statistical tests and Gaussian Processes to detect adversarial examples. The authors explore methods to enhance neural networks' resistance to adversarial attacks, including using sub-units of deep learning architectures like BID14. Calibration techniques, such as Background Check, are employed to produce calibrated probabilities for adversarial data in unobserved regions. Reliable probability estimates are measured through calibration and refinement loss using various procedures like binning and regression. Various calibration procedures like binning, logistic regression, isotonic regression, and softmax are used. BID8 shows that the logistic function is optimal for Gaussian densities with unit variance, while softmax extends this to multi-variate Gaussian densities. Calibration of neural network models has been done using Temperature Scaling by BID7 to address increasing calibration loss. A method is demonstrated to map adversarial logit scores to reliable probability estimates, crucial for resisting adversarial attacks. In this work, a method is demonstrated that uses Background Check to identify adversarial attacks. A classifier is well calibrated if the proportion of outcomes given probability p occurs p fraction of the time. Perfect calibration is defined as the expectation where random variables X, Y denote the features and class label of a uniformly randomly drawn instance from the dataset. Calibration curves visualize the calibration performance of a classifier. The observed frequency of an event is compared to the predicted frequency to create calibration curves. These curves plot the observed relative frequency against the predicted probability for all test data. Refinement loss measures the difference between a probability estimate and zero or one, providing an indication of spread in the frequency distribution. BID3 suggests measuring refinement loss to improve calibration estimates. The Brier Score, defined by BID10, consists of calibration and refinement loss. Calibration measures the difference between model score and observed frequency, while refinement loss measures the spread in the frequency distribution. Beta calibration, based on the beta distribution, can calibrate scores from models like naive bayes. In the context of adversarial attacks, models like naive bayes bias scores towards extremities when feature independence is not met. Adversarial examples are constructed through optimization problems with constraints to force mis-classification with minimal perturbation vectors. Different distance metrics, including L p metrics and the PASS score BID22, are used to measure perturbation size. Adversarial attack methods like L-BFGS from BID23 and FGSM aim to find perturbations that mis-classify instances. Adversarial attacks use optimization methods like FGSM to create perturbation vectors that mis-classify instances. BIM attack iteratively applies smaller noise vectors before clipping the image to stay within pixel values. JSMA is a forward derivative approach for crafting adversarial examples. JSMA uses a Jacobian matrix to create an adversarial saliency map, while DeepFool finds images close to decision boundaries. Background Check calibrates neural networks to defend against attacks by mapping logit scores to probabilities. Background Check introduces a framework to classify regions where adversarial data may exist by providing two values to represent a single probability value of a data instance. This approach helps avoid ambiguity and overloading the meaning of a single number representing probability, by considering distance from data density and certainty of a particular class. Additionally, an outlier background class is introduced to represent regions of space. An outlier background class, b, is introduced to represent sparse or non-existent data regions, while a foreground class, f, represents dense data regions. Instances belong to either f or b with certainty measures P (b|x) = 0 and P (f |x) = 1. The reliability factor r(x) is defined by the ratio of conditional measures. If r(x) \u2264 1, the classification is b; if r(x) > 1, the classification is f. P (x, f ) and P (x, b) are foreground and background densities. The relative foreground and background densities, q f (x) and q b (x), are defined to determine the proportion of f or b at a given point in space. Four inductive biases are used to construct q b (x), with the third bias being utilized in this work. The biases include the relationship between q b (x) and q f (x), the monotonicity of the function \u00b5, and an affine bias represented by \u00b5(x) = ax + b. The Background Check method implements inductive biases to shape q b in two ways: BCD involves generating artificial background instances around foreground data and training a binary classifier, while BCF fits a one-class model on foreground data to obtain q f and then uses an inductive bias to obtain q b. The familiarity factor r(x) allows computation of posterior probabilities P(b|x) and P(f|x). The proposed method in this work uses the BCF method for its speed in high dimensional spaces, with the measure being the one-dimensional L1 difference between elements of the logit vector. A one-class model is used to fit qf(x) with a gamma function optimized using maximum likelihood estimation. The link from qf(x) to qb(x) is established using a third inductive bias with specific domain knowledge. One neural network is trained for each attack type, parameter combination, and dataset using the Adam optimizer with a batch size of 256. The neural network is trained with the Adam optimizer, using a batch size of 256 and a learning rate of 0.001. Adversarial images are generated using different attack methods, showing distinct separation from training and test data. The neural network is trained with the Adam optimizer, using a batch size of 256 and a learning rate of 0.001. Adversarial images are generated using different attack methods, showing distinct separation from training and test data. Adversarial images have larger logit differences compared to training/test data. Regularization techniques such as L2 regularization and dropout were applied to the networks. Seven different adversarial attacks were tested against the networks. Adversarial attacks were tested on networks using various parameter combinations, generating images from test data. The attacks were white-box attacks on networks with a final softmax layer. Despite different effects on images, the networks achieved over 80% average recall on the validation set after 300 training iterations. Parameters were chosen based on image classes, with some attacks only affecting certain classes. The study evaluated the impact of perturbation vectors on average recall in adversarial attacks. Results showed that large perturbation vectors led to an 11.6% reduction in average recall, while very small perturbation vectors resulted in a 35.7% reduction. Typical perturbation vectors had a 6.9% reduction in average recall. Overall, average recall decreased in all cases except for two. The adversarial TPR generally increases with larger perturbation vectors. Three out of five large perturbation vectors achieve 100% TPR on the adversarial class. All models have higher average recall than the baseline. Histograms show adversarial examples clustered near the decision boundary or scattered among training/test data. The JSMA and DeepFool attacks found logit differences smaller than the test and training logits, yet still high enough to yield a significant confidence level when applied to the softmax function. The Madry, Momentum, and BIM attacks produced logit differences far higher than the test and training logit differences, with some attacks finding logit differences within the test and training distributions. The adversarial images with scores represented by the green histogram are very far away from the training and test data, which means that Background Check will separate them well. The central example of a successful adversarial image is noise, with a large perturbation applied. The confusion matrices show the neural network's performance with and without Background Check. The histograms in the bottom of each image display the relative frequency of examples in training, test, and adversarial classes based on score differences between logit vectors. The attack had equivalent ease generating adversarial images from either class. The confusion matrices show a reduction in average recall due to false negatives for both classes. Adversarial attacks find examples in regions where test and training data do not exist, improving classifier performance. The study discusses the need to defend against noisy and ambiguous images that can occur in the real world. Future research should focus on generative probability estimation and calibration. Background Check, a metric for adversarial defenses, could be scaled up to handle ten classes and applied to each layer of a neural network without affecting generalization. Performance of Background Check can be measured using metrics like energy distance. The study introduces a novel approach to defending neural networks against adversarial attacks by intersecting machine learning fields of calibration and adversarial defenses using Background Check principles. This approach can detect and assign adversarial attacks resulting from large perturbations to an adversarial class. Adversarial attacks can be detected and assigned to an adversarial class based on perturbations, with larger perturbations making attacks easier to detect."
}