{
    "title": "ryxnY3NYPS",
    "content": "The ability to forecast diverse possible future behaviors of an agent is crucial for safety-critical perception systems like autonomous vehicles. Generative models like variational autoencoders have been effective in learning future trajectories, but randomly drawn samples may lack diversity. This work proposes a diversity sampling function to address this issue. The proposed method aims to learn a diversity sampling function (DSF) to generate diverse future trajectories by mapping context features to latent codes decoded by a generative model. The DSF parameters are optimized using a diversity loss based on determinantal point processes (DPP), resulting in a set of diverse yet likely trajectories. This approach showcases the diversity of trajectories on 2D trajectory data. The challenging aspect of trajectory forecasting lies in the uncertainty and multi-modal nature of future trajectories for vehicles and humans. It is crucial to consider a diverse set of possible future paths to enable preemptive actions in safety-critical applications. A generative approach is needed to fully capture the multimodal distribution of future trajectories. Variational autoencoders (VAEs) are used to capture all modes of a data distribution, but random samples may lack diversity due to stochastic sampling and bias towards major modes. To address this, a diversity sampling function (DSF) is proposed to reliably generate diverse trajectory samples. The proposed Diversity Sampling Function (DSF) is a deterministic function that maps forecasting context features to latent codes, which are then decoded into future trajectory samples. To optimize the DSF, a diversity loss based on a determinantal point process (DPP) is formulated to evaluate the diversity of the samples. The DPP models negative correlations between samples, making it ideal for capturing diversity within the set of trajectory samples. The proposed approach involves using a Diversity Sampling Function (DSF) to generate diverse future trajectory samples by optimizing a determinantal point process (DPP) based diversity measure. Experimental results show that this method outperforms existing generative models in producing diverse sets of future trajectories. Recent work in trajectory forecasting has focused on generating multi-modal trajectories for pedestrians and vehicles using generative models. Some approaches use deterministic modeling to forecast one future trajectory, while others aim to predict distributions over trajectories. For example, Rhinehart et al. proposed a model that accurately forecasts multi-modal trajectories for vehicles, while Soo Park et al. use egocentric videos to predict future trajectories. Additionally, some research has explored forecasting higher dimensional trajectories, such as 3D fullbody pose sequences of human motions. Some existing work focuses on deterministic forecasting of future motions from 3D poses, images, or videos, while others use probabilistic approaches like conditional variational autoencoders to generate multiple future motions. In contrast, our approach can produce a diverse set of future motions with a limited number of samples, aiming for diversity in computer vision and machine learning problems. DPPs are efficient probabilistic models that measure diversity and quality in subsets, making them suitable for diverse subset selection. They have been used in various applications such as document summarization, recommendation systems, object detection, and grasp clustering. Previous work has also applied DPPs to mitigate mode collapse in generative adversarial networks. Our approach uses the cardinality of DPPs as a diversity loss for deep neural networks in multi-modal trajectory forecasting. Unlike previous work, we apply cardinality as an objective in generative models, aiming for stability during stochastic gradient descent. Variational autoencoders are commonly used for trajectory forecasting due to their ability to capture diverse future trajectories. Variational autoencoders (VAEs) are utilized for trajectory forecasting as they can effectively capture diverse future trajectories by mapping data points to latent codes. VAEs model the joint distribution of data samples and their latent codes, using variational inference to approximate the posterior distribution. During training, VAEs optimize the recognition model and likelihood model to maximize the evidence lower bound (ELBO). Our core technical innovation involves learning a diversity sampling function (DSF) to generate a diverse set of future trajectories in multi-modal trajectory forecasting. We use determinantal point processes (DPPs) to evaluate and promote diversity within a set of trajectories. DPPs make the inclusion of similar items less likely, enhancing the diversity of the generated trajectories. A determinantal point process (DPP) is a probability measure on a ground set Y, where a random subset Y drawn according to P is diverse due to the orthogonal features, spanning a larger volume. The DPP kernel is constructed using a similarity matrix S, promoting set diversity by making similar items less likely to be included. To evaluate and optimize diversity in future trajectories, a quality vector and similarity matrix contribute to the DPP probability of a subset. DPPs are chosen for their ability to capture global diversity and quality, essential for safety-critical applications requiring diverse outcomes. A diversity sampling function is proposed to address this need. To address the need for diverse future trajectory samples, a diversity sampling function (DSF) is proposed. The DSF generates deterministic trajectory samples by utilizing a conditional variational autoencoder (cVAE) to decode latent codes into trajectories. These trajectory samples are evaluated with a DPP-based diversity loss to optimize for diversity. The future trajectory model is learned with a cVAE using context information to infer the trajectory. In order to generate diverse future trajectory samples, a generative trajectory forecasting model p \u03b8 (x|\u03c8) is learned using a cVAE. The cVAE maps data x to its latent code z and reconstructs the data, capturing all modes of the data distribution. The encoder and decoder are optimized to maximize the variational lower bound using multivariate Gaussians. The encoder and decoder in the cVAE are neural networks that output parameters for the posterior distribution and reconstructed future trajectory. The objective function is based on Gaussian parameterization, with a weighting factor \u03b2. Training details and sampling from the model are provided. The cVAE model utilizes neural networks for encoding and decoding, with a focus on Gaussian parameterization and a weighting factor \u03b2. To address diversity issues in sampling, a new strategy is proposed to generate a varied set of trajectory samples covering major and minor modes effectively. The proposed diversity sampling function (DSF) maps context to latent codes, which are then decoded into a diverse set of future trajectories. A diversity loss based on a Determinantal Point Process (DPP) is used to optimize the DSF parameters. The DPP kernel is defined using a similarity matrix and quality vector, leading to the formulation of the diversity loss. The DPP kernel is defined as L = Diag(r) \u00b7 S \u00b7 Diag(r), where r represents trajectory quality and S measures similarity between trajectories. The kernel is a function of \u03b3 and ensures 0 \u2264 S ij \u2264 1 and S ii = 1. Quality is determined by a metric that treats all modes equally, unlike the similarity metric. The quality of each sample is measured in the latent space and is defined as treating samples inside a sphere equally, penalizing samples outside the sphere. The radius of the sphere is determined by where a certain percentage of Gaussian samples lie within, and a hyperparameter \u03c9 is set to 1 during training. At test time, a larger \u03c9 can be used to encourage the selection of more items by the DPP. The hyperparameter \u03c1 (or R) allows for a trade-off between diversity and quality in selecting samples from the ground set Y. A small R prioritizes high likelihood samples, resulting in less diversity, while a large R leads to highly diverse but less likely samples. The choice of R should be application dependent, with autonomous vehicles needing to consider diverse and less likely scenarios for robustness. After obtaining diverse samples, quality scores can be reassigned based on likelihood to prioritize more likely samples. To optimize the DSF S \u03b3 (\u03c8), a diversity loss is needed to measure the diversity of the trajectory ground set Y. The negative log likelihood heavily penalizes repeated items, causing numerical issues in optimization. Instead, the expected cardinality of the DPP is a better measure for the diversity of Y. The diversity of the trajectory ground set Y is measured using the expected cardinality of the DPP. The diversity loss is defined as L diverse (\u03b3) = \u2212tr I \u2212 (L(\u03b3) + I) \u22121, and the training procedure for S \u03b3 (\u03c8) is outlined in Alg. 1. At test time, the DSF S \u03b3 (\u03c8) is used to generate a diverse set of trajectories without repetition. The diversity of trajectory sets is measured using DPPs. MAP inference is used to find the most diverse subset by maximizing log-probability. A greedy algorithm is employed for submodular maximization. Experiments focus on comparing diversity sampling function with cVAE and other baselines, performance on balanced and imbalanced data, and generalizability. In real data, trajectory contexts vary due to noise, posing challenges for evaluation. Each context usually has one future trajectory sample, limiting evaluation of multi-modal distributions. Contexts with similar semantics should share future trajectories. To enable multi-modal evaluation, multiple future trajectories are collected for each example by clustering similar contexts. New metrics ASD and FSD are introduced to assess similarity between forecasted samples, addressing the issue of repeated samples in existing metrics. The Diversity Sampler Function (DSF) is compared with baselines such as cVAE, MCL, R2P2, and cGAN, using similar networks and hyperparameter search for fair comparisons. These methods aim to improve modeling of multi-modal distributions for vehicle trajectories. The study compares the Diversity Sampler Function (DSF) with other methods like cVAE, MCL, R2P2, and cGAN using hyperparameter search for fair comparisons. Synthetic data is used to evaluate performance, including balanced and imbalanced data scenarios in a 2D traffic scene simulation. Trajectory data is synthesized by simulating vehicle behavior and adding Gaussian noise. The study evaluates the Diversity Sampler Function (DSF) against other methods in a 2D traffic scene simulation using synthetic data. DSF outperforms baselines in all metrics for both balanced and imbalanced data, generating diverse and less repetitive trajectories. Results show improved performance with a sampling budget of N=10 for human motion forecasting. In a study comparing the Diversity Sampler Function (DSF) to other methods in a 2D traffic scene simulation, DSF outperformed baselines in all metrics for both balanced and imbalanced data. The study also applied the method to forecast future human motions using motion capture data, achieving superior results compared to other methods. In a study comparing the Diversity Sampler Function (DSF) to other methods in a 2D traffic scene simulation, DSF outperformed baselines in all metrics for both balanced and imbalanced data. The study also applied the method to forecast future human motions using motion capture data, achieving superior results compared to other methods. Our method achieves significant improvement with a large sampling budget (N = 50) and generates more diverse future human motions than baselines. Additional experiments on the Human3.6M dataset evaluate the generalization ability of our approach, predicting future motion based on observed motion. New baselines, including variants of DSF and cVAE, validate design choices of our method. The study compares new baselines DSF-NLL and DSF-COS with DSF and cVAE in terms of diversity and quality of trajectory samples. DSF-COS shows high diversity but poor quality, while DSF-NLL performs better. Results are presented for Human3.6M dataset with N=10 and N=50 samples. Our method achieves better diversity (ASD and FSD) and similar ADE and FDE for small sample sizes (N = 10) compared to DSF-NLL. However, for larger sample sizes (N = 50), NLL becomes unstable, matching the intuition that it struggles with similar samples. The baseline cVAE-LDPP performs worse than DSF in all metrics, possibly due to a lack of diversity in the latent space. The proposed forecasting approach utilizes a DSF to optimize a generative model's sample space, learning diversity with a DPP-based measure. This method generates diverse trajectories, outperforming state-of-the-art forecasting approaches in vehicle and human motion diversity. The forecasting approach utilizes a DSF to optimize a generative model's sample space, learning diversity with a DPP-based measure. It transforms noises to latent samples and decodes them into reconstructed trajectories using network architectures for synthetic data and human motion. The number of latent dimensions is 2, and ReLU activation is used for all networks. The future trajectory consists of 3 future positions of the vehicle, and the context contains past trajectories of 2 time steps and an obstacle. The forecasting approach utilizes a DSF to optimize a generative model's sample space, learning diversity with a DPP-based measure. It transforms noises to latent samples and decodes them into reconstructed trajectories using network architectures for synthetic data and human motion. The number of latent dimensions is 2, and ReLU activation is used for all networks. The future trajectory consists of 3 future positions of the vehicle, and the context contains past trajectories of 2 time steps and an obstacle. The encoder uses a CNN with three 32-channel convolutional layers to process the obstacle map. The obtained CNN features are concatenated with flattened x and h into a unified feature, which is fed into an MLP. The MLP has one 128-dim hidden layer and two heads outputting the mean \u00b5 and variance \u03c3 of the latent distribution. The decoder concatenates the CNN feature with the latent code z and flattened h into a unified feature, passing it through an MLP with one 128-dim hidden layer to output the reconstructed future trajectory x. The DSF optimizes a generative model's sample space for diversity. It uses MLPs and Bi-LSTMs to encode future trajectories and past poses, obtaining latent codes for future poses. The network architecture includes hidden layers and heads to output mean and variance of the latent distribution. The decoder reuses the Bi-LSTM for context reconstruction. The decoder in the cVAE model uses a Bi-LSTM for context h and a 128-dim forward LSTM to decode future trajectories. It takes previous pose, latent code z, and temporal features as input to generate reconstructed poses. The DSF utilizes a different 128-dim Bi-LSTM to obtain temporal features for h and an MLP to produce latent codes. The cVAE model is trained with a single sample from the posterior and optimized using Adam, with a specific weighting factor for the KL term. The model uses Adam to optimize the encoder and decoder, with a learning rate of 1e-4 and a mini batch size of 32 for synthetic data. The DSF is trained for 500 epochs on synthetic data and 100 epochs on human motion, with a scale factor of 1 for synthetic data and 1e-2 for human motions. The grouping threshold \u03b5 is set to 0.1 for synthetic data and 0.5 for human motion to build the ground truth future trajectory set. In a study by Pavllo et al., 2019, motion sequences were converted into 3D joint positions using a 17-joint skeleton. The cVAE model was trained on five subjects and tested on two subjects, with a latent dimension of 128 and a weighting factor \u03b2 of 0.1. Training examples were sampled every epoch, with optimization for 500 epochs using Adam and a learning rate of 1e-4. The batch size was set to 64, and the scale factor k for the similarity matrix was 5. The DSF was trained with a batch size of 64, sampling 1000 training examples every epoch, and optimized for 20 epochs using Adam and a learning rate of 1e-3. The grouping threshold \u03b5 for computing metrics was set to 0.1. The grouping threshold \u03b5 for computing metrics was set to 0.1. Additional qualitative results for human motion forecasting are shown in Fig. 7, with the quality and diversity of forecasted motions best seen in video 2. Figure 7 displays visualization for human motion forecasting, showing starting and final poses of 10 forecasted motion samples."
}