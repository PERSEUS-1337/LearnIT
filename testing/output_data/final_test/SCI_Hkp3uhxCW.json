{
    "title": "Hkp3uhxCW",
    "content": "In this work, a variational Bayes scheme for Recurrent Neural Networks is explored. A simple adaptation of truncated backpropagation through time improves uncertainty estimates and regularisation with minimal extra computational cost. A novel posterior approximation incorporating local gradient information further enhances the performance of Bayesian RNNs. This technique can also be applied to train Bayesian neural networks more broadly. Bayesian RNNs outperform traditional RNNs on language modelling and image captioning tasks. They introduce uncertainty and regularization through Bayesian methods, allowing the network to express uncertainty via its parameters. This approach integrates a prior to average across models during training, providing a regularization effect. Recent approaches justify dropout and weight decay as variational inference schemes. Recent work has explored justifying dropout and weight decay as variational inference schemes for RNNs. A new approach based on Bayes by Backprop has been derived, showing promising results on large scale problems. This method alters truncated backpropagation through time to estimate the posterior distribution on the weights of the RNN, leading to a cost function with an information theoretic justification. The performance of RNN models can be improved by adapting the variational posterior to a batch of data using gradients. This technique reduces variance and can be applied to other Bayesian models. The contributions of the work include efficiently applying Bayes by Backprop to RNNs, developing a novel technique to reduce variance, and improving performance on widely studied problems. Bayes by Backprop BID14 is a variational inference scheme for learning the posterior distribution on the weights of a neural network. The network is trained by minimizing the variational free energy, equivalent to maximizing the log-likelihood. The variational free energy maximizes log-likelihood subject to a KL complexity term on network parameters, acting as a regularizer. In the Gaussian case, the KL term acts as weight decay on mean parameters. Bayes by Backprop has been successful in training feedforward models and aiding exploration in reinforcement learning, but has not been applied to recurrent neural networks yet. The core of an RNN is a neural network mapping the RNN state and input observation to a new RNN state. The LSTM core equations for an RNN are detailed in the supplemental material. RNNs can be trained on sequences by backpropagation through unrolling into a feedforward network. RNN parameters are learned similarly to feedforward neural networks, with shared weights at each unrolled step receiving gradient contributions. Applying Bayes by Backprop to RNNs has not been explored yet. Bayes by Backprop (BBB) can be applied to RNNs by sampling parameters from a distribution. The variational free energy for an RNN on a sequence of length T involves likelihood calculations and gradient computations. Parameters are sampled from a normal distribution, and gradients are calculated with respect to the network parameters. The variational free energy for an RNN on a sequence of length T involves likelihood calculations and gradient computations. Parameters are sampled from a normal distribution, and gradients are calculated with respect to the network parameters. The gradients of log N (\u03b8|\u00b5, \u03c3 2 ) \u2212 log p(\u03b8) w.r.t. \u03b8, \u00b5 and \u03c3 are used to update \u00b5 and \u03c3. Training RNNs typically involves mini-batches of truncated sequences to reduce variance in gradients. The free energy of mini-batch truncated sequences can be calculated by sampling fresh parameters for each minibatch. Enhancing the variational posterior with additional information reduces variance in the learning process. Similar to Variational Auto Encoders, a powerful distribution is proposed to improve gradient estimates for the likelihood function. The variational posterior q(\u03b8|(x, y)) is constructed using a minibatch of data (x, y) from the training set to yield a single parameter vector \u03b8 per minibatch, enhancing stability in optimization. This method is supported by empirical evidence and work on VAEs, addressing challenges posed by the high dimensionality of \u03b8 \u2208 R d. The variational posterior q(\u03b8|(x, y)) is parameterized as a linear combination of \u03b8 and g \u03b8 = \u2212\u2207 \u03b8 log p(y|\u03b8, x), with \u00b5, \u03c3 \u2208 R d, and q(\u03d5) = N (\u03d5|\u00b5, \u03c3). The hierarchical posterior is defined with a free parameter \u03b7 \u2208 R d and a scalar hyper-parameter \u03c3 0.\u03b7 represents a per-parameter learning rate in the model. The variational posterior q(\u03b8|(x, y)) is parameterized with \u03b7 as a per-parameter learning rate. Ancestral sampling is used to optimize the loss where \u00b5, \u03c3, \u03b7 are model parameters. Algorithm 1 outlines the learning process with minibatch sampling and gradient updates to improve the lower bound on data likelihood. The effectiveness of the posterior over parameters is justified as long as the curvature of log p(y|\u03b8, x) is maintained. The proposed parameters in eq. 7 control the tradeoff between curvature improvement and KL loss in the variational posterior. Two options for inference under posterior sharpening are discussed, with one involving an extra gradient computation and training speed penalty. In the case of RNNs, BPTT is used for gradient computation in training loss derivation. The training loss function for posterior sharpening involves a variational approximation to the marginal likelihood p(x) that factorises hierarchically. A hierarchical prior for the parameters is assumed, and a variational posterior is chosen that conditions upon x. The expected lower bound on p(x) is derived, with similarities to line search techniques noted. Our approach introduces a variational posterior with the reparametrization trick/perturbation analysis gradient for line search, which can be seen as a trust region method. Performance gains are significant due to short term correlations in the data. Our method is related to learning to optimize, where a learning rate is learned to improve updates compared to AdaGrad or Adam. Applying Bayesian methods to neural networks has a long history, with various maximum a posteriori schemes proposed for training. BID18 introduced variational methods for compressing neural network weights as a regularizer, while BID20 suggested an MDL loss for penalizing non-robust weights in single layer networks. Several methods have been proposed for training neural networks using Bayesian methods, including Laplace approximation, hybrid Monte Carlo, and variational inference schemes. Graves (2016) derived a variational algorithm for mixture posteriors, while others have suggested dropout and Gaussian dropout as approximate variational inference schemes. Kingma et al. (2015) proposed a biased variational scheme. Our work extends previous variational schemes by using an unbiased gradient estimator and a novel posterior approximation, avoiding the need to approximate the Fisher matrix. Variational methods tend to underestimate posterior uncertainty, while expectation propagation methods often overestimate it. Several papers explore applying expectation propagation to neural networks, with different approaches proposed. Adams (2015) proposed multiple passes of assumed density filtering with early stopping for good performance on small data sets. BID16 developed a distributed expectation propagation scheme with SGLD as an inner loop. Recent studies have applied SGLD to neural networks, including LSTMs. The results of our method for language modeling and image caption generation tasks are presented, evaluated on the Penn Treebank benchmark for next word prediction using an RNN with LSTM cells and a special regularization technique. To demonstrate the effect of applying Bayes by Backprop (BBB) to a well-studied architecture, we tuned parameters on the prior distribution, learning rate, and weight initialization. The network weights were a scalar mixture of two Gaussian densities. Training involved using one sample from the posterior for estimating gradients. In the study, the researchers used one sample from the posterior for estimating gradients and computing the KL-divergence. They experimented with computing expected loss via Monte Carlo sampling or using the mean of the posterior distribution as network parameters. Results showed improvement with increased samples but not significantly better than using the mean. Comparison was made with LSTM dropout baseline and Variational LSTMs. Dynamic evaluation results were added with a learning rate of 0.1. In the study, researchers used posterior sharpening with a hierarchical prior for network parameters. They found that a small Gaussian perturbation performed well, with results not sensitive to variations. Posterior sharpening yielded upper bounds on perplexities, and variance reduction capabilities were tested. Standard BBB yielded 258 perplexity after one epoch, while the model with posterior sharpening showed improvement. The model with posterior sharpening outperformed Standard BBB with a perplexity of 227 on TAB2. Implementation on MNIST showed small speed ups. Varying model architecture can achieve lower perplexities on Penn Treebank task. Bayesian RNNs had reduced speed with na\u00efve implementation and posterior sharpening. Weight pruning effects were shown in FIG2. Weight pruning was conducted by removing weights based on their signal-to-noise ratio, with around 80% of weights being able to be dropped from the network without significantly impacting validation perplexity. Analysis on the patterns of dropped weights can be found in supplementary material. The Penn Treebank test set was used, with the reversed test set showing the last words of the standard test set in reverse order. The entropy of input sequences under a probabilistic model was defined, with further details provided in the text. The entropy gap \u2206H p is defined for a probabilistic model using the Penn Treebank test set and its reversed version. The reversed set is expected to have lower certainty for well-calibrated models. A comparison between two distributions shows that a better calibrated model will have a larger \u2206H p. The BBB model has a gap of about 0.67 nats/word when taking 10 samples. The model using MC Dropout BID11 is less calibrated with an entropy below 0.58 nats/word, but improves to below 0.62 nats/word when dropout is turned off. The BBB mean model has an entropy of 4.48 nats/word on the reversed set. Bayes by Backprop for RNNs was applied to image captioning using a pre-trained CNN to map images to an LSTM for predicting the next word in a sentence. The LSTM model was trained using Bayes by Backprop for image captioning, showing significant improvements in BLUE and CIDER scores compared to the Show and Tell model. Additionally, a random sample of captions differed between the baseline and BBB models. The Bayes by Backprop (BBB) technique was applied to RNNs, with further enhancements such as posterior sharpening. This resulted in superior performance compared to baseline models in language modelling and image captioning domains, with BBB RNNs generally outperforming the strong baseline. BBB RNNs show superior performance, regularization, and uncertainty properties compared to baseline models. They exhibit knowledge of their uncertainty, crucial for applications like self-driving cars and healthcare. Variational Bayes methods, with enhancements like posterior sharpening, are promising for RNN/LSTM models. Further research is being explored for wider adoption of these techniques. At step t, an input observation x t is used to update the RNN state to s t+1 using an LSTM core. The LSTM core consists of a state s t = (c t , h t ) with internal gates i t, f t, and o t modulating the inputs and outputs. Weight pruning was performed on the LSTM model for the Penn Treebank task, removing around 80% of the weights with minimal impact on validation perplexity. Patterns of dropped weights for an LSTM cell are shown in Figure 5. In Figure 5, pruning patterns for one LSTM cell with 650 units from a model with 80% of total weights dropped are displayed. White dots indicate dropped parameters, a horizontal white line signifies a row set to zero, and the last column shows the total number of weights removed per row."
}