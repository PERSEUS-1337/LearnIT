{
    "title": "HkCy2uqQM",
    "content": "Complex-valued neural networks have been increasingly used in recent years, outperforming real-valued neural networks in specialized architectures. They should be used when input data is complex or when the network architecture utilizes complex numbers. ComplexValued neural networks are commonly used in signal processing with a natural interpretation in the complex domain. The comparison between complex-valued and real-valued architectures is essential to ensure their function approximation ability. The number of parameters in complex-valued neural networks is a rarely studied aspect, with complex numbers increasing computational complexity but also assuming a specific structure between weights and input. The choice of activation function is crucial, with testing of 5 functions including identity, rectifier linear unit, and others. This paper explores the performance of complex-valued multi-layer perceptrons (MLP) with varying depth and width in consideration of the number of parameters and choice of activation function on benchmark classification tasks. The study tests 5 different activation functions and compares the number of real-valued parameters in complex and real cases for MLPs with the same number of units in each layer. The paper discusses complex-valued neural networks with fixed parameters or neurons per layer. It introduces the use of complex numbers in neural networks and presents activation functions and experimental results on various classification tasks. The study highlights the historical development of complex-valued neural networks and backpropagation algorithms. In the 1990s, multi-layer networks were proposed by authors BID2, BID10, and BID4. In the 2000s, complex neural networks, such as real-valued neural networks, were successfully applied to tasks involving complex-valued data. Complex convolutions have applications in image and signal processing, and recurrent networks with unitary weights were introduced to deep learning models. Complex-valued neural networks have been less popular than real-valued networks due to training difficulties and varying results in related tasks. The architecture design is less intuitive, stemming from activation function differentiability issues in the complex plane. Comparisons between real and complex-valued networks are often overlooked in publications. The text discusses the challenges and importance of exploring deep complex-valued neural networks compared to real-valued networks. It emphasizes the significance of activation functions in training and accuracy, highlighting the need to consider width, depth, and activation functions together in architecture design. The study aims to fill the gap by systematically evaluating multi-layered architectures for simple classification tasks in the complex domain. Neural networks in the complex domain use complex parameters. Training complex-valued neural networks involves differences in structure and training. The focus is on layers with complex-valued neurons in a multi-layer architecture. Complex-valued neurons are defined with complex weight and bias. Projection onto a complex weight matrix is used for complex-numbered embeddings. The activation and loss functions in complex-valued neural networks involve real or complex functions acting on complex variables. The choice of activation function can impact training difficulties, and an additional activation function may be used for interpreting output as probability. Activation and loss functions may not always be complex-differentiable. In complex-valued neural networks, activation and loss functions may not always be complex-differentiable. A complex function is holomorphic if it is entire and complex differentiable, satisfying the Cauchy-Riemann equations. This implies a stronger property than in the real-valued case. In complex-valued neural networks, functions may not always be complex-differentiable. A function can be nonholomorphic in z but still be analytic in x, y. To apply the chain rule to non-holomorphic functions, we use a different basis for partial derivatives based on Wirtinger calculus. This allows the chain rule to be applied to non-holomorphic functions for multiple complex variables. In complex-valued neural networks, the number of real parameters per layer is doubled due to the representation of complex numbers. The number of parameters reflects the network's capacity to approximate functions, with too many leading to overfitting and too few to underfitting. To compare architectures, the real-valued parameters per layer should be similar between the real and complex versions to ensure equal capacity and performance differences can be attributed to the model type. In complex-valued neural networks, the number of real parameters per layer is doubled. To compare architectures, the real-valued parameters per layer should be similar between the real and complex versions. The performance difference is due to added structure, not varying capacity. Choosing MLP architectures with an even number of hidden layers and alternating neurons addresses the comparison problem. In complex-valued neural networks, the number of real parameters per layer is doubled. To compare architectures, the real-valued parameters per layer should be similar between the real and complex versions. Choosing MLP architectures with an even number of hidden layers and alternating neurons addresses the comparison problem by working with a parameter budget. Complex numbers in neural networks add an extra structure compared to real-valued networks. Operations on complex numbers involve real and imaginary parts or magnitude and phase. This increases computational complexity, requiring more operations than real-valued networks. Doubling the number of real parameters per layer in complex-valued networks does not suffice to achieve the desired results. Complex-valued neural networks require a different approach compared to real-valued networks. Doubling the number of real parameters per layer is not enough to achieve the same results. The augmented representation of complex numbers as matrices adds complexity to the architecture design. The choice of non-linearity is crucial in both real and complex neural networks. The performance of a model is influenced by the structure of complex-valued parameters. The Liouville Theorem states that bounded holomorphic functions must be constant, so unbounded and non-holomorphic activation functions are needed for complex models. Different activation functions like identity, hyperbolic tangent, and rectifier linear unit are studied for their impact on performance. In the last layer, an activation function is applied before using softmax or sigmoid for a real loss. The squared magnitude is more efficient than the magnitude. Changing the activation function of the last layer allows for a more efficient implementation. Comparisons between real and complex-valued neural networks were made through various classification tasks and experiments. In experiments testing multi-layer perceptrons (MLP), different architectures were used with varying hidden layers and units per layer. The experiments were applied to various classification tasks such as Reuters topic classification, MNIST digit classification, CIFAR-10 Image classification, and CIFAR-100 image classification. Different activation functions were tested in each experiment. In Experiment 3, the Memory Network architecture was tested with complex-valued networks using different parameter budgets. The network utilized recurrent layers with complex weight matrices. Weight initialization was done as discussed in a previous publication, and models were trained over 100 epochs using Adam optimization. Categorical or binary cross entropy was used as the loss function, with specific activation functions for the last layer of the complex models. Results from experiments 1 and 2 show that complex and real neural networks perform similarly, with complex networks slightly underperforming. The best activation function for complex networks is relu applied separately to real and imaginary parts, while tanh and squared magnitude perform worse. Increasing the depth of the architecture significantly decreases the performance of complex networks. Increasing the width per layers outperforms increased depth in classification tasks for both real and complex neural networks. Complex-valued neural networks outperform real-valued ones in more difficult tasks despite having fewer parameters, showing a regularizing effect when combined with real-valued input. Complex networks are more sensitive to initialization compared to real networks. Complex-valued neural networks are more sensitive to initialization compared to real-valued networks and do not perform as well as expected. They should be used when data is naturally in the complex domain or can be mapped to complex numbers. The architecture needs to reflect the interaction of the real and imaginary parts for optimal performance. Test accuracy of multi-layer perceptron with k + 2 dense layers and 500,000 parameters on Reuters, CIFAR-10, and CIFAR-100 datasets. Different activation functions were tested with varying results. Memory Networks BID17 showed performance on the first three bAbI tasks. Memory Networks BID17 performed well on the first three bAbI tasks in both complex and real versions, selected from the best of 30 runs with each run having 100 epochs to converge."
}