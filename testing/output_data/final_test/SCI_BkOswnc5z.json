{
    "title": "BkOswnc5z",
    "content": "We introduce efficient algorithms for computing MinHash of probability distributions, suitable for sparse and dense data. The collision probability of these algorithms measures similarity of positive vectors. This measure is optimal for Locality Sensitive Hashing based on sampling and is a natural generalization of the Jaccard index. MinHashing BID0 is a popular algorithm for clustering and retrieval on large datasets, known for its simplicity and efficiency. MinHashing is a fundamental technique in document and graph clustering, providing stable representations of subsets of a set U under random permutations. The Jaccard index is used to measure similarity between subsets. Chum et al. introduced algorithms for incorporating weights in MinHash computations, allowing for hashing of objects other than sets. The Jaccard index is extended to non-negative vectors through the Jaccard Weighted (JW) algorithm, which improves efficiency and allows for arbitrary positive weights. JW is useful for L1 normalized vectors but unnatural for probability distributions. Converting sets to binary vectors, JW is equivalent to Jaccard index. However, switching from unweighted MinHash to JW-based MinHash decreases collision probabilities and JW is insensitive to important differences in probability distributions. The Jaccard Weighted (JW) algorithm extends the Jaccard index to non-negative vectors, allowing for arbitrary positive weights. However, JW is not suitable for probability distributions as it fails to measure similarity accurately when support differs. JW is not scale invariant and does not meet the desired properties for a replacement of the Jaccard index. The Jaccard Weighted (JW) algorithm extends the Jaccard index to non-negative vectors with arbitrary positive weights. It is not suitable for probability distributions due to inaccuracies when support differs and lack of scale invariance. J W treats vectors as \"weighted sets\" and is a new generalization of the Jaccard Index to positive vectors, called J P. The primary contribution of this work is to derive and analyze J P, a scale-invariant collision probability for probability distributions. J P is shown to be more useful than J W in many situations, especially for LSH based on sampling algorithms. It is proven that if a sampling algorithm exceeds J P on one pair, it must sacrifice collisions on a pair with higher J P. Empirical evidence suggests that J P has a tighter relationship to JensenShannon divergence and is more centered around the Jaccard index compared to J W. The text discusses the use of pseudo-random hash functions for retrieving similar documents under Jaccard-Winkler similarity. It explains the properties of exponential random variables and their monotonic transformations. The focus is on deriving and analyzing a scale-invariant collision probability, Jaccard Probability (J P), which is shown to be more useful than Jaccard-Winkler (J W) in certain situations. The text discusses the use of pseudo-random hash functions for retrieving similar documents under Jaccard-Winkler similarity. It explains the properties of exponential random variables and their transformations. The focus is on deriving a scale-invariant collision probability, Jaccard Probability (J P), which is more useful than Jaccard-Winkler (J W) in certain situations. P-MinHashes achieve J P as their pair collision probability, while W-MinHashes achieve J W. The text explores different interpretations and representations of J P and J W, including geometric views of the P-MinHash algorithm. The PMinHash algorithm constructs a function of the simplex using uniformly distributed random points on the unit k-simplex. Each point represents a probability distribution over k+1 elements. By connecting the point to the corners of the simplex, smaller simplices are created with volumes proportional to the coordinates of x. Sampling in this manner results in match probabilities proportional to the sum of intersections. The PMinHash algorithm constructs a function of the simplex using uniformly distributed random points on the unit k-simplex, creating smaller simplices with volumes proportional to the coordinates of x. Match probabilities are proportional to the sum of intersections of simplices sharing an external face. When using MinHashing with a key-value store, high collision probabilities are more efficient than low ones. The goal is to achieve the highest collision probability through sampling while maintaining discriminative collision probabilities. The joint distribution for two distributions x and y is chosen to maximize Pr[H(x) = H(y)]. The goal is to create a joint distribution of all possible distributions where collision probability is maximized, aiming for Pareto optimality. This requires no collision probability to exceed ours everywhere, following the Total Variation limit. The Total Variation limit requires no collision probability to exceed ours everywhere. To increase collision probability for one pair above its Jaccard index, collisions on a pair with even higher Jaccard index must be sacrificed. The Jaccard index is optimal on uniform distributions, with a short proof serving as a model for the general case. To analyze J P for all distributions, rearrange it to separate iteration indices within the max. Use quantifiers in the subscript to indicate a partial sum. Consider linear combinations of distributions with coefficients \u03b1 and \u03b2. For a given i, if every max chooses the same side, J P (x, y) i = min(x i , y i ). Merge elements of x and y to form x \u2032 , y \u2032. Repeat merging process until all elements are merged. This allows for more effective work with distributions. The lemma provides the algebra needed for the main proof, focusing on optimizing the collision probability between x, y, and z distributions. By rearranging indices and making strategic shifts, the optimality of J P is proven, showing the effectiveness of the method for sampling from discrete distributions. The lemma proves the optimality of J P for sampling from discrete distributions, showing that no method can have collision probabilities dominating J P. J P is Pareto optimal, with specific conditions for exceeding it. The lemma proves the optimality of J P for sampling from discrete distributions, showing that no method can have collision probabilities dominating J P. J P is Pareto optimal, with specific conditions for exceeding it. The events are pairwise disjoint, their probabilities constrained by J P summing to 1. Assuming symmetry, we conclude J P (x, z) \u2265 J P (x, y) and symmetrically J P (y, z) \u2265 J P (x, y). FIG5 illustrates the proof intuitively. In this case, the lemma proves the optimality of J P for sampling from discrete distributions, with specific conditions for exceeding it. The algorithm constructs an adversarial z around green, using a tree structure and P-MinHash to prioritize collisions on an index i. The original algorithm can be generalized to find a sampling method that exceeds J P for some pairs but is below it for others. The algorithm constructs an adversarial z around green, using a tree structure and P-MinHash to prioritize collisions on an index i closer to the root node. The probability of a collision on i is min(x i , y i ). J W is not another Pareto optimum, it is dominated by J P. Shifting the mass to new elements does not affect J W but decreases J P to equal J W. To achieve the upper bound, consider reallocating the extra mass to maximize J P while holding J W constant. To maximize J P while keeping J W constant, distribute mass to disjoint elements in sets X and Y. Regardless of X, Y choice, J P reaches total variation limit. Express as linear combination of distributions with disjoint support. 1 - J P is a metric ranking distributions as more similar than J W if extra mass is on shared elements. 1 - J P is a proper metric on probability distributions. Theorem IV.6 states that 1 \u2212 J P is a proper metric on probability distributions over a finite set \u2126. The algorithm presented is suitable for sparse data like documents or graphs, being linear in the number of non-zeros. On dense data, rehashing each element for every distribution is inefficient. Algorithm 2, \"Global-Bound\" A* Sampling, provides a stable sample from the sample space \u2126. The \"Global-Bound\" A* Sampling algorithm provides a stable sample from the sample space \u2126 by using a shared random seed. By exploring the idea of biased hash selection towards the beginning of a stream, performance can be improved by searching only a prefix of the stream for the sample. The algorithm relies on the \"Global Bound\" algorithm from A* Sampling, with a fixed random seed, to determine collision probability. The key insight of A* Sampling is that the distribution of each variable can be computed based on the rank and previous variables in a stream of independent exponential random variables. The distribution of a new variable can be found by truncating a sorted list of exponential variables and shifting it. This allows for efficient parameter changes and finding the new minimum element by examining only a small prefix of the list. The running time of finding the new minimum in A* Sampling is equivalent to rejection sampling, giving it state-of-the-art performance for computing MinHash of dense data. Algorithm 2 can handle an unbounded list of random variables and is applicable to continuous distributions. It provides J P (\u00b5, \u03bd) for finite \u2126 by finding the minimum of -log U i /\u00b5 i. For infinite \u2126, J P (\u00b5, \u03bd) can be defined by replacing summations with integrals, although this presents challenges. The formula BID1 encounters difficulties when \u2126 is not a subset of R^n or when \u00b5 or \u03bd are singular. Instead, it is defined as a limit over finer finite partitions of \u2126. J P (\u00b5, \u03bd) is defined for finite \u2126 by finding the minimum of -log U i /\u00b5 i. For infinite \u2126, J P (\u00b5, \u03bd) can be defined by replacing summations with integrals. The lemma demonstrates that the infimum of Jenson-Shannon Divergence is achieved with the most refined partition of the set \u2126 itself. JSD has a closer relationship with JP than JW, with exact bounds for JW against JSD and approximate bounds for JP against JSD. The curve that seems to lower bound JSD against JP is violated on a small percentage of pairs. The text discusses the joint distribution of Jaccard indices J P and J W, showing their conditional distributions and the log of their ratios. It also presents a theorem on applying A* sampling to sample from two probability measures simultaneously, with a collision probability equal to J P. The procedure terminates at point p \u2208 \u2126 for both \u00b5 and \u03bd is J P (\u00b5, \u03bd). A* sampling is described in BID9. In-order A* is equivalent to hierarchical partition A* in BID9. A finite partition is used in the definition of J P (\u00b5, \u03bd). Precision/recall curves show retrieval using a key-value store. Each point represents outputting o independent sums of hashes for a collision probability of 1. The cost is dominated by o, showing trade-offs at similar cost. Representative parts Q \u2208 F are distributed as exponentials with rate \u03bb(Q) with common seed. U, V are two coupled A* processes restricted to F. The collision probability of two coupled A* processes U, V restricted to F is squeezed between J P (\u00b5, \u03bd)P(T) and J P (\u00b5, \u03bd) + \u03f5. By computing similarity scores for pairs of unigram term vectors from web documents, it was determined whether achieving J P as a collision probability is a useful goal. The Jensen-Shannon divergence (JSD) is used to form information-preserving clusters of items of equal importance. J P has a tighter relationship with JSD than J W. Tight bounds on JSD as a function of J W are given by J W's monotonic relationship with Total Variation. The Jensen-Shannon divergence (JSD) is used to form information-preserving clusters of items of equal importance. J P has a tighter relationship with JSD than J W. Bounds on JSD as a function of J W are given by J W's relationship with Total Variation. J P and J W are compared to the Jaccard index of the set of terms, showing different behaviors on uniform distributions. P-MinHash is less affected by J W compared to J P. P-MinHash is less disruptive as a replacement for unweighted MinHash, with parameters like number of hashes likely to function well. To reduce collision probability, multiple hashes can be summed to form keys, while raising collision probability requires outputting multiple independent keys. AND operations are cheap, while CPU and storage costs increase linearly with the number of OR operations. As a increases linearly, o must increase exponentially to maintain the sigmoid inflection point. This gives an advantage to algorithms with higher collision probabilities, favoring J P over J W. PMinHash shows better precision and recall for low JSD documents at a lower cost. Surprisingly, it also performs slightly better for high J W documents at low cost. This is due to the collision probability bound and the behavior of summing two hashes. The text discusses a new generalization of the Jaccard index, showing its optimal qualities on probability distributions. It demonstrates similarity to the Jensen-Shannon divergence and presents two MinHashing algorithms with equivalent performance to the current state of the art."
}