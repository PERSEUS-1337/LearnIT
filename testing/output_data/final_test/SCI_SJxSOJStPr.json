{
    "title": "SJxSOJStPr",
    "content": "In this work, an expansion-based approach for task-free continual learning is proposed. The model, CN-DPM, consists of neural network experts that handle subsets of data and expands the number of experts in a principled way under the Bayesian nonparametric framework. Extensive experiments demonstrate the effectiveness of the model in addressing catastrophic forgetting. Our model, CN-DPM, utilizes neural network experts to handle subsets of data and expands the number of experts in a principled way under the Bayesian nonparametric framework. It successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and generation. The model aims to imitate human's ability to learn from a non-iid stream of data without catastrophically forgetting previously learned knowledge. Task-free continual learning (CL) is more practical and demanding compared to traditional CL methods. Existing CL methods can be categorized into regularization, replay, and expansion methods to address catastrophic forgetting. Expansion methods differ by expanding the model architecture to accommodate new data. Our goal is to propose a novel expansion-based approach for task-free continual learning (CL) inspired by the Mixture of Experts (MoE). The model consists of experts, each in charge of a subset of data, with model expansion governed by a Bayesian nonparametric framework. This approach aims to prevent catastrophic forgetting by dynamically adjusting model complexity based on data, unlike traditional methods that fix complexity beforehand. The Continual Neural Dirichlet Process Mixture (CN-DPM) model is proposed as an expansion-based approach for task-free continual learning. It consists of neural network experts that prevent catastrophic forgetting by dynamically adjusting model complexity. The model can handle both generative and discriminative tasks and has shown success in benchmark experiments on MNIST, SVHN, and CIFAR 10/100 datasets. In experiments on MNIST, SVHN, and CIFAR 10/100, the model successfully performs various types of continual learning tasks, including image classification and generation. Different approaches in continual learning include regularization, replay, and expansion methods, each aiming to prevent catastrophic forgetting during training. Hybrid methods like Gradient Episodic Memory (GEM) combine aspects of replay and regularization. Expansion-based methods add new network components to learn new data, offering advantages compared to other approaches. Task-Free Continual Learning is an understudied area where task definition is not explicitly given at training time, and the data domain may shift without clear task boundaries. Existing works in this area are based on regularization, replay, and a combination of both. Aljundi et al. (2019a) extended MAS by incorporating heuristics to update importance weights without task definitions. In their subsequent work, Aljundi et al. (2019b) enhanced the memory management algorithm of GEM to minimize catastrophic forgetting in unsupervised learning. Rao et al. (2019) also addressed model expansion and short-term memory but relied on generative replay to prevent forgetting. Their approach combines replay and expansion, differing from our method. Additionally, a brief overview of the Dirichlet process mixture (DPM) model and a variational method for approximating the posterior of DPM models is provided. The Dirichlet Process Mixture (DPM) model is used for clustering when the number of clusters is unknown. It involves sampling latent variables from a distribution and is parameterized by a concentration parameter \u03b1 and a base distribution G 0. The expected number of clusters is proportional to \u03b1, and data points belonging to the same cluster have the same latent variable. An alternative formulation uses variable zn to indicate data cluster membership \u03b8n = \u03c6zn where \u03c6k is the cluster parameter. Approximation of the Posterior of DPM Models is done using Sequential Variational Approximation (SVA) to determine \u03c1n and \u03bdk sequentially. \u03c1n,k represents the probability of data n belonging to cluster k and is called responsibility. SVA adds a new component when \u03c1K+1 exceeds a threshold. DPM for Discriminative Tasks involves using stochastic gradient descent to find the MAP estimation instead of calculating the whole distribution. It can be extended to tasks where the goal is to learn the conditional distribution p(y|x). The joint distribution modeled by each component can be decomposed, and similar tasks can be grouped into a super-task for shared parameter initialization. Our method implements a DPM model for general task-free CL, where tasks and task descriptions are not available at training or test time. It can be extended to cases where the data stream cannot be split into separate tasks. Existing methods require task definition at training or even at test time, limiting their applicability beyond specific CL settings. Our method introduces a novel expansion approach for continual learning that automatically determines when to expand and which component to use. It deals with generative tasks by generalizing them into discriminative ones in a stream of data involving different tasks. The goal is to learn the mixture distribution in an online manner, where regularization and replay methods update a single component to fit the overall distribution. Expansion-based continual learning (CL) methods aim to preserve knowledge of previous tasks by approximating task distributions with new parameters while keeping existing ones intact. In discriminative tasks, CL models the overall conditional distribution, which is a mixture of task-wise conditional distributions. However, a challenge arises in choosing the right expert given input data, as existing methods assume explicit task descriptors are provided, which is not always the case in human-like learning scenarios. In expansion-based continual learning, the challenge lies in selecting the correct expert without explicit task descriptors. A gating mechanism is needed to infer which expert should process the input data, leading to a mixture of experts model. To avoid catastrophic forgetting, coupling each expert with a generative model can be a solution, allowing for a gating mechanism without memory loss. In task-free continual learning, a Bayesian nonparametric framework like the Dirichlet process mixture (DPM) model is proposed to handle the unknown number of experts. Each sample is evaluated by every expert to calculate the assigned data count Nk per expert during training. The Continual Neural Dirichlet Process Mixture (CN-DPM) model involves experts evaluating samples to calculate responsibility, storing data in short-term memory (STM) if no expert is responsible, and creating new experts from STM data. It utilizes a generative model to compute joint distribution and approximates the posterior in an online setting. The model consists of experts with both discriminative and generative models for task-free continual learning. The Continual Neural Dirichlet Process Mixture (CN-DPM) model involves experts with discriminative and generative models for task-free continual learning. The model uses a classifier and density estimator for each expert, with the number of experts expanding via the DPM framework. Training involves sequentially assigning samples to existing or new experts based on arrival. The Continual Neural Dirichlet Process Mixture (CN-DPM) model utilizes experts with discriminative and generative models for task-free continual learning. When a new sample arrives, it is assigned to an existing expert or a new expert is created. The responsibility is computed based on existing experts, and parameters are updated accordingly. Creating a new expert immediately is avoided to prevent overfitting, as training with a single example can lead to severe overfitting. To prevent overfitting, the Continual Neural Dirichlet Process Mixture (CN-DPM) model employs short-term memory (STM) to collect data before creating a new expert. Data points classified as new are stored in the STM until it reaches capacity. A \"sleep phase\" follows, where a new expert is trained with the STM data. The STM is then emptied, and the new expert is added to the pool for learning during the \"wake phase.\" This approach assumes that STM data belong to the same expert, which is found to be acceptable in many settings. The training procedure is detailed in Algorithm 1. At test time, inference is made on p(y|x). To improve efficiency, a method is proposed to share parameters between experts in the algorithm. When adding a new expert, lateral connections are made to previous expert features to reduce parameter redundancy. Gradient blocking from the new expert prevents forgetting in existing experts, allowing for positive knowledge transfer. Additional techniques like sparse regularization can further reduce redundant parameters. These methods complement the Continual Neural Dirichlet Process Mixture (CN-DPM) model's approach to prevent overfitting. In the classification experiments, a temperature parameter is added to the classifier to adjust the relative importance of images and labels for increased accuracy. An algorithm is introduced to prune redundant experts, and practical issues of CN-DPM are discussed in the appendices. The CN-DPM model is evaluated in task-free CL scenarios with four benchmark datasets, where each task has a different data distribution. Appendices provide detailed model architecture, additional experiments, and analyses. In task-free CL scenarios, different tasks are assumed to be distinct. The MNIST dataset is split into five tasks with two classes each. Classification and generation experiments are conducted in this scenario. Another scenario involves two stages with MNIST and SVHN datasets. Split-CIFAR10 and Split-CIFAR100 scenarios are also discussed. In Split-CIFAR10, CIFAR10 is split into five tasks similar to Split-MNIST. Split-CIFAR100 consists of 20 tasks with five classes each. Previous works use task information at test time, assigning distinct output heads for each task. This makes the task more challenging as the model has to perform 100-way classification. In a challenging setting, the model must perform 100-way classification from the given input. Various baselines, including iid-offline, iid-online, Fine-tune, and Reservoir sampling, are discussed for comparison. Reservoir sampling randomly selects a fixed number of samples from a continuous stream of data, proving to be a strong baseline. The model is trained using a mini-batch from the data stream and another one of the same sizes from the memory. Gradient-Based Sample Selection (GSS) diversifies the gradients of the samples in the replay memory. Split-MNIST uses a two-hidden-layer MLP classifier with ReLU activation. VAE is used for generation experiments, with experts in CN-DPM having smaller hidden dimensions. In the experiments, experts in CN-DPM have smaller hidden dimensions. The classifier starts with 64 hidden units per layer and adds 16 units when a new expert is added. Hyperparameter \u03b1 is adjusted to create five experts for classification and 12 experts for generation. Memory sizes are set to 500 for Reservoir and CN-DPM in classification, and 1000 for generation. ResNet-18 is used as the base model, with a 10-layer ResNet for the classifier in CN-DPM. VAE has two CONV layers and two FC layers in the encoder and decoder. Different numbers of experts are created for each scenario. More details can be found in Appendix C. Reported numbers are the average of 10 runs. In the experiments, CN-DPM outperforms baselines significantly in every setting. Split-CIFAR10 experiments show that Reservoir surpasses GSS in accuracy, proving to be a powerful CL method. Performance details are shown in Table 2. In contrast to Reservoir, CN-DPM shows consistent accuracy improvement as learning progresses, especially with an increasing number of tasks. Reservoir's performance degrades with extended tasks due to overfitting from replaying the same samples, while CN-DPM avoids this issue by buffering recent examples in memory. CN-DPM outperforms Reservoir in accuracy, especially in Split-CIFAR10/100 tasks. Forgetting is minimal in classifiers, as shown by little difference between initial and final task-wise accuracies. Gating accuracy, measured by VAEs, is relatively low, indicating room for improvement. The CN-DPM model shows potential for improvement in density estimates and expert selection. It avoids catastrophic forgetting but faces challenges in choosing the right expert. Despite these issues, it performs well in task-free settings, outperforming existing methods. Future research could focus on enhancing expert selection accuracy and applying the model to different domains. The Dirichlet process mixture (DPM) model and Sequential Variational Approximation (SVA) are reviewed in an online setting. The Dirichlet process (DP) is a distribution over distributions defined over infinitely many dimensions, parameterized by a concentration parameter \u03b1 and a base distribution G0. The stick-breaking process is often used as a more intuitive construction of DP. The Dirichlet Process Mixture (DPM) model is applied to clustering problems where the number of clusters is unknown. The generative process involves sampling data x and latent variables \u03b8 from a distribution G, which is sampled from a Dirichlet process (DP). If two variables have the same \u03b8 value, they belong to the same cluster. The Dirichlet Process Mixture (DPM) model uses indicator variable z n to assign data to clusters based on \u03b8 n = \u03c6 zn. Each data point x n is sampled from a distribution parameterized by \u03b8 n. The posterior of DPM models is a DP, with the base distribution being a weighted average of G 0 and the empirical distribution. Approximate inference methods are used due to the infeasibility of exact inference. Sequential Variational Approximation (SVA) is a method used for approximate inference in Dirichlet Process Mixture (DPM) models. It simplifies the posterior distribution by approximating it as a product of individual variational probabilities. SVA sequentially updates variational parameters to approximate the posterior of z and \u03c6. Sequential Variational Approximation (SVA) updates variational parameters to minimize KL divergence between q(z, \u03c6) and the posterior. SVA adds components based on a threshold and uses stochastic gradient descent for MAP estimation. CN-DPN is grounded in nonparametric Bayesian theory, with practical issues discussed for CN-DPM's bounded expansion based on data distribution and concentration parameter. The CN-DPM model's capacity increases as it learns new tasks, avoiding the limitation of a fixed-capacity neural network. This approach allows for the addition of components as needed, essential in task-free settings with an indefinite number of tasks. Expansion is a promising direction for models in task-free settings, allowing for the addition of components as needed without fixing the capacity beforehand. The concentration parameter controls the model's sensitivity to new data, determining the discrepancy between tasks modeled by distinct components. For example, in a hand-written alphabet classifier scenario, having images for only half of the alphabets from 'a' to 'm' highlights the importance of a good concentration parameter. In a task-free setting, expansion allows for adding components as needed. The concentration parameter determines the model's sensitivity to new data, crucial for modeling discrepancies between tasks. Using ResNet-18, input images are transformed to 32\u00d732 RGB images. Experts' classifiers have varying hidden units per layer, with 16 new units introduced in later experts. VAEs' encoder and decoder use a two-layer MLP, with the encoder expanding similarly to the classifier. Parameters are not shared beyond the encoders, with a latent code dimension of 16. For generation tasks, the size of initial and additional hidden units is doubled, with a ResNet-18 base network having eight residual blocks. The classifiers in CN-DPM use a smaller version of ResNet with four residual blocks and resize the feature every block. The VAEs utilize a simple CNN-based approach. The VAEs in the current approach use a simple CNN-based architecture with two 3\u00d73 convolutions in the encoder, followed by two fully connected layers. Each convolution is followed by 2\u00d72 max-pool and ReLU activation. The number of channels and hidden units are doubled after each layer. The first expert's VAE has an unshared decoder with a 64-dimensional latent code. The classifier used is ResNet-18 with 32 channels for the first expert and additional 32 channels for each new expert. A classifier temperature parameter of 0.01 is used for Split-MNIST, Split-CIFAR10/100, and no temperature parameter for MNIST-SVHN. Weight decay of 0.00001 is applied. In the paper, weight decay 0.00001 is used for all models. Gradients are clipped with a threshold of 0.5. CN-DPM models are trained with Adam optimizer. During the sleep phase, new experts are trained with a batch size of 50. In classification tasks, VAE density estimation is improved by sampling 16 latent codes and averaging ELBOs. Different learning rates are used for classifiers and VAEs in classification tasks. Binarized MNIST is used for generation experiments, with VAEs trained to maximize Bernoulli log-likelihood. In the generation task, Bernoulli log-likelihood is used, while Gaussian log-likelihood is used for the classification task. Different learning rates are applied to the classifier and VAE of each expert in CIFAR10. The learning rates are decayed before entering the wake phase. VAEs are trained to maximize Gaussian log-likelihood. Lin (2013) proposes an algorithm to prune and merge redundant components in DPM models. The algorithm prunes and merges redundant components in DPM models by measuring similarities between experts and removing one with smaller data. An example of expert pruning is shown in Figure 4. The algorithm is tested on Split-MNIST with more than five experts created. After creating seven experts, a similarity matrix is built to identify similar pairs. Expert pairs (2/3) and (5/6) are chosen for pruning based on a threshold of 0.9. Expert 3 and 6 are pruned, resulting in a slight drop in test accuracy from 87.07% to 86.01%. Comparison with task-based methods for Split-MNIST classification shows the effectiveness of the CN-DPM method. The CN-DPM method utilizes task information during training to select experts, achieving better performance than regularization and replay methods. Task information is not available at test time. When task information is present during training, performance can be further improved. CN-DPM can perform continual learning without task boundaries, even in discriminative tasks. It learns the joint distribution p(x, y) and can generate (x, y) pairs using a complete generative model. The Fuzzy-Split-MNIST scenario adds difficulty by mixing data of existing and new tasks with linearly increasing proportions. CN-DPM successfully generates examples of all tasks without catastrophic forgetting in continual learning scenarios on Split-MNIST, Split-CIFAR10, and Split-CIFAR100. Results are presented in Table 6, 7, and 8, showing performance over multiple epochs and task repetitions. In Split-MNIST, the 10 Epochs scenario involves tasks for each digit pair over 10 epochs, while the 1 Epoch \u00d710 scenario repeats each task multiple times. The Reservoir's accuracy decreases with longer tasks due to overfitting on replayed samples. In contrast, CN-DPM's performance improves with extended learning, showing similar results in the 1 Epoch \u00d710 setting as in the 10 Epoch scenario. In the 10 Epoch setting, Reservoir's scores increase compared to both 1 Epoch and 10 Epoch due to changes in replay memory. Overfitting occurs in the 10 Epoch setting as the model fails to generalize in old tasks. In contrast, in the 1 Epoch \u00d710 setting, overfitting is greatly relieved as each task is revisited multiple times and replay memory is updated with new examples. CN-DPM does not blindly increase the number of experts to avoid having too many experts in longer scenarios. CN-DPM determines the number of experts based on task distribution, not training length. Experimental results show CN-DPM's accuracy drop is minimal with smaller memory sizes compared to Reservoir. The concentration parameter \u03b1 influences expert creation, with higher \u03b1 leading to easier expert creation. Setting log \u03b1 at -400 resulted in optimal performance. The CN-DPM model's accuracy increases as the number of experts grows with higher \u03b1 values. Tuning the concentration parameter is necessary to balance performance and model capacity. Parameter sharing reduces the number of parameters by 38% without sacrificing accuracy. Training graphs show performance metrics and expert numbers in CN-DPM compared to baselines. The CN-DPM model's accuracy improves with more experts and higher \u03b1 values. Parameter sharing reduces parameters by 38% without loss of accuracy. Comparisons with baselines are shown in training graphs. CURL is a related work with similarities but key differences, focusing on a unified latent representation shared across tasks. The CN-DPM model uses a complex generative process involving classifiers and a mixture of Gaussians for z. In contrast, the generative version of CN-DPM simplifies the process by generating x and y independently. The CN-DPM model generates x and y independently, unlike CURL which uses generative replay to prevent forgetting. Unlike CN-DPM, CURL shares a single decoder across tasks and assumes a factorized variational posterior. This can lead to catastrophic forgetting in CURL. Expansion alone is not sufficient to prevent catastrophic forgetting in CURL, requiring the use of generative replay. In contrast, CN-DPM separates components for each task, eliminating the need for additional treatment. The text chunk provides a list of tasks with corresponding time intervals."
}