{
    "title": "HJxf53EtDr",
    "content": "In this paper, the authors analyze the connections between graph convolutional networks (GCN) and matrix factorization (MF), proposing a new model called Co-training and Unitized Matrix Factorization (CUMF). Experimental results show that CUMF performs similarly or better than GCN, especially in distributed computing for semi-supervised node classification. CUMF is beneficial for large-scale and complex real-world applications. Graph convolutional networks (GCN) have been successful in various graph-based tasks like node classification, link prediction, and recommendation systems. To handle large-scale graphs in memory-constrained or distributed environments, different sampling methods like neighbor sampling and importance sampling have been proposed. Cluster-GCN offers an alternative approach to sampling for GCN on large graphs. Cluster-GCN proposes a method to convert computation on a huge matrix to computing on small matrices, addressing performance loss in distributed computing. Relational GCN extends neighbor aggregation using edge types in link prediction, while EGNNs consider more contextual features. Graph embedding methods rely on proximity to embed large information networks into low-dimensional vectors. Graph Convolutional Networks (GCN) utilize first and second-order proximity to embed large information networks into low-dimensional vector spaces. GCN captures structural information by smoothing the node representations to be more similar to their neighbors. Previous work suggests connections between GCN and matrix factorization (MF) methods, with MF being flexible and suitable for distributed computing. In this paper, the authors explore the connections between Graph Convolutional Networks (GCN) and Matrix Factorization (MF) methods. They propose unifying GCN as a special form of MF, highlighting the flexibility and efficiency of MF-based methods for tasks with complex contextual information on graph edges. The theoretical analysis reveals that GCN can be viewed as matrix factorization with co-training and unitization, offering potential benefits for large-scale real-world applications. The authors propose a new model called Co-training and Unitized Matrix Factorization (CUMF) as an alternative to Graph Convolutional Networks (GCN). Extensive experiments show that CUMF outperforms GCN on dense graphs by balancing neighbor smoothing and node classification. Distributed CUMF also outperforms cluster-GCN in distributed computing settings. In this section, the authors aim to theoretically unify GCN as a specific form of matrix factorization. They analyze how node representations are learned in GCN, formulating each layer of GCN using adjacency matrices and trainable weights. Various works have explored contextual information in matrix factorization-based methods, showing effectiveness, efficiency, and flexibility. The authors analyze how node representations are learned in GCN using adjacency matrices and trainable weights. The final representations in GCN converge to an approximate solution, which is a special form of Laplacian smoothing. The cross-entropy error of the node classification task is minimized via optimization, allowing the GCN model to be learned. The final representations in GCN converge to an approximate solution using Laplacian smoothing. A loss function is minimized to balance graph convolution and node classification tasks. Empirical experiments on Cora and Pubmed datasets verify the consistency of cosine similarity with GCN convergence. The GCN models are trained on the Cora and Pubmed datasets, showing convergence in cosine similarity between nodes and their neighbors. Unitization of representations simplifies the equation, leading to better optimization with negative log likelihood minimization. In graph embedding methods, negative edges sampling is used for better convergence by randomly sampling negative edges for each edge in the graph G. The distribution PG generates negative samples in graph G, where k is the number of negative samples for each edge. The expectation term can be explicitly expressed to improve optimization. The expectation term in graph embedding methods can be explicitly expressed to improve optimization. By combining equations, the local structure loss for specific edges can be obtained. The GCN model can be unified as matrix factorization co-training with classification loss, similar to LINE with first order proximity. Theoretical analysis shows that graph convolutional networks can be seen as implicit matrix factorization with co-training and unitization of node representations. A new model named Co-training and Unitized Matrix Factorization (CUMF) is proposed as an alternative to GCN, involving vector unitization and structural components. The proposed CUMF model combines implicit matrix factorization with negative sampling for structural part and classification loss. It co-trains the structural loss and classification loss alternately optimized in batches with positive and negative edges. The CUMF model combines implicit matrix factorization with negative sampling for structural and classification loss. It alternately optimizes structural and classification loss in batches with positive and negative edges, based on theoretical analysis. The unique feature lies in the unitization of node representations derived from the analysis. Comparing with previous methods, CUMF is clear, concise, and reasonable. Graph Attention Network (GAT) proposes to aggregate neighbors with node diversity in mind, addressing the limitations of GCN in large-scale applications. Sampling approaches like random sampling and importance sampling reduce computation, while variance controlled GCN uses sampled nodes for representation updates. Cluster-GCN partitions the graph into smaller sub-graphs for localized aggregation. Cluster-GCN supports mini-batch construction and distributed computing by aggregating within small sub-graphs. RGCN considers edge features using edge types for node transition matrices, while EGNN treats edge features as a tensor and proposes normalization techniques. EGNN variants handle edge tensors based on GCN and GAT. Key questions include roles of components in the CUMF model, performance comparison with GCN on various datasets, and the impact of hyper-parameters on method performance. The proposed CUMF model is evaluated on seven datasets, including standard citation network benchmark datasets like Cora, Citeseer, and Pubmed, as well as social networks like BlogCatalog and Flickr. Node features are used in the datasets, except for USA and Europe air-traffic networks, which do not contain node features. The CUMF model is tested on two types of datasets: Structural Datasets (Cora-F, Citeseer-F, Pubmed-F, BlogCatalog-F, Flickr-F, USA, and Europe) and Attributed Datasets (Cora, Citeseer, Pubmed, BlogCatalog, Flickr). Different variations of the model are compared, including CUMF-C (without co-training), CUMF-U (without unitization), and CUMF-C-U (without co-training and unitization). The model training involves optimizing structure loss first and then updating the classification model parameters. The model is compared against baseline methods like Planetoid* and GCN, as well as distributed methods like Random-GCN and Cluster-GCN. The methods are implemented using Tensorflow and Adam optimizer with weight decay set to zero. The mean accuracy of 10 runs with random weight initializations is reported for all methods, with specific hyper-parameters defined. In section 3, two important hyper-parameters are discussed: the number of negative edges (k) and the balance between structure loss and classification loss (b). The effects of k and b will be analyzed in the next section. Results of experiments are summarized in Table 2, Table 3, Table 4, and Figure 3. The roles of different components of the proposed method (co-training & unitization) are examined, showing that CUMF outperforms other versions significantly, verifying the importance of unitization and co-training. CUMF is also shown to be superior to GCN on all structural datasets. After verifying that CUMF is superior to GCN on structural datasets, a comparison is made between CUMF, GCN, and Planetoid on attributed datasets. Results in Table 3 show that CUMF performs consistently with GCN on sparse graphs like Cora, Citeseer, and Pubmed, but significantly outperforms GCN on denser graphs like BlogCatalog and Flickr. This indicates that CUMF is effective on both sparse and dense graphs, with significant improvements on the latter. The CUMF model is less likely to experience over-smoothing compared to GCN due to its flexible capturing of structural information. Experimental results and analysis of hyper-parameters (b & k) show the model's stability and performance on attributed datasets. The experimental results in Figure 3 show that the CUMF model's testing performance is not significantly affected by the hyperparameters k and b. For sparse datasets, b has little impact, but for dense datasets like BlogCatalog and Flickr, increasing b decreases the model's performance. This is because a higher b value focuses more on capturing structural information, which can lead to poor performance on dense datasets. In contrast to GCN, CUMF is more flexible in adjusting to graph sparsity through parameter b. Additionally, experiments demonstrate CUMF's and GCN's performance on distributed computing. In experiments comparing centralized GCN and CUMF on Cora, Citeseer, and Pubmed datasets, distributed CUMF maintains performance while distributed GCN methods suffer. The challenge for GCN lies in constructing mini-batches, which is equivalent to solving a graph partitioning problem. CUMF is the first work connecting GCN to MF, proposing a model that unifies GCN as co-training and matrix factorization. Empirical experiments show CUMF achieves similar or better performance than GCN, especially on dense graphs where GCN struggles due to over-smoothing. CUMF's MF-based architecture allows for flexibility and easy application to distributed computing. CUMF, a model that unifies GCN as co-training and matrix factorization, is highly flexible and outperforms state-of-the-art distributed GCN methods in large-scale real-world applications."
}