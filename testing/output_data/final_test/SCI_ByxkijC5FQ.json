{
    "title": "ByxkijC5FQ",
    "content": "Neural persistence is a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. It reflects best practices in deep learning like dropout and batch normalization. A neural persistence-based stopping criterion can shorten training time while maintaining accuracy. The practical successes of deep learning in various fields continue to surpass our theoretical understanding. Theoretical understanding of deep neural networks, including generalization capabilities, still lags behind practical successes. Various methods have been proposed to improve comprehension, such as feature visualization, sensitivity analysis, and statistical analysis of learned weights. Expressivity measures have been developed to explore the success of techniques like batch normalization and introduce new regularization methods. This paper introduces a new regularization method for neural networks, focusing on providing insights while maintaining theoretical generality. It presents neural persistence, a measure for characterizing network structural complexity using algebraic topology techniques. This approach integrates network weights and connectivity without relying on input data interrogation. Neural persistence is introduced as a measure for assessing the structural complexity of neural networks. It can efficiently compute and compare networks of varying sizes, capturing benefits of dropout and batch normalization. This approach utilizes algebraic topology techniques, specifically persistent homology, for analyzing complex data. Persistent homology, a mathematical framework based on algebraic topology, is used to understand high-dimensional manifolds and characterize graphs, find features in unstructured data, and analyze image manifolds. Simplicial homology, a central concept in algebraic topology, uses matrix reduction algorithms to derive homology groups that describe topological features of a simplicial complex. Homology groups describe topological features like holes of different dimensions. Betti numbers summarize the information from homology groups by counting the number of features. In the context of analyzing neural networks, BID4 and BID16 calculated bounds of Betti numbers to show the implications of different activation functions. Persistent homology is used for analyzing real-world datasets to obtain a measure of the topological complexity of decision boundaries. Persistent homology is a method used for analyzing real-world datasets by tracking topological changes in a simplicial complex as a scale is changed. It represents the creation and destruction of features as points in a filtration. Neural persistence is a novel measure for assessing the structural complexity of neural networks by utilizing persistent homology to capture network expressiveness beyond mere connectivity properties. It calculates the persistence of points in the dth persistence diagram, reflecting the presence of features or noise in the network. The method combines network structure and weight information to provide a comprehensive evaluation of network complexity. The neural persistence calculation method utilizes persistent homology to assess the structural complexity of neural networks. It involves mapping weights to connections in a feedforward neural network and analyzing the network's complexity through stratified graphs and layers. The kth layer of a stratified graph is a unique subgraph used to calculate persistent homology in neural networks. A novel filtration is needed due to weights from training potentially leading to unbounded values. The filtration mimics connecting points based on increasing distance, similar to geometrical data analysis. The framework focuses on fully-connected neural networks without considering activation functions explicitly. The framework for analyzing neural networks focuses on the influence of weights during training. A filtration method is used to transform weights, preserving the strength of connections and making the network invariant to scaling. This allows for easier comparison of different networks. Persistence diagrams are used to calculate persistent homology for each layer of a neural network. The focus is on zero-dimensional topological information, describing how connected components are created and merged during the filtration process. This approach simplifies the comparison of networks and provides easily interpretable clustering values at different weight thresholds. The study focuses on zero-dimensional persistent homology in neural networks, specifically looking at how vertices are sorted in the filtration process. The resulting persistence diagrams show a special structure with entries below the diagonal. Neural persistence is defined using the p-norm of a persistence diagram, capturing important information for analysis. Neural persistence is defined as the Euclidean distance of points in a persistence diagram to the diagonal. It is a stable summary of topological features in a neural network. The calculation process is highly efficient, with a computational complexity of O(n log n). The persistent homology is computed using an algorithm with a complexity of O(n \u00b7 \u03b1(n)). The Ackermann function is implemented and experiments are available on GitHub. Bounds for neural persistence of a single layer Gk are derived, with a lower bound realized for a fully-connected layer. The filtration degenerates to a lexicographical ordering of vertices and edges, resulting in specific pairs of points in Dk. The upper bound for neural persistence can be achieved by normalizing the neural persistence of each layer in a network, allowing for comparison between layers with different architectures. The mean normalized neural persistence is calculated by summing the neural persistence values per layer in the network. This approach avoids the issue of different layers having varying scales. The mean normalized neural persistence is obtained by summing the neural persistence values per layer in a network. Theorem 2 provides bounds for efficiency in calculating neural persistence values based on the structure of persistence diagrams. In Section A.2 of the appendix, neural persistence changes are examined for different classes of simple neural networks using a perceptron trained on the 'MNIST' dataset. Comparisons are made between neural persistence values of trained networks and random weight matrices drawn from various distributions. Trained networks show distinct neural persistence values compared to random matrices, with Gaussian matrices having higher values than uniform matrices. The study compares neural persistence values of different types of networks, showing that Gaussian networks induce functional sparsity. Results align with previous findings on small weighted networks. Further experiments explore the relationship between validation accuracy and neural persistence in deeper networks, as well as the impact of data distributions and network depth variability. This highlights the relevance of neural persistence in fully connected deep neural networks. The study examines the impact of regularization techniques on neural persistence in deep neural networks. An early stopping criterion based on neural persistence is developed and compared to the traditional validation loss criterion. Different network architectures with ReLU activation functions are used in the experiments. The mean normalized neural persistence of a two-layer neural network is compared to models with batch normalization or dropout. The results are shown in FIG4. The study compares the impact of regularization techniques like dropout on neural persistence in deep neural networks. Results show that networks designed with best practices yield higher neural persistence values on the 'MNIST' dataset. Dropout leads to a more pronounced effect on neural persistence, similar to ensemble learning. This suggests that increasing neural persistence during training could be beneficial for fixed architecture approaches. Neural persistence can serve as an early stopping criterion without needing a validation dataset to prevent overfitting. The study discusses a method called 'patience' for early stopping in training neural networks, which does not require a validation dataset to prevent overfitting. It compares this method against validation loss in various scenarios, using a parameter grid with 'patience' and burn-in rate values. The goal is to evaluate the efficacy of this measure in training processes with a set number of epochs. In evaluating the 'patience' method for early stopping in neural network training, 100 runs are performed for each dataset using the same architecture. The performance is measured in terms of median test accuracy and stopping epoch, with a scatterplot showing differences in epochs and test accuracy. The quadrants provide intuitive explanations for the results. The study evaluates the 'patience' method for early stopping in neural network training. It compares the stopping epochs and accuracy of the measure with validation loss. The barycentre of all configurations is calculated to show that the measure stops earlier with higher accuracy. The study evaluates the 'patience' method for early stopping in neural network training by comparing stopping epochs and accuracy with validation loss. Each cell in a heatmap corresponds to a parameter configuration of b and g. The heatmap of accuracy differences shows blue, white, and red representing higher, equal, or lower accuracy compared to validation loss. Algorithm 2 outlines early stopping based on mean normalized neural persistence, with a focus on updating mean persistence and monitoring training at every epoch. The goal is to have a dark green triangle for each measure, indicating consistent stopping. Neural persistence is a novel topological measure of deep neural network complexity, capturing information relevant to deep learning performance. It stops earlier to prevent overfitting and later for longer training benefits. The measure is theoretically well-defined, generally applicable, and computationally efficient. Our measure correctly identifies networks using best practices like dropout and batch normalization. An early stopping criterion was developed without relying on a separate validation set, saving valuable data for training and boosting accuracy. Experimenting with the p-norm of all weights as a proxy for neural persistence did not yield an early stopping measure, indicating that neural persistence captures important information hidden among network weights. The framework was extended to convolutional neural networks with a closed-form approximation for early stopping. Neural persistence in machine learning shows potential for topological data analysis. Assessing network dissimilarities using persistence diagrams may provide insights into generalization and learning abilities. Future research could focus on analyzing the 'function space' learned by neural networks. Traditional complexity measures like clustering coefficient are insufficient for characterizing complex random networks. Our experiments on neural networks training show that traditional complexity measures like clustering coefficient are insufficient for distinguishing different network classes. Neural persistence, however, can clearly differentiate between networks with varying accuracies, indicating its potential for topological data analysis in machine learning. Neural persistence is a subset selection problem where n out of m weights are selected, with the persistence depending on selected weights. The neural persistence equation is rewritten as NP(G k ) = \u22251 \u2212 w\u2225 p , with constraints on the selected weights. Additional plots in the section show differences in accuracy and epoch for various configurations. The heatmap displays accuracy and epoch differences for different parameter combinations. Blue indicates higher accuracy, white shows the same accuracy, and red signifies decreased accuracy. Green represents early stops compared to validation loss. Scatterplots provide a detailed view, while the bottom row shows the frequency of each measure triggered for every parameter. The bottom row of each plot displays the frequency of each measure triggered for parameter combinations. Measures are considered triggered if their stopping condition is met before the last training epoch. Comparing the 'slopes' of cells for each measure, ideally, a dark green triangle indicates all parameter configurations stop consistently. The difference matrix in the top row shows slight color skewing due to accuracy loss in certain configurations when stopping. However, many configurations show minimal accuracy loss and allow for stopping earlier. Heatmaps in the bottom row indicate neural persistence. The data set CIFAR-10 is more sensitive to parameters for early stopping, with neural persistence stopping training earlier in some configurations but requiring longer training in others. Neural persistence triggers reliably for more configurations than validation loss. Most practical configurations are located in Q2 and Q3 on the scatterplot, showing that neural persistence can outperform validation loss as an early stopping criterion in certain scenarios. In practical configurations, neural persistence can outperform validation loss as an early stopping criterion. For CIFAR-10, training for FCNs may not reliably converge. NP shows clear change points for 'Fashion-MNIST' and incremental growth for 'CIFAR-10'. For 'CIFAR-10', incremental growth is observed for some runs, making it difficult to determine a generic early stopping criterion. Neural persistence may not be reliable when the architecture struggles to learn the dataset. Future experiments will involve assessing topological measures on 'bad' and 'good' architectures. Most parameter configurations for IMDB result in earlier stopping with accuracy increases, except for one configuration leading to a severe loss of accuracy. The proposed filtration process can be applied to any bipartite graph, including convolutional layers. Each layer is represented as a bipartite graph parametrized by a sparse weight matrix. Flattening does not change the topology of the graph, allowing for the computation of the flattened pre-activation. The normalized neural persistence on the sparse weight matrix W (l) i,j is computed as an unrolled version of the fully-connected network's weight matrix. The edge filtration process for the unrolled adjacency matrix W (l) i,j can be approximated in a closed form, with creation events simplified to a list of largest filter values. The normalized neural persistence on the sparse weight matrix W (l) i,j is computed by simplifying creation events to a list of largest filter values in descending order. This simplification is shown in Equations 7-11 and implemented in Algorithm 3, resulting in DISPLAYFORM3 using DISPLAYFORM4 DISPLAYFORM5 DISPLAYFORM6 DISPLAYFORM7. Equation 7 expresses neural persistence of the bipartite graph G i,j with selected weights denoted by vector w. The neural persistence on the sparse weight matrix is computed by simplifying creation events to a list of largest filter values. A simple CNN with 32 + 2048 filters is used for experiments, avoiding the need to unroll weight matrices explicitly. The approximation method significantly reduces computation time compared to the naive approach. The model is trained on 'Fashion-MNIST' using a 'LeNet-like' CNN architecture. In early stopping experiments on a CNN model trained on 'Fashion-MNIST', stopping based on neural persistence of a convolutional layer results in a loss of accuracy up to 4%. The correlation between high neural persistence and predictive accuracy is investigated, but no correlation is found in deeper networks. Neural networks with high neural persistence were constructed for the experiments. We constructed neural networks with high neural persistence by initializing most weights with low values and few weights with high values using a beta distribution. This resulted in networks with NP \u2248 0.90 \u00b1 0.003 compared to NP \u2248 0.38 \u00b1 0.004 with Xavier initialization. Validation accuracy on 'Fashion-MNIST' was 0.10 \u00b1 0.01 and 0.09 \u00b1 0.03. Further experiments showed no improvement with deeper layers. Adding layers initially increases the variability of neural persistence by enabling the network to converge to different regimes. However, deeper architectures exhibit less variability in neural persistence. The study investigates the impact of reducing training data set size and permuting training labels on a fully connected network trained on 'MNIST'. In experiments with fully connected networks on 'MNIST' and 'Fashion-MNIST', various stopping measures were compared, including optimal test accuracy, fixed stopping, neural persistence patience, training loss patience, and validation loss patience criteria. The accuracy was reported on non-reduced, non-permuted test sets with a batch size of 32 training instances. Results were averaged over 10 runs, showing differences in datasets and patience parameters. The top and bottom panels differ in data set and patience parameters. The x-axis shows warped data fraction for accessibility. Left subplots display results of reduced data set, right subplots show permutation results. Top subplot y-axis shows accuracy on non-reduced test set, bottom subplot y-axis shows stopping criterion trigger. More data yields higher accuracy, optimal stopping gives highest accuracy. Fixed early stopping results in inferior accuracy with limited data. Neural persistence stopping triggers late with limited data for slightly better accuracy. The training loss stopping achieves similar test accuracies compared to the persistence based stopping with shorter training, on average. Using training loss as a measure for stopping is generally not advisable due to the dependency on batch size. Validation loss based stopping stops after the same number of training epochs as training loss when only a fraction of data is available, resulting in inferior test accuracy. Validation loss based stopping is triggered later or never when labels are randomly permuted, leading to overfitting and poor test accuracy. The neural persistence based stopping method achieves good performance regardless of batch size and noisy labels. It remains stable even with increasing noise in training labels, unlike validation loss stopping which leads to lower test accuracy. The patience and burn in parameters are reported in quarter epochs, and neural persistence can distinguish between networks with/without batch normalization and dropout. Figure A .13 shows test set accuracies."
}