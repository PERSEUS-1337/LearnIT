{
    "title": "HyeG9yHKPr",
    "content": "In reinforcement learning, modeling future observations and rewards to plan the agent's actions can be computationally expensive. Previous works have used partial models to address this issue, but they can be causally incorrect. This paper introduces a family of partial models that are provably causally correct, improving planning accuracy without fully modeling future observations. In model-based reinforcement learning (MBRL), an agent builds an internal representation by sensing an environment and taking actions according to a policy. The collected sensory data is used to predict future observations and can be utilized for planning, generating synthetic training data, credit assignment, learning internal representations, and exploration. Commonly explored methods in MBRL include action-conditional, next-step models. In model-based reinforcement learning (MBRL), researchers explore action-conditional, next-step models to predict future observations. However, accurately modeling all available information is often not feasible, especially when dealing with high-dimensional data like images. This challenge has led to the consideration of simpler models, known as partial models, which do not require conditioning on or generating the full set of observed data. In this paper, the focus is on model-based approaches in reinforcement learning, proposing a novel framework for learning better partial models. The key insight is that information used by the policy for decision-making, but not available to the model, can lead to incorrect predictions under a new policy. This work contrasts with previous research on non-model based bandit algorithms and causal discovery with RL. The paper addresses the issue of partial models in reinforcement learning, where information used by the policy but not available to the model can lead to incorrect predictions. The authors propose a framework combining causal reasoning, probabilistic modeling, and deep learning to tackle this problem. They identify the fundamental problem of partial models and suggest solutions to ensure causal correctness with respect to policy changes. The paper proposes solutions to address the issue of partial models in reinforcement learning, focusing on causal correctness with respect to policy changes. They investigate the effects of these solutions on models learned in different environments, using a simple example to illustrate the challenges with partial models. The paper discusses the challenges of partial models in reinforcement learning, particularly in stochastic environments. It highlights how a reward model trained on past experiences can lead to incorrect evaluations of action sequences. The behavior policy in reinforcement learning affects what the model learns and can lead to biased reward estimations. For example, using a reward model trained on past experiences may result in overestimating rewards for certain actions, leading to suboptimal decision-making. One way to avoid biased reward estimations in reinforcement learning is to make the model robust to changes in the behavior policy. This involves using an interventional conditional instead of an observational conditional to ensure more appropriate planning. The text discusses the use of interventional conditionals in reinforcement learning to address biased reward estimations. It proposes making interventions at the policy level instead of the action level to improve planning accuracy. When predicting a target variable y from a covariate x, fitting a model q \u03b8 (y|x) to a dataset of (x, y)-pairs is a standard approach. However, in some cases, making predictions with changes in the environment or self-set x values can be challenging due to unobserved confounders that affected both x and y during data generation. This can impact the accuracy of predictions made by the model q \u03b8 (y|x). When predicting y from x, fitting a model q \u03b8 (y|x) to (x, y)-pairs is common. However, setting x at prediction time changes the distribution of y to p(y|do(x)). The do-operator intervenes by fixing x to x 0 independently of its parents, altering the data-generation process. This results in a different target distribution, p(y|do(x = x 0 )). An intervention in a generative process replaces factors with new factors, such as the do-operator which changes the distribution of y to p(y|do(x)). Recovering p(y|do(x)) from observational data alone is challenging, but becomes possible with additional structure in the data-generation process. The concept of confounder u being a backdoor for the pair x \u2212 y is crucial for backdoor adjustment in causal inference. This adjustment allows for expressing p(y|do(x)) using observational data. Additionally, frontdoor adjustment can be applied when a variable z blocks the effect of x on y. These adjustment formulas are essential for computing marginal distributions under different interventions. Derivations for backdoor and frontdoor adjustment formulas are provided in Appendix A. Causally correct models are accurate after any intervention in a set I. Backdoor-adjustment and importance sampling involve fitting a distribution to maximize re-weighted likelihood, requiring re-fitting for new interventions. In causal modeling, fitting the conditional distribution p(y|z, x) can help avoid limitations of importance weights w(x, z) with high variance. There is a direct connection between partial models in Model-Based Reinforcement Learning (MBRL) and causal concepts, where predictions are made about the future conditioned on actions as covariates. The policy's state acts as a confounding variable u, and any variable mediating the effect of the state on actions is a backdoor in the computational graph. In causal modeling, fitting the conditional distribution p(y|z, x) can help avoid limitations of importance weights with high variance. The policy's state acts as a confounding variable, and any variable mediating the effect of the state on actions is a backdoor in the computational graph. The environment interacts with an agent through actions produced by a policy \u03c0(a t |s t ) and updates its state using observations y t+1. The agent does not observe or model the environment state, which is a confounder in the data generation process. In causal modeling, predicting the outcome of actions on the environment involves using action-conditional autoregressive models or models with overshoot. The former conditions predictions on all available observations and actions, while the latter predicts based only on initial observations. In causal modeling, predicting outcomes involves using action-conditional autoregressive models or models with overshoot. The models with overshoot update their state deterministically and can generate final outcomes directly without intermediate observations. Partial views and partial models are defined as functions of past observations and actions, with predictions conditioned on initial observations and actions. The presence of confounders in partial models makes it impossible to predict the target distribution accurately after changes in the covariate distribution. This means that learned partial models may not be robust against changes in policy and should not be used for planning under different policies. In reinforcement learning, the agent can control its graph and choose a backdoor variable between its internal state and the action, allowing for predictions under a broad range of interventions. This is in contrast to partial models with confounders, which are not robust against policy changes and should not be used for planning. In reinforcement learning, the agent can choose a backdoor variable for actions, making predictions under different policies. Causal Partial Models (CPM) use the backdoor as the partial view, ensuring causally correct predictions. Non-Causal Partial Models (NCPM) do not follow this approach. The backdoor z t is sampled from m(z t |s t) and the policy is conditioned on z t, \u03c0(a t |z t). This allows for simulations under new policies using the backdoor-adjustment formula. In reinforcement learning, the agent can select a backdoor variable for actions to make predictions under different policies. The components p(z t |h t ) and p(y t+1 |h t+1 ) can be learned from observational data. Modern deep-learning agents have complex graphs with many choices for the backdoor z t. Identifying z t with the agent's state s t can be informative but computationally expensive. Other choices for z t are discussed in the text. The z t vector of probabilities from a policy can inform about the state, and the intended action before exploration. Causal correction methods can be applied to any partial model, focusing on environment models proposed by Gregor et al. (2019) with a deterministic RNN backbone. The RNN states are used to condition a generative model of observed data y t without autoregressive feedback. A policy network produces z t before an action a t, allowing simulation of outcomes using Equation (4). The model components p(z t |h t) and p(y t |h t) are trained via maximum likelihood. The model components p(z t |h t) and p(y t |h t) are trained via maximum likelihood on observational data. The model usage is summarized in Algorithms 1 and 2 in Appendix D, and the effect of proposed corrections on various models and environments is analyzed. In MDP environments like the FuzzyBear MDP, optimal policies can be computed directly from the transition matrix and behavior policy. In Section 5.1, optimal policies from non-causal and causal models are compared using value iteration. The intended-action backdoor is used for analysis with a tabular representation. Section 5.2 repeats the analysis with a learned model using the policy-probabilities backdoor. Optimal policies are computed using the Dyna algorithm or expectimax. Section 5.3 provides an analysis of model rollouts in a 3D environment. Optimal values for planning based on NCPM and CPM are derived in Appendix I without using empirically trained models. The text discusses comparing optimal policies from causal and noncausal models using different backdoors for analysis. It also explores the effects of varying exploration parameters on learned models. Additionally, it examines the difference between causal and noncausal models when learning from randomly generated policies. The text compares optimal policies from causal and noncausal models using different backdoors for analysis. It also explores the effects of varying exploration parameters on learned models. Causal models consistently produce values greater than or equal to behavior policies on FuzzyBear. The simulation policy within the causal model can reproduce the behavior policy if it's good, choose the most rewarding action if it's random, or choose the opposite action if it's bad. This allows for finding a very good simulation policy. The non-causal model displays unrealistic optimism as the behavior policy improves. On AvoidFuzzyBear, the causal model consistently prefers staying at home, while the non-causal model gives varied, overly-optimistic evaluations and chooses the wrong action. In experiments with learned models trained by gradient descent, a causal model consistently prefers staying at home on AvoidFuzzyBear, while a non-causal model displays unrealistic optimism and chooses the wrong action. The optimal policy can be learned purely from off-policy experience using a general n-step-return algorithm derived from a causal model. The non-causal model chooses sub-optimal actions due to unrealistic optimism, while the causal model achieves optimal rewards by learning the optimal policy from off-policy experiences. In an experiment using the classical expectimax search, the AvoidFuzzyBear MDP was solved with a search depth of 3. The behavior policy improved as the model was trained, with only the non-causal model failing to solve the task. Planning with the non-causal model consistently chose the stochastic path with the fuzzy bear, as predicted by theoretical analysis. Models with clustered probabilities and observations were used for approximate modeling. The models used in the experiments are based on previous work by Gregor et al. (2019) and trained using the IMPALA algorithm. The agent and model architecture follow a specific description. The experiments focus on the causal correction in a 3D T-Maze environment. Results show that NCPMs are overly optimistic compared to CPMs. In Figure 6 (b), CPM generates food at the end of a rollout with a 50% chance due to random placement. CPM and NCPM differ in state-update formula and action generation. Frame generation is the same for both models. Rollouts with generated z respect randomness in food placement, while rollouts with forced z do not. The proposed modification to partial models addresses issues with causal reasoning and policy changes, ensuring correct predictions. However, it does not address robustness against other environmental interventions, which will be explored in future work. The data-generation process involves using the do-operator to compute outcomes under different conditions. The do-operator is used to compute outcomes under different conditions, such as p(y|do(x)) = p(u)p(y|x, u)du. It is not possible to compute p(y|do(x)) from the joint p(x, y) alone without extra structure. If a variable z blocks effects of u on x, p(y|do(x)) can be derived. The formula holds as long as p(z|x) > 0, \u2200x, z, and is a simple instance of frontdoor adjustment. Models (c) and (e) in Figure 3 are causally correct, while model (d) is causally incorrect. The do-operator is used to compute outcomes under different conditions. Models (c) and (e) in Figure 3 are causally correct, while model (d) is causally incorrect. Specifically, models (c) and (e) make the same prediction about future observations after intervention, while model (d) does not. If a model is perfectly trained, interventions in the model make the same predictions as in the real world, indicating causal correctness. The interventional conditional in the model is where h T +1 is a function of s 0 and a 0:T. In a perfectly trained model, q \u03b8 (y T +1 |h T +1 ) = p(y T +1 |s 0 , a 0:T ). However, the observational conditional p(y T +1 |s 0 , a 0:T ) is not generally equal to the interventional conditional p(y T +1 |s 0 , do(a 0:T )). The interventional conditional in this model is where h t is a function of s 0 , a 0:t\u22121 and z 1:t\u22121. In a perfectly trained model, q \u03b8 (y T +1 |h T +1 ) = p(y T +1 |s 0 , a 0:T , z 1:T ) and q \u03b8 (z t |h t ) = p(z t |s 0 , a 0:t\u22121 , z 1:t\u22121 ). This indicates that the model is causally correct. The empty backdoor is generally not appropriate, except when the behavior policy is not dependent on the state. This limits the behavior policy to open-loop simulations with fixed action sequences. The backdoor contains no information about observations, making it challenging to model layers accurately. The backdoor can be combined with additional information from other layers, such as extra bits from the input layer, to make z t more informative. Algorithms describe how the model is trained and used to simulate trajectories in a distributed actor-learner setup. Data collection involves sampling z t and the backdoor for each step. The causal partial models provide a fast and accurate representation of a partial view in stochastic environments, ignoring irrelevant background distractors. They are invariant of the intended action distribution and can quickly learn to focus on the executed action in deterministic regions. The autoregressive models are invariant of the policy and can evaluate any other policy within the model. The causal partial model can only run simulations with policies conditioned on specific features. The model can be used to evaluate and improve policies by estimating returns for different initial states and actions. In reinforcement learning, models of the environment can be used in various ways. Dyna and Dyna-2 utilize experiences from partial models to adapt policies locally. In MCTS, a model is used to build a tree of possibilities. Despite not having a full model, a causally correct simulation of rewards and values can be produced for generalizing policies. In reinforcement learning, models like Dyna and Dyna-2 use partial models to adapt policies locally. A causally correct simulation of rewards and values can be produced for generalizing policies. The proposed Dyna-style policy-gradient algorithm is based on models discussed in the paper, using RNNs like LSTM for agent state. The agent learns to do simulations from the model by setting the simulation state based on the agent state and action. The agent sets the simulation state at time t, updates it with recurrent network, and learns a model to simulate forward. To derive an optimal policy and value function, functions of the agent state are defined. Training a second pair of functions with policy gradients on simulated experiences is done using extra z t 's for h t that has seen z's up to z t\u22121. During training, policy gradients are used on simulated experiences to update actions, values, and policy parameters. The algorithm also trains the values and policy parameters using n-step returns computed from simulated rewards. However, the last value V h (h T ) is not directly trained and needs to be close to the agent state value V (s T ). During training, policy gradients are used on simulated experiences to update actions, values, and policy parameters. The algorithm also trains the values and policy parameters using n-step returns computed from simulated rewards. To reduce variance, sampling z t from a proposal distribution q(z t |h t ) can be done, with the correct expectation recovered using an importance weight. Data efficient training involves minimizing the KL divergence between the distribution of the used partial view and the model distribution. For tree-search, a small branching factor at chance nodes is desired, with a discrete z t variable with a small number of categories being optimal. The model approximates causal partial models by clustering modeled layers and using the cluster index as z t. A mixture of components is used to model the layer, with a clustering loss to train the best component. The categorical component index is represented by z t, with a hyperparameter \u03b2 clustering to encourage moving information to z t during training. During training, the clustering loss encourages moving information to the latent z t. A better inference can be obtained by smoothing. The cluster index may not be sufficient as a backdoor if the reconstruction loss is not zero. Two model-based evaluation metrics are derived for MDP environments: V * NCPM(\u03c0) (s 0 ) and V * CPM(\u03c0) (s 0 ). Theoretical analysis of the MDP assumes accurate learning of transition probabilities and policy from training data. Non-causal bias is introduced due to input dependency on actions. The network learns reward mean implicitly. Expectation can be computed exactly with MDP knowledge. Dependency on policy \u03c0 is evident in the learned model. The learned model's dependency on policy \u03c0 is evident. The two-step recursion can be expressed with terms S i,j and Z i. The non-causal agent incorrectly always chooses hug for a reward of +1. The causal agent samples intention z 1 and chooses hug accordingly. The causal model consistently evaluates the optimal policy without overestimating rewards, while the non-causal model unrealistically expects high rewards. Varying levels of \u03b5-exploration show that the causal model's evaluation remains constant, unaffected by randomness, while the non-causal model evaluates at a high value even when the expected reward is lower. As \u03b5 approaches 1, training data becomes more random, causing optimal evaluation to decrease to match causal evaluation. The CPM optimal value remains constant regardless of \u03b5 in theoretical analysis. In the AvoidFuzzyBear environment, the CPM optimal value is always 0.6, achieved by choosing to stay home. The backdoor variable z t is chosen as action probabilities, with a mixture-network distribution p(z t |h t ) using N c Dirichlet components. In experiments, concentration parameters for Dirichlet components were parametrized using a relu-MLP. Training was interleaved with a \"Food\" level for efficiency. Rollouts were generated after achieving ceiling performance, with color matching used to classify food blocks. Rollouts were generated shortly after the policy had achieved ceiling performance, but before the policy entropy reduces too much for exploration. As training progresses, the policy becomes more deterministic and starts to generate more food, becoming overoptimistic. Training with non-zero \u03b5-exploration can help avoid this issue."
}