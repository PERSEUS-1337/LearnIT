{
    "title": "SJDYgPgCZ",
    "content": "To understand the loss surface of Deep Neural Networks (DNNs), it is important to analyze the local minima and overall structure. The parameter domain can be divided into regions with consistent activation values. In each region, the loss surface behaves like that of linear neural networks, where every local minimum is a global minimum. Poor regions exist in overparameterized DNNs, leading to suboptimal local minima. Deep Neural Networks (DNNs) have high expressive power, surpassing shallow networks. The use of rectified linear units (ReLUs) has allowed DNNs to become even deeper, with over 100 layers. However, training DNNs can be challenging due to pathological curvatures in the loss surfaces, such as narrow valleys causing unnecessary vibrations. Various optimization methods were introduced to avoid obstacles like narrow valleys causing vibrations in deep neural networks (DNNs). These methods utilize gradient moments to preserve historical trends and address issues like getting stuck in poor local minima. Recent works have shown that errors at local minima can be as low as global minima in DNNs. In linear DNNs without activation functions, every local minimum is a global minimum, while other critical points are saddle points. It is conjectured that these properties hold in regions where activation values for each data point are the same. The activation values for each data point are the same as shown in FIG0, leading to loss surface decomposition. Poor local minima exist in large networks due to disappearing gradient flow with ReLU. Another type of poor local minima has the same loss as linear regression. Each local minimum in a network has a corresponding local minimum in a larger network by adding a node. Six nonempty regions with the same activation values are identified. The loss surface of deep linear networks has unique properties: non-convexity, every local minimum is a global minimum, and non-global minimum critical points are saddle points. This eliminates poor local minima issues with gradient descent. Activation values play a key role in network behavior, as shown in the decomposition of DNN loss surfaces. The model parameters for a network with one hidden layer include W, v, b, and c. The activation function is a rectified linear unit, and the network can be expressed in terms of activation values. The activation values play a key role in network behavior, and they are fixed to find interesting properties regardless of real activation values. The activation region is defined by fixed activation values A. A non-differentiable local minimum example is given for a specific dataset. The local curvatures of the loss function are discussed, stating that a point is a local minimum if it is a local minimum in another function with fixed activation values. The function with fixed activation values behaves similarly to linear neural networks. The function g A (x i , \u03b8) is similar to a linear neural network when values are one. Inactive parameters make the proof complex as they vary for each data point. The function can be converted into a convex function in terms of other variables. The function L g A (\u03b8) is non-convex and non-concave, with every local minimum being a global minimum. The function L g A (\u03b8) is non-convex and non-concave, with every local minimum being a global minimum. A function f (x, y) = (xy \u2212 1) 2 has a saddle point at x = y = 0. Critical points that are not global minima are saddle points. If activation values are all zeros, global minima are critical points. Saddle points exist when at least one gradient along p j or q j is not zero. Subglobal minima may not exist in the real loss surface L f (\u03b8). The parameter space consists of activation regions where saddle points exist, allowing movement without getting stuck in local minima. The loss surface L f (\u03b8) is non-convex and non-concave, with differentiable local minima being subglobal minima and non-differentiable local minima existing in regions with zero activation values. This explains the presence of poor local minima in large networks. The text discusses the presence of poor local minima in neural networks, specifically focusing on the concept of a linear region that always forms poor local minima in nonlinear datasets. It explains how manipulating biases can help identify this linear region and how subglobal minima in this region result in errors similar to linear regression. The text also mentions the importance of finding subglobal minima in the linear region to understand how the model can get stuck in this region. The text discusses the presence of poor local minima in neural networks, focusing on the linear region that forms poor local minima in nonlinear datasets. It explains how manipulating biases can help identify this region and how subglobal minima in this area result in errors similar to linear regression. The text also mentions the importance of finding subglobal minima in the linear region to understand how the model can get stuck in this region. The network has one hidden layer and no biases, with the number of hidden nodes increased from one to four. The ratio of poor regions decreases as the network size grows, shown numerically by identifying all subglobal minima of a simple network. The experiments were restricted to a small size of hidden layers and datasets to find all subglobal minima in a reasonable time. The text discusses solving linear equations to find the subglobal minimum in neural networks. It shows that larger networks are less likely to have poor subglobal minima, with the addition of nodes providing a path away from these minima. The text discusses estimating rich subglobal minima in neural networks using gradient descent methods on realistic datasets like MNIST. Experiments were conducted on networks with two hidden layers, biases, softmax outputs, and Adam Optimizer. Activation values were randomly selected and manipulated to estimate subglobal minima. Real networks were optimized to find rich subglobal minima in the loss surface. The experiments conducted on neural networks with two hidden layers showed that larger networks have more rich regions in the loss surface compared to smaller networks. Previous studies have shown that linear and deep linear networks exhibit similar properties under certain assumptions. Nonlinear networks also exhibit similar properties as linear networks when certain assumptions are met. Nonlinear DNNs exhibit similar properties to linear networks, with training error at local minima resembling global minimum error. The volume of sub-optimal local minima decreases exponentially compared to global minima. Most local minima in DNNs are near optimal, and wider networks have paths to global minima from random starting points. Starting points of DNNs approximate a diverse set of hypotheses. The loss surface of deep neural networks is a disjoint union of activation regions where every local minimum is a subglobal minimum. Poor local minima exist within the boundaries of activation regions, posing challenges for gradient descent methods. Further research is needed to understand non-differentiable local minima and extend knowledge about activation regions. The loss surface of deep neural networks consists of activation regions where local minima are subglobal minima. Understanding non-differentiable local minima and extending knowledge about activation regions is crucial for gradient descent methods.\u03b8 \u2208 R A is a differentiable point not within activation region boundaries. The local curvatures of L f (\u03b8) and L g A (\u03b8) are equivalent in region R A, making \u03b8 a local minimum (saddle point) in both functions. The lemma holds as DISPLAYFORM0 is a linear transformation of p j, q j, and c, and DISPLAYFORM1 2 is convex in terms of these variables. The lemma holds as the function is convex. The Hessian matrix is evaluated for non-zero activation values, resulting in positive and negative eigenvalues. The function is proven to be non-convex and non-concave. Gradients are organized, and critical points are identified as global minima. The lemma proves that every critical point of the function is either a global minimum or a saddle point. This is because the function is convex, leading to non-convex and non-concave behavior. Critical points are identified as global minima due to the convexity of the function."
}