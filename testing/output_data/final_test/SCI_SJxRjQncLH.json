{
    "title": "SJxRjQncLH",
    "content": "Neural networks have shown great performance in solving inverse problems in imaging, but face drawbacks compared to classical methods such as the need for expensive retraining and lack of provable error bounds. Recent work has used networks trained for image denoising as regularizers in energy minimization algorithms, achieving state-of-the-art results. However, restrictions on network architecture are necessary for provable convergence. Another approach involves training networks to output descent directions for a given energy function, with guaranteed convergence to a minimizer. Training separate networks is required for each problem and energy to minimize the energy. In this paper, a plug-and-play denoising network is combined with energy minimization methods to improve reconstruction tasks. Results show enhancements over classical methods with convergence guarantees. Image processing tasks involve modeling observed images as transformations of clean images under known operators and unknown noise. Regularization theory with iterative or variational methods is used to address ill-posed reconstruction problems. Neural networks have been successful in learning direct mappings for tasks like deblurring and denoising. Neural networks are effective for tasks like deblurring, denoising, and super-resolution, but lack guarantees on unseen data. Variational methods use energy minimization with data fidelity and regularization terms for theoretical guarantees. Regularizations like total variation may not capture complex structures well. Regularizations like total variation may not perfectly capture the complex structure of natural images. Hybrid models combining feed-forward networks and model-based approaches have been explored, including learning regularizers, designing network architectures resembling minimization algorithms, and using denoising operators instead of proximal operators. Theoretical guarantees for these approaches remain a challenge. Theoretical guarantees for algorithmic schemes utilizing denoising networks to regularize model-based inverse problems are difficult to derive unless the denoiser satisfies specific properties. Two simpler methods, gradient descent and proximal gradient methods, are considered as alternatives to primal-dual / ADMM approaches for minimizing nonconvex energies. By treating a gradient descent or proximal step on the regularization as a denoising operation, two algorithmic schemes can be formulated. Algorithmic schemes combine energy minimization methods with deep neural networks like convolutional neural networks. However, these schemes can be dangerous to use as they may diverge, as shown in an example with a noisy input image. The image became completely distorted and the algorithmic scheme diverged within the first 1000 iterations. The algorithmic scheme diverges due to the lack of a 1-Lipschitz continuous operator G. Enforcing non-expansiveness decreases denoising performance, and computing the best Lipschitz constant is proven to be NP-hard. To address this, a recent idea from [21] is adapted to force neural networks to predict a descent direction for a given model-based energy, ensuring convergence. The authors use the Euclidean projection onto the half space as the last layer of their network to improve convergence to the minimizer of the energy function. This approach shows higher peaks of the PSNR value in early iterations compared to classical gradient descent. The combination of flexible algorithmic schemes with the idea from a previous study helps safeguard the underlying algorithm. The authors introduce a continuously differentiable, strictly convex, and coercive energy function. They rewrite algorithmic schemes to resemble gradient descent iterations and safeguard them by projecting onto the half-space of descent directions. A parameter \u03b1 determines the influence of the data term and denoising, and a backtracking line-search mechanism is used to update iterates. This scheme guarantees convergence to the minimizer of the energy function. The authors propose a scheme that converges to the minimizer of the energy function. They use a discrepancy principle to stop iterations, with parameters for noise level estimation and scaling factor. The implementation was tested on image reconstruction tasks with Gaussian deblurring and single image super resolution, using a denoising network and TV regularization with Huber-norm. The authors propose a scheme for image reconstruction using TV regularization with Huber-norm. Hyperparameters were optimized through grid search. Gradient descent step on the data term was found to not yield much additional information. Different parameters were used for deblurring and super resolution tasks. Reconstruction quality was compared to ground truth over 500 iterations. PSNR quickly peaked before slowly converging. The authors combined deep learning and energy minimization methods to solve inverse problems in image reconstruction. Results showed that their prox scheme outperformed gradient descent in deblurring tasks, with PSNR values compared using a discrepancy principle. The convex combination method peaked earlier but not as high as the prox method in super resolution tasks. The authors developed a convergent algorithmic scheme by combining deep learning and energy minimization methods for image reconstruction. Their approach can adapt to different problems using a single denoising network without the need for retraining. Experimental results showed better performance than the energy minimization baseline. They are willing to discuss these aspects further at the NeurIPS workshop."
}