{
    "title": "S1lTg3RqYQ",
    "content": "Image-to-image translation has recently gained attention in deep learning, focusing on one-to-one or many-to-many mappings. A more practical approach is many-to-many mapping in an unsupervised way, which is challenging due to lack of supervision and domain variations. To address this, the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network conditions the translation process on an exemplar image in the target domain, transferring style information using Adaptive Instance Normalization. Image-to-image translation involves mapping images from one domain to another, such as converting semantic maps to real images or low-resolution to high-resolution images. The Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network uses feature masks to guide the translation process without semantic labels, ensuring semantic consistency. Experimental results demonstrate that EGSC-IT can translate images to diverse instances in the target domain while maintaining semantic coherence. The EGSC-IT framework enables image-to-image translation without the need for paired training data, making it suitable for tasks like cross-city street view or male-female face translation. The network uses feature masks to guide the translation process and ensure semantic consistency, allowing for diverse translations in the target domain while maintaining coherence. The EGSC-IT framework utilizes feature masks for image translation without paired data. It involves transferring content representation using affine parameters and cycle-consistency loss for domain mapping. The method assumes a shared latent space constraint for corresponding images from different domains. Exemplar Guided & Semantically Consistent I2I Translation (EGSC-IT) addresses the issue of failing to capture the multimodal nature of image distribution within the target domain. It proposes disentangled representations for domain-shared and domain-specific information, but acknowledges the challenge of representing complex inner-variations in multimodal domains. EGSC-IT proposes using an exemplar image in the target domain to enable multimodal image translations and explicit control over the process. This approach avoids mode collapsing issues seen in other methods. EGSC-IT proposes decomposing the latent space into domain-shared and domain-specific components for image translation. The domain-shared component focuses on image content, while the domain-specific component captures style information. AdaIN is applied to the shared content component using parameters from the target domain exemplar for translation. To maintain semantic consistency during image translation without using semantic labels, feature masks are proposed as attention modules that decouple different semantic categories in an unsupervised way. Feature masks guide the process under perceptual and adversarial losses, ensuring that diverse objects and scenes in an image are translated accurately. Our contribution introduces a novel approach for multimodal image-to-image translation with explicit style control. Feature masks are utilized for unsupervised translation, providing semantic guidance without labels. Evaluation demonstrates robustness to mode collapse and the ability to generate semantically consistent results. The curr_chunk discusses various methods for image-to-image translation, such as pix2pix, pix2pixHD, and BicycleGAN, which require paired training data. However, unsupervised methods have been developed to address this limitation in scenarios where collecting paired data is difficult. Recently, unsupervised methods have been proposed for image-to-image translation without paired training data, addressing the ill-posed problem of infinite mappings between unpaired image domains. Constraints like cycle-consistency and shared-latent space have been added to networks to regularize the learning process. The text discusses the limitations of current constraints in image translation for complex domains with large variations. A new approach is proposed to use a target domain exemplar as guidance during image translation through AdaIN. This allows for multimodal translations and explicit control over the style transfer process. Feature masks are computed to maintain semantic consistency during translation, differentiating it from previous methods like MUNIT. The text discusses style transfer in image translation, focusing on preserving semantic categories and consistency. Various methods like BID34, UNIT, and MUNIT have been used for style transfer by matching feature correlations in deep neural networks. Several feed-forward neural networks have been proposed to improve speed and flexibility in style transfer. Methods like AdaIN and WCT align features of content and style images, but suffer from non-photorealistic results. To address this, feature masks are proposed to model semantic information without using semantic labels. Our method combines AdaIN and feature masks to achieve multimodal image-to-image translations without needing ground-truth semantic labels. It aims to learn a many-to-many mapping between domains guided by an exemplar's style while maintaining semantic consistency. The image is decomposed into domain-shared content and style components for translation. The EGSC-IT framework combines domain-shared content and domain-specific style components for image translation. It uses VAE-GAN networks for each image domain and employs weight sharing to learn shared content across domains. The EGSC-IT framework utilizes VAE-GAN networks for image translation, combining domain-shared content and domain-specific style components. Weight sharing is employed to learn shared content across domains, with a common latent space for mapping image pairs from different domains. AdaIN is applied to the shared content component to incorporate domain-specific style before the decoding stage. The EGSC-IT framework uses VAE-GAN networks for image translation, combining shared content and domain-specific style components. AdaIN is applied to the shared content to incorporate domain-specific style before decoding. Different exemplar images in the target domain can translate an image in the source domain to different sub-styles, allowing for multimodal translations with explicit style control. The EGSC-IT framework utilizes VAE-GAN networks for image translation, incorporating shared content and domain-specific style components. Applying AdaIN to the shared content allows for explicit style control. However, directly applying AdaIN may mix styles of different objects and scenes, leading to inconsistent translations. To address this issue, feature masks are proposed as an alternative to semantic labels for estimating style variations. The proposed method involves computing feature masks to estimate semantic categories without using ground-truth labels. These masks, acting as attention modules, contain significant semantic information to ensure consistency during translation. The framework includes two Encoders, a feature mask network, and an AdaIN network for image translation. The proposed method involves using feature masks and AdaIN network for image translation. Two Generators and two Discriminators are used, along with a VGG sub-network for perceptual losses. The learning procedure includes VAEs, GANs, cycle-consistency, and perceptual losses. The training starts with pre-training the feature mask and AdaIN network separately within a VAE-GAN architecture. The method involves using feature masks and AdaIN network for image translation within a VAE-GAN architecture. The overall loss includes VAEs, GANs, cycle-consistency, and perceptual losses with content and style losses defined based on dataset variations and tasks. The feature maps are extracted using the first convolutional layer of VGG19. The method utilizes feature masks and AdaIN network for image translation within a VAE-GAN architecture. Various losses such as VAE, GAN, cycle-consistency, content, and style losses are defined to achieve I2I translation, multimodal translation, and semantic consistency. The losses help maintain shared latent space, encourage multimodal translation, and ensure semantic consistency by utilizing feature mask information. EGSC-IT utilizes feature mask information for semantic consistency in image translation, without relying on hard correspondences between semantic labels. Evaluation includes single-digit, multi-digit, and street view translation tasks, with results presented qualitatively and quantitatively. Ablation studies are conducted on various components, with additional evaluation on semantic segmentation and face gender translation tasks. The MNIST-Single dataset is used in a controlled experiment to test translation diversity and generalization ability. It consists of two domains with different foreground and background settings. Domain B has color variations for digits 0-4 and fixed colors for digits 5-9. The purposeful data imbalance tests for mode collapse issues in image translation. EGSC-IT successfully transfers source images into the style of exemplar images by utilizing feature masks, AdaIN, and perceptual loss. Ablating feature masks results in incorrect foreground and background shapes, while omitting AdaIN leads to the mode collapse issue in translation. The exemplar's style information helps the network learn many-to-many mappings and avoid mode collapse. Perceptual losses encourage semantic knowledge without ground-truth labels. Other I2I methods like CycleGAN and UNIT struggle with mode collapse and inaccurate translations. MUNIT can transfer style but mixes foreground and background. EGSC-IT achieves higher SSIM scores in image translation. Our full EGSC-IT outperforms other alternatives in terms of SSIM scores. Comparisons with Neural ST, AdaIN, and WCT style transfer methods show successful style transfer but lack semantic consistency. Quantitative results for style transfer methods are available in supplementary material. Visualization using t-SNE embeddings confirms EGSC-IT's ability to match target domain distributions effectively. The MNIST-Multiple dataset is used to test if a network can understand and translate digits in an image. It contains all ten digits randomly placed in grids. Two domains, A and B, have different background and foreground settings. The goal is to translate images from domain A to B while preserving semantic information and style. The experiment involves translating images from domain A to domain B, maintaining digit class content and background/foreground style. The model achieves good results without needing semantic labels or paired data. For street view translation, the model successfully translates images between GTA5 and Berkeley Deep Drive datasets, adapting to different illumination and weather conditions. The study focuses on image translation between different domains, with a comparison between MUNIT and EGSC-IT models. EGSC-IT successfully translates details like sky color and illumination for large variations. Semantic segmentation performance is used to evaluate image translation quality. Translated images from GTA5 to BDD datasets are used to train a Deeplab model, showing improved mean Intersection over Union scores. Training with translated synthetic images improves segmentation results, showing successful domain style translation with semantic consistency. Some artifacts are present in challenging cases like day to night or night to day translations. Future work may explore extending the method to semi-supervised settings. The EGSC-IT framework learns multimodal mapping across domains in an unsupervised manner. The EGSC-IT framework learns multimodal mapping across domains in an unsupervised way, combining AdaIN with feature masks to transfer style while maintaining semantic consistency. Face gender translation is demonstrated on the CelebA dataset, showing successful translation of gender and style attributes. The EGSC-IT framework uses AdaIN and feature masks for unsupervised multimodal mapping, successfully translating gender and style attributes in face images. Additionally, the method demonstrates generalization ability through letter-digit translation tasks using the EMNIST dataset. The EMNIST dataset, similar to MNIST, is used for letter-digit translation tasks. Results show successful generalization from single-digit to letter data. Comparison of style transfer methods is done using SSIM score. CycleGAN and UNIT struggle with color translation, appearing more artificial. This is attributed to their one-to-one mapping approach. MUNIT can translate backgrounds successfully but struggles with foreground color consistency, resulting in fake-looking colors. Ablation study shows that removing feature mask leads to color mismatches, while removing AdaIN reduces the model to unimodality. Larger size results for GTA5 \u2194 BDD translation are provided in Fig. 14, along with ablation study results in FIG0. The network architecture and training parameters for the translation experiments are detailed in Tab. 6. The number of down-sampling and up-sampling convolutional layers is set to n1 = 1 for single-digit translation and n1 = 3 for other experiments. The number of residual blocks in Encoder and Generator is n2 = 4 with one sharing layer, and the discriminator has n3 = 5 convolutional layers. The threshold parameter \u03b7 adjusts the feature mask's influence on information flow. The threshold parameter \u03b7 is set to 0.5 to balance information retention and semantic consistency in the feature mask. The Adam optimizer with specific parameters is used, and training stability is maintained by updating the encoder, generator, and discriminator a certain number of times per iteration. Loss weights and augmentation techniques are adjusted based on dataset variations and tasks, with a focus on face gender translation."
}