{
    "title": "B1xm3RVtwB",
    "content": "In recent years, AI has made significant progress in achieving near or superhuman performance in adversarial benchmark problems like Go, Poker, and Dota. However, real-world success often requires collaboration and communication. Hanabi, a card game focusing on theory of mind, has emerged as a new benchmark for AI to address cooperative settings. This presents a unique challenge for Reinforcement Learning agents to be informative when observed by others. Reinforcement Learning (RL) requires agents to explore to discover good policies, but this randomness can make their actions less informative during training. A new deep multi-agent RL method, Simplified Action Decoder (SAD), resolves this by allowing agents to observe the greedy actions of their team mates. SAD achieves state-of-the-art results for 2-5 players in the Hanabi challenge by combining this intuition with state prediction and multi-agent learning best practices. This contrasts with human social behavior and the focus on zero-sum games in AI progress. The focus in AI progress has been on zero-sum games like Go, poker, and chess, where communication is typically futile. However, enabling smooth social interactions requires reasoning over the intents and beliefs of others. Humans can interpret actions and act informatively when observed, showcasing unique abilities for communication and coordination. Theory of Mind (ToM) is crucial for agents to understand the intentions and beliefs of others when observed. This reasoning is essential in cooperative multi-agent settings and real-world scenarios, such as autonomous cars navigating complex traffic situations. Hanabi, a partially-observable card game, serves as a benchmark challenge for AI research in ToM. In Hanabi, players need to find conventions for effective information exchange through actions, challenging AI research in Theory of Mind. State-of-the-art agents were developed using handcrafted algorithms, outperforming deep multi-agent RL methods due to the conflict between exploration and informative policy learning. One solution to the challenge of informative policy learning in Hanabi is the Bayesian Action Decoder (BAD), which samples deterministic partial policies from a distribution conditioned on a common knowledge Bayesian belief. However, BAD's complexity and reliance on expert knowledge make it less efficient and generalizable. Additionally, BAD's training using actor-critic methods is sample inefficient and prone to local optima, requiring population-based training to mitigate these issues. The Simplified Action Decoder (SAD) is proposed as an alternative to the Bayesian Action Decoder (BAD) for informative policy learning in Hanabi. SAD addresses the limitations of BAD by utilizing a different approach that allows for exploration without sacrificing interpretability. SAD operates under the centralized training with decentralized control (CT/DC) regime, enabling information exchange among agents while ensuring compatibility with decentralized execution. This approach eliminates the need to choose between informative greedy actions and exploratory random actions during training. In the Simplified Action Decoder (SAD) approach, each agent takes a greedy action that is observed by teammates but not executed by the environment. This allows for exploration without sacrificing interpretability in training. Additionally, an auxiliary task can be trained to predict key game properties from action-observation trajectories. The distributed version of recurrent DQN is used to improve sample efficiency and reduce local optima in a multi-agent setting. A joint-action Q-function is trained using Value Decomposition Networks (VDN) for off-policy learning. The SAD approach establishes a new state-of-the-art for 2-5 player Hanabi, requiring less expert knowledge and compute. Results are easily verifiable and extendable, with plans to open-source training code and agents for further research into self-play aspects of Hanabi. Our work on Hanabi focuses on ad-hoc teamwork and differs from previous research on emergent communication protocols in deep multi-agent RL. Unlike other studies, our agents communicate through observable environment actions instead of a cheap-talk channel. Our work on Hanabi involves cooperative multi-agent learning in partially observable settings under centralized training and decentralized control. Previous research focused on hand-coded methods and heuristics, such as SmartBot and \"hat-coding\" strategies. These strategies use information theoretic ideas to reveal information to all agents simultaneously. In contrast to hand-coded methods like SmartBot, Bard et al. (2019) evaluated Deep RL methods for cooperative multi-agent learning in Hanabi. They found that while DQN and actor-critic agents performed well for 2-player games, they struggled with 3-5 players. Their DQN agent showed promise but had issues with -greedy exploration. In a ToM task, agents used -greedy exploration and then set it to zero after a short burn-in phase. Recent advancements in Hanabi by RL agents are discussed in Section 1 and Section 4. There are efforts to train agents robust to different team-mates and human-AI collaboration. Poker, a different multi-agent setting, has seen success with search techniques. Hanabi has also utilized search methods. This paper assumes a Dec-POMDP framework where N agents interact in a partially observable environment. In a ToM task, agents interact in a partially observable environment. Agents obtain observations at each time step, including the last action of the acting agent. Actions are observable in turn-based settings, with agents taking actions based on their policy. The policy is represented by recurrent neural networks like LSTMs. In cooperative multi-agent reinforcement learning, agents aim to maximize the total expected return by sharing parameters and using Q-learning to approximate expected returns for state-action pairs. The policy is represented by recurrent neural networks like LSTMs, and agents use a -greedy exploration scheme. In Deep Q-Learning (DQN), the Q-function is trained efficiently using the Bellman equation and parameterized by a deep neural network. Best practice components like double-DQN, dueling network architecture, and prioritized replay are also incorporated. A distributed training architecture is employed where different actors collect experiences in parallel. In multi-agent settings, a recurrent neural network is used with a central replay buffer. Auxiliary tasks and Independent Q-Learning are common practices, but IQL does not correct for exploratory behavior of other agents. In multi-agent settings, IQL does not take advantage of centralized training with decentralized control (CT/DC). Various approaches for learning joint-Q-functions in the CT/DC regime include Value-Decomposition-Networks (VDN) and QMIX. Bayesian reasoning is fundamental in interpreting the actions of another agent and Theory of Mind (ToM) in general. In multi-agent settings, Bayesian reasoning is crucial for interpreting the actions of other agents. Agents update their beliefs based on observed actions of teammates, but computing explicit beliefs can be impractical due to computational complexity. Higher order beliefs can be prohibitively costly when used as inputs to policies. In multi-agent settings, Bayesian reasoning is crucial for interpreting actions. Agents update beliefs based on observed actions, but computing explicit beliefs can be impractical due to complexity. RNNs are used to learn implicit representations of sufficient statistics over the distribution of the Markov state. Exploration impacts beliefs, with a focus on fully-cooperative settings and deterministic optimal policies. In multi-agent settings, Bayesian reasoning is crucial for interpreting actions. Agents update beliefs based on observed actions using RNNs to learn implicit representations of sufficient statistics. Exploration impacts beliefs in fully-cooperative settings with a focus on deterministic optimal policies. The exploration scheme \u03c0 a (u a t |O(a , \u03c4 t )) is based on a -greedy method, with the posterior including an additional term B(\u03c4 t ) that carries over unfiltered density from the prior. In the context of multi-agent settings, Bayesian reasoning is essential for interpreting actions. The posterior collapses to the prior in certain limits, making beliefs less informative. The Bayesian Action Decoder (BAD) addresses this issue by shifting exploration to deterministic partial policies, but at the cost of complexity and computation requirements. A simpler approach is proposed in this paper, highlighting the impact of -greedy exploration on decoding actions. In the Simplified Action Decoder (SAD), the acting agent can take two actions at each time step: the standard environment action and a greedy action. The greedy action is not executed but is used to update beliefs of other agents in a multi-agent setting. In the Simplified Action Decoder (SAD), the acting agent can take two actions at each time step: the standard environment action and a greedy action. The greedy action is used to update beliefs of other agents in a multi-agent setting during training, where information can be freely exchanged. This approach does not require passing extra information during decentralized control. The greedy action can be obtained from the observation function and used as input, even in cases where it is indirectly observed by all agents through environment dynamics. To decode the information in the greedy action, a learned inverse model can be used to recover the action from observation history during execution. To encourage meaningful decoding of information in the greedy action, an auxiliary task can be added to the training process. A recurrent version of DQN with distributed training, dueling networks, and prioritized replay is used. A joint Q-function using VDN is learned to address challenges in multi-agent off-policy learning. Players in a two-player matrix game take turns acting based on observed actions. The effectiveness of SAD is verified in a simplified game setting before applying it to more complex scenarios like Hanabi. Hanabi is a cooperative card game where players work together to complete piles of cards with ranks 1 to 5 and colors G/B/W/Y/R. Players have private cards drawn from two options and take actions to achieve a payout based on their cards and actions. Communication can help players achieve higher payouts, up to 10 points for every pair of cards dealt. In Hanabi, players work together to complete piles of cards with ranks 1 to 5 and colors G/B/W/Y/R. Players can observe their team mates' cards but not their own, requiring communication to understand what cards can be played. Actions include giving hints about specific ranks or colors, costing information tokens, and playing cards to progress towards completing the firework display. In Hanabi, players work together to complete piles of cards with ranks 1 to 5 and colors G/B/W/Y/R. Players can observe their team mates' cards but not their own, requiring communication to understand what cards can be played. Actions include giving hints about specific ranks or colors, costing information tokens, and playing cards to progress towards completing the firework display. Players can choose to play a card that matches the next card for the firework's color to score points, but if they fail, they lose a life token. If all life tokens are lost, the game ends and points are forfeited. The maximum score achievable is 25 points by completing all five fireworks with five cards each. The Hanabi Learning Environment (HLE) is used for experimentation to ensure reproducibility and comparability of results. In a distributed Q-learning method for MARL, a shared prioritized replay buffer is used by N asynchronous actors with a centralized trainer. Each actor thread runs K environments sequentially, utilizing a GPU to compute actions. Multiple simulations can be run with moderate resources using 80 actor threads and environments. In Hanabi experiments, 80 actor threads run 80 environments on a single machine with 40 CPU cores and 2 GPUs, avoiding the need for hundreds of CPU cores. Adding a greedy action input significantly improves performance in a matrix game, with tabular IQL achieving an average reward of 9.5 points, which increases to 9.97 \u00b1 0.02 with the addition of the greedy action input. The study compares the performance of different agents in the Hanabi benchmark game. Results show that the SAD agent outperforms IQL and VDN in terms of average score and win rate for 2, 4, and 5 players. The auxiliary task significantly improves the 2-player performance. The auxiliary task significantly boosts 2-player performance but hurts 3-5 player performance. Despite extensive training, performance has not plateaued for 3-5 players, indicating room for improvement. The best model from various training runs for each method in Table 2 establishes a new state-of-the-art (SOTA) for learning methods on the self-play part of the Hanabi challenge for 2-5 players, with significant improvements for 3-5 players. The model outperforms the ACHA and BAD agents in average score, even though they used population-based training and more compute. The counting convention used may explain the higher win rate of the BAD agent, but our baseline methods still exceed its mean score. The Simplified Action Decoder (SAD) is a new deep multi-agent RL algorithm that improves performance in the 2-player setting on the Hanabi benchmark. It outperforms previous methods and requires less compute. However, there is still room for improvement in closing the performance gap between SAD and known benchmarks. The Hanabi agent uses a dueling network architecture with 1 fully connected layer and 2 LSTM layers. The network configuration remains consistent across all experiments, with a focus on integrating search with RL for improved performance. The Hanabi agent utilizes a dueling network architecture with 1 fully connected layer and 2 LSTM layers. The maximum episode length is 80 steps, stored in the replay buffer as one training sample. LSTM hidden states are initialized as zero during training to avoid \"slate hidden states\" issue. Exploration and experience prioritization follow strategies from previous studies. Each actor executes an \u03b5-greedy policy with \u03b5 = 0.1 and \u03b1 = 7. All players in a game use the same epsilon for simplicity. Priority computation involves per time-step priority \u03b4 t as the TD error and per episode priority \u03b4 e = \u03b7 max t \u03b4 i + (1 \u2212 \u03b7)\u03b4 with \u03b7 = 0.9. Priority and importance sampling exponents are set to 0.9 and 0.6 respectively. Training involves n-step return and double Q-learning for target computation. The network uses double Q-learning for target computation with a discount factor of 0.999. Adam optimizer is used for updating the network with fixed hyper-parameters. Independent Q-Learning is used where each player estimates Q value independently. Players operate on observations to update their hidden states and write episodes into a prioritized replay buffer. The buffer contains 131072 episodes. The SAD agent is trained independently using a prioritized replay buffer with 131072 episodes. A warm-up phase with 10,000 episodes precedes training. Batch sizes vary based on the number of players in the game. The agent is built on a joint Q-function, where each player's Q value is summed. The replay buffer size is adjusted for different player counts, with batch sizes tailored accordingly. An auxiliary task can be added to aid in decoding the greedy action, such as predicting a player's own hand in Hanabi. The auxiliary task in our experiments involves predicting the status of a card in a player's hand, which can be playable, discardable, or unknown. The average cross entropy loss per card is added to the TD-error of reinforcement learning during training. Learning curves of different algorithms are shown in Figure 3, averaged over 13 seeds per algorithm per player setting."
}