{
    "title": "HyxCxhRcY7",
    "content": "The text discusses the importance of detecting anomalous inputs in machine learning systems, especially in deep learning with larger inputs. The Outlier Exposure (OE) approach leverages diverse image and text data to improve anomaly detection by training detectors against an auxiliary dataset of outliers. Extensive experiments show that OE significantly enhances detection performance, addressing issues like generative models assigning higher likelihoods to certain images. Machine Learning systems often face data unlike their training data, requiring models capable of detecting anomalies. Deep learning models provide high performance but struggle with distribution mismatches, leading to high confidence predictions on anomalous test examples. Outlier Exposure (OE) helps mitigate this issue by analyzing flexibility and robustness, identifying characteristics of the auxiliary dataset that improve performance. Deep neural network classifiers tend to give high confidence predictions on anomalous test examples, invalidating the use of prediction probabilities as calibrated confidence estimates. Detecting anomalous examples is crucial, and various methods have been developed to assign anomaly scores to inputs for detecting out-of-distribution examples without requiring modeling the full data distribution. These approaches have shown effectiveness across complex input spaces like images, text, and speech, using heuristics to detect unmodeled phenomena. In this paper, a method called Outlier Exposure (OE) is proposed to train models to detect unmodeled data by learning cues for identifying out-of-distribution inputs. By exposing the model to diverse datasets, OE helps improve existing methods for detecting anomalies. Through experiments in Computer Vision and Natural Language Processing tasks, Outlier Exposure is shown to enhance anomaly detectors' ability to generalize and perform well on unseen data. Outlier Exposure improves anomaly detectors' generalization to unseen outlier distributions, outperforming existing methods. It enhances density estimates for out-of-distribution detection and boosts neural network classifier calibration in realistic scenarios. The code is available at https://github.com/hendrycks/outlier-exposure. DeVries & Taylor (2018) propose using a pre-trained classifier as an out-of-distribution detector by attaching an auxiliary branch. Liang et al. (2018) improve OOD detectors by making the maximum softmax probability more discriminative with adversarial perturbations. Lee et al. (2018) train a classifier with a GAN to have lower confidence on GAN samples. The classifier is trained to have lower confidence on GAN samples and for each testing distribution of anomalies, they tune the classifier and GAN using samples from that out-distribution. Unlike previous works, this method is trained without tuning parameters for specific types of anomaly test distributions. Other works also encourage lower confidence on anomalous examples, with recent theoretical guarantees provided for detecting out-of-distribution examples. Outlier Exposure utilizes an auxiliary dataset disjoint from test-time data to improve network representations for anomalies. Outlier Exposure uses an auxiliary dataset separate from test-time data to enhance network representations for anomaly detection. Previous studies have shown that training on adversarial examples, pre-training on web images, Amazon reviews, or ImageNet, and learning from images scraped from search engines can improve object detection performance. Outlier Exposure involves training a model to detect whether a sample is from a known distribution or an unknown distribution using an auxiliary dataset. The model learns heuristics to generalize to unseen distributions and minimize the learning objective over its parameters. Outlier Exposure minimizes the objective over model parameters without labeled data. The specific formulation of OE depends on the task and OOD detector used. Different formulations include cross-entropy and margin ranking loss. Evaluations are done on various datasets with in-distribution data, anomalous examples, and baseline detectors. In experiments, Outlier Exposure (OE) is shown to help detectors generalize to new anomalies without access to test distribution during training. OE complements binary anomaly detectors and using real diverse data is more effective than synthetic outliers. Density estimation experiments reveal unexpected results with higher density assigned to out-of-distribution samples. Outlier Exposure helps detectors generalize to new anomalies without access to test distribution during training. Density estimation experiments show higher density for out-of-distribution samples. Evaluation metrics include AUROC, AUPR, and FPRN to assess detection performance. Outlier Exposure enhances OOD detection techniques by using multiclass classification as the original task. The FPRN metric is used to measure the probability of false alarms, with a lower FPRN indicating better performance. The classifier is represented by the function f : X \u2192 R k, ensuring 1 T f (x) = 1 and f (x) 0. Maximum Softmax Probability (MSP) is considered as a baseline for OOD detection. The OOD detection technique Outlier Exposure improves performance by fine-tuning a pre-trained classifier to have a more uniform posterior on out-of-distribution samples. Training from scratch with Outlier Exposure can result in even better performance. Outlier Exposure (OE) can improve performance by fine-tuning a pre-trained classifier to have a more uniform posterior on out-of-distribution samples. Training from scratch with OE can result in even better performance than fine-tuning. This approach works on different architectures as well. OE with a real and diverse dataset shows large gains over using synthetic samples from a GAN. Anomalous examples should have low probability density, as they are scarce in the data distribution. Density estimates are another means to detect anomalies effectively. Anomalous examples should have low probability density, as they are scarce in the data distribution. Density estimates are another means to detect anomalies effectively. OE improves density estimates on low-probability, outlying data using PixelCNN++ as a baseline OOD detector trained on CIFAR-10. The OOD score is calculated as bits per pixel (BPP), defined as nll(x)/num_pixels, where nll is the negative log-likelihood. OE is implemented with a margin loss over the log-likelihood difference between in-distribution and out-of-distribution samples. Anomalous examples should have low probability density, as they are scarce in the data distribution. OE improves density estimates on outlying data using PixelCNN++. The OOD score is calculated as bits per pixel (BPP), defined as nll(x)/num_pixels. OE is implemented with a margin loss over the log-likelihood difference between in-distribution and anomalous examples. OOD detection task simplifies the task by providing scores for sequences in unseen datasets. Models are trained for specific epochs and fine-tuned using OE on WikiText-2. Outlier Exposure can work in various classification regimes. Outlier Exposure can improve performance in classification regimes, such as multilabel classifiers on CIFAR-10. By using maximum prediction probability as the OOD score, mean AUROC increases from 88.8% to 97.1%. However, classifiers with reject options or multilabel outputs are not as competitive as OOD detectors with multiclass outputs. Diversity of OE out is crucial, as a CIFAR-100 classifier with CIFAR-10 as OE out hardly improves over the baseline. The study found that exposing a CIFAR-10 classifier to outlier classes from CIFAR-100 significantly improved performance, with the average AUPR increasing as more classes were introduced. Dataset diversity was highlighted as crucial, with experiments using a small fraction of images from a large dataset showing negligible degradation in detection performance. Additionally, Outlier Exposure was shown to improve calibration by providing confidence estimates that align with empirical correctness frequencies. Outlier Exposure improves calibration by aligning confidence estimates with empirical correctness frequencies, including out-of-distribution samples at test-time. It enhances OOD detectors and demonstrates improved performance in various settings. Outlier Exposure is a technique that enhances OOD detectors by using out-of-distribution samples to teach a network heuristics for detecting new examples. It is broadly applicable in vision and natural language settings, improves model calibration, and is computationally inexpensive. The study evaluates the performance of OOD detectors using Gaussian, Rademacher, Bernoulli, and Blobs anomalies. The detectors are compared before and after fine-tuning with Outlier Exposure, showing improved results. Blobs data consist of algorithmically generated shapes with definite edges. Icons-50, Textures, Places365, LSUN, and ImageNet are datasets used for various experiments. CIFAR-10 and CIFAR-100 are also utilized, along with Chars74K for character recognition. The curr_chunk discusses various datasets used for different experiments, including Chars74K, Places69, SNLI, IMDB, Multi30K, WMT16, Yelp, and English Web Treebank (EWT). These datasets cover character recognition, scene categories, natural language inference, sentiment classification, image descriptions, and more. Additionally, validation data sets are created for each experiment. Training a network from scratch with outlier exposure (OE) results in superior performance compared to fine-tuning with OE. For instance, a CIFAR-10 Wide ResNet trained normally has a 5.16% error rate and 34.94% FPR95. Fine-tuning increases the error rate to 5.27% and FPR95 to 9.50%, while training from scratch with OE reduces the error rate to 4.26% and FPR95 to 6.15%. Training with OE from scratch yields better error rate, calibration, and out-of-distribution detection performance compared to fine-tuning. Outlier Exposure (OE) is used for fine-tuning as it saves time and GPU memory compared to training from scratch. OE improves vision out-of-distribution (OOD) detection performance for various networks, not just Wide ResNets. Models with OE use -H(U; f (x)) as an OOD score, which considers classes with small probability mass. Additionally, models with OE are trained to give anomalous examples a uniform posterior, leading to improved performance in OOD detection. Results are percentages and an average of 10 runs for various datasets like CIFAR-10. Models integrated into decision-making processes should indicate trustworthiness and avoid overconfidence. Model calibration ensures predicted probabilities match empirical frequencies, improving accuracy in event prediction. Previous research focuses on calibrating systems for both test-time queries and encountering samples from different distributions. To evaluate a multiclass classifier's calibration, three metrics are presented. The MAD Calibration Error is estimated using Soft F1 Score, which is an improper scoring rule. Classifier confidence should be controlled to avoid overconfidence in predictions. The Soft F1 score is used to measure the calibration of a system with an imbalance between mistaken and correct decisions. It involves treating mistakes as positive examples and calculating the model's confidence in anomalies. One way to estimate a classifier's confidence is by adding a logistic regression branch to the network. One way to estimate a classifier's confidence is by adding a logistic regression branch onto the network. Confidence values are in the range of [0, 1] using different methods such as the model's logits, the logistic sigmoid, and softmax temperature tuning. Guo et al. (2017) suggest tuning a temperature parameter in the softmax to improve calibration. Training experiments are conducted on various datasets with different numbers of training examples held out. Outlier Exposure improves model calibration by fine-tuning the classifier and determining optimal temperatures. Confidence estimates for out-of-distribution points should be low. Temperature tuning can enhance calibration, but confidence cannot be less than 1/k. Adding a reject option or an additional class can address this issue. In Section 5, a simple 0-1 posterior rescaling technique improves calibration performance by allowing a network to express no confidence on out-of-distribution inputs. This technique consistently enhances calibration, especially when fine-tuned with Outlier Exposure using temperature tuning. Additional PR and ROC Curves are shown using the Tiny ImageNet dataset and various anomalous distributions in FIG8."
}