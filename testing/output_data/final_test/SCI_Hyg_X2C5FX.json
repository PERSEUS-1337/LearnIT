{
    "title": "Hyg_X2C5FX",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in real-world applications, with various improved variants. However, there is a lack of visualization and understanding of GANs, raising questions about how they represent the visual world internally, the causes of artifacts in results, and the impact of architectural choices on learning. Answering these questions could lead to new insights and better models. In this work, an analytic framework is presented to visualize and understand GANs at different levels - unit, object, and scene. Interpretable units related to object concepts are identified, causal effects are quantified, and contextual relationships are examined. Practical applications include comparing internal representations, improving GANs by locating artifact-causing units, and manipulating objects in scenes. Open source interpretation tools are provided for researchers and practitioners to better understand GAN models. In this work, the internal representations of GANs are studied to understand how they produce photorealistic images and why some variants perform better than others. Key questions include what knowledge a GAN needs to learn to generate specific images and what causes mistakes in producing unrealistic images. In this study, the internal representations of GANs are analyzed to understand how they generate realistic images and the factors influencing their performance. The focus is on how GANs represent objects in images and whether they contain variables corresponding to objects like doors and trees. The goal is to determine if these variables directly cause the generation of objects or if they are simply correlated. The study also explores how relationships between objects are represented in GANs. The study analyzes GANs' internal representations to understand how they generate realistic images and the factors influencing their performance. It focuses on how GANs represent objects like trees and doors in images, exploring if these variables directly cause object generation or are simply correlated. The study also delves into how relationships between objects are represented in GANs, showcasing a method for visualizing and understanding GANs at different levels of abstraction. The study provides a systematic analysis of GANs' internal representations, focusing on object concepts like trees. It intervenes within the network to identify units causing objects to appear or disappear, quantifying their causal effect. The study also examines the contextual relationship between causal object units and the background, showcasing practical applications for comparing internal representations, debugging, and improving GANs. Generative Adversarial Networks (GANs) have shown improvement in generating diverse results, from simple digits and faces to natural scene images and photorealistic portraits. GANs have enabled applications like visual recognition, image manipulation, and video generation. However, there is a lack of work visualizing what GANs have learned, with prior studies focusing on manipulating latent vectors to observe changes in results. The relationship between representation units and trees in the output is analyzed using dissection and intervention. Dissection measures agreement between a unit and a concept by comparing heatmaps, while intervention measures the causal effect of units on a concept. Trees increase after unit insertion and decrease after unit ablation, with the average difference in tree pixels measuring the causal effect. Visualizing deep neural networks involves reconstructing salient image features or synthesizing input images. Our work explores deep generative models trained for image generation and understanding neural representation in biology, focusing on local and distributed representations in biological neural networks. Computational models of biological learning are also discussed. Neurons play a crucial role in biological learning, with sparse and local representations aiding generalization. Deep neural networks can explain decisions through heatmaps and salience scoring. Recent studies have looked at feature vectors and individual channels' contributions to predictions. Our method aims to analyze how objects are encoded by a GAN generator's internal representations. The GAN generator G creates images from a latent vector z. Activations of specific units in the generator match object segmentations with IoU scores. The tensor r from a generator layer contains information to deduce the presence of visible classes in the image. The tensor r from a generator layer contains information to deduce the presence of visible classes in the image by encoding it in a way that allows for dissection and identification of explicit representations of object classes. The tensor r from a generator layer encodes information for identifying explicit representations of object classes in images. By measuring agreement between individual units of r and classes, causal sets of units can be identified to measure causal effects. Units can locate object classes when upsampled and thresholded, allowing for spatial agreement quantification with semantic segmentations. The spatial agreement between individual units of a generator layer and object classes in images is quantified using semantic segmentation. By thresholding and upsampling the feature map, a binary mask is produced to indicate the presence of a class in the generated image. The threshold is chosen to maximize information quality, and the intersection-over-union measure is used to rank the agreement between units and classes. The joint entropy H is analyzed for mutual information I. IoU u,c is used to rank concepts related to each unit. Examples of interpretable units with high IoU u,c are shown. Ablating tree-causal units from a GAN trained on LSUN outdoor church images reduces trees while buildings remain. Units are chosen to match specific objects like tables and sofas in generator layers. In the living room generator, layer4 has 65 sofa units out of 512. Identifying units responsible for rendering objects involves testing unit combinations through interventions. Units are ablated or inserted to determine their impact on object generation. In the living room generator, layer4 has 65 sofa units out of 512. Units are ablated or inserted to determine their impact on object generation. Featuremap r is decomposed into two parts, where r U,P are unforced components. Causality is quantified by comparing the presence of objects in different images. The average causal effect (ACE) of units U on class generation is calculated using segmentation. In the living room generator, layer4 has 65 sofa units out of 512. Units are ablated or inserted to determine their impact on object generation. Featuremap r is decomposed into two parts, where r U,P are unforced components. Causality is quantified by comparing the presence of objects in different images. The average causal effect (ACE) of units U on class generation is calculated using segmentation. To maximize the average causal effect \u03b4 U\u2192c for an object class c, a continuous intervention \u03b1 is optimized to apply per-channel scaling vectors to featuremaps. The living room generator analyzes the impact of units on object generation by decomposing featuremap r into unforced components. Causality is measured by comparing object presence in images, and the average causal effect of units on class generation is calculated using segmentation. To maximize this effect, a continuous intervention \u03b1 is optimized to apply scaling vectors to featuremaps. The optimization process involves stochastic gradient descent and ranking units based on their coefficients \u03b1 * u to achieve a stronger causal effect. Three variants of Progressive GANs trained on LSUN scene datasets are studied, and a model trained on the ADE20K scene dataset is used for image segmentation into 336 object classes and 29 parts. The model can segment input images into 336 object classes, 29 parts of large objects, and 25 materials. Object classes are expanded into additional object part classes for analyzing and comparing units. Intervention is used to locate causal units that can remove and insert objects in images. The study focuses on units correlated with object classes with diverse visual appearances, suggesting GANs generate those objects similarly. In diverse visual appearances, GANs generate objects using similar abstractions as humans. Units match different object classes in scene categories, providing insight into what the GAN has learned about the data. Units from GANs trained on various scene categories match appropriate object classes, such as stoves and cabinets in kitchen scenes. The GAN units in different network layers represent various object parts and semantic objects, with early layers entangled and later layers matching local pixel patterns like materials and colors. In Section S-6.7, all layers of GAN models are shown, with interpretable units providing insights on how architecture choices impact structures learned inside a GAN. Comparing different models, adding minibatch stddev statistics to the discriminator increases realism and diversity of concepts represented by units by over 40%, while pixelwise normalization increases units matching semantic classes by 19%. The first convolutional layer has few units matching semantic objects, but more emerge in later layers. The convolutional layer has few units matching semantic objects, but more emerge in layers 4-7 dominated by low-level materials, edges, and colors. The framework can analyze both the success and failures of GANs in producing realistic images. By identifying and ablating artifact-causing units, errors can be fixed, resulting in improved generated results. In TAB3, improved images are compared to original artifact images and a baseline with 20 randomly chosen units ablated. Fr\u00e9chet Inception Distance is used to measure image quality, showing an increase in interpretable units as model quality improves. Progressive GANs incorporate innovations like batch awareness and pixelwise normalization to enhance object matching and unit activations. Human annotations on Amazon MTurk further evaluate image quality. The manual change to a network outperforms state-of-the-art GANs models in improving image realism. By identifying and turning off specific units in a GAN, objects like people, curtains, and windows can be removed from generated scenes. However, not all objects can be erased. The ease of object removal in a GAN depends on the scene type. While windows can be removed well from conference rooms, they are more difficult to remove from other scenes. Ablating units like tables and chairs will reduce their size and density but not eliminate them. The difficulty of removal reflects the level of choice the GAN has learned for a concept. For example, a conference room is defined by the presence of chairs, so they cannot be altered. Similarly, modern building codes mandate that all bedrooms must have windows, making them difficult to remove. In a GAN trained on conference room images, specific object classes can be easily removed by ablating certain units. The average causal effect is measured by removing pixels in randomly generated images. Some object classes like people, curtains, and windows are easier to remove cleanly compared to tables and chairs. By forcing units on, we can observe the operation of a GAN by inserting features into specific locations in scenes. The GAN allows for interventions by adding door units to buildings, with effects varying based on the surrounding context. Doors can be added in plausible locations like where windows or bricks are present, but not in the sky or on trees. The average causal effects of door insertions are shown in a bar chart, highlighting the GAN's capabilities in adding doors to specific locations. The GAN allows for interventions by adding door units to buildings, with effects varying based on the surrounding context. It is not possible to trigger a door in the sky or on trees. Interventions provide insight on how a GAN enforces relationships between objects. The downstream effects of adding doors are further explored in Section S-6.5. Many parts of GAN representations can be interpreted as variables that have a causal effect on the synthesis of objects in the output. This method can be potentially applied to other generative models such as VAEs and RealNVP. The focus is on the generator rather than the discriminator to approximate the target distribution. The GAN allows for interventions by adding door units to buildings, with effects varying based on the surrounding context. Interventions provide insight on how a GAN enforces relationships between objects. The downstream effects of adding doors are further explored. In some scenes, windows are reduced in size or number rather than eliminated, or replaced by visually similar objects. The GAN allows interventions by adding door units to buildings, with effects varying based on context. Emphasizing existing doors results in larger doors. The method aims to identify interpretable structure in GANs, not compare quality or replace established metrics like FID. In this work, the authors aim to understand the internal mechanisms of a GAN and improve its performance by identifying and ablating artifact-causing units. They propose an automatic procedure using unit-specific FID scores to identify these units, which involves generating images and selecting the ones that maximize the FID score for a unit. Further research is needed to understand the relationships between layers of a GAN. The authors propose using unit-specific FID scores to identify and remove artifact-causing units in a GAN. By generating images that maximize the FID score for a unit, they found that units with bad FID scores tend to activate on images with visible artifacts. FID scores can predict units with noticeable artifacts with 50% precision and recall. Ablating the highest-FID units improves the model's performance. The curr_chunk discusses qualitative and quantitative results of a dissection method for generator units in a Progressive GAN model of a living room. The method labels units differently from humans, such as labeling a unit triggering on white letterbox shapes as 'ceiling'. A segmentation model used lacks labels for abstract shapes, leading to discrepancies in labeling between humans and the automatic method. The authors evaluate the correlation between human and automatic labeling for object concepts correlated with units in the model. The curr_chunk discusses the labeling of units in a Progressive GAN model of a living room. Human annotations were collected for 512 units, with 201 units consistently described by the same word. A segmentation-based dissection method labeled 154/201 units with IoU > 0.05, showing agreement with human annotators in most cases. A second evaluation was done to rate accuracy. The second evaluation compared human-derived and segmentation-generated labels for accuracy. Human labels scored 100% consistency, while segmentation labels were rated as accurate in 96% of cases. Failure cases occurred when human evaluators identified a different dominant concept compared to the algorithm. In some instances, the segmenter lacked confidence where humans reached a consensus. Future semantic segmentation models are expected to identify more concepts, such as abstract shapes. The segmentation model may perform poorly when the output does not resemble its training set, as seen in earlier GAN models. For example, a segmentation network was confused when analyzing units from a WGAN-GP model for LSUN bedrooms. To prevent spurious segmentation labels, a technique similar to Section S-6.1 can be used to identify and omit unrealistic units. Applying a filter to ignore segmentation on units with FID 55 or higher reduces irrelevant labels. More details on the ACE optimization and specifying the positive intervention constant k are provided in this section. Intervention involves setting units to a class-specific constant k, determined by mean featuremap activation for the target class. The optimization aims to identify causal units for the target class by sampling relevant locations. In cases where the target class is rare, optimization focuses on informative featuremap locations. During training, minibatches are formed by sampling locations P that are relevant to class c. An identical \"door\" intervention at layer4 of each pixel has different effects on later feature layers. Brighter colors in the heatmap indicate a stronger effect on layer14 features. The magnitude of feature effects at every layer is shown on the right, with changes measured by mean-normalized features. Feature changes for interventions resulting in human-visible changes are separated from those that do not result in noticeable output changes. During evaluation, causal effects are assessed using uniform samples. The region P is set to the entire image for ablations and uniformly sampled pixels for single-pixel insertions. Interventions are applied by clipping \u03b1 to its top components and zeroing the rest, with n set to 20 for comparing interventions across classes and models. In this section, interventions with n = 20 door-causal units are examined at layer4 to measure changes in later layers. Effects on downstream features are quantified and visualized in FIG5, showing stronger effects on final feature layers when interventions are near buildings. The average effect on each layer is plotted, distinguishing visible effects from non-visible ones. During training of a Progressive GAN model on bedrooms, dissection of layer4 representations shows an increase in interpretable units matching objects and object classes, along with improved object detectors. The quality and number of interpretable units increase as training progresses, with top-activating images and IoU measurements showing the evolution of the model. During successful training of a Progressive GAN model on bedrooms, dissection reveals that the model gradually learns the structure of a bedroom. The diversity of units matching high-level object concepts peaks at layer4-layer6, then declines in later layers dominated by textures, colors, and shapes."
}