{
    "title": "rkgZ3oR9FX",
    "content": "Human world knowledge is structured and flexible, with people representing objects as meaningful arrangements of semantic parts. They provide descriptions that are not only true but also relevant in the current context. A dataset was created using natural utterances referring to 3D objects from ShapeNet in various contexts. Neural models were developed with strong generalization capabilities, showing the importance of part-related words in understanding object properties. The curr_chunk discusses how a neural speaker that is 'listener-aware' produces more discriminative referring expressions than a 'listener-unaware' speaker. It highlights the importance of structured and flexible human world knowledge in representing objects. The text also mentions the challenge of obtaining and deploying such structured knowledge in machine learning. The curr_chunk discusses leveraging natural language to make fine-grained distinctions between complex object geometries in various contexts. It aims to utilize the structured nature of language to develop systems that can provide informative descriptions based on context. The curr_chunk discusses using natural language in interactive communication tasks to develop neural network models for speaker and listener roles. The models learn task-relevant correspondences between object parts and language tokens, and can generalize to new contexts and objects. The dataset includes triplets of 3D objects with referential utterances to distinguish a target object from distractors. The study involved participants playing an online reference game to distinguish a target object from distractors using geometric information. The game required one player to send a message through a chat box for their partner to select the target object. No constraints were placed on the chat box to ensure natural communicative interaction. The study involved participants playing an online reference game using geometric information. The chat box was used for communication, with the speaker's referring expressions sometimes followed by clarification questions. The dataset was built with familiar yet complex objects to elicit contrastive language. About 7,000 chairs from ShapeNet were utilized for this purpose. To achieve our objectives, we utilized a collection of approximately 7,000 chairs from ShapeNet, known for their geometric complexity and diversity. Object similarity between chairs was estimated using Point Cloud-AutoEncoder (PC-AE) for scalability and unsupervised learning. A sampling strategy was employed to construct triplets, selecting chairs based on their PC-AE embedding distances. Close and far contexts were sampled to address data inhomogeneity in repositories like ShapeNet. In total, a corpus containing 78,789 referring expressions for 4,054 triplets was collected, with 2,124 unique participants recruited. Human performance on the reference game was high, but errors were more common in close triplets. Longer utterances were used to describe targets in close triplets compared to far triplets, eliciting a wide spectrum of descriptions. Constructing neural listeners that reason about geometric relationships is a key contribution of the work, enabling the creation of an object retrieval system that operates with linguistic queries. A detailed comparison between three architectures, different regularization techniques, and two representations of 3D objects for the listening task is conducted. The communication context involves three objects denoted as O = {o 1 , o 2 , o 3 }, with corresponding word-tokenized utterances U = u 1 , u 2 , . . . and referential target t \u2208 O. The proposed listener, inspired by Monroe et al. (2017), uses a multi-modal LSTM to process word sequences and output object-utterance compatibility scores. These scores are used to compute a distribution over three objects, with a cross-entropy loss calculated against the ground-truth target indicator vector. Three object representations were experimented with to capture different aspects. The proposed approach uses different object representations, including a pretrained Point Cloud-AutoEncoder (PC-AE) and a convolutional network on single-view images of 3D objects. These representations are projected to the input space of an LSTM using fully connected layers for improved training and convergence. The addition of projection-like layers improves training and convergence in the system. The best performance is achieved by grounding the LSTM with the image code, concatenating the LSTM's final output with the point cloud code, and feeding it into a shallow MLP. Adding dropout at the input layer of the LSTM and L2 weight regularization and dropout at the FC projecting layers are crucial for significant improvements. Token codes are initialized with GloVe embedding and fine-tuned for the listening task. The proposed listener architecture scores each object separately and applies softmax normalization. Two alternative architectures are considered, one incorporating context earlier at encoding using a convolutional layer to augment each object's grounding vector with information about the other objects. The proposed architecture incorporates point clouds via a separate MLP after the LSTM. An attention mechanism over words is added to prioritize words that distinguish the target from distractors. The importance of each text-token is estimated by comparing the LSTM output for each token with the hidden state after processing the entire utterance. The proposed architecture includes an attention mechanism to prioritize words that distinguish the target from distractors. The hidden state acts as a summary of the grounded sentence, and the final output of the LSTM is defined as a point-wise product. Speaker models are inspired by the show-and-tell model developed for image captioning. The speaker's architecture includes an LSTM and a convolutional image network, receiving image code vectors and outputting a prediction over a vocabulary. The output is compared against ground-truth tokens using cross-entropy loss, with teacher-forcing used for subsequent tokens. Target vectors are fed in, achieving high accuracy rates. The speaker's architecture includes an LSTM and a convolutional image network, receiving image code vectors and outputting a prediction over a vocabulary. The output is compared against ground-truth tokens using cross-entropy loss, with teacher-forcing used for subsequent tokens. Target vectors are fed in, achieving high accuracy rates. The proposed model minimizes the length of dependence between input and output, eliminates the need to represent the target index separately, and uses a pretrained listener to select the best hyper-parameters and optimal epoch during training. This approach produces model parameters with stronger correlation between training progress and the quality of produced utterances. The speaker's architecture includes an LSTM and a convolutional image network, generating language that follows the discriminative characteristics of the referential ground truth. To test the impact of distractors, an experiment with a \"context-unaware\" speaker is conducted. A listener-aware speaker plans synthetic utterances based on their discriminative capacity, scored by the listener's probability to predict the target given the utterance. The parameter \u03b1 controls a length-penalty term to discourage unnecessary information. The parameter \u03b1 controls a length-penalty term to discourage short sentences BID6, while \u03b2 controls the relative importance of the speaker's vs. the listener's opinions. Listener generalization performance was evaluated using two tasks based on different data splits: language generalization and object generalization. Input modality and word attention were evaluated using [80%, 10%, 10%] of the data for training, validation, and testing in all experiments. Listener accuracies are shown in TAB0 (B) Lesioning highest attention words. The experiments showed that lesioning highest attention words worsens performance more than lesioning random or lowest attention words. Different architectures had higher accuracy on language generalization tasks. Attention on words slightly improved performance, especially when images were included. Images provided better input than point-clouds alone. Combining both modalities led to significant accuracy gains, suggesting complementarity between the two representations. The study evaluates how different approaches to incorporating context information affect listener performance in object generalization tasks. The At-Once architecture performs worse than Separate and Separate-Augment architectures, which achieve similar performance. Alternative strategies for context incorporation may be advantageous in close contexts. No significant differences were observed between Separate and Separate-Augment variants in far or close subpopulations. Far contexts were easier for all models than close. The study compares different architectures for incorporating context in object generalization tasks. The Separate architecture outperforms the Separate-Augment architecture, even in subpopulations with superlatives and comparatives. Nouns receive the highest attention weight in sentences, while adjectives modifying nouns also receive significant attention. In an utterance lesioning experiment, words were replaced with a <UNK> token in different schemes to evaluate listener performance. Results showed that the listener relies on context-appropriate content words to disambiguate referents, with significant performance differences from random replacements in both directions. This finding was consistent across various utterance lengths. In a lesion experiment, parts of objects were removed to test listener reliance on semantic parts mentioned in utterances. Removing random parts decreased accuracy by 11% on average. Removing random parts decreased accuracy by 11% on average, but removing the mentioned part dropped accuracy significantly more. Keeping only the mentioned part while lesioning the rest of the image had a minimal impact on accuracy. The localized information about the mentioned part was found to be necessary and sufficient for the listener model's performance. The proposed listener showed flexibility in handling natural language queries in a large database of 3D objects. The flexibility of the proposed listener model was demonstrated by measuring compatibility between objects and queries. Results showed good performance on transfer categories like sofas, tables, and lamps, indicating part-awareness in the learned embedding. Additional queries for chairs and non-chairs were included to further validate the usefulness of the neural listener's representations. Our neural listener can generate informative referring expressions for out-of-class objects in context using a search technique in ShapeNet Tables. Triplets of well-separated objects are produced with queries like 'no legs', 'modern', and 'x', where the target reflects the query semantics. The study conducted fair evaluations using a neural listener by splitting training data in half. Context-and listener-aware speakers generated synthetic referring expressions for a human evaluation. Participants were asked to select the object the speaker was referring to from the context. Approximately 2.58 responses were collected for each triplet in the experiment. The study evaluated object-generalization splits in listening experiments. The listener-aware speaker outperformed the context-unaware speaker in reference games, with human listeners performing significantly better with the listener-aware speaker. Bringing context into play earlier was found to be helpful for speakers. The study found that context-aware speakers performed better than context-unaware speakers in reference games. Human listeners also preferred the listener-aware speaker. The listener-aware speaker used a richer vocabulary compared to the context-aware speaker. Additionally, bringing context into play earlier was beneficial for speakers. Our speaker model produced promising results in image labeling and captioning, utilizing open-ended natural language. Unlike object categorization, our system communicates across diverse semantic contexts and generates fine-grained, part-based images. The use of reference games in cognitive science, based on language games explored by Wittgenstein and Lewis, has become a benchmark for context-aware NLP. Rational Speech Act models offer a probabilistic framework for understanding linguistic behavior. Rational Speech Act (RSA) models derive linguistic behavior from social cognition principles. RSA framework is based on Grice's proposal that speakers choose informative utterances. RSA considers informativity in terms of a rational listener agent. Previous work shows RSA models account for context sensitivity in human speakers. The effectiveness of RSA approach is supported by speaking results. Our results demonstrate the effectiveness of using natural language for learning to distinguish between objects based on their shared part-structure. Future applications include improving unsupervised part segmentation, 3D shape retrieval, and context-aware shape synthesis. This work advances existing context-unaware synthesis techniques. The study utilized close and far object triplets to collect contrastive language with varying specificity. Each triplet had objects alternating roles as distractors or targets, annotated by at least 4 humans. A small AE (64D) was used for meaningful Euclidean comparisons, with a manually tuned threshold for rejecting indistinguishable geometric objects in close triplets. The optimal hyperparameters for neural listener architectures using both images and point clouds with word attention are listed in Table 4. The models use an MLP with [100, 50] hidden neurons, batch normalization, and an LSTM with 100 hidden units. GloVe embedding is 100-dimensional and fine-tuned during training. Dropout is applied to point-cloud latent bottleneck codes and VGG image features. Ground-truth indicator vectors are label-smoothed. Label-smoothing with probability assignment to target and distractors improved performance by 2%. Previous work also found label-smoothing beneficial for generalization and reducing mode-collapse in GANs. A grid search was conducted for hyper-parameter optimization in listener architectures, with Separate-Augment showing better performance with a convolutional layer for encoding aggregation. The order-invariant max/mean poolings (f, g) outperformed other alternatives in three encodings compared to an FC. Using a separate MLP for point cloud data processing was slightly better than feeding them directly into the recurrent net. However, conditioning the recurrent net with point clouds and using images at the end of the pipeline significantly worsened results. Training details included 500 epochs for \"Proposed\" and \"At-Once\", and 350 epochs for \"SeparateAugment\", with learning halved every 50 epochs. During training, the learning rate was halved every 50 epochs if validation error did not improve. The model was evaluated every 5 epochs on the validation split to select parameters with the highest accuracy. The input order of geometric codes was randomly permuted for the sensitive \"At-Once\" model. ADAM optimizer was used for all experiments. Hyper-parameter search was conducted for context-aware neural-speaker using a two-state grid search. Model selection for the speaker involved using a pre-trained listener to evaluate synthetic utterances. The speaker generated utterances for validation triplets via greedy sampling every 10 epochs. The listener reported accuracy, and the model with the highest accuracy after 300 epochs was selected. Word embeddings were trained from random initialization. Bias terms were initialized with log probabilities of word frequencies. Training used SGD and ADAM with gradient clipping. Utterances had a maximal length of 33 tokens, and 50 utterances were sampled and scored for each speaker. The speaker generated utterances for validation triplets via greedy sampling every 10 epochs. The listener reported accuracy, and the model with the highest accuracy after 300 epochs was selected. Word embeddings were trained from random initialization. Bias terms were initialized with log probabilities of word frequencies. Training used SGD and ADAM with gradient clipping. Utterances had a maximal length of 33 tokens, and 50 utterances were sampled and scored for each speaker. The optimal length penalty for the context-unaware speaker is 0.7, and set to 0.6 for the rest. PC-AE was trained under the Chamfer loss with a bottleneck of 128 dimensions using point clouds of 2048 points from a 3D CAD model. VGG-16 encoding utilized the 4096-dimensional output activations of its second fully-connected layer (f c7). Fine-tuning of VGG-16 was done under the cross-entropy loss for an 8-way classification with photo-realistic rendered images of textureless meshes in the 8 largest object classes of Shape-Net. Total number of shapes was 36,632, and fine-tuning took 30 epochs with optimization of weights in the last layer first and then all layers. The network achieves a 96.9% classification accuracy on the test split. Context-aware architectures perform better on utterances associating the target with one distractor. The proposed model achieved 62.5\u00b13.7% accuracy, slightly lower than the \"At-Once\" model. The negligible gains of the \"Separate-Augment\" architecture were not worth the increase in complexity and rigidity. The literal speaker struggled to produce effective utterances in close contexts. The proposed model achieved slightly lower accuracy than the \"At-Once\" model. The \"Separate-Augment\" architecture showed negligible gains, with the literal speaker struggling to produce effective utterances in close contexts. The study measured the effect of using different \u03b1, \u03b2 values to select the top-1 scoring sentence for context-aware and unaware speakers when creating utterances for objects/contexts. The study involved participants swapping speaker and listener roles in games consisting of 69 rounds. Most participants played only one game. The most distinctive words in each triplet type were identified. Performance of different listeners in specific subpopulations was analyzed. The proposed model achieved slightly lower accuracy than the \"At-Once\" model. The \"Separate-Augment\" architecture showed negligible gains. The study measured the effect of using different values to select the top-1 scoring sentence for context-aware and unaware speakers. The study analyzed different neural-net initializations and their impact on performance. Results showed variations in accuracy for different architectures."
}