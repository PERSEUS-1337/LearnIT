{
    "title": "S1uxsye0Z",
    "content": "We propose a novel framework to adaptively adjust dropout rates in deep neural networks based on Rademacher complexity. This approach aims to balance model complexity and representation power by incorporating dropout rates into the objective function. The method is optimized using the block coordinate descent algorithm, leading to converging dropout rates that reveal meaningful patterns. Our method adapts dropout rates in deep neural networks based on Rademacher complexity, optimizing with block coordinate descent to reveal meaningful patterns and improve performance in image and document classification tasks. The model requires specifying retain rates for neurons before training, which are then kept fixed. Choosing optimal retain rates is often unclear and typically done through grid-search or rule-of-thumb. Neurons in the same layer share the same retain rate, reducing hyper-parameter search space. A novel regularizer based on Rademacher complexity is proposed for neural networks, with the Rademacher complexity bounded by dropout probabilities. The Rademacher complexity bound regularizer allows for explicit incorporation of model complexity as a regularizer in the objective function. It combines model complexity and loss function in a unified objective, enabling trade-off between complexity and representation power through a weighting coefficient. The regularizer is neuron-wise and adapts dropout probabilities during optimization, similar to ridge regression and lasso. The Rademacher complexity bound regularizer incorporates model complexity as a regularizer in the objective function, adapting dropout probabilities in a neuron-wise manner during optimization. Empirical results show changes in dropout probabilities histograms for hidden and input layers, revealing meaningful patterns on input features. This is the first effort to use Rademacher complexity bound to adjust dropout probabilities for neural networks. The paper reviews past approaches, details the proposed approach, presents empirical evaluations on image and document classification tasks, and concludes with a summary. The paper discusses prior works related to dropout training methods, such as BID0 and BID27, which extend dropout networks into complex structures by controlling dropout rates for neurons and incorporating Bayesian feature noising. These methods aim to adaptively adjust dropout rates during neural network learning. The evolutional dropout model adapts sampling probabilities during training. BID25 proposes fast approximation methods for dropout layers, while BID23 views dropout training as adaptive regularization. Gal & Ghahramani (2016) frame dropout as Bayesian inference in deep Gaussian processes. Maeda (2014) discusses a Bayesian perspective on dropout. From a Bayesian perspective, various dropout methods have been proposed, such as the binary variant and the variational dropout method. These methods allow for adaptive tuning of dropout rates at different levels, such as layer, neuron, or weight level. Recent advancements have led to tighter approximations and more sparse dropout rates. Our proposed approach operates on the Gaussian approximation of dropout models, while previous models use the canonical multiplicative dropout model. The model complexity and generalization capability of deep neural networks have been extensively studied. Various methods, such as DropConnect and PAC-Bayes, provide generalization guarantees for deep neural networks with dropout. The PAC-Bayes method provides a generalization guarantee for linear predictors with dropout, potentially offering a tighter bound. This paper introduces a bound based on Rademacher complexity, aiming to understand how dropout works and extend it to the PAC-Bayes paradigm. Theoretical guarantees on dropout effectiveness are provided, but existing bounds assume uniform dropout rates across layers, limiting their applicability. The focus is on classification using multilayer perceptrons, with potential extension to feedforward networks. The text discusses feedforward networks and the use of Bernoulli dropout in neural networks. It mentions the rectified linear activation function, Hadamard product, and the final predictions made through a softmax function with cross-entropy loss. The expected value of the neural network output is used as the deterministic output. The empirical Rademacher complexity of function class F with respect to the sample S is defined as the composition of the logistic loss function and the neural function returned from the last layer. The empirical Rademacher complexity of the loss for the dropout neural network is bounded by a certain formula involving Bernoulli parameters and Rademacher random variables. The empirical loss and Rademacher regularizer change in a monotonic way based on retain rates during training. The Rademacher regularizers change monotonically with retain rates during training on the MNIST dataset with a hidden layer of 128 ReLU units. Dropout is applied only on the hidden layer, with fixed retain rates throughout training. Rademacher regularizers are computed post-hoc after every epoch, with a learning rate of 0.01 decaying by half every 40 epochs. The empirical Rademacher complexity of the dropout network is related to p-norms of the coefficients in a multiplicative way. The Rademacher regularizers change with retain rates during training on the MNIST dataset with a hidden layer of 128 ReLU units. The Rademacher complexity of a neural network is bounded by a function of the dropout rates, allowing for unification of dropout rates and network coefficients in one objective. By imposing an upper bound of Rademacher complexity to the loss function as a regularizer, a weighting coefficient \u03bb \u2208 R + is used to balance training loss and generalization capability. The empirical loss and regularizer functions are defined based on the sample matrix X \u2208 R n\u00d7d and the number of classes k to predict. The Rademacher regularizer and dropout Bernoulli parameters \u03b8 influence the empirical loss term, where smaller \u03b8 leads to larger loss due to reduced model representation power. The Rademacher complexity bound decreases with smaller dropout Bernoulli parameters \u03b8, leading to a more generalizable model. The cross-entropy loss and empirical Rademacher regularizer are plotted during model convergence under different retain rates. The model's fitness error increases when all \u03b8 values become zeros, resulting in random predictions. Changes in true objectives in \"stochastic\" and \"deterministic\" modes are compared across different network architectures during training epochs on the MNIST dataset with Rademacher regularizer. The optimization objective incorporates Bernoulli parameters \u03b8 into the model, optimizing weight coefficient matrices W and retain rate vectors \u03b8 using a block coordinate descent algorithm. The model parameters and dropout rates are optimized alternately, leading to improvements on the true \"stochastic\" objective. During optimization, the model incorporates Bernoulli parameters \u03b8 and weight coefficient matrices W using a block coordinate descent algorithm. To speed up the process, the expected value of Bernoulli dropout variables is used to rescale the output from each layer, approximating the true objective function. This approach avoids the need to iteratively sample dropout variables, reducing computational complexity. During optimization, the model uses Bernoulli parameters and weight coefficient matrices with a block coordinate descent algorithm. The expected value of Bernoulli dropout variables is used to rescale the output from each layer, approximating the true objective function. This deterministic approximation improves running time without altering performance. The proposed approach uses the expected value of Bernoulli dropout variables to approximate the sampling process in stochastic optimization. It achieves superior performance on image and text classification tasks with different network architectures, outperforming strong baselines on various benchmark datasets. The approach uses Rademacher regularization with mini-batch stochastic gradient descent for parameter tuning and model selection. A grid search is performed on the regularization weight and dropout rates are updated periodically. Variational dropout methods are also explored for adaptive dropout rates. The study explores neuron-wise adaptive regularization for Type-A variational dropout, finding it performs best in most cases. A grid search is conducted on regularization noise parameters. Sparse variational dropout methods are found to be sensitive to regularization weights and prone to divergence. Different regularization weights are examined and adjusted during training to stabilize the regularization term. The study investigates neuron-wise adaptive regularization for Type-A variational dropout, which performs well in most cases. Regularizers are scaled to offset multiplier effects from network structure, with examples provided for Rademacher complexity regularizers. The MNIST dataset consists of hand-written digit images for classification into 10 classes. The study explores different network structures for classifying hand-written digit images from the MNIST dataset. Various models with different hidden layer architectures are compared in terms of performance. Different initialization rates for neuron retaining are tested, with specific rates yielding better results for different models. The study compares different network structures for classifying MNIST digit images. Retaining a rate of 1.0 for the input layer improves results. The retain rates for input and hidden layers change under Rademacher regularization. The network has one hidden layer with 1024 ReLU units. Retain rates converge towards bimodal distribution for input layer and unimodal for hidden layer during training. Interesting feature patterns are observed in the input layer's retain rates upon model convergence. The proposed method dynamically determines the relevance of input signals, adjusting retain rates accordingly. CIFAR10 and CIFAR100 datasets contain RGB images of various categories. Images are preprocessed by subtracting mean and applying ZCA whitening. A neural network with three convolutional layers is evaluated, each followed by max-pooling. The convolutional layers in the neural network have a 5\u00d75 receptive field with a stride of 1 pixel, and max-pooling layers pool from a 3\u00d73 pixel region with strides of 2 pixels. Two fully-connected layers with 2048 hidden units each follow the convolutional layers. Dropout rates are initialized for different layers, and the learning rate is set to 0.001, decaying exponentially every {200, 300, 400} epochs. Figure 4 shows the retain rates for input and hidden layers under Rademacher regularization with a 0.1 regularization weight. The neural network consists of two convolution layers with 32 and 64 filters, followed by a fully-connected layer with 1024 neurons. ReLU activation functions are used for all hidden units. Retain rates are initialized differently for input, convolutional, and fully-connected layers. The learning rate is 0.001 and decays by half every 300 epochs. Unlike the MNIST dataset, the retain rates for all layers diffuse throughout training and converge to a unimodal distribution, except for the input layer due to the nature of the CIFAR10 dataset. This highlights the effectiveness of the Rademacher regularizer in distinguishing features. The Rademacher regularizer effectively distinguishes informational pixels during training. Our proposed approach is compared on text classification datasets SUBJ and IMDB. SUBJ contains subjective and objective sentences from movie reviews, while IMDB consists of positive and negative movie reviews. Both datasets have a large vocabulary and are split into training and test data. The dataset used for the study includes movie reviews with low review scores. Changes in retain rates with Rademacher regularization on CIFAR10 dataset are analyzed, showing histograms for different layers during training. Unlike MNIST datasets, there is no clear pattern in retain rates for input pixels in CIFAR10 dataset. The study analyzes changes in retain rates with Rademacher regularization on CIFAR10 dataset, showing histograms for different layers during training. The performance of proposed models is summarized in Table 3, with dropout rates settings yielding the best performance. The model uses a constant learning rate of 0.001 and decays it by half every 200, 300, 400 epochs. Overall, the improvement of dropout is not as significant as in the MNIST dataset. FIG4 shows retain rate changes under Rademacher regularization with 0.005 weight on IMDB dataset. Network has one hidden layer of 1024 ReLU units. Initial retain rates: 0.8 for input, 0.5 for hidden layer. Learning rate: 0.01, halved every 200 epochs. Rates diffuse slightly upon convergence, with input layer showing interesting feature patterns. IMDB task: classify movie reviews as negative or positive. Adjectives like \"wonder(ful)\", \"best\", \"love\" yield high retain rates. The curr_chunk discusses word features with low retaining probability in movie reviews, including nouns and verbs that are less indicative. Some less informative features include \"year\", \"oscar\", and \"music\". The word \"oscar\" not being informative suggests movie reviews and Academy Awards are not necessarily correlated. Additionally, a list of popular named entities relevant to the movie industry is included. The curr_chunk provides a list of popular named entities in the movie industry, noting patterns in their retain rates during optimization. Notably, actors like \"baldwin\" and \"kidman\" show high retain rates, while \"pitt\" exhibits a decline, suggesting less relevance for review classification. Theoretical upper bound on Rademacher complexity allows direct incorporation of dropout rates into objective function for neural network regularization. Empirical evaluation shows promising results and patterns on adapted retain rates. Future work includes investigating sparsity property of learnt retain rates for sparse representation of data and neural network structure. The curr_chunk discusses exploring deep network compression, dynamically adjusting the architecture of deep neural networks, and identifying indicative features for prediction based on retain rates of certain words. The model highlights words with high retain rates like \"love\" and \"great\", while words like \"say\" and \"young\" have near zero retain rates. The analysis focuses on the retain rates of certain words in deep neural networks for predicting IMDB review sentiment. Actors and directors like \"baldwin\", \"kidman\", \"kurosawa\", and \"eastwood\" show high retain rates initially, while others like \"downey\", \"spacey\", \"spielberg\", and \"cameron\" remain consistent. Higher retain rates indicate features more indicative for classification. The text discusses the connection between different functions in a neural network, focusing on the logistic loss function and empirical Rademacher complexity. The analysis simplifies the cross-entropy loss into a single logistic function and explores the recursive expression of the neural network function. The empirical Rademacher complexity of the loss function is bounded by a certain lemma. The empirical Rademacher complexity of the function class in the neural network is recursively proven by defining a variant with absolute value inside the supremum. The cross entropy loss function is simplified and bounded by a lemma, ensuring it remains 1-Lipschitz even with truncation. The empirical Rademacher complexity of the neural network function class is proven recursively. The truncated cross entropy loss remains 1-Lipschitz, ensuring the empirical Rademacher complexity bound holds. Adding a Rademacher related regularizer is not new, with the linear class H2 having a bounded complexity. The regularizer in ridge regression serves as an upper bound for the empirical Rademacher complexity of the linear function class. The lasso problem involves adding Rademacher-related regularization to minimize empirical loss. Heuristics are designed to ensure regularization stability in neural networks with varying internal neurons. The regularizer scales mentioned are heuristic and empirical. Sections 6.3 and 6.4 are based on ad hoc heuristics. Different regularizers are used for varying values of p and q. The dropout rates convergence across multiple runs is demonstrated using a neural network with one hidden layer of 1024 ReLU units fed with the MNIST dataset. The network is trained for 10 different runs with different weight initializations, showing similar dropout behavior and distribution among runs upon model convergence."
}