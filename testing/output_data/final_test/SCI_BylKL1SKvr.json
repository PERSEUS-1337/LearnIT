{
    "title": "BylKL1SKvr",
    "content": "Deep neural networks trained on various datasets show impressive transferability, with deep features being applicable to many tasks. Pretraining on large datasets like ImageNet can enhance generalization and speed up training when fine-tuned to a smaller dataset. This paper explores the reasons behind transferability, focusing on improved generalization and optimization. It is found that transferred models tend to reach flatter minima and improve the loss landscape. The study explores the impact of transferred representations on the loss landscape during training. It is found that pretraining suppresses the principal component of the gradient, stabilizing its magnitude in back-propagation. Transferability is influenced by input and label similarity, as well as training stages. The success of deep neural networks in various fields is largely attributed to these advancements. Deep neural networks achieve great success in vision, natural language processing, and robotics due to the transferability of deep feature representations. This allows learned features from one dataset to benefit optimization and generalization on different datasets or tasks. Practitioners use pretrained deep networks as general-purpose feature extractors for various applications. Deep neural networks are used as general-purpose feature extractors for downstream tasks, with a standard practice of fine-tuning models transferred from large-scale datasets like ImageNet to avoid overfitting. ImageNet pretrained networks accelerate training for tasks like object detection and semantic segmentation. Advances in unsupervised pretrained representations have led to remarkable improvements in NLP tasks. Despite practical success, few efforts have been made to uncover the underlying mechanism of transferability in deep neural networks. The study explores how pretrained knowledge benefits generalization in deep neural networks. Results show that fine-tuning on similar datasets keeps parameters close to transferred ones, leading to flatter minima. Transferred features aid optimization, but the exact mechanism of transferability remains unclear. The study examines the benefits of pretrained knowledge in deep neural networks. Transferred features improve optimization by enhancing the smoothness of loss landscapes and controlling gradient magnitude. Transfer learning is most helpful when the tasks between pretrained and target networks are similar, rather than just the inputs. The study explores the relationship between pretraining epoch and transferability in deep neural networks. Transfer learning is beneficial when tasks are similar, and theoretical analysis supports empirical findings. Transferability initially increases but then decreases as pretraining progresses, shedding light on the fundamental properties of deep neural networks. The success of transfer learning in deep neural networks has led to studies on fine-tuning and transferability. Researchers have shown that deep features can improve object detection tasks, and a selective joint fine-tuning scheme has been introduced to enhance performance with limited training data. However, there is a decrease in transferability of deep feature representations as the difference between pretrained and target tasks increases, especially in higher layers. Additionally, catastrophic forgetting can occur, leading to the loss of pretrained knowledge. The success of transfer learning in deep neural networks has led to studies on fine-tuning and transferability. Researchers have shown that deep features can improve object detection tasks, and a selective joint fine-tuning scheme has been introduced to enhance performance with limited training data. However, there is a decrease in transferability of deep feature representations as the difference between pretrained and target tasks increases, especially in higher layers. Additionally, catastrophic forgetting can occur, leading to the loss of pretrained knowledge. Various studies have explored the impact of ImageNet pretrained features on transfer learning, with findings showing that better pretraining performance leads to improved transfer to target tasks. Techniques such as analyzing weight matrix scaling and measuring loss variation have been proposed to enhance the understanding of loss landscapes and stability in neural networks. Transfer learning in deep neural networks has shown that tasks on similar datasets to the pretrained one perform better. Two common settings are explored: training only the last layer and fine-tuning the whole network. Results indicate that networks pretrained on more similar datasets generalize better and converge faster. The knowledge learned from pretrained networks varies in preservation across different target datasets. The preservation of pretrained knowledge in transfer learning is crucial for better generalization. Experiments show that models retaining more transferred knowledge perform better on target datasets similar to the pretrained one. This is evidenced by the deviation of weight parameters from the pretrained ones, indicating that preserving knowledge leads to improved performance. The transfer process in neural networks involves implicitly bounding the transferred knowledge, which is related to the similarity between pretrained and target datasets. The weight parameters tend to stay closer to the pretrained parameters when datasets are similar, leading to improved generalization. This property is crucial for understanding the optimization trajectories of weight parameters on different datasets. Results from comparing weight matrices of training from scratch and using ImageNet pretrained representations show that local minima of different datasets using ImageNet pretraining are close to each other, while those of training from scratch and ImageNet pretraining are distant even on the same dataset. Weight matrices on the same dataset may be distant at convergence with different initializations, but stay close to the initialization when using the same pretrained parameters, indicating how transferred representations improve generalization on target datasets. Studies have shown that properties of local minima are directly linked to generalization. Using pretrained representations restricts weight matrices to stay near the pretrained weight, leading to flatter minima in large flat basins. Filter normalization is used as a visualization tool to illustrate the loss landscapes around the minima. Weight matrices deviate less from pretrained weights on similar datasets, staying in the flat region, while on different datasets, they may run out of the flat region. Techniques like BatchNorm improve loss landscapes in modern deep networks. The improvement of loss landscapes in modern deep networks is achieved through techniques like BatchNorm and residual structures. Visualizations show that loss landscapes are smoother at the initialization point in pretrained networks compared to randomly initialized networks. This smoother landscape accelerates training by ameliorating the chaos of the loss landscape with improved Lipschitzness. The impact of transferred features on loss stability is demonstrated by analyzing the variation of loss in the direction of gradient. Pretrained networks have smoother loss landscapes, leading to a mild decrease in loss with each step in the training process, while randomly initialized networks show larger variation in the loss function. Transferred features control the magnitude of gradient and smooth the loss landscape by providing appropriate transformation of gradient in each layer. Weight matrices in pretrained networks stabilize the magnitude of gradient in back-propagation, unlike randomly initialized weight matrices. During pretraining, weight matrices stabilize the magnitude of gradient by shrinking singular vectors with large values. This helps control the gradient back-propagated through pretrained layers, especially in lower layers. Pretrained networks like ResNet-50 show stable gradient magnitude and scaling, even in modern networks with BatchNorm and skip-connections. This stabilizing effect on gradient magnitude is crucial for smooth loss landscapes and efficient training. In Section B.4, the analysis visualizes the change of landscapes during back-propagation in modern networks. Transfer learning from pretrained representations can boost performance in various applications, but there are cases where it provides no help or even decreases test accuracy. The feasibility of transfer learning is an important open problem explored in this section through extensive experiments. Theoretical perspectives are discussed in the following section. The analysis aims to provide insights for practitioners on how to adopt transfer learning effectively. The importance of choosing datasets similar to the target dataset for pretraining is highlighted, along with the challenge of determining dataset similarity. The similarity between datasets is crucial for transfer learning. Experiments show that task nature, including inputs and labels, affects similarity. Pretraining models on MNIST and fine-tuning on a subset of SVHN dataset improves generalization. Different pretraining datasets result in varying performance. Choosing similar datasets for pretraining is essential for effective transfer learning. Pretraining models on similar datasets yields better performance gains in transfer learning. Fine-tuning from Fashion-MNIST does not significantly improve generalization. Pretraining on Caltech-101 boosts performance when fine-tuned to Webcam, while training on Webcam from scratch shows no improvement. Choosing a more similar dataset for pretraining leads to larger performance gains in transfer learning. The experiments challenge the common perspective of dataset similarity. The relationship between tasks is a key factor in transferability. Pretraining on Food-101 and transferring to CUB-200 shows an increase in transferability initially, but test accuracy on the target dataset decreases with continued pretraining. The test accuracy on the target dataset declines as pretraining continues, indicating reduced transferability. Model learns general knowledge early on but later fits specific knowledge of pretrained dataset, which can be detrimental to transfer performance. Instead of focusing on higher accuracy on pretraining dataset, aim for a more transferable model with appropriate epochs for fine-tuning on diverse target tasks. Transferred features have a significant impact on generalization. Extensive empirical analysis shows that transferred features significantly affect generalization and optimization performance, providing insights into the feasibility of transfer learning. Theoretical analysis is based on two-layer fully connected networks with ReLU activation and ample hidden units, aligning with experimental results. The ReLU activation function and weight matrix are key components in the analysis of transfer learning. The goal is to transfer a pretrained weight matrix W(P) to learn an accurate model W(Q) for the target distribution Q. The weight initialization is done with w r (0) \u223c N(0, \u03ba^2 I) and a r \u223c unif ({\u22121, 1}). The objective function is the squared loss, with a fixed a and W updated with gradient descent using a learning rate of \u03b7. The analysis is based on the theoretical framework of Du et al. (2019) and uses Gram matrices H \u221e P and H \u221e Q to measure convergence in transfer learning scenarios. In the analysis, Gram matrices H \u221e P and H \u221e Q are used to measure the quality of pretrained input and target input. The relationship between pretrained input and target input is quantified using the Gram matrix H \u221e P Q. The Lipschitzness of the loss function is analyzed by examining the magnitude of gradient with respect to the activations. The Lipschitz constant is reduced when pretrained and target datasets are similar in inputs and labels. Theoretical explanation of experimental results shows that the control of Lipschitz constant depends on task similarity in inputs and labels. If target label is similar to transformed pretrained label, the loss function's Lipschitzness improves significantly. The Lipschitz constant of the loss function is influenced by the similarity between pretrained and target tasks. The generalization error is directly related to the difference in weights between the tasks. The Rademacher complexity is bounded by this weight difference, impacting the generalization error. The generalization error in neural networks is influenced by the similarity between pretrained and target tasks. Fine-tuning from a similar pretrained dataset can reduce generalization error, but features pretrained on discrepant tasks may not transfer effectively. For instance, fine-tuning to a fine-grained dataset like Food-101 can result in a larger deviation from the pretrained weight, impacting performance. This paper analyzes the transferability of deep representations in neural networks from empirical and theoretical perspectives. Pretrained representations can improve generalization and optimization performance if the pretrained and target datasets are similar in input and labels. Transfer learning acts as an initialization technique that implicitly regularizes neural networks. The technique of neural networks exerts implicit regularization to prevent networks from escaping the flat region of pretrained landscape. Details of architectures, setup, and visualization methods are provided. Models are implemented on PyTorch with 2080Ti GPUs using standard ResNet-50 for object and scene recognition tasks. Pretrained models from ImageNet and Places are utilized. Fine-tuning involves a batch size of 32, initial learning rate of 0.01, and training for 200 epochs with learning rate decay. In Figure 2 (a), pretrained ResNet-50 acts as a feature extractor with a downstream classifier of a two-layer ReLU network. The backbone ResNet-50 is fixed, and the classifier is trained with momentum SGD. For digit recognition tasks, LeNet is used with specific learning rate and weight decay. Experiments are run multiple times and results are reported in Tables 1 and 2, with fine-tuning improvement calculated. The text discusses the visualization of loss landscapes using techniques similar to filter normalization. It explains the normalization process for accurate analysis, involving orthogonal vectors and filter-wise normalization. The value evaluation in the plot is described using a loss function and neural networks. The visualization of loss landscapes involves filter normalization techniques to accurately analyze the plot using a loss function and neural networks.\u03b7 is a parameter controlling the scale, set to 0.001 for studying local loss landscapes with SGD. Loss variation scale is consistent for fair comparison between pretrained and randomly initialized landscapes. The text discusses visualizing loss landscapes on different datasets using various techniques such as computing eigenvalues of Hessian and t-SNE embedding of model parameters. The results are consistent across multiple experiments, but only one dataset is shown in the main paper due to space limitations. Other results can be found in Section B. The text discusses visualizing loss landscapes on different datasets using various techniques such as computing eigenvalues of Hessian and t-SNE embedding of model parameters. Results on CUB-200 are provided in the main paper, with additional results in Section B. The experiment involves measuring the stability of loss functions by calculating the maximum changes of loss in the direction of gradient. Results on Stanford Cars dataset show that pretrained networks have a smaller variation of loss compared to randomly initialized ones, indicating a more stable optimization process. Generalization error is also improved with pretraining, as shown in Table 2. The text discusses the impact of ImageNet pretraining on generalization performance for classification tasks. It highlights that pretraining improves performance significantly for coarse-grained tasks but less so for fine-grained tasks. The loss landscape comparison between randomly initialized and pretrained networks on the Food-101 dataset is visualized. Results show that the landscapes of pretrained networks remain smooth through lower layers, while randomly initialized networks worsen, indicating a substantial gradient magnitude difference in back-propagation. The study analyzes the trajectories of weight matrices during pretraining and their effect as an initialization in target tasks. It is based on a framework for over-parametrized networks and focuses on the smoothness of landscapes in pretrained networks compared to randomly initialized ones. The study focuses on analyzing weight matrix trajectories during pretraining and their impact on target tasks. It discusses the importance of matrix ZP in analyzing these trajectories and the gradient descent process. The study also references works by Arora et al. (2019) and Du et al. (2019) to provide bounds on certain terms with probability. During pretraining, the study analyzes the influence of pretrained weight on target tasks by examining the properties of the matrix H \u221e P Q. It shows that under over-parametrized conditions, H \u221e P Q is close to the randomly initialized Gram matrix Z P (0) Z Q (P). With certain conditions, the study provides bounds on terms with probability. The study examines the influence of pretrained weights on target tasks by analyzing the properties of the matrix H \u221e P Q. Under over-parametrized conditions, H \u221e P Q closely resembles the randomly initialized Gram matrix Z P (0) Z Q (P). Bounds on terms with probability are provided. The study analyzes the impact of pretrained weights on target tasks by examining the change in activations. The distribution of w(0) r is Gaussian with mean 0 and covariance matrix \u03ba 2 I. By using Markov inequality, it is shown that the activations remain the same for both W(0) and W(P) on the target dataset. With the help of equations and lemma, the reduction in gradient magnitude due to pretrained weight matrix W(P) is calculated. The study examines the impact of pretrained weight matrix on generalization performance and convergence of models. It is shown that using pretrained weights helps reduce gradient magnitude, leading to successful convergence. The trajectory during transfer learning is investigated, and the relationship between source and target datasets is analyzed. The number of hidden nodes is set to m = \u2126, and with high probability over random initialization, the lemma derived from Theorem 3 and Lemma 1 is crucial for the analysis. The study explores the impact of pretrained weight matrices on model generalization and convergence. Using pretrained weights reduces gradient magnitude, aiding in successful convergence. The lemma derived from Theorem 3 and Lemma 1 is crucial for analysis, showing that with high probability over random initialization, certain conditions hold for estimation. The study examines the effect of pretrained weight matrices on model generalization and convergence. Pretrained weights reduce gradient magnitude, facilitating successful convergence. Conditions for estimation are shown to hold with high probability over random initialization. The proof involves classifying activations into categories and controlling perturbations to show convergence for ReLU networks."
}