{
    "title": "HJ5AUm-CZ",
    "content": "Hierarchical Bayesian methods aim to unify various tasks by framing them as inference within a single generative model. Existing approaches may fail on expressive generative networks like PixelCNNs, which describe the global distribution with little reliance on latent variables. To address this, a modification of the Variational Autoencoder called Variational Homoencoder (VHE) is developed to better utilize latent variables in training hierarchical latent variable models. This framework enables training a hierarchical PixelCNN for the Omniglot dataset, outperforming existing models on test set likelihood and achieving strong one-shot generation and near human-level classification. The VHE objective allows for strong one-shot generation and near human-level classification, extending naturally to richer dataset structures. Learning from few examples is possible with strong inductive biases, which can come from hand design or meta-learning algorithms. Recent work has explored one-and few-shot learning from various perspectives using Siamese Networks, Matching Networks, Prototypical Networks, and MANNs. Conditional generative models have been developed to generate distributions over new elements based on a few observations from a class. These models can also be used for classification by comparing conditional likelihoods and generating full sets incrementally. However, they are more suited for sequences than sets and lack a latent representation of shared structure within a set. The Variational Homoencoder (VHE) combines advantages of different models by training on a few-shot generation objective and optimizing a likelihood lower bound for a hierarchical structure. It uses encoding costs to share a latent variable c and can define p(x|c) through a local latent variable z. The Variational Homoencoder (VHE) optimizes a likelihood lower bound for a hierarchical generative model with shared latent variables. Previous work used Variational Autoencoders but faced computational limits with large datasets. VHE improves generalization by subsampling the dataset for training and extends to models with richer latent structure. The VHE objective includes encoding and reconstruction costs, sharing latent variables for better performance. The VHE objective involves encoding and reconstruction costs, with shared latent variables reducing encoding cost per element significantly. This enables the use of powerful autoregressive decoders, improving performance. The application of VHE to the Omniglot dataset using a Pixel-CNN decoder demonstrates near human-level one-shot classification and high-quality sample generation. Variational Autoencoders offer a method for learning models with intractable integrations through neural-network based approximate posterior. Variational Autoencoders (VAE) provide a method for learning models with intractable integrations through neural-network based approximate posterior inference. A VAE consists of a generative network parametrised by \u03b8 and an inference network parametrised by \u03c6, trained jointly to maximize a single objective. This objective is a lower bound on the total log likelihood of the dataset, with the inference network trained to approximate the true posterior as accurately as possible. The resulting model is a compromise between assigning high likelihood to the data and allowing accurate inference. The Neural Statistician is a Variational Autoencoder that uses Monte-Carlo integration to optimize a formulation for encoding sets. The generative model for sets introduces a latent variable c, with elements in each set assumed to be exchangeable. Calculating the variational lower bound for each set requires evaluating both q(c; X) and p(X|c) for each gradient update. The new objective in this work replaces the variational lower-bound with a constrained variational distribution for posterior inference and an unbiased stochastic approximation for the likelihood. This approach aims to learn latent representations that are quickly inferable from a small number of instances, serving as a regularization strategy. The VHE objective aims to learn generative models for sets X by lower bounding the log-likelihood using an arbitrary distribution q. It suggests a modification to the VAE by replacing the universal quantification with an expectation under any distribution of D. This approach serves as a regularization strategy for quickly inferable latent representations. The VHE objective proposes a modification to the VAE training procedure by selecting an element x at each iteration and using resampled elements to construct an approximate posterior q(c; D). This allows for the introduction of a conditional VAE bound by replacing the exact reconstruction error. The approach is applicable to datasets with various organizational structures, such as hierarchical grouping or factorial categorization. The VHE objective proposes a modification to the VAE training procedure by selecting an element x at each iteration and using resampled elements to construct an approximate posterior q(c; D). This allows for the introduction of a conditional VAE bound by replacing the exact reconstruction error. The approach is applicable to datasets with various organizational structures, such as hierarchical grouping or factorial categorization, where latent variables are inferred for each group within the dataset. The VHE objective provides a formal motivation for KL rescaling in VAEs to increase the use of latent variables, especially with autoregressive decoder models. It addresses the issue of learning a decoder with no dependence on the latent space by sharing variables across elements. This is crucial for training VAEs for sets, where the inference network must reduce its approximation error to below the total correlation of the dataset. Variational Homoencoders (VHE) address the challenge of reducing the inference network's approximation error below the total correlation of the dataset by reusing latent variables across a large set X. This approach allows VHE to learn useful representations even with small datasets, while using a powerful decoder model for accurate density estimation. The recognition network in VHE takes a small subsample as input, enabling accurate approximation of the true posterior p(c|X) from only a few examples of X. The VHE objective aims to match the posterior distribution p(c|X) with the true distribution p(c|D) by utilizing latent variables across dataset subsets. This serves as a regularizer for constrained posterior approximation, promoting models where the posterior can be accurately determined by sampled subsets. Tightening the bound can be achieved through an auxiliary inference network, left for future exploration. The loose bound has shown promising results in experiments, with under-utilization of latent variables being a potential challenge for Neural Statistician models. The Neural Statistician model faces challenges with under-utilization of latent variables when the dataset size is small or the inference network is not expressive enough. A Variational Homoencoder shows improvements in such cases, as demonstrated on simple 1D distributions. Five datasets were created, each with 100 classes from a specific parametric family. Results indicate that the Neural Statistician tends to place little information in the inference network when the dataset size is small. A Variational Homoencoder (VHE) is crucial for improving few-shot generation and classification by utilizing the latent space effectively. The VHE introduces data-resampling and KL-rescaling modifications, leading to better performance compared to a Neural Statistician model. Training involved 1200 Omniglot classes with image augmentation and affine transformations. The study involved training various models on 1200 Omniglot classes with image augmentation and affine transformations. Different models were tested, with Prototypical Networks achieving the highest accuracy of 96.0% on classification tasks. When using a standard deconvolutional architecture, little difference in classification performance is observed between models trained with different objectives. However, significant differences arise in the hierarchical PixelCNN architecture, where a Neural Statistician model shows poor classification accuracy compared to deconvolutional models. Despite improved sharpness in conditional samples, they are no longer identifiable to the cue images. The hierarchical PixelCNN architecture shows a 3-fold reduction in classification error for VHEs, achieving 98.8% accuracy with sharp and identifiable conditional samples. KL-rescaling and data resampling must be used together to prevent overfitting and improve performance. The VHEs achieve high classification accuracy and are competitive with existing deep learning approaches. They can generate new images in one-shot and are realistic and faithful to the cue image. Unlike discriminative models, VHEs model shared structure across images and perform well on the entire Omniglot test set. Sharing latent variables across elements of the same class improves generative performance. Our hierarchical PixelCNN architecture achieves state-of-the-art log likelihood results when trained using the full Variational Homoencoder objective, outperforming other models such as Generative Matching Networks. The VHE framework can be applied to models with richer category structure by building hierarchical and factorial VHE models. The hierarchical VHE extends the deconvolutional model with an extra latent layer to encode alphabet level structure for the Omniglot dataset. The model is trained using a single objective and tested for one-shot character generation and 5-shot alphabet generation. Our single trained model can learn structure at both layers of abstraction for one-shot character generation and 5-shot alphabet generation. The PixelCNN model was extended to include a 6-dimensional latent variable for style representation. A style transfer task was tested by feeding separate images into character and style encoders, resulting in faithful synthesized samples. The Variational Homoencoder is a deep hierarchical generative model that disentangles character and style factors using separate latent variables. It achieves state-of-the-art results on the Omniglot dataset by sharing latent variables across elements and training a hierarchical PixelCNN model. This model demonstrates near human-level one-shot classification performance and produces high-quality samples in one shot. The Variational Homoencoder achieves near human-level one-shot classification performance and high-quality sample generation. The framework extends to models with richer latent structure, demonstrated through hierarchical and factorial models. Variational bound can be improved by learning subsampling procedures or introducing auxiliary inference networks. These modifications may enhance performance on datasets with greater intra-class diversity. The VHE objective can be enhanced by introducing an auxiliary network to infer the subset used in the sampling procedure. This meta-inference approach has been applied to approximate inference evaluation. Preliminary experiments showed no additional benefit on the Omniglot dataset, possibly due to high similarity within character classes. The text discusses the potential benefits of using a tightened objective in domains with lower intra-class similarity, such as natural images. It suggests using a resampling trick iteratively to construct likelihood bounds over hierarchically organized data. The approach involves learning a generative model for alphabets and applying the same trick to yield a bound. The text discusses using a resampling trick to construct likelihood bounds over hierarchically organized data for alphabets X. This involves training networks together with a single objective, sampling elements x, D a, and D c for each gradient step. The architecture includes an 8x28x28 latent variable c. Five models were trained on different objectives, with occasional instability in optimization requiring halting and restarting of training. For each objective, models were trained for 100 epochs on 1000 characters from the training set. Parameters achieving the best training error were selected. Classification performance was not optimized, but found to be correlated with the generative training objective. Classification was done by calculating the expected conditional likelihood under the variational posterior. To evaluate log likelihood, 5 more models were trained with the same architecture. To compare log likelihood, 5 additional models were trained with the same architecture on a 30-20 alphabet split. The background set was divided into training and validation data, with importance weighting used to estimate total class log likelihood. Gaussian posteriors were used, and a PixelCNN model with autoregressive weights was employed. The architecture described in BID6 was extended by introducing a new 64-dimensional latent layer with a Gaussian prior. The VHE was created using a deconvolutional architecture and trained on the Caltech-101 Silhouettes dataset. Test data from 10 object classes were used to generate 1-shot and 5-shot conditional samples."
}