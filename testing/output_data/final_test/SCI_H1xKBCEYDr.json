{
    "title": "H1xKBCEYDr",
    "content": "We focus on black-box adversarial attacks using Bayesian optimization (BO) to generate adversarial examples with limited information. Our approach focuses on developing query efficient adversarial attacks for low query budgets, addressing issues with optimizing high dimensional deep learning models using dimension upsampling techniques. It achieves performance comparable to state-of-the-art black-box attacks with significantly fewer queries, reducing query count by up to 80% in low budget scenarios. Neural networks are known to be vulnerable to adversarial examples, leading to ongoing research on crafting attacks and defenses. The curr_chunk discusses methods for crafting adversarial examples and corresponding defenses, focusing on white-box and black-box attack settings. In black-box attacks, attackers have limited query budgets to obtain information about the target network. In black-box attacks, a query budget limits the number of queries allowed to the model per input. Existing papers propose attacks with query budgets of 10,000 or 100,000, but the effectiveness in severely query-limited settings, like 100-200 queries, remains unclear. This study aims to design query-efficient black-box attacks in constrained query-limited settings. This work introduces a black-box attack method using Bayesian optimization to find adversarial examples efficiently with limited queries. By reducing the optimization problem dimension and using upsampling techniques, the method can successfully generate adversarial perturbations. Bayesian optimization is used to find adversarial perturbations more successfully than existing black-box attacks in query-constrained settings. The proposed method, BAYES-ATTACK, achieves comparable success rates with fewer queries on pretrained ImageNet classifiers. Experiments on the MNIST dataset show how upsampling techniques affect attack accuracy. Bayesian Optimization is a standard baseline for black-box adversarial attacks, especially in small query budgets. Adversarial attacks can be categorized by the information received from a query, with score-based attacks being closely related. Existing approaches use derivative-free gradient estimation methods to improve attack estimates. Bayesian Optimization is a standard baseline for black-box adversarial attacks, especially in small query budgets. Various methods, such as data-dependent priors and local search, are used to find adversarial perturbations within a search space with constraints on perturbation size. Some approaches incorporate a soft constraint and perform coordinate descent to decrease the perturbation size while keeping the perturbed image misclassified. The network's output layer is received as the result of a query, often in a decision-based setting. Successful work includes reformulating the problem as a search for the nearest decision boundary direction and using random gradient-free methods. Transfer-based attacks involve training a substitute network for adversarial attacks. Recent research has explored attacks on neural networks by training a substitute network and using white-box methods to transfer these attacks to the original target network. These methods have drawbacks such as the need to train a substitute model and lower attack success rates compared to gradient-based methods. Additionally, there is interest in using Bayesian optimization for constructing adversarial perturbations, with BO playing a supporting role in various methods. The main contribution of this paper is to show that Bayesian Optimization is a scalable, query-efficient approach for large-scale black-box adversarial attacks when combined with upsampling procedures. The paper introduces the use of Bayesian Optimization for large-scale black-box adversarial attacks with upsampling procedures. It defines a threat model for evaluating adversarial attacks and assumes the goal of the attacker is to find a perturbation that causes misclassification. In the score-based black-box setting, the attack aims to find a small perturbation that causes misclassification. The attack is posed as a constrained optimization problem with a query budget. The objective function used is from previous works by Carlini & Wagner and others. The perturbation must be smaller than a given threshold in the \u221e norm, but the attack can be adapted to other norms. In the context of black-box attacks, the proposed method allows for flexibility in the attacker's goals and measurement norms. Bayesian Optimization is utilized for generating adversarial examples efficiently, especially for high-dimensional inputs like ImageNet. This method is query-efficient and suitable for problems with expensive queries. Bayesian Optimization uses a Bayesian statistical model and an acquisition function to approximate the objective function efficiently. Gaussian processes are commonly used as the surrogate model, updating the posterior distribution with new query information. The acquisition function assigns a value to each point based on the utility of querying the model at that point. The objective is to maximize the posterior of observations under a prior by selecting hyperparameters. The optimization problem is solved using the LBFGS algorithm, with expected improvement (EI) as the popular choice of acquisition function. Bayesian optimization framework iteratively updates the posterior distribution on the objective function and finds the next sampling point by optimizing the acquisition function over the current posterior distribution of GP. The process repeats until the objective function rises above 0. Bayesian optimization struggles with high-dimensional inputs like ImageNet due to decreased speed and performance. To address this, a method is needed to reduce the dimensionality of the optimization problem. By leveraging spatial local similarity in images and gradients, it is possible to reduce query complexity and learn adversarial perturbations in a lower dimension. This approach allows for the generation of final adversarial perturbations without conforming to the actual image dimensions. Our method aims to reduce the dimensionality of the optimization problem by learning a low-dimension perturbation and upsampling it to obtain the final adversarial perturbation. The objective function for Bayesian optimization in low dimension is defined, and the perturbation is projected back to ensure it remains bounded. The complete framework involves learning an adversarial perturbation in a much lower dimension. The goal is to learn an adversarial perturbation in a much lower dimension by updating a Gaussian Process on a small dataset and iteratively querying the model until finding the perturbation or running out of queries. The Bayesian optimization iterations run in low dimension, but for querying the model, the perturbation is upsampled, projected, and added. In this work, the focus is on generating successful adversarial perturbations by upsampling and projecting the perturbation to conform to the input space of the model. The final adversarial image is obtained by adding the learned perturbation to the original image. The method used for upsampling is nearest neighbor, and the perturbation bound is defined for \u221e -norm perturbations. The study compares different upsampling techniques for generating adversarial perturbations in the untargeted attack setting. The experiments focus on ImageNet classifiers and include ablation studies on the MNIST dataset. Success rates of BAYES-ATTACK are compared to other black-box attacks in terms of query budget. The success rate of different attack methods is evaluated by comparing the number of queries used to perturb images successfully. The latent dimension d is treated as a hyperparameter and tuned for both MNIST and ImageNet datasets. For ImageNet, different perturbations across channels work better for ResNet50 and VGG16-bn, while both types work equally well for Inception-v3. The Gaussian process is initialized with 5 samples from a standard normal distribution. Expected improvement is used as the acquisition function, but other functions also work well. We compare the performance of our method BAYES-ATTACK against other black-box attacks on ImageNet using pretrained ResNet50, Inception-v3, and VGG16-bn models. The perturbation bound is set to 0.05, and we evaluate the methods with low query budgets. We use NES and BANDITS-TD implementations for comparison. The proposed method BAYES-ATTACK outperforms baseline methods for query budgets < 200, showing better success rates on ResNet50 and VGG16-bn. However, for higher query budgets (> 1000), PARSIMONIOUS and BANDITS-TD perform better. BAYES-ATTACK achieves higher success rates with 80% fewer average queries compared to other methods. The BAYES-ATTACK method shows superior performance with 80% fewer queries compared to other methods, making it a compelling approach for low query settings. The generated adversarial perturbations are also compared in terms of distortion. The proposed method involves upsampling techniques for perturbation mapping in the latent dimension to the original input dimension. Different linear and non-linear upsampling schemes are compared on MNIST, including Encoder-Decoder and Interpolation methods. Interpolation methods include nearest-neighbor, bilinear, and bicubic, while encoder-decoder methods involve training variational autoencoders and simple autoencoders. The Bayesian optimization loop is run in the latent space for both approaches. The proposed method involves upsampling techniques for perturbation mapping in the latent dimension to the original input dimension. Bayesian optimization is used in latent space with a pretrained decoder for image mapping. Nearest Neighbor interpolation and VAE-based decoder show better performance in upsampling methods. Sensitivity analysis on the latent dimension hyperparameter is conducted. The study explores the performance of nearest neighbor interpolation method in different latent dimensions, showing that lower dimensions achieve better success rates than the original input dimension. Bayesian optimization is used for query-efficient black-box adversarial attacks with constrained budgets, searching for perturbations in low-dimensional latent space. The study demonstrates the effectiveness of using Bayesian optimization for adversarial attacks on deep learning architectures. The method searches for perturbations in low-dimensional latent space and maps them back to the original input space using nearest neighbor upsampling. The convergence of the objective function in the BAYES-ATTACK on RESNET50 trained on ImageNet is shown, with the iteration loop stopping once a positive objective value is reached, indicating a successful adversarial perturbation. The study demonstrates the effectiveness of using Bayesian optimization for adversarial attacks on deep learning architectures. The BAYES-ATTACK on RESNET50 trained on ImageNet is shown, with perturbations in low-dimensional latent space upsampled to the original input dimension. The convergence of the objective function is compared between different latent dimensions, showing that BO with 16 dimensions finds the adversarial perturbation in less than 200 iterations, while BO in 784 dimensions does not converge successfully. The study compares the effectiveness of BAYES-ATTACK with other methods on RESNET50 trained on ImageNet. Results show that BAYES-ATTACK achieves similar distortion levels but better attack success rates in low query budget scenarios. Our approach, similar to BANDITS-TD and PARSIMONIOUS, focuses on generating successful adversarial perturbations within a specified maximum distortion limit."
}