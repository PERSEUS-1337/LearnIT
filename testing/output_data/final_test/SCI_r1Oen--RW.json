{
    "title": "r1Oen--RW",
    "content": "Saliency methods aim to explain deep neural network predictions but lack reliability when sensitive to irrelevant factors. Adding a mean shift to input data can cause methods to incorrectly attribute. Input invariance is crucial for reliable saliency methods. Methods not satisfying this property can lead to misleading attributions. Research is ongoing to balance model complexity and interpretability in understanding neural network decision processes. Research is urgently needed to address the trade-off between model complexity and interpretability in deep neural networks. Saliency methods aim to provide insights into model predictions by ranking the explanatory power of inputs, but their reliability is hindered by divergent outcomes and lack of ground truth. This complicates the evaluation of these methods, as full transparency into model decision-making is necessary for accurate assessment. The need for reliable saliency methods in deep neural networks is crucial for accurate model interpretation. Implementation invariance and input invariance are key properties that ensure consistent attribution and sensitivity to input transformations. Many existing methods fail to satisfy input invariance, highlighting the importance of developing more reliable techniques. In this work, the authors introduce the concept of input invariance and demonstrate that certain saliency methods do not satisfy this property when a simple mean shift in the input is considered. They show that when input invariance is missing, the saliency method becomes unreliable and misleading, as changing the reference point causes the attribution to diverge. Visualizations were done on ImageNet and the VGG16 architecture. The study explores the input invariance of saliency methods in the context of the VGG16 architecture. It highlights the divergence in attribution caused by different reference points and emphasizes the importance of choosing an appropriate reference. The experiment setup evaluates the input invariance of saliency methods across two networks with identical weights and predictions. The study compares saliency attribution across two networks, f1(x) and f2(x), with f1(x) trained on input xi from training set X1. Network 1 predicts the classification of a transformed input x, while Network 2 cancels out the mean shift transformation, resulting in identical weights and outputs for f1(x) and f2(x). In an experimental framework, Network 1 is a 3-layer MLP with 1024 ReLu-activated neurons classifying MNIST images in [0,1] encoding. Network 2 classifies in [-1,0] encoding. Both networks achieve 98.3% accuracy after training. Saliency research focuses on CNNs, with methods falling into three categories: Gradients, Signal, and Attribution. The saliency research focuses on CNNs and methods are categorized into Gradients, Signal, and Attribution. Gradients show how input changes affect classification scores, Signal methods isolate input patterns stimulating neuron activation, and Attribution methods assign importance to input dimensions by decomposing output values. These methods ensure completeness in attributions compared to gradients. In the context of saliency research on CNNs, various methods are evaluated for input invariance. The experiment setup involves comparing saliency heatmaps for predictions of two networks, with one network having a mean-shifted input. Methods like raw gradients, PatternNet, and Guided Backprop produce identical saliency heatmaps despite the mean shift. The experiment evaluates different attribution methods for input invariance in saliency research on CNNs. Gradient, PatternNet, and Guided Backprop produce identical heatmaps for networks with identical weights, showing input invariance. However, these methods are not input invariant when comparing networks with different weights. The evaluation includes gradient times input, integrated gradients, and deep-taylor decomposition methods. The choice of reference point in attribution methods can lead to arbitrary input invariance. Multiplying raw gradients by the image can break attribution reliability. Different saliency heatmaps are produced by Gradient x Input, IG, and DTD for networks. Methods with specific reference points may not be sensitive to input transformations. Heatmaps of gradients alone are not sensitive to input transformations. GI multiplies the gradient with the input image, affecting attribution reliability. The choice of reference point in attribution methods can impact input invariance. Integrated Gradients (IG) and Deep Taylor Decomposition (DTD) determine input importance relative to a reference point. The reference point choice affects all subsequent attribution. IG and DTD can show different attributions based on the reference point chosen. Certain reference points can also make IG and DTD not input invariant. The Integrated Gradients (IG) method approximates an integral by a finite sum over a range of values. Evaluation of IG reference points shows that certain choices are not input invariant. For example, using a black reference point produces identical attribution heatmaps, while a zero vector reference point does not maintain input invariance. This lack of sensitivity to mean input shift is due to the determination of x 0 after the mean shift, resulting in consistent differences between x and x 0 for both networks. Deep Taylor Decomposition (DTD) determines attribution relative to a reference point neuron, which can satisfy input invariance if the right reference point is chosen. The attribution of input neurons is initialized to be equal to the output of that neuron and backpropagated to its input neurons using a distribution rule. The input invariance of DTD is evaluated using reference points determined by Layer-wise Relevance Propagation (LRP) and PatternAttribution (PA). Layer-wise Relevance Propagation (LRP) and PatternAttribution (PA) determine attribution relative to different reference points. LRP loses reliability with a zero vector root point, while PA maintains input invariance by defining the root point based on the natural direction of data variation. PA compensates for the mean in the data, making it inherently input invariant. SmoothGrad (SG) inherits invariance properties from the underlying attribution method, being insensitive to input transformations for gradient and signal methods (SG-PA and SG-GB). However, it lacks input invariance for integrated gradients and deep Taylor decomposition when using a zero vector reference point, but remains insensitive when using PatternAttribution (SG-PA) or a black image (SG-Black). The vector a in PA accounts for non-linearity by depending on covariances and removing mean shift of the input, resulting in identical attribution for both networks. SmoothGrad (SG) replaces input with noisy versions and averages attributions across them. It results in sharper visualizations for multi-layer neural networks with non-linearities. SG maintains invariance of the underlying method but may produce different attributions for each network. SmoothGrad (SG) maintains invariance of the underlying method by applying noisy versions of the input and averaging attributions across them. However, it is not input invariant when applied to certain reference points, leading to potential misrepresentation of the model. Attribution can visually diverge for the same method with different reference points, highlighting the importance of choosing the right vantage point based on the domain and task. Attribution for image recognition tasks assumes known preprocessing steps and visual inspection of salient points. However, for Audio and Language models, attribution is more challenging due to intricate input interactions. Without understanding the implications of reference point choice, the reliability of the method is limited. A constant shift in input can mislead model predictions, as most methods are sensitive to this transformation. Network 1 and Network 2 are discussed, with attribution methods except for PA being sensitive to the constant shift. In Fig. 5, attribution methods are sensitive to constant shifts, allowing manipulation of attribution heatmaps. Using a black image as a reference point for IG no longer satisfies input invariance. Experiment conducted with a hand-drawn kitten image, ensuring specific attributions to chosen samples. Shift is clipped within [-.3,.3] to maintain image recognition. Other samples in dataset are also impacted. In Fig. 6, intentional misrepresentation of explanation is demonstrated. In Fig. 6, intentional misrepresentation of model prediction explanation is highlighted, emphasizing the need for a systematic approach to reference point selection in saliency methods. Further research is required to ensure reliability without relying on case-by-case solutions. The study introduces input invariance as a crucial factor for reliable attribution in network explanations, using a hand-drawn kitten image to illustrate deceptive explanations. It demonstrates that there are input transformations that can cause attribution to fail, sparking further discussion on the topic. The research also acknowledges that while saliency methods may offer intuition for image recognition tasks, they may not be input invariant. The motivation for this work stems from the difficulty in visually inspecting catastrophic attribution failures in images compared to other modalities like audio or word vectors."
}