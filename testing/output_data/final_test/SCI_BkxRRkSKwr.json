{
    "title": "BkxRRkSKwr",
    "content": "Deep neural networks excel at handling complex semantics in natural language but are often seen as black boxes. The hierarchical explanation problem focuses on understanding how these models interpret words and phrases. Prior methods like contextual decomposition have limitations, leading to inconsistent explanation quality. This paper introduces a formal approach to quantify the importance of words and phrases for generating hierarchical explanations. By modifying contextual decomposition algorithms, a model-agnostic explanation algorithm with competitive performance is proposed. Evaluation on LSTM models and fine-tuned BERT Transformer shows promising results. Recent advances in deep neural networks have shown impressive results in natural language processing tasks. However, the interpretability of these \"black box\" models has been a limiting factor. Our algorithms outperform prior works in providing hierarchical explanations, helping to explain semantics, extract classification rules, and enhance human trust in models. Recent advances in deep neural networks have focused on enhancing model interpretability through intrinsically interpretable structures or post-hoc explanation algorithms. Post-hoc explanation methods, like additive feature attribution, assign importance scores to individual input variables but may not effectively explain compositional semantics in natural language. Recent advances in deep neural networks have focused on enhancing model interpretability through intrinsically interpretable structures or post-hoc explanation algorithms. Contextual decomposition and its hierarchical extension go beyond the additive assumption to compute the contribution made by a word/phrase to the model prediction, generating hierarchical explanations on how the model captures compositional semantics. Input occlusion and additive attributions are used to assign importance scores to words in the model. The text discusses the importance of combining phrases in contextual decomposition using Shapley Values. It highlights the need to quantify the extra importance that emerges from combining two phrases, which has not been well studied. The focus is on how the importance of the combined phrase differs from the sum of the importance of the two component phrases on their own. This approach is compared to strategies in game theory for quantifying surplus from combining groups. The text proposes a formal way to quantify the importance of individual words/phrases and develop algorithms for generating hierarchical explanations. It introduces the concept of N-context independent importance of a phrase and suggests two explanation algorithms. The text introduces two explanation algorithms, Sampling and Contextual Decomposition (SCD) and Sampling and Occlusion (SOC), which outperform competitors in sentiment analysis and relation extraction tasks. These algorithms provide hierarchical visualization of compositional semantics, extract classification rules, and enhance trust in neural network predictions. The text introduces two explanation algorithms, Sampling and Contextual Decomposition (SCD) and Sampling and Occlusion (SOC), which outperform competitors in sentiment analysis and relation extraction tasks. The work identifies key challenges in generating post-hoc hierarchical explanations and proposes a mathematically sound way to quantify context independent importance of words and phrases. Two effective hierarchical explanation algorithms are developed based on the new formulation, and experiments show consistent outperformance over several datasets. Neural models like standard RNNs, LSTM, and Transformers extract hidden states from input sequences to generate output in the label space. LSTM uses the last hidden state for prediction scores, while Transformers use the [CLS] token representation. Additive feature attribution methods measure word-level importance for model predictions. The curr_chunk discusses the limitations of additive attribution methods in explaining complex interactions between words and compositional semantics in a sentence. It introduces contextual decomposition (CD) and agglomerative contextual decomposition (ACD) algorithms as non-additive measures for capturing non-linear compositional semantics. The curr_chunk introduces contextual decomposition (CD) as a method for generating hierarchical explanations by attributing scores to phrases in the input sequence. It iteratively decomposes hidden states of the model into contributions solely made by the phrase and contributions involving words outside the phrase. This process helps in capturing non-linear compositional semantics in a sentence. Contextual Decomposition (CD) is a method for generating hierarchical explanations by attributing scores to phrases in the input sequence. It decomposes hidden states of the model into contributions solely made by the phrase and contributions involving words outside the phrase, capturing non-linear compositional semantics in a sentence. The contribution involving other words in the sentences is defined as \u03b3 = 0, and the bias term contribution is denoted as b i. CD decomposes all intermediate outputs starting from the input layer until reaching the final output of the model h T = \u03b2 + \u03b3, treating the logit score W l \u03b2 as the contribution of the given phrase p to the final prediction. The contribution of a phrase to the final prediction is represented by the logit score. A follow-up study introduces the agglomerative contextual decomposition algorithm (ACD) for neural network architectures. The decomposition of activation functions is modified, and a measure of context-independent importance of a phrase is proposed. Two explanation algorithms are instantiated from this formulation. The curr_chunk discusses the properties that an attribution method should satisfy to generate informative hierarchical explanations, focusing on non-additivity and context independence. It emphasizes the importance of quantifying the surplus when combining phrases, drawing parallels to game theory. The importance of combining two component phrases in a prediction can be quantified in game theory. The attribution methods CD and ACD aim to represent contributions solely from a given phrase, but their assigned importance scores do not satisfy context independence. The importance scores assigned by CD and ACD algorithms do not satisfy context independence. The computation of \u03b2 in CD and ACD involves the \u03b3 term of a specific input sentence, making the \u03b2 terms dependent on the context of the phrase. ACD's decomposition of activation functions seems plausible, but empirical results show unreliable explanation quality. The algorithms may not provide accurate explanations in some models. The importance measure of phrases in models is formulated to satisfy non-additivity and context independence. The context independent importance is defined as the output difference after masking out a phrase, marginalized over all possible N-word contexts. An example is given to illustrate this concept. In Figure 2 (Right), an example is shown for the sampling and masking steps in evaluating context-independent importance. The process involves masking out an N-word context surrounding a phrase p from the input x, and sampling a N-word sequence x \u03b4 from a distribution conditioned on the phrase p and other words in the sentence x. The model prediction score is denoted as s(x \u2212\u03b4 ; x \u03b4 ) after replacing the masked-out context x \u2212\u03b4 with the sampled sequence x \u03b4. The operation of masking out the phrase p from the input sentence x is represented as x\\p. The specific implementation of this operation varies across explanation algorithms. Following the process of evaluating context-independent importance, the expectation is approximated by sampling from a pre-trained language model due to the sparse occurrence of phrases in the training corpus. This helps in modeling a smoothed distribution of x \u03b4 |x \u2212\u03b4 . The explanation algorithms implement N -context independent importance, with the size of the neighborhood N as a parameter to be determined. The contextual decomposition algorithm compromises context independence when decomposing activation functions. A new formulation for context-independent importance is introduced, leading to the development of a new sampling and contextual decomposition (SCD) algorithm for hierarchical explanations. SCD modifies the decomposition of activation functions in CD by defining \u03b2 as the expected difference in activation values. The contextual decomposition algorithm introduces a new formulation for context-independent importance, leading to the development of a new sampling and contextual decomposition (SCD) algorithm for hierarchical explanations. SCD modifies the decomposition of activation functions in CD by defining \u03b2 as the expected difference in activation values. The algorithm involves sampling with a pretrained LSTM language model and obtaining a set of samples S, recording the input of the i-th non-linear activation function for each sample in S to calculate the decomposition. The i-th non-linear activation function is calculated in neural models like Transformers, involving operations such as softmax functions and layer normalization. Improved performance is observed by not decomposing the normalizer when the phrase p is shorter than a threshold. Element-wise multiplication in LSTM models is treated similarly to other nonlinear operations. Input occlusion algorithms can be integrated into the formulation to calculate the importance of a specific phrase p in an input example x. The Sampling and Occlusion (SOC) algorithm calculates the importance of phrases by sampling neighboring words and computing the average prediction difference after masking the phrase. It evaluates explanation algorithms on shallow LSTM models and deep fine-tuned BERT Transformer models for sentiment analysis datasets. The text discusses the use of fine-tuned BERT Transformer models for sentiment analysis and relation extraction tasks using different datasets. The explanation algorithm is compared with Input occlusion and Integrated Gradient+SHAP baselines. The text introduces Gradient+SHAP (GradSHAP) and hierarchical explanation algorithms like Contextual Decomposition (CD) and Agglomerative Contextual Decomposition (ACD). It compares these with a naive baseline method and explains how positive segments are identified in negative sentences. Performance of different algorithms like Sampling and Contextual Decomposition (SCD) and Sampling and Occlusion (SOC) is also evaluated. The text introduces Gradient+SHAP (GradSHAP) and hierarchical explanation algorithms like Contextual Decomposition (CD) and Agglomerative Contextual Decomposition (ACD). It compares these with a naive baseline method and explains how positive segments are identified in negative sentences. Performance of different algorithms like Sampling and Contextual Decomposition (SCD) and Sampling and Occlusion (SOC) is also evaluated. The algorithms are verified for identifying important words and phrases captured by models through quantitative evaluation protocols. Evaluation includes computing Pearson correlation between coefficients learned by a linear bag-of-words model and importance scores attributed by explanation methods for word-level explanations. For phrase-level explanations, evaluations are done using the SST-2 dataset with human annotated sentiment polarity for each phrase on constituency parsing trees. Pearson correlation between ground truth scores and importance scores assigned for phrases is calculated. The importance scores for phrases, known as phrase \u03c1, are evaluated based on annotators considering the polarity of incomplete phrases in possible contexts. K = 20 samples are drawn from N = 10 adjacent words for explanation in SOC and SCD algorithms. Table 1 displays word \u03c1 and phrase \u03c1 results from these algorithms and competitors, with our formulations achieving the highest scores. SOC and SCD outperform input occlusion and contextual decomposition algorithms on deep Transformer models. Direct Feed method shows promise on shallow LSTM networks but struggles with deeper Transformer models. The explanation algorithm presented in the study effectively identifies phrase-level classification patterns in neural classifiers, showcasing competitive results compared to other algorithms. The qualitative study demonstrates the visualization of complex compositional semantics captured by models, highlighting positive segments in negative examples and adversative conjunctions. The algorithm is shown to be a natural fit for extracting phrase-level classification rules from neural classifiers. The study's explanation algorithm effectively identifies phrase-level classification patterns in neural classifiers, showcasing competitive results. It demonstrates the visualization of complex compositional semantics captured by models and is a natural fit for extracting phrase-level classification rules. The algorithm constructs hierarchies and classification rules automatically, aiding in better trust of model predictions through human evaluation. The LSTM model utilizes Direct Feed algorithms to binarize phrase importance and construct hierarchies automatically. Results show that SOC outperforms ACD and GradSHAP on the SST-2 dataset, while SCD outperforms CD and Direct Feed on the TACRED dataset. Both SOC and SCD require specifying context region size and number of samples. The impact of these parameters is shown in Figures 6 and 7, with sampling the context yielding better performance. The text discusses the performance of sampling versus padding contexts in language models. It also mentions the increase in word and phrase \u03c1 with more samples, and how the performance saturates as the context region size grows. The importance of interpretability in neural networks is highlighted, with various techniques being explored. Researchers have explored various methods to assign importance scores to input features, including input occlusion, gradient-based algorithms, additive feature attribution methods, and Shapley value-based approaches. Efforts have been made to efficiently marginalize over alternative input features, with a focus on marginalizing over contexts. In the realm of explanations for models with structured inputs, L-Shapley and C-Shapley have been proposed for approximating Shapley values efficiently. Researchers have explored various methods to assign importance scores to input features, including input occlusion, gradient-based algorithms, additive feature attribution methods, and Shapley value-based approaches. Chen et al. (2018) propose a feature selection based approach for explanation in an information theoretic perspective. Global explanation algorithms (Guidotti et al., 2018) have also been studied for identifying generally important features. Compared with local explanation algorithms, global explanation algorithms are less studied for explaining individual predictions, but with a hierarchical organization, they can be powerful at explaining individual predictions. Global explanation algorithms are powerful for explaining individual predictions and can handle compositional semantics where local algorithms fail. Different explanation algorithms can be combined for a more comprehensive explanation. A new algorithm, Sampling and Contextual Decomposition (SCD), is proposed to quantify the importance of words and phrases in a context-independent manner. Additionally, a model-agnostic algorithm called Sampling and Occlusion is introduced for explanation purposes. The Sampling and Occlusion algorithm (SOC) is a model-agnostic explanation algorithm that generates informative hierarchical explanations and helps extract classification rules from models. Experiments on multiple datasets show the effectiveness of SOC in enhancing human trust of models. The fine-tuned BERT models achieve high accuracy and F1 scores on three datasets, outperforming LSTM models. A simple alternative approach for computing phrase importance is to feed input to the model and use the prediction score as the explanation. This method, although competitive, assumes the model performs well on incomplete sentence fragments. The LSTM model trained on inversed labels achieved the same accuracy as the original model. However, there was a significant drop in word and phrase \u03c1 for Direct Feed, while SOC and SCD remained robust. The masking operation could also cause performance drop when explaining long phrases. Resolving the risk for SOC involved implementing a masking operation for the phrase p, but empirical evidence of improved performance was not found."
}