{
    "title": "SyxKiVmedV",
    "content": "We introduce an attention mechanism to improve feature extraction for deep active learning in the semi-supervised setting. The proposed explanation-based attention shows accuracy improvements for MNIST and SVHN classification compared to uncertainty-based methods. Deep active learning minimizes annotations needed to train DNNs by selecting relevant data points from a large unlabeled dataset. In deep active learning, an attention mechanism is proposed to enhance feature extraction. The attention mechanism is inspired by image retrieval techniques and does not require training with the model. It relies on methods for generating visual explanations and attributing feature importance values. The effectiveness of explanation-based attention (EBA) mechanism for active learning (AL) combined with multi-scale feature extraction is demonstrated on image classification datasets. Experiments are conducted on distorted class-imbalanced training data, a more realistic scenario for unlabeled data. AL is a cost-effective approach in traditional machine learning pipelines and has been applied to data-demanding deep neural network (DNN) systems in semi-supervised or weakly-supervised settings. Existing AL methods struggle with high-dimensional data like images due to the lack of class and instance-level feature importance information and the inability to capture spatially-localized features. To address these limitations, the focus is on estimating spatially-multiscale features and utilizing the EBA mechanism. The EBA mechanism is used for active learning in image classification datasets. BID16 proposed augmenting the training dataset by labeling uncertain data points and pseudo-labeling high confidence predictions. Unlike BID16, pseudo labels are only used to estimate EBA vectors and find similarities between discriminative features. BID2 introduced uncertainty measurement for Bayesian inference using stochastic forward passes through a DNN with dropout layers. Data points with the highest uncertainty are selected using an acquisition function based on softmax output metrics. Recent work by Sener & Savarese (2018) introduced a feature similarity-based selection method that outperforms greedy k-center clustering. Our approach focuses on novel feature extraction, using a multi-scale EBA for active learning applications. This differs from previous methods such as using gradients for dataset subsampling. Other related works include online importance sampling methods and influence functions approach. Online importance sampling upweights samples in the mini-batch during supervised training using gradient similarity, while influence functions analyze data point importance using second-order gradient information. Pool-based active learning involves selecting new labels to be annotated and added to existing training pairs at each iteration. The goal is to optimize a deep neural network by minimizing a loss function to model parameters, with the ultimate aim of minimizing validation loss. In this work, the focus is on extracting relevant features for an oracle acquisition function. A low-complexity greedy k-center algorithm is used to select data points similar to misclassified entries in the validation dataset. Feature descriptors are defined as output of DNN layers, with a descriptor matrix for the validation and training datasets. A descriptor matrix for the validation and training datasets can be calculated using forward passes. Descriptors can be compressed for storage efficiency. A match kernel like cosine similarity can be used to match features. Feature maps extracted contain features that are not class and instance-level discriminative and spatially represent features for multiple objects. An attention mechanism is proposed to upweight discriminative features without modifying network architecture. The proposed method uses an Explainable Backpropagation Attribution (EBA) mechanism for feature selection in deep neural networks. Feature importance tensors are calculated for internal representations using backpropagation passes. Integrated gradients are used to estimate feature importance values efficiently. Pseudo-labeling strategy is employed due to the lack of labels in the process. The proposed method utilizes Explainable Backpropagation Attribution (EBA) for feature selection in deep neural networks. Pseudo-labeling strategy is used for similarity calculation without additional hyperparameters. The EBA A i can be converted to multi-scale attention vector, forming validation and train attention matrices. The complexity to generate A i is only K forward-backward passes. The method involves annotating a random subset of training data points, optimizing a DNN for this subset, and iteratively performing steps to generate descriptor-attention matrix pairs and select relevant data points. The proposed method utilizes Explainable Backpropagation Attribution (EBA) for feature selection in deep neural networks. It involves selecting relevant data points using acquisition functions and retraining the model. Various methods including random sampling and uncertainty-based approaches are evaluated on MNIST and SVHN datasets with class imbalance. Mean accuracy and standard deviation are reported after multiple experiment runs. The study evaluates different methods for feature selection in deep neural networks using Explainable Backpropagation Attribution (EBA). Results on MNIST dataset show that feature-only matching outperforms random selection by \u2248 1%, while EBA adds another 1% accuracy. Class imbalance increases the gap, with up to 20% improvement for feature-only matching and 25% with EBA. Figure 2 shows MNIST test dataset accuracy for 3 class imbalance ratios with 9 AL iterations. Figure 3 displays SVHN test dataset accuracy for 3 class imbalance ratios. Figure 3 shows the SVHN test dataset accuracy for 3 class imbalance ratios: 1 (no imbalance), 10, and 100. EBA-based methods outperform uncertainty-based approaches, except for a slight accuracy increase with a class imbalance ratio of 100. The dataset split is 500K/104K/26K, and a typical 8-layer CNN is used with specific hyperparameters. The study compared EBA-based active learning methods with uncertainty-based approaches using specific hyperparameters for SVHN dataset. Results showed that EBA methods outperformed uncertainty-based methods, especially with class imbalance. The proposed AL method showed faster convergence and higher accuracy compared to random selection. Incorporating recent image retrieval techniques, a novel EBA mechanism was introduced to enhance feature-similarity matching in deep active learning. Initial experiments on MNIST and SVHN datasets demonstrated the benefits of EBA in improving density-based active learning. The method also proved effective in class-imbalanced scenarios, highlighting the importance of additional feature supervision. Future research could explore EBA with various data distortions and biases, such as within-class bias and adversarial examples, with potential applications in object detection and image segmentation for enhanced focus on spatially-important features."
}