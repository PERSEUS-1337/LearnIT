{
    "title": "rkeXrIIt_4",
    "content": "In many settings, decision-making and control policies are learned through Behaviour Cloning (BC) and Inverse Reinforcement Learning (IRL). Recent methods for IRL, such as $f$-MAX, have shown the ability to learn effective policies with limited demonstrations. $f$-MAX is a generalization of AIRL and a subset of cost-regularized IRL framework. This work aims to compare the objectives for Learning from Demonstrations (LfD) more directly. In the continuous control domain, modern advances in reinforcement learning aim to optimize reward functions. Learning from Demonstrations (LfD) offers solutions when designing reward functions is challenging. BC and IRL are common approaches in the LfD framework. In Behavior Cloning (BC), expert actions are regressed from demonstrations, while Inverse Reinforcement Learning (IRL) infers the expert's reward function. IRL faces degeneracy issues, but the Maximum-Entropy (Max-Ent) IRL method has been successful in overcoming them. Recent research has introduced \"adversarial\" methods within the Max-Ent IRL framework. In Maximum Entropy reinforcement learning, the goal is to find a policy that maximizes reward accumulation in trajectories. Unlike standard RL, in Max-Ent IRL, we aim to find a reward function that maximizes the likelihood of sampled trajectories from an optimal policy. In Max-Ent IRL, the objective is to find a reward function that maximizes the likelihood of trajectories sampled from a policy. Methods like GAIL aim to directly recover the policy instead of the expert's reward function. In Max-Ent IRL, methods like GAIL aim to recover the policy directly instead of the expert's reward function. BID12 introduces causal entropy as the \"amount of options\" the policy has in each state. They propose a regularized Max-Ent IRL objective with a convex regularization function on cost functions. The optimal cost function is obtained using IRL \u03c8 (\u03c0 exp), and the optimal Max-Ent policy is derived using the convex conjugate of the regularization function. BID12 demonstrates that optimizing the regularized Max-Ent IRL objective leads to the same policy as directly optimizing another objective. They show that for symmetric f-divergences, a choice of \u03c8 can make the two objectives equivalent, allowing for efficient algorithms like Generative Adversarial Imitation Learning (GAIL) based on Jensen-Shannon divergence. Using RL, the student policy is trained to maximize the discriminator's output and minimize the Jensen-Shannon divergence between expert and student policy distributions. This approach combines Generative Adversarial Networks (GANs), Inverse Reinforcement Learning (IRL), and energy-based models to recover the Max-Ent reward function. Adversarial Inverse Reinforcement Learning (AIRL) implements an adversarial training approach to recover the Max-Ent reward function and train the corresponding policy. The method involves a discriminator parameterized as f(s, a) and defines the reward function as r(s, a) := log D(s, a) \u2212 log (1 \u2212 D(s, a)). The objective is to maximize the RL objective, leading to an iterative optimization process alternating between optimizing the discriminator and the policy. At convergence, the advantage function of the expert is recovered. Important considerations are made regarding extracting the true reward function from f(s, a). Adversarial methods like GAIL and AIRL show significant performance gains compared to Behaviour Cloning in Mujoco benchmarks. Max-Ent IRL achieves strong performance with limited expert demonstrations. Max-Ent IRL is the dual problem of matching reward functions. In this section, f-MAX is introduced as a method for matching distributions using f-divergences. These divergences are defined using convex, lower-semicontinuous functions and include forward and reverse KL divergences. Variational estimation methods are presented for f-divergences between arbitrary distributions. f-MAX is a method introduced for matching distributions using f-divergences, with an iterative optimization scheme presented for matching an implicit distribution to a fixed distribution using any f-divergence. This approach provides a more intuitive interpretation of similar algorithms and aims to train a policy by optimizing the f-divergence through an iterative optimization procedure. The iterative optimization procedure proposed for matching distributions using f-divergences involves optimizing T \u03c9 to approximate a fixed distribution. The policy objective is equivalent to minimizing a specific equation with respect to \u03c0, leading to convergence to the global optimum where the policy matches that of the expert's distribution. This process involves iteratively computing D f (\u03c1 exp (s, a)||\u03c1 \u03c0 (s, a)) and optimizing the policy accordingly. The policy objective in equation 9 involves minimizing the reverse KL divergence, as discussed in Ho & Ermon (2016). AIRL solves the Max-Ent IRL problem by minimizing this divergence, connecting surrogate cost functions for binary classification with f-divergences. The method discussed in BID12 minimizes symmetric 4 f-divergence between expert and policy distributions. The f-MAX method is a subset of the cost-regularized Max-Ent IRL framework. Cost functions and state-action distributions are represented as vectors in a finite state-action space. The expert is optimized using a regularization function \u03c8(c) on cost functions. The cost function regularizer in the context of f-divergence minimizes the KL divergence between expert and policy distributions. The causal entropy term is a policy regularizer weighted by \u03bb. Various Imitation Learning algorithms are represented as minimizing statistical divergences between expert and policy state-action distributions. The corollary in section 3.1 shows that direct IRL methods like AIRL and GAIL differ from Behavioral Cloning (BC) in how they optimize policies. They focus on matching state distributions in addition to action distributions, using divergences that seek modes. This leads to hypotheses that direct IRL methods perform better in low-data scenarios because the reward function depends more on states than actions. In Reinforcement Learning, optimizing with forward KL divergence leads to mode-covering behavior, while using reverse KL results in mode-seeking behavior. Mode-seeking is more beneficial for trajectory quality in low-data scenarios. An algorithm optimizing KL (\u03c1 exp (s, a)||\u03c1 \u03c0 (s, a)) is compared to Behavior Cloning and AIRL with varying expert demonstrations. The algorithm f-MAX is versatile but not suitable for forward KL. In the following sections, a direct Max-Ent IRL method is presented that optimizes the KL divergence. An algorithm for optimizing KL (\u03c1 exp (s, a)||\u03c1 \u03c0 (s, a)) is derived, similar to AIRL, using a discriminator to distinguish between expert and policy state-action pairs. This algorithm can be converted from the AIRL algorithm. The FAIRL algorithm converts the AIRL algorithm into its forward KL counterpart by modifying the reward function. The reward functions under the two settings are plotted, showing that in the forward KL version, the policy is severely punished if the expert assigns more probability mass to a state-action pair. In empirical comparisons between AIRL, FAIRL, and standard BC in Ant and Halfcheetah environments, FAIRL performs comparably to AIRL while Behaviour cloning lags significantly behind. Expert Policy: An expert policy is trained using SoftActor-Critic (SAC), a state-of-the-art reinforcement learning algorithm. The policy consists of a 2-layer MLP with ReLU activations and outputs for mean and covariance. Evaluation Setup: Expert demonstrations are generated with 4 sets containing {4, 8, 16, 32} trajectories, each subsampled by a factor of 20. The expert policy is trained using SoftActor-Critic (SAC) with a 2-layer MLP. Expert demonstrations are generated with 4 sets of trajectories, each subsampled by a factor of 20. When generating demonstrations, actions are sampled from the expert's distribution. Different learning-from-demonstration algorithms are compared by training with expert demonstrations and evaluating on test episodes. The student policy for AIRL and FAIRL has the same architecture as the expert. The student policies for AIRL and FAIRL have the same architecture as the expert, with a 2-layer MLP discriminator. Observations are normalized using expert demonstrations' mean and standard deviations. SAC BID11 is used as the RL algorithm, with temperature parameter tuned separately for AIRL and FAIRL. For BC, the model matches state-action marginals by sampling actions from the state-conditional distribution. Trained policies deploy the mode of this distribution in the deterministic setting. In the deterministic setting, evaluation results for AIRL and FAIRL are presented. FAIRL outperforms AIRL in the Ant environment with deterministic evaluation. Both AIRL and FAIRL outperform BC, especially in the low data regime. FAIRL's performance supports the hypothesis that Max-Ent IRL's performance gain is not solely due to the direction of KL divergence. In comparing AIRL and FAIRL, AIRL consistently outperforms FAIRL across tasks and expert demonstrations. However, with deterministic evaluation, FAIRL achieves a significant performance gain in the Ant environment, surpassing AIRL. This suggests that as more expert demonstrations are provided, FAIRL broadens its distribution to cover the data-distribution, resulting in lower reward trajectories. Further detailed experiments are needed to compare the two methods effectively. The motivation for this work was to understand the relation between different methods for Learning from Demonstrations. A generalization of AIRL, called f-MAX, was presented, showing that AIRL optimizes for KL divergence. Two reasons for the superior performance of AIRL were hypothesized: additional terms in the objective encouraging matching of state distributions and the direction of the KL divergence being optimized. Empirical evaluation was done with FAIRL to test these claims. FAIRL, a modification of AIRL, optimizes KL divergence (\u03c1 exp (s, a)||\u03c1 \u03c0 (s, a)) and outperformed BC similar to AIRL. Comparisons between FAIRL and AIRL shed light on the role of the direction of the optimized KL. Future work aims to explore diverse environments and different f-divergence choices for improvement. Understanding the mode-covering behavior of FAIRL for robust policies is also a key focus. In the infinite horizon case with fixed probability of transitioning to a terminal state, the discounted sum is normalized by \u0393. Analytic considerations are needed to go from equation 34 to 35, but it holds when the ranges of h and \u03c1 \u03c0 (s, a) are bounded. FAIRL performs comparably to AIRL, while Behaviour cloning lags behind significantly. FAIRL performs comparably to AIRL, while Behaviour cloning lags behind significantly. The advantage of direct Max-Ent IRL methods over BC is due to their objectives explicitly matching marginal state distributions. The optimal discriminator has the form D(s, a) = \u03c1 exp (s,a) \u03c1 exp (s,a)+\u03c1 \u03c0 (s,a)."
}