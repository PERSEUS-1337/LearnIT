{
    "title": "SJUX_MWCZ",
    "content": "When machine learning models are used for high-stakes decisions, they should predict accurately, fairly, and responsibly. To meet these requirements, a model should be able to defer judgment to a human user when it is not qualified to make a prediction. Learning to defer generalizes rejection learning by considering other agents in decision-making and optimizing complex objectives. This approach reduces bias and improves accuracy, even with biased users. Recent machine learning advances have increased reliance on automated systems in high-stakes domains like loan approvals, medical diagnosis, and criminal justice. In these situations, models should predict accurately, fairly, and responsibly. To meet these requirements, the concept of learning to defer is proposed, where the algorithm says \"I Don't Know\" when it cannot confidently satisfy the objectives. Learning to defer in automated systems is crucial for responsible decision-making. Instead of providing a prediction, the algorithm says \"I Don't Know\" when it lacks sufficient information, requiring a more qualified decision-maker. This approach extends rejection learning by considering the expected output of the decision-maker on each example, optimizing the joint system. It can be used with various training objectives, unlike traditional rejection learning focused on classification accuracy. Algorithms that defer to more informed decision-makers are essential for reliable automated systems. In automated systems, learning to defer is proposed as an alternative decision-making framework to improve accuracy and fairness. By embedding a deferring model in a pipeline, the model can defer judgment to better-informed decision makers downstream, enhancing overall system performance. Experimental results on real-world datasets from legal and health domains demonstrate the effectiveness of this approach. The algorithm learns models that can work with users to make fairer decisions in legal and health domains. Various definitions of fairness in machine learning include individual fairness, statistical parity, calibration, and disparate impact/equalized odds. It has been shown that disparate impact and calibration cannot be simultaneously satisfied. Woodworth et al. (2017) argue that fairness criteria should prioritize equal opportunity. In a subsequent paper, Woodworth et al. (2017) argue that fairness criteria should be integrated into learning algorithms, not added afterwards. BID13 and BID3 develop algorithms that incorporate equalized odds through regularization. Other works have explored the \"I don't know\" (IDK) option in learning, with BID9 proposing a framework for direct integration. KWIK learning is introduced as a theoretical framework in BID25. BID0 discusses the challenge of models learning rare cases and how human users can audit them. Wang et al. (2017) propose a cascading model that can say IDK at higher levels and pass decisions down. In previous works, fairness criteria are integrated into learning algorithms. Various papers explore the \"I don't know\" (IDK) option in learning, with some proposing frameworks for direct integration. However, the fairness impact of this procedure has not been thoroughly examined. Some papers discuss fairness in sequential decision making but lack an IDK concept or learning procedure. Varshney & Alemzadeh (2017) propose \"safety reserves\" and \"safe fail\" options. Varshney & Alemzadeh (2017) propose \"safety reserves\" and \"safe fail\" options for AI safety, without detailing how to learn these options or analyzing their impact within a larger decision-making framework. This work also relates to AI safety literature by presenting a method for machines to learn to collaborate effectively with humans, similar to previous research on human-robot cooperation. Additionally, the concept of jointly producing a fair classifier with a human aligns with previous work on algorithms for machines to align with human values. The rejection learning paradigm challenges the standard method of optimizing accuracy by considering the \"IDK Model\" as the system output. This approach involves making decisions on every example, even when the model is unsure, by allowing a human decision-maker to provide input. The text discusses the preference for a model to make predictions despite uncertainty, considering the doctor's accuracy in detecting amelanocytic melanoma. It also highlights the importance of considering fairness and bias in decision-making, emphasizing the need to evaluate the entire pipeline rather than just the model itself. The decision-making framework involves an automated model learning parameters and an external decision maker (DM) making final decisions. If the model predicts, the DM trusts it completely; if the model says IDK, the DM decides independently. The DM is assumed to be more powerful than the model, reflecting practical scenarios. Learning to defer is equivalent to rejection learning for various loss functions, including classification error, when the DM is an oracle. Learning to defer provides a modeling advantage over rejection learning when the decision maker (DM) is an oracle. This allows for optimization of various objectives, including fair IDK models. The paper discusses background on fairness setup, methods for learning IDK models in a fair way, and a learning algorithm for optimization. Results on real-world datasets are also presented, focusing on fair binary classification with known sensitive attributes. The aim is to ensure accuracy and fairness in classifiers, with a trade-off between the two objectives. Fairness constraints may lead to worse error rates. Disparate impact (DI) is used as a fairness metric, with a continuous relaxation approach. Constraining DI = 0 is equivalent to equalized odds. The text discusses the trade-off between accuracy and fairness in classifiers, using disparate impact (DI) as a fairness metric. Constraining DI = 0 is equivalent to equalized odds. The model formulations for outputting \"IDK\" include ordinal regression and neural networks with weight uncertainty, aiming to train them to be fair. The text introduces a model that can classify examples as \"positive\", \"negative\", or \"IDK\" to allow for uncertainty in predictions. It utilizes ordinal regression with three categories and involves learning two thresholds for training. The model can use hard or soft thresholds, with soft thresholds associated with sigmoid functions. The output includes three values for every score, and a score p(x) is calculated to represent the model's prediction disregarding uncertainty. The model uses thresholds to classify examples as \"positive\", \"negative\", or \"IDK\" with values P, N, and I representing the model's predictions. Different thresholds can be learned for fairness, and regularization can be applied to IDK classifiers. Model parameters include \u03b8 and thresholds \u03c4, with a regularized loss function incorporating soft thresholds. The model uses soft thresholds for regularization and disparate impact control. Soft thresholds are learned end-to-end, while hard thresholds use a post-hoc scheme. A Bayesian approach involves learning a distribution over neural network weights for uncertainty estimation. Sampling from this distribution can indicate model uncertainty. The model uses soft thresholds for regularization and disparate impact control. A Bayesian approach involves learning a distribution over neural network weights for uncertainty estimation, allowing high values to be more uncertain. Regularizing the Bayesian model can improve fairness by making solutions with low disparate impact more likely. When the model punts, the prediction is made by an external decision maker, possibly biased but more accurate. The model uses soft thresholds for regularization and disparate impact control, involving learning a distribution over neural network weights for uncertainty estimation. Regularizing the Bayesian model can improve fairness by making solutions with low disparate impact more likely. When the model punts, the prediction is made by an external decision maker, possibly biased but more accurate. The distinction between learning to punt and learning to defer is introduced, where the model aims to identify cases where it has a low chance of being correct or where the external decision maker's chance of being correct is much higher. Learning to punt is described as DM-unaware learning, while learning to defer takes the DM's information into account for its decisions. Learning to punt is described as DM-unaware learning, while learning to defer involves taking the DM's scores into account for decision-making. A mixing parameter is introduced for each example, representing the probability of deferral. This parameter corresponds to the model's uncertainty estimate and is optimized for a combination of accuracy and fairness in the loss function. The joint system's predictions are expressed in the Bayesian neural network using p and \u1ef8. The model learns to defer by comparing its predictions to the DM's, recognizing relevant information not present in the data. Evaluation includes classification error, disparate impact, and deferral rate measurements. An independent model simulates predictions made by an external DM trained on a modified dataset. The DM used in the study is trained on a dataset with extra attributes for improved accuracy but lacks fairness training. When the model outputs \"IDK,\" the DM's output is used instead. Results are shown for predicting recidivism without race discrimination and predicting Charlson Index without age discrimination on two datasets. The models were trained using neural networks with logistic or ordinal regression. Additional details on the datasets are provided in Appendix C. The study used neural networks with logistic or ordinal regression for training the DM on datasets with extra attributes. Different hyperparameters were used for COMPAS and Health datasets, with ADAM BID21 for gradient descent. Training data was split into 80% training and 20% validation, stopping after 50 epochs without a new minimum loss. Soft thresholds were used for differentiability in ordinal regression, while hard thresholds were used in the post-hoc model. Results of various models with hyperparameters were displayed. Additional details can be found in Appendices D and E. The study used neural networks with logistic or ordinal regression for training the DM on datasets with extra attributes. Different hyperparameters were used for COMPAS and Health datasets, with ADAM BID21 for gradient descent. Training data was split into 80% training and 20% validation, stopping after 50 epochs without a new minimum loss. Soft thresholds were used for differentiability in ordinal regression, while hard thresholds were used in the post-hoc model. Results of various models with hyperparameters were displayed, showing the tradeoff between accuracy and fairness. The new results introduced a DM-aware model (defer-fair) which outperformed the previous punting model. The study compared punting models to binary models with and without fairness regularization. The IDK models, which did not have access to DM scores during training, achieved a stronger combination of fairness and accuracy on both datasets. The IDK models showed a more effective accuracy-fairness tradeoff compared to the baselines. Learning to punt can be a valuable tool for improving fairness in a many-part system. It provides a clear improvement over punting models and can effectively enhance the fairness of the entire system by penalizing deferring based on the output of the decision maker. The DM-aware model can effectively identify examples where the DM's expected loss is unusually high, revealing inconsistencies or biases. Deferring can account for specific characteristics of a biased DM, as shown in the case of extreme bias. Extra information given to the DM, such as the defendant's violent recidivism, can influence the model's decision to defer. The deferring model in the study achieved roughly 27% error with 2% DI, while the punting model had 4% DI. The deferring model showed improvement in handling IDK predictions for different groups, especially in cases of violent recidivism. The model learns to defer fairly by utilizing noisy access to additional information provided by the DM during training. The model proposed learns to defer fairly, navigating the accuracy-fairness tradeoff. It is part of a decision pipeline, with a framework for evaluating deferring models by incorporating other decision makers' output. The algorithm for learning to defer in a larger system optimizes the pipeline's performance. This framework has implications for domains where automated models interact with decision-making agents, allowing for culling examples or flagging troublesome decisions. In a decision pipeline, models learn to defer fairly, balancing accuracy and fairness. By incorporating other decision makers' output, the algorithm optimizes performance in larger systems. This approach has implications for domains where automated models interact with decision-making agents, flagging troublesome decisions for further investigation. In a decision pipeline, models learn to defer fairly, balancing accuracy and fairness by incorporating other decision makers' output. The deferring framework optimizes performance in larger systems by training the system output Y sys to optimize a loss function L(Y, Y sys). The deferring framework optimizes performance in larger systems by training the system output Y sys to optimize a loss function L(Y, Y sys). Learning to defer reduces to rejection learning for various objective functions when the decision maker is an oracle. Regularization with a disparate impact term is an effective way to reduce disparate impact without significantly increasing errors, as shown in results on the COMPAS recidivism dataset. The goal is to predict whether a criminal defendant will commit a crime while on bail. The deferring framework optimizes system performance by training the output to optimize a loss function, with the decision maker acting as an oracle. The study focuses on predicting recidivism in criminal defendants on bail, with race as a sensitive variable. Additional information on violent recidivism was provided to a decision maker (DM), simulating real-world scenarios. The DM had a 24% error rate, outperforming the baseline model's 29% error rate. The dataset was split into training and test examples. The Heritage Health dataset 2 focuses on predicting the Charlson Index, a comorbidity indicator related to chances of death. The dataset includes information on age, sex, lab tests, prescriptions, and claim details. The sensitive variable is age, binarized by over/under 70 years old. The simulated decision maker achieved a 16% error rate, outperforming the baseline model. The simulated decision maker had a 16% error rate, better than the baseline model's 21% error rate. The dataset was split into 46769 training examples and 20044 test examples. A post-hoc threshold optimization search procedure was used, sampling 1000 combinations of thresholds to minimize loss on the test set. Thresholds were selected based on values of \u03b1, \u03b3, and the original binary model. Sampling for thresholds was done based on scores in the training set below and above 0.5. The sampling scheme for threshold optimization involved selecting thresholds based on scores in the training set above and below 0.5. A random search approach was used, which proved to be faster and more effective than grid search or gradient-based methods. Calculating the regularization term for expected disparate impact with soft thresholds posed challenges, leading to an alternative calculation method. The regularization term for expected disparate impact with soft thresholds was calculated using squared underlying terms. Models with different deferral rates showed varying benefits from DM-aware training, with higher deferral rate models experiencing a larger benefit. Training benefits high deferral rate models more, indicating a win in fairness over accuracy. Comparison of DM-aware and -unaware learning shows varying benefits based on deferral rate. Bins differ between datasets due to deferral rate distributions during hyperparameter search."
}