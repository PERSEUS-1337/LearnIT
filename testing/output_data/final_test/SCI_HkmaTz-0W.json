{
    "title": "HkmaTz-0W",
    "content": "Neural network training relies on finding good minimizers of non-convex loss functions. Certain network architectures and training parameters affect how easily the loss functions train and generalize. The reasons for these differences and their impact on the loss landscape are not well understood. In this paper, the structure of neural loss functions and their impact on generalization are explored using visualization methods. A \"filter normalization\" method is introduced to visualize loss function curvature and compare loss functions. The study investigates how network architecture and training parameters influence the loss landscape and the shape of minimizers in neural network training. Despite the complexity of minimizing non-convex loss functions, simple gradient methods often find global minimizers, even with randomized data and labels. The trainability of neural nets depends on network architecture, optimizer choice, and variable initialization. High-resolution visualizations are used to empirically characterize neural loss functions and explore the impact of different architecture choices on the loss landscape. The non-convex structure of neural loss functions and the geometry of minimizers affect their generalization. The article proposes a \"filter normalization\" scheme to compare different minimizers found by various methods and explores the impact of network architecture choices on loss landscape geometry. The goal is to understand how differences in loss function geometry affect neural net generalization. The study focuses on producing meaningful loss function visualizations and investigates how loss landscape geometry influences generalization error and trainability. The study introduces a visualization method called \"filter normalization\" to compare minimizers and explores the impact of network architecture on loss landscape geometry. It reveals a transition from convex to chaotic behavior in deep neural networks, correlating sharpness with generalization error. Skip connections are shown to promote flat minimizers and prevent chaotic transitions. The visualization of SGD optimization trajectories reveals that skip connections promote flat minimizers and prevent chaotic behavior in deep networks. The low dimensionality of optimization trajectories is attributed to large nearly convex regions in the loss landscape. Visualizations have the potential to answer important questions about neural networks' ability to minimize non-convex loss functions and generalize effectively. Loss landscapes are theoretical, with studies on minimizing neural loss functions using random matrix theory and spin glass theory. Local minima are shown to have low objective value and can be global minima under certain assumptions. Some approaches analyze shallow networks with one or two hidden layers, showing the potential for globally optimal or near-optimal solutions. Based on the previous context, the current chunk discusses the existence of globally optimal or near-optimal solutions in neural networks with specific structures. It also mentions the assessment of sharpness/flatness of local minima, defining \"flatness\" as the connected region around the minimum where the training loss remains low. Additionally, it introduces the concept of sharpness and how it can be encoded in the eigenvalues of the Hessian. However, it highlights issues with quantitative measures of sharpness due to network symmetries. The current chunk discusses the limitations of using measures of sharpness in neural networks due to network symmetries. It introduces the use of local entropy as a measure of sharpness, which is invariant to certain transformations but challenging to quantify for large networks. Theoretical assumptions, such as independence of input samples, are also mentioned, highlighting the importance of visualizations in verifying these assumptions. The loss function in neural networks is represented by a high-dimensional space, making visualizations challenging. One method to plot loss functions is by choosing two sets of parameters and plotting the values along the line connecting them. This approach was used by BID10 to study the loss surface. The loss surface was studied by BID10 using a 1D interpolation method to analyze the \"sharpness\" and \"flatness\" of different minima. However, this method has limitations in visualizing non-convexities and lacks consideration for batch effects. The method of using 1D interpolation plots to analyze network architectures does not consider batch normalization or invariance symmetries. It involves choosing a center point and direction vectors to plot a function. This approach was used in previous studies to explore minimization methods and optimization algorithms, but it may result in low-resolution plots that do not capture the complex non-convexity of loss surfaces. The study uses high-resolution visualizations to show how network design impacts non-convex structure. Random direction vectors are used for plotting, but this approach cannot compare different minimizers or networks due to scale invariance in network weights, especially with ReLU non-linearities and batch normalization. The scale invariance of neural networks makes it difficult to compare plots without special precautions. Large weights can result in a smooth loss function, while small weights can lead to sensitivity to perturbations. Neural networks remain unchanged when weights are re-scaled, affecting the network's behavior. The scale invariance of neural networks makes it challenging to compare plots without special precautions. To address this, filter-wise normalized directions are used to remove the scaling effect and produce meaningful loss function plots. This method differs from previous approaches by considering the norm of individual filters. The proposed scaling for meaningful loss function plots involves filter normalization to address the scale invariance of neural networks. Filter normalization helps in comparing the sharpness of minimizers, which correlates with generalization error. Using filter normalization enables accurate side-by-side comparisons, while non-filter normalized plots may appear distorted and unpredictable. Small-batch SGD is believed to produce \"flat\" minimizers. In this study, the difference between sharp and flat minimizers is explored by training a CIFAR-10 classifier with a 9-layer VGG network using Batch Normalization. Two batch sizes are used: a large batch size of 8192 and a small batch size of 128. The solutions obtained by running SGD with small and large batch sizes are compared. In this study, the difference between sharp and flat minimizers is explored by training a CIFAR-10 classifier with a 9-layer VGG network using Batch Normalization. The solutions obtained by running SGD with small and large batch sizes are compared. Linear interpolation plots are used to analyze the loss values on training and testing data sets, with the inclusion of trainable parameters for \"running mean\" and \"running variance\" in \u03b8. The plot also includes classification accuracy in red, showing the small-batch solution. The study compares sharp and flat minimizers by training a CIFAR-10 classifier with a 9-layer VGG network using Batch Normalization. Results show that small batches generalize better in all experiments, regardless of sharpness differences. Sharpness comparisons are misleading and fail to capture the true properties of the minima. Weight histograms of the networks reveal insights into the differences between minimizers. The study compares sharp and flat minimizers in training a CIFAR-10 classifier with a 9-layer VGG network using Batch Normalization. Results show that small batches generalize better in all experiments, regardless of sharpness differences. Weight histograms reveal insights into the differences between minimizers, showing that large batch minimizers have larger weights when weight decay is added. The scaling of weights is irrelevant due to batch normalization, but small weights are more sensitive to perturbations and produce sharper minimizers. The experiment in FIG1 compares loss function sharpness near minimizers using filter-normalized directions, showing subtle differences between small batch and large batch minima. Results are visualized in Figures 4 and 5, indicating that small batch minimizers with non-zero weight decay have wider contours compared to sharper large batch minimizers. Using filter-normalized plots in Figures 4 and 5, comparisons between minimizers show that sharpness correlates with generalization error. Large batches produce visually sharper minima with higher test error. Adam optimizer has larger test error than SGD, with visually sharper minima. Some neural architectures are easier to minimize than others, depending on initial parameters and skip connections. The text discusses the non-convexity of loss functions in neural architectures and the impact on training. Different architectures exhibit varying levels of non-convexity, which affects generalization error. The study explores why some architectures are easier to train and why initialization is crucial. Training various networks reveals differences in non-convexity structure, correlating with generalization error. The landscape around obtained minimizers is analyzed using visualization methods. The text discusses different classes of neural networks optimized for performance on CIFAR-10 dataset: Residual networks, \"VGG-like\" networks without shortcut connections, and \"Wide\" ResNets optimized for ImageNet. All models are trained using SGD with Nesterov. High resolution 2D plots of minimizers for different neural networks are shown in FIG4, with results displayed as contour plots for easy visualization of non-convex structures. Loss and error values for these networks are provided in TAB1. The loss landscape of neural networks is affected by network depth, with ResNet-20-noshort having a benign landscape and ResNet-56-noshort displaying dramatic non-convexities. As network depth increases, the loss surface transitions from convex to chaotic, with ResNet-110-noshort showing extreme non-convexities and large regions where gradient directions do not point towards the minimizer. The loss landscape of neural networks is affected by network depth, with ResNet-20-noshort having a benign landscape and ResNet-56-noshort displaying dramatic non-convexities. ResNet-110-noshort shows extreme non-convexities and steep regions in all directions. Shortcut connections impact the geometry of loss functions, preventing chaotic behavior as depth increases. Skip connections are most important for deep networks, while shallow networks show less noticeable effects. Residual connections prevent the explosion of non-convexity. Increased network width in neural networks, such as wider ResNets, results in loss landscapes with no chaotic behavior. The wider models exhibit flat minima and wide regions of apparent convexity, preventing non-convexity. Additionally, increasing the number of convolutional filters per layer widens minimizers and prevents chaotic behavior, as shown in the loss landscape of ResNet-56 with multiplied filter numbers. The loss landscapes for neural networks show a partitioning into regions of low loss value with convex contours and high loss value with non-convex contours. Good initialization strategies are important for training \"good\" architectures, with normalized random initialization leading to initial loss values below 2.5. Well-behaved loss landscapes are dominated by large, flat, nearly convex attractors, while random initialization may lead to higher loss values. The loss landscapes of neural networks exhibit regions of low convex loss and high non-convex loss. Chaotic landscapes have shallow convex regions leading to lower loss values. Deep networks with shallow attractors may have initial iterates in uninformative chaotic regions. SGD struggled to train a 156 layer network without skip connections. Landscape geometry significantly impacts generalization, with flatter minimizers correlating with lower test error. Filter normalization helps visualize loss function geometry, with flatter minimizers leading to lower test error. Chaotic landscapes result in worse training and test error, while more convex landscapes have lower error values. Wide ResNets with the most convex landscapes generalize the best. Random directions are ineffective for visualizing optimizer trajectories, as they fail to capture the variation in optimization trajectories. Random directions are ineffective for capturing the variation in optimization trajectories, as shown in failed visualizations in FIG6. Using one direction from initialization to solution along with a random direction provides a better visualization, but random axes still capture minimal variation, leading to misleading straight line appearances. The expected cosine similarity between Gaussian random vectors in high dimensions is problematic for visualization. Using random directions to capture variation in optimization trajectories is ineffective, as shown in failed visualizations. Instead, PCA directions can validate the low dimensionality of trajectories and provide effective visualizations by capturing more variation. This approach allows for measuring the captured variation and producing plots along the contours of the loss surface. The text discusses using PCA to analyze optimization trajectories along the contours of the loss surface. It shows how optimizer trajectories move perpendicular to the contours at early stages and become more stochastic later on, especially with weight decay and small batches. When weight decay and small batches are used in optimization trajectories, the path becomes nearly parallel to contours and orbits the solution. A drop in stepsize reduces noise, causing a trajectory to fall into the nearest local minimizer. Descent paths are low dimensional, with most variation occurring in only 2 dimensions. Optimization trajectories are dominated by movement towards a nearby attractor, reflecting the presence of wide, flat minimizers in non-chaotic landscapes. In this paper, a new visualization technique was introduced to understand the impact of choices in neural network training, such as network architecture, optimizer selection, and batch size. The advancement of neural networks has relied on anecdotal knowledge and complex theoretical results, highlighting the need for a more general understanding of their structure. Effective visualization, combined with theoretical advancements, can lead to faster training, simpler models, and improved generalization. Figure 4 shows the difference in generalization error with zero weight decay compared to 5e-4 weight decay. The first row in the visualization uses zero weight decay, while the second row sets weight decay to 5e-4."
}