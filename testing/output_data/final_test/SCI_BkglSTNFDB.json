{
    "title": "BkglSTNFDB",
    "content": "In reinforcement learning, Jin et al. (2018) introduced a Q-learning algorithm with UCB exploration policy, proving its optimal regret bound for finite-horizon episodic MDP. This paper adapts the algorithm to infinite-horizon MDP with discounted rewards, showing improved sample complexity of exploration compared to previous methods. Reinforcement learning involves algorithms that learn and plan in sequential decision-making tasks with unknown system dynamics. Markov Decision Process (MDP) is a typical model used. The agent interacts with the environment by taking actions, receiving rewards, and transitioning to new states. The goal is to maximize cumulative reward while facing the exploration-exploitation dilemma. Theoretical analyses of reinforcement learning can be categorized into those with a simulator and those without. In the former, the algorithm can query outcomes from an oracle to learn transition dynamics. The current research focuses on estimating Q values and outputting near-optimal policies with minimal calls to an oracle. Extensive literature has explored discounted infinite horizon MDPs, achieving near-optimal time and sample complexities. In finite-horizon settings, research has made significant progress in defining regret and sample complexity, with nearly-tight bounds established. In the infinite-horizon setting, sample efficiency is measured by the sample complexity of exploration, counting algorithm mistakes along the trajectory. Various model-based algorithms like Rmax, MoRmax, and UCRL-\u03b3 have been proposed for infinite horizon MDPs, but there is still a gap in state-of-the-art. In the infinite-horizon setting, sample efficiency is measured by the sample complexity of exploration. Model-based algorithms have been proposed for MDPs, but there is still a gap in state-of-the-art algorithms. Model-free algorithms are more flexible and have achieved remarkable performance on benchmarks like Atari games and simulated robot control problems. The best model-free algorithm for infinite horizon MDPs without access to a simulator has a sample complexity of exploration\u00d5( SA 4 (1\u2212\u03b3) 8 ). In the context of sample efficiency in exploration for MDPs, Delayed Q-learning has a loose sample complexity bound due to over-conservative Q-value updates. To address this, a Q-learning algorithm with UCB exploration policy is proposed to improve sample complexity. This algorithm incorporates a UCB-like exploration term to enhance performance in finite-horizon settings. The algorithm presented in the paper improves sample complexity in exploration for MDPs by introducing a Q-learning algorithm with UCB exploration policy. This algorithm's sample complexity bound is shown to be better than Delayed Q-learning, matching lower bounds up to logarithmic factors. Key technical differences between infinite-horizon and finite-horizon settings are highlighted, with detailed explanations provided in Section 3.2. The paper is organized with notation in Section 2, the algorithm in Section 3, and main theoretical results in PAC sample form. In Section 3, the paper presents main theoretical results in PAC sample complexity bounds for a Markov Decision Process defined by S, A, p, r, \u03b3. The agent interacts with the environment indefinitely, observing states, taking actions, receiving rewards, and transitioning to the next state. Policies are non-stationary control policies of the algorithm since step t. The sample complexity of exploration in reinforcement learning algorithms is defined as the number of time steps where the policy is not optimal for the current state. This measure accounts for mistakes made along the trajectory and is widely used in previous works. The PAC-MDP definition by Strehl et al. (2006) defines an algorithm as Probably Approximately Correct in Markov Decision Processes if its sample complexity is polynomial in relevant quantities. The UCB Q-learning algorithm maintains an optimistic estimation of action value function Q(s, a) and historical minimum value Q(s, a). The UCB Q-learning algorithm maintains an optimistic estimation of action value function Q(s, a) and historical minimum value Q(s, a). The sample complexity of exploration bound is provided in Theorem 1, with obstacles for proving the theorem and a high-level description of the approach outlined. Infinite horizon MDP sample complexity differs from finite horizon due to exploration in under-explored regions at any time period. Guarantees in finite horizon do not imply much in infinite horizon settings. The analysis for finite horizon MDPs cannot be directly applied to infinite horizon settings due to the need for techniques to count mistakes along the entire trajectory. Establishing sufficient conditions for being -optimal at each timestep and state is crucial for bounding sample complexity. The proof in Jin et al. (2018) cannot be directly applied to the problem due to technical reasons related to decomposing learning errors in finite horizon settings. In the infinite horizon setting, learning errors cannot be decomposed as errors from a set of consecutive time steps before t, but errors from a set of non-consecutive time steps without any structure. This makes the analysis more challenging. The goal is to establish a sufficient condition so that \u03c0 t learned at step t is -optimal for state s t. In the infinite horizon setting, learning errors cannot be decomposed as errors from a set of consecutive time steps before t, but errors from a set of non-consecutive time steps without any structure. The goal is to establish a sufficient condition for -optimality at time step t. Condition 1 is a sufficient condition for is small for a few time steps within an interval [t, t + R]. Condition 2 implies Condition 1, and a key technical lemma (Lemma 2) is used to bound the total number of bad time steps. The remaining sections organize the sufficient condition for -optimality, the key lemma, and the proof of Theorem 1. The goal is to establish a sufficient condition for -optimality at time step t. Condition 1 is a sufficient condition for small errors within an interval [t, t + R]. Condition 2 implies Condition 1, and a key technical lemma (Lemma 2) is used to bound the total number of bad time steps. The policy learned at step t, \u03c0 t, is shown to be -optimal under certain conditions. The text discusses the conditions for -optimality at time step t, with Lemma 1 bounding the number of sub-optimal actions and the sample complexity of the algorithm. Lemma 2 bounds the weighted sum of learning error, leading to the conclusion that Lemma 1 follows from Lemma 2. Definition 3 introduces a class of sequences, and Lemma 2 establishes a relationship between a (C, w)-sequence and the probability of a certain outcome. Lemma 2 provides a technical proof in the supplementary materials, explaining how Lemma 1 can be proven. By considering a specific set J and applying Lemma 2 to a weighted sum, the theorem is proved by combining Lemma 1 and Condition 2. The dependence on |J| is analyzed, leading to an upper bound with quadratic dependence on 1/\u03b7. The theorem is proven by combining Lemma 1 and Condition 2, showing a quadratic dependence on 1/\u03b7 for a specific set J. Using Azuma-Hoeffding inequality, it is shown that a certain event occurs at most T times for fixed i and j. The theorem is proven by combining Lemma 1 and Condition 2, showing a quadratic dependence on 1/\u03b7 for a specific set J. Using Azuma-Hoeffding inequality, it is shown that a certain event occurs at most T times for fixed i and j. With a union bound, it is proven that there are at most RT time steps where the number of sub-optimal steps is bounded by a certain expression. The final choice of parameters is discussed, and the implications of the results are presented, including interesting properties of the algorithm beyond its sample complexity bound. The lower bound for worst-case sample complexity is also mentioned. The gap between our results and the lower bound is in the dependence on 1/(1\u2212\u03b3) and logarithmic terms of SA. Model-free algorithms have improved sample complexity bounds, outperforming Delayed Q-learning. For model-based algorithms, better sample complexity results are achieved. Model-based algorithms have shown better sample complexity results in infinite horizon settings. The sample complexity of exploration bounds of UCB Q-learning implies a PAC algorithm for finite horizon MDPs, with a regret bound of O(\u221aT). The results suggest a lack of reductions from finite horizon sample complexity to infinite horizon sample complexity. In contrast to Delayed Q-learning, a variant of Q-learning incorporating upper confidence bound achieves a sample complexity bound of \u00d5(\u03b3). A lemma is provided to show that with probability at least 1 \u2212 \u03b4/2, the difference between V*(st) and Q*(st, at) is greater than \u03b7(1\u2212\u03b3) in step t. Lemma 2 states that for every (C, w)-sequence, with probability 1 \u2212 \u03b4/2, certain properties hold. These properties are a result of the update rule in Algorithm 1. Additionally, two auxiliary lemmas are introduced to further support the main lemma. Lemma 2 establishes properties for (C, w)-sequences with high probability. The last property is proven by induction on t, showing the algorithm maintains Q(s, a) satisfying the Bellman optimality equation. The text discusses the properties of martingale difference sequences and uses the Azuma-Hoeffding inequality to show that a certain inequality holds with high probability. It also proves that a specific assertion holds for all variables with a certain probability. The text provides a proof for lemma 2 regarding (C, w)-sequences and discusses inequalities using rearrangement and Jensen's inequality. It also introduces a mapping from a finite horizon MDP. The text introduces a mapping from a finite horizon MDP to an infinite horizon MDP, showing that running the algorithm for a certain number of time steps guarantees visiting the starting state and obtaining an optimal policy with high probability. The PAC sample complexity is also discussed, along with the reduction from regret to PAC. Theorem 2 states that Delayed Q-learning has a sample complexity of exploration of \u2126 \u22123 ln(1/\u03b4) for certain MDPs with constant S and A. The MDP considered has a state space of {a, b, c} and action set of {x, y} with specific transition probabilities and rewards. In the MDP with states {a, b, c} and actions {x, y}, transition probabilities are specified as P(c|a, y) = 10 and P(b|a, x) = 1. Rewards are all 1, except R(c, \u00b7) = 0."
}