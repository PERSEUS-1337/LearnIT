{
    "title": "HJGkisCcKm",
    "content": "We present a method for translating music across different instruments and styles using a multi-domain wavenet autoencoder. The shared encoder and domain-independent latent space allow for translations even from unseen musical domains. The method is evaluated on a dataset from professional musicians, achieving convincing results, including translating from a whistle. This could potentially enable untrained humans to create instrumental music. In this work, a novel method is presented for translating music across different instruments and styles using autoregressive models and unsupervised domain transformation. The technology allows for high-quality audio synthesis and realistic musical translations, such as converting a Mozart symphony performed by an orchestra to the style of a pianist playing Beethoven. The technology presented allows for high-quality audio synthesis and realistic musical translations across different instruments and styles. A single universal encoder is used to address all input music, enabling capabilities known as universal translation. To achieve this, a domain confusion network provides an adversarial signal to the encoder to ensure domain-specific information is not encoded. Using a domain confusion network, the encoder avoids memorizing input signals and encodes them semantically. Input audio is distorted with random pitch modulation during training, allowing the network to project out-of-domain inputs to the desired output. The network achieves high performance in converting musical instruments, approaching that of musicians. Despite lower audio quality in generated music, it is challenging to distinguish between original and output files. The network can successfully process unseen musical instruments like drums and whistles. In this work, an autoregressive model is used for generating output, with training using ground truth output instead of predicted ones. Autoregressive inference is done during test time only. During training, autoregressive inference is not practical for generating realistic samples. Circular mapping for reconstruction is unrealistic, as output during training does not represent future test output. Cross domain translation can involve multiple domains, as seen in the StarGAN method. The method described employs multiple decoders, one per domain, and uses a single encoder-decoder pair per domain with shared latent spaces. Cycle-consistency and structure are added to the latent space using a variational autoencoder loss term. This approach eliminates the need for many constraints and does not impose a VAE loss term on the latent space. In addition to not imposing a VAE loss term on the latent space, the work employs a domain confusion loss. Audio Synthesis WaveNet is an autoregressive model used for tasks like denoising waveforms and Text-To-Speech applications. Voice conversion in VQ-VAE is achieved by utilizing a variational approach. In VQ-VAE, voice conversion is achieved using a variational autoencoder with a quantized latent space conditioned on speaker identity. The decoder is based on WaveNet, with a universal encoder for all domains and separate reconstructing decoders. The latent space is domain independent, reducing source-target pathways memorization through augmentation. Invariance is achieved through a strong bottleneck effect from discretization, unlike the use of discrete latent space in other works. In unconditioned music generation, long-range dependencies are focused on for up to 24 seconds. The architecture used is the wavenet-autoencoder, with inputs collected from consumer media. Multiple decoders and an auxiliary network are trained for disentangling domain information, along with an important augmentation step. In supervised learning, audio style transfer using sequence-to-sequence recurrent networks was performed between source and target spectrograms. A graphical model was trained on Bach's polyphonic tones to capture the specificity of his chorales. Style transfer involves modifying the style while keeping the content the same. Notable contributions in this field include methods that synthesize new images while minimizing content loss. The method of audio style transfer involves minimizing content loss with respect to the content-donor sample and style loss with respect to samples of a certain style. This is achieved by comparing network activations for image categorization and statistics of activations in different layers. Concatenative synthesis is used in computer music to create output audio resembling input audio by combining short pieces from the target domain. Comparing this method to others is challenging due to their complex interfaces and tunable parameters. Our domain translation method involves training multiple autoencoder pathways, one per musical domain, with shared encoders. A softmax-based reconstruction loss is applied to each domain separately, and input data is randomly augmented to extract high-level semantic features. A domain confusion loss is applied to the latent space to ensure encoding is not domain-specific. The translation architecture diagram is shown in FIG0. The autoencoder architecture used is based on a WaveNet decoder and a WaveNet-like dilated convolution encoder. The encoder is a fully convolutional network with three blocks of ten residual layers, totaling thirty layers. Each residual layer includes a RELU nonlinearity, a non-causal dilated convolution, a second RELU, and a 1x1 convolution. The network has a fixed width of 128 channels and an additional 1x1 layer after the three blocks. The WaveNet autoencoder architecture includes three blocks of ten residual layers with a fixed width of 128 channels. An average pooling with a kernel size of 50 milliseconds is used for encoding, followed by upsampling and conditioning for the WaveNet decoder. The audio is quantized using 8-bit mu-law encoding, resulting in some loss of quality. The WaveNet decoder has either four blocks of 10 residual layers with a receptive field of 250 milliseconds, or 14 layer blocks with a larger receptive field of 4 seconds. Each residual layer contains a causal dilated convolution. The WaveNet autoencoder architecture includes a causal dilated convolution with increasing kernel size, gated hyperbolic tangent activation, and skip connections. Modifications to the nv-wavenet CUDA inference kernels include initializing skip connections with previous WAV samples and increasing kernel capacity to support 128 residual channels. To improve the generalization capability of the WaveNet autoencoder, a dedicated augmentation procedure is employed to change the pitch locally. Training is done on one-second segments with pitch modulation applied randomly. Input samples from different domains are processed by a shared encoder and domain-specific WaveNet decoders. Domain classification network is used along with random augmentation procedures for samples. The network C predicts input data domain using 1D-convolution layers and ELU BID2 nonlinearity. It minimizes classification loss and trains music autoencoders with cross entropy loss. The autoregressive decoder D j is conditioned on E's output and fed target output s j during training. The network predicts input data domain using 1D-convolution layers and ELU nonlinearity, minimizing classification loss. Music autoencoders are trained with cross entropy loss. During training, the autoregressive decoder is conditioned on the output of E and fed target output s j. In music translation experiments, the autoencoder of domain j is applied to a sample s from any domain to generate output domain j without distortion. The bottleneck during inference is the WaveNet autoregressive process optimized by CUDA kernels. Music translation experiments were conducted using human evaluation and qualitative analysis on six classical musical domains. The study involved training music autoencoders on various classical music domains, including J.S Bach's cantatas, organ works, and keyboard works, Beethoven's piano sonatas, suites for cello, wind quintet, fugues, violin sonatas, and string quartet. The dataset used was the Teldec 2000 Complete Bach collection and MusicNET BID33. Two phases of training were conducted with different depths of decoders. The study trained music autoencoders on classical music domains, including J.S Bach and Beethoven pieces. Two training phases used different decoder depths. Training batches contained 16 one-second samples per domain. The method was implemented in PyTorch and trained on eight Tesla V100 GPUs for 6 days using the ADAM optimization algorithm. The confusion loss was weighted with \u03bb = 10 \u22122. The study compared the method to human musicians using the phase one network, converting music from domain X to piano for practical reasons. Three professional musicians were employed for the task: E, a conservatory graduate specializing in music theory and piano performance; M, a professional producer, composer, and pianist; and A, a music producer. The study compared the method of converting music to piano using a universal encoder and WaveNet. Three professional musicians were employed for the task, including an audio engineer, a music producer, and a skilled player of keyboards. The task involved converting 60 segments of music from different sources, including Bach and Mozart, as well as Swing Jazz, metal guitar riffs, and instrumental Chinese music. Human evaluation was used to compare the conversions. The study evaluated the audio quality and translation accuracy of a method for converting music to piano. Two scores were presented: one for the audio quality of the output piano and another for the translation matching score. Mean Opinion Scores (MOS) were collected using CrowdMOS BID27 package to assess the quality of the audio and the accuracy of the conversion. The study evaluated audio quality and translation accuracy of converting music to piano. Results show lower audio quality compared to human results. Harpsichord conversion is better than Orchestra, while unseen domains perform best. Human musicians outperform the system, with upcoming releases for public benchmarking. Lineup experiment tests ability to identify source musical segments from conversions. The study evaluated audio quality and translation accuracy of converting music to piano. Results show lower audio quality compared to human results. Harpsichord conversion is better than Orchestra, while unseen domains perform best. Human musicians outperform the system, with upcoming releases for public benchmarking. In each test, a set of six segments is used, with one real segment and five translations. The task is hard to define, as shown by confusion matrix in FIG1. Amazon Mechanical Turk freelancers tended to choose the Mozart domain as the source, regardless of the real source and presentation order. Two amateur musicians and a professional musician were asked to participate in the experiment. The study evaluated audio quality and translation accuracy of converting music to piano, with results showing lower audio quality compared to human results. Harpsichord conversion outperformed Orchestra, while unseen domains performed the best. Human musicians outperformed the system, with upcoming releases for public benchmarking. Two amateur musicians and a professional musician were asked to identify the source sample out of six options based on authenticity, showing confusion among participants. NSynth BID5 dataset contains 1,006 instrument samples labeled with unique pitch, timbre, and envelope, with correlation measurements of embeddings retrieved using the encoder network across pitch for multiple instruments. The embedding encodes pitch information clearly, with a cosine similarity of 0.90-0.95 between instruments. The network was retrained with data from MusicNet BID33 for reproducibility. The focus is on understanding the properties of the conversion beyond timbral transfer. Our system goes beyond timbral transfer by capturing stylistic musical elements. Examples show added ornamentation notes, seamless integration of different instrument lines, and conversion of piano bass lines to cello. The focus is on understanding the properties of the conversion process. The network adds a violin part to solo piano recordings of Bach and Beethoven to match the output distribution. The system is trained on two piano domains with a reduced latent space to encourage creativity in the decoders. Stylistic differences are observed in the Bach and Beethoven outputs, showing that the network learns from the training data. The network learns stylistic elements from the training data, comparing results with Concatenative Synthesis methods. Samples include a piano fugue translated into string quartet and wind ensemble renditions, and a polyphonic fugue converted to solo cello with convincing results. Our method successfully converts orchestra pieces to piano, solo cello, and wind quintet, surpassing concatenative synthesis results. The rendition of brass instruments and orchestral music after a drum roll is more coherent compared to MATConcat, despite our network never being trained on drums, brass instruments, or an entire orchestra. The results of supplementary S3 demonstrate the versatility of the encoder module in training an entire orchestra. Supplementary S4 shows out-of-domain conversion results, including other domains from MusicNet. The domain classification network does not perform significantly better than chance when the networks converge. Ablation studies were conducted, with the first study showing learning divergence without augmentation during training. Our experiments in voice conversion show a clear advantage for applying augmentation. Ablation studies were conducted, with one study showing that without the domain classification network, the model does not perform any conversion at all. Another study focused on the latent code size for converting MIDI clips to different music domains. In voice conversion experiments, augmentation proves advantageous. Ablation studies reveal the necessity of the domain classification network for successful conversion. Latent code size impacts the conversion of MIDI clips to various music domains. Samples in the Beethoven and Bach domains show different results based on latent dimensionality. Semantic blending of musical segments from different domains is explored through linear combination of embeddings. The study explores the use of decoders to generate seamless audio shifts within a specified time range. Additionally, a second network is employed for voice conversion, showcasing the superiority of latent space embedding over direct audio conversion. The research demonstrates potential for high-level tasks like music composition through reducing the latent space size. In his Ph.D. thesis research at Tel Aviv University, Adam Polyak explores reducing the latent space size to enhance decoder creativity in generating natural yet novel outputs. The study also compares the method to VQ-VAE for voice conversion, highlighting successful training without data augmentation. The study compares the WaveNet autoencoder method to VQ-VAE for voice conversion on three datasets. Results show higher quality samples with the WaveNet method, even without data augmentation. The model is modified to have a smaller latent encoding size of R 48 for improved performance. In the VQ-VAE implementation, the latent encoding size is reduced to R 48 from R 64. The encoder consists of 6 one-dimensional convolution layers with ReLU activation, using a dictionary of 512 vectors in R 128. The quantized encoding is upsampled to condition a WaveNet decoder for all speaker domains. The model is trained using dictionary updates with Exponential Moving Averages (EMA) with decay parameter \u03b3 = 0.99 and commitment parameter \u03b2 = 1."
}