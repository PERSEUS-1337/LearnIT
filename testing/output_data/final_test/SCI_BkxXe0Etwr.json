{
    "title": "BkxXe0Etwr",
    "content": "Reinforcement learning (RL) using value-based methods like Q-learning has been successful in various domains such as games and recommender systems. These algorithms efficiently learn the optimal value function to implicitly find a policy when the action space is finite. Extending Q-learning to continuous-action RL faces challenges in solving the max-Q problem optimally. The CAQL method minimizes the Bellman residual using Q-learning with various action optimizers, showing that the max-Q problem can be solved optimally with mixed-integer programming for better policies and robustness. Optimization induces better policies and is more robust than counterparts like CEM or GA. To speed up training of CAQL, three techniques are developed: dynamic tolerance, dual filtering, and clustering. To accelerate CAQL inference, an action function is introduced to concurrently learn the optimal policy. \"CAQL is compared with state-of-the-art RL algorithms on continuous control problems with varying action constraints, showing superior performance in heavily constrained environments. RL has been successful in games and recommender systems. Value-based algorithms like Q-learning are efficient for finite action spaces, while policy-based algorithms like policy gradient are more suitable for continuous action spaces in robotics.\" Policy-based reinforcement learning algorithms such as cross-entropy policy search and ensemble critic have shown practicality in maximizing returns. Methods like entropy regularization have been developed to enhance performance. Gaussian distributions are commonly used for policy parameterization in continuous control problems, but in applications like recommender systems, where actions are high-dimensional, common action distributions may not be suitable. Admissible action sets in reinforcement learning are often constrained, such as actions needing to fall within specific ranges for safety. Value-based algorithms are well-suited for reinforcement learning settings with random admissible actions, providing advantages over policy methods. Q-learning with linear function approximation converges to optimality under reasonable assumptions, while non-convex policy-based methods have limited optimality guarantees. Empirical results show that value-based methods are more data-efficient and less sensitive to hyper-parameters. Representing actions with continuous features can address the challenge of large action spaces in value-based algorithms. The main challenge in applying value-based algorithms to continuous-action domains is selecting optimal actions. Previous work falls into three categories: global nonlinear optimizers, Q-function parameterization for tractable optimization, and approximating Q-values piecewise-linearly. These approaches aim to ensure optimality in action selection. Continuous Action Q-learning (CAQL) is a Q-learning framework for continuous actions that addresses the challenge of selecting optimal actions. It introduces a generic model for the Q-function, aiming to improve performance compared to previous approaches that may degrade performance if the domain does not conform to imposed structures. The CAQL framework introduces a Q-learning approach for continuous actions, utilizing a feed-forward neural network to model the Q-function. It minimizes the Bellman residual using \"plug-and-play\" action optimizers, with a focus on \"max-Q\" optimization. Speed-up techniques are developed for computing max-Q values, enhancing practicality for larger-scale applications. CAQL is compared with various RL algorithms on benchmark problems to showcase its effectiveness. Value-based CAQL is competitive and outperforms policy-based methods in heavily constrained environments. A Markov decision process with states X, action space A, reward function R, transition kernel P, initial state distribution \u03b2, and discount factor \u03b3 is considered. An optimal policy \u03c0* maximizes the expected cumulative return. The Bellman operator F[Q] computes the state-action value function Q. The optimal Q-function Q*(x, a) can be approximated using a deep neural network (DNN) in methods like DQN. DQN updates the value function Q\u03b8 using a target Q-function and iteratively updates the target weights for a fixed number of training steps. The DQN algorithm uses double Q-learning for more stable training, with a loss function that includes mean squared Bellman error and hinge loss. The optimal Q-network weights are specified to stabilize training. In this work, the Q-function approximation Q \u03b8 is assumed to be a feed-forward network with hidden layers and ReLU activation functions. The action space is a d-dimensional ball with a center and radius. The max-Q problem is re-written as q*. Continuous Action Q-learning (CAQL) is a framework for continuous-action value-based RL, where the Q-function is parameterized by a neural network. CAQL formulates the \"max-Q\" problem as a mixed-integer programming (MIP), addressing challenges in handling continuous actions in RL. The MIP formulation guarantees optimal action and Q-value in continuous-action Q-learning. It offers flexibility for complex action constraints in RL but is computationally intensive. Approximation methods are developed to reduce computational demands, including an action function for inference and techniques like dynamic tolerance, dual filtering, and clustering to speed up max-Q computation. The max-Q problem can be formulated as a MIP using techniques like dynamic tolerance, dual filtering, and clustering to speed up computation. Approximate optimizers like gradient ascent and cross-entropy method can trade optimality for speed in max-Q computation within CAQL. A trained ReLU network can be modeled as a MIP by formulating the nonlinear activation function with binary constraints. The ReLU network can be modeled as a MIP with binary constraints, allowing for linear time computation of M+ and M-. This formulation ensures that z is the output of the ReLU, which can be extended to ReLU networks by chaining copies of intermediate ReLU formulations. The max-Q problem in a ReLU Q-network can be reformulated as a MIP with linear objectives. The ReLU network can be modeled as a MIP with binary constraints for linear time computation of M+ and M-. The MIP formulation can return provably global optima, unlike GA and CEM, and often produces high-quality solutions even with stopping conditions. The implementation can be found in the tf.opt package. In practice, a modern MIP solver speeds up the process of solving optimization problems by combining various techniques like branch-and-bound, cutting planes, and primal heuristics. This MIP model has been used in neural network verification and analysis, with novel applications to RL. Say et al. also proposed a MIP formulation for solving planning problems with non-linear state transition dynamics, but our approach focuses on solving the max-Q problem. Gradient Ascent (GA) and Cross-Entropy Method (CEM) are optimization methods for finding the optimal action in a neural network Q-function. GA iteratively computes the optimal action using a step size, while CEM samples actions and updates the sampling distribution based on top Q-values. In traditional Q-learning, the policy \u03c0 * is implemented by acting greedily with respect to the learned Q-function. To address the computational cost of computing the optimal action in the continuous case, an action function \u03c0 w is used to approximate the greedy-action mapping \u03c0 * by training it with training data. This approach is similar to distilling an optimal policy from max-Q labels. Our method learns a state-action mapping to approximate arg max a Q \u03b8 (x, a) without requiring distribution matching. Three methods are proposed to speed up the computationally-expensive max-Q solution during training: dynamic tolerance, dual filtering, and clustering. Dynamic tolerance is crucial in the early phase of CAQL to guide the gradient efficiently. The dynamic tolerance in CAQL adjusts based on TD-error and training step to speed up the max-Q solver. It replaces the optimal policy with the action function in Bellman residual to resolve circular dependency. The main motivation of dual filtering in CAQL is to reduce the number of max-Q problems at each training step by efficiently estimating an upper bound on the max-Q label. This is achieved by discarding data based on a convex relaxation before max-Q optimization, addressing non-convexity from ReLU activation function constraints. The ReLU activation function is used in each NN layer. The relaxed NN equations replace nonlinear equality constraints with a convex set. The optimal Q-value w.r.t. the relaxed NN is denoted as q * x. The condition q * x,\u03b8 target \u2264 (Q \u03b8 (x, a) \u2212 r)/\u03b3 is a conservative certificate for checking data inactivity. Dual filtering is derived for hinge Q-learning and applies to the 2-loss counterpart. Inactive samples in the \u03c0 w-learning problem can be utilized by replacing the max-Q label q. To reduce the number of max-Q solves, online state aggregation is applied by picking centroids from next states to form minimum covering balls. Max-Q values are approximated using first-order Taylor series expansion for remaining next states. The approach involves using centroids to form minimum covering balls for state aggregation, controlling complexity with a cluster radius parameter. By combining clustering with dual filtering, inconclusive next states are refined for improved accuracy. The approach involves using centroids to form minimum covering balls for state aggregation, controlling complexity with a cluster radius parameter. By combining clustering with dual filtering, inconclusive next states are refined for improved accuracy. CAQL is compared with baseline RL algorithms DDPG, TD3, and NAF, using different max-Q optimizers like MIP, GA, and CEM. The effectiveness of CAQL is demonstrated through comparison with state-of-the-art RL methods and ablation analysis. Policy quality is influenced by Q-learning with optimal Bellman residual using MIP, rather than approximation with GA or CEM, despite higher computational costs. CAQL is trained with 2 loss to match baseline implementations and compared with hinge loss in ablation analysis. Evaluation on various benchmarks includes both default and constrained action ranges to simulate real-world scenarios and test the hypothesis that policy-based methods struggle with constrained action sets. CAQL demonstrates flexibility in handling constraints, outperforming policy-based methods. Changes in episode limits and network size result in lower returns compared to state-of-the-art benchmarks. Policy performance is evaluated every 1000 iterations without exploration. CAQL measurements are based on generated trajectories, not optimal actions. CAQL significantly outperforms NAF, DDPG, and TD3 on most benchmarks, especially when using MIP for Q-learning. CAQL-MIP policies have lower variance compared to GA and CEM. Sensitivity to hyperparameters is illustrated in Table 2 with 320 configurations. Table 2 shows the mean final returns over 320 configurations, highlighting CAQL-MIP's low sensitivity to hyperparameters in 8 out of 14 benchmarks. Additionally, the comparison of max-Q solvers reveals MIP as the most computationally intensive option. In experiments with environments having large action ranges, the performance of a small ReLU NN (32 \u00d7 16) in CAQL is worse than other methods. This is attributed to the NN's limited representation power for complex tasks, hindering learning by optimizing for the true max-Q value inaccurately. Ablation analyses on CAQL using dynamic tolerance, dual filtering, and clustering show that reducing the number of max-Q problems improves performance. Dual filtering (DF) reduces max-Q problems in CAQL by 3.2% to 26.5% across benchmarks while maintaining performance. Clustering (C) further reduces max-Q solves without impacting training performance significantly. The Dual method eliminates all max-Q computation with dual approximation. Dynamic tolerance influences CAQL policy quality, with a large tolerance (\u03c4 = 100) achieving notable results compared to the standard algorithm. Compared to the standard algorithm, GA with a large tolerance (\u03c4 = 100) speeds up training with only 1 step per max-Q optimization but sacrifices performance. GA with dynamic tolerance achieves a balance between speed and performance. Ablation analysis on CAQL-GA shows improvements in final returns and reduction in inner-maximization steps. Continuous Action Q-learning (CAQL) is a framework for handling continuous actions in value-based RL, with the Q-function parameterized by a neural network. CAQL-MIP with dynamic tolerance significantly reduces MIP elapsed time variance and latency, improving performance. Ablation analysis on CAQL-MIP shows improved final returns and reduced inner-maximization steps. In value-based RL, the Q-function is parameterized by a neural network. The inner maximization of Q-learning can be formulated as mixed-integer programming with a ReLU network. CAQL-MIP generally outperforms CAQL-GA and CAQL-CEM in benchmarks with varying action constraints. CAQL is competitive with state-of-the-art policy-based RL algorithms and is more robust in heavily-constrained environments. Future work includes extending CAQL to full batch learning and speeding up MIP computation. The optimal Q-function is trained using offline data, speeding up MIP computation for scalability, and applying CAQL to real-world RL problems. The formulation is based on the LP formulation of MDP, with a penalty-based approach to transform constrained optimization into an unconstrained one. The text discusses using a single penalty parameter \u03bb \u2265 0 and a hinge penalty function for optimizing the Q-function in off-policy and offline RL algorithms. It mentions the challenges in finding an unbiased sample average approximation due to the non-linearity of the penalty function, leading to the study of an unconstrained optimization problem. Using Jensen's inequality, the optimization problem (7) serves as an upper-bound to problem (6), with equality holding for deterministic transition functions. The unbiased SAA of problem (7) is calculated using N samples from the data-generation distribution of the replay buffer. To find the optimal Q function, the parametrized form Q \u03b8 is used for approximation and optimization of the weights \u03b8. The parametrized form Q \u03b8 is used to optimize the real weights \u03b8 in problem (8). Training involves sampling transitions, computing optimal actions, updating Q-function parameters, action function parameters, and target Q-function parameters, and decaying Gaussian noise. The Q-function NN has a nonlinear activation function, acting as a nonlinear equality constraint. Wong & Kolter (2017) proposed a convex relaxation of the ReLU non-linearity in the activation function, treating it as a nonlinear equality constraint. They introduced component-wise bounds and a convex outer-approximation to relax the ReLU constraint. The relaxed NN equations are defined across layers for scalar inputs within a real interval. The relaxed NN equations propose a convex verification problem with indicator functions for set \u039b. The optimal value q * x is an upper bound for the original problem. If q * x \u2264 (Q \u03b8 (x, a) \u2212 r)/\u03b3, the sample is discarded, otherwise it may contribute to the TD-error in the hinge loss function. Dual variables are used to speed up computation. In the next section, a numerically efficient technique is proposed to estimate an upper-bound solution to q * x. A verification criterion is used to determine if a sample from the replay buffer should be discarded for inner-maximization. The sub-optimal lower bound solution to the relaxed problem in (11) is detailed, utilizing dual variables for linear equality constraints. The convex conjugate of a real-valued function is defined as the set support function, with a specific structure for the vector-ReLU indicator. The calculation involves analyzing scalar definitions and characterizing bounds for dual vectors. Three possible cases are considered, each with different scenarios for the Lagrange multipliers. The Lagrange multipliers are calculated by considering three cases: Case I involves removing variables, Case II replaces non-linearity with a linear equality constraint, and Case III deals with scenarios where certain values fall within specific ranges. In Case III, when l j (s) < 0 < u j (s), the dual function g(\u03bd) is not decoupled across layers. An analytical solution is obtained by optimizing each term independently. The best bound is achieved by setting a specific sub-case. The dual solution to problem (11) is calculated using the defined recursion for \u03bd. The k\u2212partial NN equations are defined for k \u2208 {3, . . . , K \u2212 1}. The k-partial NN equations are defined for k \u2208 {3, . . . , K \u2212 1}. Bounds for \u1e91 k are found by solving a problem with a one-hot vector e s. Recursive computation of bounds {l j , u j } for j = 3, . . . , K \u2212 1 is done using a convex relaxation technique. The Q-function and action function use ReLU activation with specific units. SCIP 6.0.0 is used as the MIP solver with time and optimality gap limits. GA and CEM have set iterations and convergence thresholds. MIP is more robust to action dimensions than GA. The experiment setup in Appendix D compares the robustness of MIP, GA, and CEM to action dimensions. MIP latency is influenced by the state of neural network weights, being slower with dense weights but quicker with sparse weights. MIP becomes faster over training steps, except for Humanoid benchmark. The Q-function weights gradually become sparser, making the problem easier for MIP. The mean return over 320 configurations is shown in Figure 4, with \u00b1 standard deviation. Ablation analysis on CAQL-GA with dynamic tolerance is presented in Table 11, showing mean final returns and average number of GA iterations. Table 13 displays the mean final returns over all configurations. Training curves can be found in the appendix."
}