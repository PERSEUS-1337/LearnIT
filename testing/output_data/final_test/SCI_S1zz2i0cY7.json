{
    "title": "S1zz2i0cY7",
    "content": "We propose using integer networks as a solution for reliable cross-platform encoding and decoding of images using variational models, addressing the issue of entropy coding failure due to computation differences between sender and receiver. In today's world, there is a division between source coding and channel coding. Source coding represents data as sequences of bits, while channel coding represents bits as analog signals on physical channels. The universal representation of compressed data is the binary channel, consisting of binary symbols with no noise. However, this binary channel is limited compared to other latent representations in variational latent-variable models. Variational latent-variable models, like VAEs, involve an encoder model e(y | x) mapping data x to a latent representation y, and a decoder model d(x | y) capturing data likelihood based on the latents. The prior p(y) estimates the marginal distribution of latents. These models aim to compactly represent relevant data information with a small expected KL divergence between the encoder and the prior. The encoder and prior in variational latent-variable models aim to compress data with a small KL divergence. However, further conversion to binary is needed for practical data compression, typically done through range or arithmetic coding. Range coding is optimal, converging quickly to the expected KL divergence in bits for large sequences like images. The encoder and prior in variational latent-variable models aim to compress data with a small KL divergence. Range coding is optimal for practical data compression, but round-off errors can cause catastrophic error propagation between sender and receiver. The representation must be discrete-valued and noiseless, with a total ordering of scalar elements. Both sides of the binary channel must evaluate the prior identically to avoid errors. Round-off errors in floating point calculations can lead to catastrophic error propagation, especially in data compression applications where sender and receiver may use different hardware or software platforms. This computational non-determinism can cause decoding failures, particularly in models using artificial neural networks for image compression. These neural networks are vulnerable to failures when deployed on heterogeneous platforms, prompting the need for solutions to enable their use in variational models. To address computational non-determinism in neural networks for image compression on heterogeneous platforms, integer networks are proposed as a solution. These networks use integer arithmetic to ensure determinism in computation, especially in generative and compression models. By utilizing integer networks, the representation can be computed end to end with full determinism, making them suitable for a wide variety of hardware. To ensure deterministic implementation on various hardware platforms, all data types are integral and operations are limited to basic arithmetic or lookup tables. Integer multiplications are followed by dividing outputs by a learned parameter to control dynamic range. Nonlinearity can be implemented deterministically using a lookup table or clipping operation. The nonlinearity approximates hyperbolic tangent for 4-bit signed integer outputs. The nonlinearity Q(7 tanh(v/15)) can be implemented deterministically using a lookup table. Inputs are transformed linearly, a bias is added, elementwise division is performed, and a nonlinearity function is applied. All values are integers, with rounding division used to ensure integral results. In programming languages like C, this can be implemented with integer operands. In C programming, linear filter coefficients and bias are constrained to use signed integers, while scaling vector uses unsigned integers. Accumulators have larger bit width than activations and filter coefficients to reflect dynamic range of multiplicative operations. Nonlinearity function must saturate on both ends of its domain due to integer limitations. Scale nonlinearities to match bit width of w for optimal dynamic range utility. Nonlinearities are scaled to match the bit width of w, allowing for signed or unsigned number formats. A 8-bit signed b, 32-bit signed v, 32-bit unsigned c, and 8-bit unsigned w are used with QReLU. The nonlinearity can be implemented with a simple clipping operation. Another example uses a 4-bit signed b, 16-bit signed v, 16-bit unsigned c, and 4-bit signed w to approximate the hyperbolic tangent, best implemented using a lookup table. The range is scaled to fill the 4-bit signed integer number. To effectively scale the domain of the nonlinearity, it is important to choose the input scaling based on the shape of the nonlinearity and the available dynamic range. In this case, a value of 15 was chosen to represent the nonlinearity well with the lookup table, ensuring that multiple input values are mapped to each output value. This approach helps preserve the approximate shape of the nonlinearity and prevent quantization errors or overflow issues. When training neural networks with small gradient signals, integer parameters are defined as functions of their floating point equivalents. The reparameterization mapping ensures stability in training by scaling down gradient magnitudes near zero. This approach helps prevent instabilities and fluctuations in the input to the nonlinearity. The reparameterization mapping scales down gradient magnitudes on integer parameters near zero to ensure stability in training. A special rescaling function is applied to linear filter coefficients before rounding them, maximizing accuracy. To backpropagate gradient signals, gradients of the loss function with respect to parameters cannot be directly taken due to zero gradients of the rounding function. After applying a special rescaling function to linear filter coefficients before rounding, gradients are computed almost everywhere except at half-integer positions where the gradient is positive infinity. The derivative of Q is replaced with the identity function to smooth gradients across rounded values. Integer parameters H, b, and c are computed after training and used for evaluation, allowing for further reparameterization. Gradients for rounding division are obtained by substituting the necessary values. To address the \"dead unit\" problem in QReLU, the gradient of the clipping operation is replaced with a scaled generalized Gaussian probability density function. This function includes a shape parameter \u03b2, acting as a temperature parameter that converges to the gradient of the clipping operation as \u03b2 approaches infinity. This approach allows for smoother training by ensuring gradients are propagated even when activations consistently hit the bounds. The integral of the function with a shape parameter \u03b2 is plotted in figure 2. A prior on the latent representation is assumed, and range coding is applied by imposing a total ordering on the elements of y. Parameters for a known distribution are computed using an ANN, and a deterministic approach is proposed for computing these parameters. Lookup tables can be used for precomputing all possible values if needed. In the image compression model, a modified Gaussian prior is used with scale parameters \u03c3 reformulated as a function of \u03b8. The scale parameters are discretized using constants \u03c3 min, \u03c3 max, and L, chosen logarithmically to minimize KL divergence. During training, backpropagation is used through the reformulation and network g. After training, a lookup table is formed for all possible values of p based on y and \u03b8, with g implemented using integer arithmetic. In the image compression model, a modified Gaussian prior is used with scale parameters \u03c3 as a function of \u03b8. The scale parameters are discretized using constants \u03c3 min, \u03c3 max, and L. To ensure identical reconstructions of data across platforms, it can be attractive to make the entire model robust to non-determinism. Integer networks can be used in the encoder or decoder, following a construction by Jang et al. and \u00c1g\u00fastsson et al. Gradients for categorical distributions and vector quantization are produced using ANN and arg max functions, with useful gradients obtained by substituting arg max with a softmax function. Integer networks can be combined with image compression models using a modified Gaussian prior with scale parameters, allowing for platform-independent evaluation of a softmax function. The latent representation in these models can vary based on a Lagrange parameter, with the number of latent states being a hyperparameter in categorical/VQ cases but extendable in other approaches. The latter two approaches organize latent states along the real line, enabling flexibility in training. The latent states are organized along the real line. For categorical distributions and VQ, the dimensionality of the function computing parameters grows linearly with the number of latent states. BID23 and another model use deterministic encoder distributions during evaluation but switch to probabilistic versions for training. BID23 proposes an encoder distribution using a uniform distribution and an ANN. They replace the quantizer gradient with the identity and use a shift-invariant distribution during training. During evaluation, the representation is determined as y = Q(g(x) \u2212 o), where o is an offset chosen to center the distribution on a quantization bin. Training with this construction leads to instabilities, as the prior distribution fails to converge to stable parameters due to the mismatch between the piecewise constant marginal m(y) and the smooth prior p. Regularization is necessary to ensure useful gradients for the prior. During training, using a variational approximation of the marginal without regularization can lead to instabilities. To address this, a trick is proposed where the last layer of the model is defined without a nonlinearity and with floating point division. This allows for compression on CPU and decompression on GPU. The model ensures correct decompression across different platforms by implementing integer networks during training. This approach addresses decompression failure rates due to floating point round-off errors when compressing on a GPU or decompressing on a different platform. During evaluation, the representation is computed deterministically, while training of integer models takes longer and is noisier compared to floating point models. Integer networks may take longer to converge due to their larger number of filters. When comparing integer networks with floating point models, training time to convergence is similar but performance is worse. The modified model, replacing the network computing the prior with an integer network, shows identical rate-distortion performance but is more robust to cross-platform compression and decompression. Testing on four different platforms and datasets confirms this improvement. The modified model, using integer networks, improves cross-platform compression and decompression compared to the original model. It achieves a 0% failure rate in all cases. However, using integer networks may lead to decreased rate-distortion performance compared to floating point networks. Increasing the number of filters per layer can compensate for this loss in approximation capacity but may require longer training time for convergence. Recent research has focused on quantization of artificial neural networks (ANNs) for image recognition applications. Various methods, such as lower precision multiplication and bilevel quantization, have been used to reduce computation in classification networks. Some approaches involve quantization during both training and inference to minimize computation on gradients and activations. Non-uniform quantization has also been explored to replace floating point computation with integer offsets. Models used for image compression are more sensitive to capacity constraints compared to classification networks. Reductions in network size and expressive power of activation functions have a significant impact on these models. Post-hoc quantization of network activations has not yielded competitive results for this type of model. Small floating point inconsistencies in variational latent-variable models can have disastrous effects on data compression when using range coding. Unfortunately, other entropy coding algorithms that do not suffer from sensitivity to perturbations in the probability model would always produce suboptimal results due to the source coding theorem. This theorem establishes a lower bound on the average length of resulting bit sequences, which range coding achieves asymptotically. The lower bound is determined by the cross entropy between the marginal and the prior. If an entropy coding algorithm tolerates errors in the probability values, it must assume identical probability values for a range. Our approach to neural network quantization involves discretizing probability values to minimize cross entropy, which imposes a new lower bound on error tolerance. It is challenging to establish tolerance intervals for probability values computed with floating point arithmetic, especially with ANNs, leading to difficulty in providing guarantees that a given tolerance will not be exceeded. Commercial compression methods model probabilities exclusively in the discrete domain to mitigate these challenges. Our approach to neural network quantization addresses non-deterministic computation, enabling various variational model architectures for platform-independent data compression. While not assessing its impact on computational complexity, potential complexity reductions may be achieved with this approach in the future."
}