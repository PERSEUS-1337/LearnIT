{
    "title": "HktK4BeCZ",
    "content": "The problem of representing collective behavior and predicting population distribution evolution is addressed using discrete time mean field game (MFG) models, which combine game theory and Markov decision processes (MDP). By showing that a special MFG can be reduced to an MDP, the scope of mean field game theory is broadened, allowing for the inference of MFG models for real-world systems through deep inverse reinforcement learning. This method learns reward functions and forward dynamics from real data, with the first empirical test conducted on a real-world social media population. Major global events like the Arab Spring, Black Lives Matter movement, and fake news controversy in the 2016 U.S. election drive the need for new models to understand population behavior influenced by social media. Just as physical systems follow the principle of least action, population behavior is shaped by individual actions that may be optimal based on certain objectives. The rise of social media in modern societies supports this idea, as people can plan and act based on global population trends. The behavior of a population is influenced by social media trends, with individuals observing global states and choosing topics to participate in. This feedback loop can be seen in various systems and motivates the need for models that can be learned from real data. These models can be applied to different scenarios such as optimizing resource production, navigating opinion networks, and understanding transitions between states. The model detailed in Section 3, known as MFG, addresses the modeling criteria in the problem context by representing agents as a distribution. It overcomes limitations of alternative predictive methods by simulating real-world phenomena like the Mexican wave in stadiums. Time series analysis lacks the ability to represent motivation that drives behavior policy, while methods using population network structure may struggle to explain events due to implicit optimization. The MFG model offers a unique way to describe natural system behavior without imposing control, connecting with MDP and RL frameworks. It provides a computationally tractable framework for adapting IRL methods with neural networks to learn complex reward functions for large populations. The MFG model offers a data-driven method for solving real-world temporal prediction problems, bridging the gap between theory and empirical validation. It allows for learning complex reward functions for large populations and demonstrates a general approach to MFG inference through a synthesis of MFG and MDP. Our approach combines Mean Field Games (MFG) with Markov Decision Processes (MDP) to validate an MFG model of a population's activity distribution on social media, achieving better predictive performance. This synthesis has the potential to open new research directions in both fields. The MFG framework is used for modeling population distribution and making assumptions on the cost function to attain analytic solutions. We propose methods to learn the MFG reward function from data and focus on algorithms for real-world solutions. Prior work has analyzed solution properties, but we aim to provide empirical verification in our application case. Our work focuses on adapting reinforcement learning algorithms within the MFG framework to address real-world problems with unknown reward functions. Unlike previous methods that rely on Gaussian density assumptions and restrictions on agent actions, our approach overcomes computational challenges and scalability issues, enabling solutions for large real-world domains. In our work, mean field game theory is used to model systems with thousands of interacting agents for RL applications. Methods in unknown MDP estimation and inverse reinforcement learning aim to learn optimal policies while estimating unknown MDP quantities. Maximum entropy IRL framework successfully learns reward functions from expert demonstrations. The MFG model combines probabilistic frameworks with deep neural networks to learn complex reward functions from demonstrations. It extends the sample-based IRL algorithm to learn a reward function for optimal behavior in a large population. The model uses a neural network to efficiently process MFG states and actions, focusing on discretetime MFG over a complete graph in this paper. The text discusses the dynamics of a discrete time graph state Mean Field Game (MFG) model, which is derived from a continuous time MFG. It involves right stochastic matrices, value functions, reward functions, and equations like the Hamilton-Jacobi-Bellman equation and the Fokker-Planck equation. The discrete time graph state MFG model involves equations like the Hamilton-Jacobi-Bellman equation and the Fokker-Planck equation. The reward function depends on the population distribution and player choices. The model can be applied to general directed graphs, with a focus on the complete graph in this paper. The example of user activity distribution on social media is used to illustrate the model. The MFG approach is generally applicable to problems where population size vastly outnumbers discrete states. It involves a population distribution over topics on social media, with transition matrices determining the probability of people switching topics. The model can be applied to general directed graphs, with a focus on user activity distribution on social media as an example. The reward function is learned from data, with a locality assumption that the reward for a topic depends only on the action taken in that topic. The expected maximum total reward for a topic at a given time is calculated, with a terminal value set to zero. Average reward received by agents at a topic is determined based on the current distribution and chosen action. Agents aim to act optimally to maximize their expected total average reward. A right stochastic matrix P is a Nash maximizer if it satisfies certain conditions. The value function of each topic must meet optimality criteria, and a solution of the MFG involves a sequence of pairs {(\u03c0 n , V n )} n=0,...,N. A Markov decision process is used to optimize problems, focusing on discrete time MFG. The MDP is reduced to a finite-horizon deterministic MDP, with states representing population distribution and actions on a complete graph. This connection allows for efficient inverse RL methods to learn an MFG model and its reward function. The discrete time MFG is defined by states representing population distribution and transition probability matrices. The value function of the MDP in Definition 2 satisfies the Bellman optimality equation, leading to an optimal policy trajectory. The MFG framework connects with MDP by using inverse RL algorithms to solve for unknown rewards. Unlike previous research, a new approach learns reward functions from real population behavior data, grounding the MFG representation in reality. The MFG framework connects with MDP using inverse RL algorithms to learn reward functions from real population behavior data, grounding the representation in reality. Leveraging MFG forward dynamics, a sample-based IRL method minimizes relative entropy between probability distributions over trajectories, without assuming the true distribution of optimal demonstrations. The text discusses the use of maximum entropy distribution to sample trajectories with higher rewards, estimating the partition function using multiple importance sampling. It also mentions sampling action matrices from stochastic policies and calculating the negative log likelihood of demonstration trajectories. The approach builds on Guided Cost Learning to train a deep neural network for reward function approximation. The text introduces a novel approach using a combination of convolutional neural nets and fully-connected layers to efficiently process action matrices and state vectors in a single architecture. By setting importance weights to unity for numerical stability, successful learning of a reward representation is achieved. The forward MDP solver performs gradient ascent on the policy's expected start value to find better policies informed by domain knowledge about the human population. The joint distribution F (P ; \u03c0, \u03b8) is constructed based on domain knowledge of human population behavior on social media. It involves d-dimensional Dirichlet distributions parameterized by \u03b1 i j, which is determined by the population density difference. A scaling factor c can be applied for variance reduction. The parameterized policy \u03b8) ) is used for sampling P n based on \u03c0 n, and its logarithmic gradient can be utilized in a policy gradient algorithm. An approximate value function V (\u03c0; w) is learned as a baseline. The effectiveness of our method is demonstrated through experiments involving the inference of a reward function and prediction of population trajectory over time. Using data from a Twitter population, we model the evolution of distribution over topics and time steps. The demonstration trajectory consists of state-action pairs measured each day. The MFG framework can model populations of large size and extract informative rewards and policies. Performance is evaluated using Jenson-Shanon Divergence metric. Four sets of state-action pairs are used for evaluation. The study analyzed reward distributions for train and test trajectories, showing generalizability of the learned reward function. The reward landscape was further explored, revealing that states with high initial popularity paired with actions favoring transition to topics with higher density achieved the highest rewards. Uniformly distributed state vectors attained the lowest rewards, while states with a small negative mass gradient attained medium rewards. The study analyzed reward distributions for train and test trajectories, showing generalizability of the learned reward function. States with high initial popularity paired with actions favoring transition to topics with higher density achieved the highest rewards. States with a small negative mass gradient from topic 1 to topic d attained medium rewards. The MFG model had 58% smaller error than VAR when evaluated on the JSD between generated trajectories. The MFG model outperformed VAR and RNN in predicting population dynamics by accurately representing population trends and transitions between topics. MFG required only initial population distributions for training, while VAR and RNN used distributions over all hours of each day. The structured approximation of MFG led to better performance with fewer training samples compared to VAR and RNN. The real-world dataset size and RNN's focus on state transitions without considering actions may have contributed to RNN's lower performance. Our method utilizes deep policy and value networks to address bias in policy parameterization, demonstrating a data-driven approach to solving a mean field game model of population evolution. The MFG framework is scalable to large populations and represents population density, leading to powerful learning capabilities. Experiments on real data show the effectiveness of MFG in learning rewards. Our experiments demonstrate that the MFG framework is effective for learning rewards and policies to predict real-world population trajectories accurately. The method outperformed alternatives even with a simple policy parameterization, encouraging exploration of neural networks for more complex applications. Future work includes developing efficient methods for solving discrete time MFG in a broader setting and integrating MFG with social network models for a more comprehensive understanding of social dynamics. A mean field game is defined by a Hamilton-Jacobi-Bellman (HJB) equation evolving backwards in time and a Fokker-Planck equation evolving forward in time. The reward function is often presented as a cost function in the MFG context. The HJB equation in mean field games is an analogue to the backward equation. The discrete time HJB equation in mean field games can be obtained by discretizing the equation using a semi-implicit scheme. The forward evolving Fokker-Planck equation for the continuous-time graph-state MFG is given by a specific formula involving partial derivatives. An Euler discretization of the equation with unit time step can be written in a specific form. The paper focuses on solving a mean field game using a reward function and policy learned through GCL BID9 and actor-critic algorithms. Sample trajectories are generated, and expert demonstrations are used to refine the reward function and policy. The reward network utilizes two components for the process. The reward network utilizes two convolutional layers to process the action matrix P, flattened and concatenated with the state vector \u03c0, and processed by fully-connected layers with regularization and dropout. The network initializes layers using the Xavier normal initializer in Tensorflow. Twitter users in a specific geographical region mainly see trending topics relevant to that area. An experiment focused on Atlanta, Georgia, with 406 active users forming the fixed population. Data was collected for 27 days, recording top trending topics and user responses hourly. The data collection process involved recording user responses to trending topics on Twitter in Atlanta, Georgia. The hourly count of people not responding to any topic was noted as a \"null topic\". Trajectories were created with hourly measurements of population distribution over 15 topics and their transition matrix over 16 hours. The training set consisted of trajectories over the first 21 days. The study involved using different methods for training trajectories over multiple days. MFG used initial distribution \u03c0 0 and policy F (P ; \u03c0 n , \u03b8) for training, while VAR and RNN used all measured distributions. RNN used a simple recurrent unit with ReLU activation, while VAR was implemented using Statsmodels in Python. Prediction accuracy was evaluated on 6 test days. The entropy of the probability distribution was calculated, and the continuous case involved deriving the probability density. The study involved training trajectories over multiple days using different methods. The entropy of the probability distribution was calculated, and the continuous case involved deriving the probability density using multiple importance sampling to estimate the partition function in the maximum entropy IRL framework. The study discussed training trajectories over multiple days using various methods and calculating the entropy of probability distributions. It also involved deriving probability density using multiple importance sampling to estimate the partition function in the maximum entropy IRL framework. The general MFG is not reducible to single-agent MDPs or equivalent to a multi-agent MDP due to the reward function's dependence on the full Nash maximizer matrix. Each topic in the discrete space MFG is considered a separate entity associated with a value. Each topic in the discrete space MFG is treated as a separate entity with a value. Assigning a value to each topic involves defining each tuple as a state, leading to the issue of state specification and transition dependencies. The value function resembles the Bellman optimality equation for a single-agent stochastic MDP, with transition probabilities dependent on the current topic. The selected action induces a stochastic transition to a new state. The action q selected in state i induces a stochastic transition to topic j, with the next distribution \u03c0 n+1 determined by the Nash maximizer matrix P n. There is a formal difference between P(s|s,a)V*(s) and q j V(j,(P n)T\u03c0 n). Modeling every agent in the MFG is impractical, making an exact reduction to a multi-agent MDP challenging. A discrete state space multi-agent MDP involves d agents moving within environment states S and action spaces A 1,...,A d. The probability function P(s|s,a) models environment transitions based on agent actions and reward functions. In a Markov game, each topic is treated as an \"agent\" with fixed labels, accumulating rewards independently. The value function for an agent is defined solely based on itself, not on other agents' value functions. The MFG equation for V n i depends on V n+1 j of all topics j, preventing a reduction to a standard Markov game due to mixing between value functions."
}