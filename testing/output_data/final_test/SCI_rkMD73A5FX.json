{
    "title": "rkMD73A5FX",
    "content": "Mah\u00e9 is a novel approach that provides Model-Agnostic Hierarchical Explanations for how powerful machine learning models capture complex dependencies like double negation in sentences and scene interactions in images. It offers context-dependent explanations through a local interpretation algorithm and context-free explanations by generalizing interactions. Experimental results show that Mah\u00e9 outperforms state-of-the-art methods in interpreting local interactions and explaining context-free interactions. Machine learning models, like deep neural networks, excel at modeling complex dependencies in structured data such as text, images, and DNA sequences. This paper aims to explain the type of dependencies captured in black-box models by interpreting context-dependent and context-free representations. Context-dependent representations depend on specific data instances, while context-free representations behave similarly independent of context. In the study of interpretable machine learning, context-free representations exhibit global behaviors independent of specific instances, while context-dependent representations focus on local dependencies within individual data instances. The research explores the interaction relationship between predictions and input features to understand complex dependencies in machine learning models. Mah\u00e9 is a framework for explaining context-dependent and context-free structures of complex prediction models, focusing on neural networks. It provides hierarchical explanations based on local interpretations, showing group-variable relationships in predictions. Mah\u00e9 is a framework for explaining context-dependent and context-free structures of complex prediction models, focusing on neural networks. It provides hierarchical explanations based on local interpretations, showing group-variable relationships in predictions. Explanation 0.6 none (linear LIME) this movie is not bad 0.8 { 4 , 5 } not bad 0.9 { 2 , 4 , 5 } movie not bad. Mah\u00e9 fits a neural network to perturbed samples to learn the decision boundary for classification, indicating interactions between variables. Attribution scores for these interactions can be shown for data instances. Mah\u00e9 provides context-dependent explanations for complex prediction models, determining if interactions are context-free. It outperforms other methods in interpreting local interactions and can successfully find context-free explanations. The framework can also edit models based on promising cases identified during experiments. Mah\u00e9 achieves improved context-dependent explanations and model-agnostic generality for local interaction interpretation in deep learning models. It is the first to provide context-free explanations of interactions and offers a direction for modifying context-free interactions without significant performance degradation. Feature attribution in interpretation includes additive and sensitivity approaches, which analyze how features contribute to model output and how sensitive the output is to changes in features. Sensitivity attribution analyzes model output sensitivity to feature changes. Additive attribution techniques like LIME and CD are examples. Methods like Integrated Gradients and DeepLIFT are used for sensitivity attribution. Mah\u00e9 offers additive attribution interpretations with non-additive variable interactions. Interpreting non-additive interactions in machine learning models has been challenging due to their lack of exact functional identity. Methods like NID, BID42, Additive Groves, CD, Tree-Shap, and GLMs with multiplicative interactions are used to interpret specific interactions, particularly multiplicative ones. LIME is a popular model interpretation method that provides additive attribution interpretations. Variants of LIME, such as Anchors and LIME-SUP, have been proposed to explain model predictions. Unlike previous methods, our approach focuses on providing local interpretations of non-additive interactions. Anchors and LIME-SUP are model interpretation methods that provide explanations for model predictions. Anchors generates context-free explanations but does not consider interactions, while LIME-SUP touches on interactions without studying their interpretation. The focus is on providing local interpretations of non-additive interactions, using a target function f and a local approximation \u03c6 that is interpretable. LIME uses a linear approximation to generate samples in a local vicinity of data points for accurate attribution scores. For complex models, a generalized additive model (GAM) can be used to achieve a closer fit to the nonlinear functional surface. A generalized additive model (GAM) called BID5 provides attribution scores for each feature. Nonadditive attributions and interactions between variables (BID17) can offer a better fit to the complex local vicinity. Using Multilayer Perceptrons (MLPs), g i and g i are learned for generating attribution scores. Adding non-additive interactions increases the representational power. The Mah\u00e9 framework introduces context-dependent and context-free explanations of interactions by identifying local interactions and learning non-additive models to provide local interaction scores. This approach enhances the representational capacity by detecting interactions and building models for them, focusing on using non-additive models for local interaction attribution. The Mah\u00e9 framework introduces context-dependent and context-free explanations of interactions by identifying local interactions and learning non-additive models to provide local interaction scores. This approach enhances the representational capacity by detecting interactions and building models for them. The method involves sampling points in the neighborhood of a data instance x with a maximum distance under a distance metric d, where features are converted to one-hot binary representation for categorical data. The framework introduces context-dependent explanations by flipping random features and weighting distances with a Gaussian kernel. The choice of \u03c3 depends on stability and interaction orders, which can be tuned for the task at hand. The framework is flexible to any interaction detection method, such as the neural interaction detection (NID) framework for interpreting neural network weights to obtain interactions. The NID framework BID42 accurately ranks any-order non-additive interactions in neural networks by interpreting learned weights. It is a polynomial-time algorithm that outperforms methods requiring exponential model training. NID detects interactions by tracing high-strength 1-regularized weights from features to hidden units in MLPs. The NID framework accurately ranks non-additive interactions in neural networks by interpreting learned weights efficiently. It outperforms methods requiring exponential model training by detecting interactions through high-strength 1-regularized weights in MLPs. NID has advantages such as universal approximation capabilities of MLPs and independence of features in sampled points. However, a disadvantage is the curse of dimensionality for MLPs. The curse of dimensionality for MLPs is a disadvantage in the application of NID, especially when p is large. To reduce overfitting, large input dimensions should be minimized, often done in image analysis by using superpixels as features. After obtaining an interaction ranking from NID, GAMs with interactions can be learned for top-K interactions. The Mah\u00e9 framework includes different levels of hierarchical explanations, with the last level having K interactions. The hierarchy shows additive attributions of individual features in the first level. The additive attributions of individual features are extracted by a trained \u03c6(\u00b7) in Eqs. 1 or 2, like linear LIME explanations. Parameters w of \u03c6(\u00b7; w, b) are frozen before constructing \u03c6 K (\u00b7) in Eq. 3 with interaction models. Interaction attribution of g K (\u00b7) is presented in the hierarchy levels. Sampling points for a mixture of continuous and one-hot categorical variables involves adapting the approach for binary features. Training interaction models g i on the residual of \u03c6 prevents degeneracy. The practice of training interaction models on the residual of \u03c6 prevents degeneracy of univariate functions in \u03c6. The fit of each \u03c6 K can be explained via predictive performance, and the stopping criteria for hierarchical levels can depend on this. To provide context-free explanations, ideal conditions for generalizing local explanations are defined. Definition 1 (Generalizing Local Explanations) explains that a local explanation of a model output f at a data point x is true for f(x) and depends on samples in the local vicinity of x. The explanation is considered global if it holds true for all data samples, including those outside the local vicinity, with the possibility of local modifications to the model. The local explanation of a model output depends on samples in the local vicinity of a data point x. It is considered global if it holds true for all data samples, including those outside the local vicinity, with the possibility of local modifications to the model. The context-free explanation of interaction I suggests that the attribution will generally have the same polarity when a local interaction exists. This work focuses on providing evidence of context-free interactions by checking if the explanation is consistent with specific conditions for the interaction of interest I. The text discusses checking the consistency of explanations for a specific interaction of interest I by modifying the model's output at a data instance x using a trained model g k (x I ). This helps determine if consistent explanations across data instances are more than just coincidence, especially when limited data instances are available for testing. In this paper, the authors modify interactions by negating them using a specified magnitude c. This modification affects interactions outside the local vicinity, supporting the manifold hypothesis that similar data lie near a low-dimensional manifold in a high-dimensional space. Studies have shown that this hypothesis applies to data representations learned by neural networks and is frequently used to visualize how deep networks represent data clusters. In this paper, the authors generalize explanations on how deep networks represent data clusters and interactions for language tasks using a distance metric. Local interactions for language tasks are defined by words and their positional order, allowing comparisons using edit distance. The results were not very sensitive to the defined distance metrics for each domain. The study evaluates Mah\u00e9's effectiveness on synthetic and real-world datasets, comparing it to other local interaction modeling baselines. Synthetic datasets have 10 features, while real-world datasets involve state-of-the-art models like DNA-CNN. State-of-the-art models include DNA-CNN, Sentiment-LSTM, ResNet152, and Transformer. Hyperparameters used in experiments involve n = 1k local-vicinity samples for synthetic experiments and n = 5k samples for real-world datasets. Distance metrics vary for each model. The experiments involve using different distance metrics for DNA-CNN, ResNet152, Sentiment-LSTM, and Transformer models. Superpixel and word approaches are used for explaining ResNet152 and Sentiment-LSTM respectively. Hyperparameters for the neural networks include MLPs with specific hidden layer sizes and regularization. In experiments with neural networks like DNA-CNN and Transformer models, interaction models in the GAM have architectures of 30-10. They are trained with regularization and learning rate values. Linear approximation is made for univariate functions to compare Mah\u00e9 and linear LIME methods. Neural networks train with early stopping, and Level L + 1 is determined based on validation performance. Evaluation of Mah\u00e9's explanations is compared to other methods using synthetic data. The study evaluates interaction interpretation methods on synthetic data using functions F1-F4 with continuous features uniformly distributed between -1 to 1. Complex base models are trained on this data, and different local interaction interpretation methods are run on 10 trials of 20 data instances. The stability of each interpretation method is evaluated by training base models with different random initializations between trials. The Mean Squared Error (MSE) is computed to assess how well each method fits to interactions. The Mah\u00e9 framework outperforms other methods in detecting and fitting interactions, being the only model-agnostic approach. It shows superior performance in evaluating context-dependent explanations on real-world data. The Mah\u00e9 framework demonstrates superior performance in evaluating context-dependent explanations on real-world data by assessing prediction performance as interactions are added and comparing Mah\u00e9 explanations with LIME explanations. We compare LIME and Mah\u00e9 explanations by presenting them to 60 Amazon Mechanical Turk users for evaluation. Mah\u00e9 is adjusted to show only one interaction and merge its attribution with subsumed features' attributions to make the difference subtle. Evaluators are shown explanations for 40 sentences with detected interactions, and their preferences are recorded through majority voting. Each evaluator can choose between explanations for up to 4 sentences. Additional conditions for sentence selection and examples are provided in Appendix B. The majority of preferred explanations (65%, p = 0.029) include interactions, supporting their inclusion in hierarchical explanations. Context-dependent hierarchical explanations for ResNet152, Sentiment-LSTM, and Transformer are provided in FIG6, Table 6, and Appendix E respectively. Superpixels in image explanations often interact to support predictions, with some exceptions like water not being detected as important in the prediction of water buffalo due to various reasons such as mix of training images. In this section, examples of context-free interactions found by Mah\u00e9 are shown. The study focuses on interactions learned by Sentiment-LSTM using data from IMDB movie reviews. The polarities of certain local interactions are consistently the same, indicating a global behavior that is not coincidental. Modifications to local interaction behavior in Sentiment-LSTM are made to observe changes in this global behavior. The Sentiment-LSTM model shows consistent global behavior in local interactions, with modifications made by Mah\u00e9 to rectify misrepresentations. The changes only slightly impact test accuracy, maintaining the original learned representation. Interactions in the model are detected with specific distances and word separations. The study also explores identifying context-free interactions in Transformers for English-to-French translations. The study explores interactions in English-to-French translations, focusing on the use of the French word \"cet\" for local interaction extraction. The presence of \"cet\" is used as a binary prediction variable, with positive polarities towards it. Modifications to the Transformer model result in negative polarities, similar to Sentiment-LSTM interactions. The study examines interactions in English-to-French translations, specifically focusing on the word \"cet\" and its impact on the Transformer model. Modifications to the model result in negative polarities, similar to Sentiment-LSTM interactions. Results from experiments on DNA-CNN and ResNet152 also show similar trends in interaction positions. Mah\u00e9 is a model-agnostic framework that provides context-dependent and context-free explanations of local interactions. It outperforms existing approaches and shows that local interactions can be context-free. The study examines interactions in English-to-French translations, focusing on the word \"cet\" and its impact on the Transformer model. Results from experiments on DNA-CNN and ResNet152 show similar trends in interaction positions. In future work, the goal is to make finding context-free interactions more efficient and explore how model behavior can be altered by editing interactions or univariate effects. Additionally, there is a desire to closely examine the interpretations provided by Mah\u00e9 for new insights into structured data. Examples of context-dependent hierarchical explanations on Sentiment-LSTM are illustrated in Table 6, showcasing interaction attributions in different colors. Various conditions were considered when selecting sentences for Mechanical Turk evaluators, ensuring a significant attribution difference between LIME and other methods. The text discusses the selection process for sentences presented to Mechanical Turk evaluators, ensuring a significant attribution difference between LIME and Mah\u00e9. The interface used by workers and randomly selected examples for analysis are also mentioned. The visualization tool for explanations is provided by the official code repository of LIME. The text discusses the runtime comparison between linear LIME and Mah\u00e9 for context-dependent explanations. It includes the average runtime for different tasks such as local inference, NID interaction detection, training linear and interaction models, and determining interaction consistency. The runtime comparison between linear LIME and Mah\u00e9 for context-dependent explanations includes tests on 80 contexts. DNA-CNN takes longer due to relaxed cutoff criteria. GLM and GA2M are used as baselines. Mah\u00e9 shows significant improvements over baselines. Examples of context-dependent hierarchical explanations on Transformer are provided in Table 7. Hierarchical explanations on Transformer show interaction attribution at each level, with green and red indicating contributions. Visualized attributions of linear LIME and Mah\u00e9 are normalized. Top-5 attributions by magnitude are shown for LIME."
}