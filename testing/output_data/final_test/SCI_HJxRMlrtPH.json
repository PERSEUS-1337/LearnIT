{
    "title": "HJxRMlrtPH",
    "content": "Generative networks are challenging to certify due to the need to capture non-convexity for precise bounds on output. A new verifier, ApproxLine, can certify properties of generative networks by performing abstract interpretation and verifying interpolations in latent space. This is crucial as neural networks are increasingly used in applications like facial recognition and autonomous driving. The current research focuses on certifying complex specifications beyond norm-bounded balls, including visible transformations. A method has been developed to efficiently solve verification problems for networks with inputs restricted to a line segment. This approach has been applied to certify non-norm-bounded properties of ACAS Xu networks and enhance Integrated Gradients. This work extends techniques to approximate EXACTLINE and provide guaranteed bounds on output probabilities using probabilistic abstract interpretation. It introduces a verification system, APPROXLINE, for capturing non-convexity and computing tight deterministic bounds on probabilities. This work introduces APPROXLINE, the first application of probabilistic abstract interpretation to neural networks. It demonstrates consistency of image attributes in autoencoders for CelebA and deterministic verification of non-convex specifications like classifier robustness to different features. The methodology of robustness verification is introduced, showcasing probability bounds for generative specifications over flipped images. The text introduces a verification technique for neural networks, focusing on deterministic robustness. It defines robustness specifications as input and output sets, with the goal of proving a network is robust for a specific specification. The problem is challenging due to its NP-hard nature, often requiring relaxation for practical verification. The text discusses the challenge of determining robustness in neural networks, often relaxed to allow false negatives. It explores probabilistic robustness bounds and the use of generative models for interpretable transformations between outputs. The text discusses the challenge of determining robustness in neural networks, often relaxed to allow false negatives. It explores probabilistic robustness bounds and the use of generative models for interpretable transformations between outputs. In analyzing neural networks restricted to line segments, Sotoudeh & Thakur (2019) compute succinct representations of piecewise-linear neural networks. This involves verifying generators using trusted attribute detectors or verifying attribute detectors based on trusted generators. The goal is to determine what fraction of the line segment between two outputs generates outputs with shared attributes. The text discusses computing polygonal chains to represent line segments in neural networks. It involves incrementally computing normalized distances on input segments for network outputs. Affine operations like matrix multiplication or convolution can be applied to each node. ReLU is computed per-dimension, introducing more segments if needed. This method extends EXACTLINE for analyzing neural networks on line segments. The text discusses extending EXACTLINE to perform exact probabilistic inference on decreasing dimension segments. To improve scaling, APPROXLINE is introduced, maintaining a list of polygonal chains and interval constraints for neural network activations. The text introduces a novel relaxation heuristic called APPROXLINE, which combines adjacent and short line segments in polygonal chains with interval constraints for probabilistic inference in neural networks. The probabilities associated with the segments remain unchanged when converted to intervals, and the Python pseudocode for propagation of APPROXLINE is provided in the appendix. The relaxation heuristic APPROXLINE combines line segments in a two-dimensional space to form interval constraints for probabilistic inference. The weights on the segments represent probabilities, and the approximation results in an interval constraint with a center and radius, as well as a polygonal chain. The text discusses computing lower and upper bounds on robustness probability using a polygonal chain with weighted segments. It contrasts with a dual approach by Dvijotham et al. (2018a) for verifying probabilistic properties over input sets. APPROXLINE provides tight bounds on the probability of misclassification with 100% confidence, outperforming PROVEN which only offers 99.99% confidence bounds. It is faster and more effective than sampling-based techniques like Clopper & Pearson (1934). Unlike smoothing methods, APPROXLINE offers deterministic guarantees rather than statistical robustness defenses. In our work, we assume a neural network can be decomposed into piecewise-linear layers. An abstract domain is a set of symbolic representations of program states. The (X, Y)-robustness property of a neural network N can be written using abstract transformers that overapproximate the effect of a function. Abstract interpretation provides a sound method to certify neural network robustness by overapproximating the effect of a function. It involves abstract domains like the box domain B and powerset domain P(A) n, where elements are sets of A n. The concretization function maps elements to their concrete values. Union domains combine abstract domains A and A , with elements represented as tuples. The concretization function for union domains combines the concretization of the individual domains. Probabilistic abstract interpretation replaces deterministic points with measures from a set of probability measures over R n. A probabilistic abstract domain represents sets of measures over program states. The concretization function maps abstract elements to the set of measures they represent. Measurable functions have corresponding probabilistic concrete transformers. Probabilistic abstract interpretation replaces deterministic points with measures from a set of probability measures over R n. A probabilistic abstract transformer abstracts the probabilistic concrete transformer by computing bounds on robustness probabilities. Any deterministic abstract domain can be directly interpreted as a probabilistic abstract domain. Convex combinations of probabilistic abstract domains can be formed to combine elements with probabilities in the range [0, 1]. The concretization function maps tuples in abstract domains to elements independently, preserving probabilities. The convex combination domain consists of tuples with elements transformed independently. APPROXLINE and its non-convex relaxations are defined for probabilistic inference, utilizing EXACTLINE to create an abstract domain E with polygonal chains in R n. The abstract domain consists of polygonal paths and boxes, representing neural network activations. The EXACTLINE primitive concatenates line segments in R n, eliminating duplicate points and applying a function f. The abstract element S(x1, x2) represents a single line segment connecting x1 and x2. The standard lifting of abstract transformers for the EXACTLINE and box domains propagates line segments using the EXACTLINE domain. Relaxation operators may apply before lifted abstract transformers, removing line segments and splitting polygonal chains. The relaxation operators in the EXACTLINE and box domains involve replacing line segments with bounding boxes and merging multiple boxes. By applying relaxation heuristics before each convolutional layer, a tradeoff between precision and speed is achieved. The EXACTLINE domain involves turning line segments into bounding boxes and merging them. Each polygonal chain is split into new chains and boxes. Symbolic representations of measures are stored for each line segment on the chain. Our probabilistic abstract domain combines the convex combination domains of the probabilistic EXACTLINE domain and the standard lifting of the box domain. It stores abstract elements with probabilistic polygonal chains and boxes, with concretization defined accordingly. Deterministic relaxations can be adapted for the probabilistic setting, utilizing bounding boxes for line segments. In a probabilistic setting, line segments are replaced by bounding boxes with weights in the weight vector \u03bb. When merging boxes, their weights are combined. Bounds are computed for the output distribution of a neural network to determine robustness probabilities. The probabilistic EXACTLINE domain tracks distribution with probability mass in determined locations and boxes. Bounds are calculated using deterministic box concretization. In a probabilistic setting, line segments are replaced by bounding boxes with weights in the weight vector \u03bb. When merging boxes, their weights are combined. Bounds are computed for the output distribution of a neural network to determine robustness probabilities. The probabilistic EXACTLINE domain tracks distribution with probability mass in determined locations and boxes. Bounds are calculated using deterministic box concretization. Here, the APPROXLINE analysis is implemented in the DiffAI framework, utilizing GPU parallelization with PyTorch. The method can compute exact results without approximation by setting the relaxation percentage to 0. The GPU implementation is named EXACT and is capable of exact probabilistic inference. The machine used for generative specifications has a GeForce GTX 1080 with 12 GB of GPU memory and four processors with a total of 64 GB of RAM. Decoders from autoencoders with 32 or 64 latent dimensions are trained using VAE and CycleAE methods to reconstruct CelebA images of size 64 \u00d7 64. The network architectures are detailed in Appendix B, with the decoder having 74128 neurons and the attribute detector having 24676 neurons. VAE is a variational autoencoder with latent dimensions, while CycleAE is a repurposed CycleGAN for unsupervised style transfer. The generator behaves like a GAN with encodings evenly distributed in the latent space. A normal distribution in l dimensions is used for the embedding space P, with a discriminator D P. The image distribution Q is evaluated using the BEGAN method, which assesses realism based on an autoencoder. An Attribute Detector is trained to recognize 40 attributes from CelebA, with a linear output and trained using Adam to minimize L1 loss. The attribute detector is trained using Adam to minimize L1 loss. It is trained for 300 epochs to verify the consistency of a generative model with respect to an attribute detector. Various verification goals can be pursued, such as checking the generative model's correctness with a trusted classifier or the classifier's robustness to interpretable interpolations between data points. In this section, we demonstrate that probabilistic APPROXLINE is precise and efficient in determining the probability of attribute consistency between two ground truth inputs with matching attributes. We compare APPROXLINE to other methods for providing probabilistic bounds, specifically for CycleAE 32 trained for 200 epochs. The attribute detector and decoder are omitted from the discussion if their context is clear. The text discusses comparing probabilistic bounds for average attribute consistency using different methods, including probabilistic APPROXLINE, EXACT, and HZono. Sampling with binomial confidence intervals is also compared. The goal is to find a precise and efficient method for determining attribute consistency between ground truth inputs. The text compares probabilistic bounds for average attribute consistency using different methods: APPROXLINE, EXACT, and HZono. Sampling with binomial confidence intervals is also evaluated. The results show that HZono is the fastest but unable to prove specifications, while Sampling and EXACT are not significantly slower than APPROXLINE but produce large probabilistic bounds. APPROXLINE provides narrow probabilistic bounds quickly, while EXACT and Sampling have wider bounds due to timeouts and memory issues. The text demonstrates using domain-specific checks for attribute consistency in generative models. Comparing models for turning heads, VAEs can generate intermediate poses from flipped images. The effectiveness of different autoencoding models in generating images of different orientations of a human face was compared using APPROXLINE. The lower bounds on correctness of the flipped images were computed quickly, with CycleAE 64 being the fastest at 30 seconds. The attribute detector should recognize the same attributes for all interpolations in the CelebA dataset. The attribute detector in APPROXLINE demonstrates robustness to flipping for most attributes, but not all. Using probabilistic APPROXLINE, the average lower bound on correct attribute appearance in output images is found. Attribute detection for one feature is shown to be invariant to transformations in independent features, such as adding a mustache to an image. In this study, the authors utilized the APPROXLINE method to verify the robustness of attributes in neural networks. They found that 30 out of 40 attributes remained robust when a mustache was added, including \"bald\" and \"young\". However, the attribute \"NoBeard\" was not entirely robust, with a robustness probability range of 0.83522 to 0.83528. The study presented a scalable non-convex relaxation approach that proved to be faster and more precise than previous methods for verifying neural network properties. The study presented a scalable non-convex relaxation approach that proved to be faster and more precise than previous methods for verifying neural network properties. The method is faster and more precise than previous methods for the same networks, including sampling, allowing verification of properties based on visual transformations induced by generative networks for the first time. The models use the same encoders, decoders, and attribute detectors. Latent Descriminator is a fully connected feed forward network with 5 hidden layers. Encoder is a standard convolutional neural network. Decoder is a transposed convolutional network. Attribute Detector has 24676 neurons. Refinement schemes start with the most precise approximation and progressively try less precise ones. Starting with the most precise approximation conserves time. The study explores the impact of modifying the approximation parameters p and N of APPROXLINE on speed and precision. The number of clusters (N) and the percentage of nodes permitted to be clustered (p) are varied to determine their effect. The results are visualized in Figure 5, showing how different parameter values influence the outcome. Additionally, the deterministic verification of the interpolative specification using APPROXLINE is demonstrated in Figure 6."
}