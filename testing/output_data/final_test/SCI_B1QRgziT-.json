{
    "title": "B1QRgziT-",
    "content": "The paper introduces spectral normalization as a technique to stabilize the training of the discriminator in generative adversarial networks. It is computationally light and easy to implement. Spectral normalization was tested on CIFAR10, STL-10, and ILSVRC2012 datasets, confirming its effectiveness in improving image quality in SN-GANs. GANs are a popular framework for generative models, with a generator mimicking a target distribution and a discriminator distinguishing between the two. The paper introduces spectral normalization as a technique to stabilize GAN training, being computationally light and easy to implement. GANs have gained attention for their ability to learn structured probability distributions and the training of the discriminator as an estimator for the density ratio between model and target distributions. Challenges include controlling the discriminator's performance and inaccurate density ratio estimation in high-dimensional spaces. In this paper, a novel weight normalization method called spectral normalization is proposed to stabilize the training of discriminator networks in GANs. The method addresses challenges such as discriminator failure to learn multimodal structures and the existence of discriminators that can perfectly distinguish model distribution from target distribution, leading to training halts for the generator. The normalization method requires tuning only the Lipschitz constant as a hyper-parameter, eliminating the need for intensive tuning. The spectral normalization method proposed in this study simplifies implementation and requires minimal hyper-parameter tuning. It outperforms other regularization techniques in improving image quality in GANs. The proposed method involves a neural network discriminator with learning parameters and activation functions. The final output is determined by an activation function corresponding to a distance measure. The standard formulation of GANs involves a min-max optimization over generator and discriminator functions. The activation function used in the discriminator plays a crucial role in the adversarial optimization process. Recent works in the machine learning community emphasize the importance of Lipschitz continuity in selecting discriminators for GANs. The optimal discriminator for a fixed generator in the standard GAN formulation is given by a specific function. Introducing a regularity condition to the derivative of the function is proposed to address potential unboundedness or incomputability issues. Successful methods have been suggested to control the Lipschitz property in discriminator selection. In this array, works by Qi (2017) and BID12 propose methods to control the Lipschitz constant of the discriminator through regularization terms defined on input examples. The paper introduces spectral normalization as a method to normalize weight matrices and control the Lipschitz constant. Spectral normalization controls the Lipschitz constant of the discriminator function by constraining the spectral norm of each layer's weight matrix. This normalization ensures that the Lipschitz constraint is satisfied, bounding the Lipschitz norm of the discriminator function. Spectral normalization controls the Lipschitz constant of the discriminator function by constraining the spectral norm of each layer's weight matrix. Unlike spectral norm \"regularization\" which adds an explicit regularization term, our method sets the spectral norm to a designated value. Our approach augments the cost function with a sample data dependent regularization function, while spectral norm regularization imposes sample data independent regularization. The spectral norm \u03c3(W) is used to regularize each layer of the discriminator by controlling the Lipschitz constant. The power iteration method can be used to estimate \u03c3(W) efficiently, reducing computational cost. The gradient of W SN(W) with respect to Wij is calculated using the first left and right singular vectors of W. Spectral normalization is applied to each layer of the discriminator to control the Lipschitz constant. The derivative of the discriminator's weights with respect to the spectral normalization is calculated using the first left and right singular vectors of the weights. The regularization term penalizes the first singular components with an adaptive coefficient to prevent the transformation of each layer from becoming too sensitive in one direction. Spectral normalization also allows for a new parametrization of the model. Spectral normalization is used to create a new parametrization for the model, splitting the layer map into two components: a spectrally normalized map and a spectral norm constant. This parametrization improves GAN performance. Weight normalization normalizes the 2 norm of each row vector in the weight matrix, equivalent to Frobenius normalization. However, these normalizations unintentionally impose stronger constraints on the matrix than intended. Weight normalization conflicts with the desire to use as many features as possible for discrimination, as it reduces the rank and limits the number of features available. This can lead to arbitrary model distributions in GAN algorithms. Spectral normalization avoids conflicts with weight normalization by allowing the parameter matrix to use as many features as possible while satisfying local 1-Lipschitz constraint. It provides more freedom in choosing the number of singular components to feed to the next layer of the discriminator. Orthonormal regularization differs from spectral normalization as it sets all singular values to one, destroying spectrum information. BID12 used Gradient penalty method with WGAN, enforcing 1-Lipschitz constant on the discriminator. This approach avoids mentioned problems. Spectral normalization is more stable than WGAN-GP in training due to its regularization effect on the operator space, which is not easily destabilized by high learning rates. This method also requires less computational cost compared to WGAN-GP. Normalization with single-step power iteration is compared to another method in terms of computational cost for the same number of updates. Extensive experiments on unsupervised image generation were conducted on CIFAR-10, STL-10, and ILSVRC2012 datasets. The section discusses objective functions, optimization settings, and performance measures for evaluating the generated images. The text discusses the evaluation of images generated by trained generators using convolutional neural networks on CIFAR-10, STL-10, and ImageNet datasets. The spectral norm for convolutional weights was evaluated as a 2-D matrix. Parameters of the generator were trained with batch normalization, and a standard objective function for adversarial loss was used. Latent variable dz was set to 128 for all experiments. For the experiments, updates of G used alternate cost proposed by BID11 and BID38, while updates of D used original cost. The algorithm was tested with hinge loss for discriminator and generator, optimizing objectives to minimize reverse KL divergence. Hinge loss algorithm also performed well with inception score and FID. For WGAN-GP, objective function included a regularization term introduced in the appendix. Inception score and FID were used for quantitative assessment of generated examples. In this section, the accuracy of spectral normalization (SN-GAN) during training and the algorithm's performance dependence on optimizer hyperparameters are reported. Performance quality is compared against other regularization/normalization techniques for discriminator networks, including Weight clipping, WGAN-GP, batch-normalization (BN), layer normalization (LN), weight normalization (WN), and orthonormal regularization. The stand-alone efficacy of the gradient penalty is evaluated by applying it to the standard adversarial loss of GANs, referred to as 'GAN-GP'. For weight clipping in GAN-GP, the clipping constant c was set at 0.01. The gradient penalty had \u03bb set to 10. Orthonormal initialization was used for weights in D. The multiplier parameter \u03b3 was excluded in weight normalization, batch normalization, and layer normalization to maintain Lipschitz condition. Adam optimizer was used in all experiments with 6 settings for n dis. We tested 6 settings for n dis and learning rate \u03b1 in the generator updates. Settings A, B, and C were used in previous works, while D, E, and F had more aggressive learning rates. The architectures of the networks are detailed in Table 3. The spectral norm of each layer was monitored during training. In the experiments, hyper-parameter settings were tested for spectral normalization, WGAN-GP, and orthonormal regularization. Spectral normalization showed robustness with aggressive learning rates and momentum parameters. WGAN-GP struggled with high learning rates and momentum parameters. Orthonormal regularization had mixed results. The method used in the study was found to be more robust to changes in training settings. The optimal performance of weight normalization was inferior to both WGAN-GP and spectral normalization on STL-10 dataset. Spectral normalization showed better scores than other methods on CIFAR-10 and STL-10 datasets. SN-GANs outperformed other methods with optimal settings and hinge loss. SN-GANs fell behind orthonormal regularization for STL-10 with the same number of iterations. Images produced by generators trained with WGAN-GP, weight normalization, and spectral normalization showed SN-GANs consistently performed better than GANs. SN-GANs outperformed GANs with weight normalization in generating clearer and more diverse images. WGAN-GP struggled with high learning rates and momentums. The comparison against benchmark methods and testing on ResNet based GANs showed the superiority of SN-GANs. Refer to tables in the appendix for detailed network information. Our algorithm implementation outperformed predecessors in performance, as shown in Table 4 and 5 in the appendix. Singular values analysis on discriminator D's weight matrices revealed differences between weight clipping, weight normalization, and spectral normalization methods. Weight matrices trained with spectral normalization showed a more broadly distributed singular values pattern. In the context of spectral normalization, distributions on low-dimensional nonlinear data manifolds in high-dimensional spaces can be problematic due to rank deficiencies in lower layers. Outputs of lower layers undergo rectified linear transformations, leading to oversimplified discriminators. Comparing spectral normalization with weight normalization, spectral normalization generates more diverse and complex images. Training time for SN-GANs on CIFAR-10 is slightly slower than weight normalization but faster than WGAN-GP. WGAN-GP is slower due to the need to calculate the gradient of gradient norm. Spectral normalization addresses rank deficiencies in lower layers, leading to more diverse images. SN-GANs on CIFAR-10 have slightly slower training than weight normalization but faster than WGAN-GP, which requires calculating the gradient of gradient norm. Computational time for SN-GANs on STL-10 is comparable to vanilla GANs due to negligible cost of power iteration. Additional experiments highlight differences between spectral normalization and orthonormal regularization, which emphasizes all feature dimensions equally. The study focused on increasing the feature space dimension, particularly in the final layer, using spectral normalization. Results showed that orthonormal regularization's performance decreased with higher feature map dimensions, while SN-GANs remained robust. The method was also effective on a large high-dimensional dataset with 1000 classes. The study utilized GANs on the ILRSVRC2012 dataset with 1000 classes, compressed to 128 \u00d7 128 pixels. They replaced the standard GANs loss with hinge loss for conditional GANs. GANs without normalization failed, while GANs with orthonormal normalization and spectral normalization produced images. The orthonormal normalization plateaued in performance, while spectral normalization continued to improve. This research successfully generated decent images from the ImageNet dataset with a single pair of discriminator and generator. The study compared the performance of SN-GANs and AC-GANs in terms of mode-collapse, with SN-GANs showing less collapse. SN-GANs also outperformed orthonormal regularization in image generation tasks. Spectral normalization is proposed as a stabilizer for GAN training, leading to more diverse generated examples. The study compared SN-GANs and AC-GANs, with SN-GANs showing less mode-collapse and outperforming orthonormal regularization in image generation tasks. Spectral normalization is proposed as a stabilizer for GAN training, leading to more diverse generated examples. The method imposes global regularization on the discriminator and can be used in combinations. Future work includes further investigation of the method's performance and experimentation on larger datasets. The shortcut in Section 2.1 involves randomly initialized vectors for each weight, utilizing the power method to produce singular vectors. The text discusses the use of spectral normalization as a stabilizer for GAN training, leading to more diverse generated examples. The method involves approximating the spectral norm of weight matrices using singular vectors and reusing computed vectors to achieve satisfactory performance with minimal computational cost. The implementation of spectral normalization is compared to the computational time of forward and backward propagations in neural networks. The text introduces spectral normalization as a stabilizer for GAN training, improving diversity in generated examples. It involves approximating spectral norms of weight matrices using singular vectors for efficient performance. Inception score is used to evaluate the quality of generated images, showing a strong correlation with human judgment. The text discusses the Fr\u00e9chet inception distance as a measure of image quality in generated examples. It compares distributions using 2nd order information from the final layer of the inception model, calculating the distance between the true and generated distributions over multiple samples. The experiments compared the ResNet architecture of BID12 with standard CNN using Adam optimization. Doubling the feature map in the generator for SN-GANs improved results, but deteriorated performance for WGAN-GP. Images were resized to 128x128 pixels, and conditional batch normalization was used for the generator network of conditional GANs. Conditional batch normalization (CBN) was used in the generator network of conditional GANs, replacing the standard batch normalization layer. The networks were trained with Adam optimization, 450K generator updates, and linear decay for the learning rate. ResNet architectures were utilized for CIFAR-10, STL-10, and ImageNet datasets, with modifications such as setting lReLU function slopes to 0.1 and removing BN layers in the discriminator. In the conditional GANs model, the ResBlock architecture was modified by replacing the batch normalization layer with conditional batch normalization. The projection discriminator model used the same architecture as in BID23. Integer labels were embedded into a 128-dimensional vector for computational ease. Spectral normalization accuracy remained consistent throughout training, with minor deviations in certain convolutional layers. The comparative study explores the impact of spectral normalization and other regularization methods on discriminators. Weight normalization and weight clipping impose constraints on weight matrices, limiting the search space. Spectral normalization remains consistent in accuracy, with deviations in some convolutional layers. The weight normalization and weight clipping techniques favor low-rank weight matrices in the discriminator, leading to dependency on select few features. These regularization methods were initially aimed at improving generalization performance in supervised training but have found success as discriminators' regularizers in GANs. Weight normalization and weight clipping techniques aim to regularize discriminators in GANs by making the trained discriminator K-Lipschitz for a prescribed K. However, weight normalization imposes restrictions on the choice of weight matrices, leading to a conflict between retaining input norm and using as many features as possible for discrimination. Our spectral normalization method avoids conflicts between weight normalization and maximizing the number of features for discrimination in GANs. Unlike weight normalization, spectral normalization allows for more freedom in choosing the number of features while satisfying the Lipschitz constraint. Spectral normalization provides more flexibility in selecting the number of features for the discriminator compared to weight normalization. Weight normalization and Frobenius normalization tend to favor skewed distributions of singular values, limiting the choice of feature dimensions. Spectral normalization, on the other hand, does not compromise the number of feature dimensions used by the discriminator. In comparison to weight clipping and other regularization methods, spectral normalization allows for more feature dimensions in the discriminator, leading to a wider variety and higher inception score in synthetic dataset generation. Weight clipping, like weight normalization and Frobenius normalization, suffers from the capacity underuse problem, favoring discriminators with only a few select features. Spectral normalization is a more efficient method compared to weight clipping for controlling the spectral norm in training discriminators. It allows for updates with low computational cost while maintaining the normalization constraint. This approach is more effective in handling the capacity underuse problem and leads to higher inception scores in synthetic dataset generation. BID12 introduced a technique to enhance stability in training Wasserstein GANs by placing a K-Lipschitz constraint on the discriminator. They used a regularizer function with a balancing coefficient \u03bb and successfully trained a GAN based on ResNet BID13. Their method directly imposes a local 1-Lipschitz constraint on the discriminator function, avoiding round-about normalization. This approach addresses the capacity underuse problem but may struggle with penalizing gradients at sample points. Our spectral normalization method provides more stable regularization compared to penalizing gradients at sample points. It is not affected by changes in the generative distribution during training and can handle high learning rates without performance issues. Additionally, it is computationally more efficient than WGAN-GP due to requiring fewer computations. In comparing forward and backward propagation methods, the gradient penalty can complement spectral normalization. The combination of WGAN-GP and spectral normalization with reparametrization improves generated examples quality. A new algorithm can be developed by considering a different parametrization of the weight matrix of the discriminator. In this addendum, reparametrization methods using weight normalization and spectral normalization are compared experimentally for training the discriminator of WGAN-GP. The same CNN architecture is used for WGAN-GP, while a ResNet-based CNN architecture is used for spectral normalization. The experimental comparison of reparametrization methods using weight normalization and spectral normalization for training the discriminator of WGAN-GP showed significant improvements in the inception score. Spectral normalization demonstrated better performance in reducing overfitting compared to weight normalization, leading to a higher final score. Spectral normalization outperformed weight normalization in reducing overfitting during training, achieving an inception score of 7.28 compared to 7.04 and 6.69 for spectral normalization and vanilla normalization, respectively."
}