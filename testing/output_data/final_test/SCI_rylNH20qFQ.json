{
    "title": "rylNH20qFQ",
    "content": "In this paper, the authors propose 3D shape programs that integrate bottom-up recognition systems with top-down symbolic program structure to capture both low-level geometry and high-level structural priors for 3D shapes. Their end-to-end differentiable model learns 3D shape programs by reconstructing shapes in a self-supervised manner, demonstrating promising results in shape reconstruction. The model accurately infers and executes 3D shape programs for complex shapes, integrating with an image-to-shape module for more accurate reconstructions. Humans recognize shapes based on parts, regularities, and abstract concepts, crucial for visual recognition and reasoning. Recent AI systems have shown impressive progress in 3D shape understanding. Recent AI systems have made significant progress in 3D shape understanding, utilizing shape repositories like ShapeNet. Different shape representations such as voxels, point clouds, and meshes have been used, but struggle to capture strong shape priors like sharp edges and smooth surfaces. Some recent studies have explored modeling 3D shapes as a collection of primitives with simple operations. This paper proposes representing 3D shapes as shape programs to capture high-level regularities such as symmetry and repetition. 3D shapes are represented as shape programs using a domain-specific language with basic shape primitives and structural priors. Inverse procedural graphics involves inferring programs from voxels, related to inferring procedural graphics programs from images. Success is seen with large shape-component libraries or sparse solution spaces. Kulkarni et al. (2015a) tackled inverse graphics by inferring probabilistic programs for generating 2D images from 3D models. In this work, the idea of inverse procedural graphics is extended to 3-D voxel representations, applying to large datasets like ShapeNet. Instead of matching components to a library of shapes, a neural network is used to directly infer shapes and parameters. Recent papers have explored using geometric primitives to describe shapes in the context of deep learning, addressing the problem of inferring 3-D CAD programs from perceptual input. In 3-D program inference, the goal is to reconstruct programs with meaningful parts and spatial regularity. Other graphics papers explore regularity without using programs. Work in HCI addresses inferring parametric graphics primitives. Instead of learning procedural primitives individually, this work learns multiple simultaneously. The domain-specific language for 3D shapes varies based on object types. Program synthesis techniques are used to infer 2D shapes. In the AI literature, Ellis et al. (2018) used symbolic program synthesis to infer 2D graphics programs from images. They extended their work by using neural nets for faster inference of low-level cues like strokes. A purely end-to-end network can recover 3D graphics programs from voxels, relevant to RobustFill (Devlin et al., 2017) and the SPIRAL system (Ganin et al., 2018). SPIRAL explains images in terms of \"brush strokes\", while our work explains 3D voxels in terms of high-level objects and meaningful parts. Neural Program Interpreters (NPI) have been studied for executing programs. In the AI literature, Neural Program Interpreters (NPI) have been extensively studied for executing programs that abstract tasks like sorting and arithmetic. NPI decomposes program execution into primitive operations and predicts the next step based on the environment and previous actions. Various improvements have been made, including adding recursion and executing programs for visual question answering. This paper introduces a 3D shape program executor that renders shapes from programs, defining a domain-specific language for shape synthesis. Each shape primitive represents a meaningful part of an object. The 3D shape program executor renders shapes from programs, defining a domain-specific language for shape synthesis. Each shape primitive models a semantically-meaningful part of an object, with geometric and semantic attributes specifying its position, orientation, and role within the whole shape. For statements capture high-level regularity across parts, allowing recursive generation of shape programs. The core of the 3D shape program generator consists of two LSTMs: the Block LSTM emits features for each program block, while the Step LSTM outputs programs inside each block. The goal is to predict a 3D shape program that reconstructs the input shape when executed. The model, called Shape Programs, includes a program generator and a neural program executor, allowing for self-supervised learning. The program generator for 3D shapes uses two LSTMs: Block LSTM connects blocks, and Step LSTM generates programs. It decomposes programs into drawing and compound statements, inspired by human perception. The model aims to reconstruct input shapes through self-supervised learning. The program generator for 3D shapes uses two LSTMs: Block LSTM renders shapes and feeds features into a 3D ConvNet. The Step LSTM predicts program tokens based on block features, allowing for different block lengths. Two decoders generate output based on LSTM hidden states, producing softmax classification probabilities over program primitives. The program generator for 3D shapes utilizes a Step LSTM to predict program tokens based on block features, with input from a synthetic dataset of simple program templates. The program executor consists of an LSTM and a 3D DeconvNet for decoding features into a 3D shape. The text discusses the training of a neural program executor to generate shapes from programs, using cross-entropy loss for program ID classification and L-2 loss for argument regression. The executor is differentiable, allowing for gradient-based fine-tuning of the program generator. Our neural program executor uses a differentiable approach for fine-tuning on unannotated shapes, enabling effective generalization to novel shapes. By decomposing the problem into block-level execution and integrating shapes through max-pooling, the executor can handle long sequences of programs. The LSTM and deconv CNN architecture processes block-level programs to generate the desired shape. Training involves cross-entropy loss for program ID classification and L-2 loss for argument regression. CNN generates shapes using a program executor trained on synthetic data with weighted binary cross-entropy losses. The disentangled representation allows adaptation to real-world datasets like ShapeNet without program-level supervision. The neural program executor executes predicted programs for shape reconstruction. The program generator outputs block programs interpreted as 3D shapes by the neural program executor. Max-pooling is used to reconstruct the shape, enabling handling of programs of variable length. Gradients propagate through vacant tokens for guided adaptation. In the experiments, a single model is used to predict programs for multiple categories, pretrained on a synthetic dataset and adapted to target datasets like ShapeNet and Pix3D. The program generator is trained with simple templates on 100,000 chairs and tables, evaluated on 5,000 examples. The model components are trained with Adam, and gradients propagate through vacant tokens for guided adaptation. The program executor, trained on synthetic block programs and shapes, shows high accuracy in predicting shapes. The neural program executor approximates the graphics engine well, with an IoU of 0.93 for single drawing statements and 0.88 for compound statements. Guided adaptation is validated on unseen examples from ShapeNet, with quantitative results showing effectiveness in generating programs. After generating programs from input shapes, the model is evaluated using metrics like IoU, Chamfer distance, and Earth Mover's distance. Results show a disparity between synthetic and ShapeNet datasets, with significant improvement seen after guided adaptation. Comparison with other methods like BID19 and CSGNet reveals enhancements in IoU for specific objects. Our model without guided adaptation outperforms BID19 and CSGNet by capturing regularities such as symmetry and translation. The NN baseline suggests that memorizing the training set does not generalize well. Training the program generator directly on ShapeNet without pre-training failed due to the complex combinatorial space of programs. Simple initial programs like 10 table templates are sufficient for pre-training. The model achieves good performance under execution-guided adaptation, improving program generation and shape reconstruction for tables and chairs. It can add or delete programs to enhance accuracy, such as adding translations or removing unnecessary components. The model's adaptability is demonstrated through modifications to compound programs, leading to more accurate representations. The model demonstrates adaptability by improving program generation and shape reconstruction for tables and chairs through modifications to compound programs. New templates emerge after adaptation, indicating the generator's ability to map complex relationships. Stability and connectivity are crucial for real-world shapes, which are better captured using program representations. Comparisons with BID19 show significant improvements in the model's performance. In TAB7, the model is compared against BID19, showing significant improvements in shape stability. The model can represent multiple identical objects using translations and rotations. Pre-GA, chairs had lower connectivity, but significant improvements were seen with GA. The model demonstrates generalization ability and produces stable and connected shapes. Guided adaptation further improves results for Bed, Bench, Cabinet, and Sofa categories. The model shows significant improvements in shape stability compared to BID19 in TAB7. Pre-trained model performs poorly on unseen shapes but improves with guided adaptation, with IoU improvements for bed, sofa, cabinet, and bench. The framework's generalization ability is illustrated, with applications in completing fragmentary shapes from 2D images using MarrNet and generating smooth and complete shapes. Our model enhances shape completion by producing visually appealing results compared to MarrNet, with 78.9% preference in a user study. It introduces 3D shape programs as a new representation and infers shape programs using a neural program synthesizer and executor. Experiments on ShapeNet and Pix3D demonstrate the model's ability to explain shapes as programs and reconstruct 3D shapes from color images. Future work includes analyzing the neural program executor and exploring design choices. The neural program executor's intermediate representation is a 64-dimensional vector output by the LSTM, allowing for manipulation of individual dimensions to visualize generated voxels capturing geometric features. The design of the DSL for shape programs makes semantic commitments, supporting semantic correspondences across shapes but potentially limiting generalization to shapes outside training classes. The current instantiation focuses on furniture semantics, showing good generalization within this superclass. In future work, the model aims to learn shape primitives directly from data to adapt to new superclasses or domains. For program synthesis, neural nets are used for fast inference, taking 5 ms to infer a shape program. Various structured search approaches were considered but deemed too slow for the goals, with constraint solving being one option. The approach to structured search involves constraint solving using the Z3 SMT solver. Efficient integration of discrete search and amortized inference is a promising research direction. Semantics and shape primitives for furniture are detailed, with some semantics being category-specific while others are shared across different shape categories. The program generator model includes a 3D ConvNet with 8 convolutional layers, each with a kernel size of 3 except for the first layer with a kernel size of 5. The output channels are (8, 16, 16, 32, 32, 64, 64, 64) respectively, and the output of the last layer is averaged over. The program generator model utilizes a 3D ConvNet with 8 convolutional layers, outputting channels (8, 16, 16, 32, 32, 64, 64, 64) which are averaged over. The Block LSTM and Step LSTM have similar structures with input and hidden state dimensions of 64. The output is fed into a sigmoid function to generate a 3D voxel shape. End-to-end differentiability is achieved through the neural program executor design, enabling the whole pipeline to be differentiable. ShapeNet aims to be the ImageNet of shapes. Our synthetic dataset provides minimal guidance to the network for shape reconstruction. When directly testing on ShapeNet with a model pretrained on synthetic data, there was a significant drop in IoU due to the complexity disparity. Guided adaptation further enhances the pretrained model for better performance on ShapeNet data. In Figure A2, A3, and FIG2, improvements in program inference and shape reconstruction on ShapeNet are noted after guided adaptation from synthetic data. Mispredictions decrease significantly after guided adaptation, especially when using a model pretrained on synthetic chairs and tables for other classes. Reconstruction of ShapeNet beds is shown in FIG5."
}