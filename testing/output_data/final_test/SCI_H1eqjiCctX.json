{
    "title": "H1eqjiCctX",
    "content": "Word embedding is a powerful tool in natural language processing, used to map words to low-dimensional vectors. A generative model is proposed in this paper for word embedding composition, which captures specific syntactic relations between words. The correlations between three words form a tensor with a low rank Tucker decomposition, resulting in improved word embeddings and core tensor for better composition. Experimental results validate the effectiveness of this new method. Word embeddings map words to low-dimensional vectors, capturing semantic relationships. However, a challenge is how to combine embeddings of co-occurring words into a phrase. Simply adding embeddings may not capture the combined meaning accurately. In this paper, a model for word embeddings is proposed that incorporates syntactic information for better compositions of word pairs. The model, called syntactic RAND-WALK, can capture specific syntactic relations like adjective-noun or verb-object pairs. This approach builds upon previous work on word embeddings and composition models. The model proposed in this paper, called syntactic RAND-WALK, incorporates syntactic information for better compositions of word pairs, such as adjective-noun or verb-object pairs. It uses a core tensor to capture relations between words and their context, allowing for the definition of a matrix for the adjective that can act on the noun's embedding with fewer parameters to train. This model utilizes high order statistics and offers similar interpretations as previous models. The model proposed in this paper, called syntactic RAND-WALK, incorporates syntactic information for better compositions of word pairs. It suggests using high order co-occurrences of words and a tensor decomposition algorithm to learn embeddings. This approach allows for a natural way of learning the model and approaching the composition problem. The model proposed, called syntactic RAND-WALK, incorporates syntactic information for better compositions of word pairs using a tensor decomposition algorithm. It allows for a natural way of learning the model and approaching the composition problem. The model discusses the composition of words a, b with theoretical insights, using embeddings and a core tensor for correction. Experimental evaluations show that the model learned satisfies new assumptions and provides qualitative and quantitative results for new embeddings. The novel composition method captures the specific meaning of adjective-noun phrases effectively. The curr_chunk discusses the limitations of word embedding methods in capturing syntactic information and the need for syntax-aware embedding algorithms. Various approaches, such as syntax-oriented skip-gram and neural networks, have been proposed to address this issue. The curr_chunk discusses syntax-awareness in word embeddings, including a neural network model with a negative sample set for capturing syntactic elements. Various works have addressed composition for word embeddings, with theoretical justifications and mathematical frameworks presented. Mitchell & Lapata (2008; 2010) explore different composition methods. The curr_chunk discusses different composition methods for word embeddings, including training word embeddings for adjective-noun pairs and using matrix-vector multiplication for composition. This model aims to create a syntax-aware composition function that can be learned alongside word embeddings. The curr_chunk discusses generalizing word embedding methods through tensor factorization, specifically using CP and Tucker decompositions on word co-occurrence counts. This approach is different from previous composition methods and aims to incorporate syntactic structure into the model. The curr_chunk introduces the notation and basics of third-order tensors, which are three-way indexed arrays that can be interpreted as trilinear functions over three vectors. This operation is linear in the vectors x, y, and z, similar to applying a matrix to a vector. The curr_chunk discusses tensor decompositions, specifically Tucker rank and Tucker decomposition for third-order tensors. It explains how a tensor can be decomposed into a core tensor and matrices efficiently, with a focus on CP decomposition when the core tensor is diagonal. Carroll & Chang (1970); Harshman (1970) introduced the CP decomposition of a tensor as the sum of d rank-1 tensors. Unlike matrix factorizations, the CP decomposition is challenging to compute in the general case. Later, the RAND-WALK model for syntactic word embeddings naturally leads to a Tucker decomposition. The model considers a corpus of text as a sequence of random variables with word embeddings in a vocabulary V of n words. Each word has a word embedding v w \u2208 R d with a prior v w = s \u00b7v. The RAND-WALK model for word embeddings involves a slow-moving random walk of discourse vectors {c t} that mix quickly to a uniform distribution on a unit sphere. The model specifies a conditional probability distribution based on latent variables, but it cannot capture syntactic relationships between words. In the RAND-WALK model, syntactic word pairs are introduced to address complex dependencies between words. A trilinear form defined by a tensor T is used to mediate the interaction between a discourse vector c and word embeddings v and v of relevant words. In the extended model Syntactic RAND-WALK, a trilinear form tensor T is introduced as a latent random variable to model syntactic word pairs within the text. This model allows each discourse vector to generate a pair of words with a small probability, incorporating syntactic relationships between words. The graphical model depiction for a syntactic word pair is shown in FIG0. The Syntactic RAND-WALK model involves word embeddings, discourse vectors generated by random walk, and probabilities of generating words or syntactic pairs based on the vectors. Marginal probabilities of word pairs and triples are calculated under this model. The Syntactic RAND-WALK model involves word embeddings, discourse vectors generated by random walk, and probabilities of generating words or syntactic pairs based on the vectors. Marginal probabilities of word pairs and triples are calculated under this model. These marginal probabilities are closely related to the model parameters, and the main bottleneck in computing them is the normalization of conditional probabilities specified in equations. The composition tensor T is (K, )-bounded to ensure reasonable ranges for exp( c t , v w +T (v a , v w , c t )). The tensor component T (v a , v b , c) must be comparable to v b , c. The concentration of partition functions is stated in Lemma 1. Lemma 1 states the concentration of partition functions for the syntactic RAND-WALK model. It also provides constants for co-occurrence probabilities of words in the model. The word embeddings are required to be norm-bounded and satisfy certain conditions for the partition functions. The main result of the study shows that under certain conditions, the partition functions in the syntactic RAND-WALK model are bounded. This is supported by Lemma 2, which establishes bounds on word embeddings and partition functions with high probability. The model suggests that discourse vectors capture the meaning of the text. The discourse vector c for a syntactic word pair (a, b) is a suitable representation for the phrase. The MAP estimate \u0109 of c given [a, b] is obtained through tensor composition, involving additive composition. The composition tensor allows for linear or multiplicative interactions between words, as proposed in previous studies. The parameters of the syntactic RAND-WALK model are learned by relating joint probabilities between words to word embeddings and composition tensor. A simple formula is derived using PMI for 3 words, which cancels out partition numbers. Corollary 1 follows from Theorem 1, stating the relationship between events as in Theorem 1. The parameters of the syntactic RAND-WALK model are obtained through Tucker decomposition of the PMI3 tensor, providing a theoretical basis for using third-order pointwise mutual information in learning word embeddings. The model is trained on a pre-processed English Wikipedia corpus, resulting in a vocabulary of 68,279 words. Co-occurrence counts are generated for adjective-noun-word and verb-object-word tensors using a window size of 5. The text discusses the process of training word embeddings using the RAND-WALK model and the composition tensor T. It involves identifying adjective-noun and verb-object word pairs, using context windows, and optimizing the co-occurrence counts. The objective function used is similar to the one in Arora et al. (2015) and aims to minimize the number of parameters. To reduce parameters, the tensor T is constrained to have CP rank 1000. Training embeddings and tensor jointly did not significantly impact results. Experiments focus on syntactic word pairs, verifying key properties of RAND-WALK model assumptions. The tensor T is (K, )-bounded with mean and maximum values for different pairs. Despite outliers, it is bounded for K = 4 and = 0.25. Partition functions Z c,a concentrate around their means, confirmed by testing with random vectors. Performance of the new composition for word pairs is evaluated. The performance of a new composition method for adjective-noun and verb-object pairs is tested by computing a composed vector and retrieving words with closest embeddings. Results show that the tensor composition method can retrieve more specific words related to the phrase, but also sometimes retrieves unrelated words. This could be due to the sparseness of co-occurrence of three words. In some cases, the tensor composition method is comparable or inferior to the additive composition method, especially for low-frequency phrases. The tensor composition method is tested on adjective-noun and verb-object phrase similarity tasks using word embeddings. Human similarity judgments are somewhat noisy, with intersubject agreement at 0.52. The method aims to produce similarity scores correlating with human scores using Spearman and Pearson correlations as evaluation metrics. The tensor composition method is tested on similarity tasks using word embeddings. The data is split into a development set and a test set of humans. The weighted tensor composition yields worse performance than the simple additive composition. Different compositions are compared, including a weighted additive composition. The method is also compared to the smoothed inverse. The study compares a weighted additive composition method to the smoothed inverse frequency weighting method for sentence embeddings. Additionally, a hybrid embedding approach combining the sif embedding and tensor component is evaluated for performance improvement. Experiments are conducted using pre-computed word embeddings such as GloVe and cbow vectors. The study compares a weighted additive composition method to the smoothed inverse frequency weighting method for sentence embeddings, using pre-computed word embeddings like GloVe and cbow vectors. The tensor composition method outperforms additive compositions in most cases, except for the Spearman correlation on cbow vectors. The study compares different composition methods for sentence embeddings using pre-computed word embeddings like GloVe and cbow vectors. The sif embeddings outperform additive and tensor methods, but combining sif embeddings with tensor components yields the best performance. Results show high consistency for optimal weight parameters across different embeddings. Predicting phrase similarity is challenging, with sif embeddings performing worse than unweighted vector addition. Adding the tensor component improves sif embeddings. The study compares different composition methods for sentence embeddings using pre-computed word embeddings like GloVe and cbow vectors. The tensor composition method achieves the best results for the glove and cbow vectors, while weighted addition works best for the randwalk vectors. Results demonstrate that the composition tensor can improve the quality of phrase embeddings, especially for verb-object phrases like \"giving birth\" and \"solve problem\". The additive composition method struggles to capture actions in verb-object phrases compared to the tensor composition method. Adjective-noun phrases like \"United States\", \"Soviet Union\", and \"European Union\" function more as compound proper nouns. The tensor composition method retrieves more relevant words for \"European Union\" but also produces some false positives. In contrast to the additive composition method, the tensor composition method also generates false positives. When analyzing phrases like \"expensive taste\" and \"awful taste\", both methods retrieve related words, but struggle to capture the overall meaning. The additive composition fails to capture the sense of phrases like \"refined taste\", retrieving irrelevant words related to food taste. The tensor composition method retrieves more relevant words compared to the additive composition method. It consistently retrieves words like \"confidante\", \"confided\", \"coworker\", and \"protoge\", which are fairly relevant. The tensor composition is tested for sentiment analysis using a movie review dataset. The study uses movie review datasets to analyze adjective-noun pairs and word embeddings. A tensor method is compared to a baseline method for sentiment analysis, showing a slight edge but not significant differences. The main Theorem 1 is established in this section. The main Theorem 1 establishes the connection between model parameters and word correlations. Section B.1 analyzes the partition function, while Section B.2 proves the main theorem. Concentrations of partition functions are proven in Lemma 1, requiring the tensor to be K-bounded. The lemma discusses the bounded tensor T and the probability of choosing words based on word embeddings. It focuses on proving the partition function's relationship with the norm of the vector r. The lemma proves that the norm of the vector r = T(va, \u00b7, c) + c is concentrated when the tensor T is (K, )-bounded. It involves representing c as a standard spherical Gaussian vector and utilizing the singular value decomposition of M = T(va, \u00b7, \u00b7) + I. The lemma proves concentration bounds for the norm of the vector r = T(va, \u00b7, c) + c when the tensor T is (K, )-bounded. It involves representing c as a standard spherical Gaussian vector and utilizing the singular value decomposition of M = T(va, \u00b7, \u00b7) + I. U and V are orthogonal matrices, leading to y = V T z having the same distribution as z. Concentration bounds are then applied to both the numerator and denominator of a quantity involving generalized \u03c7 2 random variables. Further bounds are derived for matrices A and B, ultimately concluding the proof. The lemma provides concentration bounds for the norm of the vector r = T(va, \u00b7, c) + c when the tensor T is (K, )-bounded. It involves representing c as a standard spherical Gaussian vector and utilizing the singular value decomposition of M = T(va, \u00b7, \u00b7) + I. Further bounds are derived for matrices A and B, ultimately concluding the proof. The proof shows that the expected partition function is concentrated for most discourse vectors, and when there are a finite number of words, it is concentrated around its expectation. This is based on a lemma from Arora et al. (2015) which provides concentration bounds for a fixed vector r. The proof of Lemma 1 is then presented. The proof of Lemma 1 shows that the expected partition function is concentrated for most discourse vectors. By using Lemmas 5 and 6, it is proven that with high probability, the partition function is within a certain range for a fraction of discourse vectors and word embeddings. The proof of Theorem 1 and Corollary 1 is similar to Theorem 2.2 in Arora et al. (2015). The proof considers discourse vectors and word generation. Lemma 2 is also proven, showing the concentration of the expected partition function. In the proof, the unit sphere is covered by metric balls of small radius, ensuring the partition function is bounded below by a constant with high probability. The partition function at any point on the sphere is not far from the function at a ball center if the norms of the vectors are not too large. The norms of the vectors are appropriately controlled."
}