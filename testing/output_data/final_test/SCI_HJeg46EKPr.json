{
    "title": "HJeg46EKPr",
    "content": "In this work, an Integrative Tensor-based Anomaly Detection (ITAD) framework is introduced for detecting anomalies in satellite systems. 3rd-order tensors are constructed with telemetry data from KOMPSAT-2, and anomaly scores are calculated using CANDECOMP/PARAFAC decomposition. The results show that this tensor-based approach can improve accuracy and reduce false positives in anomaly detection for satellite systems. An Integrative Tensor-based Anomaly Detection (ITAD) framework is effective in achieving higher accuracy and reducing false positives in detecting anomalies in satellite systems. Anomalies in satellites are challenging due to complex system interactions, requiring the comparison of multiple telemetry data. Previous studies have focused on detecting anomalies in satellite systems. The Integrative Tensor-based Anomaly Detection (ITAD) framework aims to reduce false positives in detecting anomalies in satellite systems by analyzing multiple telemetry data as a whole. This approach is more effective in identifying true anomalies in complex systems compared to analyzing individual telemetry data. The ITAD framework uses a 3rd-order tensor to capture telemetry data and detect anomalies by decomposing it into component matrices. Cluster analysis is performed on one matrix to calculate anomaly scores based on distance from cluster centroids. Dynamic thresholding is applied to adjust detection thresholds over time. Experiments on real telemetries show effective anomaly detection and reduced false positives compared to other methods. Tensor decomposition for anomaly detection has been successfully applied in various fields such as neuroscience, environmental monitoring, video surveillance, network security, and remote sensing. The method involves using multiway principal components analysis (MPCA) to extract information from multivariate trajectory data, reducing space while representing the original data effectively. The tensor-based anomaly detection method is widely used in various fields like neuroscience, environmental monitoring, video surveillance, network security, and remote sensing. It involves using multiway principal components analysis (MPCA) to detect anomalies in an unsupervised manner. The score plot-based model is commonly used in anomaly detection without labels, where component matrices from tensor decomposition are used to calculate score plots for anomaly detection. The score plot can be 1-dimensional, 2-dimensional, or 3-dimensional based on the characteristics of the tensor dataset. In our work, we use a score-based method to detect anomalies by analyzing one of the component matrices comprehensively. Anomaly Detection for Satellite Operation typically involves using the Out-Of-Limit (OOL) method to define a nominal range for anomaly detection. In anomaly detection for satellite systems, the Out-Of-Limit (OOL) method is commonly used along with additional methods such as dimensionality reduction, nearest neighbors, and clustering algorithms. Korea Multi-Purpose Satellite 2 (KOMPSAT-2) is a national monitoring satellite launched in 2006, generating over 3,000 telemetries from various subsystems. In this study, 88 types of telemetries were collected from the KOMPSAT-2 satellite for 10 months, categorized into 7 subsystems. An Integrative Tensor-based Anomaly Detection (ITAD) framework was developed, addressing missing sensor values using linear interpolation. After linear interpolation, telemetry values are normalized using min-max normalization to a range between 0 and 1. Timestamps are compressed by grouping multiple rows into one due to many values remaining constant for extended periods. This compression reduces dataset size significantly, providing computational benefits. After linear interpolation and normalization, telemetry values are compressed to reduce dataset size significantly. Feature extraction involves statistical methods to extract 8 features for each telemetry time series, resulting in a final feature vector. This allows for computational efficiency while maintaining critical information. Tensor Construction and Decomposition: A 3rd-order telemetry tensor is constructed with time \u00d7 feature vector \u00d7 telemetry. The tensor is decomposed using the CANDECOMP/PARAFAC (CP) method to obtain component matrices A, B, and C. Matrix A describes time-to-factor, matrix B shows feature vector-to-factor relationships, and matrix C captures telemetry-to-factor connections. This approach effectively handles high-dimensional time series data. The final matrix C captures the telemetry-to-factor matrix to characterize factor effects on telemetry. The alternating least squares (ALS) method is used to find optimal solutions for CP decomposition, iteratively optimizing one component at a time. The procedure involves fixing component matrices B and C to solve for A, then fixing A and C to find B, and finally fixing A and B to solve for C. This process is repeated until convergence is reached. The CP decomposition process involves selecting an optimal rank r by measuring the reconstruction error between the original tensor X and the approximated tensor X. The smaller the reconstruction error, the closer the approximate tensor is to the original tensor. Clustering Analysis: The original telemetry data is highly unbalanced, with most elements being normal and only a few anomalies. To address this issue, a clustering method is applied to group major patterns of telemetry samples, representing normal data behavior. The matrix A is chosen for clustering to identify anomaly occurrence time. Various clustering algorithms like GMM, IF, k-means, and OCSVM are tested, with k-means showing fewer false positives. In our work, we exclusively use the k-means algorithm for clustering due to fewer false positives compared to other methods. Silhouette analysis is utilized to determine the optimal number of clusters (k) by assessing the samples' correct assignment. The silhouette coefficient ranges from -1 to 1, with positive values indicating correct assignments, negative values suggesting potential misassignments, and zero values representing samples on cluster boundaries. The optimal k value is selected based on the largest positive silhouette coefficient. Note: Tensor decomposition can be considered a clustering mechanism, with the component matrix A capturing comprehensive characteristics of telemetries in the same subsystem. In our research, we use kmeans clustering and tensor decomposition to identify anomalies in time instances. An anomaly score is calculated based on the Euclidean distance between normal and abnormal data. Time samples are assigned to clusters, and anomalies exhibit different patterns not belonging to any cluster. The Euclidean distance measures similarity to the centroid of the nearest cluster. An anomaly score is calculated based on the Euclidean distance between normal and abnormal data. Setting a fixed threshold may lead to false positives and an inability to detect contextual anomalies. The data-driven dynamic thresholding method addresses the issue of false positives by adjusting the threshold value based on the mean and standard deviation of data values within a time window. An anomaly is detected when the anomaly score is a certain distance away from the mean, preventing false positives. In this experiment, 88 types of telemetries with over 43,716 samples are used to construct 3rd-order telemetry tensors. Each sample has a feature vector of 8 statistical quantities. The tensors are decomposed into component matrices using CP decomposition with ALS method. Convergence criteria include stopping updating at 100 iterations or when tolerance is less than 10^-8. Reconstruction errors are calculated for different ranks to determine the optimal rank. The experiment involved constructing 3rd-order telemetry tensors from 88 types of telemetries with over 43,716 samples. CP decomposition with ALS method was used to decompose the tensors into component matrices. Reconstruction errors were calculated for different ranks to determine the optimal rank, which varied from 11 to 29 for each subsystem. K-means clustering was applied to the component matrix A with time-to-factor information to detect anomaly points. Silhouette analysis was used to determine the optimal k for each subsystem. The dynamic thresholding method was used to adjust threshold values based on environmental changes, with trade-offs between window size and coefficient parameter. Experiments were conducted to determine optimal values for each subsystem, finding that a window size of 108 or 198 and a coefficient parameter of 6 resulted in the best performance. The dynamic thresholding method was fine-tuned for anomaly detection performance across different window sizes. A comparison with other anomaly detection methods for satellite systems was conducted, including One-Class SVM (OCSVM) and NOS-TRADAMUS. Performance metrics such as true positives (TP) and false positives (FP) were evaluated based on anomalies labeled by domain experts at KARI. The importance of detecting anomalies to ensure proper satellite functionality was emphasized. The satellite operators aim to accurately detect anomalies while reducing false positives in a highly unbalanced dataset. Various methods show similar detection performance in true positives (TP) but differ in false positives (FP). The ITAD approach outperforms in reducing false positives compared to other methods, with only 1 false positive detected. MPPCACD shows the second-best performance in FP but is not as effective as the ITAD approach. Overall, the ITAD framework shows superior performance in reducing false positives. The ITAD approach outperforms other methods in reducing false positives, achieving the highest precision of 66.67%. While recall performance remains consistent across all approaches, ITAD excels in F1 score by more than two-fold at 66.67% compared to 28.57%. Anomaly detection methods often generate high false positives due to their inability to consider multiple telemetries simultaneously, leading to the detection of temporary glitches as trivial outliers. In subsystem4, a temporal glitch on August 17th is identified as a trivial outlier by the ITAD approach, while other methods label it as an anomaly. ITAD does not report trivial outliers in subsystem3, subsystem4, and subsystem5, unlike other methods. Temporary glitches in multiple telemetries at the same timestamp are flagged as anomalies by ITAD. The ITAD approach accurately identifies temporary glitches in multiple telemetries as anomalies, reducing false positives. Integrative analysis using a tensor-based method for satellite monitoring shows effectiveness. Choosing an appropriate rank-size r is a complex problem with no general algorithm. The reconstruction error is used to select r, but there is a risk of overfactoring and not achieving an optimal solution. To address the challenge of selecting the optimal rank r for satellite monitoring, the Core Consistency Diagnostic (CORCONDIA) method proposed by Bro and Kiers (2003) will be applied in future work. This method assesses core consistency and measures similarity between core and super-diagonal arrays for more accurate results. Despite using a 10-month real telemetry dataset with few anomalies, the focus is on reducing false positives to help satellite operators identify true anomalies. This approach is crucial as mission-critical systems would fail quickly in the presence of many anomalies. The researchers are working on improving anomaly detection for satellite telemetry data by collecting more datasets with anomalies and developing a better performance metric. They are also planning to deploy their tensor-based anomaly detection method on the KOMPSAT-2 satellite in 2020. The researchers developed an Integrated Tensor-based Anomaly Detection (ITAD) framework using the KOMPSAT-2 satellite telemetry dataset. Their approach can analyze multiple telemetries simultaneously, achieving higher precision and F1 score while reducing false positives. Future plans include improving the algorithm with the CORCONDIA method, finding an optimal rank, and incorporating datasets with more anomalies. Their work lays the foundation for space anomaly detection using an integrated tensor-based mechanism applicable to various multivariate time-series anomaly detection scenarios. A tensor is a multi-dimensional array where vectors and scalars are considered as tensors of different orders. Expressing a tensor as a sum of a rank-1 tensor allows for effective handling of datasets with multi-modal aspects. This method was first proposed in 1927 by Hitchcock. In 1970, Carroll and Chang proposed CANDECOMP and Harshman suggested PARAFAC for higher-order data. The CP decomposition by Kiers decomposes Nth order data into a linear sum of rank-1 tensors. Appellof and Davidson pioneered the use of CP model in chemical systems, while Andersen and Bro contributed to practical tensor applications. Acar et al. applied tensor decomposition to data mining, analyzing online chatroom data. Tensor decomposition is a commonly used method in data mining. It involves constructing a 3rd-order tensor with user \u00d7 keyword \u00d7 time spaces and using the Tucker decomposition to decompose the tensor into a core tensor and three matrices with different ranks. This method is effective for extracting information from data in an unsupervised manner. Tensor decomposition is an effective unsupervised method for extracting characteristics of Nth-dimensional data, preserving the N th-order structure for more accurate results. Silhouette analysis is used to determine the optimal k for k-means clustering, selecting the value with the largest positive silhouette coefficient. The optimal r is chosen by minimizing the reconstruction error as the rank r is increased. The reconstruction error is minimized by increasing the rank r until the approximated tensor can reconstruct over 90% of the original telemetry tensor. A fixed threshold approach can lead to high false positives, as illustrated in an example with telemetry AFSS1OMI of subsystem2. Increasing the window size (w) from 9 to 198 and the coefficient parameter (m) from 4 to 6 influences the detection of false positives and true positives. The best performance is achieved when w is either 108 or 198 for all subsystems, with m = 6 typically resulting in the best performance for any window size."
}