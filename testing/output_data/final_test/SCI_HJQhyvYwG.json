{
    "title": "HJQhyvYwG",
    "content": "DuoRC is a new dataset for Reading Comprehension with 186,089 question-answer pairs from movie plots on Wikipedia and IMDb. Questions and answers are created from different versions of the same movie, ensuring little lexical overlap. The DuoRC dataset consists of question-answer pairs from different versions of the same movie, requiring deeper language understanding and background knowledge. State-of-the-art neural RC models struggle with this dataset, achieving a lower F1 score compared to the SQuAD dataset. This opens up new research avenues for exploring novel neural techniques in Reading Comprehension. DuoRC dataset complements other Reading Comprehension datasets for studying language understanding. Progress in Natural Language Understanding is measured through tasks like machine translation and question-answering. Reading Comprehension systems must understand text passages to answer questions, highlighting the need for evolving dataset benchmarks to reflect true language understanding challenges. The progress in Reading Comprehension (RC) has been significant with benchmark datasets like SQuAD, TriviaQA, MS MARCO, MovieQA, and cloze-style datasets. However, these datasets have limitations in studying language understanding, as they mainly focus on factual descriptive passages and lack narratives that require reasoning and background knowledge. Additionally, the questions in these datasets often have a high lexical overlap with the passage or contain a high noise level. Recent work has shown that existing models struggle with adversarially inserted sentences in question answering tasks due to high word overlap but lack of answer content. TriviaQA is noisy due to distant supervision, while cloze-style datasets allow models to reach near human performance easily. This limits the complexity of language understanding required for reading comprehension tasks. To address these limitations, DuoRC is proposed to push the state-of-the-art in language understanding for RC. DuoRC is a dataset designed to challenge existing models in language understanding for reading comprehension. It contains questions with low lexical overlap, requires background knowledge for answers, involves complex reasoning across multiple sentences, and includes unanswerable questions. The dataset consists of QA pairs from different versions of movie plots written by different authors. The DuoRC dataset contains QA pairs from different versions of movie plots, challenging models in language understanding for reading comprehension. Crowd workers on Amazon Mechanical Turk were tasked with creating QA pairs from one version of the plot and answering questions from a different version, highlighting challenges such as varying plot detail, narration style, and vocabulary. The DuoRC dataset contains QA pairs from different movie plot versions, challenging models in language understanding. Crowd workers on Amazon Mechanical Turk created QA pairs from one plot version and answered questions from another, facing challenges like varying plot detail and vocabulary. For some questions, alternative but valid answers were obtained from the second plot, showcasing complex reasoning required for resolving coreferences and using common sense knowledge. The DuoRC dataset presents challenges for machines in language understanding, requiring capabilities such as knowledge graph utilization, common-sense knowledge, paraphrase understanding, inferencing across events, and detecting unanswerable questions. Workers marked some questions as \"unanswerable\" due to insufficient information in the plot, highlighting the need for machines to exhibit the ability to detect unanswerability. Current RC systems lack the ability to detect unanswerable questions and delegate them to humans. DuoRC dataset challenges existing models, showing room for improvement and new research avenues in reading comprehension. It is not a replacement for other datasets but can complement them. The dataset discussed is not meant to replace existing RC datasets but can be used in conjunction with them to tackle various language understanding challenges. There has been a rise in RC datasets in recent years, each with different question and answer creation methods. Some datasets involve answers corresponding to a span in the document, while others use web queries as questions with synthesized answers. Cloze-style datasets automatically create questions by deleting a word/entity from a sentence. Additionally, there are datasets with multiple choice questions where the task is to select one correct answer from given options. Each new dataset in the field of Reading Comprehension (RC) brings new challenges and helps in building better QA systems. It prevents research from stagnating and enables exploration of novel neural approaches in complex language understanding. The DuoRC dataset presents 4 new challenges not found in existing RC datasets, allowing researchers to tackle unsolved problems in NER on general web text, domain-specific text, and noisy social media text. The curr_chunk discusses the dataset collection process for a new QA dataset. It involved extracting parallel movie plots from IMDb and Wikipedia, retaining 7680 movies with plots longer than 100 words. The IMDb plots were found to be more descriptive than the Wikipedia plots. Additionally, QA pairs were collected from shorter versions of the plots. The longer version of the plot is almost double the size of the shorter version, usually 500 words long. Workers on AMT were shown the shorter version and asked to create QA pairs from it. They could either pick an answer directly from the document or synthesize one. 70% of the time, answers were directly from the document, while 30% were synthesized. 85,773 QA pairs were collected along with their corresponding documents, forming the SelfRC dataset. Workers on AMT were asked to answer questions from a longer version of the plot by selecting an answer from the document, synthesizing one, or marking the question as not answerable. Results showed that 50% of answers matched a span in the document, 37% were synthesized, and 13% were marked as not answerable. Personal knowledge about the movie was not allowed for answering. A dataset called ParaphraseRC was created by collecting 100,316 {question, answer, document} triplets where questions were taken from one version of the document and answers from a different version. A wait period of 2-3 weeks was introduced between data collection phases to ensure fresh workers and reduce information bias. The number of unique questions in ParaphraseRC is the same as in SelfRC, but ParaphraseRC has more triplets (100,316 vs 85,773) due to not creating new questions from the longer plot version. The dataset DuoRC contains a total of 186,089 {question, answer, document} triplets, combining questions from Wikipedia plots with IMDb versions. The distribution of different Wh-type questions is shown in Fig. 2, with varying answers to the same question in the two versions. Additional statistics can be found in TAB1 and Appendix B. The DuoRC dataset contains question-answer pairs from Wikipedia plots and IMDb versions, with different answers for the same question in the two documents. Only 40.7% of questions have the same answer, 37.8% have no overlap, and 21% have partial overlap. Examples in Appendix A highlight the challenges of ParaphraseRC compared to SelfRC. Various state-of-the-art RC and language generation models are discussed to establish baseline performance on the dataset. The DuoRC dataset presents challenges for question answering models as answers are not always exact spans in the document. Around 30% of answers in SelfRC and 50% in ParaphraseRC are synthesized by humans. Despite this, adapting span prediction models from SQuAD dataset can still be beneficial. Two models are proposed: a basic span prediction model trained on instances where answers match document spans, and another model for synthesized answers. The purpose of the model is to evaluate if our dataset is harder than SQuAD even when answers match document spans. The goal is to test state-of-the-art models like DCN BID20 on DuoRC, especially in the ParaphraseRC setup. The BiDAF model is chosen as it meets performance criteria and ensures reproducibility. The SpanModel is employed as one of the models in the study, utilizing the BiDAF model for span prediction. Another model follows a two-stage process, predicting the span first and then generating the answer. This model uses a query-based abstractive summarization model for answer generation, similar to the task at hand. The training data format for this model is {query, mini-document, generated answer}. Refer to the original paper for more details on the models used. The GenModel, a two-stage model for answer generation, is discussed in the original paper BID7. BID15 also proposed a model for the MS MARCO dataset, but their code has not been released. NLP pre-processing for a good ParaphraseRC model would ideally include knowledge graph utilization, common-sense knowledge, paraphrase/semantic understanding, multiple-sentence inferencing, and educated guesswork. These challenges are beyond the scope of a single paper. In establishing a baseline for DuoRC, the paper addresses challenges such as paraphrase understanding, coreference resolution, and handling long passages using standard NLP techniques. Relevant sentences are extracted from the document to aid the span detector, even if they have minimal word overlap with the question. Named entities in the question may not be directly present in the relevant sentence. The text discusses resolving coreferences in a document by using Stanford coreference resolution. It also explains how to determine the relevance of a sentence to a question based on word matching criteria. If no sentence has at least 50% overlap with the question, sentences with a 30% overlap are considered. The text discusses evaluating the SpanModel and GenModel for answer generation. The training data is augmented, and the test set is split into subsets for evaluation. The SpanModel predicts spans in the document, while the GenModel generates answers. The SpanModel is evaluated on a subset where the answer matches a span, and also on the entire test set. The GenModel is evaluated on the Full Test Set, which contains answers not corresponding to spans in the document. Training the GenModel involves two stages: predicting spans and generating answers. The second stage requires training data of the form {x = span, y = answer} from instances where the answer matches a span or is synthesized. For the second type of instances, various approaches are considered to find the approximate span from which the answer could have been generated. One method is to treat the entire document as the true span, while another is to extract named entities, noun phrases, and verb phrases from the question to create a lucene query for extracting relevant portions of the document. The model trained using the LCS based method gave the best results for finding the approximate span for the answer. Evaluation metrics include Accuracy and F-score, with results summarized in TAB3. Comparing SpanModel and GenModel shows differences in performance. The SpanModel outperforms the GenModel in finding exact spans in the document for answers. The GenModel struggles with generating correct answers even when the span is predicted correctly. The GenModel struggles to generate accurate answers, often producing more or fewer words than the true answer. Comparing SelfRC and ParaphraseRC in TAB4, the models perform worse for ParaphraseRC, confirming it as a more challenging task. NLP pre-processing is used for ParaphraseRC to identify relevant sentences, evaluating its effectiveness by computing the percentage of pruned document and average recall of the answer. The recall is reported for the span-based subset of the data in TAB3 using different pruning strategies. Comparing the SpanModel with and without Paraphrasing in TAB4 for ParaphraseRC shows that the pre-processing step improves the model's performance. The Oracle pre-processing step is used to validate the effect of adding complexity to the ParaphraseRC plot. The SpanModel with Oracle preprocessed data shows a slight improvement in performance over rule-based preprocessing. Cross-testing between SelfRC and ParaphraseRC datasets reveals that merging training data from both improves model performance on individual test sets. The DuoRC dataset, consisting of 186K question-answer pairs from parallel movie-plots, presents new challenges for complex language understanding. It ensures minimal lexical overlap between questions and answers, promoting research on question-answering with external knowledge and common-sense reasoning. This dataset aims to push the boundaries of state-of-the-art RC models. The state-of-the-art RC models, despite achieving near human performance on the SQuAD dataset, perform poorly on the DuoRC dataset. This highlights the need for further research exploration. The dataset showcases examples of plots used to create and answer questions. Questions based on smaller plots can be answered straightforwardly, while those based on larger plots require more complex inferencing. Plot contents are truncated to show snippets for answering questions. Blue and cyan colors indicate answer availability in smaller plots, while red and orange colors are used for larger plots. The manual verification of question-answer pairs revealed that ParaphraseRC often provides no answer due to lack of information or the need for guesswork. SelfRC and ParaphraseRC answers may differ due to phrasal paraphrases, subjective questions, or multiple valid answers to objective questions. The manual verification of question-answer pairs showed discrepancies in answers due to different spellings or variations in names."
}