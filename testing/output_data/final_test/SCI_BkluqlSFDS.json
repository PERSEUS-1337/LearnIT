{
    "title": "BkluqlSFDS",
    "content": "Federated learning enables edge devices to collaboratively learn a shared model while keeping training data on device. The Federated matched averaging (FedMA) algorithm is designed for modern neural network architectures like CNNs and LSTMs. FedMA constructs the shared global model by matching and averaging hidden elements with similar feature extraction signatures. Experiments show that FedMA outperforms other federated learning algorithms on deep CNN and LSTM architectures, improving communication efficiency for edge devices with access to abundant data. Federated learning allows edge devices to collaboratively train a shared global model while keeping data on device. It addresses concerns of data privacy, network bandwidth, and device availability. Clients train models independently and upload locally trained models to the data center for aggregation using methods like FedAvg and FedProx. Agnostic Federated Learning (AFL) optimizes a centralized distribution formed by a mixture of client distributions. FedProx adds a proximal term to limit the impact of local updates on the global model. FedAvg's coordinate-wise weight averaging can hinder performance due to the permutation invariant nature of neural network parameters. PFNM addresses this by finding permutations of NN parameters before averaging. In response to the issue of permutation invariance in neural network parameters, PFNM addresses this by finding permutations of NN parameters before averaging. PFNM has shown improved performance and communication efficiency, but was limited to fully connected NNs. In this work, the authors demonstrate how PFNM can be applied to CNNs and LSTMs, with minor improvements observed. They also introduce Federated Matched Averaging (FedMA), a new layers-wise federated learning algorithm for modern CNNs and LSTMs, which incorporates matching and model size adaptation principles from PFNM. FedMA is empirically studied with real datasets under federated learning constraints. In this work, the discussion revolves around permutation invariance classes of neural network architectures, focusing on averaging in the parameter space of NNs. Starting with a single hidden layer fully connected network, the analysis progresses to deep architectures, convolutional, and recurrent architectures. The permutation invariance of fully connected architectures is explored, highlighting the equivalence of parametrizations due to summation being a permutation invariant operation. The use of permutation matrices in orthogonal transformations on rows is also emphasized. The discussion focuses on averaging in the parameter space of neural network architectures, particularly addressing the issue of permutation invariance. It explains the importance of undoing permutations before averaging neural networks in the weight space to achieve meaningful results. The text also introduces the concept of permutation matrices as orthogonal transformations on rows in neural networks. The text discusses solving optimization problems with inverse permutations using the Hungarian matching algorithm. It highlights the data heterogeneity in Federated Learning, where each client learns unique feature extractors. The global model size is flexible to accommodate varying local model sizes. The text discusses using the Hungarian algorithm for matched averaging in Federated Learning to handle data heterogeneity. It adapts global model size iteratively and avoids poor matches by creating new global neurons when needed. In Federated Learning, the Hungarian algorithm is used for matched averaging to address data heterogeneity. The global model size is adjusted iteratively, creating new global neurons when necessary to avoid poor matches. The size of the new global model is determined by a maximum bipartite matching formulation. Technical details include permuting weights and padding with \"dummy\" neurons. In Federated Learning, the Hungarian algorithm is used for matched averaging to address data heterogeneity. The global model size is adjusted iteratively, creating new global neurons when necessary to avoid poor matches. Technical details include permuting weights and padding with \"dummy\" neurons. To complete the matched averaging optimization procedure, specify similarity, threshold, and model size penalty. The setup follows Yurochkin et al. (2019) for computing the maximum a posteriori estimate (MAP) using the Beta-Bernoulli process (BBP) and Indian Buffet Process prior. The procedure for solving the setup is referred to as BBP-MAP. Our matched averaging perspective allows for the formulation of averaging widely used architectures such as CNNs and LSTMs as instances of equation 2, utilizing BBP-MAP as a solver. Permutation invariance in deep fully connected networks is discussed as a building block for handling LSTMs and CNN architectures like VGG. The extension of equation 1 recursively defines deep FC networks, with \u03c3(\u00b7) representing any conventionally used function. Permutation invariance is crucial in understanding CNNs, where neurons are replaced by features x = x0 and \u03a0N is the identity for output classes. Recursive matched averaging is used for deep FCs obtained from multiple clients, requiring inverse permutations for each layer. Permutations within consecutive layers pose a challenging optimization problem, leading to a recursive formulation starting with an identity permutation for the base case. The key observation in understanding permutation invariance of CNNs is that channels, not neurons, define the invariance. Convolutional operations are defined by input/output channels and filter dimensions. Permutations on weights and input channels do not affect the CNN's forward pass. Matched averaging can be applied to deep CNN layers recursively. This extends PFNM to CNNs. The extension of PFNM to CNNs performs well on MNIST with simpler architectures like LeNet but breaks down for more complex architectures like VGG-9 needed for CIFAR-10. Permutation invariance in recurrent architectures is associated with the ordering of hidden states, with hidden-to-hidden weights being permutation invariant. The permutation of hidden states affects both rows and columns of an RNN. To align hidden-to-hidden weights, a quadratic assignment problem arises, which is NP-hard. A solution involves matching input-to-hidden weights to find permutations and computing federated hidden-to-hidden weights accordingly. In LSTMs, multiple cell states have individual hidden-to-hidden and input-to-hidden weights. Input-to-hidden weights are stacked into a weight matrix when computing permutation matrices. Deep LSTMs are processed recursively similar to deep FCs. Extending PFNM to CNNs and LSTMs fails on deep architectures, as shown in Figure 1. Recursive handling with matched averaging may lead to poor solutions. The proposed Federated Matched Averaging (FedMA) approach addresses the limitations of matched averaging on deep architectures by implementing a layer-wise matching scheme. The data center gathers the first layer's weights from clients, performs one-layer matching, and broadcasts the matched weights for consecutive layers. This process is repeated for each layer, with weighted averaging based on class proportions. FedMA requires communication rounds equal to the number of layers in a network and performs well on deeper CNN architectures like VGG-9. FedMA outperforms other federated learning approaches in heterogeneous data scenarios, but still lags behind entire data training. The goal is to improve performance by striving to achieve the upper bound set by entire data training. To enhance performance, FedMA with communication is proposed, where local clients receive the global model and reconstruct their models based on matching results. This approach keeps the global model small compared to using the full model across clients. Empirical study compares FedMA with FedAvg and FedProx, analyzing performance with increasing clients and visualizing matching behavior. Studies are conducted on three real-world datasets. Experimental Setup: FedMA and baseline methods were implemented in PyTorch for a simulated federated learning environment. The data center samples all clients for training in each communication round. Data augmentation and normalization were applied to the CIFAR-10 dataset. Batch normalization layers in the VGG architecture were excluded. Two data partition strategies were considered for CIFAR-10. For CIFAR-10, two data partition strategies were considered: homogeneous and heterogeneous. The heterogeneous partition involved sampling proportions for each class to allocate training instances to local clients. The original test set in CIFAR-10 was used as the global test set. For the Shakespeare dataset, each speaking role in each play was considered a different client, resulting in inherent heterogeneity. The dataset was preprocessed to filter out clients with less than 10k data points, resulting in 132 clients. 80% of the data was used for training. In this experiment, 80% of the data was used for training, resulting in 132 clients with at least 10k data points. Out of these, J = 66 clients were randomly sampled for the experiments. The goal was to compare FedMA with FedAvg and FedProx in terms of message size exchanged and communication rounds needed for good performance. The methods were evaluated on CIFAR-10 with VGG-9 local models and on Shakespeare dataset with 1-layer LSTM network under the heterogeneous federated learning scenario. In the experiment, FedMA outperforms FedAvg and FedProx in terms of convergence rate and message size, especially when evaluating as a function of message size. Tuning parameters like local training epochs and coefficient \u00b5 in FedProx were crucial for achieving the best model accuracy over the global test set. In the experiment, the effect of local training epochs on FedAvg, FedProx, and FedMA was studied using VGG-9 on CIFAR-10 under a heterogeneous setup. Different numbers of local training epochs were considered, and it was observed that training longer favored the convergence rate of FedMA, leading to better global model performance on local models with higher quality. FedMA outperforms FedAvg and FedProx in terms of global model performance on local models with higher quality. FedAvg shows accuracy deterioration with longer local training, while FedProx mitigates it to some extent. Real-world data often exhibit biases, affecting classifier performance on under-represented domains. In federated learning, data biases can impact classifier performance on under-represented domains. FedMA can address this issue by treating each domain as a separate client, allowing local models to learn meaningful relations without aggregate biases. FedMA has shown strong performance in federated learning tasks. FedMA has demonstrated strong performance in federated learning tasks with heterogeneous data. An experiment was conducted to simulate a skewed domain problem using the CIFAR-10 dataset. By creating grayscale-dominated and colored-dominated classes, it was found that training on the entire dataset led to poor test performance due to uninformative correlations. This highlights the importance of addressing biases in data to improve classifier performance. The experiment compared regular training on CIFAR-10 dataset with federated learning approaches. FedMA outperformed other methods, suggesting it can eliminate data biases and surpass entire data training. Selective data collection is another approach to address data bias. In the experiment, selective data collection is suggested to address data bias by collecting more colored images for grayscale dominated classes and vice versa. The Color Balanced approach performs well but may be expensive. Oversampling from available data is an alternative, where underrepresented domains are sampled to equalize the proportion of color and grayscale images for each class. However, oversampling may lead to overfitting. Data efficiency in federated learning is crucial as deep learning models perform better with more training data. However, the challenge arises when new clients join the system with their own data distributions, potentially deteriorating performance. To study this, the CIFAR-10 dataset was partitioned into homogeneous and heterogeneous pieces. This approach aims to address the issue of data diversity and overfitting in oversampled images, showing marginal improvement compared to centralized training and performing worse than FedMA. The CIFAR-10 training set is partitioned into 25 heterogeneous sub-datasets. A 5-step experimental study is conducted in federated learning, showing that FedMA outperforms FedAvg in utilizing communication rounds efficiently. FedMA identifies matching groups of convolutional filters for weight averaging. FedMA identifies matching groups of convolutional filters and averages them into global filters. Visualizations show that matched filters and global filters found with FedMA extract the same features from input images, while FedAvg's global filter averages out specific features. FedMA is a layer-wise federated learning algorithm designed for modern CNNs and LSTMs using probabilistic matching. FedMA is a layer-wise federated learning algorithm for modern CNNs and LSTMs, focusing on probabilistic matching and model size adaptation. Future work includes extending FedMA to optimize averaging strategies and supporting additional building blocks like residual structures and batch normalization layers in CNNs. The VGG-9 architecture used in experiments follows standard data augmentation and normalization processes for the CIFAR-10 dataset. The final global VGG and LSTM models returned by FRB with communication have channel pixel values normalized by subtracting the mean and dividing by the standard deviation in each color channel."
}