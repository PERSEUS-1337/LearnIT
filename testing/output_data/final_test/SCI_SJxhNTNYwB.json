{
    "title": "SJxhNTNYwB",
    "content": "The new method for black-box adversarial attack involves learning a low-dimensional embedding using a pretrained model and efficiently searching within the embedding space to attack an unknown target network. This approach improves query efficiency across different target network architectures, resulting in a significant reduction in the number of queries required. The method also shows success in attacking adversarially defended networks on CIFAR10 and ImageNet, reducing queries and improving attack success rates. The wide adoption of neural network models in modern applications has raised security concerns due to vulnerability to adversarial examples. Attack methods can be categorized as white-box or black-box, with transfer-based and score-based approaches used for black-box attacks. Transfer-based methods involve pretraining a source model to generate adversarial examples for an unknown target network. Score-based attacks require a loss-oracle to query the target network. The attack involves using a loss-oracle to query the target network for its gradient approximation. Combining transfer-based and score-based black-box attack methods can efficiently exploit a pretrained white-box source network to target an unknown black-box model. The paper proposes a method called TRansferable EMbedding based Black-box Attack (TREMBA) that utilizes a pretrained white-box source network to guide black-box attacks. TREMBA involves training an encoder-decoder to generate adversarial perturbations in a low-dimensional embedding space and applying NES for optimization. TREMBA utilizes a low-dimensional embedding space to generate adversarial perturbations for a target network using NES. This approach captures high-level semantic features from the source model, allowing for transferability across different models and increased query efficiency. Unlike previous works, TREMBA focuses on black-box attacks and does not require dynamic distillation for generator training. Our approach utilizes a generator trained as an encoder-decoder to create a low-dimensional embedding space for efficient adversarial perturbations. Unlike AutoZOOM, which only reconstructs inputs, our method takes advantage of pretrained networks for better performance in attacking regular networks. Our new method, TREMBA, utilizes a pretrained source network to generate adversarial perturbations efficiently for attacking target networks. It produces perturbations with high-level semantic features, significantly reducing the number of queries needed. TREMBA is effective across different networks, especially for targeted attacks with low transferability, and can be applied to state-of-the-art defended models. TREMBA is a new method for generating adversarial perturbations efficiently to attack target networks. It increases success rate by 10% and reduces queries by over 50% compared to other black-box attacks. The white-box attack requires full knowledge of the target model and various methods have been proposed to find adversarial examples with improved success rates. Generators can create adversarial noises successfully, and black-box attacks can be transfer-based, score-based, or decision-based. Transfer-based methods transfer adversarial noises from a source model to a target network, while score-based attacks query the target network's output scores. AutoZOOM aims to enhance query efficiency by reducing the sampling space. AutoZOOM aims to improve query efficiency by reducing sampling space with a bilinear transformation or autoencoder. Different attack methods include decision-based attacks where the attacker only knows the output label, and boundary attacks are powerful in this setting. Some teams combined transfer-based and decision-based attacks in the NeurIPS 2018 Adversarial Vision Challenge. N Attack used a regression network for initialization in score-based attacks. Defense strategies can also use gradient information from surrogate models to accelerate scored-based attacks. Defense methods for neural networks include gradient masking based methods, adversarial training, and feature denoising to improve robustness. Adversarial training involves a minimax game where the inner maximizer finds perturbations to attack the network using methods like FGSM, PGD, and adversarial generator. Feature denoising has shown to enhance neural network robustness on ImageNet. Feature denoising can enhance neural network robustness on ImageNet by finding small perturbations for un-targeted and targeted attacks. Adversarial perturbations are transferable across different DNNs, allowing white-box perturbations to be successful on black-box networks. Even if the perturbation fails to create an adversarial example, it can still serve as a good starting point. This paper introduces a method to train a generator using information from the source network F_s to improve black-box attacks on an unknown target network F_t. The generator consists of an encoder E and a decoder D, generating adversarial perturbations \u03b4 to fool F_s. Training the generator aims to create perturbations that can deceive F_s. The paper introduces a method to train a generator for black-box attacks on a target network F_t. The generator minimizes the hinge loss for un-targeted attacks and uses a margin parameter \u03ba for targeted attacks. The focus is on the \u221e norm, with a point-wise tanh function applied to the output D(z) to bound the perturbations. Other loss functions like cross entropy could also be used. The approach is designed for a new black-box DNN classifier F_t, where only output queries are allowed. The paper presents a method for training a generator for black-box attacks on a target network F_t, focusing on the \u221e norm and using a point-wise tanh function to bound perturbations. The approach is tailored for a new black-box DNN classifier F_t, allowing only output queries. The method employs NES to approximate the gradient of a surrogate loss to find adversarial examples, updating \u03b4 iteratively with a learning rate \u03b7, minibatch size b, and gaussian samples \u03c9 k. Removing the sign function from the gradient has been shown to result in more effective attacks. In this work, the sign function is removed from the gradient calculation to accelerate the search process. The generator explores weaknesses in the source DNN to produce effective perturbations for attacking it. The method can also effectively attack a different unknown target network. The gradient in the embedding space z is estimated using NES, updating z with stochastic gradient descent. The approach does not require explicit projection as the perturbations already satisfy the specified constraints. After removing the sign function from gradient calculation to accelerate the search process, applying NES on the embedding space z can speed up the search for adversarial examples. Adversarial perturbations of TREMBA trained on the source network contain high-level semantic patterns likely to be adversarial patterns of the target network. Searching over z is like searching for adversarial examples in a lower-dimensional space with less concentrated distributions, making it easier to find effective adversarial patterns. The efficiency of TREMBA was evaluated on undefended networks in MNIST and ImageNet datasets. After removing the sign function from gradient calculation to accelerate the search process, applying NES on the embedding space z can speed up the search for adversarial examples. TREMBA's efficiency was evaluated on adversarially defended networks in CIFAR10 and ImageNet, as well as attacking Google Cloud Vision API to demonstrate its black-box model generalization capability. The hinge loss was used as the surrogate loss for un-targeted and targeted attacks, comparing TREMBA to four methods including NES and Trans-NES. AutoZOOM and P-RGF are two methods compared for attacking target networks. AutoZOOM utilizes an unsupervised autoencoder, while P-RGF is a prior-guided random gradient-free method. Different success rates among methods require efficiency comparisons at various levels. The average number of queries is adjusted to a unified upper limit for fair evaluation. The success rate of different attack methods was unified by setting an upper limit on the number of queries. The curve of success rate at various query levels was plotted to show detailed behavior. Only correctly classified images were considered for success rate and average queries. Momentum, learning decay, and fine-tuned learning rates were used for optimization. Hyperparameters and architectures of generators and classifiers were listed in appendices. The success rate of different attack methods was analyzed by setting a query limit. Four neural networks were trained on MNIST, achieving about 99% accuracy. The generator G was trained on ConvNet1* and tested on the MNIST test set. TREMBA showed a 50% reduction in queries compared to other attacks, with a slightly lower success rate than Trans-NES. P-RGF and Trans-P-RGF performed poorly on MNIST. TREMBA consistently achieved higher success rates at various query levels. The success rates of various adversarial attacks were evaluated on different neural networks using the ImageNet validation set. The attacks were tested on VGG19, Resnet34, DenseNet121, and MobilenetV2. The source model \u03b5 = 0.03125 was used for both targeted and untargeted attacks. Results for attacking class 0 (tench) are shown in Table 2 and Figure 2, with the average queries for TREMBA being around 1000. TREMBA achieves lower average queries compared to other methods for attacking different classes on ImageNet. The perturbations produced by TREMBA reflect high-level semantic patterns of the targeted class, making them easier to transfer. This effectiveness in searching for the target network is demonstrated through examples in the appendices. TREMBA outperforms other methods in attacking different ensembles of source models, showing better transferability with more source networks. The performance of TREMBA remains superior even when varying \u03b5 values. Additionally, the choice of sample size and dimension of the embedding space does not solely account for the performance gain of TREMBA. This section presents the results for attacking defended networks. The results for attacking defended networks show that TREMBA achieves higher success rates with lower number of queries compared to other methods. The defense models tested were for CIFAR10 and ImageNet, with successful transfer of adversarial perturbations to different architectures. TREMBA achieves higher success rates with fewer queries, showing a 10% improvement in success rate on ImageNet and reducing average queries by over 50% on CIFAR10. AutoZOOM outperforms Trans-NES on defended models, generating low-frequency adversarial perturbations that are effective against defended networks. However, TREMBA still produces better adversarial patterns. Optimizing the starting point for attacking defended networks can enhance effectiveness. The method TREMBA OSP, optimized for attacking defended networks, shows higher success rates at small query levels compared to TREMBA. It was also tested on the Google Cloud Vision API with \u03b5 = 0.05 for un-targeted attacks, achieving higher accuracy and lower query numbers. TREMBA is a novel method for generating adversarial patterns for unknown networks, showing improved success rates and fewer queries. TREMBA is a method for generating adversarial patterns for unknown networks, achieving significant improvements in black-box adversarial attacks on MNIST and ImageNet. It is especially efficient in targeted attacks and attacking defended networks, with a nearly 10% improvement in success rate and reduced query numbers. TREMBA combines transfer-based and score-based attack methods for higher efficiency in finding adversarial examples. TREMBA achieves high success rates in targeted attacks on ImageNet, outperforming other methods. It reduces the number of queries by more than half in various networks. Future work may involve combining TREMBA with conditional image generation methods for attacking multiple targeted classes. Searching in the embedding space of generator remains effective for black-box adversarial attacks on ImageNet models, showcasing examples of perturbations that reveal features of the target class. Different source ensemble models were evaluated, with successful targeted attacks shown for ImageNet using Trans-NES PGD and Trans-P-RGF methods. The study evaluated different ensemble source networks for targeted attacks on ImageNet, with Trans-NES PGD and Trans-P-RGF being the best variants. Results showed that TREMBA outperformed other methods in generalizing to different strengths of adversarial attacks. Hyperparameter sweep on Densenet121 indicated that b = 20 may not be the best choice for Trans-NES, but performance was not very sensitive to b for TREMBA. The study evaluated different ensemble source networks for targeted attacks on ImageNet, with Trans-NES PGD and Trans-P-RGF being the best variants. TREMBA outperformed other methods in generalizing to different strengths of adversarial attacks. Changes were made to the autoencoder architecture and the dimension of the embedding space, but the performance did not improve. Additionally, adjustments were made to the data-dependent prior of Trans-P-RGF, but the performance remained unchanged. The study compared different ensemble source networks for targeted attacks on ImageNet, with TREMBA outperforming other methods. Changes to the autoencoder architecture and embedding space dimension did not improve performance. An example of attacking Google Cloud Vision API is shown, where TREMBA successfully manipulated the classification of a shark. TREMBA required fewer queries than CombOpt, showcasing the improvement of combining transfer-based and score-based attacks. The architectures of ConvNet1, ConvNet2, and FCNet are listed in Table 12. The architectures of ConvNet1, ConvNet2, and FCNet are listed in Table 12. The architecture of ResNeXt used in CIFAR10 is specified with depth 20, cardinality 8, and widen factor 4. Generator architectures for three datasets are detailed in Table 13. Different batch sizes were used for different models, with \u03ba set to 200.0. Experiments were conducted using pytorch on NVIDIA RTX 2080Ti. Hyperparameters for all models are listed in Tables 14 to 19. The hyperparameters for all algorithms were fine-tuned using pytorch on NVIDIA RTX 2080Ti. A sample size of b = 20 was set for fair comparisons. Hyperparameters for TREMBA are listed in Table 18."
}