{
    "title": "B1EA-M-0Z",
    "content": "Recently, kernel functions have been developed to mimic multi-layer random neural networks, but only outside of a Bayesian framework. These kernels can be used as covariance functions for Gaussian processes, enabling fully Bayesian prediction with deep neural networks. In this work, the exact equivalence between infinitely wide, deep networks and Gaussian Processes (GPs) with a specific covariance function is derived. A computationally efficient pipeline is developed to compute this covariance function. The resulting GP is used for Bayesian inference in deep neural networks on MNIST and CIFAR-10 datasets. Trained neural network accuracy approaches that of the corresponding GP as layer width increases, and GP uncertainty is correlated with network prediction error. Test performance improves as finite-width networks become more similar to a GP, with GP-based predictions typically outperforming those of finite-width networks. The prior distribution over weights and variances in the GP formulation is connected to signal propagation in random neural networks. In BID17, an equivalence between deep neural networks and Gaussian processes was derived for one layer networks in the limit of infinite width. Neal (1994a) suggested a similar correspondence for deeper networks. In the limit of infinite width, the function computed by a deep fully-connected neural network is drawn from a Gaussian process. The kernel of this Gaussian process is well-defined for single hidden-layer networks. In the case of single hidden-layer networks, the kernel of the Gaussian process is well known. This allows for exact Bayesian inference for regression using neural networks by replacing an i.i.d. prior over weights and biases with a corresponding GP prior over functions. The correspondence between deep and wide neural networks and GPs is utilized for Bayesian training on regression tasks, touching on aspects of GPs, Bayesian learning, and compositional kernels. The correspondence between infinite neural networks and Gaussian Processes (GPs) was first noted by BID17. BID25 computes analytic GP kernels for single hidden-layer neural networks with error function or Gaussian nonlinearities, highlighting the use of GP priors for exact Bayesian inference in regression. BID6 discusses building deep GPs and observes degenerate forms of kernels composed infinitely. BID10 also explores constructing kernels equivalent to infinitely wide deep neural networks. Related work in compositional kernel constructions includes deriving kernels for polynomial rectified nonlinearities like Sign and ReLU, applicable in GPs. BID4 extends compositional kernels to neural networks with general DAGs, proving degeneracy in infinitely composed kernels from fully-connected topologies. BID19 and BID24 analyze recurrence relations in fully-connected networks with bounded nonlinearities, focusing on expressivity and trainability. In contrast to previous work on stacking GPs, our study focuses on GPs that correspond directly to deep, infinitely wide neural networks. Previous research has explored GP models with deep kernels, but our work highlights the equivalence between deep neural networks and GPs with compositional kernels. Our GP kernels are more general, allowing for a wider range of nonlinearities compared to previous approaches. Our work introduces the Neural Network GP (NNGP), which corresponds to a deep, infinitely wide neural network. It differs from previous works by utilizing GPs with fewer parameters that correspond to the hyperparameters of the neural network. The NNGP is able to handle general nonlinearities and incorporates additional hyperparameters in the kernels. The Neural Network GP (NNGP) is a computationally efficient method for deep neural networks with fixed hyperparameters. It focuses on exact Bayesian inference for regression tasks and compares to prediction by trained neural networks on tasks like MNIST and CIFAR-10 classification. Future work aims to extend GPs to softmax classification with cross entropy loss. The experiments conducted compare Bayesian predictions on MNIST and CIFAR-10 against NNs trained with gradient-based approaches. Results show that NNGP performance is competitive and surpasses conventional training. Increasing network width leads to similar performance between NNs and NNGP, with GP uncertainty strongly linked to prediction error. NNGP performance depends on various hyperparameters. The NNGP performance is strongly correlated with prediction error and depends on the structure of the kernel. The correspondence between GPs and deep neural networks is established, focusing on the Central Limit Theorem application. The text discusses single-hidden layer and multi-layer cases of fully-connected neural networks. The text discusses the correspondence between single-hidden layer neural networks and Gaussian processes, emphasizing the independence of weight and bias parameters. It also mentions the Central Limit Theorem application in the context of network output. The Central Limit Theorem implies that z 1 i (x) in a neural network will be Gaussian distributed in the limit of infinite width N 1 \u2192 \u221e. This leads to the conclusion that z DISPLAYFORM2, a Gaussian process with mean \u00b5 1 and covariance K 1, is independent of i. The parameters have zero mean, ensuring independence of z 1 i and z 1 j despite utilizing the same features from the hidden layer. This reasoning can be extended to deeper layers by induction. The section discusses extending the reasoning to deeper layers through induction, ensuring input to each layer is governed by a Gaussian Process. An alternative derivation using Bayesian marginalization is provided in Appendix C. Another work explores convergence towards a GP when all layers have infinite width simultaneously. The text discusses the recursive relationship between Gaussian Processes (GP) governing the network's output, emphasizing the iterative computations to obtain the final GP. The relationships between covariance matrices and a deterministic function are highlighted, allowing for the computation of the GP for the network's final output. Relations derived in mean field theory of signal propagation in random neural networks and compositional kernels. Equation 5 can be computed analytically for certain activation functions. GP prior over functions used for Bayesian inference, with distribution over functions constrained to training inputs. The text discusses the use of Gaussian Processes (GPs) for Bayesian training of deep neural networks, where the noise model consists of a Gaussian with variance \u03c3^2 centered at z. The predicted distribution for z* | D, x* is determined from matrix computations, corresponding to fully Bayesian training. The text discusses the use of Gaussian Processes (GPs) for Bayesian training of deep neural networks, where the covariance function is determined by the choice of GP prior, depending on depth, nonlinearity, and weight and bias variances. The construction of the covariance matrix for the equivalent GP involves computing Gaussian integrals for all pairs of training-training and training-test points recursively for all layers. Integration can be done analytically for some nonlinearities like ReLU, but for arbitrary nonlinearities, numerical computation is required. The algorithm for K L involves computing integrals independently for each pair of datapoints and each layer. By careful pipelining and preprocessing inputs to have identical norm, the cost can be improved. The process is broken down into several steps: generating pre-activations, variances, and correlations. The algorithm for computing K L involves generating a lookup table for the function F \u03c6, approximating Gaussian integrals, and computing K l (x, x ) for each pair of datapoints in layer l. The process is optimized by preprocessing inputs to have identical norm. The computational recipe allows for computing the covariance matrix for the NNGP with any nonlinearity \u03c6. This can be implemented using accelerated tensor operations, with the computation of K L being faster than solving linear equations. The kernel function computed numerically closely matches the analytical solution for the ReLU nonlinearity, showing the angular dependence and evolution with depth. The deep network kernel's shape and properties are solely determined by hyperparameters. The shape and properties of a deep network kernel are determined by hyperparameters. Gaussian Processes (GPs) provide exact marginal likelihood estimates, allowing for principled hyperparameter selection and nonlinearity design. An open-source implementation of the algorithm is available. NNGPs are compared with SGD trained neural networks on MNIST and CIFAR-10 datasets. Training involves mean squared error loss for direct comparison to GP predictions. Future work may involve evaluating NNGP on cross entropy. The NNGP often outperforms trained finite width networks, using ReLU or Tanh nonlinearities and a one-hot encoding for class labels. Performance comparisons can be seen in TAB0 and FIG0. Future work may involve incorporating dropout into the NNGP covariance matrix. The NNGP often outperforms finite width networks in neural network performance, especially with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 datasets are compared between the best performing NNGP and SGD trained neural networks. The best finite-width neural networks trained with a variant of SGD approach the performance of NNGP with increasing layer width, raising interesting points about the power of neural networks in flexible representation learning. In experiments comparing fixed basis functions in NNGP with SGD, no significant performance advantage was found. There is a potential link between SGD and Bayesian inference, with recent work suggesting SGD can approximate Bayesian inference under certain conditions. The similarity in performance between the widest neural network and NNGP indicates that wider networks may generalize better. Results from training fully-connected networks on CIFAR-10 show that wider networks tend to generalize better. In experiments with NNGP on CIFAR-10, uncertainty estimates are provided for predictions, which are highly correlated with prediction error. NNGPs can train effectively by converging to a fixed-point kernel, unlike conventional neural networks. In experiments with NNGP on CIFAR-10, uncertainty estimates for predictions are highly correlated with prediction error. NNGPs converge to a fixed-point kernel, showing distinct phases called \"ordered\" and \"chaotic\" related to weights and biases competition. The ordered phase results in similar features for dissimilar inputs due to common bias vectors, while the chaotic phase is dominated by weight variance leading to dissimilarity among similar inputs. In the chaotic phase, weight variance dominates, causing similar inputs to become dissimilar as they are randomly projected by weight matrices. The covariance function decays exponentially towards a fixed point form over a characteristic depth-scale \u03be. Initializing networks on a specific line allows for training significantly deeper neural networks. For ReLU networks, there are subtleties due to the unbounded nature of the nonlinearity, but the fixed-point covariance remains bounded. The ReLU nonlinearity exhibits two phases: a \"bounded\" phase with finite q* and an unbounded phase with either infinite or zero q*. Depth scales control convergence to fixed points, limiting trainable depth. NNGP performance closely follows the phase diagram, with high accuracy near criticality and poor performance far from criticality. Near criticality, models show high accuracy but accuracy drops quickly away from the phase boundary with increasing depth of the GP kernel. As the depth increases, the difference in information available to the model becomes too small to be represented accurately, leading to a degradation in test accuracy. By utilizing the limit of infinite width, a correspondence between priors on deep neural networks and Gaussian processes is established, enabling exact Bayesian inference for regression. The performance of deep neural networks in regression tasks is competitive with the best fully-connected models, even without stochastic gradient-based training. Experiments show that optimized neural networks approach the performance of Gaussian processes with increasing width. Further investigation is needed to determine if stochastic gradient-based optimization approximates Bayesian inference. The NNGP provides explicit estimates of uncertainty, useful for predicting model failure in critical applications of deep learning or for active learning tasks. A DRAWS FROM AN NNGP PRIOR FIG5 illustrates the nature of the GP prior for the ReLU nonlinearity. The angular structure of the kernel and its evolution with depth is also illustrated, showing good agreement between computed and analytic forms. In this section, an alternate derivation of the equivalence between infinitely wide deep neural networks and Gaussian processes is presented. The angular structure of the kernel for the ReLU nonlinearity is illustrated, showing a flattening with depth increase as predicted. Good agreement is observed between the computed and analytic forms of the kernel. In this alternate derivation, the equivalence between infinitely wide deep neural networks and Gaussian processes is explored by marginalizing over intermediate layers. Weight and bias parameters are drawn from independent Gaussians. The distribution p(z L |x) over network outputs is sought, with intervening layers having width N l. The second moment matrix for each layer is defined, and intermediate random variables corresponding to these second moments are considered. The pre-activations z l are described by a Gaussian process conditioned on the second moment matrix K l. The Gaussian process for each layer is exact even for finite width N l, justified by drawing parameters from a Gaussian. The graphical model in Figure 7 depicts the network's computation. The joint distribution can be decomposed, and as N l grows large, p K l |K l\u22121 converges to a Gaussian with shrinking variance. In the infinite width limit, it becomes a delta function. In Section 3, experiments were conducted using MNIST and CIFAR-10 datasets with specific data splits. Hyperparameters for training neural networks were optimized through random search. Learning rate, weight decay constant, \u03c3 w, \u03c3 b, and mini-batch size were varied within specified ranges. For Gaussian processes, a grid of 30 was used for depth and nonlinearity. For NNGP experiments, computation times were reported. Grid generation took 440-460s with 6 CPUs, constructing K DD for each layer took 90-140s, and solving linear equations via Cholesky decomposition took 180-220s for 1000 test points. Default values for target noise \u03c3 2 were set and increased by a factor of 10 when needed. BID21 was referred to for standard GP regression implementation. The relationship between target MSE and GP uncertainty estimate for smaller training set sizes is shown in Figure 8."
}