{
    "title": "SJlKrkSFPH",
    "content": "Formal verification techniques for machine learning models, such as randomized smoothing, offer provable guarantees on properties like robustness to adversarial perturbations. Randomized smoothing scales well to large architectures and doesn't require knowledge of the model's internal structure. This paper introduces a general framework for proving robustness properties of smoothed machine learning models in a black-box setting. In the black-box setting, a methodology using randomized smoothing procedures with arbitrary measures proves the robustness of machine learning models. This approach achieves state-of-the-art certified robustness on various datasets and tasks against adversarial perturbations. Despite efforts to improve adversarial robustness, models previously considered robust have been compromised by stronger attacks. Recent research has highlighted the vulnerability of supposedly robust models to stronger attacks, leading to the development of methods that provide provable guarantees against misclassification. While progress has been made in computing provable guarantees for image and text classification tasks, these methods require extensive knowledge of the predictor's architecture and are not easily adaptable to new models. Additionally, the computational complexity of these methods increases significantly with input dimension and model size. Recent research has shown that the computational complexity of methods for verifying classifier robustness grows significantly with input dimension and model size. To address this issue, a randomized smoothing strategy has been proposed, where the robustness of classifiers can be more easily verified by using a smoothed version of the base classifier. This smoothed classifier involves taking a \"majority vote\" over predictions on random inputs drawn from a probability distribution. Verifying the robustness of the smoothed classifier is simpler than verifying the original classifier and only requires estimating the distribution of outputs under random inputs. The text discusses a black-box verification framework for classifier robustness. It introduces a generalized randomized smoothing procedure for black-box verification, improving upon previous results. The framework allows for obtaining robustness certificates by solving a small convex optimization problem. The certificates generalize previous results and can characterize adversarial perturbations via divergence-based bounds on the smoothing measure. The text introduces a generalized randomized smoothing procedure for black-box verification, improving upon previous results. It evaluates the framework experimentally on image and classification tasks, obtaining robustness certificates that improve upon other black-box methods. The text introduces a certifiably robust model for audio classification tasks, specifically for Librispeech with variable-length inputs. It focuses on investigating the robustness of a smoothed classifier against adversarial perturbations of different sizes. The text discusses the challenges of norm-bounded adversarial attacks on a fixed input x \u2208 X with h s (x) = +1. It proposes studying the attack in the space of probability measures over inputs, P(X), as an alternative approach to address the non-convex optimization problem. The text discusses studying norm-bounded adversarial attacks on a fixed input x \u2208 X with h s (x) = +1 in the space of probability measures \u03bd \u2208 P(X) subject to divergence constraints. The relaxed problem can be efficiently solved for various divergences, including f-divergences. The general verification problem involves a specification \u03c6 : X \u2192 Z \u2286 R and a reference measure \u03c1, with perturbed distributions D. The text introduces two certification problems for adversarial robustness: information-limited robust certification and full-information robust certification. In the former, only access to a reference distribution and probabilities is given, while in the latter, full access to the specification is provided. The text introduces information-limited robust certification for perturbed distributions, where guarantees need to hold for a class of specifications. This task can be efficiently solved using convex optimization for perturbation sets specified by an f-divergence bound. The framework developed in this study provides a general method for robust certification in both information-limited and full-information scenarios. It can recover certificates from prior works and outperforms them in the information-limited setting. Additionally, it can utilize full-information to provide tighter certificates for smoothed probabilistic classifiers. The study's framework offers a method for robust certification in various scenarios, including information-limited and full-information settings. It aims to provide tighter certificates for smoothed probabilistic classifiers by verifying the robustness of predictions to perturbations. The study introduces a method for robust certification of smoothed probabilistic classifiers by verifying robustness to perturbations. It discusses specific relaxations of the set D x, focusing on constraint sets defined by f-divergences. These divergences have useful properties and include well-known instances like relative entropy and total variation. The paper introduces a method for robust certification of smoothed probabilistic classifiers by verifying robustness to perturbations. It discusses relaxations of the set D x defined by f-divergences, allowing for tighter constraints using multiple divergence-based functions. The paper introduces a method for robust certification of smoothed probabilistic classifiers by verifying robustness to perturbations using various divergences such as R\u00e9nyi and KL. The framework can be applied with any smoothing measures and f divergences, with a focus on Hockey-Stick divergences for optimal certificates in the information-limited setting. The paper discusses obtaining bounds on f-divergences for different classes of smoothing measures, including product measures and norm-based smoothing measures. It highlights the use of R\u00e9nyi-divergences and Lagrangian relaxation for optimization problems in estimating upper bounds efficiently. The paper discusses reducing robust blackbox certification problems to convex optimization for constraint sets defined by f-divergences. Theorems provide the foundation for verification procedures, showing that robust certification is achieved through non-negative convex optimization. The paper discusses reducing robust blackbox certification problems to convex optimization for constraint sets defined by f-divergences. The dual of the verification optimization problem has the desired form, with the special case reducing to Proposition 1 of Duchi & Namkoong (2018). To build a practical certification algorithm, optimal values of \u03bb and \u03ba must be computed, and the expectation in Eq. 6 estimated using sampling due to the black-box nature of \u03c6. The paper discusses reducing robust blackbox certification problems to convex optimization for constraint sets defined by f-divergences. To build a practical certification algorithm, optimal values of \u03bb and \u03ba must be computed, and the expectation in Eq. 6 estimated using sampling. The estimation involves computing high-confidence lower and upper bounds on the objective function, with details of the subroutines provided in the appendix. Theorem 2 discusses verifying information-limited robust certification for specifications S at \u03c1 with respect to D F. It involves a convex optimization problem with probability distributions \u03b8 and \u03b6. The proof is based on computing the expectation in Eq. 6, leading to a certification algorithm presented in Algorithm 2. The algorithm in Algorithm 2 computes a high-confidence lower bound on the probability of the correct class under randomized smoothing. It uses a convex optimization problem and can be solved using an off-the-shelf solver for the general case. Closed-form solutions are available for specific cases. Inputs include query access to classifier, correct label, sampling access to reference distribution, divergences, bounds, sample sizes, and confidence level. The method determines the most likely label produced by the classifier and obtains a lower bound on the probability of outputting the correct class with confidence. Theoretical results show that for smoothed probabilistic classifiers, the full-information certificate is superior to the information-limited one. By defining the f-divergence relaxation using specific parameters, the computed certificate is proven to be tight. A soft binary classifier H predicts label +1 for x with H(x) > 1/2, and the smoothed classifier Hs predicts label +1 if \u03c6 is full-information robustly certified at \u00b5(x) with respect to Dx. Theorem 1 shows that the full distribution is crucial for robustness certification. Comparing full-information and information-limited approaches, the latter provides a weaker certificate for soft classifiers. The optimization problem in Eq. 6 yields a higher optimal value than that in Eq. 7. The ideal certification of robustness with respect to specific sets is shown to have a gap from the tractable solution. The gap between ideal D x and tractable constraint sets D F can be closed in information-limited robust certification by measuring hockey-stick divergences of every non-negative order \u03b2 \u2265 0. The optimal information-limited certificate can be obtained by applying theorem 2 to D HS. Table 1 summarizes differences in smoothing measures, offline computation cost, perturbations considered, and use of information beyond \u03b8 a, \u03b8 b. The optimal certificates for hard classifiers smoothed by Gaussian noise can be computed efficiently using hockey-stick divergences. The certificates can be improved by considering perturbations beyond \u03b8 a, \u03b8 b, and computing optimal certificates for a given smoothing measure in the information-limited setting. The certificates for hard classifiers smoothed by Gaussian noise can be efficiently computed using hockey-stick divergences. Lee et al. (2019) derived optimal certificates in the information-limited setting, but their framework has limitations in accommodating natural smoothing measures like Gaussian or Laplacian measures. The complexity of computing the certificates in their framework is significant, requiring O(d^3) computation. Our framework can derive tighter certificates for arbitrary classifiers in the full-information setting. Li et al. (2019) derived robustness certificates for classifiers smoothed by Gaussian or Laplacian noise under 2 or 1 perturbations, which can be seen as special cases of our results. Lecuyer et al. (2019) introduced pixel differential privacy (pixelDP) and demonstrated smoothing measures satisfying pixelDP with respect to certain perturbations. Smoothing measures satisfying pixelDP lead to adversarially robust classifiers. PixelDP can be seen as a special instance of a certification framework with specific hockey-stick divergences. Certificates from pixelDP are dominated by certificates from the framework. Training a ResNet-152 model on ImageNet with data augmentation using Gaussian noise for certification. Full-information certificates use two hockey-stick divergences. Our approach tunes parameters to optimize the certification problem, reducing to a previous method for information-limited certificates. A comparison on 50 examples shows stronger certificates from full information verification. Each blue dot in Figure 2 represents a test point, with x and y coordinates indicating different certification procedures. The full-information certification procedure takes .2s per example. The full-information certification procedure takes .2s per example, while the limited-information certification takes .002s per example. Both certificates incur the same sampling cost and the full-information certificate is always stronger. For example, the full-information method can certify robustness to 2 perturbations of radius = 9.42, while the limited-information method can only certify for perturbation radius = 2.69. In comparison to Lee et al. (2019), our framework explores scalability and tightness trade-offs. Certificates were computed using different models and parameters for Binary MNIST and ImageNet datasets. Certification procedures were run on all test examples for Binary MNIST and every 100th example from the validation set for ImageNet. The proportion of examples with certified accuracy is reported in Table 2. Building robust audio classifiers against adversarial attacks is challenging due to the complexity of audio processing architectures. A new approach shows promise in certifying the robustness of a speaker recognition classifier by focusing on perturbations that zero out parts of the audio sample. This method addresses missing data issues in audio signals caused by recording errors or network transmission problems. The existing method for computing robustness certificates is computationally expensive and impractical for real-time applications. The study presents results on certifying the robustness of an audio classifier for speaker recognition, using a new approach that focuses on perturbations in audio samples. The classifier was tested on a dataset of sentence utterances from ten speakers, with a DeepSpeaker architecture trained for 50,000 steps. This is a significant advancement towards certifying classifiers in audio and those operating on variable-length inputs. The study introduced a new approach for certifying the robustness of an audio classifier for speaker recognition. The classifier, based on the DeepSpeaker architecture, was trained with the Adam optimizer for 50,000 steps. Different models were trained with varying smoothing values, and certification was performed using f-divergence constraints. The framework showed significant improvements in robustness certificates and computation time for both image classification and audio tasks. Our framework enables scalable computation of robustness verification for complex predictors and structured perturbations modeled using f-divergence constraints. The condition E X\u223c\u03bd [\u03c6 c,c (X)] \u2265 0 for all c \u2208 Y \\ {c} is equivalent to c \u2208 arg max y\u2208Y P X\u223c\u03bd [h(X) = y]. This proves the required robustness certificate for soft classifiers like the soft-max layer of a neural network. Our methodology provides robustness guarantees for smoothed soft classifiers by applying a smoothing measure to the input. The set of top labels remains unchanged when moving the input within a certain range, ensuring robustness of the classification rule. Our methodology ensures robustness of classifiers by allowing them to abstain if the score gap is below a certain threshold \u03b3. This certification guarantees that the classifier will not abstain and will return the label with the highest score. The paper utilizes properties of f-divergences, which can be defined for any convex function f, to support its claims. Further details and proofs can be found in referenced sources. The optimization problem involves a convex function f satisfying certain requirements. The Lagrangian relaxation is formed to maximize separately over each variable, leading to an upper bound that can be minimized to obtain the tightest bound. The minimization for each variable can be solved in closed-form or through simple 1-dimensional minimization for most smoothing measures. In optimization, the problem is rewritten in terms of the likelihood ratio and can be solved using Lagrangian duality. The optimal value determines if the specification is robustly certified or not. The proof of correctness of the certificate Eq. 6 is concluded by restricting \u03bb \u2265 0 such that i \u03bb i = 1. When \u03c6 is ternary valued, the optimization over \u03ba, \u03bb can be written as max. The constraints can be rewritten in terms of \u03b6 = \u03b8 \u03b3 to ensure \u03b6 is a probability distribution over {+1, 0, \u22121}. In this section, closed-form certificates for the information-limited setting are presented, derived from Theorem 2 for M = 1. Certificates for various f-divergences are provided, including Hockey-Stick and R\u00e9nyi divergences. The R\u00e9nyi divergences are not proper f-divergences, but are defined differently. The infinite R\u00e9nyi divergence is defined as the limit \u03b1 \u2192 \u221e of the divergence function. Certificates for the divergence depend on the gap between \u03b8 a and \u03b8 b. The certificate for the smoothed hard classifier takes a specific form. The objective function is optimized by setting \u03ba to its upper bound \u03bb \u2212 1. The derivative with respect to \u03bb has different values based on \u03b2\u03b8 a. The certificate is non-negative only under certain conditions. The certificate for the infinite R\u00e9nyi divergence is derived by considering cases where \u03b1 \u2265 1 and \u03b1 \u2264 1 separately. By setting the derivative with respect to \u03bb to 0 and solving for \u03bb, the optimal certificate is obtained. The certificate simplifies to a specific form under certain conditions, with constraints on \u03ba to ensure positivity. The proof of Theorem 4 demonstrates that in the information-limited scenario, robust certification under constraints D can be achieved by knowing the envelope of D with respect to hockey-stick divergences of order \u03b2 \u2265 0. This involves optimizing an expression over a certain range of values for \u03b3. The optimization problem Eq. 12 can be rewritten in vector form in R3. The optimization over \u03a8 can be replaced with the convex hull of \u03a8 satisfying the constraints. The problem reduces to a convex optimization problem in \u03a8. Minimizing each term independently for each x \u2208 X is possible. The optimization problem can be rewritten in vector form in R3 and the convex hull of \u03a8 can be used to minimize each term independently for each x \u2208 X. The Lagrangian is analyzed in two cases, leading to the dual problem formulation. Strong duality ensures that the optimal value matches the original problem, indicating robust certification with respect to D if a certain condition is met. Limited robust certification with respect to D holds if Eq. 15 has a non-negative optimal value for each \u03bd \u2208 D. This is equivalent to information-limited robust certification with respect to D HS, where the optimal value depends on the Hockey-stick divergences. The convex optimization problem must have a non-negative optimal value, coinciding with theorem 2 applied to the constraint set D HS defined by these divergences. The proof of Theorem 4 shows that the optimal limited-information certificate problem can be solved for any constraint set defined by f-divergences. Leveraging results from Balle & Wang (2018), Corollary 5 demonstrates information-limited robust certification for a Gaussian measure. This is achieved by solving a specific equation and setting the derivatives to 0. The equivalence between Corollary 5 and the optimal certification in (Cohen et al., 2019, Theorem 1) is established by setting derivatives to 0 with respect to \u03bb a, \u03bb b, and ensuring non-negative optimal solution. Pixel differential privacy (pixelDP) introduced in Lecuyer et al. (2019) uses a similarity measure between distributions for (\u03b5, \u03c4)-pixelDP certification. Lecuyer et al. (2019) introduced pixel differential privacy (pixelDP) using a smoothing measure \u00b5 for adversarially robust classifiers against perturbations. Their result aligns with the framework where D \u03b5,\u03c4 can be expressed as D F, providing limited-information black-box certification for smoothed classifiers. The certificate obtained for the relaxation of D \u03b5,\u03c4 improves on previous certificates. The optimal certificates for the constraint set D from Theorem 2 are stronger than those from Eq. 21. Lemma 8 discusses a smoothing measure \u00b5 and convex functions. The optimization problem in Eq. 22 can be solved in closed form for several norms, including 1, 2, \u221e norms, and matrix spectral norm. Every f-divergence meeting the conditions of Lemma 8 can be efficiently estimated for these norms, providing a flexible class of f-divergences. Efficient sampling methods exist for estimating f-divergences for various norms, including 1, 2, and \u221e norms. Sampling from the log-concave measure \u00b5(x) can be done using polynomial time algorithms, with more efficient methods available for most norms. Sampling from \u00b5(x) reduces to sampling from \u00b5(0), which can be achieved easily for norms like 1 and \u221e. Theorem 9 provides a proof of correctness for an efficient sampling procedure involving sampling from a Gamma distribution. Theorem 10 also offers a similar result for a specific case, and Table 5 lists sampling procedures for various norms. The empirical Bernstein bound is derived, showing that the convergence rate can be O(1/N) instead of the standard O(1/\u221aN) under certain conditions. The text discusses using an empirical Bernstein bound to approximate expectations with high probability. This bound can be applied to approximate a function to be maximized for given values of \u03bb and \u03ba. Algorithm 3 provides details on these procedures, utilizing a convex optimization solver in the ESTIMETEOPT subroutine. The text discusses using an empirical Bernstein bound to approximate expectations with high probability. Algorithm 3 utilizes a convex optimization solver in the ESTIMETEOPT subroutine to return \u03ba * , \u03bb * . Additionally, discrete perturbations can be handled in the framework, with the ability to extend to structured discrete perturbations by introducing coupling terms between the perturbations. The text discusses using an empirical Bernstein bound to approximate expectations with high probability. Algorithm 3 utilizes a convex optimization solver in the ESTIMETEOPT subroutine to return \u03ba * , \u03bb * . Additionally, discrete perturbations can be handled in the framework, with the ability to extend to structured discrete perturbations by introducing coupling terms between the perturbations. The certificates are compared to Lecuyer et al. (2019) on MNIST, CIFAR-10, and ImageNet datasets using a Markov Chain approach. Hyperparameters for training and certification on each dataset are described, including a three-layer CNN ReLU classifier for MNIST with specific training and certification parameters. The text discusses using empirical Bernstein bounds to approximate expectations with high probability. Training and certification hyperparameters for MNIST, CIFAR-10, and ImageNet datasets are described, including specific values for N, \u00d1, \u03b6, smoothing values, batch sizes, learning rates, and training steps for different classifiers. Certified accuracy results for MNIST, CIFAR-10, and ImageNet datasets are reported, showing significant outperformance compared to previous studies."
}