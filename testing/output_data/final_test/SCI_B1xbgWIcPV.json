{
    "title": "B1xbgWIcPV",
    "content": "The paper proposes a deductive learning approach for action planning models based on domain-specific knowledge from logic formulae. This approach requires minimal input observability and aims to learn high-quality action models by leveraging domain knowledge to constrain the space of possible models. Previous approaches to learning action models have been data-intensive and relied on a large volume of training samples. The paper proposes a deductive learning approach for action planning models based on domain-specific knowledge from logic formulae. This approach aims to learn high-quality action models by leveraging domain knowledge to constrain the space of possible models. Unlike data-intensive approaches, this method requires minimal input observability. The Simultaneous Learning and Filtering (SLAF) approach BID2 utilizes logical inference to build a complete explanation through a CNF formula representing the initial belief state and plan observation. The formula is updated with every action and state of the observation, extracting satisfying models with a SAT solver. Another recent approach focuses on learning action models from plan observations as a planning task, determining preconditions and effects of action models. This paper explores the impact of using mixed input data in learning action models, specifically focusing on the extreme case of having a single observation sample. It aims to determine if the lack of training samples can be overcome with domain knowledge, considering the difficulty and cost of obtaining enough training observations in some domains. The study aims to compare the informational power of domain observations with the representational power of domain-specific knowledge. It focuses on one-shot learning with domain knowledge to overcome the lack of training samples. The approach involves using a planning-based satisfiability framework to configure additional constraints and work with a minimal set of observations. The study focuses on one-shot learning with domain knowledge to address the lack of training samples. It utilizes a planning-based satisfiability framework to incorporate constraints and work with minimal observations. The task-solving process involves defining states with positive and negative literals, planning actions with preconditions and effects, and specifying action semantics with applicability and successor state functions. The text discusses the execution of actions in a planning problem, where the initial state and goal conditions are defined. It explains how applying an action in a state results in a new state, with positive and negative effects. A plan is an action sequence that induces a trajectory from the initial state to the goal state. The text discusses the execution of actions in a planning problem, where a plan induces a trajectory from the initial state to the goal state. It explains the baseline learning approach using actions with conditional effects, and the learning of STRIPS action models through a compilation scheme. The text discusses the definition of a STRIPS action model \u03be, which includes the name, parameters, preconditions, negative effects, and positive effects. The Baseline Learning System (BLS) creates a propositional encoding of the planning task P based on observations of plan executions. The set of propositions that can appear in the action model are FOL interpretations of predicates shaping the variables. For example, in a blocksworld scenario, the set contains elements like handempty, holding(v1), clear(v1), ontable(v1), and on(v1, v1). The text discusses determining elements of a STRIPS action model \u03be that shape preconditions, positive and negative effects of each action model. The decision on preconditions, negative effects, and positive effects is based on the plan that solves task P. Two sets of actions are defined in P: insert actions and apply actions. In the blocksworld domain, insert actions in a plan solving P will include actions like (insert pre stack holding v1), (insert eff stack clear v1), (insert eff stack clear v2). The BLS compilation outputs a plan completing the empty input domain model by specifying preconditions and effects of each action model. The one-shot learning task involves learning action models from domain-specific knowledge. The solution space of a learning task involves analyzing STRIPS action models with initial empty models, learning examples, and domain-specific knowledge. The task aims to complete input domain models by specifying preconditions and effects of each action model. The space of possible STRIPS schemata is constrained by syntactic and observation constraints. Syntactic constraints ensure consistency with del(\u03be) \u2286 pre(\u03be), del(\u03be)\u2229add(\u03be) = \u2205, and pre(\u03be)\u2229add(\u03be) = \u2205. Observation constraints require the solution to be consistent with semantic constraints derived from learning samples. This limits the space of possible action models. The size of the space of possible STRIPS models is 2 2\u00d7|I \u03a8,\u03be | due to elements appearing in both preconditions and effects. The belonging of an element to the preconditions, positive effects, or negative effects of an action is encoded using two types of fluents, pre \u03be e and eff \u03be e. This compact encoding allows for effective exploitation of syntactic constraints. Domain-specific knowledge in the form of state constraints further restricts the space of action models. In the blocksworld domain, certain constraints like a block cannot be on top of itself are applied. State constraints are general and have been utilized in various contexts. State constraints in planning are abstract representations that relate variable values in each state of a plan. State invariants, a type of state constraint, help compute more compact state representations for planning problems and improve efficiency in satisfiability planning. A mutex is a binary clause that serves as a state invariant. In planning, state invariants like mutexes are binary clauses that represent mutually exclusive properties of objects, aiding in efficient planning by identifying constraints on variable values. Schematic mutexes are domain-specific knowledge that help complete partially observed states and prune possibilities. Schematic mutexes aid in completing partially observed states and pruning inconsistent action models. They are defined as pairs of predicates that satisfy specific formulae, helping to identify constraints in planning tasks. In this section, schematic mutexes are used to complete partial states in observations by adding new literals. The set of schematic mutexes \u03a6 is applied in a pre-processing stage to extend the state observation with new literals. The goal is to learn action models consistent with the schematic mutexes in \u03a6. The schematic mutexes in \u03a6 ensure consistency of newly generated states induced by learned actions. New conditional effects are added to the insert and apply actions of the BLS compilation to prevent inconsistencies. These effects prevent insertion of schematic mutex preconditions and ensure consistency with input observations. In theory, conditional effects of type 1-5 are added to insert and apply actions in the BLS compilation to ensure consistency with input observations and prevent inconsistencies. This guarantees that all states traversed by a plan are consistent with the input set of schematic mutexes \u03a6. In the BLS compilation, conditional effects are added to insert and apply actions to maintain consistency with input observations and prevent inconsistencies. Once a precondition or effect is inserted into the domain model M, it cannot be undone. Actions in the planning task P can only update state fluents using (apply) \u03be,\u03c9 actions. A solution for the learning task \u039b = M, O, \u03a6 can be computed with a classical plan \u03c0 that solves P. The size of the compilation in the BLS method is mainly determined by the elements captured in the action model using predicates. The compilation ensures that all models satisfying the mutexes are preserved. The size of the compilation is influenced by the arity of predicates, variables, and parameters of action models. The larger these factors, the larger the compilation size. The compilation process in classical planners aims to reduce the size of the output by constraining FOL interpretations and introducing biases towards shorter solution plans. This bias can be eliminated by defining a cost function for actions, but in practice, a different approach is used to disregard the cost of insert actions. In practice, a SAT-based planner is used to optimize plan cost with zero-cost actions by applying all actions for inserting preconditions and effects in a single planning step. The plan horizon is always bounded to 2, and this approach is effective in handling dead-ends and symmetries in action model insertion. The evaluation of models for reproducibility involves using IPC domains from the PLANNING.DOMAINS repository. Learning examples are generated via random walks with a new parameter, \u03c3, indicating observability degree. Experiments are conducted on an Intel Core i5 with 16 GB RAM. Evaluation metrics include precision and recall for action models. Source code, scripts, benchmarks, and input state-invariants are available at https://github.com/anonsub/oneshot-learning. In the evaluation of models for reproducibility, precision and recall metrics are used to compare learned models against a reference model. Precision measures correctness, while recall measures completeness. The first experiment aims to determine if plan observation can be replaced by domain knowledge. Four settings are evaluated, starting with minimal observability as the baseline. The evaluation compares four settings, showing that observability is more informative than domain knowledge. Minimal observability with no \u03a6 (setting 1) has marginal gains compared to using only domain knowledge (setting 2). However, observability (setting 3) outperforms domain knowledge, and using \u03a6 under minimal observability (setting 1) shows a significant improvement over observability alone (setting 3). The experiment shows that using \u03a6 significantly improves learned models, regardless of the degree of observability in the learning examples. The results indicate that observability is more beneficial than domain knowledge, and using \u03a6 enriches both observations and the learning process, leading to better models. The quality of action models learned with limited observable learning examples can be comparable to those with complete examples when domain knowledge is utilized. Introducing domain-specific knowledge, such as schematic mutexes, can improve the performance of the learning system by narrowing down the search space and offsetting the lack of learning examples. Authors found that the number of trajectories needed scales gracefully with the number of predicates and actions, showing that learning accurate models depends on the quality of learning examples. Their proposal relies on easily deducible domain knowledge, allowing learning from non-fully observable examples. Learning from a 30%-observable example with domain-specific knowledge is comparable to learning from a complete plan observation."
}