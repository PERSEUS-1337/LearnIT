{
    "title": "HJz6QhR9YQ",
    "content": "Deep Reinforcement Learning algorithms can solve complex decision-making problems, but struggle with difficult multi-agent competitive games like real-time strategy games. Current self-play algorithms have limitations when opponents are suboptimal, requiring a new learning approach. Hierarchical Agent with Self-play (HASP) is developed to create hierarchically structured policies that outperform conventional self-play in competitive games. Hierarchical Agent with Self-play (HASP) outperforms conventional self-play in competitive games by utilizing a diverse pool of sub-policies from Counter Self-Play (CSP). The ensemble policy generated by HASP achieves better performance against unseen opponents with sub-optimal policies, achieving a 77% win rate against FloBot, a top-ranked open-source agent. This approach shows promise in addressing the challenges posed by complex real-time strategic competitive games for deep reinforcement learning algorithms. In competitive games like Go, finding Nash Equilibrium solutions is crucial, but real-world opponents may not always follow these solutions. Human players excel at analyzing strategies and exploiting opponents' weaknesses, even if their own strategies can be exploited. Exploitation plays a central role in sports. Exploration of game-playing strategies that learn to exploit opponents' suboptimality is a promising research direction for more human-like artificial agents. A new algorithm, Hierarchical Agent with Self-Play, focuses on learning a wider variety of behaviors by avoiding equilibrium solutions. The algorithm is designed for two-player, symmetric, extensive form games of imperfect information, with potential for generalization to more players and asymmetric games. The Hierarchical Agent with Self-Play algorithm aims to learn strong strategies for multi-player games by automatically acquiring diverse behaviors through self-play. This approach has been successful in handling complex games like backgammon, Go, Poker, continuous control, and modern video games. The algorithm combines multiple exploitable policies into an ensemble model, drawing on the experience of sub-policies to respond effectively to opponents. The algorithm combines exploitable policies into an ensemble model to learn diverse strategies for multiplayer games. It outperforms conventional self-play, achieving a 77% win rate against strong opponents like FloBot in generals.io. Markov games with observations and actions for players are used to describe real-world multi-agent problems. The algorithm aims to find optimal strategies in multiplayer games by combining exploitable policies into an ensemble model. It outperforms conventional self-play and has shown success against strong opponents like FloBot in generals.io. Markov games are used to model real-world multi-agent problems. Self-play is a powerful technique in two-player, zero-sum games, often requiring learning and self-play for complex problems. Various game-theoretic techniques like fictitious play and counterfactual regret minimization, combined with deep reinforcement learning, have shown success in achieving Nash equilibrium. The combination of deep learning, planning, and self-play led to the creation of AlphaGo and AlphaZero agents. Recently, self-play has achieved success in complex games like Dota 2, proving challenging for current methods due to long-term decisions and large action spaces. Generals.io is proposed as a research environment similar to Dota 2 and Starcraft II, offering fast simulation and a large player community for evaluation. The game involves selecting grids and executing actions, with armies moving to adjacent grids after each action. In Generals.io, players control armies on a grid, with each turn generating more armies on taken cities and generals grids. The game involves moving armies between tiles, with players unable to see tiles not adjacent to their own. Players can freely move armies between their tiles, subtracting army from enemy tiles when moving into them. In Generals.io, players control armies on a grid, generating more armies on taken cities and generals grids each turn. The goal is to take ownership of the opponent's general. The game involves moving armies between tiles, with a fog-of-war mechanic requiring players to actively seek out information and learn strategies to defend, expand territory, and invest in cities. Players online employ various tactics, such as rushing opponents early on or staying small and hiding. In Generals.io, players use various strategies like rushing opponents early on, staying small and investing in cities, or expanding to suffocate opponents. Using Generals offers benefits over SC2, as bots in SC2 can access hidden information and cheat with short reaction times. Generals has a simple game board and a delay between turns to reduce the impact of reaction time. The python version of Generals interfaces with OpenAI Gym BID1, achieving over 1000 frames per second with a single GPU. Resources are available on the developer's website, including an API for online play. The work focuses on creating a hierarchically structured agent with strong and diverse sub-policies learned through self-play. The high-level policy in Generals is trained to choose from diverse sub-policies based on observations from a 12 by 12 map. The architecture and PPO algorithm used are detailed in Sections 3.1 and 3.3. The training framework adopts the Proximal Policy Optimization algorithm, which maximizes a surrogate objective while penalizing large policy changes. Sampling trajectories and performing SGD on the dataset are key steps in optimizing the surrogate objective. The policy network in Generals uses a fully convolutional architecture to maintain translation invariance and generalize similar events to different locations. Parameters are shared between the value network and policy network. Diverse and robust sub-policies are achieved through self-play, which requires occasional success by performing random actions. Winning a game of Generals by chance on a large map is challenging due to the large state and action space. To address the sparse signal problem in Generals, methods like dense rewards, imitation from demonstrations, and exploration curriculum are used. Imitating from demonstrations as a warm start helps overcome exploration issues, but struggles against decent agents. Conventional self-play with an \"achievement reward\" is used to generate diverse strategies initially. In Generals, methods like dense rewards, imitation from demonstrations, and exploration curriculum are used to address the sparse signal problem. Counter Self-Play is implemented to train agents against the most recent self for faster learning. Sub-policies are generated with a focus on \"style\" rather than \"strength\" to create diverse strategies. The current agent becomes the counter strategy to the previous agent, leading to the learning of high-level policy. The high-level policy is trained based on sub-policies that are best-responses to different opponents. The high-level policy selects a sub-policy at each step in the game and executes the action based on it. Unlike prior work, the sub-policies are fixed during training the high-level policy. The high-level policy can be computed by finding the pairwise expected payoffs of the sub-policies and solving a matrix game. The study involves testing the HASP algorithm against baseline methods in two competitive games to evaluate the diversity and effectiveness of sub-policies. The experiment aims to determine the impact of HASP on creating a stronger and more robust agent compared to conventional self-play methods. The comparison includes self-play with imitation warm start and a baseline that randomly selects past strategies. The study compares the HASP algorithm with baseline methods in competitive games to assess sub-policy diversity and effectiveness. HASP aims to enhance agent strength compared to traditional self-play methods. The game involves Rock-Paper-Scissors played over multiple turns, with rewards based on winning more turns than the opponent. Players may adopt strategies like countering opponent actions. Videos, codes, and additional information can be found at https://sites.google.com/view/hasp/home. The study compares the HASP algorithm with baseline methods in competitive games to assess sub-policy diversity and effectiveness in Rock-Paper-Scissors. HASP aims to enhance agent strength compared to traditional self-play methods. The algorithm results in ten different policies, which are projected onto a 2D plane using t-SNE. Linear Programming is used to find a Nash-equilibrium mixed strategy based on the learned sub-policies. More information is available at https://sites.google.com/view/hasp/home. The study compares the HASP algorithm with baseline methods in competitive games to assess sub-policy diversity and effectiveness in Rock-Paper-Scissors. Using PPO to learn stochastic policies proved challenging due to the tendency for best-responses to be pure strategies. Policies learned under self-play were close to deterministic and exploitable. HASP resulted in less exploitable policies closer to equilibrium compared to self-play runs. Randomly choosing a sub-policy also reduced exploitability, though not as effectively as HASP. In the study comparing the HASP algorithm with baseline methods in competitive games, PPO struggled to learn diverse policies due to best-responses being pure strategies. HASP produced less exploitable policies closer to equilibrium compared to self-play. Randomly choosing a sub-policy also reduced exploitability, though not as effectively as HASP. In Generals experiments, agents were initialized with behavioral cloning on FloBot, learning different styles and strategies to break symmetry and encourage faster game endings. In Generals, the HASP algorithm trained agents to prefer owning territory on the top half of the map. Phase 1 of HASP produced 6 diverse sub-policies with different playing styles. Testing the agent against these sub-policies showed meaningful diversity, with best responses yielding the highest reward. The HASP algorithm trained agents in Generals to prefer owning territory on the top half of the map. The final usage frequencies of sub-policies when facing different playing styles are shown in Table 5. The HASP agent outperformed baselines against held-out agents trained with additional rewards. In this paper, a novel learning approach called Hierarchical Agent with Self-Play (HASP) is investigated for learning strategies in competitive games. The method involves learning opponent-dependent sub-policies and has shown better generalization to unseen opponents compared to conventional self-play approaches. Additionally, HASP outperforms conventional self-play in learning optimal mixed strategies in simpler matrix games. While the method has achieved good results, there are areas for improvement in future research. In future research, we aim to improve performance on larger versions of Generals and investigate the effects of our algorithm on exploration with sparse reward."
}