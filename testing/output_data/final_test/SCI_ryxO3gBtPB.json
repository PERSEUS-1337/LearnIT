{
    "title": "ryxO3gBtPB",
    "content": "Model distillation involves distilling the knowledge of a complex model into a simpler one. This paper introduces dataset distillation, where the knowledge from a large training dataset is distilled into a small one by synthesizing a small number of data points. Experiments on various datasets show the advantage of this approach over alternative methods. The paper introduces dataset distillation as a method to distill the knowledge of a large training dataset into a small one by synthesizing a small number of data points. Unlike network distillation, where the model is distilled, dataset distillation keeps the model fixed and encapsulates the knowledge of the entire training dataset into synthetic training images. This approach has shown to be effective in alleviating forgetting in continual learning settings. Dataset Distillation is a method that compresses a large training dataset into a small number of synthetic images, maintaining good performance on these images. It raises questions about the amount of data encoded in a training set and the possibility of loading a network with dataset knowledge using only a few images. Dataset Distillation compresses large training datasets into a small set of synthetic images, enabling applications that require data compression. It can alleviate catastrophic forgetting in continual learning by storing distilled images as memory. The possibility of training an image classification model on synthetic images not on the natural image manifold is explored, showing it is indeed possible. Dataset Distillation compresses large training datasets into a small set of synthetic images for fast model training. By deriving network weights from distilled images, we can achieve high test accuracy with only a few data points. These distilled images efficiently store knowledge for continual learning and are optimized instead of network weights for training objectives. Our method generates distilled images for randomly initialized networks, optimizing pixel values to transfer knowledge efficiently. By reusing the same distilled images iteratively, we enhance performance. A few distilled images can achieve high model performance, even with fixed initialization. This approach is beneficial for fast model fine-tuning on pre-trained networks. Our approach involves model fine-tuning on various initialization settings and outperforms existing methods on multiple datasets. Storing our distilled data instead of past data samples as episodic memory proves to be more effective for continual learning. Knowledge distillation, inspired by network distillation, is a key aspect of our method. Our method aims to compress the knowledge of an entire dataset into a few synthetic data, distinct from network distillation and teaching dimension concepts. Various techniques like dataset pruning and core-set construction are used to distill knowledge into a small subset for model training. Various techniques like dataset pruning, core-set construction, and instance selection aim to select a subset of training data for model training. These methods aim to compress the knowledge of an entire dataset into a few synthetic data, distinct from network distillation and teaching dimension concepts. Our work focuses on dataset distillation, where we learn synthetic training data instead of tuning hyperparameters. This approach, not extensively explored before, allows our distilled images to perform well across random initialization weights. Our work focuses on dataset distillation, aiming to understand the intrinsic properties of training data rather than specific trained models. Unlike prior approaches, we distill full datasets into a few synthetic samples to create a much-reduced dataset that performs almost as well as the original. The text discusses an optimization algorithm for training a network with fixed initialization and random weights. It also explores a linear network case and the distribution of initial weights. The approach is extended to reuse distilled images over 2,000 gradient descent steps. Additionally, it discusses dataset distillation for different initialization distributions and the use of distilled images as episodic memory for continual learning tasks. The text discusses optimizing neural network training with fixed initialization and random weights using synthetic distilled training data. It explores using a tiny set of distilled images and a specific learning rate to speed up convergence, compared to standard training methods. The text discusses optimizing neural network training with fixed initialization and random weights using synthetic distilled training data. It explores using a tiny set of distilled images and a specific learning rate to speed up convergence, compared to standard training methods. The distilled data and optimized learning rate obtained from synthetic data greatly boost performance on the real test set by minimizing the objective function and deriving new weights. The text discusses optimizing neural network training with fixed initialization and random weights using synthetic distilled training data. It explores using a tiny set of distilled images and a specific learning rate to speed up convergence. The distilled data is optimized for a given initialization but does not generalize well to other initializations. To address this, a small number of distilled data is calculated to work for networks with random initializations from a specific distribution. During optimization, distilled data is tailored for randomly initialized networks, showing good generalization to unseen initializations. The distilled images capture discriminative features of each category. Experimentation with different initialization strategies reveals that using random real images yields better results than random initialization. A single gradient descent step is insufficient for proper learning of compact set distilled data. In the context of optimizing randomly initialized networks, distilled data is crucial for generalization. The study focuses on deriving a lower bound on the size of distilled data needed for a simple model with arbitrary initial parameters in one gradient descent step. The goal is to achieve the same performance as training on the full dataset. The formulation is applied to a linear regression problem with quadratic loss, aiming to learn synthetic data-target pairs to minimize the error. The study focuses on deriving a lower bound on the size of distilled data needed for generalization in optimizing randomly initialized networks. It considers a simple model with arbitrary initial parameters in one gradient descent step, aiming to achieve the same performance as training on the full dataset. The analysis suggests that any small number of distilled data fail to generalize to arbitrary initial parameters. The study analyzes the need for a lower bound on distilled data size for generalization in optimizing randomly initialized networks. It extends Algorithm 1 to multiple gradient descent steps to address the limitations of using one step. Different learning rates are used for each step, and back-gradient optimization is employed to reduce memory and computational intensity. In this work, back-gradient optimization is utilized for faster gradient calculation in reverse-mode differentiation. Different choices for the distribution of initial weights are explored, including random initialization, fixed initialization, and random pre-trained weights. In this work, the distribution of initial weights is explored, including random pre-trained weights. Domain mismatch and dataset bias are addressed by distillation with pre-trained weights, adapting CNN models to new datasets efficiently. Learning methods store a subset of training samples in a small memory buffer to maintain performance on stored samples. Distilled images serve as a compressed memory of past training data. The Gradient Episodic Memory (GEM) method enforces constraints for new models to perform well on old data. Distilled data is used to construct constraints for optimization. Our method compares favorably against baselines using real images in various experiments, including regular image classifications on MNIST and CIFAR10, adaptation from ImageNet to PASCAL-VOC and CUB-200, and continual learning on permuted MNIST and CIFAR100. Baselines include random real images, optimized real images, and k-means++ clustering. Our method applies k-means++ clustering to extract cluster centroids for each category. Experimental results comparing our method with various baselines, including average real images, are presented. Additional details on training classifiers and baselines can be found in the appendix. Experimental results on training classifiers from scratch or adapting from pre-trained weights are presented. For MNIST, distilled images are trained with LENET achieving about 99% test accuracy. CIFAR10 uses a network architecture achieving around 80% test accuracy. ImageNet adaptations utilize ALEXNET with specific GD steps for each dataset. Results are reported for random initializations and pre-trained weights over 200 models. Distilled images can boost the performance of a LENET network on MNIST from 8.25% to 93.82% with just 10 images. Using 100 images can raise the accuracy to 94.41%. Similarly, on CIFAR10, 100 distilled images can improve the accuracy from 10.75% to 45.15% in 50 GD steps. Distilled images trained with random initializations using Xavier Initialization do not require a specific initial point. In Section 4.2, preliminary results show gains from using distilled images in classifier networks during continual learning. Adapting models among MNIST, USPS, and SVHN using 100 distilled images outperforms baselines in most settings. The 100 distilled images are split into 10 minibatches for 10 sequential GD steps, iterated 3 times. Adapting an ALEXNET pre-trained on ImageNet to PASCAL-VOC and CUB-200 is also discussed. Our method significantly outperforms baselines by using multiple gradient descent steps with distilled images. Results show that our method with fixed and random initializations outperforms baselines on CIFAR10 and MNIST datasets. Additionally, an extended setting of our algorithm pre-trains weights on a specific dataset for improved performance. In this section, the method involves training distilled images on pre-trained models and evaluating them on unseen models. The approach is more effective than baselines on adaptation between MNIST, USPS, and SVHN datasets. The method outperforms a few-shot domain adaptation method and shows superior performance with fixed pre-trained weights on all tasks. Our method effectively compresses information from target datasets by using fixed pre-trained weights on tasks like PASCAL-VOC and CUB-200. It outperforms baselines significantly with just one distilled image per category, performing on par with fine-tuning on full datasets. We modify Gradient Episodic Memory (GEM) to store distilled data for each task in a small buffer size, unlike previous experiments with large memory buffers. In the experiment, two continual learning tasks are considered with a small memory buffer size. For Permuted MNIST, 20 classification tasks are created with 1,000 training images each, while for CIFAR100, 20 tasks are formed with 2,500 training images each. The classifier used has 2 hidden layers with 100 neurons. Training details include 2000 GD steps for Permuted MNIST and 200 GD steps for CIFAR100. In this study, dataset distillation is used to compress the knowledge of entire training data into a few synthetic training images. The approach significantly improves overall accuracy on all tasks and reduces buffer size compared to the original GEM method. The paper demonstrates how training a network with a small number of distilled images can achieve good performance. The distilled images efficiently store the memory of previous tasks. The distilled images efficiently store the memory of previous tasks in continual learning. Challenges remain for knowledge distillation, such as limitations to specific network architectures and increasing computation requirements. However, the findings show potential for training large models with a few distilled data, leading to applications like accelerating network evaluation. In experiments, dropout layers are disabled in networks for distillation to reduce randomness and computational cost. Distilled learning rates are initialized with a constant between 0.001 and 0.02, using the Adam solver with a learning rate of 0.001. Initial weights are randomly sampled for optimization steps, and experiments are conducted on various NVIDIA GPUs. The experiments involved using fixed initial weights and up to four GPUs for training, with each training session lasting 1 to 6 hours. Different methods were used for evaluating performance, including random real images, optimized real images, k-means++, and average real images. In experiments involving fixed initial weights and up to four GPUs for training, various evaluation methods were used, including random real images, optimized real images, k-means++, and average real images. The model was evaluated only once due to deterministic average images. Softplus was applied to a scalar trained parameter to ensure the optimized learning rate is positive. In continual learning experiments on CIFAR10 dataset, Batch normalization was replaced with Group normalization in RESNET18 for a fair comparison with GEM. Distilled images were initialized with N (0, 1) or random real samples for dataset distillation experiments. In experiments involving fixed initial weights and up to four GPUs for training, various evaluation methods were used, including random real images, optimized real images, k-means++, and average real images. The model was evaluated only once due to deterministic average images. Softplus was applied to a scalar trained parameter to ensure the optimized learning rate is positive. In continual learning experiments on CIFAR10 dataset, Batch normalization was replaced with Group normalization in RESNET18 for a fair comparison with GEM. Distilled images were initialized with N (0, 1) or random real samples for dataset distillation experiments. Figures 4 and 5 show distilled images trained for random initializations on MNIST and CI-FAR10. Figures 6, 7, and 8 show distilled images trained for adapting random pre-trained models on digits datasets including MNIST, USPS, and SVHN. Figure 6 shows dataset distillation for adapting random pretrained models from USPS to MNIST, with 100 distilled images split into 10 gradient descent steps. The images train average test accuracy from 67.54% to 92.74%. Figure 7 depicts dataset distillation for adapting random pretrained models from MNIST to USPS, with 100 distilled images split into 10 gradient descent steps. Figure 8 shows dataset distillation for adapting random pretrained models from SVHN to MNIST. 100 distilled images are split into 10 gradient descent steps, iterated over three times for a total of 30 steps. The images improve average test accuracy from 90.43% \u00b1 2.97% to 95.38% \u00b1 1.81%. The 10 steps of dataset distillation are iterated over three times, totaling 30 gradient descent steps. Test accuracy improves from 51.64% \u00b1 2.77% to 85.21% \u00b1 4.73% on 200 held out models."
}