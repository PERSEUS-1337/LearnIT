{
    "title": "r1gPtjcH_N",
    "content": "Supervised machine learning models for high-value computer vision applications like medical image classification often need large labeled datasets, which are costly and static. To address this, observational supervision using gaze data can reduce the amount of hand-labeled data required for training. By supervising a visual attention layer with gaze information, the model's performance can be improved with fewer labeled examples, especially for challenging tasks in medical imaging. Recent advances in eye tracking technology have made it possible to passively collect gaze data during tasks, reducing the need for large hand-labeled datasets in supervised machine learning for medical imaging applications. Gaze information has been found to be most helpful on difficult tasks, improving model performance with fewer labeled examples. Recent advances in eye tracking technology have transformed from intrusive and inaccurate to viable for real-time gaze data collection. By mapping gaze data to visual attention layer activations, models can draw influence from spatial regions heavily utilized by human annotators. Incorporating noisy observational signals alongside traditional labels can reduce the amount of hand-labeled data needed for model performance. This technique was applied to an image classification task, showing that model performance can be maintained using only 50% of the training images when gaze information is utilized. Incorporating gaze information during training can maintain model performance with as little as 50% of the training images. Observational signals are more beneficial for difficult tasks, potentially reducing the need for labeled data in medical imaging tasks. This approach draws inspiration from weak supervision and attention-based models, aiming to improve model training feasibility. Eye tracking data and click stream monitoring can provide rich information for training machine learning models, improving image classification and model optimization. This approach reduces the need for labeled data and draws inspiration from weak supervision and attention-based models. Integration of visual attention techniques into computer vision algorithms has become a significant research interest. An attention mechanism represents a learned weighting applied to different subsamples of a given input. Recent efforts to incorporate visual attention layers into CNN training have led to state-of-the-art performance on tasks such as image classification, object recognition, visual question answering, and medical image segmentation. Incorporating visual attention techniques into computer vision algorithms has shown significant progress in tasks like image classification and medical image segmentation. Gaze data integration improves performance across different training set sizes, especially in challenging classifications. The focus is on training neural network architectures with gaze data for practical deployment in various settings. Integrating eye tracking signals into the training loss function improves neural network performance for practical deployment in settings without eye trackers. This involves balancing classification loss with a visual attention loss term, regularization, and model parameters. The view of a convolutional filter can be determined via upsampling, with M calculated directly. CAMs generated by CNNs provide signals at a lower resolution than the original image. Downsampling H aligns its resolution with M. Gaze data can improve model training procedures, reducing the amount of training data needed for performance. Testing this hypothesis involves training a 3-layer CNN with gaze data to enhance classification performance. CNN with gaze data improves classification performance on the POET dataset BID11 by using 2D Gaussians to construct heatmaps based on fixation points. The dataset contains 6,270 images across 10 classes with eye tracking data from five labelers for each image. Models are trained with and without gaze data, referred to as \"gaze-augmented\" and \"standard\" networks, respectively. Integrating gaze-based loss into training improves model performance with fewer samples. Gaze-augmented models achieve similar performance using 50% less training data, with an average ROC-AUC of 0.85 for 2,000 images with gaze data compared to 0.86 for 4,000 images without gaze data. Performance improvement is less significant in high and low data regimes. The study shows that integrating gaze data into model training can improve performance with fewer samples. There is a decrease in model variance when using gaze data, indicating additional constraints on the model parameter space. The relationship between task difficulty and the usefulness of gaze data is examined, with some classes showing substantial improvement in performance. The study examines the impact of integrating gaze data into model training, showing performance improvements for some classes. A negative correlation between task difficulty and performance gains is observed, with gaze data benefiting more difficult classes. Qualitative evaluation reveals high activations in relevant image regions for gaze-augmented models. Incorporating observational gaze signals into CNN training improves classification performance by focusing on relevant image parts, reducing the need for labeled data. Performance gains are more significant for challenging tasks. Incorporating observational gaze signals into CNN training improves classification performance, especially for challenging tasks. Future work will focus on fully characterizing the circumstances where gains from gaze-augmented models are seen, with plans to apply the technique to medical imaging tasks and improve model robustness. The study utilized simple three-layer CNNs with specific configurations and data preprocessing techniques to enhance performance. The study utilized three-layer CNNs with specific configurations and data preprocessing techniques to improve classification performance. Model training involved using the Adam optimizer with specific parameters for 30 epochs and cross-validating across different train-dev splits. Random hyperparameter search was performed for each combination, and model improvement for each class and training set size was shown in Fig. 4. Inspection of the dataset revealed variations in appearance for images with the label cow, while images with the label aeroplane typically included only one aeroplane near the center. The study used CNNs with specific configurations and data preprocessing techniques to improve classification performance. Gaze data for identifying difficult classes like cow may be more helpful than for easier classes like aeroplane. Mean ROC-AUC improvement from training with gaze data for each class at different training set sizes is shown in Figure 4."
}