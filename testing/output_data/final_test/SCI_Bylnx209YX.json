{
    "title": "Bylnx209YX",
    "content": "Deep learning models for graphs have advanced the state of the art on many tasks, but their robustness is not well understood. Research investigates training time attacks on graph neural networks for node classification by perturbing the graph structure. Using meta-gradients to optimize the graph as a hyperparameter, small perturbations significantly decrease performance, even leading to worse results than a baseline that ignores relational information. These attacks do not require knowledge of the target classifiers. Graphs are a powerful representation for modeling diverse data from various domains like biology, chemistry, and social networks. Machine learning on graph data has a long history, with tasks including node classification. Recent attention has been on deep learning approaches for graph data, particularly graph convolutional methods. Recent advancements in graph convolutional approaches have significantly improved node classification. However, these methods are vulnerable to adversarial attacks, both during testing (evasion) and training (poisoning attacks). The strength of models using graph convolution lies in exploiting neighborhood information to enhance classification, but this also poses a major vulnerability. Network effects like homophily support classification but also enable indirect adversarial attacks. Existing attacks on node classification exploit these vulnerabilities. In this work, a new algorithm for poisoning attacks on node classification models is proposed, aiming to compromise the global performance of the model. The approach is based on meta learning principles, treating the input data as a hyperparameter to learn. These attacks can render the model near-useless for production use, even without access to the target classifier. Adversarial attacks on machine learning models are studied in both the machine learning and security community. Adversarial examples are intentionally created to mislead machine learning models, especially deep neural networks. Most attacks assume independent and continuous data instances, which is not the case for tasks on graphs like node classification. Research on adversarial attacks for graph learning tasks is limited. Researchers have recently started studying adversarial attacks on deep learning for graphs. BID18 consider both test-time and training-time attacks on node classification models, focusing on improving robustness through adversarial noise in node features. They address the bilevel optimization problem and evaluate transferability of attacks, unlike previous work that only considered evasion attacks on graph classification. Researchers have recently studied adversarial attacks on deep learning for graphs, focusing on improving robustness through adversarial noise in node features. BID18 consider both test-time and training-time attacks on node classification models, circumventing the bilevel optimization problem by using a surrogate model. Bojchevski & G\u00fcnnemann (2018b) propose poisoning attacks on unsupervised node representation learning, exploiting perturbation theory to maximize loss in DeepWalk training. Meta-learning, also known as learning to learn, involves optimizing the learning algorithm itself by adjusting hyperparameters, updating neural network parameters, or modifying the activation function of a model. Gradient-based hyperparameter optimization involves differentiating the training phase to obtain gradients for optimization. This work explores using meta-learning for training-time attacks by modifying the training data to reduce performance post-training. While successful in attacking simple linear models, little success was found with deep neural networks on continuous data, and deep learning models were not considered for discrete datasets with more than two classes. In this work, a new algorithm is proposed for global attacks on deep node classification models during training. The focus is on poisoning attacks using meta learning, specifically for semi-supervised node classification on attributed graphs. The goal is to infer class labels for unlabeled nodes in the graph. The goal is to learn a function that maps nodes to classes in a transductive learning setting. Parameters are learned by minimizing a loss function on labeled training nodes. Adversarial attacks are deliberate perturbations to achieve desired outcomes on machine learning models. In our work, the attacker's goal is to increase the misclassification rate of a node classification algorithm by modifying the data. The attacker can have limited knowledge about the training data and model parameters but aims to reduce the overall classification performance of the model through global attacks. The attacker aims to degrade the performance of deep neural networks by modifying data, with knowledge about the data and graph structure. They impose a budget constraint on the attacks and ensure no node becomes disconnected during the attack. The attacker aims to optimize the loss function by modifying data and graph structure within a budget constraint, ensuring no node becomes disconnected during the attack. BID18's unnoticeability constraint on the degree distribution prevents significant changes, with efficient violation checks to minimize computational overhead. The algorithm can be adapted to change node features, and admissible perturbations on the data are denoted as \u03a6(G). Poisoning attacks are formulated as a bilevel optimization problem. The attacker aims to optimize the loss function by decreasing the model's generalization performance on unlabeled nodes. One approach is to maximize the loss on labeled nodes, with the first attack option being to choose the loss function as the negative of the training loss. Semi-supervised node classification is a form of transductive learning where all data samples and attributes are known at training time. This insight can be used to obtain a second variant of the loss function. The attacker can learn a model on labeled data to estimate labels of unlabeled nodes and perform self-learning. Only labeled node labels are used for training, while L self estimates generalization loss. Experimental evaluation compares two versions of the attack. The attacker aims to maximize classification loss on the modified graph, a challenging bilevel problem in the graph setting. In the graph setting, the attacker's actions are discrete, making the problem more challenging due to a vast action space and inability to use gradient-based methods. The bilevel problem is tackled using meta-gradients, traditionally used in meta-learning to improve time and data efficiency in learning machine learning models. The core idea behind the adversarial attack algorithm is to treat the graph structure matrix as a hyperparameter and compute the gradient of the attacker's loss after training with respect to it. This meta-gradient indicates how the attacker loss will change for small perturbations on the data, crucial for a poisoning attacker. The attacker can use the meta-gradient to perform a meta update on the data to minimize the loss. The final poisoned data is obtained after performing multiple meta updates using gradient descent. This update rule is not suitable for problems with discrete data. In the proposed approach, the update rule is adjusted to handle discrete data like graphs. The attacker uses a surrogate model for poisoning attacks, and the poisoned data is then used to train deep learning models for node classification. The approach aims to preserve data sparsity and discreteness while evaluating performance degradation due to the attack. The surrogate model used for poisoning attacks is a linearized two-layer graph convolutional network. Changes are made to the graph structure A, treating node attributes X as constant during attacks. A score function is defined to assess the impact of actions on the attacker objective, with the flipping of meta-gradients for connected node pairs. The meta update function inserts or deletes edges based on the highest score perturbation, ensuring compliance with attack constraints. To reduce computational costs, a first-order approximation is proposed for computing meta gradients. The parameters at time t are independent of the data, preventing gradient propagation through them. The meta update function inserts or deletes edges based on the highest score perturbation to comply with attack constraints. A first-order approximation is used to compute meta gradients, disregarding training dynamics. BID10 propose a heuristic meta gradient to update initial weights towards the local optimum for faster convergence in multi-task learning. This heuristic direction is based on the observed increase in training loss during the procedure, achieving similar results as the meta gradient but more efficiently. The heuristic discussed achieves similar results as the meta gradient but is more computationally efficient. It is adapted for adversarial attacks on graphs and involves estimating loss on unlabeled nodes using self-learning. This approximation has a smaller memory footprint and avoids computing second-order derivatives. The algorithm summary can be found in Appendix A. The text discusses evaluating adversarial attacks on graph datasets using labeled and unlabeled nodes. The approach is evaluated on CITESEER, CORA-ML, and POLBLOGS datasets. The transferability of attacks is assessed by training deep node classification models on poisoned data using Graph Convolutional Networks and Column Networks. The code for the approach is available online. The text evaluates adversarial attacks on graph datasets using Graph Convolutional Networks (GCN) and Column Networks (CLN) for node classification. The attacks are repeated on different splits of labeled/unlabeled nodes and target classifiers are trained multiple times per attack. The uncertainty in the results is indicated by 95% confidence intervals. Meta-gradient approaches are used to compute the meta-gradient for the attacks. The text evaluates adversarial attacks on graph datasets using Graph Convolutional Networks (GCN) and Column Networks (CLN) for node classification. Meta-gradient approaches are used to compute the meta-gradient for the attacks. Comparing different meta gradient heuristics, the results show that A-Meta-Self consistently performs weaker than A-Meta-Both. The meta-gradient approach and its approximations are compared with various baselines and Nettack BID18. The text evaluates adversarial attacks on graph datasets using Graph Convolutional Networks (GCN) and Column Networks (CLN) for node classification. Comparing different meta gradient heuristics, the results show that A-Meta-Self consistently performs weaker than A-Meta-Both. Baselines like DICE and First-order approximations are also considered in the evaluation. Nettack is not designed for global attacks, but for each perturbation, a target node is randomly selected and attacked using Nettack. Meta w/ Oracle corresponds to the meta-gradient approach used in the study. In the study, different methods for adversarial attacks on graph datasets using Graph Convolutional Networks (GCN) and Column Networks (CLN) are evaluated. The unnoticeability constraint introduced by BID18 is enforced for all methods. Misclassification rates achieved by modifying edges in the graph are presented in tables. The meta-gradient with self-training approach is discussed. Our meta-gradient with self-training (Meta-Self) results in the strongest drop in performance across all models and datasets, with up to a 48% increase in misclassification rate by changing only 5% of the edges in GCN on CORA-ML. Memory efficient meta-gradient approximations also lead to significant increases in misclassifications, outperforming baselines and even matching the more expensive meta-gradient in some cases. Using only 10 training iterations for computing the meta gradient can negatively impact performance, and our heuristic successfully attacks datasets with around 20K nodes. Our focus is on poisoning attacks by modifying graph structures. Our method can be applied to node feature attacks as well, as shown in a proof of concept in Appendix E. The drop in classification performance of GCN on CORA-ML for increasing edge insertions/deletions is illustrated in Fig. 1. Meta-Self can reduce classification accuracy below 50%. Fig. 2 compares the classification accuracy of GCN and CLN with a baseline operating on node attributes only. By perturbing 5% of the edges, GCN and CLN perform worse than the baseline. The impact of perturbing edges on GCN and CLN performance is analyzed. Results show that modifying the graph structure can negatively affect classification accuracy, highlighting the importance of the training procedure in graph models. The poisoning attack works by derailing the training procedure, leading to suboptimal weights. The adversarial changes created by the meta-gradient approach can be destructive, impacting the training procedure and leading to 'bad' weights. Analyzing the attacks can help understand why these changes are so harmful and what patterns they follow. Comparing edges inserted by the meta-gradient approach to the original edges in the network can provide insights into the impact on shortest path lengths and edge centrality. This knowledge can potentially help detect adversarial attacks without the need for expensive computations. In (c) we observe the node degree distributions of the original graph and the nodes selected for adversarial edges. The algorithm tends to connect nodes with higher-than-average shortest paths and low degrees. About 80% of the meta attack's perturbations are edge insertions, connecting nodes from different classes. However, this alone cannot explain the destructive performance of the meta-gradient compared to the DICE baseline. The attacker has full knowledge of the graph structure and node attributes in the experiments. The algorithm was also tested on a sub-graph of CORA-ML and CITESEER. Our algorithm for training-time adversarial attacks on graphs focuses on node classification. Using meta-gradients, we consistently increase misclassification rates even in a highly restricted setting, demonstrating the effectiveness of our method. Experiments show that attacks created with our approach lead to a strong decrease in classification performance. Our approach leads to a significant decrease in classification performance of graph convolutional models and transfer to unsupervised models. Small perturbations can make graph neural networks perform worse than a baseline. We propose cheaper approximations of metagradients that also impact node classification models. The edges inserted/removed by our algorithm are highly destructive, posing a challenge for detection and defense against attacks. The text discusses a method for modifying graphs to impact node classification models. The approach involves training a surrogate model on the input graph, predicting labels of unlabeled nodes, and inserting or removing edges based on constraints. The dataset statistics are also presented, showing characteristics of the datasets used in the work. The meta gradient attack method involves computing meta gradients for each node pair, leading to a memory and computational complexity of O(N^2). Additional memory cost is incurred by storing the weight trajectory during training, resulting in a memory complexity of O(N^2 + T \u00b7 |\u03b8|). The computational complexity for second-order derivatives is O(T \u00b7 N^2). The meta gradient heuristics have a similar computational complexity but lower memory complexity of O(N^2). The computational and memory complexity of adversarial attacks for graphs can be reduced by pre-filtering elements in the adjacency matrix. This optimization allows for execution on graphs with roughly 20K nodes using a commodity GPU. Performance optimization for reducing complexity is left for future work. In experiments, perturbations in edges/node features are made with a 10% rate. MetaSelf with features selects perturbations with the highest meta gradient score. Enforcing unnoticeability constraints proposed by BID18 does not significantly limit attack performance. The method can be applied to node feature attacks, especially with binary features. In experiments, perturbations in edges/node features are made with a 10% rate. Among the datasets evaluated, CITESEER has binary node features. Results show that combined attacks on node features and graph structure have slightly lower impact than structure-only attacks. Future work can explore assigning different costs to structure and feature changes for more effective attacks. In experiments, perturbations in edges/node features are made with a 10% rate. Additional results show successful attacks on PUBMED dataset with misclassification rates at 1% and 10% perturbed edges. Results also display how classification accuracies change for different attack methods and datasets. The text chunk provides data on various attack methods and their effectiveness on different datasets. Key details include success rates, misclassification rates, and classification accuracies for different attack methods. The data chunk presents success rates, misclassification rates, and classification accuracies for different attack methods on various datasets."
}