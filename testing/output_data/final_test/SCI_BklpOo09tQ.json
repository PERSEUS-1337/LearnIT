{
    "title": "BklpOo09tQ",
    "content": "In recent years, deep neural networks have shown exceptional performance in various machine learning tasks. However, they are susceptible to adversarial examples, which are legitimate examples with small imperceptible perturbations. Adversarial training, involving the addition of adversarial examples during training, enhances model robustness against attacks. Madry et al. (2017) propose using projected gradient descent (PGD) as a universal first-order adversary for effective adversarial training. However, the computational cost of this approach is a concern. In this paper, researchers propose a two-step defense approach against adversarial attacks, demonstrating its effectiveness and improvements over existing strategies. Despite the outstanding performance of deep neural networks in various tasks, they are vulnerable to attacks by adversarial examples. These examples are crafted perturbations added to legitimate examples to deceive the model. The proposed approach generates strong adversarial examples at a cost similar to two runs of the fast gradient sign method, providing robust defense comparable to multi-step adversarial training. Adversarial examples are generated by adding perturbations to input samples, making them indistinguishable to humans. These perturbations can cause misclassification by deep neural networks. The vulnerability is attributed to model linearity, low flexibility, or decision boundary characteristics. Research explores black-box and white-box attack models, with white-box attacks having complete access to model information. Several white-box attack methods have been proposed (BID2, BID12, BID17, BID0, BID9) where the attacker has complete access to model information. Defenses have been developed to mitigate adversarial attacks by expanding training data, modifying training procedures, and using external models as network add-ons. Adversarial training is a simple yet effective method to enhance classifier robustness. Adversarial training is a method to improve neural network robustness against attacks by generating adversarial examples using the same attack mechanism. However, it may not work well against different attack strategies due to gradient masking. Iterative multi-step attacks, like projected gradient descent (PGD), are effective in enhancing resistance. Using PGD in adversarial training can boost defense against various attacks, but typically requires a large number of back propagation steps. The paper proposes an efficient two-step adversarial defense technique, called e2SAD, to defend against whitebox and blackbox attacks. It aims to generate adversarial examples with a quality comparable to expensive adversarial training methods like IFGSM BID5. The first step involves using a simple one-step attack method like FGSM, while the second step maximizes the model's vulnerability to reveal a high-quality defense. The paper introduces e2SAD, a two-step adversarial defense technique that efficiently generates two adversarial examples per input to enhance model robustness against one-step and iterative attacks. By incorporating these examples into the loss function, the model achieves strong defense with reduced training time compared to IFGSM-based methods. Key contributions include demonstrating vulnerability of the classifier, robustness to white box attacks, and effectiveness of the final loss function in enhancing model resilience. The curr_chunk discusses techniques like soft labels and hyperparameter tuning for robust defense against black box attacks. It also explains attack models aiming to find perturbations leading to misclassification, with two threat levels: white box and black box. The curr_chunk explains adversarial attacks on models, including white-box and black-box scenarios. Techniques like FGSM, IFGSM, and PGD IFGSM are used to generate adversarial examples by perturbing input data iteratively. In our implementation, we set a = \u03b5 N for normalizing input vectors. Adversarial Training is a defense approach that augments the training dataset with adversarial examples. We adopt the adversarial training equation proposed in BID2 as the loss function, where \u03b1 specifies the importance of adversarial examples. We compare FGSM and IFGSM for generating adversarial examples. In the context of adversarial training, different methods like FGSM and IFGSM are used to generate adversarial examples. Defense methods like BID4 and BID7 view training a robust model as a minmax optimization problem. Hamm proposes a new approach to find the true optimal solution instead of targeting saddle points. BID3 uses FGSM for inner maximization and changes the minimization objective function for outer minimization. Adversarial training increases model robustness against white box attacks by augmenting the training dataset with adversarial examples. However, it is less effective against different attack strategies like multi-step attacks IFGSM and PGD compared to one-step FGSM. The proposed efficient two-step adversarial defense (e2SAD) approach aims to develop a defense method that is robust against both one-step FGSM and multiple-step attacks like IFGSM, while minimizing the cost of back propagation. It only takes two steps of back propagation to find adversarial examples, making it more efficient than traditional methods. The e2SAD approach aims to defend against one-step and multiple-step attacks efficiently by generating adversarial examples in two steps. The loss function includes terms for original inputs, first step adversarial examples, and dissimilarity in categorical distribution between pairs of adversarial examples. The e2SAD approach is structured to defend against one-step and multiple-step attacks efficiently by generating adversarial examples in two steps. The first step aims to find vulnerable neighborhoods around clean training data points to make the model robust to one-step attacks. This is achieved by applying a one-step attack method like FGSM to maximize loss around each input, creating the first adversarial example. This example's loss is included in the final loss function to guide the training process in reducing losses for both the original input and the adversarial example, defending against one-step attacks. The e2SAD approach aims to defend against multi-step attacks efficiently by generating adversarial examples in two steps. The key challenge is to find a second adversarial example close to the original one that can reveal the model's vulnerability similar to expensive multi-step attacks. In methods like IFGSM or PGD, each adversarial example is typically found by perturbing the preceding one to maximize loss based on cross entropy. The e2SAD approach aims to defend against multi-step attacks efficiently by generating adversarial examples in two steps. Instead of maximizing loss based on cross entropy, it suggests maintaining similarity in predictions around each clean input to create a well-regularized decision boundary. The e2SAD approach aims to defend against multi-step attacks efficiently by generating adversarial examples in two steps. In the second step, the goal is to find a second adversarial example with a maximally different categorical distribution from the first adversarial example in the neighborhood. This dissimilarity is measured using cross entropy, and FGSM is used to locate the second adversarial point. The use of categorical distribution as a measure of dissimilarity is justified by the fact that loss value alone may not indicate misclassification accurately. The e2SAD approach aims to efficiently defend against multi-step attacks by generating adversarial examples in two steps. In the second step, the model misclassifies inputs even when the loss is lower, illustrating how the optimization objective can influence the generation of adversarial examples in a three-class classification problem. The choice of optimization objective may impact the generation of adversarial examples, as shown in Figure 2. The e2SAD approach generates adversarial examples in two steps, with the optimization objective influencing the outcome. Figure 2 illustrates the impact of using different objectives in a three-class classification problem. Starting from the green cross, maximizing the loss produces the red cross, while using cross entropy dissimilarity leads to the yellow cross, which results in misclassification. The CE dissimilarity measure leads to the yellow cross, with misclassification occurring. The effectiveness of the measure in finding stronger adversarial points is highlighted. The loss function for training the final model is designed based on adversarial examples generated in two steps of e2SAD. The proposed e2SAD approach uses label smoothing for training, assigning soft labels to improve performance. The algorithm includes hyperparameters \u03b1 and \u03bb to weight losses for clean inputs, first adversarial inputs, and dissimilarity between adversarial inputs. The final loss function aims to defend against misclassification. The proposed e2SAD approach uses hyperparameters \u03b1 and \u03bb to balance defense against one-step and multi-step adversarial attacks. The final loss function aims to defend against misclassification, with \u03b1 and \u03bb chosen to meet different defense needs. The process includes training on a dataset with initial model parameters and updating the model using backpropagation. Visual demonstrations of the adversarial example generation process and loss surfaces are provided to showcase the effectiveness of e2SAD. The proposed e2SAD method is compared with adversarial training techniques using single-step FGSM and multi-step IFGSM. Experiments are conducted on the MNIST and SVHN datasets with various adversarial attacks considered. The e2SAD method is compared with FGSM and IFGSM adversarial training techniques on CNN models with specific configurations. Hyperparameters for e2SAD are set as \u03b1 = 0.6, \u03bb = 0.1, \u03b5 1 = 0.3, and \u03b5 2 = 0.1. The second step of e2SAD does not clamp the adversarial point within a norm ball for increased searching ability. The second step of e2SAD does not clamp the second adversarial point within a norm ball. Models are trained on MNIST for 30,000 iterations with a batch size of 256. Performance comparison under white-box attacks shows different defense levels among models. FGSM adversarial training defends well against FGSM attacks but not IFGSM attacks. IFGSM adversarial training performs well against IFGSM adversaries and is robust against FGSM attacks. The e2SAD method demonstrates high accuracy for clean data and FGSM attacks at various noise levels, outperforming IFGSM adversarial training. It offers strong defense against FGSM attacks while maintaining robustness against IFGSM attacks. The e2SAD method provides a strong defense against FGSM attacks with only two steps of gradient calculation, reducing computational cost compared to IFGSM adversarial training. Label smoothing is used effectively to help the model generalize well. The model is trained using a mixture of clean and adversarial examples to improve performance. Results show robustness against FGSM attacks but vulnerability to IFGSM attacks. Adversarial examples generated by IFGSM on substitute models are used to attack different models. Substitution models are trained with hard labels or label smoothing and then attacked with IFGSM for generating adversarial examples. The proposed e2SAD approach provides a strong defense against black-box IFGSM attacks from various substitute models, achieving an accuracy of nearly 90% or higher. It outperforms natural training and FGSM adversarial training in most cases. The e2SAD approach outperforms natural training and FGSM adversarial training, especially when compared to models trained with 30-steps IFGSM adversarial training. The Street View House Numbers (SVHN) dataset is used, which is a harder real-world dataset than MNIST. Three different models are trained with CNN configuration and compared. The e2SAD model outperforms all other models in defending against white-box attacks on the SVHN dataset. Specifically, the baseline model shows no defense, while e2SAD demonstrates significantly stronger robustness against IFGSM attacks. The e2SAD model offers strong defense against both one-step FGSM attacks and multi-step IFGSM attacks, outperforming other models in robustness. It achieves this by utilizing a two-step structure to reveal vulnerabilities in the model and improve its overall resilience. The e2SAD model enhances defense against various adversarial attacks by incorporating both one-step and multi-step adversaries in the training loss function. It demonstrates effectiveness against white-box and black-box attacks, offering a balanced defense mechanism that can be further improved through hyperparameter tuning and new techniques. The e2SAD model enhances defense against adversarial attacks by incorporating one-step and multi-step adversaries in the training loss function. It generates adversarial examples for clean images from the MNIST dataset and visualizes the loss surface around the images using search directions g1 and g2. The loss surface is shown as a function of t1 and t2, with the blue dot representing the model's loss over the perturbed images for the entire minibatch. The blue dot at (0, 0) location represents the loss of the minibatch of clean images. The red line illustrates the two-step e2SAD adversarial searching direction, showing losses summed over sets of adversarial examples. This demonstrates the effectiveness of e2SAD in defending against both one-step and multi-step adversarial attacks. The baseline model is compared to models trained with adversarial training using FGSM and IFGSM attacks, as well as the proposed e2SAD approach. All models are trained on the MNIST dataset and their loss surfaces are visualized in 2D space for both types of attacks. The loss surfaces of models trained with adversarial training using FGSM and IFGSM attacks, as well as the e2SAD approach, are visualized in 2D space for the MNIST dataset. Search directions are identified around clean images, generating perturbed images to show the loss as a function of different parameters. The e2SAD model shows the flattest loss surface with the lowest average value in a 2-dimensional adversarial space, indicating its effectiveness against FGSM and IFGSM attacks."
}