{
    "title": "Skgb5h4KPH",
    "content": "We study the training process of Deep Neural Networks (DNNs) from a Fourier analysis perspective, demonstrating a universal Frequency Principle (F-Principle) where DNNs fit target functions from low to high frequencies. This principle is opposite to conventional iterative numerical schemes and results from the regularity of activation functions. It implies a bias towards fitting training data with low-frequency functions, explaining the generalization performance of DNNs on real datasets. The training process of Deep Neural Networks (DNNs) follows a Frequency Principle (F-Principle) where low frequencies are fitted first, followed by high frequencies. This behavior was observed empirically in synthetic data with MSE loss during DNN training. The F-Principle in deep learning varies across network structures and datasets, posing challenges in demonstrating its universality, especially in high-dimensional real problems. This study introduces projection and filtering methods to show the existence of the F-Principle in DNN training for high-dimensional benchmarks like MNIST and CIFAR10. The study demonstrates the universality of the F-Principle in deep learning by considering different DNN architectures, activation functions, and loss functions. It highlights the stark difference between DNNs and conventional numerical schemes in terms of convergence behavior, showing that DNNs can accelerate the convergence of low frequencies in computational problems. The F-Principle can accelerate the convergence of low frequencies for computational problems and explain the smoothness of activation functions. It provides insights into generalization in DNNs for real datasets and learning the parity function. Our main contributions include demonstrating the F-Principle for MNIST/CIFAR10 using various architectures and loss functions, accelerating convergence of low frequencies in solving differential equations with DNN-based schemes, illustrating how activation function smoothness affects the F-Principle, and explaining the implicit bias of DNNs towards low frequency functions and its impact on generalization. The concept of \"frequency\" is central to the understanding of F-Principle. Image frequency refers to the rate of change of intensity across neighboring pixels, while response frequency pertains to the mapping of input-output functions. For example, in a simplified classification problem using MNIST data, response frequency maps pixel values to binary labels. The concept of \"frequency\" is central to the understanding of F-Principle in deep neural networks. It involves mapping pixel values to binary labels based on the rate of change of intensity across neighboring pixels. The training process fits target functions from low to high frequencies, where high frequencies can lead to significant changes in output with small changes in input. The F-Principle is rigorously defined through the frequency defined by the Fourier transform and the converging speed defined by the relative error. Experimental demonstration of F-Principle is done using high-dimensional real datasets, examining individual frequencies and coarse-grained frequencies. Real datasets differ from synthetic data used in previous studies, and it is important to verify the F-Principle in high-dimensional real datasets. In this section, the high dimensional discrete Fourier transform is discussed, focusing on one direction in the Fourier space through a projection method for each examination. The dataset {(x i , y i )} n\u22121 i=0 is considered, with one entry of 10-d output denoted by y i \u2208 R. The number of all possible k grows exponentially on dimension d, and a direction of k in the Fourier space is chosen for each examination. The projection of x i on the direction p 1 is calculated, and for each training dataset, p 1 is chosen accordingly. The direction p 1 is chosen as the first principle component of the input space for each training dataset. The convergence behavior of different frequency components during training is examined by computing the relative difference between the DNN output and the target function for selected important frequencies. The F-Principle is empirically shown in the selected direction during the training process of DNNs applied to MNIST/CIFAR10 with cross-entropy loss. The experimental details can be found in Appendix C. The experimental details of the paper can be found in Appendix C. The DNN captures low frequencies first and gradually captures higher frequencies, consistent with the F-Principle. Similar phenomena are observed for other components of the output vector and directions of p. The projection method enables visualization of the F-Principle in one direction for each examination at the level of individual frequency components. However, this method alone is insufficient to verify the F-Principle at all potentially. The text discusses a filtering method to examine the convergence of low and high frequencies in deep neural networks. The method splits the frequency domain into low and high parts and compares the convergence of errors. This approach complements the projection method in verifying the F-Principle for high-dimensional data. The text introduces a method using Fourier transform of a Gaussian function to approximate high-dimensional Fourier transform computations, allowing for the examination of low and high frequencies convergence in deep neural networks. This method complements the projection method in verifying the F-Principle for high-dimensional data. The text introduces a method using Fourier transform of a Gaussian function to approximate high-dimensional Fourier transform computations in deep neural networks. The examination of low and high frequencies convergence is crucial for verifying the F-Principle. The experimental procedure involves training the DNN with original datasets like MNIST or CIFAR10, followed by filtering to derive low and high frequency parts. The text discusses deriving high and low frequency parts in deep neural networks using Fourier transform of a Gaussian function. The convergence of these frequencies is examined to verify the F-Principle. Experimental procedures involve training DNNs with datasets like MNIST and CIFAR10, followed by filtering to analyze convergence. The text discusses the convergence of high and low-frequency parts in deep neural networks using Fourier transform. Results show that the low-frequency part converges faster than the high-frequency part, regardless of the loss function used. This analysis applies to both SGD and GD optimization methods. In this section, the text highlights the difference between a DNN-based solver and the Jacobi method when solving Poisson's equation. A DNN-based scheme is proposed with an empirical loss function, showcasing a stark contrast in behavior during training/iteration, explained by the F-Principle. The empirical loss function in the DNN-based solver includes a penalty term from the Dirichlet boundary condition. The convergence of low and high frequencies in the Fourier transform of the analytical solution is compared to the Jacobi method. DNN can be combined with traditional numerical methods to accelerate convergence for computational problems. The Jacobi method is used with carefully chosen steps to provide a good initial guess for learning low frequencies. The convergence of low and high frequencies is compared between DNN and Jacobi method, highlighting the importance of choosing the right number of steps for efficient convergence. The Jacobi method is used for learning low frequencies, while DNNs are better for high frequencies. Combining both methods could lead to faster schemes in scientific computing. Theoretical work by Luo et al. (2019) explores the FPrinciple in DNNs at different frequencies, showing how DNN regularity affects loss function decay rates in the frequency domain. The activation function \u03c3(x) = tanh(x) connects smoothness with gradient and convergence priorities in the frequency domain for a DNN with one hidden layer. The total loss function is defined in the Fourier domain, equivalent to MSE loss by Parseval's theorem. Theorems related to DNNs with one hidden layer are presented. Theorem 1 states that in a DNN with one hidden layer using tanh activation function, lower-frequency gradients dominate over higher-frequency ones for small weights. Parseval's theorem equates MSE loss in spatial domain to L2 loss in Fourier domain. Theorem 2 discusses training in Fourier domain with only two non-zero frequencies. The DNN with one hidden layer uses tanh activation function. The target function has two non-zero frequencies. Understanding the difference in generalization performance of DNNs is crucial. Fourier analysis is used to show the qualitative difference between good and bad generalization performance. The F-Principle is applied to explain the varying generalization performances of DNNs. The generalization performance of DNNs is analyzed using Fourier analysis on MNIST/CIFAR10 datasets. The analysis shows that the output of a well-trained DNN recovers dominant low frequencies, indicating good generalization performance. The Fourier analysis on MNIST/CIFAR10 datasets shows that a well-trained DNN recovers dominant low frequencies, indicating good generalization performance. However, for the parity function, the Fourier transform of the DNN output significantly deviates from the target function at almost all frequencies, indicating bad generalization performance. The F-Principle suggests that DNNs are biased towards functions with more power at low frequencies during training, leading to artificial low frequencies in the training dataset. The distribution of power in Fourier domain of MNIST/CIFAR10 datasets shows significant differences, impacting the generalization performances of DNNs. Various factors such as complexity measures, local properties of loss functions, stability of optimization algorithms, and implicit bias of the training process contribute to the explanation of why DNNs often generalize well. The Fourier analysis can provide insights into the success and failure of DNNs. The F-Principle was discovered through simple synthetic data and not very deep networks. It was later examined in the MNIST dataset, but noise was added which contaminated the labels. The study verified that the F-Principle holds in the training process of MNIST and CIFAR10 datasets. The F-Principle was verified in the training process of MNIST and CIFAR10 datasets for CNNs, fully connected networks, and very deep networks like VGG16. Theoretical studies by Luo et al. and Rahaman et al. provided rigorous proofs and analysis of the F-Principle for general DNNs. Thm 1 was also used in analyzing nonlinear collaborative schemes for deep network training. Applications of the F-Principle were explored in experiments considering response frequency defined for mapping functions between inputs and outputs. The F-Principle was applied in training CNNs, fully connected networks, and VGG16 on MNIST and CIFAR10 datasets. Theoretical studies provided proofs for general DNNs. The mapping function g was analyzed using the nonuniform discrete Fourier transform, allowing for convergence analysis of the DNN output. The frequency structure of the training data was reflected in \u0177k. The F-Principle was applied in training CNNs, fully connected networks, and VGG16 on MNIST and CIFAR10 datasets. The frequency structure of the training data is reflected in \u0177k, with high frequencies corresponding to sharp changes in output for nearby points. A Gaussian filter is applied to suppress high frequencies, and a projection approach is proposed to fix k at a specific direction. The filtering approach is detailed in Sections 3 and 4. During the training of a DNN to fit a 1-d target function with three frequency components, the convergence behavior of different frequency components is examined using MSE. The DNN converges the first frequency peak quickly but the second frequency peak much slower. The DNN converges the first frequency peak quickly, followed by the second frequency peak at a slower rate. The F-Principle is investigated on real datasets with various loss functions. Experimental details are provided in the Appendix. The DNN parameters are initialized with a Gaussian distribution. Training is done with a tanh-DNN and full batch training. The DNN is trained using the Adam optimizer with a learning rate of 0.0002 and MSE loss function. Results for MNIST and CIFAR10 datasets are shown for different network architectures. The network architecture includes convolution and max pooling layers followed by a fully-connected DNN with specific widths. The DNN is trained with cross entropy loss using the Adam optimizer. Results for MNIST and CIFAR10 datasets are presented with details on learning rates and batch sizes. The network architecture includes convolution and max pooling layers followed by a fully-connected DNN. Training details include learning rates, batch sizes, and optimizer used. Specific configurations are provided for MNIST and CIFAR10 datasets. The DNN architecture includes convolution and max pooling layers, with specific configurations for different datasets. Training details such as learning rates, batch sizes, and optimizer are provided. The Poisson's equation is solved using a central difference scheme. The central difference scheme is used to solve Poisson's equation, resulting in a linear system Au = g. Iterative schemes like the Jacobi method are employed to solve this system. The Jacobi iteration process involves error analysis to determine convergence speed based on eigenvalues. The converging rate is controlled by the eigenvalues of the iteration matrix. The Jacobi method's convergence rate is controlled by eigenvalues, with lower frequencies converging slower. Parameters in a DNN include weights and biases, with the Fourier transform used to define amplitude deviation. The descent increment at any direction with respect to parameter \u03b8 lj is determined by the partial derivatives of L(k). The contribution from frequency k to the total amount at \u03b8 lj is calculated using a function F lj (\u03b8 j , k). A theorem is presented for a one hidden layer DNN with activation function \u03c3(x) = tanh x, showing the existence of positive constants for certain frequencies k 1 and k 2. The proof involves showing that the ratio of measures of certain sets is bounded by an exponential function. This is done by considering specific cases and utilizing inequalities to derive the desired result. The analysis involves various parameters and conditions to establish the validity of the statement. The proof involves bounding the ratio of measures of sets using an exponential function. For a DNN with one hidden layer and tanh activation, if the target function has two non-zero frequencies, the loss function decreases at different rates. Positive constants c and C exist such that for small \u03b4, a ball B \u03b4 \u2282 R m centered at the origin satisfies the Lebesgue measure condition. The proof involves using the gradient descent algorithm to show that Eq. (45) is a sufficient condition for S. The DNN is trained to fit a natural image by capturing details from low to high frequencies. The Fourier transform of the image with respect to x is studied as an illustration of the F-Principle. The DNN is trained to fit a natural image by capturing details from low to high frequencies. The Fourier transform of the image with respect to x is studied to illustrate the F-Principle. The DNN can capture the 1-d slice well after training, with the amplitudes of the frequency components converging from low to high. Initialization with large parameters leads to noisy DNN output at test pixels. The DNN output fluctuates for pixels at the red dashed lines, with unclear convergence order of frequency peaks compared to small initial parameters."
}