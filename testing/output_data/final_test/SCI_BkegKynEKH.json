{
    "title": "BkegKynEKH",
    "content": "We propose a probabilistic framework for session-based recommendation, updating a latent variable for user state as they view more items. Computational solutions include the re-parameterization trick and the Bouchard bound for the softmax function. Variational auto-encoder and Expectation-Maximization algorithms are used to tighten the variational bound. The Bouchard bound enables fast noisy gradients, resulting in a fully probabilistic algorithm similar to word2vec and a fast online EM algorithm. Our model presents a generative process for user's co-viewed products in sessions, using a K-dimensional latent variable to represent user interests. The session length is denoted by T u, and Bayesian inference is used to infer the user's interests based on their viewing history. Our proposal suggests using Bayesian inference to infer user interests as a feature for training a recommender system. Approximations can be made accurately using Stan or variational methods. The results show good agreement, with single product views indicating interest with uncertainty, while multiple views show high certainty. The model introduced for next item prediction involves using Monte Carlo samples to approximate the true predictive distribution. The Bouchard bound introduces further approximation and variational parameters to produce an analytical bound. The re-parameterization trick involves simulating and optimizing a noisy lower bound to prevent variational parameters from growing with data. To prevent variational parameters from growing with data, a variational auto-encoder is employed. The method involves using a flexible function for approximation and testing the model's predictive ability. The noisy lower bound can be decomposed over the denominator of the softmax, similar to the word2vec algorithm. The text discusses two alternative methods for training a model: Bouch/AE, a linear variational auto-encoder using the Bouchard bound, and RT/Deep AE, a deep auto-encoder using the re-parameterization trick. Results for recall@5 and discounted cumulative gain at 5 are shown in Table 1. The EM algorithm allows an approximation of q(\u03c9) assuming (\u03a8, \u03c1) and a user history v1, .., vT are known. The algorithm is the dual of the one presented in Bouchard (2007). The EM algorithm is a fixed point update that decomposes into a sum, with terms from the softmax in the denominator. By substituting a co-ordinate descent update with a gradient descent step update, the algorithm can be written as a sum. This allows for the application of the Robbins Monro algorithm, making it faster than the generic EM algorithm for large P. The online EM algorithm and negative sampling SGD approach are distinct in their efficiency. The online EM algorithm and negative sampling SGD approach sub-sample the denominator of the softmax, allowing for more efficient optimization compared to other methods like stochastic variational inference. Our method uses an auto-encoder for direct SGD optimization, differentiating it from previous approaches."
}