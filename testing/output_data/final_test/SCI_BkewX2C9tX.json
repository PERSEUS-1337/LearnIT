{
    "title": "BkewX2C9tX",
    "content": "Federated learning involves distributing model training among multiple agents who share only model updates for aggregation. This work explores model poisoning attacks by a single malicious agent in federated learning, aiming to cause misclassification with high confidence. Various attack strategies are considered, including boosting the malicious agent's update and using alternating minimization for stealth. Parameter estimation for benign agents' updates is also used to enhance attack success, along with interpretability techniques for visualization. Federated learning, introduced by BID11, is a popular method for distributed deep neural network training. It involves multiple agents training a model in rounds, sharing only parameter updates. Model poisoning attacks by a single malicious agent aim to cause misclassification with high confidence. Interpretability techniques are used to visualize model decisions, showing vulnerability in the federated learning setting. The centralized parameter server facilitates aggregation of updates from selected agents for model training in federated learning. Privacy concerns drive the design to prevent server visibility into agents' local data. A single malicious agent aims to introduce a targeted backdoor in the global model by optimizing for a different objective in each round. The malicious agent in federated learning aims to achieve its objective by optimizing updates while maintaining stealth. Various model poisoning attacks are proposed, including malicious update boosting to counter benign agents' effects. However, boosted updates can be detected as aberrant using stealth measures. The alternating minimization strategy improves attack stealth by boosting updates for the malicious objective. Estimating other agents' updates enhances attack success rates. Visual explanations show that backdoors can be inserted without drastic changes in model focus. In experiments with adversaries controlling a single malicious agent, it was shown that they can influence the global model to misclassify examples with high confidence. Attacks on Fashion-MNIST and Adult Census datasets with 10 and 100 agents resulted in 100% confidence misclassification. An alternating minimization attack ensured the global model converged to the same accuracy as without adversaries. Simple estimation of benign agents' updates improved attack success. Model poisoning attacks are largely unexplored compared to data poisoning attacks. Previous works focused on defending against Byzantine adversaries sending arbitrary gradient updates, aiming for suboptimal models. In contrast, the goal here is to ensure convergence to effective models that misclassify certain examples. The Krum aggregation mechanism is not resilient to these attack strategies. Another study involves colluding agents inducing targeted model replacement at convergence time. Our goal is to induce targeted misclassification in the global model by a single malicious agent in federated learning, even far from convergence. The model poisoning attacks we investigate involve K agents with private data shards, aiming to learn a global parameter vector. In federated learning, the server aims to learn a global parameter vector that minimizes loss over the training data. Traditional poisoning attacks involve malicious agents poisoning data, but in this case, an agent poisons the model updates sent to the server, posing a threat due to the influence on the global model parameters and the variability of updates from non-i.i.d local data. The adversary model assumes one malicious agent in federated learning with access to specific training data, aiming to explore successful model poisoning attacks even with constraints. In the model poisoning setting, a malicious agent aims to increase overall loss on a data subset to achieve targeted misclassification. The agent cannot access the global parameter vector and can only influence it through weight updates provided to the server. The malicious agent aims to achieve her objective on the t th iteration by solving a specific problem. Two different datasets are used to illustrate attack strategies: Fashion-MNIST with 28 \u00d7 28 grayscale images of clothing and footwear items, and the UCI Adult dataset with information about adults from the 1994 US Census. Fashion-MNIST achieved 91.7% accuracy with a Convolutional Neural Network, while the UCI Adult dataset achieved 84.8% accuracy with a fully connected neural network. The fully connected neural network achieved 84.8% accuracy on test set BID6 for the model architecture. Experiments were conducted with the number of agents set to 10 and 100, running federated learning until reaching a pre-specified test accuracy or maximum time steps. The malicious agent aims to misclassify a single example in a desired target class, with a random sample chosen from the test set. For Fashion-MNIST, the target class is '5' (sandal). The malicious agent in the federated learning setting aims to misclassify a sample from the Fashion-MNIST dataset belonging to class '5' (sandal) into class '7' (sneaker). They need to optimize over an estimate of the parameter vector w t G due to the nature of the federated learning algorithm. The limited information poisoning objective in federated learning involves choosing a good estimator for the parameter vector. The malicious agent can ignore the effects of other agents and use gradient-based optimizers like SGD. To overcome scaling effects, the final update returned needs to be boosted. The malicious agent can run gradient-based optimizers starting from w to minimize the loss with respect to \u03b4, achieving the adversarial objective in 4 out of 10 iterations. This attack is successful in causing the global model to misclassify the chosen example in the target class, with delayed updates and higher computational cost compared to a benign agent. The malicious agent can run gradient-based optimizers to achieve its adversarial objective, causing the global model to misclassify examples. Methods for detecting malicious updates are discussed, including accuracy checking of weight updates sent to the server. The server can detect malicious agents by comparing the validation accuracy of their model to the global model. The spread of distances between weight updates can make attacks stealthier. The spread of distances between weight updates can reduce, making attacks stealthier. Qualitative and quantitative methods can be used by the server to detect malicious weight updates. Visualization of weight update distributions can reveal differences between benign and malicious agents. The difference between weight updates from benign and malicious agents is more pronounced for later time steps. A quantitative method using pairwise distances can help identify outliers and detect the malicious agent. The spread of distances between updates can reveal differences, enabling detection of malicious agents. The malicious agent can optimize over the adversarial objective and training loss for its local data shard to bypass detection methods. By concatenating batches of data with instances to be misclassified, the adversarial objective is satisfied with high confidence. However, the accuracy on validation remains low, making this attack easily detectable. The weight update distribution for this attack is visually similar to benign agents but differs in range, enabling detection. The malicious agent can optimize over the adversarial objective and training loss to bypass detection methods by boosting specific weight updates. However, this results in sparse and low-magnitude distributions compared to benign agents. To address this, an alternating minimization attack strategy is proposed to decouple updates for different objectives. The alternating minimization attack strategy aims to achieve its goals by closely matching the accuracy of the malicious model with that of the global model. This attack is able to bypass accuracy checking methods and is stealthier than baseline attacks, as shown by the distribution of weight updates and distances between malicious and benign updates. The malicious agent can increase attack stealth by adding a distance-based constraint on the intermediate weight vector. Constraints based on weight distribution metrics like Wasserstein or total variation distances may be used. The attack achieves its objective at the global model with high confidence from time step t = 2, closely tracking the success of the global model. The weight update distribution for this attack is similar to that of a benign agent. The malicious agent aims to estimate the effects of other agents' updates and choose an appropriate estimate for \u03b4 at each time step. When chosen, it uses global parameter vectors to negate other agents' effects, but estimation inaccuracy and lack of optimizer adjustment pose challenges. The malicious agent uses pre-optimization correction to estimate other agents' updates, assuming accurate information, but faces challenges due to estimation inaccuracy and lack of optimizer adjustment. Results show that attacks using previous step estimation with pre-optimization correction are more effective in achieving the adversarial objective. Neural networks are often seen as black boxes, but interpretability techniques like Layerwise Relevance Propagation help analyze network decisions. Interpretability techniques like Layerwise Relevance Propagation are used to analyze neural network decisions by visualizing neuron activations, image features, and attributing prediction scores. These techniques provide insights into internal feature representations and help discriminate between benign and malicious models. In this paper, the vulnerability of multi-party machine learning algorithms like federated learning to model poisoning adversaries is explored. Targeted perturbation of model parameters with bounded noise ensures stealth in attacks. Future work will focus on more sophisticated detection strategies at the server to provide guarantees against attackers. The vulnerability of federated learning to model poisoning adversaries is demonstrated in this paper. Attacks show that basic federated learning and Byzantine resilient aggregation mechanisms are susceptible. Detection mechanisms can make attacks more challenging but can be overcome. Multi-party machine learning algorithms resistant to these attacks need to be developed. Increasing the number of agents to 100 makes malicious agent selection less frequent, making attacks more challenging. The baseline attack can introduce a targeted backdoor but cannot ensure it for every step due to benign agent updates. The alternating minimization attack can introduce a backdoor and improve classification accuracy on test data, but performance is limited by data availability for malicious agents. Data augmentation may help enhance accuracy. Weight update distributions for different attack strategies on a CNN trained on the Fashion MNIST dataset are shown in Figure B, with varying results for benign and malicious agents. The alternating minimization attack without distance constraints reduces the difference in distributions, but the closest distributions are achieved with the alternating minimization attack with constraints. The alternating minimization attack with distance constraints achieves the closest weight update distributions. 'Krum' is a gradient aggregation mechanism resilient to Byzantine adversaries but ineffective against attacks aiming for convergence to effective models with targeted backdoors. At each time step t, the server receives updates (\u03b4 t 1 , . . . , \u03b4 t n ). The closest updates are chosen to form a set C i, and their distances are added up to give a score S(\u03b4). An attack on Krum with a boosting factor of \u03bb = 2 in a federated learning setup with 10 agents shows the effectiveness of model poisoning attacks. The malicious agent's update is chosen by Krum for most time steps, but the global model struggles to achieve high test accuracy. The alternating minimization attack maintains high test accuracy while the malicious agent is chosen for fewer time steps, demonstrating the vulnerability of Krum to targeted backdoors."
}