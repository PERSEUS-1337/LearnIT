{
    "title": "BkeStsCcKQ",
    "content": "Deep artificial neural networks, like humans and animals, have critical periods where a temporary stimulus deficit can hinder skill development. The extent of impairment depends on the deficit window and network size. Deficits not affecting low-level statistics can be overcome with more training. Fisher Information is used to measure connectivity between network layers during training. Information initially rises but then decreases, leading to a loss of \"Information Plasticity\". The first few epochs are crucial for creating strong connections. The initial learning phase in neural networks is crucial for creating strong connections that remain stable throughout training. Forgetting information in the weights is necessary for achieving invariance and disentanglement in representation learning. Critical periods in learning systems, whether biological or artificial, are essential for skill development and can lead to permanent deficits if missed. During early post-natal development, sensory deficits can cause permanent skill impairment. Critical periods affect various species and systems, leading to conditions like amblyopia if uncorrected. Deep neural networks respond to sensory deficits similarly to humans and animals, suggesting critical periods may be related to information processing rather than biochemical factors. The study examines the information in weights of DNNs during training, showing a non-monotonic increase followed by a reduction phase. Critical periods coincide with the memorization phase, and permanent impairment occurs if deficits are not corrected early. This behavior is consistent across tasks and network architectures. The study focuses on the initial learning phase of artificial neural networks (ANNs) and compares responses to early deficits with animal models. Despite differences, similarities are found in how deficits affect network bias towards optimal solutions. The quality of the solution in artificial neural networks depends critically on the period prior to initial convergence, rather than the final or early training phases. Similar to animal models, sensory deficits during critical periods induce changes in network architecture. The Fisher Information of the weights increases during a \"learning phase\" and then consolidates during a \"compression\" phase. During the learning phase, the network's Fisher Information increases, followed by a consolidation phase where it decreases and stabilizes. Sensitivity to critical-period-inducing deficits peaks when the Fisher Information peaks. The hierarchy of low-level and high-level features in the training data plays a crucial role in the network's effective connectivity. The loss of \"Information Plasticity\" in the network, not influenced by external factors, leads to the existence of critical periods. Amblyopia is an example of a critical period-inducing deficit. Amblyopia, a critical period-inducing deficit, can affect humans and is caused by unilateral cataracts during infancy or childhood. Even after surgical correction, reduced visual acuity in one eye may persist. Sensitivity to deficits is highest during the early rapid learning phase, before plateauing. The network's performance is largely unaffected by temporary deficits afterwards. The ability of patients to regain normal acuity in the affected eye depends on the deficit's duration and age of onset. To replicate this in artificial neural networks (ANNs), a convolutional network is trained to classify objects in images. The effect of cataracts is simulated by downsampling and then upsampling the images, blurring them. The network's final performance is graphed after training for 300 epochs. The study graphed the final performance of a neural network correcting a deficit over different epochs. A critical period was observed where if the blur was not removed within the first 60 epochs, the final performance significantly decreased. This phenomenon resembled critical periods seen in early development in humans and animal models. The study observed a critical period in a neural network's early learning phase where sensitivity to image blurring peaked around 30 epochs. Later deficits had little to no effect on the network's final performance. This phenomenon resembled critical periods seen in early development in humans and animal models. Olson and Freeman conducted an experiment on kittens, monocularly deprived for 10-12 days after birth, to study the effects on sensitivity. Different training data modifications were evaluated, including complete sensory deprivation and high-level modifications. Permanent impairment in performance was observed if deficits were not corrected early. Critical periods during early learning phases were found to be crucial for test accuracy improvement. The study focused on monocularly deprived kittens to analyze visual acuity development and sensitivity to deficits. Results showed that early rapid learning phases were critical for test accuracy improvement, with the network largely unaffected by temporary deficits afterwards. The degree of functional disconnection in V1 monocular cells also varied with the age of kittens at the onset of deficits. The study focused on the critical early transient phase in determining the final solution of training an artificial neural network. Information Plasticity, quantified using Fisher Information, decreases during the reorganization phase and correlates with deficit sensitivity during critical periods. Our contribution discusses the importance of forgetting in achieving invariance to nuisance variability and independence of representation components. Loss of Information Plasticity may lead to reduced adaptability later in training, potentially impacting neural plasticity. These findings also shed light on the practice of pre-training models before fine-tuning for transfer learning. Pre-training for transfer learning can be detrimental, even with similar tasks. Critical period-related deficits, like amblyopia, can be caused by sensory deprivation. High-level perturbations do not induce critical periods, while network depth affects the critical period profile. The study aims to investigate the effects of deficits in deep neural networks (DNNs) by training a standard All-CNN architecture on small images from the CIFAR-10 dataset. The depth of the network influences the impact of deficits during the critical period, similar to the effects of cataracts on patients' visual acuity. The study simulates the effect of cataracts on deep neural networks by downsampling and then upsampling images in the dataset. If the blur is not removed within the first 40-60 epochs, the network's final performance is severely decreased. This decrease in performance follows trends observed in animals and can be compared to the loss of visual acuity in kittens. The sensitivity to a blur deficit in deep neural networks peaks around 30 epochs during early rapid learning, similar to observations in kittens monocularly deprived from birth. High-level deficits do not have a critical period, as seen in experiments with animals. Deficits in deep neural networks do not have a critical period for learning, unlike in animals. Even high-level deficits like image flipping or label permutation do not significantly impact the network's performance, as it quickly recovers after correction. This suggests a complex relationship between data distribution and optimization algorithms. Further experiments with drastic deficits, such as replacing images with white noise, show the network's ability to adapt and recover. The extreme deficit of replacing images with white noise in a neural network shows milder effects compared to image blur. Training with white noise provides no information on natural images, leading to less severe consequences. This phenomenon is similar to sensory deprivation in animals, where dark-rearing can have less severe effects than light-reared animals. Further details can be found in Appendix C. The fully-connected network trained on MNIST and ResNet-18 trained on CIFAR-10 both exhibit a critical period for image blur deficit. The presence of this critical period is not dependent on the convolutional structure or natural images. ResNets show a sharper critical period than standard convolutional networks, suggesting it is not due to vanishing gradients. The depth of the network plays a critical role in the presence of the critical period. Additionally, the critical period persists even with a constant learning rate, ruling out annealed learning rate as an explanation. When using Adam optimization, a critical period similar to standard SGD is observed. Training has two phases: information sharply increases, then decreases during a consolidation phase. Test accuracy improves slightly as less information is stored in the weights. The weights' Fisher Information correlates with the network's sensitivity to critical periods. The connection between FIM and connectivity is compared to synaptic density during development. The connection between FIM and connectivity is compared to synaptic density during development in the visual cortex of macaques. A rapid increase in connectivity is followed by synapse elimination throughout life. Effects of critical period-inducing blurring on Fisher Information show that impaired networks use more information to solve tasks. Increasing weight decay can make critical periods longer and less sharp, slowing network convergence and limiting higher layer changes. The initial phases of training are crucial for both animals and DNNs. In animals, changes in brain architecture are linked to deficits, while in artificial networks, connectivity remains fixed. Perturbing weights in a network can reveal the importance of specific connections by measuring the change in the final distribution. The Fisher Information Matrix (FIM) measures how perturbing network weights affects output, suggesting low FIM weights can be pruned without impacting performance. FIM also approximates the Hessian of the loss function, indicating curvature of the loss landscape during training. The FIM measures the curvature of the loss landscape during training, connecting it to optimization procedures. The full FIM is too large to compute, so the trace is used to measure global connection strength efficiently. Off-diagonal terms are captured using the logdeterminant of the full matrix. ResNets are used for their smooth landscape, while other architectures employ a more robust FIM estimator based on noise injection. The FIM is a measure of information in the model, with phases of learning where connection strength increases initially but decreases later. This \"forgetting\" phase eliminates redundant connections while performance continues to improve. During the \"forgetting\" phase of learning, unnecessary synapses are pruned in both biological brains and DNNs. This process is closely related to sensitivity to deficits like image blur, as shown in quantitative analysis. Sensitivity during critical periods aligns with the FIM trend, despite being computed at different points in network training. Deficits have a noticeable impact on connection strength and performance. The presence of deficits during network training leads to increased information retention even after the deficits are removed. Layer-wise analysis shows that deficits can shift the importance of connections in the network, especially when initially trained on blurred data. Trained on blurred data, the network's connections are dominated by the top layer. Removing deficits early allows the network to reorganize, changing information distribution across layers. If deficits are removed after consolidation, the network loses its ability to change connectivity, indicating a lack of Information Plasticity. The analysis of the Fisher Information Matrix (FIM) provides insights into the loss function geometry and learning dynamics. The Fisher Information Matrix (FIM) analysis reveals the network's learning dynamics and loss function geometry. Learning involves crossing bottlenecks, with high curvature regions in the initial phase and lower curvature regions during consolidation. Early convergence phases are crucial for guiding the network towards the optimal valley. In the presence of an image blur deficit until epoch 100, resources are allocated more to higher layers (6-7) than middle layers. The blur deficit at epoch 100 leads to more resources allocated to higher layers (6-7) rather than middle layers, resulting in the destruction of low-and mid-level features. Even if the deficit is removed, the middle layers remain underdeveloped, showing a redistribution of information and changes in effective connectivity known as \"Information Plasticity\". A vertical flip deficit does not induce a critical period, with no impact on the quantity of information in the layers. Artificial neural networks exhibit critical period phenomena, with the initial transient phase playing a crucial role in determining the network's asymptotic performance. The use of Fisher Information reveals how the network rapidly adjusts its connectivity to optimize information processing. This work is inspired by biological critical periods and highlights the importance of studying the initial phase in artificial networks. Artificial neural networks exhibit behaviors similar to critical periods in biology. The initial rapid memorization phase is followed by a loss of Information Plasticity, which surprisingly improves performance. Two distinct phases of training have been observed, linked to an increase in gradients' covariance. This work highlights the importance of studying the initial phase in artificial networks. The study links two training phases in artificial neural networks to an increase in gradients' covariance. Unlike Fisher Information analysis, the covariance and norm of gradients do not correlate with sensitivity to critical periods. However, a connection between Fisher Information analysis and information in activations can be established. Our analysis supports claims from previous research and expands on the advantages of Fisher's Information in analyzing deep learning models. Fisher's Information allows for easy estimation, is less sensitive to mutual information estimator choice, and enables probing of connectivity changes across network layers. This analysis emphasizes the importance of considering both the amount and accessibility of information in network activations. In our work, we focus on the temporal evolution of the weights in deep learning models. It is important to note that simpler weights in a network also require a simpler smooth representation, as measured by the RBF embedding. The FIM requires a simpler smooth representation, measured by the RBF embedding, to resist perturbations of the weights. Analyzing the joint spatio-temporal evolution of the network using both frameworks could be interesting. Focusing on the information of the weights provides insight into the \"effective connectivity\" during critical periods, which can be compared to similar readouts in animals. Deficits in deprived animals are reflected in abnormalities in the circuitry of visual pathways, characterized in DNNs. Abnormalities in the visual pathways of DNNs are characterized by the FIM to study \"effective connectivity\". Sensitivity to critical periods aligns with changes in synaptic plasticity. Visual deficits impact higher layers more than lower layers, and reversing these effects after the critical period is challenging. Similar responses are observed in animal models. In animal models, visual pathways show little remodeling at lower and higher levels upon deprivation. Critical periods in neuronal networks suggest initial instability and plasticity, leading to stable configurations. Sensitivity to deficits peaks during connection remodeling, with different connectivity profiles observed in trained networks. High-level deficits like image flipping and label permutation also impact learning. Pre-training on blurred data can decrease network performance, unlike common pretraining practices that improve it. Deficits like image flipping do not have a critical period and can impact learning. The network training process involves moving towards high-curvature regions of the loss landscape before eventually converging to a flat minimum. This transition is explained by the interpretation of the FIM as an approximation of the local curvature, suggesting the network crosses narrow bottlenecks to learn useful features. After training, neural networks move towards high-curvature regions of the loss landscape before converging to flat minima. The critical period occurs upon crossing a bottleneck, with evidence showing that convergence to flat minima correlates with good generalization performance. The network's performance is mostly determined during the early \"sensitive\" phase, with final sharpness at convergence potentially being an epiphenomenon. The goal is to understand artificial networks rather than investigate the human or animal brain. The goal of \"Artificial Neuroscience\" is to develop explainable artificial intelligence systems by studying information processing in artificial networks using biological phenomena as probes. This approach aims to address the lack of understanding in modern artificial networks compared to traditional mathematical models used in neuroscience. The Fully Connected network used for the MNIST experiments has hidden layers of size [2500, 2000, 1500, 1000, 500] with batch normalization and ReLU activations. Data augmentation includes random translations and horizontal flipping, with images padded to size 32 \u00d7 32. For MNIST, images are padded to size 32 \u00d7 32 and random horizontal flipping is applied. The trace of the Fisher Information Matrix is computed using a derived expression, dependent on local gradients of the loss with respect to weights. Different techniques are used for irregular loss landscapes, such as ResNets BID20 or a method proposed in BID0 involving minimizing a diagonal matrix \u03a3 with a parameter \u03b2 for smoothness. The Fisher Information Matrix trace is estimated using a parameter \u03b2 for smoothness in the approximation. The method in BID14 efficiently minimizes the loss function L, which can be locally approximated. By setting the derivative with respect to \u03a3 to zero, \u03a3 ii = \u03b2/H ii is obtained. This allows for estimating the trace of the Hessian and the Fisher information. Sensitivity curves and synaptic density profiles are fitted using an exponential equation with unconstrained parameters. The Fisher Information trace is related to the training of a network without deficits. The Fisher Information trace is related to the training of a network without deficits, with sensitivity computed using a window of size k. Presence of deficits does not decrease gradient magnitude in the last epochs, suggesting vanishing gradients are not the cause of critical periods for deficits. Additional plots show gradient means and standard deviation during training with different deficits. The network allocates more resources to lower-layers during a noise deficit compared to a blur deficit. When a blur deficit is present, the network learns only smooth filters for low-frequencies, and struggles to learn high-frequency filters even after the deficit is removed. Critical periods are task-specific, with visual acuity being the focus in this study. In animals, visual acuity is traditionally measured by testing the ability to discriminate between black-and-white contrast gratings and a uniform gray field. Convolutional Neural Networks measure performance through image classification tasks optimized for CNNs. Cataracts were simulated in DNN experiments to explore interactions with data structure and network architecture. In FIG0, the performance loss in a DNN trained with a cataract-like deficit is compared to results from monocularly deprived kittens, showing similar trends. Simulating complete visual deprivation in a neural network is complex, as a constant blank input leads to trivial learning. To model sensory deprivation, random Gaussian noise is used as input data for the neural network. This approach forces the network to memorize the association between noise patterns and their corresponding labels, creating a non-trivial learning task."
}