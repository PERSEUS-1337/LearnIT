{
    "title": "HkxeThNFPH",
    "content": "We study continuous action reinforcement learning problems where the agent must interact with the environment through safe policies to stay in desirable situations. These problems are formulated as constrained Markov decision processes (CMDPs), and we propose safe policy optimization algorithms based on a Lyapunov approach. Our algorithms can utilize standard policy gradient methods like deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO) to train a neural network policy while ensuring near-constraint satisfaction for every policy update. Compared to existing constrained PG algorithms, ours are more data efficient as they can utilize both on-policy and off-policy data. Our action-projection algorithm leads to less conservative policy updates and integrates naturally into an end-to-end PG training pipeline. Evaluation against state-of-the-art baselines on simulated tasks and a real-world robot obstacle-avoidance problem shows the effectiveness of our algorithms in balancing performance and constraint satisfaction in reinforcement learning. In reinforcement learning, optimizing the expected sum of rewards is the goal, but it can be harmful in some cases. Safety is crucial, and constraints can be incorporated to ensure safe policies are followed during training and convergence. Constrained Markov decision process (CMDP) is a standard model for RL with constraints, where the agent must satisfy auxiliary cost expectations in addition to the main objective. Solving CMDPs with unknown models or large state/action spaces can be challenging. The Lagrangian method is commonly used, but it lacks safety guarantees during training. Constrained policy optimization (CPO) is a recent algorithm that aims to solve CMDPs at scale while ensuring safety. Constrained policy optimization (CPO) extends TRPO to handle constraints effectively, showing promising results in scalability, performance, and constraint satisfaction. Chow et al. utilize Lyapunov functions to ensure stability in RL, guarantee closed-loop stability, and bring model-based RL agents back to a \"region of attraction\" during exploration. In this paper, the authors extend Lyapunov-based algorithms to tackle continuous action problems in control theory and robotics. They propose safe RL algorithms that can handle large or infinite action spaces, addressing challenges in policy updates and optimization over continuous action spaces. The authors propose safe RL algorithms for handling large or infinite action spaces in control theory and robotics. They switch to policy gradient algorithms to address challenges in policy updates and propose two approaches to solve constrained policy optimization problems. The authors introduce safe RL algorithms for large or infinite action spaces in control theory and robotics. They propose two approaches for constrained policy optimization: policy parameter projection and action projection, ensuring safety at every policy update. Our proposed algorithms for constrained policy optimization offer a safeguard policy update rule to ensure near-constraint satisfaction and can work with any on-policy (e.g., PPO) and off-policy (e.g., DDPG) PG algorithm, making them potentially more data-efficient. Our algorithms for constrained policy optimization can be trained end-to-end, outperforming baselines in balancing performance and constraint satisfaction. They are evaluated on various tasks and show better generalization to new environments. In constrained policy optimization, the RL problem involves modeling the agent's interaction with the environment as a Markov decision process (MDP). The MDP includes state and action spaces, a discounting factor, immediate cost function, transition probability distribution, and initial state. Constraints on cumulative cost are addressed using Constrained Markov Decision Processes (CMDPs), which introduce additional costs and constraints. The optimization problem in Constrained Markov Decision Processes (CMDPs) involves finding an optimal policy that satisfies a safety constraint on the expected cumulative cost. Policy gradient algorithms are used to optimize policies by estimating the gradient of the expected cumulative cost. If the feasibility set is non-empty, an optimal policy exists in the class of stationary Markovian policies. Policy gradient (PG) algorithms optimize policies by estimating the gradient of the expected cumulative cost induced by the policy. Stochastic policies are parameterized by a vector \u03b8, and DDPG and PPO are popular PG algorithms for solving continuous control problems. DDPG is an off-policy Q-learning algorithm that trains a deterministic policy and a Q-value approximator. The PPO algorithm used in this paper is a penalty form of TRPO with an adaptive rule to tune the DKL penalty weight. It trains a policy by optimizing a loss function that includes a penalty on the KL-divergence between current and previous policies. The Lagrangian method is used to address constraints in CMDPs by adding constraint costs to task costs and transforming the optimization problem into a penalty form.\u03b8 and \u03bb are jointly optimized to find a saddle-point of the penalized objective. In this paper, the Lagrangian method is discussed for addressing constraints in CMDPs by adding constraint costs to task costs. The optimization problem is transformed into a penalty form, with \u03b8 and \u03bb jointly optimized to find a saddle-point of the penalized objective. The approach involves optimizing the cost function c(x, a) + \u03bbd(x) using stochastic gradient descent, but it may lead to unsafe policies due to the current value of \u03bb. The Lagrangian method may not be suitable for problems where safety is crucial during training. The Lyapunov-based approach to CMDPs is extended to PG algorithms, introducing important terms and notations from previous work for developing safe PG algorithms. The Lyapunov approach aims to construct a Lyapunov function that guarantees an optimal policy in CMDPs. Chow et al. (2018) show that the Lyapunov function can be expressed as L\u03c0B(x) := E\u221et=0 \u03b3td(xt) + (xt) | \u03c0B, x, where \u03c0B is a feasible policy satisfying Lyapunov constraints. This function ensures that FL contains an optimal solution of the optimization problem. The Lyapunov approach aims to construct a Lyapunov function that guarantees an optimal policy in CMDPs. Chow et al. (2018) propose ways to construct an auxiliary constraint cost \u03c0B, x, for policy improvement. They introduce safe policy and value iteration algorithms to solve an LP during policy improvement, involving value and state-action value functions. In this paper, the focus is on problems with large action spaces, making solving optimization problems numerically challenging. The approach switches from value-function-based algorithms to PG algorithms and proposes an optimization problem with Lyapunov constraints suitable for PG. Two efficient methods are presented to solve the proposed optimization problem for CMDPs, ensuring safety both at convergence and during training. The Lyapunov-based safe PG algorithms presented in this paper aim to solve CMDPs efficiently by incorporating two approaches: \u03b8-projection and a-projection. The \u03b8-projection approach utilizes a constrained optimization method to project the policy parameter \u03b8 onto feasible solutions, while the a-projection approach embeds Lyapunov constraints into the policy network. These methods ensure safety in convergence and training. The minorization-maximization technique in conservative PG and Taylor series expansion can be applied to on-policy and off-policy algorithms. The surrogate cumulative cost is used to improve policy parameters by approximating the objective function with its Taylor series expansion. The Taylor series expansion is used to approximate the objective function and constraints in the safe PG algorithm, allowing for updates to the policy. To address numerical instability, the Lyapunov constraint is approximated using an average constraint surrogate. The Lyapunov value function gradient can be effectively computed by choosing a special case for the auxiliary constraint surrogate. This leads to an approximation of the average constraint surrogate, showing that CPO belongs to the class of Lyapunov-based PG algorithms with \u03b8-projection. SDDPG and SPPO are the DDPG and PPO versions of these safe PG algorithms. The Lyapunov approach breaks down trajectory-based constraints into single-step state dependent constraints. When dealing with infinite state spaces, enforcing Lyapunov constraints directly into policy update optimization is challenging. To address this, a safety layer concept is leveraged to embed Lyapunov constraints into the policy network, reformulating the CMDP problem as an unconstrained optimization task for policy parameter optimization using standard PG algorithms. The safety layer ensures feasible actions by projecting unconstrained actions onto Lyapunov constraints, guaranteeing safety during training. The projection problem involves mapping policy parameters to the feasibility set induced by the constraints. The Lyapunov safety layer projects unconstrained actions onto constraints to ensure feasible policy, simplifying policy optimization by solving an unconstrained problem using a standard PG algorithm. The safety layer approximates the Lyapunov constraint with a first-order Taylor series to control the trade-off between projecting on unconstrained and baseline policies. The Lyapunov safety layer simplifies policy optimization by projecting unconstrained actions onto constraints. The solution to the CMDP problem can be effectively computed using an in-graph QP-solver, such as OPT-Net. The policy \u03c0 \u039e(\u03c0 B ,\u03b8) (x) has an analytical solution when the CMDP has a single Lyapunov constraint. The closed-form solution for the optimization problem in the CMDP with a single Lyapunov constraint involves projecting unconstrained actions onto a safe hyper-plane. This solution can be extended to handle multiple constraints if only one is active at a time. The Lyapunov-based safe PG algorithms, SDDPG a-projection and SPPO a-projection, are evaluated for performance and robustness in simulated robot scenarios. The study evaluates the performance and robustness of Lyapunov-based safe PG algorithms in simulated robot locomotion tasks with physical constraints. Tasks include HalfCheetah-Safe, Point-Circle, Point-Gather, and Ant-Gather, each with specific constraints to ensure safe and smooth movement. The study introduced tasks with constraints based on MuJoCo tasks Point and Ant. Algorithms were compared with unconstrained methods DDPG and PPO, and constrained methods Lagrangian and on-policy CPO. SPPO algorithm, a variant of CPO, was used for policy optimization without backtracking line-search. Our Lyapunov-based PG algorithms are stable in learning and converge to feasible policies with reasonable performance. The algorithms quickly stabilize the constraint cost below the threshold, while unconstrained DDPG and PPO violate constraints. Lagrangian method can be sensitive to the initialization of the Lagrange multiplier \u03bb 0. The experiments show that DDPG algorithms tend to learn faster than PPO counterparts in HalfCheetah, PointGather, and AntGather tasks. However, PPO algorithms perform better in terms of constraint satisfaction. A-projection in DDPG and PPO leads to faster convergence and lower constraint violation compared to \u03b8-projection counterparts. This suggests that a-projection generates smoother gradient updates during training. The curr_chunk discusses evaluating safe policy optimization algorithms on a real robot task involving map-less navigation. The goal is to drive a robot to a goal efficiently while avoiding collisions to limit damage. The robot's observations include goal position, velocity, and Lidar measurements, with actions being linear and angular velocity. The robot must navigate to goal positions in a new environment without a map, using Lidar measurements and velocity actions. A constraint cost is imposed to limit collision impact energy, with a threshold for maximum tolerable impact energy. The robot navigates to goal positions in a new environment using Lidar measurements and velocity actions. A CMDP constraint is required for collision avoidance, with experimental results showing Lyapunov-based PG algorithms have higher success rates in controlling constraints and minimizing distance to goal. The unconstrained method yields lower distance to goal but violates constraints more frequently, leading to lower success rates. The Lagrangian approach is less robust to parameter initialization, resulting in lower success rates and higher variability compared to Lyapunov-based methods. All algorithms converged prematurely with constraints above the threshold due to function approximation error and problem stochasticity. The Lagrangian method tends to zigzag and has more collisions, while SDDPG chooses a safer path to reach the goal. Generalization to longer trajectories and new environments is evaluated in a 22 by 18 meters environment with goals placed within 5 to 10 meters from the robot's initial state. In a larger evaluation environment, success rates decrease as goals are further away. Safety methods outperform unconstrained and Lagrangian methods, even in more challenging tasks. The SL-DDPG policy was deployed on a real Fetch robot in an office environment, completing tasks through narrow corridors and around people. The robot successfully completed tasks in an office environment, avoiding obstacles and exhibiting safety behavior. Lyapunov-based algorithms were used to develop safe RL algorithms for continuous action problems, incorporating \u03b8-projection and a-projection. The algorithms were evaluated on high-dimensional simulated robot locomotion tasks. Our algorithms were evaluated on high-dimensional simulated robot locomotion tasks and an indoor robot navigation problem, demonstrating safe learning, better data-efficiency, and scalability for real-world problems. Future work includes extending a-projection to stochastic policies and using the Lyapunov approach for safe exploration in model-based RL. Assumptions are made regarding the differentiability and feasibility of state-action pairs in the constrained problem. Step size schedules are defined to ensure smoothness and the existence of a local saddle point in policy updates. The update time-scales are categorized based on user-defined parameters. The Lagrangian relaxation procedure is used to convert the constrained problem into an unconstrained problem with a linear function. A local saddle point is found for the minimax optimization problem, ensuring convergence within a hyper-dimensional ball. Policy gradient and actor-critic algorithms are presented to solve the optimization problem incrementally at each time-step. The algorithm uses policy gradient to optimize parameters \u03b8 and \u03bb by descending in \u03b8 and ascending in \u03bb. Trajectories are generated using the current policy \u03c0 \u03b8 k, and gradients are estimated to update \u03b8, \u03bb. The cost, constraint cost, and probability of trajectories are defined, with pseudo-code provided for the proposed PG algorithm. The proposed PG algorithm optimizes parameters \u03b8 and \u03bb using policy gradient. Trajectories are generated with the current policy, and gradients are estimated for updates. The algorithm ensures convergence with step-size schedules for \u03b8 and \u03bb updates on different time-scales. The proposed PG algorithm optimizes parameters \u03b8 and \u03bb using policy gradient. Two actor-critic algorithms are introduced to address high variance in gradient estimates, updating parameters incrementally after each state-action transition. The actor-critic algorithms presented optimize parameters \u03b8 and \u03bb using policy gradient. The algorithms use gradient estimates and a projection operator to ensure convergence. Different step-size schedules are used for critic, policy, and Lagrange multiplier updates, resulting in three time-scale stochastic approximation algorithms. The PG theorem is used to show the relationship between the discounted visiting distribution, action-value function, TD error, and value function estimator V \u03b8. The actor-critic algorithms optimize parameters \u03b8 and \u03bb using policy gradient. The critic traditionally uses linear approximation for the value function, but with advances in deep neural networks, it is now common to model the critic with a deep neural network. The Lyapunov approach to solving CMDPs is revisited in this section. The Lyapunov approach to solving CMDPs, as proposed by Chow et al. (2018), focuses on developing safe policy optimization algorithms. Assuming access to a baseline feasible policy, Lyapunov functions are defined with constraints called Lyapunov constraints. Any Lyapunov-induced policy is feasible and satisfies certain properties, but may not always be optimal. Designing a Lyapunov function is necessary to find an optimal policy. The main goal is to construct a Lyapunov function that guarantees an optimal policy. Chow et al. (2018) show that the Lyapunov function can be expressed with an auxiliary constraint cost and if the baseline policy satisfies certain conditions, then the Lyapunov function candidate also satisfies the properties needed for an optimal policy. Efficient estimation of the distance between baseline and optimal policies is crucial. The set of Lyapunov-induced feasible policies can lead to an optimal policy, with a unique fixed point V * for the safe Bellman operator. The safe Bellman operator has a unique fixed point V* which can be used to construct an optimal policy through greedification. Chow et al. (2018) propose approximating V* with an auxiliary constraint cost to include the optimal policy in the set of feasible policies. This approach aims to address challenges in verifying the assumption required for solving the problem using standard dynamic programming algorithms. The safe policy iteration algorithm proposed by Chow et al. (2018) aims to include the optimal policy in the set of feasible policies by solving a linear program. The algorithm updates the policy by evaluating the Lyapunov function and cost value function iteratively. The safe policy iteration algorithm updates the Lyapunov function via bootstrapping, ensuring consistent feasibility and monotonic policy improvement. Despite its value-function-based nature, it can be adapted for continuous action problems by leveraging its properties to develop safe policy optimization algorithms. Our algorithms combine DDPG or PPO with a SPI-inspired critic to evaluate and guarantee safe policy updates. The Lyapunov function is used to restrict policy selection. The derivation of \u03b8-projection and a-projection procedures is detailed, along with pseudo-codes of safe PG algorithms. The objective function in the constrained minimization problem contains a regularization term to control the distance between \u03b8 and \u03b8 B. The original Lyapunov constraint can be approximated with a simpler constraint. A more stable way is to use the average constraint surrogate. In a special case, the auxiliary constraint surrogate is chosen as a constant. The Lyapunov action-value function is written as Q L \u03b8 B (x, a) and the Lyapunov average constraint surrogate is re-written using an auxiliary constraint cost to ensure constraint satisfaction. This constraint is equivalent to the one used in CPO. The Lyapunov action-value function Q L\u03c0 B (x, a) can be re-written using a first-order Taylor series expansion. The objective function in the action-projection problem contains a regularization term to control the distance between actions. The original action-based Lyapunov constraint can be approximated with a simplified constraint. If the auxiliary cost is state-independent, the action-gradient term is equal to the gradient of the constraint action-value function Q W \u03b8 B. The proof of the a-projection approach is completed by deriving safe Lyapunov-based policy gradient algorithms with \u03b8-projection and a-projection. A safeguard policy update rule is proposed to address infeasible policy updates caused by function approximation errors. The safeguard update quickly recovers from bad steps but may be overly conservative. It uses a limiting search direction when the policy is unsafe. To extend this to multiple constraints, gradient descent is done on the constraint with the worst violation. Constraint tightening can also be done by setting the threshold as d0 \u00b7 (1 \u2212 \u03b4), where \u03b4 \u2208 (0, 1) provides a safety buffer. Cost-shaping techniques can smooth out sparse constraint costs for safety, but require knowledge of the environment. The construction of the cost-shaping term for safe PG algorithms is complex due to the need for environment knowledge. The process involves estimating critics Q \u03b8 (x, a) and Q D,\u03b8 (x, a) using trajectories, training functions for DDPG and PPO, and updating policy parameters based on a QP problem solution with an LP constraint. The experiments involve safety-augmented versions of MuJoCo domains like HalfCheetah-Safe and Point Circle. Safety constraints are added to the HalfCheetah environment to limit speed, while the Point Circle environment features a point mass controlled via a pivot. The agent in the Point Circle environment is a point mass controlled via a pivot, rewarded for moving counter-clockwise along a circle of radius 15. The safety constraint requires the agent to stay within |x| \u2264 2.5. Episodes are 65 steps long, and the constraint threshold is 7. The policy parameter is updated using trajectories to estimate the critic and constraint critic functions. The agent in the Point Circle environment is controlled by a point mass and rewarded for moving along a circle. The safety constraint requires the agent to stay within a certain radius. In the Point Gather environment, the agent collects apples and avoids bombs, with specific rewards and penalties. The safety constraint is based on the number of bombs collected. In the Ant Circle environment, the agent is an Ant robot that collects apples and avoids bombs. Rewards and penalties are given for collecting apples and bombs, with a penalty for premature termination. Different agents are used in the experiments, each with specific state and action spaces. Neural networks are used to model actor and critic policies, with GAE-\u03bb used for gradient estimation. In the experiments with different agents in the Ant Circle environment, various algorithms (SDDPG, SPPO, CPO, etc.) were tested with different parameter settings through grid-search. The best results for each algorithm were chosen based on return and constraint satisfaction criteria. The experiments in the Ant Circle environment tested various algorithms (SPPO, SDDPG, CPO, etc.) with different parameter settings through grid-search. The best results for each algorithm were chosen based on return and constraint satisfaction criteria, with systematic exploration of hyper-parameters. In experiments comparing safe RL algorithms derived from \u03b8-projection and a-projection with unconstrained and Lagrangian baselines in PointGather, AntGather, PointCircle, and HalfCheetahSafe environments using DDPG and PPO versions, the Lagrangian algorithm outperforms safe RL algorithms in PointCircle DDPG but violates constraints more often. Only in PointCircle PPO does Lagrangian perform similarly in terms of return and constraint violation. The effectiveness of Lyapunov-based safe RL algorithms is demonstrated in various experiments, showing superior performance compared to the Lagrangian method. The mapless navigation task involves guiding a robot to a hidden goal position without collisions, showcasing the algorithm's capabilities in continuous control tasks. The agent's observations include relative goal position, relative goal velocity, and Lidar measurements. The action is a linear and angular velocity vector at the robot's center of mass. The transition probability captures the noisy differential drive robot dynamics. The P2P task involves navigating a robot to reach a goal within 30 centimeters using a simplified kinematics simulator with added noise. The robot weighs 150 kilograms, can reach a maximum speed of 7 km/h, and faces challenges due to sensor noise, localization issues, and unexpected obstacles. More details can be found in (Chiang et al., 2019). The robot, weighing 150 kilograms and reaching a maximum speed of 7 km/h, operates in a non-discounting CMDP framework with a finite horizon of T = 100. The goal is to train a policy \u03c0* to navigate the robot to the goal while minimizing the impact energy of obstacle collisions. The immediate cost for reaching the goal is measured by c(x, a) = g^2, and a constraint cost d(x, a) = \u0121 \u00b7 1{ l \u2264 r impact }/T is imposed to limit collision impact energy. The total impact energy is proportional to the robot's speed during collisions, with a constraint threshold d0 defining the maximum tolerable impact energy. The robot is guided along the shortest path to the goal while minimizing obstacle collision impact energy. Safety during training is crucial due to limited data for policy deployment and sample collection."
}