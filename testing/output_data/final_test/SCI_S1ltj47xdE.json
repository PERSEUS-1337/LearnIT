{
    "title": "S1ltj47xdE",
    "content": "In this paper, a weak supervision framework for neural ranking tasks is proposed based on the data programming paradigm. The framework leverages multiple weak supervision signals from unsupervised ranking functions and semantic feature similarities. A BERT-based passage-ranking model trained in this framework achieves new state-of-the-art performances on benchmark datasets without using ground-truth training labels. The model outperforms the BM25 baseline by a large margin on all three datasets and even beats previous state-of-the-art results on two datasets. The progress of deep neural ranking models in the information retrieval (IR) and reading comprehension (RC) communities is evident with the development of large-scale datasets. Weak supervision, utilizing low-quality labels from various sources, is a strategy to overcome the expensive process of creating hand-labeled ranking datasets. Signals extracted from noisy labels are used to train models, as demonstrated by BID4. In the context of information retrieval and reading comprehension, weak supervision techniques are utilized to train deep neural ranking models. BID4 demonstrated the effectiveness of training models on labels generated from BM25 scores. BID11 expanded on this by incorporating external news corpus for training. This work focuses on leveraging weak supervision signals from diverse sources when relevance judgments are not available. BID14 introduced a data programming framework for weakly supervised data creation and model training. In neural ranking models, binary labels are generated for each query-passage pair, with a focus on BERT-based ranking. In this work, a BERT-based ranking model BID5 is utilized, achieving state-of-the-art performance on benchmark datasets with full supervision. The contributions include a data programming framework for ranking tasks and training the BERT model using weak supervision signals, outperforming BM25 baseline and previous state-of-the-art performance on three datasets without ground-truth labels. In this work, a BERT-based ranking model BID5 is utilized, achieving state-of-the-art performance on benchmark datasets with full supervision. The model is trained using weak supervision signals to estimate the relevance of passages to a given query. BERT is applied as the scoring model, concatenating the query and passage for input. The final output is the relevance score between the query and passage. In the supervised setting, the BERT ranking model is trained using pairwise hinge loss with ground truth relevance labels. BERT-PR utilizes weak supervision for PR tasks, following a pipeline with labeling functions to generate noisy labels and aggregating them for more accurate labels. The text discusses the use of noisy labels to improve accuracy in training a supervised model for ranking queries. It simplifies the task by labeling candidate passages as strongly related or not to the query, allowing for easy generation of training instances. The labeling function assigns positive, negative, or neutral labels to query-passage pairs based on similarity scores. The text discusses categorizing passages for each query based on similarity scores. Four scoring functions are applied, including BM25 score and cosine similarity. Label aggregation is used to improve label quality through majority voting. After applying label aggregation strategies, query-passage pairs are associated with binary labels and confidence scores. Triplet training instances are generated by combining positive and negative pairs sharing the same query. The geometric mean of confidence scores is used for the triplet confidence score. In weak supervision settings, the geometric mean of confidence scores from original pairs is used to train a supervised model on passage-ranking datasets. Different maximum sequence lengths and batch sizes are set for BERT model on WikipassageQA, InsuranceQA v2, and MS-MARCO datasets. Learning rate is varied during training. In weak supervision settings, the geometric mean of confidence scores from original pairs is used to train a supervised model on passage-ranking datasets with varying maximum sequence lengths and batch sizes for BERT model. For training, learning rate is swept over {1e\u22125, 2e\u22125, 3e\u22125} with a maximum of 10,000 steps and a warmup ratio of 0.1. Four labeling functions are defined, utilizing BM25 and TF-IDF scoring functions. Cosine-similarity of BERT features and universal sentence embedding is calculated. Precision and recall at 1 (P@1, R@1), and AUC are used to evaluate labeling function quality. Results are compared with ground truth labels. A simple generative model (GM) and majority voting strategy are used to estimate true labels, with aggregated label quality shown in the results. After training the BERT-PR model using weak supervision on BM25 scores, it outperforms the unsupervised BM25 baseline. By utilizing a simple generative model over labeling functions, BERT-PR trained on GM labels shows improved performance compared to BM25 baselines and BERT-PR trained solely on BM25 scores. BERT-PR trained on GM labels outperforms BERT-PR trained on BM25 by around 10% on all three datasets. Weak supervision models even beat previous SOTA performances in fully supervised settings, showing great potential in real applications. Noise-aware training did not significantly improve performances, leaving room for future research. In this work, a weak supervision pipeline for neural ranking models based on data programming is proposed. A new PR model based on BERT achieves state-of-the-art results, outperforming the BM25 baseline and previous SOTA performances with full supervision. Future research can focus on improving the aggregation of pseudo ranking labels and designing generative models for ranking labels directly. The formulation in BID14 assumes conditional independence of labeling functions given the true label. The probabilistic graphic model is parameterized based on this assumption, with optimal parameters found by maximizing the marginal likelihood. The model's identifiability issue is addressed by setting \u03b1 i > 0.5. The noise-aware training objective in BID14 introduces a method to better incorporate noise in generated labels by using confidence scores for correct training instances. For example, in the case of PR, the confidence score represents the relevance of one item over another."
}