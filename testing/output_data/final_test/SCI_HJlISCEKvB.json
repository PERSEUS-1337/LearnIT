{
    "title": "HJlISCEKvB",
    "content": "Generative adversarial networks (GANs) are limited by the shape of the noise distribution they use. To address this, a novel formulation of multi-generator models called Noise Prior GAN (NPGAN) has been introduced. NPGAN learns a prior over generators conditioned on noise, allowing for optimal sampling rates and noise shaping, resulting in increased expressivity and flexibility. The Noise Prior GAN (NPGAN) surpasses single generator models and previous multi-generator models in expressivity and flexibility. Generative models like GANs are crucial for various applications such as synthesizing conversations, creating artwork, or designing biological agents. These models make a crucial modeling decision known as the manifold assumption, assuming that high-dimensional data lies on a single low-dimensional manifold. Generative Adversarial Networks (GANs) assume data lies on a single low-dimensional manifold, but recent work highlights situations where data lies on multiple disconnected manifolds. GANs must learn a continuous cover of these manifolds, generating off-manifold points in between. These points are a small fraction of the total distribution and have minimal impact on typical GAN evaluation measures. In some applications, the presence of outliers in generated distributions can be more catastrophic than slight imperfections in modeling dense regions. For instance, in the Turing Test, generating gibberish can reveal the identity of an artificial agent. Concerns also arise regarding GAN convergence proofs due to disconnected manifolds. The Noise-Prior GAN (NPGAN) architecture learns a prior over generators conditioned on noise distribution z, controlling sampling frequency and shaping input. It trains multiple generators using regularizations to learn separate manifolds by changing the initial noise distribution. Our approach in the Noise-Prior GAN (NPGAN) architecture involves using multiple generators and a neural network to divide the noise space and dispatch it to each generator. This framework is differentiable, allowing optimization of the NP network and generators during training. Our approach in the Noise-Prior GAN (NPGAN) architecture involves optimizing the NP network and generators during training to increase expressivity. By dividing the space into slices and assigning them to generators, we create four disconnected manifolds with just two generators, offering more flexibility and robustness compared to previous models. This approach is more generalized and addresses misspecification between noise distribution. Our approach introduces a multi-generator ensemble to learn a prior over the noise space, improving performance on disconnected and complex-but-connected manifolds. Previous works have used multiple generators with different approaches. Some works use multiple generators with various weight tying methods and noise sources to enhance diversity in generator outputs and improve convergence and equilibrium existence in the loss landscape. The DeLiGAN (Gurumurthy et al., 2017) utilized a single generator and a Gaussian mixture model latent space to handle diverse datasets with limited datapoints. It selected a random Gaussian from the mixture, added \u00b5 i to Normal(0, 1) noise, and multiplied it by \u03c3 i. However, unlike our approach, the selection of each Gaussian component is not differentiable, with equal probabilities for selection and training one component at a time. The MGAN (Hoang et al., 2018) addressed mode collapse by using multiple generators with a new loss term to encourage learning different parts of the data space. Unlike our approach, they use a single noise source and let the generators project it to different parts of the space. The DMWGANPL (Khayatkhoei et al., 2018) utilized multiple generators to address disconnected manifolds, each receiving a single noise sample to learn different parts of the space. Unlike previous works, they did not assume equal probabilities for selecting each generator. The DMWGANPL utilized multiple generators to address disconnected manifolds, each receiving a single noise sample to learn different parts of the space. Unlike previous works, they did not assume equal probabilities for selecting each generator. Instead, they sample each generator G i with probability r i to maximize mutual information between their distribution and Q(G i |x). This approach prevents sampling redundant generators and is separate from the minimax GAN game. Each generator gets the same noise sample from a Normal(0, 1) distribution, indirectly affecting the quality of generated images in the GAN framework. Our NPGAN framework utilizes multiple distinct generators to handle different inputs efficiently. By using a Noise Prior (NP) network to delegate inputs to the most suitable generator, we optimize the overall generator G. This approach is inspired by machine teaching and knowledge distillation, allowing for better performance during training. Our NPGAN framework utilizes multiple distinct generators optimized by a Noise Prior (NP) network to efficiently handle different inputs. The NP network determines how to divide inputs across generators to model P X, allowing for better performance during training. The NPGAN framework uses multiple generators optimized by a Noise Prior network to efficiently model P X by dividing inputs across generators. The distribution over generators is learned by another network conditioned on input noise, allowing for flexibility in shaping the input for each generator. This approach does not rely on specific properties of P X, enabling N P to adapt to the shape of P X as needed. The NPGAN framework incorporates multiple generators into a GAN by modeling a prior over the generators conditioned on noise input. It directly participates in the GAN minimax game and learns how to best utilize the generators. Experimental results show its ability to model distributions compared to other models like MGAN, DMWGANPL, and DeLiGAN. The models used in the study had 100-dimensional uniform or normal distributions for generators, with specific architecture and activations. Single generator models struggled to capture the distribution accurately, leading to off-support outliers. Multi-generator models with three generators were then evaluated to better represent the disconnected manifolds in the synthetic data. The MGAN and DeLiGAN fail to model disconnected manifolds effectively due to sampling generators with fixed probabilities. DMWGANPL learns unequal sampling of generators but struggles to accurately model data. NPGAN assigns each manifold to an individual generator, matching data distribution without generating off-manifold points. The NPGAN outperforms other models by assigning each manifold to an individual generator, avoiding off-manifold points and accurately modeling data distribution. The NPGAN effectively models disconnected manifolds with multiple generators, as shown in Table 1. By using two-dimensional uniform noise, the network can easily model non-equally sampled manifolds and match their density with the number of generators. This approach allows the NPGAN to create a third manifold without a dedicated generator for it, as seen in Figure 4a and 4c. The NPGAN effectively models disconnected manifolds with multiple generators, matching the density of Gaussians in the data with three generators (c-d) or creating a discontinuity with two generators (a-b). Table 1 shows scores for each model on artificial datasets and the number of generators used. The score represents the percentage of generated points off the manifold for Gaussian data and the percentage of real points without any generated point in its neighborhood for parabolas. Other models target distinct areas of the data space with each generator, while single generator networks struggle with disconnected parts of the data space. The NPGAN allows for modeling complex shapes by giving each generator full flexibility to shape the input. When testing with intersecting parabolas, single generators struggle to effectively model the shape. Different models like DeLiGAN, MGAN, and DMWGANPL face challenges in coordinating generators to generate intersecting shapes due to penalties that push generated points away from each other. The NPGAN effectively models complex data distributions with two or three generators, unlike other models that struggle to generate intersecting shapes. It quantitatively evaluates the dataset by calculating the percentage of real points not modeled within a certain radius, showing that other models leave significant parts of the data unmodeled. The NPGAN's ability to model complex distributions is confirmed by its low score with both two and three generators. The NPGAN can effectively model disconnected, unevenly sampled manifolds on real images by combining two datasets, CelebA and a photographs dataset. The models use a DCGAN architecture with two generators to match the data distribution, requiring one generator to cover a discontinuity or learn to sample at a differential rate. The models used different configurations for the generators, including varying stride length, kernel size, and batch normalization. Training was done on minibatches with an Adam optimizer. In some models, generators shared the same network with adjustments for noise, while in others, each generator had its own weights to maintain parameter balance. Randomly selected images from each model showed the effects of fixed-rate sampling in some generators. The consequences of different sampling generators (MGAN, DeLiGAN, DMWGANPL) were observed, with some generators producing blurry images or a mix of CelebA images and photos. FID scores in Table 2 indicate an imbalance in modeling the dataset. NPGAN learned to sample more from the generator that exclusively makes photos. The NPGAN effectively models a dataset with disconnected, unevenly sampled data, outperforming other models even without disconnected data. The effects of properties like class/mode imbalance dominate the results, as shown by modifying the dataset to create connections between Face and Photo images through linear interpolation. The ConnectedFace+Photo dataset is created by adding interpolations between Face and Photo images with a mixing coefficient \u03b1. The results of the experiment remain consistent, showing that other models struggle with density imbalances, which overshadow the impact of data disconnection. FID scores are most influenced by model performance in areas of high data density rather than outlier points. The presence of off-manifold outliers can pose significant challenges. The presence of off-manifold outliers like those in Figure 7b-d can be problematic in contexts with higher sensitivity to outliers than FID score captures. NPGAN's ability to model CIFAR10 dataset produced similar FID scores but introducing a new measure, outlier manifold distance, to assess the model's worst samples. NPGAN introduces a new measure called outlier manifold distance to evaluate a model's worst samples, which is sensitive to outliers regardless of the model's best samples. This distance is calculated by averaging the distances of the 1% furthest generated points from the real data manifold. NPGAN performs the best in outlier manifold distance compared to other models. The NPGAN introduces a novel formulation of multiple-generator models with a prior over the generators, resulting in improved expressivity and flexibility. Experimenting with two to five generators in the NPGAN architecture, the images generated become more homogeneous as the number of generators increases. When comparing two to five generators in the NPGAN architecture, the images become more specialized, with each generator focusing on specific categories. However, there is no improvement in FID score compared to the baseline one-generator model due to the metric treating all points equally and being influenced by where the most points are located. The NPGAN architecture shows improvements in FID scores on CIFAR10 dataset, achieving a score of 26.4 compared to 25.8 with one generator. It can scale to state-of-the-art architecture without compromising quality and is robust to connected datasets. A new outlier manifold distance metric shows NPGAN outperforming other models in outlier detection. NPGAN outperforms other models in outlier detection using a new metric. In machine teaching, the teacher network NP adapts to changing target functions for optimal student learning. The NP network forms individualized curriculums for students to model the target function effectively. The NP network in machine teaching is related to knowledge distillation, where a teacher network compresses knowledge into smaller models for specific problems."
}