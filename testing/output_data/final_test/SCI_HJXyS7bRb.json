{
    "title": "HJXyS7bRb",
    "content": "Building chatbots for tasks like booking flight tickets is a challenge in natural language understanding. Progress has been made using sequence2sequence modeling, but optimizing models for goals remains a challenge. New methods aim to address this by optimizing rewards, but often provide little guidance for language construction. A new approach proposed in this paper aims to bridge the gap by isolating model-level information between agents in goal-oriented dialogue systems. Language construction is emphasized as an important aspect in achieving rewards. The study focuses on optimizing reward in goal-oriented conversational models by emphasizing language construction for information exchange between agents. Results show that the method outperforms baseline models and generates human-readable conversations. This approach aims to address the limitations of end-to-end neural networks in goal-oriented dialogues. Building goal-oriented conversational models involves directing conversations towards accomplishing a specific task, unlike chitchat models. These models must guide the dialogue to progress the task, such as in flight booking where responses need to meet customer expectations. This presents a new challenge for neural network-based models. Building goal-oriented conversational models is challenging for neural network-based models due to the difficulty in transferring success from chitchat dialogues. Chitchat models tend to remember exact context settings, making it hard to generate appropriate responses in goal-oriented dialogues. Acquiring dialogue data for all conditions is difficult, and optimizing for likelihood of utterances can lead to losing track of dialogue progress. In this paper, a two-party model for goal-oriented conversation is proposed to address challenges in dialogue progress and response reliability. The model involves two agents with access to hidden information, requiring one agent to generate correct actions based on this information. The hidden information can be interpreted using natural language, facilitating exchange between the agents. The two-party model involves agents with hidden information that can be exchanged using natural language. Agents establish a protocol to talk and complete tasks correctly, enabling self-play without human supervision. Isolation of information between agents ensures coupling between task reward and language models. Agents guide conversations to acquire key information for generating correct actions, strengthened through self-play with reinforcement learning. The self-play model strengthens dialogue models by leveraging large-scale unsupervised knowledge and generating initial conditions for dialogue through user restrictions and available information. This creates a perfect reinforcement learning environment to estimate rewards and train a more reliable chatbot. The model's performance is validated by training a supervised model based on fully supervised dialogue utterance, action states, and hidden conditions. The supervised model is used to initialize a self-play training model for dialogue systems. The models are evaluated on self-play data sets, showing a 37% improvement in average rewards. The specific scenario considered is booking flight tickets, where customer and service agents interact based on hidden information. The service agent's goal is to assist the customer agent in fulfilling requests. The service agent aims to assist the customer agent in booking flights by taking appropriate \"action states\" during the conversation. The effectiveness of the dialogue is evaluated based on the correct execution of these actions. The model consists of two mostly independent components, with token embedding E being the only shared part optimized across the entire model. The model structure includes components for the service agent and customer agent, trained with user restrictions and database information. Hidden knowledge is encoded using GRU for user restrictions and database entries. The model structure involves components for the service agent and customer agent, utilizing GRU to encode hidden knowledge from user restrictions and database entries. Dialogue context is represented using two multi-layered GRU units, GRU c en and GRU s en, with an embedding matrix E applied for encoding. Special tokens like \"<1>\", \"<2>\", and \"<eod>\" mark the start and end of turns for each agent in the dialogue. The model utilizes two different encoders for the service agent and customer agent to focus on relevant context. The encoded embedding is decoded using multi-layered GRU units, with separate decoders for each agent. The output is treated as logits to predict tokens. The decoder GRU's cell state is initialized using concatenated embeddings specific to each agent. Initial start tokens are fed into the decoder based on the agent speaking, generating an utterance each time an input is received. The dialogue encoder generates utterances based on input tokens, with the conversation ending when an end of dialogue token is encountered. The service agent concludes by generating action states, which can be a single token or a sequence. The seq2seq decoder is initialized with the last hidden state of the dialogue encoder. During supervised learning, the model is optimized by considering loss from both the dialogue and action. During supervised learning, the model optimizes by considering loss from both dialogue and action states. The loss function combines the two components of the model. In the self-play setting, dialogue supervision is absent, and the model is initialized with weights from the supervised setting. Two model instances interact, one with fixed parameters and the other with trainable parameters. During self-play, two model instances interact to generate dialogue and actions. A reward terminal is generated based on the predicted actions, which are fed to the environment. The algorithm aims to maximize this reward by using a reward discount during training. Dialogue is generated by sampling from policy, and a value network is built to estimate returns for each agent. The algorithm utilizes a multi-layered GRU to encode dialogue context and hidden knowledge, with each agent having its own value network. A doc product is made between the encoded context and hidden knowledge. The value function is estimated through optimizing mean square error. The policy network structure is based on supervised learning, with weights initialized from the learned model. The REINFORCE algorithm is adopted for training. A simulator is built to generate data for reward evaluation in a dialogue simulation environment. The simulator workflow involves user requests leading to terminal status actions. The dataset consists of 500k samples divided into different sets for training, evaluation, and inference. Dialogue settings and flight database are randomly generated for each sample. Features and values are listed in TAB2. The conversation model needs to generalize between user restrictions and database information to succeed. Only 30 candidate flights are generated for each dialogue to limit database data. Priors are set to ensure a high booking rate even with a small number of flights. 55.34% of user restrictions result in a booking, while 24.68% result in a flight not found state. The dialogue data is generated based on user restrictions and database information. Conversations are initiated randomly with different language templates. Actions are generated based on dialogue setups, containing fields for dialogue state, person's name, and recommended flight number. Special token \"<flight empty>\" is outputted for flight cancellations. In the case of canceling a flight, a special token \"<flight empty>\" is outputted. Rewards are generated based on predicted actions and dialogue, following language rules and matching the ground truth state. Matching the correct action name accounts for 50% of the final reward. The name matching the correct action counts for 50% of the final reward, while the other 50% is based on database recommendation checks. Implementation details include using 8 Nvidia Tesla P100 GPUs for supervised model training and 30 machines for self-play. The model uses 4 layers of GRU in utterance decoding and encoding, with a value network of 2 layers of GRU. The supervised model, trained with 2 layer GRU and 256 units, showed low perplexity and satisfactory BLEU score on the evaluation dataset. However, its performance dropped significantly in the self-play evaluation. The model achieved a perfect score in the supervised setting but did not transfer its success to the self-play scenario. The supervised model's performance dropped significantly in the self-play evaluation, despite achieving a perfect score initially. Self-play improved the reward by over 37%, showcasing clear progress. Qualitative results demonstrate successful and unsuccessful interactions with customers. The dominant paradigm in dialogue modeling involves handcrafted rules and statistical models like MDP, POMDP, and rule-based models. The dialogue system field has evolved with the introduction of seq2seq modeling and neural-based learning techniques. Recent works utilize deep reinforcement learning for diverse conversations or goal-oriented models. Some models are based on self-play but can degenerate into classification problems. Other research combines dialogue with vision problems. In this paper, a new approach to model goal-oriented conversations was proposed based on information isolation and action states. By using supervised learning with self-plays on action states, the training coverage was expanded, and the model was exposed to unseen data. Results showed that self-play significantly improved the reward function compared to supervised learning. This approach can be easily applied in scenarios where obtaining dialogue data is challenging."
}