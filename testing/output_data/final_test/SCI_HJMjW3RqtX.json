{
    "title": "HJMjW3RqtX",
    "content": "Humans are skilled at high-fidelity imitation, quickly solving tasks and learning new skills. An off-policy RL algorithm called MetaMimic is introduced to bridge the gap in achieving these abilities in autonomous agents. MetaMimic can learn policies for one-shot imitation of diverse skills and improve task efficiency. It relies on storing experiences in memory and replaying them to train deep neural network policies. This paper presents the largest neural networks for deep RL and demonstrates the need for larger networks with normalization to achieve high-fidelity imitation on challenging tasks. The results show that AI agents can learn policies for one-shot imitation from vision without access to demonstrator actions. This AI agent can mimic demonstrations with high-fidelity or forego imitation to solve tasks more efficiently. Motor control tasks can be deceptively difficult, such as tying shoe-laces, which many struggle with even after mastering other skills like object recognition and speech. 6 year olds struggle with walking, speech, and reading comprehension. High-fidelity imitation, closely mimicking a demonstration trajectory, is challenging for AI agents but easy for humans. Many recent works focus on using expensive reinforcement learning methods for imitation tasks. In this paper, a meta-learning approach called MetaMimic is introduced to enable high-fidelity one-shot imitation policies in AI agents through off-policy RL. These policies can mimic new skills with just a single demonstration as input, allowing agents to acquire a diverse set of skills. The challenge lies in training massive deep neural networks for representing many behaviors, which has been an open question in RL methods due to inherent variance. The study demonstrates the feasibility of training massive high-fidelity imitation models. MetaMimic introduces a one-shot imitation policy using off-policy RL with a massive deep neural network. It enables a robot arm to mimic any demonstration and populates its replay memory with rich experiences. The study shows that bigger networks generalize better, proving RL as a scalable framework for AI agent design. MetaMimic algorithm enables one-shot high-fidelity imitation from video in complex manipulation tasks, outperforming D4PG RL agent. Larger networks improve generalization, and increasing demonstrations during training leads to better one-shot imitation. Instance normalization is highlighted as important. MetaMimic algorithm learns one-shot high-fidelity imitation policies and unconditional task policies that outperform demonstrators. Component 1 processes demonstrations to create rich experiences and a one-shot imitation policy, while Component 2 generates an unconditional task policy. D4PG is used as an efficient off-policy RL algorithm for training. In practice, D4PG is used for training the agent's policy from demonstration data. Neural network architectures are described for learning high-fidelity imitation policies. Training these policies results in a memory of experiences that can be replayed to learn unconditional policies. The algorithms for the imitation learner, task actor, and task learner are provided in the appendix. The text discusses the imitation of a single stochastic task using a parameterized policy \u03c0 \u03b8 that produces actions based on observations and demonstrations. The goal is to estimate the policy parameters \u03b8 that maximize the expected imitation return. The environment renderer E generates observation sequences given a policy, aiming for high-fidelity imitation. The text discusses one-shot imitation learning using a parameterized policy \u03c0 \u03b8 to maximize expected imitation return. The D4PG algorithm is used for training the imitation policy through RL with trajectory sampling. MetaMimic builds on the D4PG algorithm for one-shot imitation learning, using independent actors to insert trajectory data into a replay table. A specific choice of reward and policy is made, with a single demonstration sampled at the beginning of each episode. The actor interacts with the environment based on the demonstration, simplifying the model to consider local context. In MetaMimic, the model simplifies to only consider local context, interpreting the future state as a goal state for goal-conditional imitation. The reward is computed at each timestep, and a large convolutional neural network is used for high-fidelity imitation tasks. The network architecture includes a residual network with twenty convolutional layers, instance normalization, layer normalization, and exponential linear units. The imitation policy and task policy share a similar network architecture, but the task policy does not receive a goal. In this work, a simple reward function based on Euclidean distance over observations is experimented with. A combination of observations with and without information about objects in the environment is found to work best. The imitation policy is able to imitate diverse goals explicitly given by the demonstration, rewarding the agent based on reaching those goals. The imitation policy can imitate diverse goals explicitly given by demonstrations, requiring a high-capacity deep neural network architecture with specific features like residual connections and instance normalization for improved performance. This architecture outperformed a smaller model in experiments. The IMPALA network proposed by BID10 is large compared to previous networks used in important RL and control milestones. MetaMimic fills its replay memory with rich experiences to learn tasks quickly using RL, which requires both exploration and learning to improve action generation for generalization. Our method focuses on high-fidelity imitation by generating its own observations, actions, and rewards, without requiring access to privileged information about the demonstration. Our approach involves storing all experiences generated by the imitation policy in an experience-replay memory to aid in exploration. Additionally, an off-policy task learner is introduced to optimize an unconditional task policy based on rewards, allowing for learning from transitions asynchronously generated by the imitation actors. The task learner optimizes cumulative reward in an off-policy manner without needing privileged information about demonstrations. Imitation trajectories, likely in high reward areas, help with exploration. Trajectories are augmented with samples from additional task actors. The task learner trains its policy using both imitation and task trajectories. As the imitation policy improves, rewarding experiences are added to replay memory for the task learner to draw on. In the robotic block stacking setup, rewarding experiences are stored in replay memory for the task learner to utilize in off-policy learning. The performance of imitation and task policies is analyzed in a challenging environment, showing that more demonstrations improve generalization with the imitation policy. In a robotic block stacking task, the policy can closely mimic the training set but struggles with generalization. With 500 demonstrations, the policy has difficulty mimicking all trajectories but maintains similar imitation rewards on train and validation sets. The figure illustrates that higher imitation rewards lead to higher task rewards. The environment includes a Kinova Jaco arm with six arm joints and three actuated fingers, simulated in MuJoCo, for learning high-fidelity imitation from human demonstrations and generalizing to new initial conditions. The study collected 500 episodes of demonstrations using a SpaceNavigator 3D motion controller for a block stacking task. The agent was trained using D4PG on visual input and demonstration sequences from expert episodes. The policy struggled with generalization but showed higher task rewards with higher imitation rewards. MetaMimic uses larger neural networks and normalization to improve performance in high-fidelity imitation tasks. The ResNet model used by the IMPALA agent is compared to MetaMimic's larger networks inspired by ResNet34, showing significant improvements in task reward, imitation reward when tracking pixels, and imitation reward when tracking arm joint positions and velocities. This approach represents the largest neural network trained end-to-end using reinforcement learning for mimicking diverse test-set demonstration videos. The test demonstrations in MetaMimic show lower average cumulative reward compared to training, achieving 52% of the reward without task reward through high-fidelity imitation. This is done with fewer assumptions on the environment and demonstration than other methods, allowing imitation without actions at training time and mimicking block stacking using proprioceptive features for computing the imitation reward. MetaMimic's imitation policy can mimic block stacking demonstrations purely from video at test time without the need for a task reward. Policies trained on a small number of demonstrations achieve high imitation reward in training but low reward on validation, while policies trained on a bigger set generalize much better. Performance varies from 67% on the training set, with policies trained on 50 or more demonstrations generalizing very well. The study demonstrates that using a large ResNet34-style network outperforms smaller networks in imitation learning tasks. Instance normalization is shown to further improve performance compared to batch normalization, helping to avoid distribution drift between training and running the policy. The dense reward signal allows for effective training of even large models. The study shows that using a large ResNet34-style network is beneficial in imitation learning tasks. Instance normalization is preferred over batch normalization to prevent distribution drift. The task policy, trained concurrently with the imitation policy, outperforms demonstration videos without requiring a demonstration sequence at test time. The task policy, trained with a large ResNet34-style network and instance normalization, outperforms demonstration videos without needing a demonstration sequence at test time. MetaMimic matches the performance of methods with access to additional information, even without expert rewards or actions. The task policy, trained with a large ResNet34-style network and instance normalization, outperforms demonstration videos without needing a demonstration sequence at test time. The policy can outperform demonstrations by 50% in task reward. Using demonstrations as a curriculum in RL is a powerful technique, enabling the agent to learn from rewarding parts of demonstration episodes. This approach benefits RL and imitation learning but requires an environment that can be reset to any demonstration state, often only possible in simulation. Comparing D4PG and the method shows significant differences in performance. Our method, when trained with a demonstration curriculum, outperforms D4PG. Without the curriculum, our method performs as well as D4PG with the curriculum. Despite requiring time for the imitation policy to take off, our task policy catches up with D4PGfD quickly and reaches the same performance. This demonstrates the efficiency of our approach. The efficiency of our imitation policy is demonstrated by its performance, which matches that of the policy trained with D4PGfD. Prior works in imitation learning focus on behavior cloning (BC) and inverse RL (IRL) approaches. BC uses supervised learning to learn a task policy, while IRL learns a reward function from demonstrations and then uses RL to maximize that reward. Generative Adversarial Imitation Learning (GAIL) is an example of IRL method that constructs a reward function based on expert-generated observations. Our approach to one-shot imitation learning focuses on high-fidelity imitation through tracking rewards, which has gained popularity in games and control environments. Unlike other methods that generalize from a single demonstration, our method aims to faithfully execute the same plan as the demonstrator at test time. MetaMimic is a novel approach in imitation learning that trains a single policy to closely track hundreds of demonstration trajectories and generalize to new demonstrations. It is related to recent work on learned inverse dynamics models that do not require expert demonstrations, but rely on high-level control policies and structured exploration. The approach described in the curr_chunk utilizes supervised learning and multi-task off-policy reinforcement learning to solve sparse reward tasks faster. It eliminates the need for expert actions through high-fidelity imitation, improving exploration. MetaMimic is a method introduced in the paper to train a high-fidelity one-shot imitation policy and efficiently train a task policy using a large neural network. The one-shot imitation policy can generalize to unseen trajectories and mimic them closely, while the task policy can quickly outperform the demonstrator by bootstrapping on imitation experiences. The framework can be extended to incorporate existing methods for learning third-person imitation rewards and to further develop MetaMimic. MetaMimic is a method introduced to train a high-fidelity one-shot imitation policy and efficiently train a task policy using a large neural network. The framework can be extended to imitate demonstrations of various tasks and generalize to unseen tasks. To improve application to robotic tasks, addressing how to relax initialization constraints for high-fidelity imitation is desirable. D4PG is used as the main training algorithm, utilizing Q-learning for policy evaluation and Deterministic Policy Gradients for policy optimization. It maintains a replay memory that allows for off-policy learning. D4PG utilizes target networks for stability and distributed training, along with distributional value functions and multi-step returns for efficiency. It maintains online value and policy networks, updating target networks periodically. The distributional value function in D4PG uses a random variable Z to update the policy. D4PG uses a random variable Z to construct a bootstrap target for training the value function. It adopts a projection \u03a6 and cross entropy loss for training. The learning process in D4PG is distributed, with independent actor processes acting in parallel. D4PG uses a large number of independent actor processes to interact with the environment and write data to a central replay memory. The learners draw samples from the replay memory for learning and also serve as parameter servers to the actors. When using a demonstration curriculum, the initial state is randomly sampled from the first 300 steps of a random demonstration. In Jaco arm experiments, the L2 distance between the hand and target block vectors is computed, and the episode terminates if the distance exceeds a threshold of 0.01."
}