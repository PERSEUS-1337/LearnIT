{
    "title": "SJgfQH6PQ4",
    "content": "The transformer model uses attention to refine lexical representations with information from the context. Gated shortcut connections are introduced to alleviate the bottleneck of representing and propagating lexical features in each layer. This modification improves translation tasks and reduces the amount of lexical information passed along hidden layers. The transformer model BID28 has become popular for neural machine translation due to its fast training, ability for lexical disambiguation, and capturing long-distance dependencies. Recent studies have explored features encoded in neural translation models, highlighting the importance of lexical connections in the hidden layers. Within transformer models for neural machine translation, different layers prioritize various information types, with lower layers focusing on morphological and syntactic processing, while semantic features are concentrated towards the top layers. Information exchange in the transformer encoder and decoder occurs sequentially, with each layer attending to the output of the previous layer. To propagate input features to upper layers successfully, the translation model must store them in its intermediate representations. To address the representation bottleneck in transformer models for neural machine translation, we propose adding lexical shortcuts as gated skip connections between the embedding layer and subsequent self-attention sub-layers in both encoder and decoder. This allows the model to access relevant lexical information directly, leading to improved translation quality across multiple language pairs and corpus sizes. The study demonstrates improved translation quality by adding lexical shortcuts to the transformer model, reducing a representation bottleneck. These shortcuts show significant enhancements in translation quality across various language pairs, making them compatible with other NMT architectures. The study shows that adding lexical shortcuts to the transformer model improves translation quality across different language pairs. Shortcuts are found to enhance word sense disambiguation and are best applied to the self-attention mechanism in both encoder and decoder. The transformer consists of an encoder and decoder, with each composed of identical layers containing self-attention mechanisms. The transformer model consists of encoder and decoder layers with self-attention mechanisms. The attention mechanism is implemented as multihead, scaled dot-product attention for considering different context sub-spaces. Residual connections aid with signal propagation, and inputs are projected into a common representation sub-space using weight matrices. Each head in multi-head attention has its own set of keys, values, and queries. The transformer model utilizes multi-head attention with separate sets of keys, values, and queries for each head. Attention is defined as a function over projected representations, with the pre-softmax dot-product divided by the square root of the key dimensionality to prevent large magnitudes. The translated sequence is generated by passing the decoder output through a softmax layer. The attention mechanism establishes parameterized connections between layers. The transformer model utilizes multi-head attention with separate sets of keys, values, and queries for each head to establish parameterized connections between layers. Gated connections between the embedding layer and self-attention sub-layers in the encoder and decoder are added to reintroduce lexical content. Lexical features are projected into the appropriate latent space using layer-specific weight matrices, and binary gates inspired by the Gated Recurrent Unit are used to account for variable importance. These lexical shortcuts are reminiscent of highway connections proposed in previous work. The transformer model incorporates multi-head attention to establish connections between layers. It estimates lexical relevance for attention steps by comparing lexical and latent features, followed by combining them using a weighted sum. The key and value arrays are then passed to the multi-head attention function. In an alternative formulation, embeddings and previous layer outputs are concatenated before projection. The transformer model incorporates multi-head attention to establish connections between layers, estimating lexical relevance by comparing lexical and latent features. The model then combines them using a weighted sum, passing key and value arrays to the multi-head attention function. A feature-fusion step integrates lexical information into the attention inputs, improving gradient flow during back-propagation with the introduction of shortcuts. This aids in the training process by providing an implicit 'deep supervision' effect. The proposed approach involves extending the transformer model with lexical shortcuts in each self-attention layer to improve training efficiency. The model's learning signal is derived from the overall optimization objective and connected layers. The study evaluates the impact of lexical shortcuts on 5 WMT translation tasks, with details on model configurations, data pre-processing, and training setup provided in the appendix. The code for the modified transformer model is publicly available for result reproduction. The study evaluates the impact of lexical shortcuts on 5 WMT translation tasks, with model performance validated on various test sets. Final results are reported on multiple tests sets from the news domain for each direction, with translation quality evaluated using sacre-BLEU. The evaluation of translation quality using sacre-BLEU BID16 shows improvements in transformer model performance with lexical shortcuts, outperforming transformer-BASE by 0.5 BLEU on average. Feature-fusion further enhances performance, with gains of up to 1.4 BLEU for EN\u2192DE and 0.8 BLEU on average for other translation directions. The addition of lexical shortcuts has a smaller impact on transformer-BIG compared to transformer-BASE. The larger hidden state size of transformer-BIG may reduce the benefits of dynamic lexical access. Transformer-BASE, with lexical connections, performs comparably to transformer-BIG despite having fewer parameters. The average increase in BLEU is lowest for DE\u2192EN and highest for EN\u2192RU, possibly due to language topology differences. One possible explanation for the difference in translation performance could be the language topology. English, being morphologically weak, relies more on sentence context than individual words. This results in fewer lexical features propagated across the network, reducing the need for added shortcuts. A thorough analysis of the transformer models' internal representations and learning behavior supports this hypothesis. The proposed approach aims to address the limitation of transformers in learning and representing various types of information by providing direct connections to the embedding layer. A probing study is conducted to estimate the amount of lexical content in each hidden state of the encoder and decoder. This involves training separate lexical classifiers for each layer of a frozen translation model to analyze internal representations. The study involves training lexical classifiers for each layer of a translation model to analyze hidden state representations. The classifiers reconstruct sub-words in the source and target sentences, showing that immediate access to lexical information reduces the representation bottleneck across encoder and decoder layers, with consistent results across different language pairs. The study analyzes hidden state representations in translation models by training lexical classifiers for each layer. The decoder retains fewer lexical features compared to the encoder, possibly due to the additional information it needs to represent. Adding shortcut connections can increase the dissimilarity between layers in the decoder. The study examines the impact of lexical shortcuts on the retention and propagation of lexical features in the decoder layers of translation models. Shortcut connections reduce layer similarity in both encoder and decoder, as shown through cosine similarity calculations. Scaling down the transformer model suggests that shortcuts may benefit smaller models more in terms of translation quality improvements. Visualizations of the findings are available in the appendix. The study explores the impact of lexical shortcuts on translation quality in transformer models. Scaling down the model suggests that shortcuts may benefit smaller models more. The experiments support the existence of a representation bottleneck in NMT models. The study investigates the impact of shortcut connections on translation quality in transformer models, focusing on decoder-to-encoder attention. Integrating shortcuts in this area shows improvement, but not as significant as with self-attention. Combining both methods leads to decreased translation quality, suggesting that improvements are not cumulative. The study explores the impact of shortcut connections on translation quality in transformer models, specifically focusing on decoder-to-encoder attention. Integrating shortcuts in this area shows improvement, but not as significant as with self-attention. Enabling shortcuts in both encoder and decoder leads to the best translation performance and training stability. The study investigates the impact of non-lexical shortcuts on translation quality in transformer models, specifically focusing on decoder-to-encoder attention. Results show that while non-lexical shortcuts improve over the baseline model, they perform worse than lexical connections, indicating that lexical shortcuts contribute significantly to translation quality beyond just signal flow or trainable parameters. The study explores the effects of lexical shortcuts on the transformer's learning dynamics, particularly in relation to word-sense disambiguation in translation. By widening the representation bottleneck, the model's capacity for learning information from sentence context is increased. The addition of lexical shortcuts is evaluated for aiding disambiguation in trained DE\u2192EN models on the ContraWSD corpus BID19. The dataset pairs source sentences with multiple translations, varying the sense of selected nouns. The standard transformer provides a strong baseline, but improvements are seen with the addition of lexical shortcuts. Adding direct connections to the embedding layer improves the accuracy of the baseline model for word-sense disambiguation. Recent literature suggests extending the standard transformer architecture with adaptive model depth and layer-wise transparent attention. This study introduces additional connectivity to facilitate the accessibility of lexical information throughout the model, reducing the need to represent and propagate lexical features along hidden states. Our proposed shortcut connections enhance the model's capacity for learning novel information by representing and propagating lexical features along hidden states. Drawing inspiration from research on internal dynamics and learned representations in deep neural networks, we focus on the transformer model's ability to refine input representations through attention. The role of lexical features in neural machine translation (NMT) is crucial for improving translation quality, especially in low-resource settings. The proposed method widens the representation bottleneck in the transformer by refining input representations through attention, iteratively refining features through successive layers. The transformer not only refines input features but also learns new information with increasing model depth. Our modified models introduce lexical shortcuts to widen the representation bottleneck in the transformer, resulting in improved BLEU scores on WMT datasets. The shortcuts enhance the network's ability to learn diverse information and improve WSD capability. This study provides insights into the nature of information encoded by transformer layers and supports the iterative refinement view of feature learning. Future work will explore additional methods to enhance model performance. In future work, the study aims to explore ways to enhance translation models' internal representations. Experiments were conducted using the transformer-BASE configuration with specific settings for encoder and decoder layers, embedding dimensions, attention heads, and feedforward sub-layer dimensions. Trained models were optimized using Adam and a learning rate schedule. The number of warm-up steps was adjusted based on model modifications to accommodate parameter size increase. The study explores enhancing translation models' internal representations by using lexical shortcuts and feature-fusion to accommodate parameter size increase in the transformer-BIG model. Experiments are limited to EN\u2192DE due to computational constraints. Models are trained on four Nvidia P100 Tesla GPUs with synchronous data parallelization. Probing experiments use classifiers with a single hidden layer of 512 units. Transformer-BASE models are trained for 150,000 updates, while transformer-BIG models have 16 attention heads and doubled dimensions. The study compares training times and parameter sizes of transformer-BASE and transformer-BIG models. Transformer-BASE is trained for 150,000 updates, while transformer-BIG stops at 300,000 updates. Training base models takes around 43 hours, increasing to around 46 hours with shortcut connections (50 hours with feature-fusion). Validation is done every 4000 steps. Test-BLEU scores are averaged over the final 5 checkpoints for transformer-BASE and final 16 for transformer-BIG. Tokenization, cleaning, and truecasing are done using scripts from the Moses toolkit. The training corpus is truecased using Moses toolkit scripts and applies byte-pair encoding BID22 to address the open vocabulary issue. Cleaning is not done for validation and test sets. Different BPE merge operations and vocabulary thresholds are set for different language pairs. BPE vocabulary is learned jointly for the source and target languages, requiring a transliteration step for Russian data preprocessing. Cosine similarity scores between layers in transformer-BASE and its variant with lexical shortcuts are shown in FIG6. Fine-grained probing studies evaluate classification accuracy based on part-of-speech tags and sub-word frequencies, with test sets parsed using TreeTagger BID21. The evaluation of lexical features in transformer models shows no significant effects of part-of-speech tags or sub-word frequencies. Classification accuracy is lower for infrequent sub-words due to limited training data. Evaluation is done on different test sets for different language pairs. Activation patterns of lexical shortcut gates are essential for training transformer variants but remain challenging to analyze. Despite successful training of transformer variants with lexical connections, no distinct patterns were found in the activations of individual gates. They prioritize lexical and hidden features equally regardless of training progress or word characteristics."
}