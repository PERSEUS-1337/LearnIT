{
    "title": "HJxYZ-5paX",
    "content": "Open-domain question answering (QA) is a key challenge in AI and NLP, showcasing progress in AI generalizability. The ARC Challenge dataset, with 2,590 challenging science questions, highlights the limitations of current QA systems. A system is presented that reformulates questions to retrieve relevant text from a science corpus, utilizing ConceptNet and a textual entailment system trained on SciTail for support identification. The system utilizes a textual entailment system trained on SciTail to identify support in retrieved results and outperforms strong baselines on the end-to-end QA task. It combines query reformulation, background knowledge, and textual entailment to excel on the ARC dataset, which consists of challenging science questions. The ARC dataset poses a challenge for current systems, with only 31.70% accuracy achieved by existing models. New techniques have recently improved accuracy to 42.32% on the Challenge set. Older models like DecompAttn and BiDAF perform only slightly better than random chance on this dataset. Boratko et al. identified a key approach to tackling the ARC Challenge dataset. In Boratko et al. [2018a,b], a new avenue of attack on ARC was identified, emphasizing the importance of query reformulations to improve sentence retrieval quality. They demonstrated a 42% score increase on ARC-Easy using a pre-trained DrQA model. Many top-performing systems for ARC now utilize natural language inference (NLI) models to answer questions, which classify relationships between hypotheses and premises as entailment, contradiction, or neutral. NLI models have improved performance on NLP tasks and gained popularity due to large datasets. Techniques applied to ARC include using pre-trained graph embeddings and a reading comprehension approach. ARC Challenge poses a unique obstacle in open domain QA. Our approach combines salient evidence retrieval and NLI model judgment. Our approach for the ARC dataset involves using the original question to generate reformulations for retrieving supporting text, coupled with a textual entailment model and decision rules for accurate answers. Lessons learned and key issues are discussed for advancing in natural language question answering. Improving open-domain question answering has been a long-standing area of research in NLP and AI. The Watson project BID16, also known as DeepQA, is a famous example of a question answering system. Prior work includes datasets like BID18 for reading comprehension questions and MCTest for elementary-level children's questions. Various techniques such as pattern matching, rules, and logistic regression have been explored for these datasets. The MCTest dataset consists of 660 children's stories used for creating questions with restricted vocabulary. Some questions require multi-hop reasoning. Techniques like those from BID39 and BID48 have performed well on this dataset. SQuAD dataset uses Wikipedia passages for question answering, requiring logical reasoning but less complex than AI2 standardized tests dataset. The SQuAD dataset has an extended version with over 50,000 additional unanswerable questions. NewsQA dataset uses passages from news articles to create questions. Most datasets are closed world/domain, where the answer is in a provided text snippet. In contrast, open domain datasets cover the entire question answering pipeline. SearchQA BID14 and TriviaQA BID19 are datasets containing question-answer pairs with evidence, but they fall short of being complete open domain QA datasets. Datasets from standardized science tests are crucial for complex reasoning techniques. BID11's survey highlighted the need for advanced inference methods to answer early science questions. The AI2 Science Questions dataset and the SciQ dataset contain multiple choice science questions for elementary and middle school students. The SciQ dataset was created through crowdsourcing, where workers constructed questions based on given passages. Query expansion and reformulation in information retrieval is important for improving search accuracy. Query expansion and reformulation in information retrieval aim to improve search accuracy by generating new queries with additional terms and weights to retrieve relevant results. This technique has been applied in various applications such as Web Personalization, Information Filtering, and Multimedia IR. In the context of question answering systems, paraphrase-based approaches and machine translation techniques have shown promising results for structured query generation and answer selection. Open-vocabulary reformulation using reinforcement learning improves performance on factoid-based datasets like SearchQA. Retrieving relevant documents/passages is crucial for open domain question answering systems, as errors in this module significantly impact system accuracy. Even state-of-the-art systems struggle with passage retrieval errors, leading to subpar performance on challenging datasets like the ARC corpus. Recent studies have highlighted the importance of addressing these early errors in passage retrieval. Recent studies have emphasized the significance of improving passage retrieval modules for enhancing QA systems. The overall pipeline includes a Rewriter module for question reformulation, a Retriever module for obtaining relevant passages, and a Resolver module for selecting final answers based on retrieved evidence. The pipeline for enhancing QA systems includes a Rewriter module for question reformulation, a Retriever module for obtaining relevant passages, and a Resolver module for selecting final answers based on retrieved evidence. The Rewriter module generates reformulated queries using a term selector and background knowledge, which are then passed to the Retriever to retrieve relevant passages. The Resolver contains an entailment model and decision function to select final answers. The pipeline for enhancing QA systems includes a Rewriter module for question reformulation and a Retriever module for obtaining relevant passages. The entailment model and decision function are used to select final answers. The Rewriter module investigates different approaches to reformulate queries by retaining salient terms. The query reformulation model translates input terms into a sequence of 0s and 1s to select salient terms. It utilizes an encoder layer with pre-trained embeddings and a decoder layer with an attention mechanism. Another approach involves four models using the NCRF++ toolkit with bi-directional LSTM. The hidden layer incorporates pre-trained GloVe embeddings and ConceptNet knowledge base graph embeddings. Entities are linked to the text using surface form matching. A CRF layer is used for final prediction, trained for 50 iterations. The rewriter models (seq2seq and NCRF++) are trained and tested on the Essential Terms dataset. Results are compared with state-of-the-art systems like ET Classifier BID22 and ET Net BID26, which use SVM and various features for classification. The experimental evaluation compared NCRF++ with ET Classifier and ET Net on the Essential Terms dataset. NCRF++ outperformed the seq2seq model in all metrics and was competitive with ET Net and ET Classifier. It showed better accuracy and recall than ET Classifier, although its F1-score was slightly lower. Incorporating outside knowledge significantly improves the quality of passages for downstream reasoning. Queries are sent to the Retriever, which passes them along with passages to the Resolver module. Elasticsearch is used for text indexing on the ARC Corpus. The 14M-sentence corpus covers 95% of questions in the ARC Challenge. The ARC Dataset covers 95% of questions in the ARC Challenge and contains relevant facts useful for solving annotated questions. Future work includes augmenting the corpus with additional search indices and knowledge sources. The system divides the process of selecting answers into an entailment module and decision rule, allowing for more informed design choices. Previous systems combined these components, but separating them enables individual study. Reading comprehension models like BiDAF have been adapted for multiple-choice QA tasks by selecting spans in concatenated passages from IR results. Recent high-scoring systems on the ARC Leaderboard rely on textual entailment models to convert multiple choice questions into entailment problems. The approach involves using handcrafted heuristics to create fill-in-the-blank statements and generating hypotheses for candidate answers. Match-LSTM, trained on SciTail, is chosen as the textual entailment model due to its accuracy in reading comprehension tasks. Match-LSTM models trained on SciTail achieve 84% accuracy on test (88% on dev), outperforming other recent entailment models. The model consists of an attention layer and a matching layer, using bi-directional LSTMs to generate contextual representations of premise and hypothesis. An attention mechanism is then used to determine the weighted representation of words in the hypothesis. The Match-LSTM model achieves high accuracy on the SciTail dataset by using bi-directional LSTMs for contextual representations of premise and hypothesis, along with an attention mechanism for weighted word representation. The ARC Dataset study involved converting question answering systems to work with its format, including aggregating entailment system outputs using the AI2 Rule based on Elasticsearch scores. The entailment system measures entailment for answer options from retrieved passages and passes information to a separate decision rule module. The MaxEntail Top-k rule selects answers with the highest entailment probability from top-k passages per query. Our goal is to evaluate the impact of query reformulation and result filtering techniques on overcoming the IR bottleneck in open-domain question answering. To avoid overfitting, the Resolver module uses the same match-LSTM model trained on SciTail, while query reformulation models in the Rewriter module are trained on Essential Terms data with variations in architecture and embedding technique. Hyperparameters are tuned on the ARC Challenge dev set for optimal performance. The results of 12 models and two decision rules on the ARC Challenge dev set are summarized in FIG1 and FIG2, with final results for the test set in Table 2. Comparing FIG1 (AI2 Rule) and FIG2 (Max Entailment of top-K per answer) shows that max entailment of top-k is a better rule overall. The importance of investigating the number of passages per query is highlighted, with the recommendation not to combine Elasticsearch scores across passages. Comparing sub-figures in FIG1, there is empirical support for investigating splitting the hypothesis due to the difference in question and answer lengths between Challenge and Easy sets. Splitting multi-sentence questions by forming the hypothesis from the final sentence and pre-pending earlier sentences to each premise can address poor performance on ARC Challenge. In general, modified hypothesis splitting shows a small improvement in scores. Including background knowledge via ConceptNet embeddings enhances downstream reasoning tasks, especially seen in FIG2. Rewritten queries outperform original questions, with CompleX and PPMI embeddings performing better than the base rewriter. Results on the dev set are used to evaluate decision rules on the test set: AI2 Rule, Top-2 per query, and Top-30 per query. Top-2 is chosen for its similarity to the AI2 Rule, while Top-30 shows a clear and long peak in performance. The results of the test set show that models using the Top-2 rule outperform the AI2 rule with high confidence. Split treatments perform better on the dev set, while non-split treatments perform better on the test set. ConceptNet embeddings perform well on the dev set but worse on the test set. The ARC leaderboard standings are also provided in Table 3 for context. Our system's performance on the ARC dataset is compared with state-of-the-art systems in Table 3. Despite a slight drop in accuracy from 36.37% to 33.20%, we demonstrate the risk of overfitting to a small development set. BID26 also follows a similar approach as suggested by Boratko et al. in transforming natural language questions into queries. The system presented in the current text chunk answers science exam questions by retrieving evidence from a large corpus based on keywords from the original query. The approach integrates entailment models and co-attention to improve result quality. Our system combines query rewriting, background knowledge, and textual entailment to outperform baselines on the ARC dataset. It incorporates ConceptNet and a generic entailment model trained on SciTail to achieve near state-of-the-art performance on QA tasks. Researchers should consider the impact of tools like Elasticsearch on model performance and avoid discarding answer candidates based solely on relevance scores. The original AI2 Rule is too aggressive in pruning candidates. Utilizing an entailment model that leverages background knowledge in a more principled way could help filter unproductive search results. Results show that tuning to the dev partition of the Challenge set is extremely sensitive, which is crucial for generating significant improvements on the ARC dataset."
}