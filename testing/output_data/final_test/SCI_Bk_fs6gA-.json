{
    "title": "Bk_fs6gA-",
    "content": "The paper introduces a framework for solving combinatorial optimization problems using a memory-augmented neural model trained with deep reinforcement learning. The model outperformed hand-crafted solvers on Binary Linear Programming instances with up to 1000 variables and 700 constraints. An intelligent agent with long-term memory processes raw data and transfers it into knowledge for later use in inference. Memory can be used in inference by recalling segments, matching stored concepts with new data, or solving complex problems. Neural models encode long-term memories in weights between layers and short-term memories in hidden states. Memory augmented neural networks involve a controller writing and reading from a memory bank. In Memory Augmented Neural Networks (MANNs), the controller reads from memory using addressing mechanisms and generates a read vector for the next time step. However, the memory in MANNs is not long-term and is resettable, limiting its ability to capture general knowledge. This is crucial for tasks like natural language processing where general knowledge is needed to answer open-ended questions. Storing intermediate multiplication steps can make tasks like long-digits multiplication more efficient. Neural networks with long-term memory can store features of inputs, creating shortcut paths for learning correct targets. Unlike regular MANNs and RNNs, long-term memory provides shortcut connections to input features and previous time steps. This allows the network to cheat from memory for answers and focus on two stages during training. The No Free Lunch Theorem of optimization states that algorithms are equivalent on average across all problems, implying specialization is key. Learning optimization algorithms from scratch using input-output examples can outperform other algorithms in certain classes. The focus of this paper is on designing neural models to solve Binary Linear Programming, a special case of Integer Linear Programming. Binary LP aims to optimize a linear function under certain constraints and is proven to express the complexity class NP. The proposed framework utilizes reinforcement learning for long-term memory neural models. The framework utilizes reinforcement learning for long-term memory neural models to store useful information for solving similar instances. The model's generalization capability to complex instances beyond the training data set is analyzed. Previous work on learning to learn and meta-learning is discussed, highlighting the importance in artificial intelligence. Neural networks have been applied to combinatorial optimization, with the common approach being the use of Hopfield networks. The common approach in solving the Travelling Salesman Problem (TSP) is using neural networks like Hopfield networks. Other neural approaches such as Elastic nets and Pointer networks have also been applied to TSP. Pointer networks, similar to sequence-to-sequence neural networks, are used to solve combinatorial problems like TSP. A recent approach introduced a pointer network optimized with policy gradient methods for TSP and Knapsack problems. In contrast, this work focuses on a general mathematical formulation of combinatorial optimization problems using a memory neural network. The memory neural network introduced in this work can learn and store input features as neural memories to solve Binary LP instances efficiently. The model utilizes a persistent coupled memory that is not resettable, allowing it to generate better solutions than handcrafted solvers with minimal data. The operation of the Long-Term Memory Network involves an encoder producing a memory vector, a memory controller deciding whether to store or delete it, and a decoder producing un-normalized weights over memory locations to generate the output. The memory neural network introduced in this work utilizes a sequence-to-sequence approach to solve Binary LP efficiently. It encodes the entire problem inputs into a fixed vector and uses a decoder to find the optimal solution by iterating over feasible solutions. The model operates similarly to a naive linear programming solver but with improved efficiency. The Long-Term Memory Network (LTMN) consists of a core network, a memory bank in the form of a matrix, and a memory controller. The memory bank is a matrix where each row is addressed by a pointer. The memory controller uses reinforcement learning for hard attention and does not allow clearing of memory contents. The memory controller in the Long-Term Memory Network (LTMN) uses reinforcement learning as a hard attention mechanism to store memory fragments from the core network. It decides whether to store or discard these fragments, which are essential for backpropagation to work. The final output depends on the memory contents representation and input embedding, similar to end-to-end memory networks used in question answering tasks. The Long-Term Memory Network (LTMN) uses reinforcement learning as a hard attention mechanism to store memory fragments from the core network. The memories are hidden vectors from each layer, and for sequential data, a recurrent neural network is used. The attention mechanism BID1 is used to attend to memory locations based on a control signal or previous output. The sequence A describes constraint sets, with coefficients and bounds for each constraint. The encoder module encodes the sequence A into a fixed vector and produces a linear projection stored in memory. Equations (1) and (2) describe the constraints encoder operation. The decoder in the Long-Term Memory Network uses a fixed vector as initial hidden state and reads costs from a sequence to produce a hidden state vector. This hidden state is used to address memory and generate a final output using a soft-max layer for probability distribution estimation. The Long-Term Memory Network decoder estimates the probability distribution P(o t | h a , w t , r t) by using a recurrent layer to produce un-normalized weights over memory slots. The weights determine how much each memory slot contributes to the read vector, with a weight of -1 transforming the slot contents. The reader weights are learned through backpropagation. The memory controller in a Long-Term Memory Network is provided with a memory fragment at time t and decides whether to store or discard it using reinforcement learning. The controller interacts with the environment, which includes the memory bank, current pointer, and previous memories. The controller's window acts as short-term memory with limited size, and it chooses to store useful memories. The memory controller in a Long-Term Memory Network uses a window as short-term memory with limited size. The controller chooses to store the last memory vector in the window, providing insights into the memories produced by the core network. The controller can perform actions such as storing the current memory, deleting the current slot, incrementing or decrementing the pointer. Actions are evaluated through a reward function based on the model's performance in solving examples correctly. The memory controller in a Long-Term Memory Network uses a window as short-term memory with limited size. The controller chooses to store the last memory vector in the window, providing insights into the memories produced by the core network. The reward function evaluates if the current memory leads to a better response than empty memory. The memory controller receives a reward when non-empty memory results in solving an example correctly, and empty memory results in an incorrect response. The Long-Term Memory Network's memory controller uses a window as short-term memory. An RL agent interacts with the environment, receiving memories per example solved. The agent selects actions based on a policy and receives rewards. Deep Q-learning is used to train the memory controller, with a neural network estimating action values. The network is implemented as a stacked LSTM receiving memory contents, window contents, and a pointer. The memory agent in the Long-Term Memory Network uses deep Q-learning algorithm to construct useful memory entries during each episode. The agent learns to construct memory like a puzzle, with no final state to reach. In contrast to Atari games, where the agent quickly reaches a final state, the memory agent's episode length T needs to be determined to simulate the learning process effectively. The memory agent in the Long-Term Memory Network uses deep Q-learning algorithm to construct useful memory entries during each episode. The final state of the agent is reached when the LTMN processes the last example in an episode, leading to an infinite number of final states. The episode length T is crucial to simulate the learning process effectively, allowing the agent a limited time to accumulate rewards. Training the reader and memory controller agent jointly is critical as they depend on each other for proper functioning. Training the core network for each action the memory agent performs on the memory is essential. The memory agent in the Long-Term Memory Network uses deep Q-learning algorithm to construct useful memory entries during each episode. To effectively train the LTMN core network, the episode length should be as the same as the number of iterations, which is equal to N/m where N is the size of dataset and m is the batch size. The episode length T is chosen randomly between [k, N/m] to simulate various solving sessions. The episode length T is randomly chosen between [k, N/m] to simulate solving sessions. Two separate data sets are generated for training the core network and memory controller. The training dataset consists of 14k Binary LP instances for the core network, 3k instances for the memory controller, and 3k for validation/test. The coefficients for the objective function and constraints matrix are generated between [-99,99] to ensure sparsity. To ensure sparsity in the constraints matrix, a random sparsity level SL is generated where at most 1/3 of coefficients are zeros. Supervised targets are generated using PuLP BID14 solver. Problems with feasible solutions are created and compared with LTMN results using COIN-OR solver as baseline. Randomly select an episode length T and solve examples using Encoder to get memory vectors. Store memory vectors and select actions based on Q-values. Sample random minibatches of transitions for training. The encoder and decoder in the model are implemented as recurrent neural networks with GRU layers. By connecting multiple layers in the encoder, the model can learn more features from input-output examples of optimization problems. The decoder consists of two stacked GRU layers and a linear layer for embedding costs. All weights are initialized for training. The decoder in the model has a linear layer of 64 units and the rest with 256 units. Weights are initialized uniformly. Dropout ratio is set to 0.2 in all GRU layers. Gradient clipping is applied. Learning rate starts at 0.001 and decreases by a factor of 0.5 every 10 epochs. Memory controller agent is a stacked LSTM with 2 layers of 200 units. Batch normalization is used. Adam BID10 optimizer is employed. Window size is 5. Generalization loss is used for early stopping. LTMN performance is compared to baseline solver and sequence-to-sequence approach. Training continues until a reasonable average total reward per episode is achieved. The LTMN model is compared to the baseline solver and sequence-to-sequence approach using GRU units with similar parameters. The models are trained using the RMSprop algorithm with cross entropy loss. A metric is defined to evaluate the quality of solutions, with higher average costs indicating better performance. Feasible solutions are sampled using different temperatures. The LTMN model is evaluated against the baseline solver on a test set of 1000 instances with 10 variables and 1-5 constraints. The LTMN model outperformed the baseline solver by a large margin, with average costs nearly 51.7% higher. Random sampling was shown to be effective in generating solutions, as the untrained model performed poorly. The LTMN model was also compared to the seq-to-seq GRU model, which failed to generate feasible solutions. Despite a smaller number of instances, the LTMN model did not require a large training dataset like other models for the Travelling Salesman Problem. The LTMN model demonstrated superior performance over the baseline solver, even with larger instances. It outperformed the baseline solver on test sets with increasing variables and constraints, showing its generalization ability. Additionally, LTMN excelled in solving very large instances with 1000 variables and 700 constraints, proving the effectiveness of using long-term memory networks for Binary LP instances. The study compares average costs with memory reset between examples and evaluates memory vectors' similarity for generating solutions. Memory vectors are compared using cosine similarity to ensure they represent the same feasible solutions. The controller is enforced to store a memory vector and generate a solution based on it. Three metrics are defined for memory evaluation. The study evaluates memory usage in generating solutions by comparing memory vectors' similarity using cosine similarity. 10K instances were generated with 10 variables and constraints. 181 instances had the same feasible solutions, with high memory trues indicating non-empty memory helped generate better solutions. The study evaluates memory usage in generating solutions by comparing memory vectors' similarity using cosine similarity. It shows that a long-term memory is effective in generating better solutions by memorizing useful input features. A Long Term Memory Network (LTMN) coupled with a neural network was able to learn from supervised targets and outperform a handcrafted solver in solving Binary LP instances. The LTMN model, coupled with a neural network, outperformed a handcrafted solver by generating better solutions and generalizing to more complex instances beyond the training set."
}