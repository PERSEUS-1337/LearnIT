{
    "title": "r1z1UjA5FX",
    "content": "The study enhances the robustness of deep neural networks against adversarial attacks by using an interpolating function as the output activation. This approach significantly improves classification accuracy and stability to perturbations. The defense strategy shows substantial accuracy improvements compared to existing methods, and an intuitive explanation is provided through analyzing the feature space geometry. The code for reproducibility will be available on GitHub. Adversarial attacks on deep neural networks, such as autonomous cars, robotics, and malware detection systems, have evolved with advanced schemes to deceive the networks. These attacks are successful in both white-box and black-box scenarios due to the transferability of adversarial examples. Universal perturbations can easily manipulate images to misclassify on multiple neural networks. In response to the evolving adversarial attacks on deep neural networks, a proposal is made to defend against such attacks by changing the DNNs' output activation function to a manifold-interpolating function. This approach aims to utilize training data information seamlessly during inference. By incorporating total variation minimization (TVM) and augmented training, state-of-the-art defense results on the CIFAR-10 benchmark are demonstrated. Additionally, it is shown that adversarial images generated from attacking DNNs with an interpolating function are more transferable to other DNNs compared to attacking standard DNNs. Defensive distillation is also mentioned as a recent method to increase DNN stability and reduce the success rate of attacks. Recently proposed methods aim to increase the stability of DNNs against adversarial attacks by modifying training data and using image transformations like bit-depth reduction and JPEG compression. These non-differentiable transformations make it harder for gradient-based attacks to succeed. Additionally, purifying adversarial images with PixelDefend and adversarial training are suggested as effective defense strategies. In response to adversarial attacks, various defense methods have been proposed, including PixelDefend for purifying adversarial images and adversarial training to enhance DNN stability. Additionally, GANs are utilized for defense purposes. Advanced attack methods have also been developed, with some focusing on attacking DNNs using obfuscated gradients. Utilizing non-parametric interpolating functions instead of softmax functions as output activation has shown improved generalization accuracy and stability, especially with limited training data. The authors present a combination of deep and manifold learning to enhance geometric information utilization in DNNs. The DNNs are trained using data-dependent activation BID28 and softmax output activation. The training procedure involves forward propagation, loss computation, and backpropagation to update weights. Testing data predictions are made using the optimized model. BID28 proposes replacing softmax activation with a data-dependent interpolating function for improved accuracy and stability. The harmonic extension is a method to interpolate a function u on the entire dataset X by minimizing the Dirichlet energy functional with a weight function w(x, y) typically chosen as Gaussian. Solving the linear system provides labels for unlabeled data x \u2208 X/X te, but this interpolation is invalid when labeled data is scarce. When labeled data is scarce, the weights of the labeled data are increased in the Euler-Lagrange equation to address the issue. The solution, named weighted nonlocal Laplacian (WNLL), requires reserving a small portion of data/label pairs as a template for interpolation. Directly replacing softmax with WNLL poses difficulties in backpropagation, so an auxiliary neural net is used to train WNLL-activated DNNs. The text discusses the training of DNNs with WNLL activation using auxiliary neural nets. Three benchmark attack methods are considered: FGSM, IFGSM, and CW-L2 attacks. The DNN classifier with softmax activation is denoted as \u1ef9 = f (\u03b8, x) for a given instance. The text discusses the FGSM, IFGSM, and CW-L2 attacks for training DNNs with WNLL activation using auxiliary neural nets. The optimal perturbation for generating adversarial images is iteratively calculated, and the CW-L2 attack aims to bypass defensive distillation by searching for adversarial images classified to a specific class. Carlini et al. propose a surrogate optimization problem to address the difficulty in satisfying the equality constraint. Carlini et al. propose a surrogate optimization problem for generating adversarial images, using WNLL activation in DNNs. The attacks clip image values between 0 and 1, focusing on untargeted attacks. The DNNs are modified with WNLL as output activation, defending against attacks efficiently. The loss function for DNNs with WNLL activation is formulated for attacks on a batch of images. The batch size has minimal influence on the attack and defense outcomes. The attack methods are applied to ResNet-56 with softmax or WNLL activation, running 10 iterations for IFGSM attacks and using CW-L2 attacks in both scenarios. The CW-L2 attacks with parameters c = 10 and \u03ba = 0 successfully fool DNN classifiers on ResNet-56. Adversarial images are imperceptible but strong in fooling DNNs, with a stronger attack perturbation of = 0.08 resulting in noisier images. TV minimized images remove detailed information. The TVM removes detailed information from original and adversarial images, making it harder for humans to classify them. Adversarial images resulting from attacking DNNs are visually hard to discern. Two visualization strategies are considered for ResNet-56 with softmax activation. The study adds a 2 by 2 fully connected layer before the softmax in ResNet-56, showing that it does not affect performance. Adversarial images generated using FGSM cause features to overlap, making classification challenging. Principal components analysis demonstrates clear separation between clean and adversarial images. The study demonstrates that DNNs with WNLL are more robust to small random perturbations, as shown in FIG4. Adversarial attacks cause features to overlap, but TVM helps eliminate outliers. Adversarial images can be correctly classified with the interpolating function guided by training data. The study shows that DNNs with WNLL are robust to small perturbations. Adversarial attacks cause feature overlap, but TVM eliminates outliers. Training data guides the interpolating function to classify adversarial images correctly. Defensive strategies include data-dependent activation, input transformation, and training data augmentation. ResNet-56 is trained on original data, TV minimized data, and a combination of both, with additional TVM transformation for boosting defensive performance. The study demonstrates the robustness of DNNs with WNLL activation to adversarial attacks. Adversarial images generated using FGSM, IFGSM, and CW-L2 are transferable between DNNs with different activation functions. When classifying these adversarial images, ResNet-56 with WNLL activation shows significantly higher accuracy compared to softmax activation. The study shows that DNNs with WNLL activation are more robust to adversarial attacks compared to softmax activation. The defense method combines WNLL activation, TVM, and data augmentation to improve accuracy in various attack scenarios. Testing accuracy remains high even with strong attacks, with FGSM being a weaker attack method. IFGSM and CW-L2 can fool ResNet-56, but the defense maintains accuracy above 68.0%. Our defense strategy, which includes WNLL activation, TVM, and data augmentation, improves accuracy against adversarial attacks. Testing accuracy remains high even under strong attacks, with ResNet-56 performing similarly to PixelDefend on CIFAR-10. Additionally, our defense is more stable to IFGSM attacks and additive to adversarial training. Simple TVM alone cannot defend against FGSM attacks, but WNLL activation significantly improves testing accuracy against adversarial attacks. By analyzing the influence of adversarial perturbations on DNNs' features, a defense strategy is proposed using data-dependent activation functions, total variation minimization, and training data augmentation. Results show improved robustness against adversarial attacks on ResNet-56 with CIFAR-10. Total variation minimization simplifies adversarial images, aiding in removing perturbations. Further exploration includes applying other denoising methods for defense."
}