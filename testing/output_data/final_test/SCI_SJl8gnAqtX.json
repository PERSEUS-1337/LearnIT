{
    "title": "SJl8gnAqtX",
    "content": "We propose a new application of embedding techniques for problem retrieval in adaptive tutoring. Our hierarchical problem embedding algorithm, Prob2Vec, achieves 96.88% accuracy on a problem similarity test, outperforming state-of-the-art methods. Prob2Vec can distinguish fine-grained differences among problems, a task that is challenging for humans. The traditional teaching methods in STEM courses do not consider learners' different abilities, background knowledge, and learning goals, leading to inefficiencies and discouragement. To address the issue of concept labeling with imbalanced training data, a novel negative pre-training algorithm is proposed to reduce false negative and positive ratios for classification. E-learning methods aim to personalize the learning process by providing adaptive curriculum sequences. Web-based tools recommend problems based on difficulty level, but may not consider concept continuity or mixture of concepts. This can hinder a learner's ability to solve realistic problems involving a combination of concepts. The recommender system for adaptive practice in e-learning aims to address the challenge of determining similarity/dissimilarity of problems with a mixture of concepts. To overcome the limitations of supervised methods in learning similarity scores, a numerical representation of problems on mixture of concepts with a similarity measure is proposed. This approach involves using vector representations (problem embedding) to capture similarity across single and mixed concept problems. Creating a proper problem representation that captures mathematical similarity is challenging. Existing text representation methods fail to work for math problems as they focus on lexical and semantic similarity, which is not valid for specialized topics like math. Subject-related keywords are not informative for embedding math problems, leading to a lack of correlation between problem wording and similarity. The text discusses how the similarity of math problems is not determined by problem wording but by the conceptual ideas behind them. A new hierarchical approach called Prob2Vec is proposed, which involves an abstraction step to project problems into a set of concepts. This concept space can represent a wide variety of problems, allowing for better problem similarity detection in problem recommender applications. The Prob2Vec algorithm constructs vector representations of math problems based on concept cooccurrence, achieving 96.88% accuracy in a problem similarity test. This approach captures common concepts and continuity among them, outperforming existing methods that directly apply sentence embedding. Prob2Vec algorithm achieves 75% accuracy in applying sentence embedding to distinguish fine-grained differences among math problems. The obtained problem embedding is used in an e-learning tool's recommender system for an undergraduate probability course with successful results on hundreds of students, benefiting minorities. The concept labeling sub-problem faces challenges due to dimensionality explosion, with as many as 2N problem types. The text discusses challenges in classifying problem types due to lack of data and imbalance in training samples. It proposes pre-training neural networks with negative samples to improve classification accuracy. Additionally, it mentions the success of word embedding techniques like Word2Vec in various applications. In this work, various methods like Doc2Vec, Paper2Vec, Gene2Vec, Graph2Vec, and others are discussed, all utilizing techniques originally proposed for word embedding with domain-specific modifications. The work also explores problem embedding for personalized problem recommendation, building on prior research on word, phrase, sentence, and paragraph embedding techniques. The paper discusses various methods for text embedding, including BID methods for multiplicative functions, uniform averaging, autoencoder models, LSTM and convolutional neural networks. A new method proposed by BID0 outperforms previous methods and serves as the baseline. The paper outlines the data set description, Prob2Vec method for problem embedding, negative pre-training for concept extraction, and similarity detection test setup. Section 4 discusses the setup for similarity detection test for problem embedding and compares the performance of the proposed Prob2Vec method with baselines. The paper introduces a set of problems for an undergraduate probability course, labeled with concepts for proper problem representation. The Prob2Vec method uses an automated concept extractor for problem embedding. The Prob2Vec method utilizes an automated concept extractor for problem embedding, eliminating the need for human labeling. Traditional text embedding methods struggle with accurately detecting similarities between problems due to the lack of mapping conceptual ideas into mathematical words. To address this issue, a hierarchical method involving abstraction and embedding steps is proposed for generating precise automated problem embeddings. The Prob2Vec method proposes a hierarchical approach for generating precise automated problem embeddings. It involves an abstraction step where problems are mapped into representative concepts to move problem embedding from lexical similarity to conceptual similarity. This method aims to capture similarities among mathematical problems through automated concept extraction, eliminating the need for human labeling. The Prob2Vec method proposes a hierarchical approach for automated problem embeddings by mapping problems to representative concepts for conceptual similarity. Rule-based mappings are used for concept extraction, followed by concept embedding using a method similar to Skip-gram in Word2Vec. The Prob2Vec method proposes a hierarchical approach for automated problem embeddings by mapping problems to representative concepts for conceptual similarity. The neural network is trained using one-hot coding to create embedded concept vectors with 10 features in the hidden layer. The neural network is trained on pairs of concepts using one-hot coding, learning statistics from the frequency of pairs fed into it. Input and output are represented as one-hot vectors during training, but after training, the output is a probability. After training the neural network on pairs of concepts using one-hot coding, the output is a probability distribution on all concepts. The hidden layer weight matrix contains concept vectors, which are obtained by scaling concept embeddings with their frequencies. Problem embeddings are calculated based on concept frequencies. Similarity between problems is determined by the cosine of the angle between their vector representations. The text discusses the challenges of concept extraction using rule-based methods over supervised or unsupervised classification due to limited training data and the complexity of categorizing problems based on a large number of concepts. The approach aims to minimize false positives for each category. The text discusses the challenges of concept extraction using rule-based methods over supervised or unsupervised classification due to limited training data and the complexity of categorizing problems based on a large number of concepts. Utilizing a rule-based classifier for problem labeling, the approach achieves low false positive and false negative rates for all concepts. The accuracy in similarity detection test is observed to be 100% when using problem concepts annotated by experts, but there may not be a unique global optimum for problem concepts leading to good performance. The text discusses the challenges of concept extraction using rule-based methods and proposes a neural network-based concept extractor to complement the rule-based version. A novel negative pre-training method is introduced to reduce false negative and positive ratios for concept extraction with imbalanced training data sets. This method outperforms transfer learning algorithms for tackling imbalanced data sets. Tricks for reducing false negative and positive ratios are also proposed. The neural network for concept extraction uses an embedding layer with linear perceptrons, followed by two layers of perceptrons with sigmoid activation, and an output layer with a single perceptron. The embedding layer is initialized by Glove BID33 for common words, and weights are initialized uniformly for words not in Glove. Each concept has a separate neural network due to imbalanced positive and negative samples. The training of a neural network for concept extraction faces challenges due to imbalanced positive and negative samples. To address this, tricks such as negative pre-training are used to reduce false negatives and false positives by up to 76.51%. Only a few problems out of 635 are labeled with specific concepts, making training difficult with limited positive samples. In the training of a neural network for concept extraction, two phases are proposed to address imbalanced positive and negative samples. The first phase involves pre-training on a pure set of negative samples, followed by training on a balanced mixture of positive and negative samples in the second phase. This approach utilizes negative pre-training to reduce false negatives and false positives, resulting in lower FN and FP compared to down sampling. The network learns the structure of negative samples in the first phase of negative pre-training, providing a warm start for the second phase. One-shot learning involves training the neural network on bags of problems with equal numbers of negative and positive samples before fine-tuning on the concept of interest. Word selection is crucial due to limited positive training samples, with an expert TA selecting informative words to reduce overfitting and improve convergence. In the process of problem embedding, an expert TA selects informative words related to probability to reduce the size of the embedding matrix and improve the neural network's performance. This selection results in a significant reduction in false positive and false negative ratios, indicating that the chosen words are more representative than the original ones. These selected words are used in problem embedding for modified baselines, showing that keyword-based versions do not capture problem similarity effectively. In the process of problem embedding, a ground truth on problem similarity is needed. Four TAs select random triplets of problems and order them based on similarity. Results are brought into a consensus, with 64 triplets chosen. The problems are divided into modules, making similarity detection challenging. The similarity gap histogram for the 64 triplets is shown in FIG2. The problem embedding project utilizes annotation for problem concepts and Skip-gram-based embedding, achieving 100.00% accuracy in similarity detection. Expert annotation is unbiased, and the similarity gap histogram highlights the challenge of detecting similarity in problem triplets. Prob2Vec is compared with baseline text embedding methods in determining similarity, with results reported in table 1. The Glove-based method for problem embedding involves averaging Glove word embeddings and removing the first singular vector. SVD-based problem embedding follows a hierarchical approach similar to Prob2Vec. Baseline methods are detailed in the appendix. The best Glove-based method shows error rates in different similarity gap bins, with 6 errors out of 20 triplets in the [0.01, 0.21] range. The best Glove-based method makes six errors out of 20 triplets in the similarity detection test. Interesting patterns on concept continuity and similarity are observed from Prob2Vec concept embedding. NN-based concept embedding can capture relations between concepts quickly with few training samples. The NN-based concept extractor at university struggles to understand ML-parameter-E concept related to differentiation. Bayes-formula is linked to law-of-total-probability. Comparison of methods for training NN-based concept extractor is shown in Table 3 for five concepts. The NN-based concept extractor at university struggles to understand ML-parameter-E concept related to differentiation. Comparison of methods for training NN-based concept extractor is shown in Table 3 for five concepts. Training and cross-validation are done for 100 rounds to find empirical false negative and positive ratios. Employing word selection and negative pre-training reduces false negative and positive ratios by 43.67% to 76.51% compared to naive down sampling method. The study compares different methods for training a NN-based concept extractor, showing that word selection and negative pre-training are effective in reducing false negative and positive ratios. A hierarchical embedding method called Prob2Vec is proposed, which outperforms baselines by more than 20% in similarity detection tests. The Prob2Vec embedding vectors are used in an e-learning tool's recommender system for an undergraduate probability course. Negative pre-training is also suggested for training with imbalanced data sets to decrease false negatives and positives. In future work, graphical models and problem embedding vectors will be used to evaluate student strengths and weaknesses for more effective problem recommendation. Different values for hyper-parameter a were tested, with a = 10^-5 and a = 10^-3 working best for the data set. The 3-SVD method can be used for concept embedding instead of Prob2Vec. Prob2Vec introduces concept embedding using an SVD-based method instead of Skip-gram. The co-occurrence matrix is formed based on the number of co-occurrences of concepts in problems. The SVD decomposition of the PPMI matrix results in concept embeddings, with different variants available. The concept embedding BID21 involves deriving embedding of N concepts using matrices U d and S d, with a slightly different definition of the PPMI matrix."
}