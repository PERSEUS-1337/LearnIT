{
    "title": "HJDV5YxCW",
    "content": "Recent work has shown that using very-low-bitwidth representations in models can still yield accurate results. A method to train models with a mixture of bitwidths is proposed to fine-tune the accuracy/speed trade-off. The \"middle-out\" criterion is introduced to determine the bitwidth for each value, leading to superlinear scaling of accuracy with bitwidth. Recent advancements in AI have shown that Convolutional Neural Nets (CNNs) are surpassing human performance in vision tasks. However, CNNs are computationally intensive and typically run on GPUs, which can be costly for mobile or embedded applications. To address this, techniques like binarization have been developed to reduce model size and operations. Binarized CNNs use single-bit values and bitwise operations to achieve up to 32x model size reduction and 64x fewer operations, although they may sacrifice some accuracy. Recent advancements in AI have shown that Binarized CNNs can reduce model size and operations by up to 64\u00d7, but they are less accurate. Research aims to narrow the accuracy gap between binary and floating point models by adding bits to activations and weights. However, using more bits significantly increases computation and memory requirements. There is a push to use fewer bits while maintaining acceptable accuracy, as current binary approximations are limited to the same number of bits for all values, leading to substantial accuracy differences between bits. For example, 1-bit accuracy is deemed unsatisfactory while 2-bit accuracy is considered high. Recent advancements in AI have shown that Binarized CNNs can reduce model size and operations by up to 64\u00d7, but they are less accurate. Research aims to narrow the accuracy gap between binary and floating point models by adding bits to activations and weights. However, using more bits significantly increases computation and memory requirements. In order to bridge the gap between integer bits, Heterogeneous Bitwidth Neural Networks (HBNNs) are introduced, which use a mix of integer bitwidths to allow values to have effectively fractional bitwidths. HBNNs can approximate each value better than fixed-bitwidth schemes, giving disproportionate accuracy gains for the number of effective bits used. For instance, Alexnet trained with an average of 1.4 bits has comparable accuracy to training with a fixed two bits. The main contributions are proposing HBNNs to break the integer-bitwidth barrier in binarized networks and introducing the middle-out bitwidth selection algorithm. The study explores heterogeneous binarization on the ImageNet dataset using an AlexNet architecture, showing that HBNNs yield small and fast networks with improved accuracy compared to 2-bit binarized networks. The benefits of heterogeneous binarization are also demonstrated on MobileNet BID6, even on modern architectures. Various techniques for binarization are discussed, with weights maintained in floating point format during training. In propagation, weights and activations are passed through a binarization function to create a small, discrete set. A custom gradient is applied during backwards propagation to update the floating point weights for the binarization layer. After training, the binarization function is applied to create true binary weights for inference. Binarization was first introduced by BID1, with BinaryConnect converting 32-bit tensors to 1-bit variants using a stochastic equation. The hard sigmoid function is used for binarization, and a custom gradient function is applied. BinaryConnect showed excellent results. BinaryConnect initially used DISPLAYFORM1 for binarization, showing good results on CIFAR-10 and MNIST but only achieving 27.9% accuracy on ImageNet. Later, improvements were made by simplifying the binarization process with the straight-through estimator. BID10 further enhanced the model by adding a scalar term to improve the fit of binarized values to floating-point values, resulting in higher fidelity with minimal extra computation. The authors achieved a significant improvement in accuracy on ImageNet using scalars and the straight-through estimator gradient. Increasing the number of bits for quantizing activations also boosted accuracy. Different binarization functions were used, with all n-bit schemes requiring similar computation and accuracy. The residual error binarization function was extended for binarizing to multiple bits. Residual error binarization extends the function for binarizing to multiple bits, with each additional bit taking a step from the previous bit. This process approximates inputs using one of 2^n values. There is a performance gap between 1-bit and 2-bit networks, with the highest single-bit performer, Xnor-Net, being roughly 7 percentage points less accurate than the 2-bit networks. Residual error binarization extends the function for binarizing to multiple bits, with each additional bit taking a step from the previous bit. Xnor-Net remains roughly 7 percentage points less accurate than the 2-bit variant, while the 1-bit equivalent lags by 4 points. The question arises whether it is possible to attain accuracies closer to those of 2-bit models while running at speeds closer to those of 1-bit variants. In this section, we discuss extending residual error binarization to allow heterogeneous bitwidths, which offers representational benefits. We propose methods for distributing bits in a mixture of bitwidths and modify the binarization equation accordingly. Unlike homogeneous binarization, heterogeneous binarization generates binarized values based on specified bitwidths, leading to increased speed and compression in HBNNs. Heterogeneous binarization generates binarized values with up to n steps, doubling the number of distinct values compared to homogeneous binarization. By targeting values that benefit from fewer bits, overall approximation accuracy can improve. The distribution of bits in a heterogeneous binary tensor is crucial for high representational power, equivalent to determining how M should be generated. When computing M, the goal is to assign bitwidths to fractions of T based on specified constraints. Algorithm 1 outlines the process of computing M, sorting values by suitability for binarization to different bitwidths. The algorithm iterates through each bitwidth to determine the percentage of T to binarize, ultimately generating a bit map M for heterogeneous binarization of T. The algorithm steps through each bitwidth b and selects the most suitable values of T to be binarized. Several selection methods are proposed, including Top-Down, Middle-Out, Bottom-Up, and Random. Middle-Out selection is based on the intuition that values close to the center benefit from not taking a step. Our results show that Middle-Out selection is much better than other techniques for binarizing values near the center of the data distribution. Binarizing weights reduces model size and computations by replacing 32-bit-float weights with a small number of bits packed into 64-bit integers. Computation reduction is achieved by binarizing both inputs and weights, allowing for popcount-xnor operations. However, heterogeneous bitwidth tensors cannot be efficiently packed into integers. Packing and performing xnor-popcounts on a heterogeneous tensor requires an additional tensor indicating the bitwidth of each value. Custom hardware like ASIC or FPGA can efficiently perform individual bit operations, reducing total gates and power consumption. Evaluation of HBNNs aims to address accuracy scaling with an uninformed bit distribution. In this section, the focus is on evaluating the accuracy scaling with an uninformed bit distribution using AlexNet with batch-normalization (AlexNet-BN). Batch normalization is crucial for binary networks to distribute values around zero, and binarization functions are inserted within the convolutional layers. A floating point copy of weights is maintained for back-propagation, while binarization is done during forward propagation. During back-propagation, weights are updated and binarized during forward propagation using the straight-through estimator for gradients. A scaling layer is added to reduce the large outputs of a binary layer. Models are trained using an SGD solver with specific parameters and randomly initialized weights for 90 epochs in the PyTorch framework. Two experiments are conducted to measure the ability of different binarization schemes to approximate a floating point tensor. A \"poor man's\" approach to HBNNs is used as a baseline, where the number of bits each kernel is fixed upfront. For the experiment, 10 mixes of 1, 2, and 3-bit kernels were considered to sweep average bitwidths between 1 and 2 using the CIFAR-10 dataset. A deliberately hobbled model with a maximum accuracy of roughly 78% served as the baseline 32-bit variant. The results showed that accuracy increases linearly with average bitwidth, but the hope is for better scaling with \"data-aware\" bitwidth selection techniques. The experiment compared different binarization methods using a large tensor of random values. Middle-out selection method outperformed others, approximating input similar to standard 2-bit binarization. The actual bit mix affected performance, with a mix of 40%/50%/10% showing notable results. The experiment compared different binarization methods using a large tensor of random values. The actual bit mix changes performance rather than just the average, with some mixtures better suited for approximating a distribution. Applying selection techniques to the AlexNet-BN model showed improved results. The experiment compared different binarization methods using a large tensor of random values. Middle-Out selection outperforms other techniques, achieving a favorable trade-off between accuracy and bitwidth. BID3 achieved state-of-the-art results by binarizing weights to 2 bits in an AlexNet-BN model. The study focused on binarizing AlexNet-BN models with fractional bitwidths using middle-out selection to achieve comparable accuracy with fewer than two bits. Results showed nearly identical top-1 accuracy to full 2 bit results with an average of only 1.4 bits. Accuracy scaled super-linearly with respect to bitwidth, increasing quickly from 1 bit to 1.3 bits before approaching full precision accuracy. Different mixes of bits with the same average bitwidth gave similar results, indicating that bit composition during training does not significantly impact performance. Training from scratch, the composition of a bit mix does not significantly impact performance. Incorporating stochastic layer binarization may improve low-bitwidth results. Applying 1.4 bit binarization to MobileNet with heterogeneous binarization achieved a top-1 accuracy of 65.1%. Our HBNN achieved a top-1 accuracy of 65.1% by binarizing both inputs and weights to reduce operations in a binary network. Binarizing inputs excludes the first and last layers to maintain accuracy. Comparing HBNN accuracy to recent results on AlexNet-BN showed improvements. TAB0 presents various results on accuracy achieved through different input and weight binarization techniques. Using 1.4 bits for binarizing inputs and weights resulted in a top-1 accuracy of 53.2%, outperforming other methods with similar operations. Binarizing inputs to 1 bit and weights to 1.4 bits also showed significant accuracy improvement. Overall, the study found that using more than 1.4 average bits had minimal impact on accuracy. In this paper, Heterogeneous Bitwidth Neural Networks (HBNNs) are introduced as a new type of binary network that allows fractional bitwidths. A method of distributing bits across a tensor leads to a linear relationship between accuracy and number of bits, with Middle-Out bit selection being the top performing technique. Extensive experiments on the ImageNet dataset with AlexNet and MobileNet models validate the effectiveness of HBNNs in tuning accuracy, compression, and speed trade-offs. The effectiveness of Heterogeneous Bitwidth Neural Networks (HBNNs) is validated through experiments on ImageNet dataset with AlexNet and MobileNet models. HBNNs match or outperform competing binarization techniques while using fewer bits, enabling applications requiring high compression and speed with accuracy. Future work includes modifying bit selection for CPU computation and developing an HBNN FPGA implementation for showcasing speed and accuracy benefits."
}