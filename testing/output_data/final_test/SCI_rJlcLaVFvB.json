{
    "title": "rJlcLaVFvB",
    "content": "Hierarchical Sparse Coding (HSC) is a model for representing structured data like images efficiently. A new model, Sparse Deep Predictive Coding (SDPC), is introduced to evaluate the impact of inter-layer feedback connections. SDPC outperforms a Hierarchical Lasso (Hi-La) network in prediction error reduction by transferring error between layers. Sparse Deep Predictive Coding (SDPC) is shown to outperform the Hierarchical Lasso (Hi-La) model in reducing prediction error by transferring error between layers. The inference stage of SDPC converges faster and accelerates the learning process. Qualitative analysis reveals that SDPC features are more generic and informative compared to Hi-La. Sparse Coding is a successful method for efficiently representing signals by encoding them as a linear combination of few features from a dictionary. Sparse Coding (SC) involves two main subproblems: inference (coding) and dictionary learning. Inference aims to find a sparse representation of input data using algorithms like ISTA, FISTA, Matching Pursuit, Coordinate Descent, or ADMM. Once the representation is obtained, atoms can be learned from the data using methods like gradient descent or online dictionary learning. SC offers an unsupervised framework for learning basis vectors and input representations simultaneously, with successful applications in image restoration, feature extraction, and classification. Sparse Coding (SC) involves inference and dictionary learning. SC algorithms are used for image restoration, feature extraction, and classification. Hierarchical Sparse Coding (HSC) aims to model the hierarchical structure of the visual cortex. Various methods have been proposed for HSC optimization, but their neuronal implementation is not always considered. Predictive Coding (PC) is a neural model that combines feedforward and feedback connections to solve the inverse problem of vision. It involves updating neural populations to minimize unexpected neural signals. PC has been applied in supervised object recognition and unsupervised prediction of future video frames, with the flexibility to introduce a sparse prior to each layer, making it a bio-plausible formulation. The study compares Predictive Coding (PC) with another bio-plausible formulation for optimizing Hierarchical Sparse Coding of images. It aims to investigate the effects of top-down connections in PC, computational differences, convergence, and learned atoms. The study defines two mathematical formulations: Hierarchical Lasso (Hi-La) and 2-Layers Sparse, to solve the HSC problem. The study compares Hierarchical Lasso (Hi-La) and 2-Layers Sparse Predictive Coding (2L-SPC) for image optimization. It analyzes prediction error distribution, convergence, and learned features of both models. Mathematical symbols are used for representation. The core objective of Hierarchical Sparse Coding (HSC) is to infer internal state variables for input images and learn parameters to solve the inverse problem. Internal state variables' sparsity is constrained by a scalar \u03b1. 4-dimensional tensors represent vectors and matrices, with input images represented as tensors of size [1, c x , w x , h x ]. A 2-dimensional convolutional structure is imposed on the parameters {D i}. The Hierarchical Sparse Coding (HSC) framework involves inferring internal state variables for input images and learning parameters to solve the inverse problem. The parameters {D i} are structured as Toeplitz matrices, representing synaptic weights between neural populations. To maintain locality in neural processing, a loss function with a sparsity penalty is minimized for each layer using gradient-based methods like the Iterative Shrinkage Thresholding Algorithm (ISTA). The HSC framework involves inferring internal state variables for input images and learning parameters using the ISTA algorithm. FISTA, an accelerated version of ISTA, is used in practice for faster convergence. Eq. 3 represents a recurrent layer called the Lasso layer, with D T i as a decoding dictionary for back-projection. The Hierarchical Lasso (Hi-La) network involves stacking Lasso layers together to update sparse maps recursively until reaching a stable point. An alternative approach is using Predictive Coding (PC) theory, which considers both bottom-up and top-down prediction errors to find a trade-off in representation. The Hierarchical Lasso (Hi-La) network involves stacking Lasso layers together to update sparse maps recursively until reaching a stable point. An alternative approach is using Predictive Coding (PC) theory, which considers both bottom-up and top-down prediction errors to find a trade-off in representation. The Sparse Predictive Coding (SPC) layer forms the building block of the 2-Layers Sparse Predictive Coding (2L-SPC) network, which includes an inter-layer feedback connection to materialize the influence from upper-layers. The inference process is finalized once the relative variation of \u03b3 t i w.r.t to \u03b3 t\u22121 i is below a threshold denoted T stab, typically requiring 30 to 100 iterations. The Hierarchical Lasso (Hi-La) network involves stacking Lasso layers to update sparse maps recursively until convergence. Dictionaries are updated using gradient descent, with learning and inference processes defined by minimizing a specific problem. Dictionaries are initialized randomly and normalized after each update to prevent redundant solutions. The Hi-La network involves stacking Lasso layers to update sparse maps recursively until convergence. Dictionaries are updated using gradient descent, with learning and inference processes defined by minimizing a specific problem. The dictionary learning loss is the same for both 2L-SPC and Hi-La models. PyTorch 1.0 was used to implement, train, and test the models. The code and simulations are available on www.github.com/XXX/XXX. Training and testing were done on 4 different databases, including STL-10 with 100000 colored images representing 10 classes of objects. The AT&T database contains 400 grayscale images of faces from 40 subjects, split into training and testing sets. The Chicago Face Database consists of 1,804 color photographs of faces resized for computational efficiency, also split into training and testing sets. The dataset includes images of handwritten digits from the MNIST dataset, split into training and testing sets. The images are pre-processed using Local Contrast Normalization and whitening for fair comparison between models. Parameters for training are summarized in tables for different databases. In this study, the dictionary D i is learned using stochastic gradient descent on the training set with specific parameters. Sparsity parameters of each layer are varied to evaluate their impact on 2L-SPC and Hi-La networks. Cross-validation is performed by running simulations multiple times with different random dictionary initializations. The central tendency of the curves is defined by the median of the runs and its variation by the Median Absolute Deviation (MAD). The analysis focuses on the global performance of the models on the testing set. The global prediction error is analyzed for 2L-SPC and Hi-La models with varying sparsity parameters. The error increases with higher \u03bb values in both models. The first layer prediction error is consistently higher for 2L-SPC, while the second layer prediction error remains lower. The prediction error among layers varies with different sparsity parameters in 2L-SPC and Hi-La models. The first layer prediction error is consistently lower for Hi-La compared to 2L-SPC, with an increase in \u03bb leading to a faster rise in prediction error for Hi-La. The increase of \u03bb 1 in the 2L-SPC model is partially absorbed by the second layer, leading to a slight increase in prediction error. In contrast, increasing \u03bb 2 does not affect the first layer prediction error in the Hi-La model, while it slightly increases in the 2L-SPC model. This difference is due to the inter-layer feedback connection in the 2L-SPC model. The 2L-SPC model shows sensitivity to \u03bb 1 sparsity over \u03bb 2, with a minimum relative difference of 10.6% when \u03bb 1 is maximal and \u03bb 2 is minimal. The feedback connection is more efficient with lower sparsity in the first layer, mitigating global prediction error by distributing it better among layers. The 2L-SPC model demonstrates sensitivity to the sparsity of the first layer, showing a more efficient feedback connection with lower sparsity. This leads to better distribution of prediction error among layers, resulting in less global prediction error. Additionally, the 2L-SPC model requires fewer iterations to converge towards a stable state compared to the Hi-La model, with data dispersion being more pronounced in the latter. The 2L-SPC model shows sensitivity to first layer sparsity, leading to better error distribution and lower global prediction error. It requires fewer iterations to converge compared to the Hi-La model, with data dispersion more pronounced in the latter. The evolution of iterations and prediction error is analyzed on various databases. The 2L-SPC model outperforms the Hi-La model in terms of prediction error, with a faster convergence rate and lower error in the initial epochs. The inter-layer feedback connection in the 2L-SPC contributes to this improvement. The model's first and second layer RFs have specific sizes and are selected based on activation probability. The impact of the inter-layer feedback connection on the dictionaries can be visualized through Receptive Fields (RFs). First layer RFs are Gabor-like filters, while second layer RFs represent more abstract concepts. The 2L-SPC model shows longer curvatures in second layer RFs compared to the Hi-La model, covering a bigger part of the input image with more contextual details. The Hi-La second layer RFs are over-fitted to specific faces, with activation probabilities of 0.25% and 0.69% for Hi-La and 2L-SPC respectively. The lowest activation probability of the second layer's atoms is higher for 2L-SPC than for Hi-La (0.30% versus 0.16%). When sorting all features by activation probabilities, the phenomenon is more evident. The filters are displayed in various figures for different datasets. The 2L-SPC model with inter-layer feedback connections shows more generic and informative features compared to the Hierarchical Lasso model. This extra connection helps the internal state variables converge towards a balance between accurate prediction and predictability. Experimental results on 4 databases demonstrate that the inter-layer feedback connection mitigates overfitting and improves the overall performance of the network. The 2L-SPC model utilizes inter-layer feedback connections to distribute prediction error, accelerate convergence, and enhance learning. It incorporates contextual information for more informative features and differs from CNNs by leveraging local sparse coding in a hierarchical and unsupervised manner. This approach is novel and shows robust results compared to other algorithms. ML-ISTA from ) is trained using supervised learning. Results are robust across different databases and sparsity levels. Future work will focus on generalizing results to deeper networks and different sparse coding algorithms. The 2L-SPC framework has practical applications in image inpainting, denoising, and super-resolution. The 2L-SPC inference algorithm involves initializing variables and momentum strength, with a stability threshold. Global prediction error is shown to vary with different parameters. The global prediction error is evaluated on testing sets for 2L-SPC and Hi-La networks with varying sparsity levels in different layers. Experiments were conducted on STL-10, CFD, and MNIST databases. The distribution of prediction error among layers is shown when varying parameters. The number of iterations needed for inference varies with different parameters. The number of iterations needed for stability criteria and global prediction error evolution are analyzed for 2L-SPC and Hi-La networks on the AT&T testing set. The second-layer effective dictionary generation process is also discussed, resembling the concept of preferred stimulus in neuroscience."
}