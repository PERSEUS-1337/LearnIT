{
    "title": "rkgTdkrtPH",
    "content": "Knowledge graph has gained attention for its applications, but still suffers from incompletion and errors. A unified Generative Adversarial Networks framework is proposed to combine knowledge graph completion and error detection. Experiments show superior performance compared to existing algorithms. Representation of knowledge is crucial in various real-world applications like web search, question answering, and personalized recommendation. Knowledge graphs are constructed by extracting triples from unstructured text, representing relations between entities. Despite extensive construction of knowledge graphs like Freebase, DBPedia, and YAGO, they suffer from sparsity and errors due to the inability to collect all information and automatic mechanisms used for construction. Various methods for knowledge graph refinement have been proposed to address errors in knowledge graphs, including ambiguous, conflicting, and redundant information. These methods focus on knowledge graph completion, adding missing knowledge, and error detection, identifying incorrect triples. Knowledge graph embedding (KGE) is currently the state-of-the-art for completion, but they lack robustness to noise. Error detection is challenging due to noisy data, with reasoning-based methods being widely used for this task. In this paper, the authors propose a unified GAN framework called NoiGAN to combine error detection and knowledge graph embedding tasks. This approach aims to address the challenges of noisy data and improve the reliability of knowledge graph embedding by integrating error detection and embedding learning. NoiGAN is a unified GAN framework for noise-aware knowledge graph embedding. It consists of a noise-aware KGE model and an adversarial learning framework for error detection. The KGE model uses GAN's confidence score to eliminate noisy data, while GAN requires high-quality embeddings and credible examples from the KGE model. Cooperation between the two components improves their capabilities. The error detection model and KGE model benefit from each other, with the KGE model enhancing embedding quality and the error detection model improving noisy triple distinction. The proposed framework can be applied to various KGE models to handle noisy knowledge graphs effectively. Experimental results show the superiority of the new algorithm over existing ones, with the KGE model and GAN boosting performance in knowledge graph completion and noise detection. Embedding-based methods lead in knowledge graph completion by capturing entity similarity through low-dimensional vectors. The KGE model enhances embedding quality and the error detection model improves noisy triple distinction. Two categories of KGE models are translational distance and semantic matching models. Representative approaches include TransE, TransH, TransR, RESCAL, DistMult, and ComplEx. Negative sampling is used to optimize the KGE model by minimizing margin based ranking loss. Random sampling is a conventional method to construct negative samples, but they are often too easy. In this paper, a novel technique is proposed to improve negative sampling in embedding models by addressing noisy data. Existing methods for error detection in knowledge graphs can be ontology-based or anomaly detection-based. Ontology-based methods utilize additional ontology information and logic programming to uncover contradictions. The curr_chunk discusses different methods for error detection in knowledge graphs, including ontology-based methods and anomaly detection-based methods. Anomaly detection methods may not accurately identify errors, while a novel confidence-aware framework proposes incorporating triple confidence into KGE models to detect noises. However, this approach may be influenced by model bias. The proposed framework, NoiGAN, aims to learn noise-aware KG embedding using Generative Adversarial Networks (GAN). It includes a Noise aware KGE model with a confidence score, a Triple generator that generates likely noisy triples, and a Discriminator that distinguishes correct from noisy triples. The noise-aware KGE model in the proposed NoiGAN framework assigns confidence scores to correct triples and guides the generator to produce higher quality noisy data. It defines a knowledge graph as a set of entities, relations, and observed facts. The model aims to eliminate noise impact on embedding vectors by incorporating confidence scores, making it adaptable to various KGE models. The proposed NoiGAN framework assigns confidence scores to correct triples to guide the generator in producing higher quality noisy data. It adapts easily to KGE models like TransE, aiming to learn low-dimensional vectors for entities and relations in the same space. The scoring function of TransE minimizes the distance between entities in the knowledge graph. To optimize the KGE model, a loss function similar to negative sampling is used, with multiple negative triples sampled for each observed fact to reduce randomness. The triples set is created by replacing entities in observed triples with randomly sampled entities from set E. Existing embedding models assume all triple facts in knowledge graphs are true, but this is inaccurate due to errors. Confidence scores are used to determine if a triple is noisy, with a focus on reducing model bias. Unlike previous methods, a discriminator is used to learn confidence scores impartially. In Section 3.2.2, a noise-aware KGE model is introduced to enhance embedding quality by identifying errors in the knowledge graph. Confidence scores, denoted as C(h, r, t), are used to distinguish noisy data from the learning process. The model can assign binary or soft values to these scores. Previous approaches relied on costly label information or ontology data for error detection, but the proposed model aims to address this challenge more impartially and effectively. The proposed adversarial learning framework utilizes GAN to detect noise in knowledge graphs by working with a noise-aware KGE model. The framework consists of a generator and a discriminator, with the goal of producing a good discriminator to distinguish noisy triples from true triples. The adversarial learning framework uses GAN to detect noise in knowledge graphs with a noise-aware KGE model. The generator creates confusing triples to improve the quality of noisy data for the discriminator, which aims to differentiate between true and fake triples. The generator's goal is to produce high-quality fake triples to deceive the discriminator and improve error identification in the knowledge graph. The noise-aware KGE model incorporates noisy triples as negative samples to train the model. A two-layer neural network is used to select likely noisy triples from candidate negative samples, improving error identification in the knowledge graph. The noise-aware KGE model uses embedding vectors to determine the probability of noisy triples. Training the generator involves maximizing the expected reward from the discriminator, which evaluates the likelihood of a triple being true. Initially, the generator produces random \"noise\" but learns to improve its distribution through reinforcement learning. The generator in the noise-aware KGE model continually learns to generate more confusing triples to improve the discriminator's capacity. It also produces high-quality negative samples for the model. The discriminator distinguishes true triples from noisy ones, with both positive and negative examples needed for training. Utilizing top 10% triples helps avoid memorizing noise in the knowledge graph, which can lead to poor generalization performance. The discriminator in the noise-aware KGE model aims to distinguish true triples from noisy ones by minimizing cross entropy loss using a two-layer neural network. It utilizes top 10% triples as positive training examples and defines C(h, r, t) for each triple in the knowledge graph. The proposed model distinguishes between hard and soft versions based on the probability of a triple being true. The process involves initializing confidence scores, training a noise-aware KGE model, selecting top 10% triples as positive examples, training a GAN, and updating confidence scores. The noise-aware KGE model is trained with random negative sampling to obtain reliable embedding vectors. The GAN model is trained with random negative sampling to generate noisy data for the discriminator. The generator learns the probability distribution of negative samples with guidance from the discriminator. Top 10% triple facts are selected as credible examples. The discriminator then learns to distinguish between positive and negative triples. The noise-aware KGE model is retrained using the confidence scores and generated negative samples until convergence. The noise-aware KGE model updates embedding vectors until convergence to obtain final entity and relation representations. NoiGAN is evaluated on benchmark datasets with noisy triples introduced by substituting true entities. Five KGs are constructed with different ratios of noisy triples. NoiGAN is compared with state-of-the-art KGE models, including TransE, DistMult, RotateE, attention-based method, CKRL, and KBGAN. CKRL with local triple confidence is used as a baseline. The same loss function and negative sampling strategies are employed for fair comparison. The NoiGAN model is evaluated in two versions, soft and hard, and compared with TransE and RotateE using a grid search strategy. Parameters like embedding dimension, batch size, and margin are varied. Entity and relation embeddings are uniformly initialized without regularization. Discriminator and generator are implemented as two-layer neural networks. The NoiGAN model utilizes two-layer neural networks for the generator, with hidden states set to 10. It is evaluated based on classification performance, distinguishing noises in knowledge graphs. Experiments are conducted on benchmark datasets with different ratios of noisy triples. The discriminator in NoiGAN (hard) directly classifies noise, while in NoiGAN (soft), it assigns a soft score between 0 to 1 to each triple. Triple classification is based on the probability of a triple being true. The NoiGAN model uses neural networks to detect noise in knowledge graphs. Evaluation metrics include AUC and identifying actual noises. Results show NoiGAN-TransE outperforms CKRL (LT) in detecting noises in training datasets. Our method, NoiGAN-TransE, demonstrates superior noise detection capabilities compared to baselines. It outperforms NoiGAN-TransE (soft) by completely eliminating noisy triples. Additionally, our approach excels in learning embedding quality and surpasses existing state-of-the-art algorithms. Experiments on benchmark datasets with varying ratios of noisy triples validate the effectiveness of our method. Evaluation Metric: The evaluation metrics used include Hit@K (H@K) and Mean Reciprocal Rank (MRR). Results show that NoiGAN consistently outperforms baseline methods on noisy datasets, especially with 100% noise. NoiGAN models show significant performance gains on datasets with 100% noise. NoiGAN-TransE or NoiGAN-RotatE perform best on noisy datasets, with the hard version outperforming the soft version. NoiGAN outperforms attention-based methods in terms of robustness. The discriminator in NoiGAN effectively distinguishes noisy triples, with performance improvements increasing as noise rate in KGs rises. The NoiGAN model, specifically the NoiGAN-TransE (hard) version, is proposed to combine knowledge graph completion and error detection tasks. It outperforms TransE in detecting tricky errors like logic and grammar errors. The framework consists of a noise-aware KGE model for completion and an adversarial learning framework for error detection. The NoiGAN model combines knowledge graph completion and error detection tasks using a noise-aware KGE model and an adversarial learning framework. The two components boost each other's performance iteratively, leading to superior results in both completion and error detection tasks."
}