{
    "title": "Bym0cU1CZ",
    "content": "In this work, the authors introduce dialogue acts into open domain dialogue generation to improve response quality in conversations. By jointly modeling dialogue act selection and response generation, they achieve significant improvements in both machine-machine simulation and human-machine conversation. This approach also allows for better understanding of why these improvements occur. Conversational agents are becoming more common, assisting users with tasks or engaging in social chat. Traditional research focuses on task-oriented dialogue systems with handcrafted dialogue acts, but recent interest is in end-to-end dialogue learning without assumptions on dialogue acts. Efforts are mainly on non-task-oriented chit-chat, aiming to overcome limitations and scale to new domains. In this work, the focus is on introducing dialogue acts into open domain dialogue generation to achieve interpretability and controllability in non-task-oriented dialogues. Existing research has mainly concentrated on generating relevant and diverse responses for static contexts, but it is unclear if this is sufficient for engagement in dynamic interactions. In this study, the focus is on incorporating dialogue acts into open domain dialogue generation to enhance interpretability and controllability in non-task-oriented conversations. The research aims to address the effectiveness of designing dialogue acts for understanding engagement in human-human open domain dialogue, learning a dialogue generation model with these acts, and evaluating the model's performance in practice. The study establishes a dialogue act taxonomy for open domain conversation, revealing that people drive social chat by switching contexts and asking questions in addition to responding with relevance and diversity. To enhance interpretability and engagement in open domain dialogue generation, the study proposes jointly modeling dialogue act selection and response generation using neural networks. By incorporating dialogue acts, the model improves response diversity, manages conversation flow, and enhances human engagement. This approach involves learning from human-human interactions and optimizing act selection through reinforcement learning. The study proposes a model that enhances human engagement in dialogue generation by incorporating dialogue acts. The model outperforms state-of-the-art methods in generating quality responses and sustaining long-term conversations in both machine-machine and human-machine interactions. Key contributions include designing dialogue acts based on human behavior analysis, joint modeling of act selection and response generation, and utilizing supervised learning for training. The study proposes a model for dialogue generation that incorporates dialogue acts to enhance human engagement. It outperforms existing methods in generating quality responses and sustaining conversations. Key contributions include designing dialogue acts based on human behavior analysis, joint modeling of act selection and response generation, and utilizing supervised learning for training. The model is empirically verified for effectiveness through various metrics and simulations. The curr_chunk discusses defining dialogue acts based on existing work and organizing them into high-level categories. It also mentions building a classifier with neural networks. The curr_chunk discusses the creation of a dataset by crawling dialogues from Baidu Tieba for dialogue act analysis. It includes details on the training and validation sets used for this purpose. The dataset for dialogue act analysis was created by sampling 9 million dialogues for training, 90 thousand for validation, and 1000 for testing. The Standford Chinese word segmenter was used to tokenize utterances. 500 dialogues were randomly sampled for dialogue act labeling by 3 native speakers, with moderate agreement among labelers. Frequencies of dialogue acts were reported, providing further insights for dialogue analysis. The data analysis revealed that context switching is a common skill in dialogues, with 78.2% of dialogues containing at least one context switch. Dialogues with more than 10 turns are more likely to have a context switch (85%), while shorter dialogues (less than 5 turns) have a lower rate (47%). On average, dialogues switch context every 3.39 turns. Additionally, 13.9% of context maintenance acts are questions, highlighting their importance in open domain conversations. In open domain conversations, 13.9% of context maintenance acts are questions, with a higher percentage in certain contexts. Dialogues with questions have an average of 3.92 turns, emphasizing the importance of asking questions. This observation presents challenges for existing dialogue models and suggests the need for a new approach. The text discusses learning conversational patterns from data by building a classifier to tag dialogues with dialogue acts using neural networks. The approach aims to sequentially tag utterances in a dialogue with dialogue acts based on previous inputs and outputs. The text describes the process of using bidirectional recurrent neural networks with gated recurrent units to calculate a dialogue act distribution based on previous inputs and outputs. This involves concatenating hidden states, embeddings, and using a multi-layer perceptron to learn and minimize cross entropy with dialogue acts. The objective function of learning is to formulate the probability of a dialogue act. Labeled dialogues are split for training/validation/test, achieving 70.1% accuracy on the test data. Dialogue generation learning is done using large scale dialogues tagged with dialogue acts, followed by model optimization with reinforcement learning for long-term conversation. The dialogue model consists of a policy network and a generation network. A dialogue act is selected from the policy network, and a response is generated from the generation network based on the conversation history and the dialogue act. The model can be customized for controllability or optimization. The policy network architecture involves encoding utterance and act sequences, using hierarchical and GRU encoders, to predict dialogue acts for the next turn. The generation network simplifies decoding responses by considering dialogue acts and previous utterances. The architecture of the generation network involves concatenating previous utterances and attaching a special word in encoding. The dialogue model is learned through supervised learning to fit human-human interactions, but does not explicitly encourage long-term conversations. The policy network is learned by fitting to existing conversation history without knowledge of future dialogue acts. The dialogue model is optimized through reinforcement learning by letting two models engage in self-play to improve performance. The simulation involves the models generating responses in a dialogue until a set number of turns. The generation network is fixed to maintain human-like responses, while the policy network is optimized through reinforcement learning. The policy network in the dialogue model is optimized through reinforcement learning, with a reward function defined based on conversation length and response relevance. The simulated dialogue is terminated based on cosine similarity between embeddings of utterances. The policy network in the dialogue model is optimized through reinforcement learning to prevent bad infinite loops in conversations. The reward function is based on conversation length and response relevance, with the dialogue terminated if certain conditions are met. The relevance between a response and context is measured using a dual LSTM model trained with 30 million data samples. The objective is to maximize expected future reward, calculated using the Reinforce algorithm. The objective is calculated using the Reinforce algorithm BID35 : DISPLAYFORM3 with baselines including S2SA, HRED, VHRED, and RL-S2S. Experiments are conducted with data in TAB2. The study compares different dialogue generation models, including SL-DAGM and RL-DAGM, using supervised and reinforcement learning. The models are evaluated based on the quality of responses generated in response to given contexts. Top responses from different models are rated by native speakers for quality. The study evaluates dialogue generation models like SL-DAGM and RL-DAGM using supervised and reinforcement learning. Responses are rated by annotators based on relevance, informativeness, and interest. Model improvements over baseline methods are statistically significant. Automatic metrics like BLEU and embedding-based metrics are used for comparison. The study evaluates dialogue generation models like SL-DAGM and RL-DAGM using supervised and reinforcement learning. Responses are rated based on relevance, informativeness, and interest. Dialogue acts improve response diversity significantly, leading to more varied responses. The dimension of dialogue acts adds variations to generated responses, although they may sometimes diverge from the ground truth. Automatic metrics show improvements are not significant due to response diversity. An example is provided to illustrate the advantages of the models. The study evaluates dialogue generation models like SL-DAGM and RL-DAGM using supervised and reinforcement learning. Responses are rated based on relevance, informativeness, and interest. Dialogue acts improve response diversity significantly, leading to more varied responses. The models demonstrate good interpretability and controllability. More examples are provided in the appendix. Additionally, conversation engagement is studied through machine-machine simulation. The study evaluates dialogue generation models like SL-DAGM and RL-DAGM using supervised and reinforcement learning. Experiments compare SL-DAGM and RL-DAGM with RL-S2S in machine-machine simulation and human-machine conversation. Dialogues are evaluated based on average length. Machine-machine simulation involves two bots with the same model in 1000 dialogues, while human-machine conversation includes 5 native speakers interacting with the bots. The evaluation compares SL-DAGM and RL-DAGM models in dialogue generation. Testers interact with bots in 100 dialogues each. Dialogues end if conversation cannot continue or if bot gives repetitive responses. Evaluation results show SL-DAGM and RL-DAGM lead to longer conversations with significant improvements over the baseline. The evaluation compared SL-DAGM and RL-DAGM models in dialogue generation, showing significant improvements over the baseline. RL-DAGM outperformed SL-DAGM in both experiments, capturing conversational patterns and pro-actively switching contexts. In machine-machine simulation, RL had a higher percentage of dialogues with at least one CS compared to SL, while in human-machine conversation, RL also showed improvement. In machine-machine simulation, the average lengths of dialogues without context switch are 4.78 (SL) and 2.67 (RL), comparable to or worse than RL-S2S. However, with context switch, the average lengths are 8.66 (SL) and 8.18 (RL). RL promotes context switch for engagement, even with a slight sacrifice on relevance. Additionally, 36.5% (SL) and 32.4% (RL) of dialogues in machine-machine simulation contain at least one question, while in human-machine conversation, the percentages are 17.7% (SL) and 22.3% (RL). The study analyzes how dialogue acts affect generated responses, using metrics like distinct-1, distinct-2, words out of context ratio, and average response length. Responses from CS are longer and more informative than CM, with statements and answers being more informative than questions. BLEU scores and embedding metrics show no significant difference among responses from different dialogue acts. Existing dialogue models are designed for open domain conversation or specific task completion. Various extensions have been proposed to address different challenges such as the \"safe response\" problem, modeling conversation contexts, biasing responses, and optimizing strategies. On the other hand, POMDP breaks down the development of task-oriented dialogue systems into natural language. In this work, dialogue acts are introduced into open domain dialogue generation to explain social interactions, control response generation, and guide human-machine conversations. Previous research has focused on analyzing open domain dialogues with dialogue acts for utterance classification. The dialogue act design in this work goes beyond utterance classification or clustering, aiming to interpret open domain dialogues and generate responses based on dialogue acts. By jointly modeling dialogue act selection and response generation, the proposed models outperform state-of-the-art methods in various scenarios. The proposed models in this work aim to interpret open domain dialogues and generate responses based on dialogue acts, outperforming state-of-the-art methods. The encoder's hidden state is determined by word embeddings, and decoding involves an attention mechanism to calculate context vectors. Dialogue acts are represented as probability distributions in training data. In training, dialogue acts are represented as probability distributions by averaging labels from annotators. Word embeddings are pre-trained using word2vec with an embedding size of 200. The model's parameters are updated using back-propagation and stochastic gradient descent with AdaDelta algorithm. The best performing model on validation data is selected for testing. The generation network is trained with word embedding size of 620 and hidden vector size of 1024. The encoder and decoder vocabularies contain 30,000 words each. AdaDelta algorithm BID40 is used with a batch size of 128. The initial learning rate is 1.0 and is halved if perplexity on validation increases. Training stops if perplexity keeps increasing for two epochs. The policy network has word embedding size of 100, dialogue act size of 100, and hidden state size of biGRU as 100. The MLP has 50 neurons in the first layer and 7 neurons in the second layer. Vectors in the policy network are smaller than 5. The policy network in the generation network has smaller sizes than 5. In reinforcement learning, the mini-batch size is 60 with a fixed learning rate of 0.05. A dual LSTM BID18 is used for reward estimation with word embedding and hidden states sizes of 100. Responses are generated with a beam size of 20. RL-S2S defines 8 dull responses and compares SL-DAGM and RL-DAGM with baseline models for response quality. Machine-machine simulation examples are provided in TAB0. Our models' dialogues smoothly progress under the management of dialogue acts, unlike RL-S2S dialogues that quickly converge to loops. Our models, managed by dialogue acts, improve human-like dialogues with context continuation and proper timing. Examples in TAB0 show deeper dialogues with richer content compared to baseline, although some responses may be nonsensical. Future direction includes enhancing the generation network with knowledge. The study aims to enhance the generation network with knowledge for improved human-machine conversation. Quantitative results of human-machine conversation tests are presented in FIG1, showing consistency with the numbers in TAB5 (b). Example 2 in TAB0 illustrates a human-machine conversation."
}