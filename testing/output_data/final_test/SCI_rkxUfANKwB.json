{
    "title": "rkxUfANKwB",
    "content": "Variational autoencoders (VAEs) for molecules face challenges with non-unique SMILES strings and computational costs of graph convolutions. To address this, a new approach encodes multiple SMILES strings of a molecule using recurrent neural networks and attentional pooling to create a fixed-length latent representation. This All SMILES VAE aims to map molecules to latent representations efficiently. The design of new pharmaceuticals, OLED materials, and photovoltaics requires optimization within the space of molecules. Molecules correspond to graphs with nodes labeled by atoms and edges labeled by bonds. The space of molecules is discrete and sparse, with most combinations not forming stable molecules. Efficient optimization of molecules is challenging due to the need to exclude unstable molecules and the sensitivity of properties to small changes. Gradient-based optimization over a continuous space can be achieved by mapping to the space of molecules. Initial approaches involved training a variational autoencoder on SMILES string representations to facilitate Bayesian optimization of molecular properties. The process involves Bayesian optimization of molecular properties within the latent space using neural networks or semi-supervised VAEs conditioning the decoder on molecular properties. Recurrent neural networks can also model SMILES strings directly without an explicit latent space. The SMILES system represents molecules as character strings by traversing a spanning tree of the molecular graph. Context-free and attribute grammars are used to constrain the decoder, increasing the validity of generated SMILES strings. Operating on the space of molecular graphs can avoid invalid strings and chemical rule violations. On the space of molecular graphs, molecules are represented by SMILES strings, with distances between different strings varying. Generative models of SMILES strings can complicate mapping to molecular properties. Sequence-to-sequence transcoders have been trained to map between different SMILES strings of a single molecule to address this issue. Reinforcement learning combined with adversarial methods has been used to train molecule growth strategies, achieving optimization of molecular properties. However, these techniques scale poorly to real-world properties, requiring time-consuming experiments. A generative model capturing the geometry of molecular graphs, rather than SMILES strings, could improve molecular property optimization. The All SMILES VAE uses recurrent neural networks on multiple SMILES strings to efficiently infer a latent representation sensitive to molecular features. A fixed-length latent representation is distilled using attentional mechanisms, allowing for improved molecular property prediction and optimization. The All SMILES VAE utilizes recurrent neural networks to infer a latent representation linked to molecular features, enabling effective optimization. The latent space is closely related to the space of molecules and smooth with respect to properties, enhancing gradient-based property optimization. The model defines a generative model over an observed space with a prior distribution over a latent space and a conditional likelihood of observed states given the latent configuration. The evidence lower bound (ELBO) is maximized as the true loglikelihood is intractable, leading to a stochastic autoencoder. The ELBO defines a stochastic autoencoder with encoder q(z|x) and decoder p(x|z), utilizing graph convolutions for effective molecule encoding. Message passing on molecular graphs involves summing messages from adjacent nodes, with possible edge type dependencies or attention mechanisms. Gating mechanisms like in LSTM and GRU are sometimes applied to messages. Message passing on molecular graphs is similar to convolutional neural networks for images, with hidden layers and kernels. While visual networks use hundreds of layers, molecule encoders typically use three to seven rounds of message passing. In molecular graph message passing, information propagation is limited to a geodesic distance of one within the graph, which hinders effective traversal in datasets with large average and maximum diameters. Long-range information propagation is crucial for understanding non-local molecular properties in various domains like pharmaceuticals. Pharmacological efficacy depends on high binding affinity for specific targets and low affinity for others. Changes in shape or charge distribution of a molecule affect its binding affinity with receptors, inducing shifts and rotations. Efficient OLEDs rely on properties like singlet-triplet energy gap, which are linked to long-range electronic interactions. VAEs can capture non-local properties if information is efficiently passed across the molecular graph. Gated RNNs on SMILES strings effectively transmit messages through hidden states. Gated RNNs on SMILES strings transmit messages through a flattened spanning tree of the molecular graph, allowing information to propagate through all branches in a single pass. This approach differs from explicit graph-based message passing by updating messages sequentially down the chain, influencing all later messages with earlier information. With a well-chosen spanning tree, information can pass the entire width of the molecular graph in a single RNN. The All SMILES encoder combines RNNs and graph convolutions to efficiently propagate information through multiple SMILES strings of the same molecule. It harmonizes messages between parallel representations by replacing messages to a single atom with their pooled average. The All SMILES encoder combines RNNs and graph convolutions to propagate information through multiple SMILES strings of the same molecule. It harmonizes atom representations across strings and applies a stack of modules to achieve this. The All SMILES encoder harmonizes atom representations across multiple SMILES strings of the same molecule using elementwise sigmoid gating for pooling, replacing the original atom representation in each input stream for subsequent layer normalizations and GRUs. The All SMILES encoder harmonizes atom representations using elementwise sigmoid gating for pooling. The approximating posterior distills encodings into a hierarchy of autoregressive Gaussian distributions. The mean and log-variance of the first layer are parametrized by max-pooling the terminal hidden states of the final encoder GRUs. Subsequent layers use Bahdanau-style attention. The hierarchical layers utilize Bahdanau-style attention over pooled atom vectors, with a query vector defined by a one-hidden-layer network of ReLUs. This attention mechanism is also available to property regressors for aggregating contributions across molecules. The output undergoes batch renormalization and a linear transformation to compute conditional mean and log-variance. The prior has an autoregressive structure using neural networks of ReLUs instead of Bahdanau-style attention. The hierarchical layers use Bahdanau-style attention over pooled atom vectors, with a query vector defined by a one-hidden-layer network of ReLUs. The decoder is a single-layer LSTM, with the initial cell state conditioned on the previous latent samples. For molecular optimization tasks, the KL-term in the ELBO is scaled up by the number of SMILES strings in the decoder. The approximating posterior consists of an autoregressive set of Gaussian distributions, with subsets of latent variables produced via Bahdanau-style attention. The decoder in the neural network uses a single-layer LSTM and is trained with teacher forcing to reconstruct SMILES strings representing the same molecule. It autoregressively produces a sequence of categorical distributions for each SMILES character, decoding multiple SMILES in parallel with shared parameters. The decoder becomes more certain about the string as decoding proceeds. The All SMILES VAE is a generative model over molecules' structure and properties. The decoder uses a single-layer LSTM and is trained with teacher forcing to reconstruct SMILES strings. It becomes more certain about the string as decoding proceeds. Grammatical constraints can be enforced by parsing the character sequence with a pushdown automaton. The representation of the molecule input to the encoder is a set of SMILES strings disjoint from the decoding target. The decoder in the All SMILES VAE model reconstructs SMILES strings using a single-layer LSTM and teacher forcing. The encoder input is a set of SMILES strings different from the decoder target. The conditional log likelihood of molecular properties is scaled by \u03bb, and the KL term is scaled by the number of SMILES strings in the decoder. Regressors and classifiers in p(\u03c1|z) are parametrized by a linear layer with an activation function. The All SMILES VAE model uses a width-5 beam search decoder to reconstruct molecules from a latent representation. It employs a set of randomly selected SMILES strings for encoding and decoding targets. The computational complexity is O(M \u00b7 b) per layer, where M = 5 is the number of random SMILES. The computational complexity of each layer in the All SMILES encoder is O(M \u00b7 b), where M = 5 is the number of random SMILES strings used. On the other hand, graph convolutions require a number of layers proportional to the graph diameter, with a complexity of O(b^2) to pass information through all molecules. Unlike RNNs, graph convolutions have a fixed architecture for all molecules. The All SMILES VAE jointly trains property regressors with the generative model, using linear regressors for logP and MW, logistic regressors for QED and toxicity measures. Gradient-based optimization is performed on the latent space to produce an optimized molecule. Optimizing predicted property without constraints on latent space involves finding the maximum a posteriori (MAP) latent point for a conditional likelihood. Property regressors and decoder accuracy is limited to the trained domain. Gaussian prior optimization pulls towards the origin, while unconstrained optimization diverges from it. By using the reparametrization trick, we can map the autoregressive prior back to a standard Gaussian, constraining optimization to a spherical shell to reduce prediction errors and improve optimization. The hierarchical radius constraint of the thin spherical shell image is a nonlinear function of previous layers. Evaluation of the All SMILES VAE is done on subsets of the ZINC database and the Tox21 dataset. Additional experiments and ablations of novel model components are described in the appendices. Optimization in molecular properties involves a bijection between molecules and regions in a latent space, with a regressor that is differentiable almost everywhere. Confirming this bijection is challenging due to difficulties in finding the full latent space preimage of a molecule. Mapping from molecules to latent space and back is necessary for injectivity and surjectivity. The encoder uses an approximating posterior, selecting the mean of each conditional Gaussian distribution and employing beam search. Using a beam search decoder, 87.4% \u00b1 1% of a held-out test set of ZINC250k is accurately reconstructed. 98.5% \u00b1 0.1% of samples from the prior decode to valid SMILES strings. All molecules decoded from 50,000 independent samples were unique and 99.958% were novel. The average synthetic accessibility score was 2.97 \u00b1 0.01. The goal is to optimize molecules for physical properties like fluorescence quantum yield, binding affinity, and low toxicity. The All SMILES VAE significantly improves semi-supervised prediction of molecular properties like logP, MW, and QED, which are crucial for drug efficacy. This is achieved by training networks on known physical properties and conducting experiments on a small number of molecules. The All SMILES VAE improves property prediction and toxicity on various datasets, surpassing state-of-the-art methods without ensembling or expert feature engineering. The All SMILES VAE enhances property prediction by optimizing in the latent space and projecting back to a molecule representation. It outperforms existing methods without ensembling or expert feature engineering. The All SMILES VAE improves property prediction by optimizing in the latent space and projecting back to a molecule representation. It surpasses previous methods without ensembling or expert feature engineering. Table 2 compares top values from trajectories, while Table 3 shows ablations of model components affecting mean absolute error (MAE) of logP and QED predictions. The ablation study investigates the impact of atom harmonization on toxicity prediction, highlighting its importance for nonlinear properties of molecules. The ONE SMILES ENC approach feeds a single SMILES string to the encoder, while still reconstructing multiple outputs. The ablation study explores the impact of atom harmonization on toxicity prediction. ONE SMILES ENC/DEC reduces the decoder set to one, encoding and decoding a shared SMILES string. This approach allows the latent representation to encode specific SMILES details, leading to a decrease in performance. In NO POSTERIOR HIERARCHY, latent variables are moved to the first layer of the hierarchy with a standard Gaussian prior. The All SMILES encoder uses stacked RNNs on multiple SMILES strings for efficient information passing in molecular graphs. Attentional mechanisms in the approximating posterior summarize features into a fixed-length representation for property prediction. Regressors achieve state-of-the-art property prediction through gradient-based optimization of latent representations, coupled with a simple RNN decoder. The All SMILES VAE is trained on the ZINC250k dataset for molecular property optimization and fully supervised prediction. For semi-supervised property prediction, the ZINC310k dataset is used. The ZINC310k dataset consists of 310,000 organic molecules with 6 to 38 heavy atoms. The dataset is curated from ZINC15 and can be accessed from a specific GitHub link. The molecular diameters in the dataset range from 11.1 to 24, with graph convolution typically using three to seven rounds of message passing. For semi-supervised toxicity prediction, the Tox21 dataset accessed through DeepChem package is used. The Tox21 dataset accessed through the DeepChem package contains binarized binding affinities against up to 12 proteins for 6264 training, 783 validation, and 784 test molecules. It includes molecules with up to 140 atoms, including metal atoms not present in standard molecular generative modeling datasets. An unsupervised dataset of 1.5 million molecules from the PubChem database was curated to address these challenges, used only for the Tox21 prediction task. The full All SMILES VAE architecture is detailed in Figure 10. The All SMILES VAE architecture in Figure 10 involves the evidence lower bound (ELBO) of log-likelihood, conditional log-likelihoods, Kullback-Leibler divergence, latent state z sampling, and decoding into SMILES strings using LSTMs. Encoder blocks receive linear embeddings of original SMILES strings. Gaussian distribution with mean \u00b5 and log-variance log \u03c3 2 is determined by a neural network. Encoder stacks have depth three with 512 hidden units in each GRU. The approximating posterior has four layers of hierarchy with 128 hidden units in the attentional query vector neural network. The one-hidden-layer neural network in the All SMILES VAE architecture computes the attentional query vector using separate GRUs for z 1 and atom representations for z >1. The single-layer LSTM decoder has 2048 hidden units and training is done with ADAM. Multiple SMILES strings are used for encoding and decoding, selected with RDkit. The ELBO term D KL [q(z|x)||p(z)] is not scaled down by the number of latent units, but separate reconstructions are included for multiple SMILES strings. For molecular optimization tasks, the KL term is scaled up by the number of SMILES strings in the decoder. The decoder in the architecture utilizes multiple single-SMILES VAEs in parallel, with unscaled KL term for property prediction. Convolutional neural networks on images leverage spatial geometry for local message passing. Recurrent neural networks like LSTMs and GRUs model one-dimensional sequences. The kernel in these networks has a width of two. Recurrent neural networks like LSTMs and GRUs have a width of two in their kernel. Information can propagate through the network efficiently in a single pass, reducing the number of layers needed. GRUs are defined by Cho et al. (2014) as a gated, weighted sum of the previous message and current input, with element-wise transformations applied. Recurrent neural networks like LSTMs and GRUs have a width of two in their kernel, allowing efficient information propagation in a single pass. LSTMs involve forget, input, and output gates, while GRUs use a gated, weighted sum of previous and current input with element-wise transformations. Message passing in graphs involves neighbors of nodes and pointwise nonlinearities. Graph convolutions involve passing messages between nodes using non-linear functions like logistic or rectified linear units. Different approaches include normalizing messages by the square root of node degrees, maintaining separate messages for nodes and edges, and adding gating mechanisms similar to GRUs. LSTMs can capture some aspects of message passing in graph convolutions, such as ignoring certain symbols in input sequences. The LSTM model can ignore certain symbols in input sequences, such as open and close parentheses, to ensure messages propagate along connected paths in a molecular graph. Multiple LSTMs exchanging messages can generate all messages produced by a graph convolution, with atom-based pooling combining messages from each LSTM. This allows every LSTM to access the messages produced by the graph convolution. The LSTM decoder generates SMILES strings by accessing messages from a graph convolution. The hierarchical approximating posterior defines a conditional Gaussian distribution using an attentional mechanism. The final output of the attentional mechanism is subject to batch renormalization and a linear transformation to compute the conditional mean and log-variance. The optimization in the latent space is performed by projecting each layer of latent variables onto a sphere defined by their conditional Gaussian distribution, and then optimizing with respect to the angles. The radius of the sphere is determined by the prior distribution and is equal to \u221a N \u2212 1 for a standard Gaussian. The pseudocode for this process is shown in Algorithm 2. The optimization in the latent space involves projecting latent variables onto a sphere defined by their conditional Gaussian distribution and optimizing with respect to angles. A hyperparameter is added to the objective function to ensure optimization in well-trained regions of latent space. Additionally, a hard tanh function is applied to latent variables before the regressor to moderate linearity. 1000 random samples from the prior are optimized to convergence, with the last point from each trajectory used for comparison with previous work. Molecular optimization in the latent space is computationally inexpensive and robust to hyperparameters. Previous work used one consistent SMILES string for input and target in molecular variational autoencoders. Additional optimization trajectories in latent space could improve molecular optimization at minimal expense. The use of multiple SMILES strings of a single molecule as input to a RNN encoder in molecular VAEs allows for information propagation through all graph pathways in parallel, overcoming limitations of previous approaches. The All SMILES VAE extends the use of multiple SMILES strings of a molecule in a RNN encoder to learn a generative model of molecules. This approach regularizes molecular optimization and property prediction. Bjerrum & Sattarov (2018) and Winter et al. (2019a) trained transcoders for mapping between SMILES strings of the same molecule, but without a generative model or prior distributions in their latent spaces. The All SMILES VAE introduces a hierarchical radius constraint on optimization in the latent space for molecular property prediction. It is compared to various state-of-the-art algorithms for molecular optimization tasks. The curr_chunk discusses various algorithms for molecular property prediction, including network implementations like GCPN, MolDQN, and DeepChem. It also compares extended connectivity fingerprints and graph convolutions for input to machine learning techniques. Toxicity prediction is compared to PotentialNet, ToxicBlend, and other results. The All SMILES VAE is trained on a challenging task of optimizing molecules via the latent space, ensuring that the latent representation captures the molecule rather than its specific SMILES encoding. This requires the decoder LSTM to produce a complex, highly multimodal distribution over SMILES strings. The All SMILES VAE uses a decoder LSTM to generate a diverse distribution of SMILES strings, making decoding challenging. Beam search is employed to find the most probable SMILES string. This approach simplifies optimization in the latent space but differs from prior work using stochastic encoders and decoders. Sparse Gaussian processes are used in molecular variational autoencoders to model molecular properties based on fixed latent representations. These parametric regressors are more powerful than linear regressors but perform poorly when not trained jointly with the generative model. The study compares the accuracy of property prediction using VAE with less powerful regressors. Results show significant improvement in accuracy compared to sparse Gaussian process regression. The mean absolute error is three times smaller for logP and MW predictions. Visual demonstration in Figure 12 illustrates the accuracy of property prediction in a 2D latent space. The Tox21 dataset includes assessments for nuclear receptor signaling pathways and stress response pathways. AUC-ROC values are reported for each assay independently in Table 6, with the average AUC-ROC reported in Table 1. The study does not include results from a previous evaluation. In Figure 13, an optimization trajectory for drug-likeness estimate (QED) is presented. The loss function in the ELBO of the All SMILES VAE is scaled by the number of SMILES strings in the decoder, allowing for message passing between encoders. Leaving the KL term unscaled results in less regularization in latent space embeddings, leading to a wider search space for molecules. Figure 14 demonstrates that optimization for penalized log P yields long aliphatic chains. In contrast to Table 2, optimization for penalized log P alone without a log prior regularizer yields better results, showing very long aliphatic chains with high penalized log P values. The subset of the SMILES grammar captured by previous studies is equivalent to a context-free grammar and does not include the ability to represent multiple disconnected molecules in a single SMILES string or wildcard atoms. The SMILES grammar is represented by a context-free grammar, with unique defining symbols for productions. Chiral productions can be resolved by parsing characters up to the next production. The true production is uniquely identified by the next symbols when there is a choice between productions. The SMILES grammar is enforced with a pushdown automaton running in parallel with the decoder RNN, tracking progress within the representation of each atom and the sequence of atoms and bonds. The decoder RNN's output symbols are restricted to those consistent with the pushdown automaton's current state. Symbols like ( and [ are pushed onto the stack when emitted and must be popped to emit ) or ]. The SMILES notation uses brackets to represent atoms with optional information like isotope number, chiral symbol, hydrogen count, charge, and class. Parsing branched atoms involves tracking ringbonds and branches using a pushdown automaton. The SMILES notation uses brackets to represent atoms with optional information. Parsing branched atoms involves tracking ringbonds and branches using a pushdown automaton. Only a branched atom can emit a branch, and only branch produces the symbol ). Ringbonds are constrained to come in pairs with the same bond label on both sides. Semantic restrictions of ringbond matching and valence shell constraints can be enforced during feedforward production of a SMILES string using a pushdown stack and a small random access memory. The SMILES notation involves tracking ringbonds and branches using a pushdown automaton. Ringbonds must match and valence shell constraints are enforced during SMILES string production. The molecule terminates when all ringbond bits are off. The decoder can receive input on open ringbonds to preferentially close them. The set of nested atomic contexts can be deep, corresponding to branching in the SMILES string's spanning tree. Once a branch is entered, it must be traversed entirely before the SMILES string continues. When processing a molecule in SMILES notation, each atom is described by a branched atom and its preceding bond. Branches must be traversed completely before returning to the parent atom. Valence shell information is pushed onto the stack for each atom encountered, updating the count with each new bond observed. When processing a molecule in SMILES notation, valence shell information is updated for each atom by pushing on the updated count. Remaining bonds are filled by implicit hydrogen atoms, and the number of available bonds is provided as input to the decoder RNN. Graph-based representations have been suggested to avoid degeneracy in SMILES strings. Graph-based representations can help avoid degeneracy in the decoder when processing molecules in SMILES notation. Various methods imply an ordering among nodes and edges, but canonical orderings can make generative modeling more challenging. Graph matching procedures can ensure correct molecule assignment, but do not eliminate decoder output degeneracy, leading to a highly multimodal generative loss function."
}