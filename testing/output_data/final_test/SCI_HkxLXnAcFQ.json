{
    "title": "HkxLXnAcFQ",
    "content": "Few-shot classification involves training a classifier to recognize new classes with limited labeled examples. This paper provides a comparative analysis of different few-shot classification algorithms, showing that deeper backbones can reduce performance gaps. A modified baseline method achieves competitive results compared to state-of-the-art on various datasets. Additionally, a new experimental setting evaluates cross-domain generalization ability. Results suggest that reducing intra-class variation is crucial for shallow feature backbones. In a realistic, cross-domain evaluation setting, a baseline method with standard fine-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms. Deep learning models rely heavily on abundant labeled instances for visual recognition tasks, limiting applicability due to human annotation cost and data scarcity in some classes. Human visual systems can recognize new classes with very few labeled examples, contrasting with current vision systems. Few-shot classification, a problem of learning to generalize to unseen classes with limited labeled examples, has attracted considerable attention. Meta-learning is a promising direction for few-shot classification, where transferable knowledge is extracted and propagated from tasks to improve generalization. Different methods include model initialization, metric learning, and hallucination-based approaches. Another approach involves directly predicting classifier weights for novel classes, showing promising results. In this paper, a detailed empirical study is presented to address challenges in few-shot classification algorithms. The implementation details among algorithms obscure performance comparisons, and evaluations lack domain shift between base and novel classes, making scenarios unrealistic. The study aims to shed new light on the few-shot classification problem by conducting consistent comparative experiments. In a detailed empirical study on few-shot classification algorithms, consistent comparative experiments were conducted to compare different methods. Results showed that using a deep backbone narrowed the performance gap between methods with limited domain differences. Replacing the linear classifier with a distance-based classifier made the baseline method competitive with current state-of-art meta-learning algorithms. Introducing a practical evaluation setting with domain shift between base and novel classes showed that sophisticated few-shot learning algorithms did not outperform the baseline. Source code and model implementations were made available for consistent evaluation. Our contributions include providing a unified testbed for few-shot classification algorithms, showing that a shallow backbone leads to favorable results for methods reducing intra-class variation, and demonstrating competitive performance of a distance-based classifier compared to state-of-the-art meta-learning methods. Additionally, we investigate a practical evaluation setting with domain differences between base and novel classes. Few-shot classification algorithms struggle with domain shifts, emphasizing the need to adapt to domain differences. These algorithms aim to recognize new classes with limited labeled examples. Various approaches, such as initialization-based, metric learning-based, and hallucination-based methods, have been developed to improve data efficiency in few-shot learning. Initialization-based methods focus on learning optimal model initialization to enable learning classifiers for novel classes with minimal labeled examples. Distance metric learning based methods address few-shot classification by \"learning to compare\" to determine similarity between images for classification of unseen inputs. Meta-learning methods condition their predictions on sophisticated comparison models. Distance metric learning methods compare images for classification in few-shot scenarios. Three methods were compared, with a simple baseline method showing competitive performance compared to more complex algorithms. Other methods like BID10 and BID24 also developed similar approaches to the baseline method. Our focus is on reducing intra-class variation in a baseline method using base class data to achieve competitive performance. Hallucination-based methods address data deficiency by learning to augment, either by transferring appearance variations from base classes or using GAN models to transfer style. Another approach integrates a generator into a meta-learning algorithm for data augmentation. The curr_chunk discusses the impact of domain adaptation techniques on few-shot classification algorithms, specifically focusing on the domain difference and the limited amount of data available in the new domain. The study excludes hallucination-based methods and aims to improve classification accuracy by integrating a generator into a meta-learning algorithm. The curr_chunk discusses the limitations of existing few-shot classification algorithms in handling domain shift and compares different problem settings. It outlines the baseline model, its variant, and representative meta-learning algorithms studied in experiments for training classifiers for novel classes with few labeled examples. The curr_chunk describes the training procedure for a baseline model and its variant, Baseline++, for few-shot classification. It involves training a feature extractor and classifier from scratch, followed by fine-tuning to recognize novel classes using few labeled examples. The model aims to reduce intra-class variation. The Baseline++ model aims to reduce intra-class variation among features during training for few-shot classification. It uses a different classifier design compared to the original Baseline model, with a weight matrix Wb in the training stage and Wn in the fine-tuning stage. The weight matrix Wb consists of d-dimensional weight vectors for each class, and cosine similarity is computed to obtain similarity scores. The Baseline++ model utilizes weight vectors for each class to compute similarity scores and predict probabilities using a softmax function. The classifier is based on cosine distance between input features and learned prototypes, reducing intra-class variations. This distance-based classification approach prevents weight vectors from collapsing to zeros and has been extensively studied in the field. Meta-learning methods have been extensively studied and recently revisited in the few-shot classification setting. Three distance metric learning based methods are considered, including MatchingNet and ProtoNet. Meta-learning involves training a classification model to minimize prediction loss on a small support set. The algorithms consist of a meta-training and meta-testing stage, where N classes are randomly selected, and small support and query sets are sampled. The goal is to learn to learn from the support set. In meta-learning, a classification model is trained to minimize prediction loss on a small support set. Different methods, like MatchingNet and ProtoNet, compare distances between query and support features to make predictions. The model is adapted in the meta-testing stage to predict novel classes with new support sets. In meta-learning, various methods like MatchingNet, ProtoNet, RelationNet, and MAML are used for few-shot classification. These methods compare distances between query and support features to make predictions and adapt the model in the meta-testing stage. Different scenarios such as generic object recognition and fine-grained image classification are addressed in this context. For object recognition, mini-ImageNet dataset with 100 classes and 600 images per class is used. For fine-grained classification, CUB dataset with 200 classes and 11,788 images is utilized. In the cross-domain scenario, mini-ImageNet is the base class. In the cross-domain scenario, mini-ImageNet is used as the base class for evaluating domain shifts in few-shot classification approaches. Training details include 400 epochs with a batch size of 16 for Baseline and Baseline++ methods. Meta-learning methods involve training 60,000 episodes for 1-shot and 40,000 episodes for 5-shot tasks. Each episode consists of N classes for N-way classification, with k labeled instances for support and 16 instances for query sets in a k-shot task. Results are averaged over 600 experiments in the fine-tuning or meta-testing stage. In 600 experiments, 5 classes are randomly sampled from novel classes, with k instances for support and 16 for query sets. Baseline methods train a new classifier with the entire support set, while meta-learning methods condition the classification model on the support set. All methods are trained from scratch using the Adam optimizer with data augmentation. Adjustments are made for Baseline++ and MatchingNet methods. In experiments on few-shot classification, different approaches are used for MatchingNet, RelationNet, and MAML to optimize training efficiency. The experiments focus on 1-shot and 5-shot classification with a Conv-4 backbone and 5-way classification for novel classes during fine-tuning or meta-testing. In validating our few-shot classification implementation on the mini-ImageNet dataset, our re-implementation of meta-learning methods closely matches reported performance, with minor differences attributed to our implementation. Our re-implementation of few-shot classification methods closely matches reported performance on the mini-ImageNet dataset. Minor discrepancies are attributed to different random seeds and implementation differences. Data augmentation was not applied in the Baseline approach, while ProtoNet used 30-way classification in 1-shot and 20-way in 5-shot during meta-training. Modifications were made to ensure fair comparison among methods, including using the same optimizer. Our implementation also improved the performance of some methods, such as significantly enhancing the Baseline approach under the 5-shot setting. Our re-implementation of few-shot classification methods closely matches reported performance on the mini-ImageNet dataset. Minor discrepancies are attributed to different random seeds and implementation differences. Data augmentation was not applied in the Baseline approach, while ProtoNet used 30-way classification in 1-shot and 20-way in 5-shot during meta-training. Modifications were made to ensure fair comparison among methods, including using the same optimizer. Our implementation also improved the performance of some methods, such as significantly enhancing the Baseline approach under the 5-shot setting. The Baseline method's performance is severely underestimated due to the lack of data augmentation in previous implementations, leading to over-fitting. Our Baseline with augmentation shows improvement and could perform even better with matching reported statistics. Adjustments to the input score in MatchingNet's softmax layer improved results, while using a deeper backbone resolved issues in ProtoNet's performance. After validating our re-implementation and reporting accuracy, Baseline++ significantly improves performance and competes with other meta-learning methods by reducing intra-class variation. Experimentation with deeper backbones shows potential for further enhancement in few-shot classification. Experimentation with deeper backbones, such as Conv-6, ResNet-10, 18, and 34, shows potential for enhancing few-shot classification. Results on the CUB dataset indicate that as the backbone gets deeper, the gap among different methods decreases. ProtoNet also improves rapidly with deeper backbones. In the CUB dataset, deeper backbones reduce the gap among methods, improving ProtoNet accuracy. However, mini-ImageNet results show a more complex scenario, with some metalearning methods performing worse with deeper backbones. Domain differences between base and novel classes impact few-shot classification results. In the context of domain differences between base and novel classes impacting few-shot classification results, experiments were conducted with a ResNet-18 feature backbone. The Baseline outperformed all meta-learning methods in a new cross-domain scenario: mini-ImageNet \u2192CUB. Meta-learning methods struggle to adapt to novel classes that are too different due to all base support sets being within the same dataset. The Baseline method outperforms meta-learning methods in adapting to novel classes with domain differences. Baseline accuracy increases with larger domain differences, highlighting the importance of adaptation based on few novel class instances. Adapting meta-learning methods involves fixing features and training a new classifier, which is applied to MatchingNet and ProtoNet. Fixing features is not feasible for MAML. The Baseline method outperforms meta-learning methods in adapting to novel classes with domain differences. Meta-learning methods like MatchingNet and ProtoNet involve fixing features and training a new classifier. However, fixing features is not feasible for MAML, which updates the model with the support set for a few iterations. Further adaptation by updating for more iterations can significantly improve the performance of MatchingNet and MAML, especially in scenarios like miniImageNet \u2192CUB. Lack of adaptation is the reason they fall behind the Baseline method. The importance of learning adaptation in the meta-training stage for future meta-learning research in few-shot classification is highlighted. The Baseline++ model is competitive under standard conditions, while the Baseline model achieves competitive performance with recent state-of-the-art meta-learning algorithms. The Baseline model achieves competitive performance with recent state-of-the-art meta-learning algorithms on CUB and mini-ImageNet datasets using a deeper feature backbone. It also outperforms all evaluated meta-learning algorithms in scenarios with domain shift between base and novel classes. The source code is made publicly available to benefit the community in addressing domain shifts in few-shot learning. The relationship between domain adaptation and few-shot classification is discussed to clarify experimental settings. Domain adaptation aims to adapt knowledge from the source dataset to the target dataset, while few-shot classification learns from base classes to classify novel ones. The goal of few-shot classification is to classify novel classes in the same dataset by learning from base classes. Recent work addresses the intersection of domain adaptation and few-shot classification, highlighting the impact of domain shift. Different meta-learning works use varying terminology, with discussions on domain shift and the use of the Omniglot dataset for character recognition. The Omniglot dataset BID17 is used for character recognition in few-shot classification algorithms. It contains 1,623 characters from 50 languages, augmented by rotations to create 6492 classes. The classes are split into base, validation, and novel classes following specific protocols. For cross-domain character recognition, Omniglot is used without Latin characters and rotation augmentation as base classes, while the EMNIST dataset contains 10 digits and alphabets in English. In few-shot classification using the Omniglot dataset, meta-learning methods outperform baseline and baseline++ in 1-shot tasks. Data augmentation is not used for Omniglot characters due to their black-and-white, center-aligned, and rotation-sensitive nature. Over-fitting is reduced in novel classes with sufficient examples, leading to comparable performance in 5-shot classification. The use of 1-NN classifier in the test stage shows better performance in 1-shot setting compared to softmax classifier, while softmax classifier performs better in 5-shot setting. Different mini-ImageNet datasets were used, making direct comparison with previous results challenging. MAML versions converge similarly in accuracy despite different convergence speeds, reflecting differences in optimization methods. Deeper network depths show decreased intra-class variation. Deeper network depths lead to decreased intra-class variation, measured using the Davies-Bouldin index. The statistics show a reduction in intra-class variation for both base and novel class features with deeper backbones. High-resolution figures in FIG1 and detailed statistics in TAB9 aid in comparison. The experiments involve 5-way meta-training in different testing scenarios. In experiments involving 5-way meta-training, the Baseline++ method outperforms other methods in larger N-way classification scenarios due to reduced intra-class variation and improved performance in both shallow and deeper backbone settings. In larger N-way classification scenarios, meta-learning algorithms may experience a drop in performance due to the increased difficulty of tasks. Addressing this issue by training with a larger N-way classification in the meta-training stage could lead to memory constraints, hindering the training of models with deeper backbones like ResNet. Baseline++ outperforms other methods in shallow or deeper backbone settings in a N-way meta-testing experiment on mini-ImageNet with 5-shot."
}