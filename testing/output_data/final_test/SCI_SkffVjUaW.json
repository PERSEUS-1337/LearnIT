{
    "title": "SkffVjUaW",
    "content": "Successful training of convolutional neural networks often involves deep architectures with high numbers of features. Regularization and pruning techniques are used to reduce redundancy. A novel bottom-up approach is introduced to expand representations in fixed-depth architectures, starting with a single feature per layer and increasing width greedily. A computationally efficient method based on feature time evolution is proposed to determine feature importance and network capacity. Automatically expanded architectures converge to similar topologies with fewer parameters or improved accuracy. Convolutional neural networks (CNNs) show improved performance with fewer parameters and increased accuracy, diverging from traditional design patterns. Estimating representational capacity in deep neural networks remains a challenge, with various approaches proposed to address network sizes and architecture complexities. Recent work includes reinforcement learning-based meta-learning for CNN layer selection and other architecture selection algorithms. In deep neural network design processes, it is suggested to consider layer types or depth selected by experienced engineers based on prior knowledge. Various techniques focus on improving established architectures through distillation, compression, pruning, and small capacity increases in transfer-learning scenarios. Recent efforts in reducing representational complexity in deep neural networks include employing different regularization terms during training and using activation magnitudes and small weight values as pruning metrics. This work proposes a bottom-up approach with a computationally efficient metric to evaluate feature importance at any point. The proposed approach introduces a computationally efficient metric to evaluate feature importance during neural network training. A bottom-up greedy algorithm is used to expand fixed-depth networks based on this metric, leading to improved performance on various datasets. The expanded architectures show increased feature counts in early to intermediate layers before decreasing in complexity. The choice and size of deep neural network models determine representational capacity and affect training accuracy. Training neural networks is complicated by optimization algorithms and model regularization, defining effective capacity. Increasing model sizes can help address training challenges but comes with higher memory and computation costs. Proposing a bottom-up approach to increase capacity in neural networks with a new metric to gauge effective capacity in training. Regularization techniques like L2-norm and L1-norm are commonly used in stochastic gradient descent algorithms to prevent overfitting and improve generalization. Methods like dropout and batch normalization are also utilized. Our approach is inspired by earlier works that measure feature importance using the L1-norm of weight tensors. We assign a single importance value to each feature based on its values, focusing on the relative amount of change a feature experiences with respect to its original state as an indicator of importance. Features undergoing high structural changes are deemed more vital. Features that undergo high structural changes are considered more important in deep neural networks. Features that are randomly initialized and do not change may be due to already optimal initialization or limitations in representational capacity, cost function, regularization, or optimization algorithm. Monitoring the effective capacity during learning can help assess feature importance. Monitoring the effective capacity during learning involves tracking the normalized cross-correlation of weights with their initialization state in neural networks. This method applies to multi-layer perceptrons and features with spatial dimensions, providing insight into feature importance based on structural changes. The metric measures feature importance by tracking structural changes in weights, with invariance to translations or rescaling. Unlike other measures, it allows for a bottom-up approach in adding features incrementally. The proposed method incrementally adds features to converge to architectures that capture task complexity without training large networks initially. Unlike other methods, it uses a fixed-depth prior and does not consider flexible depth in the network. The method incrementally adds features to converge to architectures that capture task complexity without training large networks initially. It uses a fixed-depth prior and does not consider flexible depth in the network. The algorithm initializes each layer with one feature and proceeds with mini-batch SGD, updating parameters to avoid falling into local minima. The algorithm incrementally adds features to converge to architectures that capture task complexity without training large networks initially. It uses a fixed-depth prior and avoids local minima by rapidly converging to stable solutions with minimal computational overhead. The chosen metric for architecture expansion is based on weight multiplication, requiring no gradient calculations. The algorithm adds features incrementally to capture task complexity without training large networks initially. It uses weight multiplication for architecture expansion, allowing for modular computation independently of SGD optimization. Revisiting established architectures, comparisons are made with expanded variants starting from a single feature in each layer. The algorithm incrementally adds features to capture task complexity without training large networks initially. It uses weight multiplication for architecture expansion, allowing for modular computation independently of SGD optimization. The CNN architectures are carefully chosen and tuned through extensive hyper-parameter search. The layers involving sub-sampling are decoupled to analyze their role in representational capacity. The study demonstrates how representational capacity in automatically constructed networks scales with increasing task difficulty using datasets like MNIST, CIFAR10 & 100, and ImageNet. Training is inspired by Zagoruyko & Komodakis (2016) with minimal preprocessing, focusing on trainset mean and standard deviation. The algorithm is shown to be applicable to large scale challenges like ImageNet with \"Alexnet\". The methodology involves training architectures on datasets using specific hardware, with code available in Torch7 BID3 and PyTorch. An example is provided using equation 2 for pruning to measure feature importance. Comparison is made between the normalized cross-correlation metric and other metrics like L1 weight norm. Pruning of a GFCNN is shown in FIG2, indicating network size challenges with different datasets. The study compares different metrics for pruning neural network architectures trained on MNIST and CIFAR100 datasets. The normalized cross-correlation metric is highlighted for its ability to determine feature importance without the need for individual layer threshold values. This metric shows that features are more important in CIFAR100 compared to MNIST. The study focuses on comparing metrics for pruning neural network architectures trained on MNIST and CIFAR100 datasets. It highlights the normalized cross-correlation metric for determining feature importance without individual layer threshold values. The authors emphasize the bottom-up widening of architectures and mention that pruning strategies may not be as desirable due to various limitations and complexities involved. The study emphasizes expanding neural network architectures from low to high representational capacity by adding features gradually. The training procedure involves algorithm 1 to increase complexity. Architecture expansion settings include adding features one at a time or in stacks for speed-ups. The study also explores the impact of late re-initialization suppression and stability parameters to end network expansion. The study focuses on gradually expanding neural network architectures by adding features, with settings for ending expansion if stability is reached. Experiments with MNIST and CIFAR10 & 100 datasets show independent layer expansion and allocation of more features for CIFAR100. All architectures converge to a similar parameter count but at different times, as seen in the evolution of parameters over five experiments. The study explores expanding neural network architectures gradually by adding features, with settings for ending expansion if stability is reached. Experiments with MNIST and CIFAR10 & 100 datasets show independent layer expansion and allocation of more features for CIFAR100. Architectures converge to similar parameter counts but at different times, with a trend of increasing network capacity with dataset complexity. The study discusses the expansion of neural network architectures by adding features gradually. It mentions the time efficiency of algorithm 1 compared to manual search methods. Hardware memory limitations affect the expansion of WRN CIFAR100 architecture. Increasing layer width in shallow GFCNN architectures can improve accuracy, but there are limits, especially with heavy regularization. This aligns with findings from other works like Ba & Caurana (2014) and BID30. The expanded alternate architecture outperforms large reference models like VGG-E and WRN-28-10 on CIFAR and MNIST datasets. The unconventional network topology of the expanded architectures plays a key role in their success. The study does not include experiments with excessive preprocessing, data augmentation, or oscillating learning rates, but still achieves accuracies rivaling state-of-the-art performances. The expanded architectures, evolved from an expansion algorithm, show systematic variations in representational capacity with dataset difficulty. Topologies differ from reference architectures and achieve accuracies rivaling state-of-the-art performances. The expanded architectures show systematic variations in representational capacity with dataset difficulty, with topological convergence and high feature dimensionality in early to intermediate layers. Accuracy is improved through topological re-arrangement, even with minimal deviation from the reference parameter count. Pooling replaced with larger stride convolutions leads to independent changes in sub-sampling layer dimensionality, suggesting complex sub-sampling operations are learned. This is an extension to the proposed all-convolutional variant of BID28. Evolved network topologies with higher feature amounts in early to intermediate layers map data into higher dimensional space to separate it into clusters, allowing for easier aggregation of specific features to distinguish class subsets. This behavior is confirmed in all visualized evolved network topologies. The evolved network topologies, visualized in the appendix, show similar formation in trained residual networks. An interesting question for future research is whether plainly stacked architectures can perform similarly to residual networks with differing feature dimensionality arrangements. Two experiments on the ImageNet dataset using an all-convolutional Alexnet demonstrate the methodology's applicability to large scale. Results are shown in table 2, with expanded architectures in the appendix, indicating benefits from topological rearrangement. In this work, a novel bottom-up algorithm is introduced to expand neural network architectures by starting with one feature per layer and widening them until a suitable representational capacity is achieved. The algorithm aims to increase the efficiency and intuitiveness of evaluating feature importance in architectures. The study acknowledges the limitations in evaluating more complex architectures like deep VGG and residual networks due to resource constraints. The study introduces a novel algorithm to expand neural network architectures by increasing the number of representations in early to intermediate layers. Future work includes re-evaluating deep architectures with new insights on network topologies and feature initialization. The algorithm aims to improve efficiency in evaluating feature importance. The datasets used for training include MNIST, CIFAR10 & 100, and ImageNet. Training is inspired by Zagoruyko & Komodakis (2016) with minimal preprocessing. All data is preprocessed using trainset mean and standard deviation, and crossentropy is used as the loss function. The training process includes using crossentropy as the loss function, weight initialization following a normal distribution, batch-normalization, specific parameters for batch-size, weight-decay, momentum, and nesterov momentum. Different initial learning rates are used for CIFAR and MNIST datasets. CIFAR10 & 100 are trained for 200 epochs with a scheduled learning rate reduction, while MNIST is trained for 60 epochs with a single learning rate reduction. Data augmentation techniques are applied to CIFAR10 & 100 training. During training, random horizontal flips are applied to the data. The image is rescaled to 224x224 and a centered crop is taken. Preprocessing involves subtraction and division of trainset mean and standard deviation. The learning rate starts at 0.1 and decreases by a factor of 0.1 every 30 epochs, with a total of 74 epochs for training. Architecture expansion requires more epochs due to re-initialization. The architecture GFCNN BID5 consists of a three convolution layer network with larger filters and two fully-connected layers. The VGG BID27 network includes \"VGG-A\" and \"VGG-E\" architectures with three fully-connected layers. The number of features in the MLP is set to 512 per layer instead of 4096. Batch normalization is used before activation functions. WRN is a Wide Residual Network architecture with a depth of 28 convolutional layers. The network architecture utilizes a depth of 28 convolutional layers with a width-factor of 10. The fully-connected layers are replaced with convolutional layers for improved performance based on previous experiments and pruning results. The distinction between representational and effective capacity of deep neural network models is highlighted. The neural network's capability to fit data depends on optimization and regularization choices. An expanded GFCNN-all-conv architecture on CIFAR100 dataset shows improved loss and training accuracy, with slight benefits in validation accuracy. The expanded GFCNN-all-conv architecture on the CIFAR100 dataset addresses under-fitting caused by heavy regularization, increasing layer width and parameters. This leads to improved loss and training accuracy, with only a slight increase in validation accuracy. The algorithm aims to address under-fitting caused by heavy regularization by adjusting the initialization of new features during training in the expanded GFCNN-all-conv architecture on the CIFAR100 dataset. This helps prevent features from becoming obsolete or unused. The algorithm adjusts the initialization of new features during training in the expanded GFCNN-all-conv architecture on the CIFAR100 dataset to prevent under-fitting caused by heavy regularization. This ensures that newly added features align with already learned features, preventing perturbations to the classifier. The evolved architectures in the study feature topologies with large dimensionality in early to intermediate layers, unlike conventional CNN designs. Pooling is replaced with larger stride convolutions, leading to changes in the dimensionality of layers with sub-sampling. Various network architectures, including shallow, VGG-A, wide residual 28 layer, and expanded Alexnet, are presented with their respective all-convolutional variants. The study presents evolved architectures with large dimensionality in early to intermediate layers, replacing pooling with larger stride convolutions. The dimensionality of layers with sub-sampling changes independently, suggesting complex pooling operations are learned. This is an extension to the proposed all-convolutional variant of BID28."
}