{
    "title": "HkOhuyA6-",
    "content": "Graph classification is currently dominated by graph kernels, but Convolutional Neural Networks (CNNs) offer a promising alternative. Many extensions of CNNs have been proposed to process graphs. In this paper, a novel method is introduced to represent graphs as multi-channel image-like structures for handling them with vanilla 2D CNNs. This approach outperforms state-of-the-art graph kernels and graph CNNs on some datasets and is more time-efficient. Graph classification is a key task in machine learning and artificial intelligence, with applications in bioinformatics, NLP, and social network analysis. Graph mining, particularly graph classification, is an active area of research. The current state-of-the-art in graph classification is dominated by graph kernels, but Convolutional Neural Networks (CNNs) offer a promising alternative. A novel method introduced in this study represents graphs as multi-channel image-like structures for processing with vanilla 2D CNNs, outperforming existing methods on some datasets. Graph classification is dominated by graph kernels, which compute similarity between graphs using substructures like random walks and shortest paths. However, graph kernels have limitations such as high time complexity due to the need to compute pairwise similarities between all graphs in the training set. The cost of training in graph classification increases rapidly due to the high time complexity of computing pairwise similarities between graphs. Processing large graphs can become prohibitive, and finding support vectors can pose a problem on big datasets. Graph kernels have disjoint feature and rule learning, where features are fixed and not optimized for the task. Graph kernels focus on local properties of graphs, ignoring global structure and suffering from high complexity. A simple approach is proposed to convert a graph into a multi-channel image-like structure for processing by a 2D CNN. This involves embedding nodes, compressing the space with PCA, extracting 2D slices, and computing 2D histograms to create an \"image\" representation of the graph. The graph is represented by a stack of 2D histograms, each representing a channel. The final representation of a graph is independent of its size. The method addresses graph kernel limitations by offering constant time complexity at the instance level and linear time complexity at the dataset level. Node embeddings can be obtained in linear time. Features are learned directly from raw data during training using a 2D CNN classifier. Our approach utilizes state-of-the-art graph node embedding techniques to capture local and global properties of graphs, eliminating the need for handcrafted features. Convolutional Neural Networks (CNNs) are specifically designed for regular grids, allowing them to compose higher-level features from lower-level features in a hierarchical way. Regular grids in CNNs enable the capture of patterns of increasing complexity. The adjacency and Laplacian matrices of a graph are not spatially correlated like pixels in images, making them unsuitable for input to a 2D CNN. Graph node embeddings are used to address the lack of spatial dependence in adjacency and Laplacian matrices of graphs. Euclidean distance in the embedding space reflects node similarity. Alignment and compression with PCA help address stochastic nature of node embedding techniques. To ensure comparability of graph embeddings, PCA is applied to retain the first d principal components, serving a compression purpose. 2D histograms are computed by extracting slices from the PCA space and discretizing them into grids with node counts in each bin. The graph is represented as a stack of 2D histograms of compressed node embeddings, with each bin counting the nodes in that bin. The histograms are computed from different PCA dimensions, ensuring channels are sorted by information content. Each bin is like a pixel associated with a vector of size d/2, representing nodes in that bin in the embedding space. The experimental setup involved implementing a variant of LeNet-5 for the MNIST dataset, achieving 99.45% accuracy. The architecture includes four convolutional-pooling layers in parallel with region sizes of 3, 4, 5, and 6, followed by two fully-connected layers. Dropout regularization is used, and ReLU activations are employed, except for the final layer which uses softmax for outputting class probabilities. The study utilized node2vec BID14 with 64 filters in the convolutional block, increasing to 96 filters after max pooling. Experiments were conducted on datasets like REDDIT-B and IMDB-B to predict class labels in unweighted, undirected graphs. In the REDDIT and COLLAB datasets, there are varying levels of class imbalance, with some datasets having a maximum ratio of 1:5 and 1:3.4. Graphs with less than 10 nodes were excluded due to the requirement of a 10-dimensional embedding space for 5 channels. The REDDIT datasets consist of graphs representing threads where nodes are users connected by edges based on responses to comments. REDDIT-B distinguishes between Q&A and discussion communities, while REDDIT-5K and REDDIT-12K contain graphs from 5 and 11 topic-specific forums. COLLAB graphs are hop-1 neighborhoods of researchers in a scientific collaboration network. In COLLAB and IMDB-B datasets, graphs represent hop-1 neighborhoods of researchers and actors linked by collaborations. Graphs are labeled by subfields of Physics or movie genres. The model was compared to graph kernels BID29 and BID30, which compute graph similarity based on count vectors of subgraphs. The WL kernel framework operates on top of any graph kernel, updating vertex labels based on neighbors to boost performance. Graphs with unlabeled nodes use node degrees as labels. The WL framework with subtree graph kernel BID12 is efficient, along with a C-SVM classifier. In experiments involving spectral embeddings, node coordinates were limited to the range of [-1, 1]. A 2D CNN architecture was tested on MNIST images, leading to the decision to learn 2D histograms with 28 bins in each direction, resulting in a resolution of 14 pixels per unit. In experiments involving spectral embeddings, node coordinates were limited to the range of [-1, 1]. A resolution of 14 pixels per unit was used for 2D histograms with 28 bins in each direction. The impact of resolution and number of channels was explored on a coarse grid, with optimal values summarized in Table 2. Parameters p and q of node2vec were then tuned for the best results. The parameters p and q of node2vec bias random walks in exploring larger graph areas or local neighborhoods, encoding similarity between nodes. Our approach outperforms all baselines on REDDIT-12K and REDDIT-B datasets, with significant accuracy improvements. Our approach outperforms baselines on various datasets, with significant accuracy improvements. It also shows superior time complexity compared to graph kernels. The 2D CNN approach offers constant processing time for training examples, making it more efficient than graph kernels which depend on graph size. Node2vec scales linearly with the number of nodes, making it usable on big graphs. Additionally, the 2D CNN method has linear time complexity for training sets, while graph kernels have quadratic complexity. This results in superior time efficiency on large datasets. The outstanding performance of Convolutional Neural Networks (CNNs) in computer vision has led to research efforts in generalizing CNNs to graphs. CNNs offer a more efficient alternative to kernel-based methods due to weight sharing, constant time complexity for each training example, and linear complexity with respect to dataset size. Processing large datasets with graph kernels would be intractable compared to the efficiency of CNNs. Generalizing CNNs to graphs is challenging due to the irregular nature of graphs. Possible solutions include spatial and spectral techniques. Spectral approaches use the convolution theorem for graph convolutions in the Fourier domain, while spatial methods operate directly on the graph. In contrast to spectral methods, spatial methods like BID24 and BID33 operate directly on the graph structure. In BID24, neighborhood graphs are created for a sequence of nodes and normalized to serve as receptive fields. A 1D CNN architecture is then applied to these fields. Our paper introduces a novel graph representation that allows graphs to be processed by vanilla 2D CNN architectures, encoding graphs as stacks of 2D histograms of node embeddings. Our method encodes graphs as stacks of 2D histograms of node embeddings, outperforming graph CNN baselines with simplicity and accuracy. Maintaining absolute count values is crucial for performance, as size information influences category associations. Increasing channels beyond 5 does not improve results. The study introduces a novel method for representing graphs as multi-channel image-like structures using node embeddings, allowing them to be processed by 2D CNNs. The method is flexible and can be extended to various types of graphs and node/edge attributes. The independence of the approach from the image classification model used is highlighted as an advantage. The study introduces a novel method for representing graphs as multi-channel image-like structures using node embeddings, allowing them to be processed by 2D CNNs. The classification model used is another advantage, with the potential for even better results with more recent models. Performance is expected to improve as graph node embedding algorithms and CNN architectures for images advance in the future. Efforts may be required to find the right combination of parameters for optimal results on specific datasets. The study demonstrates that CNN architectures designed for images can be applied to graph processing by representing graphs as stacks of two-dimensional histograms of node embeddings. Results show that this approach is competitive with state-of-the-art graph kernels and graph CNN models, sometimes even outperforming them. The method is efficient in terms of time complexity, allowing for processing larger datasets with bigger graphs."
}