{
    "title": "HyNA5iRcFQ",
    "content": "In this work, the researchers investigate whether a well-trained discrete-space neural network seq2seq model can generate malicious outputs and how to efficiently find input sequences that trigger such outputs. They use an empirical methodology and a discrete optimization algorithm to identify trigger inputs for dialogue response generation models trained on real-world datasets. The researchers investigate if a well-trained seq2seq model can produce malicious responses. Adversarial attacks on deep neural networks have serious implications for model security and have led to the development of more robust training procedures. Adversarial attacks on deep neural networks, particularly in NLP tasks, require heuristics like local search and projected gradient to keep the input valid. Text classification and seq2seq models have been shown to be vulnerable to such attacks, where adversarial examples maintain the original input's semantic meaning but lead to incorrect outputs. This work explores the possibility of a well-trained seq2seq model producing malicious responses. The curr_chunk discusses the potential for a well-trained seq2seq model to generate malicious responses, termed as egregious outputs. The study aims to determine if specific input sequences can trigger such behavior, highlighting the unpredictability of deep learning models and their susceptibility to adversarial attacks. The study focuses on identifying egregious outputs in seq2seq models, emphasizing the unpredictability and susceptibility to adversarial attacks. The research explores differences in generating unacceptable responses and does not require input sequences to be similar to training data. The framework can be applied to various discrete-space seq2seq tasks using RNN-based encoder-decoder models. The study examines seq2seq models' vulnerability to adversarial attacks and unpredictability in generating unacceptable responses. It explores differences in outputs without the need for similar training data. The framework applies to discrete-space seq2seq tasks using RNN-based encoder-decoder models, with a focus on context vector construction and latent representations. The study focuses on context vector construction in seq2seq models, using the last latent vector in the encoder's outputs and attention mask vectors to construct the context vector. This context vector is then concatenated with the embedding vector of the current word and fed into a decoder LSTM language model for prediction. Training involves minimizing the negative log-likelihood of the reference target sentence using maximum-likelihood training with stochastic gradient descent. The study focuses on context vector construction in seq2seq models, using the last latent vector in the encoder's outputs and attention mask vectors. The negative log-likelihood (NLL) of the reference target sentence is minimized during training, with two popular ways of decoding considered: greedy decoding and sampling. Greedy decoding provides stable outputs, while sampling is used in dialogue response generation. In this section, the focus is on the model's greedy decoding behavior. The study conducts two preliminary explorations: optimization on a continuous relaxation of the discrete input space and brute-force enumeration on a synthetic seq2seq task. The continuous relaxation experiment provides insights into algorithm design for discrete optimization, while experiments on brute-force enumeration are deferred to Appendix B. The study explores a relaxation of the problem by regarding the input space of the seq2seq model as continuous and finding sequences that generate egregious outputs using the Ubuntu conversational data. The study focuses on training a seq2seq attention model using conversational data to generate malicious responses. A list of 1000 malicious response sentences and 500 normal responses are used as target sequences. The model is optimized using SGD on the continuous relaxation of the input embedding or one-hot vector space. The goal is to output the target sentence with greedy decoding, while the input vector is randomly initialized. The study focuses on training a seq2seq attention model using conversational data to generate malicious responses. Optimization includes applying LASSO regularization to encourage one-hot input vectors. Results show successful hit rates after one-hot projection. The study investigates the generation of malicious responses using a seq2seq attention model trained on conversational data. Results show successful hit rates after one-hot projection, indicating the need for further investigation into the optimization process. In Section 4, a discrete optimization algorithm is designed to ensure valid updates in the discrete space. The study aims to determine if a well-trained seq2seq model can generate malicious outputs by creating lists of such outputs and using the algorithm to find input sequences that trigger them. The conditions for identifying target outputs are defined, along with the objective functions and optimization algorithm. It is shown that no input sequence in a synthetic task causes the model to generate malicious outputs during greedy decoding. The study then explores if malicious outputs are generated during sampling. The study explores the generation of malicious outputs by a well-trained seq2seq model. It questions whether egregious outputs will be produced during sampling, focusing on the average word-level log-likelihood for such outputs. A stronger type of hit, o-sample-min-hit, is defined to check the minimum word log-likelihood of the target sentence. Trigger inputs are identified as input sequences that cause the model to generate malicious outputs. The study defines three types of hits for trigger input sequences in adversarial attacks on a well-trained seq2seq model: o-greedy-hit, o-sample-avg-k(1)-hit, and o-sample-min-k(1)-hit. These hits are based on the model's generation of the target sentence with specific log-probability thresholds. In adversarial attacks on a well-trained seq2seq model, hits are defined based on log-probability thresholds for trigger input sequences. A major issue is the lack of constraint on trigger inputs, leading to ungrammatical outputs. To address this, a high LM score for trigger inputs is required, termed io-sample-min/avg-k-hit. This involves ensuring the average log-likelihood of the trigger input sequence is above a threshold T in minus log(k), with a LSTM LM trained on the same data. In adversarial attacks on seq2seq models, hits are defined based on log-probability thresholds for trigger input sequences. To address ungrammatical outputs, a high LM score for trigger inputs is required. Different hit types are addressed by modifying the objective function to ignore terms that have already met the requirements. When optimizing for different hit types in adversarial attacks on seq2seq models, the objective function is modified to focus on specific criteria. The algorithm's behavior resembles Projected Gradient Descent, with a challenge in discrete optimization. Continuous relaxation is no longer relied upon, and a local updating algorithm is proposed to find trigger input sequences for target sequences. The algorithm gibbs-enum is proposed for optimizing hit types in adversarial attacks on seq2seq models. It utilizes gradient information to narrow the search range for finding the best local input sequence. The algorithm updates every index of the input sequence in one \"sweep\" and stops if no improvement is gained. The algorithm gibbs-enum, named for its similarity to Gibbs sampling, is formulated in Algorithm 1. It initializes x * by sampling from the LM for io-hit, or uniformly for other cases. The algorithm runs 10 times with different initializations, selecting the x * with the best L(\u00b7) value. Performance analysis and parameter tuning details can be found in Appendix D. In this section, the gibbs-enum algorithm is used to detect erroneous outputs in seq2seq models for dialogue generation tasks using three conversational dialogue datasets: Ubuntu, Switchboard, and OpenSubtitles. The Ubuntu Dialogue Corpus consists of two-person conversations from Ubuntu chat logs. The seq2seq model is trained on 200k dialogues for training and 5k dialogues for testing, with a vocabulary of the 30k most frequent words. Out-of-vocabulary words are mapped to the <UNK> token. The Switchboard Dialogue Act Corpus is a collection of telephone conversations annotated with dialogue acts. 1.1k dialogues are used for training and 50 for testing. The speakers in the dialogues converse in a friendly manner. The OpenSubtitles dataset contains malicious, impolite, or aggressive sentences from movie subtitles. It includes 5k movies for training and 100 movies for testing dialogue response generation using a seq2seq model. The study focuses on generating responses in dialogue history using a seq2seq model with specific input and output sequence lengths. They test the model's ability to generate malicious sentences by creating a list of prototype malicious sentences and using heuristics to expand it. The dataset used for testing includes malicious, impolite, or aggressive sentences from movie subtitles. The study focuses on generating responses in dialogue history using a seq2seq model with specific input and output sequence lengths. The set of target sentences for Ubuntu and Switchboard are slightly different. The mal list can't be used to evaluate the algorithm due to unknown trigger inputs. A normal list for Ubuntu data is created by extracting 500 greedy decoding outputs. The o-greedyhit on the normal list is reported as a measurement of algorithm performance. Sampling during decoding is used for constructing the normal target list for Switchboard and OpenSubtitles test data due to the \"generic outputs\" problem in dialogue response generation. The study focuses on generating responses in dialogue history using a seq2seq model with specific input and output sequence lengths. The normal target list is switched to sampling during decoding, only sampling words with log-probability larger than the threshold T out. Random lists are created with 500 random sequences using the 1k most frequent words for each data-set. LSTM based LM and seq2seq models are trained with one hidden layer of size 600 and an embedding size of 300. Mini-batch size is set to 64 and SGD training is applied with a fixed starting learning rate for 10 iterations. The study focuses on training seq2seq models with specific input and output sequence lengths. The gibbs-enum algorithm is then applied to find trigger inputs for targets in different lists. High hit rates are achieved on the normal list, showing the algorithm's ability to find trigger inputs. The study demonstrates the robustness of the seq2seq model during greedy decoding, showing high hit rates on normal lists but no o-greedy-hit on malicious lists. The model sacrifices diversity by often outputting common sentences during greedy decoding. The study shows that the seq2seq model sacrifices diversity by generating common sentences during greedy decoding. It also highlights the presence of trigger inputs causing the model to produce egregious outputs, even more than \"proper\" ones. The study reveals that trigger inputs can lead to model generating inappropriate outputs, surpassing \"proper\" ones. Examples show high relevance between trigger inputs and targets, with LM regularization improving grammaticality. Attention models outperform last-h models, possibly due to greater flexibility in latent vectors. Models trained on Ubuntu data exhibit higher hit rates. Models trained on Ubuntu data show higher hit rates in generating outputs that are easier to manipulate. Additionally, models trained on Ubuntu data exhibit higher hit rates in generating appropriate responses. Models trained on Ubuntu data exhibit higher hit rates in generating outputs that are easier to manipulate. The reason for this phenomenon is the higher correlation between inputs and outputs in Ubuntu data, making the models more vulnerable to manipulation on the input side. This is evidenced by a larger performance gap between language models and seq2seq models on Ubuntu data compared to Switchboard data. The frequent occurrence of the word \"kill\" in Ubuntu data contributes to the target phrase \"i will kill you\" being generated more often. The model's high probability assignment to \"i will kill you\" is due to lack of context understanding. Egregious outputs stem from model only learning \"what to say\" but not \"what not to say\". Random list has zero hit rate for both models. Decoder in seq2seq model, similar to LM, can prevent ungrammatical outputs. The seq2seq model, like a language model, can prevent ungrammatical outputs, showing robustness against arbitrary manipulation. Adversarial attacks in deep learning often focus on computer vision tasks, with defense strategies proposed to enhance model robustness. Recent interest in analyzing deep learning model robustness for NLP tasks, particularly in sentence classification and seq2seq tasks. Recent work has focused on seq2seq tasks like text summarization and machine translation. Various attack types have been studied, including adding small perturbations to text in classification tasks and testing how much the output could change in seq2seq models. The biggest challenge is discrete optimization for neural networks, as applying gradients directly on the input can make it invalid. Perturbation heuristics have been proposed to address this issue. In this work, a simple and effective algorithm called gibbs-enum is proposed for NLP adversarial attacks. The algorithm utilizes gradient information to speed up the search process. A solid testbed is provided to evaluate the algorithm's ability to find trigger inputs, which is a novel approach compared to previous works. One major challenge in NLP adversarial attacks is defining the closeness of the adversarial example to the original input, as even small edits can change the sentence's meaning significantly. Hand-crafted rules are typically used to address this challenge. The work proposes using a LM to constrain trigger inputs for detecting \"egregious outputs\" in discrete-space seq2seq models, focusing on sequences of words with malicious meanings. This approach differs from previous works that use hand-crafted rules to address the challenge of defining the closeness of adversarial examples to the original input. In this study, the focus is on identifying malicious sequences of words in dialogue response generation tasks. The research aims to determine if well-trained seq2seq models can produce egregious outputs by designing an optimization algorithm to find trigger inputs for such outputs. The findings show that popular conversational datasets can lead to a significant number of inappropriate outputs when specific trigger inputs are used. This work represents a significant advancement in addressing the issue of generating malicious content in dialogue response models. The study focuses on identifying malicious sequences in dialogue response generation using well-trained seq2seq models. It introduces an optimization algorithm to find trigger inputs that lead to inappropriate outputs. The objective function L c is formulated for the continuous relaxation of the one-hot input space in Section 3.1, aiming to make the input vector x as one-hot-like as possible through sigmoid transformation and LASSO regularization. The objective function L c encourages small values in x and a big maximum value. SGD is used to minimize L c w.r.t x. LASSO regularization is effective in making x close to a one-hot vector. Different \u03bb c values are used for normal and malicious target lists. In Table 1, the decoding output changes drastically after one-hot projection. The 2-norm difference between h enc t before and after projection is shown in FIG3. Although the difference on each x t or x emb t is small, it quickly aggregates in the encoder's output h enc t, leading to different generation behavior in the decoder. To explore a discrete-space seq2seq model's behavior, enumerating all possible input sequences is costly due to large vocabulary sizes. A synthetic character-based seq2seq task is created to study this behavior. We create a synthetic character-based seq2seq task using the Penn Treebank text data to predict the next word given the current word. To address the limited vocabulary issue, noise is added by randomly flipping characters in half of the words. Four target lists are created: normal list with 10k words, reverse list with reversed character sequences excluding duplicates, random list with 18k sequences generated by a repeating heuristic. The study involved training last-h and attention seq2seq models on a dataset with 10k normal, 7k reverse, 18k random, and 500 malicious character sequences. The vocabulary size was 33, and the maximum input sequence length was 6. Hit rates for each target list were reported, with a focus on out-of-vocabulary sequences. The study trained seq2seq models on different types of character sequences, with a focus on out-of-vocabulary outputs. The models had high hit rates on normal lists, but zero hit rates on random and malicious lists. The generated OOV outputs were similar to English words but not exact, indicating the model's behavior is close to the proper domain. The study trained seq2seq models on various character sequences, focusing on out-of-vocabulary outputs. The models showed high hit rates on regular lists but zero hit rates on random and malicious lists. Results suggest the model is robust during greedy decoding, with low substring hit rates. The study also examines the model's sampling behavior and hit types. The study focused on training seq2seq models on character sequences, particularly on out-of-vocabulary outputs. The models performed well on regular lists but poorly on random and malicious lists. Results indicate the model's robustness during greedy decoding, with low substring hit rates. The study also analyzed the model's sampling behavior and different hit types, such as \"o/io,\" \"greedy/sample,\" \"avg/min,\" and \"k1/2.\" In the study, the loss curve of objective function with different hit types on normal, malicious, and random lists for Ubuntu data was analyzed. The optimization procedure quickly converged, showing a large gap in loss between targets in different lists. Gibbs-enum was run with varying random initializations, revealing insights into the model's behavior. In experiments with the Ubuntu normal list for the last-h model, running gibbs-enum with different random initializations and enumeration try times showed complementary performance gains that quickly saturated at around 60% hit rate. Setting G to 100 effectively narrowed the search space. Auxiliary materials for experiments on real-world dialogue datasets were provided, including data samples from Ubuntu/Switchboard/OpenSubtitles Dialogue corpus and examples of creating the mal list and extending prototypes. The experiments on Ubuntu normal list showed performance gains with different random initializations and enumeration try times, reaching around 60% hit rate. Setting G to 100 narrowed the search space. The hit rates increased drastically when k was set to 2, indicating a small likelihood gap between \"proper\" and \"egregious\" outputs. The experiments on Ubuntu data showed performance gains with different random initializations and enumeration try times, reaching around 60% hit rate. Attention model is used, and trigger inputs are optimized for io-sample-min-hit. An obvious phenomenon is that the uncommon part of the target sentence is assigned with low probability, demonstrating the robustness of seq2seq models. Model's generation behavior is shown in Figure 5, with the red line representing the threshold for sample-min-hit. More trigger input/target pairs are found by the gibbs-enum algorithm for Ubuntu and Switchboard data. OpenSubtitles data has a non-zero hit rate for o-greedy-hit. The curr_chunk discusses a conversation where someone is being criticized and told to go away. It also includes a mention of finding information in Ubuntu data and a discussion about solving a problem with a GUI. The curr_chunk involves a conversation where someone is being criticized and told to go away. It also includes a mention of finding information in Ubuntu data and a discussion about solving a problem with a GUI. The conversation includes attempts to help someone and questions about killing a process. The curr_chunk involves troubleshooting a PID bug by suggesting different ways to kill a process. There is a mention of using \"kill -9\" or \"sudo kill -9 <pid>\" commands. The conversation also includes a suggestion to check Ubuntu data for solutions and a recommendation to seek help in a specific forum. The curr_chunk involves a conversation where the speaker is dismissive and unhelpful towards someone seeking Linux advice. The speaker repeatedly says \"I do not care\" and shows a lack of interest in providing assistance. The conversation ends abruptly with the speaker expressing frustration. The speaker in the curr_chunk is unhelpful and dismissive towards someone seeking Linux advice, repeatedly saying \"I do not care\" and showing a lack of interest in providing assistance. The conversation ends abruptly with the speaker expressing frustration. The speaker in the curr_chunk is unhelpful and dismissive towards someone seeking Linux advice, repeatedly saying \"I do not care\" and showing a lack of interest in providing assistance. The conversation ends abruptly with the speaker expressing frustration. The speaker's responses are vague and unhelpful, indicating a lack of willingness to assist. The speaker in the curr_chunk is hostile and insulting, repeatedly expressing hatred and wishing harm upon the listener. The conversation is filled with derogatory remarks and a lack of empathy. The speaker in the curr_chunk expresses intense dislike and hostility towards the listener, making derogatory remarks and wishing harm upon them."
}