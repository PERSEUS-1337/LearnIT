{
    "title": "H1eA7AEtvS",
    "content": "Increasing model size during pretraining of natural language representations can lead to better performance on downstream tasks. However, further increases in model size can be challenging due to memory limitations and longer training times. To address these issues, two parameter-reduction techniques are introduced to reduce memory consumption and speed up training of BERT. These methods have been shown to significantly improve scalability compared to the original BERT model. Additionally, a self-supervised loss focusing on inter-sentence coherence has been implemented, resulting in improved performance on tasks involving multi-sentence inputs. The best model achieved new state-of-the-art results on various benchmarks while having fewer parameters than BERT-large. Pre-training has led to breakthroughs in language representation learning, benefiting NLP tasks with limited training data. Machine performance on the RACE test in China has significantly improved, with models achieving up to 89.4% accuracy. The importance of large networks in achieving these results is highlighted. The importance of large networks in achieving state-of-the-art performance in NLP tasks is evident. Pre-training large models and distilling them down to smaller ones has become common practice. However, challenges such as memory limitations and training speed hinder the scalability of models with hundreds of millions or billions of parameters. Simply increasing the hidden size of models like BERT-large may not always lead to better performance. Increasing the hidden size of BERT-large can lead to worse performance, as shown in Table 1 and Fig. 1. Existing solutions to this problem include model parallelization and memory management, but they do not fully address communication overhead and model degradation. To tackle these issues, a Lite BERT (ALBERT) architecture with fewer parameters has been designed, incorporating two parameter reduction techniques to scale pre-trained models effectively. The ALBERT architecture incorporates two parameter reduction techniques to scale pre-trained models effectively. The first technique involves factorized embedding parameterization, separating hidden layer size from vocabulary embedding size. The second technique is cross-layer parameter sharing, preventing parameter growth with network depth. These techniques significantly reduce BERT's parameters without compromising performance, improving parameter-efficiency and training speed. ALBERT introduces regularization for training stability and generalization, along with a self-supervised loss for sentence-order prediction. This allows for larger configurations with fewer parameters than BERT-large, achieving better performance on GLUE, SQuAD, and RACE benchmarks. The RACE accuracy is pushed to 89.4%, GLUE benchmark to 89.4, and SQuAD 2.0 F1 score to 92.2, establishing new state-of-the-art results in natural language understanding. The shift in NLP tasks towards full-network pre-training followed by task-specific fine-tuning has shown that larger model size improves performance. However, increasing the hidden size to 2048 leads to model degradation and worse performance, contrary to the belief that bigger is always better in representation learning for natural language. Scaling up representation learning for natural language is challenging due to computational constraints, especially in terms of GPU/TPU memory limitations. Chen et al. (2016) proposed gradient checkpointing to reduce memory requirements at the cost of speed. Sharing parameters across layers has been explored to reduce memory consumption and increase training speed. Previous work has explored sharing parameters across layers to increase training speed. Dehghani et al. (2018) found that networks with cross-layer parameter sharing perform better on language tasks. Recent work by Bai et al. (2019) introduced a Deep Equilibrium Model for transformer networks. Hao et al. (2019) combined a parameter-sharing transformer with the standard one to increase the number of parameters. ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text, related to discourse coherence. Various effective pretraining objectives for sentence embedding learning include Skipthought and FastSent embeddings, predicting future sentences, and explicit discourse markers. ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text, different from traditional sentence-level objectives. This approach is compared to BERT's loss in experiments, showing that sentence ordering is more challenging and beneficial for downstream tasks. ALBERT introduces factorized embedding parameterization as a key improvement over BERT, separating the WordPiece embedding size from the hidden layer size for better modeling and practical reasons. ALBERT introduces factorized embedding parameterization to separate WordPiece embedding size from hidden layer size for more efficient modeling and practical reasons. This allows for better context-dependent representations and prevents the model from becoming too large with billions of parameters. ALBERT uses factorization of embedding parameters to reduce the size of the parameters, projecting one-hot vectors into a lower dimensional space before the hidden space. This parameter reduction is significant when the hidden space is much larger. Cross-layer parameter sharing is also proposed to improve parameter efficiency in ALBERT. ALBERT utilizes parameter sharing across layers, with a default decision to share all parameters. This design choice is compared against other strategies in experiments. Dehghani et al. (2018) and Bai et al. (2019) have explored similar strategies for Transformer networks. While Dehghani et al. found UT outperformed a vanilla Transformer, Bai et al. showed their DQEs reach an equilibrium point. However, measurements on L2 distances and cosine similarity indicate that ALBERT's embeddings are oscillating rather than converging. ALBERT's embeddings show smoother transitions between layers compared to BERT, indicating the effect of weight-sharing on stabilizing network parameters. Despite a drop in metrics, the solution space for ALBERT parameters differs from BERT. In addition to masked language modeling loss, BERT uses next-sentence prediction loss for inter-sentence coherence. The NSP objective was designed to improve downstream task performance but was found to be unreliable and subsequently eliminated. The ineffectiveness of NSP is attributed to its lack of difficulty compared to MLM, as it conflates topic prediction and coherence prediction. Inter-sentence modeling remains important. ALBERT uses a sentence-order prediction (SOP) loss to model inter-sentence coherence, focusing on coherence rather than topic prediction. The SOP loss forces the model to learn finer-grained distinctions about discourse-level coherence properties, outperforming the NSP task. ALBERT models improve downstream task performance for multi-sentence encoding tasks by analyzing misaligned coherence cues. Compared to BERT models, ALBERT models have significantly smaller parameter sizes, with ALBERT-large having 18x fewer parameters than BERT-large. For example, ALBERT-xlarge with H = 2048 has only 60M parameters, while an ALBERT-xxlarge configuration with H = 2048 has 1.27 billion parameters. The configurations of main BERT and ALBERT models are analyzed in this paper, with ALBERT models showing improved parameter efficiency. Experimental setup includes pretraining on BOOKCORPUS and English Wikipedia. Inputs are formatted as x 1 = x 1,1 , x 1,2 \u00b7 \u00b7 \u00b7 and x 2 = x 1,1 , x 1,2 \u00b7 \u00b7 \u00b7. The input sequences are formatted as x1 and x2 segments, with a maximum length of 512 and a 10% probability of shorter sequences. Masked inputs for MLM targets are generated using n-gram masking with a maximum n-gram length of 3. Model updates use a batch size of 4096 and a LAMB optimizer with a learning rate of 0.00176. Training is done on Cloud TPU V3 with the number of TPUs ranging from 64 to 1024. Training was conducted on Cloud TPU V3 with the number of TPUs used ranging from 64 to 1024. The experimental setup described is used for various versions of BERT and ALBERT models. A development set is created to monitor training progress, based on sets from SQuAD and RACE. Accuracies for MLM and sentence classification tasks are reported. Models are evaluated on popular benchmarks including the GLUE benchmark, SQuAD, and RACE datasets. The impact of design choices on ALBERT's parameter efficiency is quantified by comparing it to BERT-large. ALBERT-xxlarge achieves significant improvements with only around 70% of BERT-large's parameters, as shown in Table 3. Notable improvements are seen in various downstream tasks such as SQuAD v1.1, SQuAD v2.0, MNLI, SST-2, and RACE. ALBERT models show significant improvements in downstream tasks like MNLI, SST-2, and RACE compared to BERT models. BERT-xlarge performs worse than BERT-base on all metrics, indicating difficulty in training larger models. ALBERT models have higher data throughput due to less communication and computations, with ALBERT-xlarge being trained 2.4x faster than BERT-xlarge. Based on ablation experiments, ALBERT shows that an embedding size of 128 is optimal for performance. Cross-layer parameter-sharing strategies were also tested, with the all-shared strategy showing promising results. In experiments, different parameter-sharing strategies were compared for ALBERT models with embedding sizes of 768 and 128. Sharing all parameters hurt performance, especially for E = 768 (-2.5 on Avg) compared to E = 128 (-1.5 on Avg). Sharing FFN-layer parameters caused most of the performance drop, while sharing attention parameters had minimal impact. Other strategies like dividing layers into groups for parameter sharing were also explored. The experimental results show that decreasing group size M improves performance but increases overall parameters. The default choice is the all-shared strategy. Comparing parameter-sharing strategies for ALBERT models, sharing all parameters hurt performance, especially for E = 768. Sharing FFN-layer parameters caused the most performance drop. The NSP loss adds no discriminative power to the SOP task, while the SOP loss performs well on the NSP task. The SOP loss improves NSP and SOP task accuracy (78.9% and 86.5% respectively) and enhances downstream task performance (+1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE). ALBERT's depth and width affect performance, with a 1-layer model showing increased performance compared to a 3-layer model with the same parameters. Increasing the number of layers in an ALBERT model shows significant performance improvement, but there are diminishing returns beyond 12 layers. Similarly, increasing the hidden size leads to performance gains with diminishing returns, and a size of 6144 results in a decline in performance. None of these models overfit the training data, and they have higher training and development loss compared to the best-performing ALBERT configurations. Table 8 shows the impact of increasing the hidden-layer size for an ALBERT-large 3-layer setup. Comparing data throughput for BERT-large and ALBERT-xxlarge, BERT-large has a 3.17x higher throughput. When training time is controlled instead of training steps, ALBERT-xxlarge outperforms BERT-large by 1.5% on average after the same training duration. In Section 4.7, the study compares ALBERT-large with different layer configurations and explores the impact of wider ALBERT configurations like ALBERT-xxlarge. Results show that increasing the number of layers in ALBERT-xxlarge has negligible effects on downstream accuracy, indicating that a 12-layer configuration is sufficient. Additionally, the study reports on the impact of additional data used by XLNet and RoBERTa. The study compares ALBERT-large with different layer configurations and explores wider ALBERT configurations like ALBERT-xxlarge. Results show that increasing the number of layers in ALBERT-xxlarge has minimal effects on downstream accuracy. Additional data used by XLNet and RoBERTa significantly boosts performance. Removing dropout further increases model capacity and improves MLM accuracy. The study found that removing dropout can improve performance in large Transformer-based models like ALBERT-xxlarge. Further experimentation is needed to see if this phenomenon applies to other transformer-based architectures. State-of-the-art results were reported for fine-tuning in single-model and ensemble settings. The study reports state-of-the-art results for fine-tuning ALBERT-xxlarge models in single-task settings. The best-performing configuration includes combined MLM and SOP losses, no dropout, and selection of checkpoints based on development set performance. Ensemble models are created by averaging predictions from models fine-tuned with different architectures for benchmarks like GLUE, RACE, and SQuAD. Both single-model and ensemble results show the effectiveness of ALBERT. ALBERT improves state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. The single model achieves an accuracy of 86.5%, which is 2.4% better than the state-of-the-art ensemble model. ALBERT-xxlarge has fewer parameters than BERT-large but achieves significantly better results. ALBERT-xxlarge, despite having fewer parameters than BERT-large, achieves significantly better results. To enhance its performance further, methods like sparse attention and block attention can be used to speed up training and inference. Additionally, exploring hard example mining and efficient language modeling training could provide additional representation power. There is potential for improving language representations beyond sentence order prediction, suggesting more dimensions yet to be captured by current self-supervised training losses. The dataset for multi-choice reading comprehension, collected from English examinations in China, consists of nearly 100,000 questions with 4 candidate answers. Models use the concatenation of passage, question, and candidate answers for prediction. The dataset includes middle school and high school domains, with models trained and accuracies reported on both domains. Hyperparameters for downstream tasks are adapted from previous work."
}