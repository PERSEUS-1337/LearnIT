{
    "title": "HJeT3yrtDr",
    "content": "Recent work has shown the impressive cross-lingual abilities of multilingual BERT (M-BERT) despite being trained without specific cross-lingual objectives or aligned data. A study was conducted to analyze the factors contributing to M-BERT's cross-lingual performance, including linguistic properties, model architecture, and learning objectives. The study focused on Spanish, Hindi, and Russian languages, evaluating tasks like textual entailment and named entity recognition. It was found that lexical overlap between languages has minimal impact on cross-lingual success, while network depth is crucial. Multilingual BERT (M-BERT) is a Transformer-based language model trained on raw Wikipedia text of 104 languages. Unlike previous approaches, M-BERT does not require supervision or alignment between languages during training. Despite lacking explicit cross-lingual objectives, M-BERT produces representations that generalize well across languages. In this study, the success of M-BERT is analyzed by studying its performance on NLP tasks in two languages (source and target). The focus is on the key components influencing the model's success, with an emphasis on linguistic properties, network architecture, and learning objectives. The study investigates the contribution of word-piece overlap and structural similarity between source and target languages to the cross-lingual capabilities of B-BERT. Contrary to previous hypotheses, B-BERT shows cross-lingual capabilities even without word-piece overlap, suggesting other aspects of language similarity play a role in its success. The study explores the importance of network depth and total model parameters in B-BERT for both monolingual and cross-lingual performance. Multi-head attention is not crucial, as a single attention head can provide satisfactory results. The next sentence prediction objective negatively impacts performance. The study found that the next sentence prediction objective hurts the model's performance, while language identification does not affect B-BERT's crosslingual performance. Experiments showed that word-piece level tokenization outperforms character-level and word-level tokenization. The study conducted experiments on English-Spanish, English-Russian, and English-Hindi language pairs for cross-lingual Named Entity Recognition and Textual Entailment tasks. Previous studies on M-BERT have also been conducted. This work examines how B-BERT performs cross-lingually by probing its components, unlike previous studies that treated M-BERT as a black box. The study also challenges the usefulness of the next sentence prediction objective in the cross-lingual setting. In a study on cross-lingual abilities, it is shown that the number of attention heads is not crucial. The research provides insights into multilingual BERT's cross-lingual capabilities and introduces a methodology for analyzing language similarities. Additionally, the study delves into B-BERT's linguistic, architectural, and learning dimensions for enhancing cross-lingual neural models. BERT is a Transformer-based pre-training language representation model used in Natural Language Processing. It is trained with Masked Language Modelling and Next Sentence Prediction objectives. BERT follows two steps: Pre-training and fine-tuning. BERT is pre-trained using Masked Language Modelling and Next Sentence Prediction objectives on BooksCorpus and English Wikipedia text. It uses wordpiece tokenization and is fine-tuned for supervised tasks. Multilingual BERT follows a similar pre-training process with Wikipedia text from the top 104 languages, adjusting for differences in size. Despite lacking specific cross-lingual objectives or data, it exhibits cross-lingual ability. The cross-lingual ability of multilingual BERT (M-BERT) is analyzed in three dimensions: linguistics, architecture, and input/learning objective. Languages share similarities, such as word similarities and structure similarities, which contribute to M-BERT's effectiveness. This linguistic perspective considers word-piece overlap and structure similarity as factors in M-BERT's cross-lingual ability. The study analyzes the cross-lingual effectiveness of B-BERT, focusing on the architecture of BERT and its ability to extract semantic features. It explores the influence of transformer model components like depth, attention heads, and parameters on cross-lingual ability. The study also examines the impact of learning objectives and input tokenization methods on cross-lingual transferring, conducting experiments on Textual Entailment and Named Entity Recognition tasks. The study evaluates the cross-lingual performance of B-BERT in tasks like Textual Entailment (TE) and Named Entity Recognition (NER). TE assesses language understanding at a sentence level, while NER does so at a token level. XNLI dataset is used for TE evaluation, with inputs in multiple languages classified into entailment, contradiction, or neutral relationships. LORELEI dataset is used for Cross-Lingual NER evaluation. The NER dataset used for cross-lingual evaluation includes news and social media text labeled in English, Hindi, Spanish, and Russian. Training, development, and testing data is subsampled from English NER data, while the entire datasets of Hindi, Spanish, and Russian are used for testing. The vocabulary size is fixed at 60000, and B-BERT models trained on different languages are denoted as A-B. For pretraining, en, es, and ru Wikipedia are subsampled to 1GB, while the entire Wikipedia is used for Hindi. B-BERT training uses a batch size of 32, learning rate of 0.0001, and 2M training steps. XNLI finetuning follows BERT's approach in English. NER tasks involve extracting BERT features and finetuning a Bi-LSTM CRF model. B-BERT shows cross-lingual ability without word-piece overlap. Previous studies suggest selecting source languages with more word-piece overlap for cross-lingual transfer learning. In this section, the study focuses on the importance of structural similarity over word-piece overlap for the cross-lingual ability of B-BERT. The experiment compares cross-lingual performance with and without word-piece overlap, using a new corpus called Fake-English. The study compares cross-lingual performance with and without word-piece overlap using a new corpus called Fake-English. The results show the contribution of word-pieces to the model's success, with a significant drop in performance when word-piece overlap is removed. The study shows that B-BERT is cross-lingual even without word-piece overlap, contrary to previous hypotheses. Language structure includes properties like morphology and word frequency. BERT transfers well between languages with different vocabularies, such as Fake-English to English, Spanish, Hindi, and Russian. The study highlights the importance of structural similarity between languages for cross-lingual transferability. B-BERT effectively recognizes language structure, potentially due to its architecture. This suggests the need for further research on language structure despite its ambiguous definition. In this section, the study focuses on the contribution of different components of B-BERT architecture, such as depth, multi-head attention, and the total number of parameters, to understand their impact on cross-lingual ability. Cross-lingual experiments are conducted on the XNLI dataset using Fake-English as the source and Russian as the target language. The deepness of B-BERT is found to be crucial for extracting good language features, which enhances its cross-lingual effectiveness. The effect of depth on both monolingual and cross-lingual performance is analyzed. The study focuses on the impact of B-BERT architecture components like depth on cross-lingual ability. Using Fake-English and Russian, the depth of B-BERT is varied to analyze its effect on performance. Deeper models show better performance in English and improved cross-lingual ability. A correlation between English performance and cross-lingual ability is observed, supporting the assumption that depth plays a crucial role. The study analyzes the impact of multi-head attention on the cross-lingual ability of B-BERT. The number of attention heads does not significantly affect cross-lingual ability, as B-BERT performs well even with a single attention head. This finding aligns with recent studies on monolingual BERT. The study examines the impact of the total number of parameters on cross-lingual performance of B-BERT, with a focus on Fake-English and Russian languages on XNLI data. The number of parameters is found to be less significant than depth, but below a certain threshold, the number of parameters becomes significant in extracting good semantic features. The study analyzes the impact of the total number of parameters on the cross-lingual ability of B-BERT, focusing on Fake-English and Russian languages on XNLI data. It is found that a certain minimum number of parameters is required for extracting good semantic and structural features. The difference in performance between Fake-English and Russian serves as a measure of cross-lingual ability. The study also explores the effect of input representation and learning objectives on B-BERT's cross-lingual ability. In this work, the impact of different pre-training objectives on cross-lingual performance of BERT is studied. The addition of a language identity marker to the input does not harm cross-lingual performance. The study also investigates the effects of characters, words, and word-pieces vocabulary on BERT's performance. The NSP objective of BERT is to predict whether the second sentence comes after the first one in the original text. Adding a language identity marker doesn't affect B-BERT's cross-lingual ability. The study compares the performance of B-BERT pre-trained with and without the NSP objective, showing that it hurts crosslingual performance more than monolingual performance. In 2019, a study examined the impact of adding a language identifier in input data for B-BERT. Different end of string tokens were used as language identity markers. Comparing B-BERT performance with character, word-piece, and word tokenized input, it was found that word-piece tokenized input yielded better monolingual and cross-lingual performance. This is attributed to word-pieces carrying more information and addressing unseen words better. This paper presents a systematic empirical study on the cross-lingual ability of B-BERT, analyzing linguistic properties, neural architecture, and input representation. A new language, Fake-English, was created to study word-piece overlap effects. Results show that word-piece overlap and multi-head attention are not significant, while structural similarity and B-BERT depth play a role. The study focused on the cross-lingual ability of B-BERT, highlighting the importance of structural similarity and B-BERT depth. Future work could explore interactions among languages and the impact of adding related languages to M-BERT. The term Structural Similarity was introduced, with potential for further refinement and experimentation to better understand its components. In Table 8, a significant drop in entailment performance of B-BERT is observed when premise and hypothesis are in different languages. This could be due to BERT making decisions based on matching words or phrases. The study analyzes word-ordering and word-frequency similarity to better understand structural similarity. The study analyzes the effect of word-ordering similarity on cross-lingual transferability by shuffling random words in sentences during pretraining. This hides the similarity from B-BERT, allowing sentences to be treated as Bags of Words when fully permuted. During fine-tuning and testing, word-order similarity is crucial for cross-lingual transferability in B-BERT. Shuffling words in sentences affects performance, but other structural similarities still contribute to cross-lingual ability. The importance of word-order similarity for cross-lingual transferability in B-BERT is highlighted. Shuffling words in sentences impacts performance, but other structural similarities also play a role. The study explores if word frequency alone can lead to good cross-lingual representations, but it is not very useful. B-BERT is trained using Fake-English and a newly generated target corpus based on word frequency. The study explores the importance of word-order similarity for cross-lingual transferability in B-BERT. Unigram frequencies alone are not sufficient for learning, suggesting bi-gram or tri-gram frequencies might be useful. Experiments on multilingual BERT show comparable performance with reduced parameters and attention heads. Interest in reducing BERT size is growing, with comparable performance achievable by maintaining or increasing depth. The study shows that comparable performance can be achieved by reducing the number of parameters in multilingual BERT. By maintaining or increasing depth, a significant reduction in parameters can be made while still maintaining performance. The insights from bilingual BERT are applicable to multilingual BERT, and fewer parameters and attention heads are needed with enough depth. The study found that reducing the number of parameters in multilingual BERT can maintain performance. A drastic change in performance was observed in Russian when the number of parameters decreased. The minimum number of parameters required for 12 layers and 12 attention heads was identified. The effect of total number of parameters on Fake-English and Russian language performance was studied, showing a significant impact on cross-lingual ability. Reducing the number of parameters in multilingual BERT has a significant impact on cross-lingual ability. Performance drastically changes when parameters are reduced from 11.83M to 7.23M, indicating a threshold for the number of parameters."
}