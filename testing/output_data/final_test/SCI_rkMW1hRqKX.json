{
    "title": "rkMW1hRqKX",
    "content": "Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance. It is efficient, has no hyper-parameters, and does not require pre-training or joint optimization with conditional log-likelihood. OCD achieves state-of-the-art performance on end-to-end speech recognition, with $9.3\\%$ WER on Wall Street Journal and $4.5\\%$ WER on Librispeech datasets. Improving seq2seq models can advance machine translation, speech recognition, and other domains like image captioning, parsing, summarization, and program synthesis. Recent architectures using convolution and self-attention have proven useful for efficient training. Despite attempts to address limitations, Maximum Likelihood Estimation (MLE) is still considered. Despite limitations of Maximum Likelihood Estimation (MLE), it remains the dominant approach for training seq2seq models. Alternative methods require pre-training or joint optimization with conditional log-likelihood, making them difficult to implement and tune. These alternatives do not significantly outperform a well-tuned MLE baseline, especially with label smoothing and scheduled sampling. This paper introduces an efficient algorithm for optimizing seq2seq models based on edit distance, drawing inspiration from search-based structured prediction and policy distillation. The proposed Optimal Completion Distillation (OCD) algorithm efficiently optimizes seq2seq models based on edit distance. It outperforms Maximum Likelihood Estimation (MLE) on real-world datasets, identifying optimal extensions for generated sequences. OCD demonstrates effectiveness in end-to-end speech recognition, achieving impressive results on the Wall Street Journal dataset. Optimal Completion Distillation (OCD) algorithm improves seq2seq models for speech recognition, achieving low error rates on datasets like Wall Street Journal and Librispeech. It focuses on learning a mapping x \u2192 y from input to output sequences y * \u2208 Y. The final sequence prediction is done through probabilistic model p \u03b8 using exact or approximate inference methods like beam search. The standard approach to optimizing parameters in conditional probabilistic models involves maximizing a conditional log-likelihood. Maximum Likelihood Estimation (MLE) is commonly used in sequence to sequence learning, with BID54 proposing the use of recurrent neural networks (RNNs) for autoregressive seq2seq modeling. Autoregressive models estimate the conditional probability of the target sequence given the source one token at a time. The conditional probability is decomposed via the chain rule, with a special end-of-sequence token appended to handle variable length sequences. The curr_chunk discusses different architectures for estimating the probability of a token given a prefix and input, including LSTM, GRU cells, soft attention, convolution, and self-attention. A new objective function for optimizing autoregressive seq2seq models is presented, focusing on maximizing conditional log-likelihood. There is a mismatch between prefixes seen during training and inference. The curr_chunk discusses the mismatch between training and inference in autoregressive seq2seq modeling, highlighting the impact on generalization and the optimization of log-probability versus task evaluation metrics. To address this, the approach avoids training on ground truth target sequences. The approach discussed in the curr_chunk avoids training on ground truth target sequences by using sequences generated by sampling from the current model. OCD solves a prefix-specific problem to find optimal extensions for the best completions according to the task evaluation metric. The curr_chunk discusses the notion of optimal completion in sequence learning, relating it to reinforcement learning's optimal Q-values. It defines Q-values for a prefix and extending token, aiming to achieve maximal scores. The optimal extension for a prefix y <t is defined as tokens with maximal Q-values. This allows for sampling on-policy from the model or off-policy. Table 1 shows an example target and generated sample. Some prefixes have multiple optimal extensions with the same edit distance. Q-values are transformed using a temperature parameter \u03c4. In experiments, \u03c4 \u2192 0 for hard targets without hyper-parameter tuning. Training examples involve drawing a full sequence y \u223c p. The OCD objective involves minimizing KL divergence between optimal policy and model distribution for each token extension. Optimal Q-values are computed for each prefix to construct the optimal policy distribution. Knowledge of optimal policy is distilled into the model using KL loss. A sample sequence from the Wall Street Journal dataset demonstrates imperfect model predictions. The dynamic programming algorithm efficiently calculates optimal Q-values for prefixes of a sequence y based on edit distance evaluation metric. The algorithm discussed does not increase time complexity over MLE when certain assumptions hold. In genetic applications, OCD is less efficient than MLE. However, in practice, OCD cost is often negligible compared to neural network passes. The goal is to identify optimal suffixes with minimum edit distance. The algorithm aims to find optimal suffixes with minimum edit distance, which is lower bounded by a certain value. The edit distance is calculated by tracing a path connecting cells in a table, with operations being non-decreasing. The algorithm finds optimal suffixes with minimum edit distance, which is lower bounded by a certain value. The set of optimal suffixes is limited to suffixes corresponding to a specific edit path. By calculating edit distances efficiently, optimal extensions can be identified for maximum reward. Our work introduces a modified Levenshtein algorithm to efficiently compute optimal Q-values for all tokens. It builds upon Learning to Search and Imitation Learning techniques, such as DAgger, where a policy is optimized to mimic an expert teacher. In our approach, called OCD, the behavior policy is obtained from an online student without access to an oracle policy during training. The optimal policy is derived by finding optimal Q-values. AggreVaTeD assumes access to unbiased Q-value estimates. AggreVaTeD BID52 assumes access to unbiased Q-values and uses variance reduction techniques for policy optimization. OCD calculates exact Q-values and uses regular SGD for optimization. Our work is related to Policy Distillation BID48, where a DQN agent is used as an expert teacher to distill Q-value estimates into a smaller student network using a KL loss. OCD estimates exact Q-values using dynamic programming. Q-values are estimated using dynamic programming in OCD, which efficiently calculates exact Q-values in O(V + T) per step. Unlike L2S and SeaRNN, OCD does not require ground truth prefixes for training and solely relies on model samples. Approaches based on multiple roll-outs struggle to scale to real-world datasets with long sequences or large vocabularies. OCD efficiently calculates exact Q-values in O(V + T) per step without requiring ground truth prefixes for training. Unlike RL-based methods, OCD reduces variance by decomposing the sequence-level objective into token level optimal completion targets, stabilizing the model without the need for MLE pretraining or joint optimization with log-likelihood. Reward Augmented Maximum Likelihood (RAML) and its variants BID35 BID19 BID59 are similar to RL-based approaches, but instead of sampling from the model's distribution, RAML samples sequences from the true exponentiated reward distribution. However, sampling from the true distribution is often difficult and intractable, leading to similar problems as RL-based methods. SPG BID17 changes the policy gradient formulation to sample from a reward shaped model distribution, providing a heuristic to decompose ROUGE score. Despite lower variance, SPG still faces credit assignment issues like RAML and RL-based methods. OCD excels at training from scratch, making it a suitable alternative to MLE pretraining or joint optimization. Experiments were conducted on speech recognition benchmarks without language model rescoring. Our proposed OCD algorithm outperforms strong baselines like MLE and SS on WSJ and Librispeech datasets, achieving a new state-of-the-art. The model is an attention-based seq2seq network with a deep convolutional frontend, using beam search during inference. The architecture and hyperparameter details of our models are described in Appendix C. We analyze key characteristics of the OCD model and compare results with other baselines and state-of-the-art methods. During training, the generated prefixes do not match the ground truth sequence. OCD outperforms baselines like MLE and SS on WSJ and Librispeech datasets, achieving a new state-of-the-art. The study compares the OCD model with MLE and SS on WSJ and Librispeech datasets, showing OCD outperforms them and achieves a new state-of-the-art. OCD targets are the ground truth tokens, leading to improved generalization and alleviating the mismatch between training and inference. The impact of edit distance and different losses on test CER and WER is also investigated. The study compares the OCD model with MLE and SS on WSJ and Librispeech datasets, showing OCD outperforms them and achieves a new state-of-the-art. OCD targets are the ground truth tokens, leading to improved generalization and alleviating the mismatch between training and inference. The impact of edit distance and different losses on test CER and WER is also investigated. The optimizer's role is highlighted by experimenting with different losses, comparing the test CER and WER of schedule sampling with a fixed probability schedule and OCD model. The significant drop in CER of schedule sampling emphasizes the necessity of pretraining or joint training with MLE for such models. OCD does not require MLE pretraining or joint optimization with MLE, and prefixes are always sampled from the model from the start of training. Fine-tuning a pre-trained schedule sampling model increases the CER, emphasizing the importance of making the loss a priority. The study compares the OCD model with MLE and SS on WSJ and Librispeech datasets, showing OCD outperforms them and achieves a new state-of-the-art. OCD targets are the ground truth tokens, leading to improved generalization and alleviating the mismatch between training and inference. The impact of edit distance and different losses on test CER and WER is also investigated. The optimizer's role is highlighted by experimenting with different losses, comparing the test CER and WER of schedule sampling with a fixed probability schedule and OCD model. The significant drop in CER of schedule sampling emphasizes the necessity of pretraining or joint training with MLE for such models. OCD does not require MLE pretraining or joint optimization with MLE, and prefixes are always sampled from the model from the start of training. Fine-tuning a pre-trained schedule sampling model increases the CER, emphasizing the importance of making the loss a priority. With 100% sampling increases the CER to 3.8%. Appendix D covers optimizing Edit distance rather than Hamming distance. TAB0 compares OCD with several Optimal Completion Target (OCT) models, experimenting with different strategies for selecting the correct target. Our model trained with OCD achieves 3.1% CER and 9.3% WER, outperforming the baseline by 14% on CER and 12% on WER. It substantially outperforms prior work in terms of CER and WER. The model is trained on the Librispeech dataset using Byte Pair Encoding (BPE) BID49. Our model trained with OCD achieves 4.5% WER on the test-clean set, outperforming the baseline by 21%. The architecture and hyperparameter details are described in the appendix, and the validation and training WER curves are shown in FIG3. OCD starts outperforming MLE after 13 epochs on training decodings and after 9 epochs on validation decodings. The paper introduces Optimal Completion Distillation (OCD), a training method for improving autoregressive sequence models based on edit distance. OCD achieves 4.5% WER on test-clean, a 21% improvement over the baseline. It also outperforms the state-of-the-art results on test-other, demonstrating its effectiveness in optimizing models. Optimal Completion Distillation (OCD) is a training method for autoregressive sequence models based on edit distance. It achieves 3.1% CER and 9.3% WER on WSJ speech recognition task, and 4.5% WER on Librispeech without a language model. OCD outperforms all published work on end-to-end speech recognition without introducing new hyper-parameters. The total time complexity for calculating sequence loss using OCD is O(T^2 + |V|T), where V is the vocabulary size and T is the sequence length. The Optimal Completion Distillation (OCD) algorithm has a time complexity of O(|V|T) for calculating sequence loss, with memory complexity also at O(|V|T). While OCD has similar complexity to MLE, it is generally slower due to online sampling. Overall, a naive implementation of OCD is up to 20% slower than MLE in terms of step time. The Optimal Completion Distillation (OCD) algorithm, although similar in complexity to Maximum Likelihood Estimation (MLE), can be as fast as MLE baseline when trained off-policy. An example of how OCD works is by extracting optimal targets and their Q*-values from edit distances between prefixes of reference and hypothesis sequences. This process determines the next character based on minimum edit distance. The Optimal Completion Distillation (OCD) algorithm uses edit distances to determine the next character based on minimum edit distance. Teacher forcing for sequence learning has limitations due to the discrepancy between training and test objectives. The Optimal Completion Distillation (OCD) algorithm aims to optimize the empirical reward objective on the training set by defining an optimal completion policy. Unlike policy gradient approaches, OCD incorporates ground truth information to alleviate the credit assignment problem and optimize token level log-loss. The Optimal Completion Distillation (OCD) algorithm optimizes token level log-loss and addresses the credit assignment problem. It introduces the concept of exposure bias BID43 and formalizes it as a classification problem in autoregressive models. The key challenge is the model's inability to generalize to new prefixes during inference that are dissimilar to the training data. During inference, using a large beam size in beam search can lead to wrong generalizations in the sequence optimization process. Training on arbitrary prefixes can help address this issue. Optimal Completion Distillation (OCD) can train on any prefix due to its off-policy nature. Increasing beam size during inference decreases performance on WSJ datasets for Maximum Likelihood Estimation (MLE) and Scheduled Sampling (SS), with OCD showing better performance with a maximum Word Error Rate (WER) below 10%. The input audio signal is processed into 80-dimensional filterbank features, normalized with per-speaker mean and variance. The encoder consists of 2 convolutional layers with 3x3 filters, followed by a convolutional LSTM. The encoder in the model consists of 2 convolutional layers with 3x3 filters, followed by a convolutional LSTM and 3 LSTM layers with 256 cell size. Batch-normalization is applied between each layer. The attention-based decoder is a 1-layer LSTM with 256 cell size and content-based attention. Training details include using Xavier initializer, 300 epochs of batch size 8, and tuning learning rates separately for baseline and OCD models. TensorFlow BID0 is used for experiments on Librispeech dataset with a larger batch size of 16 and smaller learning rates. Increasing the number of LSTM layers in the encoder to 6 and the cell size to 384, OCD achieves lower edit distance compared to MLE for fixed Hamming distances during training, indicating that OCD is optimizing for edit distance as intended."
}