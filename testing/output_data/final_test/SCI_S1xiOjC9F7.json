{
    "title": "S1xiOjC9F7",
    "content": "This paper introduces Graph Neural Networks for embedding graphs in vector spaces for efficient similarity reasoning. It also presents a Graph Matching Network model for computing similarity scores between pairs of graphs using a cross-graph attention-based mechanism. The models are effective in various domains, including control-flow-graph based function similarity search for detecting vulnerabilities in software systems. Graph neural networks (GNNs) have proven to be effective in learning representations of structured data and solving supervised prediction problems on graphs. These models exploit relational structures found in various domains and outperform domain-specific baseline systems. GNNs are invariant to permutations of graph elements and compute node representations through a propagation process. In this paper, the focus is on similarity learning for graph structured objects, particularly in the context of similarity-based retrieval in graph databases. One application is in computer security for binary function similarity search to identify vulnerabilities in statically linked libraries. The focus is on similarity learning for graph structured objects, specifically in the context of similarity-based retrieval in graph databases. The application is in computer security for binary function similarity search to identify vulnerabilities in statically linked libraries. The challenge lies in identifying vulnerable statically linked libraries in closed-source software, with no current solutions available. The similarity learning problem involves comparing control flow graphs annotated with assembly instructions, where subtle differences can make two graphs semantically very different. A successful model should exploit graph structures and reason about graph similarity from both structures and learned semantics. The focus is on similarity learning for graph structured objects, specifically in the context of similarity-based retrieval in graph databases. Investigating the use of Graph Neural Networks (GNNs) to embed graphs into a vector space for efficient retrieval. The model maps each graph independently to an embedding vector for similarity computation in the vector space. An extension to GNNs called Graph Matching Networks is proposed. Graph Matching Networks (GMNs) are proposed as an extension to Graph Neural Networks (GNNs) for similarity learning. GMNs compute a similarity score through a cross-graph attention mechanism to associate nodes across graphs and identify differences. This matching model outperforms established baselines and structure agnostic models on tasks such as graph edit-distance learning, binary function similarity search, and mesh retrieval. The paper introduces Graph Matching Networks (GMNs) as an improvement over Graph Neural Networks (GNNs) for similarity learning. GMNs outperform graph embedding models and Siamese networks, demonstrating superior performance in various applications. The contributions include showcasing how GNNs can generate graph embeddings for similarity learning, introducing GMNs for cross-graph attention-based matching, and achieving strong performance compared to hand-engineered baselines. The models have evolved with modern deep learning components, incorporating graph convolutions and graph convolutional networks for node updates. Graph Neural Networks (GNNs) have been successfully applied in various domains, mainly focusing on supervised prediction problems. The paper introduces Graph Matching Networks (GMNs) as an enhancement over GNNs for similarity learning, with a focus on graph similarity search and graph kernels in database and data mining communities. Graph kernels are kernels on graphs designed to capture graph similarity, used in kernel methods for graph classification. They measure similarity between walks or paths on graphs, limited-sized substructures, and sub-tree structures. These kernels are typically used in models for similarity learning in database and data mining communities. Graph kernels are hand-designed and motivated by graph theory, used in models for similarity learning. Our graph neural network based similarity learning framework learns the similarity metric end-to-end, unlike traditional approaches. Distance Metric Learning focuses on learning a distance metric between data points, assuming the data lies in a vector space and learning a linear metric matrix to measure distance. In this paper, the focus is on representation and similarity metric learning for graphs, going beyond typical methods by modeling cross-graph matchings. Siamese Networks are used for visual similarity learning, consisting of two networks with shared parameters to compute representations independently. Siamese networks are used to compute representations and similarity scores for two input images independently. In contrast, graph matching networks are more powerful as they perform cross-graph computations, fusing information from both graphs early in the process. Another approach, a cross-example attention model, has been proposed for visual similarity. The goal is to produce a similarity score between two graphs represented as sets of nodes and edges. The text discusses graph similarity learning using graph embedding models, specifically GNNs and GMNs. Graphs are represented by nodes and edges with associated feature vectors. Two models are proposed for learning graph similarity: one based on standard GNNs and the other on more powerful GMNs. The GNN embedding model consists of an encoder, propagation layers, and an aggregator. The goal is to measure the similarity between graphs by embedding them into vectors and using a similarity metric in that vector space. The encoder in the graph embedding model maps node and edge features to initial vectors through MLPs. Propagation layers aggregate node representations using message passing, and an aggregator computes a graph-level representation after multiple propagation rounds. Graph matching networks compute similarity between two graphs by transforming node representations and aggregating them using weighted sums with gating vectors. This method is more powerful than simple sums and considers the structure of the graphs, unlike models that treat data as independent nodes. The similarity between graphs is computed using metrics like Euclidean, cosine, or Hamming similarities in the vector space. Graph matching networks compute similarity between two graphs by transforming node representations and aggregating them using weighted sums with gating vectors. These models compute the similarity score jointly on the pair, incorporating cross-graph matching vectors to measure how well nodes in one graph can be matched to nodes in the other. The proposed graph matching network changes the node update module in each propagation layer to consider aggregated messages on the edges and cross-graph matching vectors. The attention module in the DISPLAYFORM1 vector space similarity metric computes attention weights for pairs of nodes across two graphs, with a computation cost of O(|V1||V2|). In contrast, the GNN embedding model has a cost of O(|V| + |E|) for each propagation round. The GMNs leverage extra computation power to enhance cross-graph communications. When the attention weights are peaked at the exact match, the cross-graph communications are reduced to zero vectors. The proposed graph similarity learning models can adjust graph representations to capture differences across graphs, enhancing sensitivity to these variations. Pairwise training requires labeled pairs as positive or negative, while triplet training focuses on relative similarity. The text describes the losses used in pairwise and triplet training for graph similarity learning models. Pairwise training focuses on labeled pairs with a margin-based loss, while triplet training optimizes a margin-based triplet loss to encourage differences between closer pairs. The goal is to have binary graph representation vectors for efficient searching in large graph databases. The text discusses the benefits of using binary graph representation vectors for efficient nearest neighbor search algorithms. By transforming the vectors through a tanh function and optimizing pair and triplet losses, positive pairs are pushed to have high similarity while negative pairs have low similarity. These losses are found to be more stable than margin-based losses for Hamming similarity in graph similarity learning models. The graph matching networks (GMNs) excel in graph similarity learning tasks, outperforming other methods. Graph edit distance measures similarity between graphs by the minimum number of edit operations needed to transform one graph into another. Despite being NP-hard, approximations are used for computation. The GSL models can learn structural similarity between graphs effectively. The graph matching networks (GMNs) excel in learning graph similarity tasks, outperforming other methods. Training data is generated by sampling random binomial graphs and creating positive and negative examples. The model predicts similarity scores for graph pairs. The dimensionality of node and graph vectors is fixed, with better performance observed with increasing propagation steps. Results are compared with the Weisfeiler Lehman (WL) kernel. The performance of different models is evaluated using pair AUC and triplet accuracy metrics on fixed sets of graph pairs and triplets. GSL models are trained and evaluated on specific graph distributions with varying parameters, showing results in Table 1. The evaluation results in Table 1 show that GSL models perform better than generic baselines by learning on specific graph distributions. GMNs consistently outperform GNNs, with cross-graph attention providing insights into their workings. More experiments on generalization capabilities are included in Appendix B.1. The focus is on binary function similarity search. Binary function similarity search is crucial in computer security when analyzing binaries without access to the source code. By extracting control-flow graphs (CFGs) from binaries, we can identify vulnerabilities by searching for similar binaries in a library. Each node in a CFG represents a basic block of assembly instructions, with edges indicating control flow through branching, loops, or function calls. In computer security, binary function similarity search is essential for analyzing binaries without source code access. By extracting control-flow graphs (CFGs) from binaries, vulnerabilities can be identified by searching for similar binaries in a library. Previous approaches include graph theoretical matching algorithms and embedding methods based on graph neural networks. Further study is conducted on graph embedding and matching models, exploring pair and triplet training, propagation steps, and learning node features from assembly instructions. We train and evaluate our model on data generated by compiling the popular open source video processing software ffmpeg using different compilers gcc and clang, resulting in 7940 functions and roughly 8 CFGs per function. The CFGs have an average size of around 55 nodes per graph, with some larger graphs having up to a few thousand nodes. Different compiler optimization levels result in CFGs of varying sizes for the same function. The data is split with 80% for training, 10% for validation, and 10% for testing. The models are trained to learn a similarity metric on CFGs to search through a library of binaries invariant to compiler type and optimization levels. Our graph embedding and matching models are compared with Google's BID16 tool for function similarity search in binaries. BID16 uses a graph hashing process to encode CFG representations, while we also map CFGs to 128-dimensional binary vectors for similarity search. Our graph embedding and matching models, utilizing binary vectors and Hamming similarity, show improved performance with more propagation steps in different data settings. The models are evaluated based on pair AUC and triplet accuracy, demonstrating consistent performance enhancements. The graph embedding and matching models consistently improve with more propagation steps, outperforming the structure agnostic model with 0 propagation steps. The graph embedding model is better than baselines with enough propagation steps, while graph matching models outperform embedding models. The WL kernel achieved 0.619 AUC and 24.5% triplet accuracy, but our models learn task-specific features for better performance. The effects of design decisions in the GMN model are carefully examined and compared against alternatives like Graph Convolutional Network (GCN). The comparison between Graph Convolutional Network (GCN) and Siamese versions of GNN/GCN embedding models is conducted to evaluate alternatives for the function similarity search task. The Siamese model predicts distance values by concatenating graph vectors and passing them through a 2-layer MLP. The importance of cross-graph attention early in the similarity computation process is highlighted, as Siamese networks fuse representations at the end. Experiments are also conducted on the COIL-DEL mesh graph dataset, treating graphs in the same class as similar. In this study, the function similarity search task was set up using graph neural networks. Results from Table 2 show that the GNN embedding model outperforms the GCN model, Siamese network architecture is better for learning similarity, and GMNs excel in cross-graph information communication. The unique challenges and benefits of graph similarity learning are discussed, emphasizing the classification setting for graph embedding models. The graph matching networks are proposed as a stronger alternative to graph embedding models, offering the ability to compare graphs at all levels and allocate capacity towards embedding or matching. This added expressivity comes with increased computation cost. The graph matching networks offer increased expressivity for comparing graphs but come with added computation costs. These models are best used when focusing on individual pair similarities or in a retrieval setting with a faster filtering model to narrow down search results before using the matching model to improve precision. Developing neural models for graph similarity learning is an important research direction with various challenges to address. In graph embedding and matching models, using an MLP with one hidden layer as the message module with ReLU nonlinearity is beneficial. Initializing the weights of the message module to be small helps stabilize training. The size of the hidden layer and output is set to 2D for node state vectors of dimension D. At the beginning of training, large scales in message vectors can hinder learning. Using GRUs as f node modules generally outperformed one-hidden layer MLPs. The aggregator module utilizes a single linear layer for node transformation and gating. The logistic sigmoid function is applied element-wise in the output layer. The logistic sigmoid function is used for graph vectors, followed by an MLP with one hidden layer for further transformation. Attention weights are computed for the matching model using different similarity measures. Node state vector dimensionality is fixed at 32, and graph vector dimensionality at 128. Model size is tuned for function similarity search, avoiding overfitting. The same settings are used for the edit distance task. For the edit distance learning task, larger models can improve performance. Parameters like triplet vs pair training, propagation layers, and parameter sharing were explored. Pair training was slightly better, more layers improved performance, and sharing parameters was useful. Learning rate was fixed at 0.001 with the Adam optimizer. The baseline WL kernel labels nodes by degree and updates node representations iteratively based on neighbor node patterns. The kernel computes values as a dot product of graph representation vectors, allowing for large dimensional representation vectors for each graph. This advantage over other models lies in the different 'feature' types for different pairs of graphs due to varying node patterns. The WL kernel has an advantage over other models due to its ability to handle varying node patterns. In experiments, the GSL models trained on small graphs showed good generalization to larger graphs, with results presented in Table 3. The GSL models trained on small graphs can generalize to larger graphs, but performance decreases on much larger graphs. The fixed sized graph vector limits effectiveness, while the WL kernel has more effective features for computing similarity. GSL models perform better when trained on graphs from relevant distributions. Testing on different k p and k n combinations showed improved performance on k p = 1, k n = 4 compared to k p = 1, k n = 2. When evaluated on graphs with different k values, the performance varies. Training on k p = 2, k n = 3 shows better results on certain graphs compared to k p = 1, k n = 2. However, models trained on k p = 2, k n = 3 do not have significant advantages in generalizing to larger graphs. The edges are initialized with constant vectors of 1s and the encoder MLP for the edges is an identity mapping. The encoder MLP is a linear layer when using the CFG graph structure. For assembly instructions, each node has a list of associated assembly code, with operator types extracted and embedded into vectors. Hyperparameters like triplet vs pair training, learning rates, propagation layers, and parameter sharing were explored. Triplet training slightly outperformed pair training, smaller learning rates were more stable, more propagation layers generally helped, and using different propagation layer parameters yielded better results. The study explored hyperparameters like triplet vs pair training, learning rates, propagation layers, and parameter sharing. Different propagation layer parameters were found to perform better than shared parameters. GRUs were more stable and performed better than MLPs overall. Additionally, the study tested models on a smaller dataset obtained from compiling the compression software unrar, highlighting the challenge of overfitting due to the dataset's size. The study investigated hyperparameters like triplet vs pair training, learning rates, and propagation layers. The graph matching model used shared parameters for propagation and matching layers, trained with 5 propagation layers. Different propagation layer parameters outperformed shared parameters. Attention visualizations showed sensible maps even with T > 5, with non-converging distributions. The dataset's unreliability makes drawing conclusions challenging. The study explored hyperparameters like triplet vs pair training, learning rates, and propagation layers. The graph matching model used shared parameters for propagation and matching layers, trained with 5 propagation layers. Attention visualizations showed sensible maps even with T > 5, with non-converging distributions. To improve model performance, separate key, query, and value vectors for each node should be computed as in the tensor2tensor self-attention formulation by Vaswani et al. (2017)."
}