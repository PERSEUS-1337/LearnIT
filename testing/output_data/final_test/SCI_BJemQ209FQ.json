{
    "title": "BJemQ209FQ",
    "content": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. Recent approaches improve success rates on simple environments with human demonstrations, but struggle in complex environments with millions of possible instructions. To address this, guided RL approaches are proposed to generate unlimited experience for learning by decomposing instructions into sub-instructions and scheduling a curriculum for gradual learning. The text discusses a novel meta-learning framework for training a deep reinforcement learning agent to follow instructions without human demonstrations. The agent is trained on smaller, synthetic instructions using a QWeb neural network architecture. The agent outperforms the baseline, achieving a 100% success rate on challenging environments. The focus is on training reinforcement learning agents to navigate the web by following specific instructions. The text introduces a meta-learning framework for training a deep reinforcement learning agent to navigate the web by following instructions. The agent learns through large state and action spaces with sparse rewards, such as booking a flight ticket. The task involves filling out web elements correctly and selecting the cheapest flight option. The difficulty lies in selecting the correct airport/date combination among numerous options. The agent learns to navigate the web by following instructions, with the task of booking a flight ticket. Learning from large sets of instructions is challenging due to sparse rewards and a high number of possible tasks. Previous work has shown success in guiding exploration through human demonstrations and pretrained word embeddings. In this work, two methods for reinforcement learning in large state and action spaces with sparse rewards for web navigation are presented. The first method, curriculum-DQN, utilizes expert demonstrations or an instruction-following policy to guide exploration starting with an easier task. Curriculum-DQN guides exploration in web navigation by breaking instructions into sub-instructions and assigning easier tasks to the agent. A metalearning framework trains a generative model for expert demonstrations using any web navigation policy. This allows for the autonomous generation of new expert demonstrations. The paper introduces an instructor agent, a meta-trainer, that generates new expert demonstrations to train the navigator. Two novel neural network architectures, QWeb and INET, are used for encoding web navigation Q-value functions. These architectures combine self-attention, LSTMs, and shallow encoding. The performance of these approaches is tested on Miniwob and Miniwob++ datasets. The instructor agent tests performance on Miniwob tasks, improving upon baselines and outperforming previous work. The focus is on web navigation, with potential applications in goal-oriented tasks. The work explores curriculum learning approaches and meta-training for deep Q networks. BID7 encodes DOM trees by extracting spatial and hierarchical features, using shallow encodings to enhance learning semantic relationships. BID10 utilizes an attention-based DQN for navigating in home environments with visual inputs, generating Q values for atomic actions in a 3D environment. Modifying reward functions and using augmented rewards for Web navigation tasks are practices explored. Curriculum learning methods are studied to divide complex tasks into smaller components. Our curriculum learning involves dividing complex tasks into smaller subtasks for easier solving. Meta-learning utilizes past experiences to continuously learn new tasks. Our framework generates instruction and goal pairs to set the environment and provides dense rewards for effective training of a low-level navigator agent using DQN. DQN aims to maximize rewards by rolling out episodes based on Q(s, a) suggestions. Sparse rewards are only given at the end of successful episodes, making training difficult in large state and action spaces. An instruction I = [F = (K, V )] is provided as key-value pairs to guide the agent in a Web environment. The environment state consists of instructions and a DOM tree representation. Rewards are based on comparing the final state with the goal state. Actions are limited to Click and Type actions on DOM elements. Composite actions are defined by a hierarchy of atomic actions. The composite Q value function is defined based on a graph layout, modeling each node's dependencies. Actions include selecting a DOM element, clicking, and typing. A gating mechanism combines Q values from shallow and deep encodings. The agent picks a DOM element with the highest Q value, decides to click or type, and selects a value from the instruction for typing actions. Our proposed model, QWeb, generates Q values for observations and atomic actions. It includes shallow DOM and instruction encoding layers to handle large input vocabulary. Reward augmentation and curriculum learning are used to address learning challenges. QWeb encodes instruction fields into fixed-length vectors and identifies overlapping words between DOM attributes and instructions. The DOM tree is encoded by linearizing the tree structure and running a bidirectional LSTM network on top of the DOM elements sequence. Output of the LSTM network and encoding of the instruction fields are used to generate Q values for each atomic action. Instructions are represented by vectors corresponding to different fields, with keys and values encoded and transformed via a fully connected layer. The encoding of a field is computed by generating overlapping word sequences for each field in the instruction and each attribute of a DOM element. Word embeddings are averaged over each sequence and attribute to compute the embedding of a DOM element conditioned on each instruction field. A self-attention mechanism is used to compute a probability distribution over instruction fields and reduce this instruction-aware embedding into a single DOM element encoding. The conditional embedding of a DOM element is the weighted average of these embeddings. Encoding of DOM trees involves representing attributes by averaging word embeddings, encoding DOM elements as the average of attribute embeddings, and using a bidirectional LSTM network to encode the DOM tree. Q values for typing a field from the instruction to a DOM element are generated using a trainable vector, and Q values for click or type actions on a DOM element are generated through a FC layer. In flight-booking environments with a large input vocabulary, deep Q network is augmented with shallow encodings of instruction and DOM tree to improve semantic similarity learning. Shallow encoding matrix is generated by computing word-based similarities between instruction fields and DOM element attributes, including sibling encodings to incorporate DOM hierarchy. Summation over columns and rows generates shallow input vectors for DOM elements. The shallow encoding matrix is used to generate input vectors for DOM elements and instruction fields. These vectors are transformed and scaled to generate single values. Final Q values are computed using a gating mechanism between deep and shallow Q values. Potential based rewards are used to augment the environment reward function by comparing the final state to the goal state. In the flight-booking environment, potential-based rewards are computed by comparing the current state to the goal state. The agent receives a positive reward when the potential for the current state increases compared to the previous state. Curriculum learning is implemented by breaking down instructions into sub-instructions, allowing the agent to focus on solving a subset of tasks. Two curriculum learning strategies are practiced to train the agent effectively. In the flight-booking environment, curriculum learning strategies are used to train QWeb effectively. Two strategies include warm-starting an episode and simulating sub-goals. Warm-starting involves placing the agent closer to the goal state to learn a small number of sub-instructions. The agent independently visits each DOM element with a certain probability and uses an ORACLE policy to perform correct actions. This process is illustrated in FIG0 for the flight-booking environment. The training process in the flight-booking environment involves curriculum learning strategies like warm-starting episodes and simulating sub-goals. Warm-starting begins with a high number and gradually decreases towards 0.0 over a set number of steps. Sub-goals are simulated by selecting a subset of elements and using an ORACLE to generate a sub-goal. QWeb receives a positive reward for successfully reaching this sub-goal during training. The training process involves gradually increasing the number of elements in the DOM tree towards a maximum limit. If the limit is reached, the environment reverts to the original state. To address situations without human demonstrations, a meta-training approach combines curriculum learning and reward augmentation. In the first phase, an instructor agent generates new instructions with a DQN agent. The instructor then provides demonstrations for the QWeb agent using a rule-based approach. The training process involves using a rule-based policy to provide demonstrations for the QWeb agent. An instruction state is defined by a sampled goal and a key from a predefined set. Instruction actions involve selecting a DOM element and generating a value corresponding to the key. The agent receives a positive reward for correct values. The instructor agent receives rewards based on the correctness of generated values for keys. Training involves using a sampling procedure similar to curriculum learning. Q value function approximation is learned for instruction generation using a DOM tree encoder. The key in the environment state is encoded for selecting DOM elements based on similarity. Probability distribution over DOM elements is generated. The instructor agent receives rewards based on the correctness of generated values for keys. Training involves using a sampling procedure similar to curriculum learning. Q value function approximation is learned for instruction generation using a DOM tree encoder. Probability distribution over DOM elements is generated by similarity between key and DOM elements. Q values for attributes are computed by combining two Q values. A rule-based randomized policy is used to iteratively visit DOM elements and take actions like Click(e) or Type(e, t). In the flight-booking environment, RRND randomly selects type sequences from a knowledge source to interact with DOM elements. The final DOM tree is used to generate instructions for web navigation. QWeb takes actions and receives rewards, which are used to compute the final reward. Different policies can be designed depending on the environment, and goal states can be generated accordingly. In this work, policy design can collect desired final states, even if the goal states generated may not be valid. MetaQWeb can train QWeb using incomplete episodes and instruction-goal pairs from the web navigation environment. Various signals can be utilized with MetaQWeb, such as generating supervised episodes, behavioral cloning, curriculum scheduling, and using it as a behavior policy for off-policy learning. The approach uses potential-based dense rewards (R1) and is evaluated on Miniwob BID11 and Miniwob++ benchmark tasks involving clicking and typing. In this study, the QWeb is tested in challenging environments like social-media-all and book-flight-form. The latter involves a large number of states and actions, with structured instructions and a DOM tree representation. The environment has a limit of 100 DOM elements and 3 fields. The number of possible actions and variables can reach up to 300 and 600, respectively. The complexity increases with over 700 airports. In challenging environments like social-media-all and book-flight-form, QWeb faces tasks with over 700 airports and a large number of states and actions. The task length is 12, with more than 7000 possible values in instructions and DOM elements. QWeb receives a sparse reward at the end of each episode, with a success rate metric used for evaluation. Comparing to previous approaches, QWeb aims to find successful episodes with as few actions as possible. QWeb approaches involve pre-training with behavioral cloning and fine-tuning using RL, as well as an alternating training approach with program-policy and neural-policy. Evaluation on Miniwob environments shows QWeb's performance matching previous state-of-the-art without shallow encoding or curriculum learning. QWeb approaches involve pre-training with behavioral cloning and fine-tuning using RL, as well as an alternating training approach with program-policy and neural-policy. Evaluation on Miniwob environments shows QWeb's performance matching previous state-of-the-art without shallow encoding or curriculum learning. QWeb can match the performance of previous state-of-the-art on simple environments, setting a strong baseline for evaluating improvements on more difficult environments. The effectiveness of using a biLSTM encoder for encoding DOM hierarchy is confirmed, giving consistently competitive results. Evaluation on more difficult environments like click-pie and social-media-all reveals challenges due to large vocabulary sizes and task lengths. The QWeb approach successfully solves tasks in environments with large vocabulary sizes and task lengths, outperforming previous methods without human demonstration. Shallow encoding and augmented rewards are crucial in the social-media-all environment to train a successful agent. Without augmented rewards, the QWeb quickly overfits and converges to terminating episodes early to avoid step penalties. Using demonstrations can lead to faster learning in various environments, with LIU18 requiring less than 1000 steps and QWeb needing over 3000 steps. However, there is a drop in performance in the more difficult click-pie environment. LIU18 reaches a 52% success rate at 13k steps, while QWeb achieves the same at 31k steps and 100% success at 175k steps. In the book-flight-form environment, QWeb struggles to generate successful episodes without certain improvements, particularly shallow encoding. Without shallow encoding, QWeb struggles to learn good semantic matching even with a dense reward. Analyzing the cause reveals that QWeb tends to click the submit button at the first time step to avoid negative rewards. By using shallow encoding, both curriculum approaches show significant improvements with success rates above 90%. Augmenting the reward with a potential-based dense reward leads to complete task resolution. MetaQWeb's performance is evaluated after these findings. After evaluating the performance of INET on generating successful instructions, it was found that most errors were due to incorrect DOM element prediction, particularly in date fields. MetaQWeb was then trained using meta-training and tested in two different cases: using instructions generated by MetaQWeb and in the original environment. Results showed that MetaQWeb performed strongly in both cases, coming very close to solving the task. The MetaQWeb agent showed strong performance in solving tasks, with 75% of errors coming from incorrect date field instructions. The MetaQWeb was trained with instructions generated by an instructor agent, leading to some inaccuracies in the training data. This resulted in a performance difference compared to models trained with original instructions. In this work, two approaches were presented for training DQN agents in challenging web navigation environments with sparse rewards and large state and action spaces. One approach involves using expert demonstrations and curriculum learning to decompose difficult instructions, while the other approach utilizes a meta-trainer to generate goal state and instruction pairs with dense reward signals. These models outperform previous state-of-the-art models. The study demonstrates the effectiveness of using expert demonstrations and curriculum learning to train DQN agents in challenging web navigation environments. The models outperform previous state-of-the-art models and future work will explore applying the models to a broader set of navigation tasks. The study shows the success of using expert demonstrations and curriculum learning to train DQN agents in web navigation. Future work will apply the models to a wider range of tasks. Thanks to the Dialogue team at Google Research and anonymous reviewers for their feedback."
}