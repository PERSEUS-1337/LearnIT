{
    "title": "Sygg3JHtwB",
    "content": "The paper introduces a new approach called step size optimization (SSO) for adapting step sizes in gradient methods. SSO formulates step size adaptation as an optimization problem using ADMM, without requiring second-order information or probabilistic models. Stochastic SSO is also introduced for stochastic learning environments. Experimental results show that integrating SSO into SGD and Adam outperformed other adaptive gradient methods on benchmark datasets. Step size adaptation is crucial in gradient methods for optimizing performance in machine learning. While various approaches have been studied, practical usage is limited due to lack of empirical evidence and complexity. Heuristic methods like AdaGrad, RMSProp, and Adam are commonly used instead. The paper proposes a new optimization-based approach called step size optimization (SSO) for adapting step sizes in training models. This approach aims to improve upon existing methods like L 4 and AdaBound by addressing issues such as hyperparameters and manual design requirements. The paper introduces step size optimization (SSO) as a method to adapt step sizes in training models. SSO directly optimizes the step size to minimize the loss function, without requiring second-order information or probabilistic models. It offers a practical implementation with L2 regularization and stochastic learning environments. Step 1 - The goal of step size optimization (SSO) is to find the optimal step size that minimizes the loss function with respect to the step size \u03b7. SSO is integrated into gradient methods like SSO-SGD and SSO-Adam, which are compared with state-of-the-art step size adaptation methods on benchmark datasets.\n\nStep 4 - Step size optimization (SSO) aims to minimize the loss function by finding the optimal step size \u03b7. It is integrated into gradient methods like SSO-SGD and SSO-Adam, which are compared with other step size adaptation methods on benchmark datasets. Step size optimization (SSO) involves updating the model by moving in the opposite direction of the gradient. In real-world problems, directly solving the optimization problem is challenging due to the nonlinearity of the loss function. Linearizing the loss function around \u03b8 is necessary, with an inequality constraint for the upper bound of the step size to ensure validity. SSO adapts the step size by defining an upper bound for the step size. The augmented Lagrangian technique is used in step size optimization (SSO) to handle constrained optimization problems. It involves transforming inequality constraints into equality constraints using slack variables and dual variables. The balancing parameter \u00b5 is gradually increased in the optimization process to ensure feasibility for the equality constraints. The optimization algorithm using ADMM minimizes the augmented Lagrangian to find the optimal step size for equality constraints. ADMM iteratively optimizes primal and dual variables in a Gauss-Seidel manner to solve the optimization problem. The step size optimization problem with equality constraints involves primal variables \u03b7 and slack variables s1, s2. ADMM can optimize these variables along with dual variables \u03bb1, \u03bb2. The process involves minimizing the augmented Lagrangian to find the optimal step size. The gradient descent method with SSO optimizes \u03b8 by computing true and adaptive gradients, determining the augmented Lagrangian for step size optimization, and updating \u03b8 with the optimized step size and adaptive gradient. Additionally, an upper bound decay for SSO is introduced to address convergence issues caused by different regularization terms in the augmented Lagrangian. The upper bound decay method is integrated into SSO to address convergence issues caused by different regularization terms. It uses exponential decay with a decay factor \u03b3 \u2208 (0, 1) to gradually reduce the step size over training. This method indirectly reduces the step size by decreasing the upper bound, ensuring the step size converges to zero. The upper bound decay is more flexible than step size decay and provides an optimal step size for training. In SSO, the upper bound decay method is utilized to improve convergence by gradually reducing the step size using exponential decay. This approach is more flexible than traditional step size decay methods and ensures optimal step size for training. Additionally, SSO with L2 regularization is derived, incorporating a hyperparameter \u03b2 to balance the loss function and regularization term. The step size optimization problem is solved using ADMM with specific update rules for dual variables. The update rules of the dual variables in SSO are independent of the loss function and regularization term. The step size converges to a value within a range, leading to convergence of slack variables. However, if the optimal step size exceeds the upper bound, the step size may converge near the upper bound. This is not ideal for training as a smaller step size is needed for gradient methods to converge effectively. In stochastic learning environments, a small step size is crucial for gradient methods to converge. SSO with upper bound decay handles this effectively by reducing the upper bound over training. The optimization problem in stochastic environments involves formulating it on a mini-batch with a loss function for each sample. A conservative penalty term is introduced to prevent large changes in model parameters with respect to the step size. This leads to deriving the optimization algorithm for the step size in stochastic environments. In stochastic learning environments, optimizing the step size in SSO with L2 regularization involves applying linearization and ADMM. The update rule for \u03b7 is similar to deterministic SSO with L2 regularization. The time complexity of the gradient method with SSO is comparable to the vanilla gradient method in large-scale problems. The empirical time complexity analysis of the gradient method with SSO is similar to the vanilla gradient method in large-scale optimization problems like training deep neural networks. In experiments, the effectiveness of SSO in gradient-based optimization was validated by comparing SSO-SGD and SSO-Adam with other state-of-the-art gradient methods. The optimization problem focused on training deep neural networks using cross-entropy loss with L2 regularization on four benchmark datasets. The experiments were conducted on benchmark datasets MNIST, SVHN, CIFAR-10, and CIFAR-100 using CNN and ResNet models. Training loss and highest test accuracy were measured, with results reported in Table 1. Initial learning rates were selected through grid search, and hyperparameters of Adam were set accordingly. For Adam optimization, hyperparameters were set following the original papers. The upper bound for SSO was 0.5 for MNIST and 1 for other datasets. A decay factor of 0.95 was used for all datasets. The number of iterations for optimizing step size was fixed at 20. Regularization coefficient was selected via grid search. Batch size was 128 for all datasets, with specific architectures used for each dataset. The ResNet model with three residual blocks and one fully-connected output layer was used for SVHN and CIFAR datasets. Experiments were conducted using PyTorch on an NVIDIA GeForce RTX 2080 Ti. Different gradient methods, including L4-Adam, AdaBound, SSO-SGD, and SSO-Adam, were compared on the MNIST dataset, showing rapid reduction of training loss. SSO-Adam exhibited the highest test accuracy, but other methods also achieved similar accuracies due to the simplicity of the MNIST dataset. To accurately evaluate each method's effectiveness, performance was compared on SVHN. On CIFAR datasets, ResNet with three residual blocks and one fully-connected layer was used. SSO-SGD and SSO-Adam outperformed competitors in test accuracy, with SSO-SGD showing a 3% improvement over AdaBound. Both methods consistently outperformed competitors in test accuracy across different experiments. In experiments, SSO-SGD achieved 5% higher test accuracy than L 4 -Adam. SSO-SGD and SSO-Adam showed better generalization due to their design for stochastic learning environments. The test accuracy was measured over execution time, with SSO methods requiring less time compared to L 4 -Adam. On CIFAR-100 dataset, SSO-SGD and SSO-Adam had similar execution times to RMSProp and Adam, while L 4 -Adam took longer. Sensitivity of SSO hyperparameter was tested on MNIST dataset by changing the initial upper bound. Optimized step sizes for each epoch were measured using SSOAdam with upper bound decay, showing a gradual reduction in learning rate over epochs. Optimized step sizes for each epoch on MNIST dataset gradually reduced over the epochs, with the maximums of the optimized step sizes overlapping in Fig. 7."
}