{
    "title": "B1Z3W-b0W",
    "content": "In this paper, the authors propose iterative inference models that optimize a variational lower bound through encoding gradients. These models generalize VAEs and outperform standard inference models on benchmark datasets, offering insights into recent empirical findings. Generative models enable learning structure from data in unsupervised or semisupervised settings for tasks in computer vision and robotics. Latent variable models, particularly suited for learning hidden structure, use deep neural networks to learn complex relationships like object identities and dynamics. Exact posterior inference in these models is computationally challenging, leading to the use of scalable approximate methods like variational inference with Gaussian distributions. Variational inference aims to find the distribution that closely matches the true posterior by maximizing a lower bound on the marginal log-likelihood. Amortized inference avoids computing optimized approximate posteriors for each data example by learning a separate inference model. When neural networks parameterize the generative and inference models, it is called a variational setup. Iterative inference models are introduced as a new class of inference models inspired by recent work in learning to learn. These models learn to iteratively estimate the approximate posterior by encoding gradients, improving inference estimates over standard models with distributed computation. Latent variable models use local latent variables to model observations with global parameters. Learning model parameters and inferring the posterior are challenging, leading to the use of approximate inference methods like variational inference. Variational inference is used in latent variable models to approximate the true posterior by introducing an approximate posterior q(z|x) and minimizing the KL-divergence. The evidence lower bound (ELBO) is defined as E z\u223cq(z|x) [log p \u03b8 (x, z) \u2212 log q(z|x)], with the first term representing reconstruction and the second term representing regularization. Minimizing the KL-divergence helps in fitting the data and quantifying the dissimilarity between the latent representation and the prior. In variational inference, the KL-divergence is minimized by maximizing the evidence lower bound (ELBO) to approximate the true posterior. This involves optimizing the approximate posterior q(z|x) to perform inference and learning. The optimization procedures follow the variational EM algorithm, alternating between expectation and maximization steps until convergence. When q(z|x) is parametric, the expectation step involves finding optimal distribution parameters. Amortized inference replaces local approximate posterior optimization with global parameter optimization, improving efficiency for expressive generative models. The variational auto-encoder (VAE) utilizes the reparameterization trick to propagate stochastic gradients. The formulation involves propagating stochastic gradients from the generative model to the inference model, both parameterized by neural networks. It introduces standard inference models and compares them to iterative inference models, showing improved final inference estimates. Iterative inference models are introduced as an extension and improvement upon standard inference models in VAEs. Variational inference involves maximizing L with respect to the parameters of q(z|x), which is a non-convex optimization problem. Despite the complexity, inference models can learn to output reasonable estimates of q(z|x) across data examples. These models aim to replace traditional optimization techniques with a learned mapping from x. In VAEs, iterative inference models aim to replace traditional optimization techniques by learning a mapping from x to q(z|x). The optimization surface of L is visualized using a 2-D latent Gaussian model with a binarized MNIST digit. The inference model provides a near-optimal estimate in one forward pass without manual tuning, but is limited to a single estimate. The inference model in VAEs, without hand tuning, provides a single estimate but does not reach the optimal estimate, leading to an \"amortization gap\". Despite lacking convergence guarantees, inference models have shown empirical success. However, they are limited to single-step estimation procedures, potentially resulting in inferior inference estimates and impacting the quality of the generative model. To address this, inspiration is drawn from learning to learn, where an optimizer model can learn to optimize the parameters of another model. Neural networks can learn to optimize the parameters of another neural network model. The optimizer model adjusts update step sizes to improve the optimizee's loss, potentially speeding up optimization. The computational graph is differentiable, allowing the optimizer itself to be learned. The curr_chunk discusses iterative inference models that update approximate posterior estimates in latent variable models. It highlights differences from previous work in variational inference optimization, the use of nonrecurrent optimization models, and a novel model formulation for approximating gradient steps. The curr_chunk discusses iterative inference models for updating approximate posterior estimates in latent variable models, proposing a new approach that utilizes hidden states within recurrent neural networks. It contrasts with standard inference models and variational EM, presenting a novel model formulation for approximating gradient steps. The curr_chunk discusses iterative inference models for latent Gaussian generative models, focusing on parameter updating and gradient derivation. Latent Gaussian models are commonly used in VAEs for continuous-valued latent variables. The approximate posterior updates are crucial in this context. The curr_chunk discusses the iterative inference models for latent Gaussian generative models, focusing on parameter updating and gradient derivation. The approximate posterior can be any probability density, typically Gaussian. The model uses a Gaussian output density for continuous observations and derives the approximate posterior parameter gradients. The curr_chunk discusses deriving gradients for Bernoulli output distribution in iterative inference models for latent Gaussian generative models. The gradients involve precision-weighted errors at observed and latent levels, with Jacobian matrices of the approximate posterior parameters. The gradients are stochastic due to the evaluation process. The approximate posterior gradients in iterative inference models for latent Gaussian models involve stochastic evaluations using Monte Carlo samples of z \u223c q(z|x). These gradients are easier to compute and can approximate higher-order derivatives for faster convergence. The formulation allows for efficient computation of gradient information for updating the approximate posterior parameters. The iterative inference model for latent Gaussian models involves computing distribution parameters of p(x|z), p(z), and q(z|x) using stochastic evaluations. By encoding common terms, derivative calculations can be offloaded onto the inference model, similar to the approach in DRAW. However, DRAW does not explicitly consider latent errors or approximate posterior estimates, which must be implicitly handled by the model's hidden states. In Section 5.2, iterative inference models learn to infer without requiring gradients at test time and perform well with few iterations. Single-iteration models proposed in Section 4.1 are equivalent to standard inference models under certain assumptions. In hierarchical latent variable models, higher level latent variables provide empirical priors on lower level variables, making the approximate posterior gradients observation-dependent. This is in contrast to standard inference models, which can be seen as single-step optimization models approximating derivatives at a single latent point. Iterative inference models naturally handle cases where standard inference models fail, as they do not require gradients at test time and perform well with few iterations. The approximate posterior gradients in a hierarchical latent Gaussian model involve bottom-up and top-down errors. Standard inference models lack top-down prior information, leading to poor posterior estimates. S\u00f8nderby et al. (2016) proposed a \"top-down inference\" technique to address this issue. Experiments were conducted using latent Gaussian models trained on MNIST and Omniglot datasets. The experiments involved using various datasets like MNIST and Omniglot, with different output distributions. Fully-connected neural networks were used, and values were estimated using a specific number of samples. Additional experiment details can be found in the appendix, with source code to be released online. Testing was done on a 2D latent Gaussian model trained on MNIST to confirm the ability of iterative inference models to optimize the approximate posterior. The approximate posterior is optimized through a feedforward neural network, adjusting step sizes to reach a near-optimal estimate. Data reconstructions improve during inference optimization, matching the data examples more closely. The iterative inference models demonstrate improved reconstruction quality by directly improving with additional samples and inference iterations, providing advantageous qualitative differences over standard inference models. Training on MNIST with different numbers of approximate posterior samples shows that iterative models can output more precise updates. The iterative inference model improves by more than 1 nat with additional samples, while the standard inference model improves by roughly 0.5 nats. Different encoding schemes were tested, all outperforming standard inference models on MNIST across various architectures. The iterative inference model, with encoding of data and errors, outperforms standard models with the same architecture. Higher order derivatives approximation helps with fewer inference iterations but may limit performance with additional iterations. Iterative models show improved marginal log-likelihood on MNIST and CIFAR-10 compared to standard models. See Appendix C.5 for more details and discussion. The iterative inference model outperforms standard models in terms of inference capabilities and computational efficiency. It converges faster to better estimates even with only local gradient information. The model introduces an approximate posterior distribution for a latent variable model, showing superior performance compared to variational EM. See Appendix C.6 for more details and discussion. The objective of variational inference is to maximize L with respect to the parameters of the factorized Gaussian density q(z|x). The gradients \u2207 \u00b5q L and \u2207 \u03c3 2 q L are derived to solve this optimization problem. The log-prior is evaluated using the reparameterization trick to express z as \u00b5 q + \u03c3 q. The gradients \u2207\u00b5q and \u2207\u03c32q are derived to maximize the factorized Gaussian density q(z|x). Using the reparameterization trick, the log-approximate posterior is expressed as a function of z. The reparameterization trick is used to re-express gradients in the context of maximizing the factorized Gaussian density q(z|x). The gradient w.r.t. \u00b5q is zero, while the gradient w.r.t. \u03c32q is derived. The conditional likelihood form varies based on the data type, such as binary or continuous. The form: DISPLAYFORM0 where \u00b5 x = \u00b5 x (z, \u03b8) is the mean of the output distribution. Gradients DISPLAYFORM1 and DISPLAYFORM2 are computed using the reparameterization trick. Equations 36 and 37 are re-expressed as DISPLAYFORM3 and DISPLAYFORM4. The log of a Gaussian output density is represented as DISPLAYFORM9 with \u00b5 x as the mean and \u03c3 2 x as the variance. Derivation using \u03c3 2 x = \u03c3 2 x (z, \u03b8) results in additional gradient terms in \u2207 \u00b5q L and \u2207 \u03c3 2 q L. Gradients DISPLAYFORM10 and DISPLAYFORM11 are computed using the reparameterization trick. Despite different distribution forms, Bernoulli and Gaussian output distributions yield similar approximate posterior gradients. Hierarchical latent variable models factorize latent variables over multiple levels, with variables at higher levels providing empirical priors on variables at lower levels. This model offers more flexibility in representing the intricacies of each data example. The gradients of the output model involve the Jacobian multiplied by a weighted error term, similar to those of a one-level model. Hierarchical latent variable models factorize latent variables over multiple levels, with variables at higher levels providing empirical priors on variables at lower levels. The gradients of the output model involve a \"bottom-up\" gradient from reconstruction errors and a \"top-down\" error from priors. Equation 5 provides a general form for an iterative inference model with specific implementation details. Gradients can be on vastly different scales, which is handled in the code for reproducing experiments. To address the issue of gradients on different scales in neural network training, a technique is proposed to replace \u2207 \u03bb L with a concatenation of [\u03b1 log(|\u2207 \u03bb L| + ), sign(\u2207 \u03bb L)]. This is applied to parameters in \u03bb = {\u00b5 q , log \u03c3 2 q}. Errors are encoded using the concatenation of [\u03b5 x , \u03b5 z]. Global variances on output and prior densities are used, omitting \u03c3 2 x and \u03c3 2 p as they are constant. Encoding current estimates of \u00b5 q and log \u03c3 2 q is found beneficial. Handling the changing distributions of gradients or errors during learning and inference remains a challenge, requiring further work on developing iterative encoding architectures. In developing iterative encoding architectures, input normalization or saturation may be necessary. A gated updating scheme, known as a \"highway\" connection, is used for model output. This approach updates approximate posterior parameters using element-wise multiplication and a gating function. Compared to a residual updating scheme, this method showed improved performance and stability. In experiments with latent Gaussian models, means receive updates over many iterations while variances receive fewer updates. Further work is needed to develop schemes for updating both parameters effectively. Iterative inference models are parameterized as neural networks, with the possibility of using feed-forward networks instead of recurrent neural networks. Feed-forward networks lack the ability to capture non-local curvature information but can still update output estimates with local curvature information. Further research is needed to develop effective schemes for updating both parameters in these models. To update the output estimate, optimizer parameter gradients are propagated from the optimizee's loss at each optimization step. This approach aids in training recurrent iterative inference models and is essential for training feed-forward iterative inference models. With a recurrent model, gradients are calculated using stochastic backpropagation through time, while with a feedforward model, gradients are accumulated at each step and averaged over the total number of steps. Feed-forward models maintain a constant memory footprint but are limited to local optimization information. Overall, iterative inference models were found to be not difficult to train. Overall, iterative inference models were found to be not difficult to train. Care must be taken to ensure input gradients stay within a reasonable range, with the log transformation trick proving effective. The level of stochasticity in gradients significantly impacts performance, especially in the Gaussian case. In all experiments, inference and generative model parameters were jointly learned using the AdaM optimizer with a learning rate of 0.0002. The models used a learning rate of 0.0002 and exponential linear unit activation functions. \"Highway\" connections between hidden layers were added for stability and performance. Iterative inference models were implemented as feed-forward networks for easier comparison. Models with 2 latent dimensions and a point estimate approximate posterior were trained for visualization. Trained models with 2 latent dimensions and a point estimate approximate posterior using a 2D Dirac delta function. Models were trained on binarized MNIST dataset with neural networks having 2 hidden layers. KL-divergences were estimated with 1 sample of z \u223c q(z|x). Optimization surface was evaluated on a grid with range [-5, 5] for each latent variable. The model was evaluated on a grid with range [-5, 5] in increments of 0.05 for each latent variable. To approximate the MAP estimate, the optimization surface was up-sampled using cubic interpolation. Various visualizations were shown after training for different epochs on different datasets. Iterative inference models were trained on various datasets using different inference iterations. Difficulty was faced in obtaining sharp reconstructions for CIFAR-10 in a reasonable number of iterations. The model architecture included hidden layers with 512 units, a latent space of size 64, and a symmetric iterative inference model with highway connections. Different configurations were used for Street View House Numbers and CIFAR-10 datasets. Models were trained by drawing samples from the approximate posterior distribution and using the reparameterization trick for lower variance ELBO estimates. Iterative inference models were trained for 5 inference iterations, and all models were trained for 1,500 epochs. The model architecture for all encoding schemes was identical to the previous section. Models were trained for 1,500 epochs with a single approximate posterior sample. Comparing performance between standard and iterative inference models showed minimal variation. Training models with the same architecture using each inference model form allowed for quantitative comparison. The architecture for training iterative inference models on MNIST and CIFAR-10 involved one-level and hierarchical models. One-level models for MNIST had a latent variable size of 64 and 2 hidden layers with 512 units each. Hierarchical models had 2 levels with latent variables of size 64 and 32, with 2 hidden layers of 512 units at the first level and 256 units at the second level. The hierarchical models for CIFAR-10 included batch normalization layers at each hidden layer, highway skip connections, and a latent variable size of 1024. Results were reported to be worse than typical literature due to the use of small fully-connected networks instead of larger convolutional networks. Hierarchical iterative inference models on CIFAR-10 were challenging to train due to numerical instabilities. A comparison with Variational EM showed that iterative inference models outperformed conventional optimization techniques significantly. The iterative inference model, trained on CIFAR-10, outperformed conventional optimization techniques significantly. Despite having less derivative information, it quickly reached a stable estimate and remained stable for hundreds of iterations. The model also outperformed optimizers in terms of wall clock time. Krishnan et al. (2017) propose closing the amortization gap by performing inference optimization steps after encoding data with a standard model, showing gains on sparse, high-dimensional data like text and ratings. Similar findings were observed on the RCV1 dataset, using normalized TF-IDF features and a multinomial distribution model with 2-layer networks. The iterative inference model was trained with 16 steps, evaluated by reporting perplexity on the test set. The iterative inference models outperform standard inference models on the test set by a similar margin as reported by Krishnan et al. (2017). Despite having fewer input parameters and running fewer optimization steps, the iterative inference model shows improved optimization capabilities over successive iterations. The model shows improved optimization capabilities over successive iterations, with a near-optimal estimate achieved and a 25% relative improvement in the ELBO."
}