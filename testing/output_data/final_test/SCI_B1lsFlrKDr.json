{
    "title": "B1lsFlrKDr",
    "content": "We present two end-to-end autoencoding models for semi-supervised graph-based dependency parsing: the Local Autoencoding Parser (LAP) and the Global Autoencoding Parser (GAP). These models use deep neural networks to encode input into latent variables and reconstruct the input. Experiments on WSJ and UD datasets show that the models can leverage unlabeled data to improve performance with limited labeled data. Dependency parsing captures bi-lexical relationships. Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences. Efficient parsers have been developed using various neural architectures, but they require large amounts of labeled data. Semisupervised parsing aims to alleviate this problem by combining a small amount of labeled data. In this paper, two end-to-end semi-supervised parsers, Locally Autoencoding Parser (LAP) and Globally Autoencoding Parser (GAP), based on probabilistic autoencoder models are proposed to improve parsing performance for low-resource languages. LAP uses continuous latent variables for better tree inference representation, while GAP forms a probability distribution over dependency trees. The GAP model proposed in this paper differs from previous work by directly computing the posterior for calculating the Evidence Lower Bound (ELBO) instead of sampling from the latent tree structure. The contributions include two autoencoding parsers for semi-supervised dependency parsing, a tractable inference algorithm for computing the latent dependency tree posterior analytically, and improved performance on WSJ and UD datasets with unlabeled data. The GAP model improved results on WSJ and UD datasets compared to a semi-supervised parser. Dependency parsing studies focus on graph-based and transition-based parsers, with recent advancements in using embeddings and neural architectures for better performance. Neural architectures are now commonly used for learning structural decisions in parsing. Architectures for learning representation for scoring structural decisions have been studied by Andor et al. (2016), Kiperwasser & Goldberg (2016), and Wiseman & Rush (2016). Variational approaches, such as Variational Autoencoder (VAE) (Kingma & Welling, 2014), have been applied in recent NLP works to improve the lower bound of the original objective. The Conditional VAE (CVAE) is studied in a semi-supervised context, with a focus on latent representation learning. A dependency tree is considered as the latent variable, allowing for exact inference without approximation by sampling, tightening the lower bound. Our graph-based parser constructs a directed spanning tree for a given sentence by factorizing the score into individual arc scores, describing the likelihood of forming an arc between the head and modifier words. The scoring is based on individual arcs, focusing on the dependency connections in the sentence. The scoring in the tree-based parser is based on individual arcs, focusing on first-order parsing. A neural architecture similar to Kiperwasser & Goldberg (2016) is used to extract representations for head-modifier arcs. A scoring matrix is created for each sentence to represent all possible arcs connecting words. Graph-based parsers are then built using this scoring matrix. Further exploration of neural architectures for scoring is not the main focus, but it is noted that performance can be enhanced with advanced techniques. In the context of exploring neural architectures for scoring in tree-based parsers, the use of Variational Autoencoder (VAE) and Tree Conditional Random Field models is discussed. VAE involves generating latent variables z from a prior distribution to maximize log-likelihood, while CRF models input sequences with labels. The focus is on improving performance through advanced neural architectures. The CRF model scores input sequences with labels using globally normalized probability. Tree CRF models extend linear chain CRF to trees, such as dependency trees. The model resolves node pairs with direction to form a tree, with potentials in exponential form. A VAE model is extended for dependency parsing by creating a latent representation position-wise for sequential parsing. The VAE framework uses a continuous latent variable as neural representations for dependency parsing tasks. Each token in a sentence is represented by a high-dimensional Gaussian variable, allowing for contextual information retention and grouping tokens with similar properties. The original VAE setup is adjusted for semi-supervised tasks by incorporating labeled examples, following recent conditional variational formulations. The proposed full probabilistic model aims to maximize supervised and unsupervised parsing by optimizing the log joint probability for sentences with a golden tree or the log marginal probability without it. The model uses a tree-CRF to model the probability of a tree and optimizes the ELBO due to the intractability of directly optimizing the objective. Instead of autoencoding at the sequence level, the model reconstructs the input sentence using the dependency tree as a structured latent variable. The model uses a tree-CRF to optimize the log joint probability for sentences with a golden tree or the log marginal probability without it. It reconstructs the input sentence using the dependency tree as a structured latent variable, with a discriminative and generative component. The latent variable is the dependency tree structure, and the generative model reconstructs the sentence from the factor graph as a Bayesian network. The discriminative component is parameterized by \u03a6, representing the parameters of the underlying neural networks. The model utilizes a tree-CRF to optimize the log joint probability for sentences with a golden tree or the log marginal probability without it. It reconstructs the input sentence using the dependency tree as a structured latent variable, with a discriminative and generative component. The generative process involves each head reconstructing its modifier with a probability parameterized by \u0398, where \u0398 is a matrix of vocabulary size. The probability of reconstructing the input as modifiers in the generative process is determined by the sentence length and the probability of a head generating its modifier. The proposed model offers a unified learning framework for sentences with or without a golden parse tree. The model uses a tree-CRF to optimize log joint probability for sentences with a golden tree or log marginal probability without it. It reconstructs input sentences using dependency trees as structured latent variables, with a generative process involving heads reconstructing modifiers based on \u0398. The algorithm marginalizes over possible parse trees to compute Z and U, updating parameters using Adam and computing posterior Q(T) in an arc factored manner. During training, the objective function is chosen based on whether the input sentence has a golden parse tree or not. Instead of directly optimizing the loss, the evidence lower bound (ELBO) of log P \u0398,\u03a6 (m|x) is maximized. Entropy regularization is incorporated to account for posterior unambiguity. During training, the algorithm adds an entropy term to the objective function to maximize the evidence lower bound (ELBO) of log P \u0398,\u03a6 (m|x). The regularization term raises the expectation of Q(T) to the power of 1 1\u2212\u03c3, with \u03c3 annealed from 1 to 0.3. The model benefits from fixing parameter \u03a6 when data is unlabeled and optimizing w.r.t. parameter \u0398. The approach to approximate latent variables in the posterior distribution Q(T) is different from VAE-type models, as argued in this model. In contrast to previous models, this model proposes an analytically tractable expectation of the latent variable using a specialized belief propagation algorithm. Performance was evaluated on the WSJ dataset and multiple languages from UD 2.3. Our models, trained on multiple languages from UD 2.3, focus on semi-supervised learning for low-resource languages. The data used in our experiments is detailed in Table 3. We simulate low-resource language scenarios by annotating 10% of the training set. Our neural architecture includes word embeddings of dimension 100, POS embeddings of dimension 25, and a bi-LSTM hidden layer of dimension 125. Separate bi-LSTMs are used for words and POSs in LAP, while GAP utilizes a \"POS to POS\" decoder. In LAP and GAP models, separate bi-LSTMs are used for words and POSs. The training phase involves using Adam to update parameters, with the decoder in GAP updated using global optima. Hyper-parameters remain constant across experiments. Model performance is evaluated on the WSJ dataset and compared with other semi-supervised parsing models like CRFAE and DPPP. The LAP and GAP models utilize unlabeled data to improve performance compared to using only labeled data. The LAP model slightly underperforms the NMP model due to increased complexity, but achieves comparable results in semi-supervised parsing as the DPPP model. Our LAP model achieved comparable results on semi-supervised parsing as the DPPP model, while being simpler and more straightforward. The GAP model outperformed all methods by effectively utilizing unlabeled data. In Table 2, different models were compared on multiple languages from UD, trained in fully supervised or semi-supervised fashion. Self-training using NMP with labeled and unlabeled data showed deteriorated performance without proper use of unlabeled data. LAP and GAP models benefited from unlabeled data, achieving similar performance as NMP and NTP when using labeled data only. No external word embeddings were used to simulate low-resource scenarios. The LAP and GAP models benefit from unlabeled data, improving performance compared to using labeled data only. GAP generally outperforms LAP and is especially useful for low-resource languages with few annotations. Self-training on labeled and unlabeled data with the NMP model deteriorates performance, especially with small training data sizes. The GAP model, an end-to-end learning system with neural architecture, utilizes both labeled and unlabeled data to improve parsing performance without external resources. It outperforms previous semisupervised parsing systems on the WSJ dataset due to its discriminative and generative components working together to avoid overfitting. Additionally, the model analytically computes the expectation and marginalization of the latent variable, leading to improved performance. The curr_chunk discusses the encoder-decoder architecture in the context of the ELBO objective. It explains how the encoder approximates the posterior distribution and encodes input into the latent space. The decoder, acting as a generative model, regenerates the input from the latent space. The text also mentions the use of mean field approximation and conditional independence assumption in the generative model. The encoder and decoder in the VAE framework are trained jointly to minimize KL divergence. The encoder Q \u03c6 (z t |x t ) uses a bi-LSTM to transform x t, followed by MLPs to compute mean \u00b5 zt and variance \u03c3 2 zt. The generative model P \u03b8 (x t |z t ) uses an MLP to predict words over the vocabulary for reconstruction probability measurement. The \"re-parameterization\" trick is applied for training. In the VAE framework, the encoder and decoder are trained jointly to minimize KL divergence. The \"re-parameterization\" trick is used to sample z t from Q \u03c6 (z t |x t) by forming z t = \u00b5 zt + \u03c3 2 zt. The temperature on the KL divergence term is annealed during training. LAP learns priors from data iteratively, initializing them from a standard Gaussian distribution. During training, priors are updated using the last optimized posterior based on empirical data distribution. Previous studies have shown the importance of POS tags and external embeddings in characterizing dependency relationships. The variational autoencoding framework can reconstruct POS tags and external embeddings in addition to word embeddings. The variational objective in the framework involves reconstructing words, POS tags, and external embeddings. The algorithm computes arc scores for projective trees and uses inside and outside tables to compute the expectation of latent trees. The algorithm computes arc scores for projective trees and uses inside and outside tables to compute the expectation of latent trees in an arc-decomposed manner. The matrix P contains the expectation of individual arcs by marginalizing over all other arcs except itself. Modifications are needed to calculate the expectation w.r.t. the posterior distribution Q(T). The strict convexity of ELBO w.r.t. \u0398 is derived in this section, with details on the statistics of the WSJ data set and the languages used in UD 2.3. The statistics of the languages used in UD 2.3 are discussed by Marcus et al. (1993) and McDonald et al. (2013)."
}