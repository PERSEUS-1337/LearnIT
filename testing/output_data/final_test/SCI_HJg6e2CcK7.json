{
    "title": "HJg6e2CcK7",
    "content": "Deep neural networks are vulnerable to backdoor attacks, where adversaries can control the model's behavior by altering training examples. A new approach using adversarial examples and GAN-generated data makes poisoned inputs appear benign, unlike traditional attacks. This method avoids detection through mislabeled inputs and can deceive human inspection. Real-world deployment of advanced systems like speech recognition, machine translation, and game playing faces challenges in security and reliability due to adversarial examples. These imperceptible perturbations can misclassify inputs with high confidence, posing a threat to model inference. Additionally, the vulnerability of machine learning models extends to the training phase, where large datasets are required for optimal performance, leading to high costs in data generation and curation. Data poisoning attacks are a concern in machine learning, where malicious noise is injected into training sets to degrade model performance. While some attacks aim to reduce test accuracy, targeted poisoning attacks specifically aim to misclassify certain inputs. These attacks can be mitigated by evaluating on a holdout set to detect poor model performance. Recently, a backdoor attack was proposed by BID9, aiming to plant a backdoor in any model trained on a poisoned training set. This backdoor is activated during inference by a trigger, forcing the model to predict a specific target label. This attack is difficult to detect and relies on randomly selecting a portion of the training set to apply the backdoor trigger. The BID9 backdoor attack relies on poisoned inputs to plant a trigger in models for specific label predictions. The attack can be detected through outlier filtering and human inspection, revealing the backdoor. The paper investigates the necessity of using clearly mislabeled images in backdoor attacks. It analyzes the BID9 attack's effectiveness when a data filtering technique is applied, finding that poisoned inputs can be easily identified as outliers. Restricting the attack to rely on correctly labeled inputs renders it ineffective. A new approach is developed to synthesize poisoned inputs that appear plausible to humans. The approach involves making small changes to inputs to make them harder to classify while keeping the original label plausible. Two methods are used: GAN-based interpolation and adversarial p-bounded perturbations. These methods improve the original attack by using less conspicuous backdoor triggers and performing better with data augmentation. Data poisoning attacks involve injecting malicious samples into the training set to reduce the model's performance during test time. Adversarial perturbations outperform interpolation-based methods due to deep neural networks memorizing backdoor patterns. These attacks aim to either lower the model's accuracy on the entire test set or target specific examples for misclassification. While effective, these attacks are not highly threatening in real-world scenarios. Recently, BID9 proposed a different approach to data poisoning attacks called backdoor attacks. These attacks aim to associate a backdoor pattern with a specific target label, causing the model to predict that label when the pattern is present. The attack involves modifying a small number of inputs in the training set to contain the backdoor pattern and be labeled with the target label. Backdoor attacks involve injecting a backdoor pattern into the training set to manipulate the model's predictions. These attacks are difficult to detect and give the attacker control over test examples. The BID9 attack relies on injecting mislabeled inputs into the training set, assuming no filtering or manual inspection of outliers. The study investigated the BID9 attack by training a classifier on clean inputs and evaluating it on a poisoned dataset. The classifier assigned near-zero probability to most poisoned samples, indicating random labeling. Manual inspection revealed over 20 poisoned images among 300 training images. The study found over 20 poisoned images out of 300 training images. The attack is ineffective when restricted to poison examples of the target class. The poisoned samples contain enough information for correct classification without relying on the backdoor pattern. The BID9 attack is detectable through data filtering, prompting investigation into improvement methods. In the context of data poisoning, it is crucial to ensure that poisoned samples appear plausible under human scrutiny. The BID9 attack, originally focused on transfer learning, can be applied to data poisoning by setting a few pixels to create a specific pattern. The attack is detectable through data filtering, prompting the need for improvement methods. The BID9 attack, when restricted to only clean labels, is ineffective with only one class exceeding 50% attack success even at 25% poisoning. The attack success rate is defined as the percentage of test examples not labeled as the target that are classified as the target class when the backdoor pattern is applied. The focus is on clean-label attacks where poisoned samples have plausible labels to avoid detection. The BID9 attack is ineffective when restricted to poison inputs of the target class without changing true labels. The attack's ineffectiveness is due to the ability of poisoned samples to be correctly classified by a standard classifier. To introduce a successful backdoor, perturbations are made to the poisoned samples to make learning the input characteristics more difficult. This forces the model to rely more on the backdoor pattern for correct predictions. Plausible perturbations are essential to avoid detection, and two methods are explored for synthesizing these perturbations. The text discusses methods for synthesizing perturbations to create hard training samples using generative models like GANs and VAEs. The goal is to produce inputs that are not easily identified as outliers, by leveraging the semantic meaningfulness of the embedding space. The text discusses generating images using a random vector in a latent space with a generator. Encodings are optimized to match target images, allowing for smooth interpolation between classes. Adversarial examples are perturbed inputs used to deceive neural networks. These perturbations can transfer across models and architectures. In this case, adversarial examples are applied during training to encourage the model to memorize a backdoor pattern as a feature. Adversarial examples are computed independently using p-bounded perturbations with projected gradient descent. Adversarial perturbations are constructed for a fixed classifier with a specific norm and bound. Adversarially trained models are used to construct these perturbations, resulting in improved performance on the target class. Example poisoned samples are visualized, showing improved performance with increasing \u03b5. The attack success rate increases as \u03b5 increases, especially for 1.5% and 6% poisoning percentages. A 2 norm-bounded attack with \u03b5 = 600 resulted in high success rates on all classes when poisoning a 1.5% or greater proportion of the target label data. The attack applies a backdoor trigger to poisoned inputs without changing their labels, resulting in plausible labels for the poisoned images. Increasing the attack magnitude leads to more powerful attacks, affecting the original labels' plausibility. Evaluating attacks on various target classes and poisoned sample amounts shows significant effectiveness compared to baseline attacks. Adversarial perturbations are more effective than GAN-based attacks in poisoning attacks. The effectiveness of p-bounded adversarial perturbations in backdoor attacks surpasses GAN-based interpolation methods, especially with large perturbations. Simply using the backdoor pattern is not enough for a successful attack; the reliance on the backdoor pattern must overpower the signal from salient image features. Interpolation-based attacks have mediocre success due to the lack of a strong signal in the inputs created through interpolation. The adversarially perturbed inputs in backdoor attacks contain a strong signal, allowing reliance on the backdoor pattern to overcome natural features during inference. Adding Gaussian noise to poisoned inputs can make the attack more effective, but excessive noise diminishes the attack's success as the poisoned images lose meaningful information about the original label. Weak reliance on the backdoor allows correct classification by the classifier, which largely ignores the backdoor during testing. In this section, the text describes generating samples for backdoor attacks with plausible labels. Experimenting with undetectable backdoor patterns did not significantly impact the attack's success. The poisoned images did not differ significantly from the original set after adversarial perturbation. Training models without data augmentation to avoid obscuring the pattern. In Appendix B.2, experiments with data augmentation hinder attacks, but modifying the attack to introduce additional backdoor patterns improves success rates. Data augmentation can still hinder attacks in a stochastic manner. Further experimentation with backdoor patterns can lead to stronger attacks. The threat model involves choosing a target class label and poisoning a fraction of training inputs with a consistent backdoor pattern. The backdoor pattern involves a small black-and-white square in the bottom-right corner of an image. A classifier is trained on this poisoned dataset, and the attack success rate is evaluated on the test set. Experiments are conducted on the CIFAR-10 dataset with different methods to increase classification difficulty. In experiments targeting all ten classes individually, poisoning proportions of 0.4%, 1.5%, 6%, 25%, and 100% are tested to evaluate the attack at various scales. A standard ResNet BID11 network is used with specific configurations for training. The training procedure involved normalization and a step size schedule starting at 0.1, decreasing to 0.01 at 40,000 steps, and further to 0.001 at 60,000 steps. The total number of training steps was 100,000. The model's standard accuracy was unaffected by attacks except at 100% poisoning, where a significant decline of up to 10 percentage points was observed. This decrease was attributed to the model relying solely on the backdoor pattern, leading to incorrect predictions when the pattern was absent. Using 1000 steps of gradient descent with a step size of 0.1, the GAN was trained on images of two classes for interpolation attacks with \u03c4 values ranging from 0 to 0.3. Images generated with \u03c4 \u2264 0.2 were found to be plausible, leading to improved attack success. \u03c4 = 0.2 was chosen for further investigation due to its performance improvement, especially for poisoning attacks on all classes. The study shows improvement over the baseline for poisoning attacks on all classes, particularly at 1.5% and 6% poisoning percentages. Adversarial examples were constructed using a PGD attack on adversarially trained models, with perturbations generated for pre-trained models using 2 and \u221e norms. Maximum perturbations for the airplane class were tested at 300, 600, and 1200 for 2-norm attacks. The study demonstrated improved performance in poisoning attacks on all classes, especially at 1.5% and 6% poisoning levels. Adversarial examples were created using PGD attacks on pre-trained models with perturbations normalized to pixel values. Increasing the perturbation values resulted in better attack success rates, with \u03b5 = 600 chosen for further investigation due to its performance and plausibility. For further investigation, 2-based perturbations with \u03b5 = 600 were chosen. The attack success rate is higher than the clean-label BID9 attack baseline, except at the lowest poisoning percentage. Backdoor attacks of BID9 can be detected with a simple data filtering scheme, as they rely on mostly mislabeled inputs. To make the attack more insidious, perturbing the poisoned inputs to make them harder to classify without changing the true label is proposed. We propose methods for increasing classification difficulty in backdoor attacks, including adversarial perturbations and GAN-based interpolation. These methods make backdoor attacks harder to detect, emphasizing the need for better protection of ML models. The original BID9 attack on CIFAR-10 dataset allowed unlimited poisoned samples, while we study a limited poisoning threat model. In a limited poisoning threat model, the attack success rate is high with a small number of poisoned samples. Despite focusing on the plausibility of the base image, incorrect labels are prevalent. To address potential suspicion, a modified backdoor pattern is considered. To mitigate suspicion in a limited poisoning threat model, a modified backdoor pattern is considered by perturbing original pixel values with a backdoor pattern amplitude. This attack is extended to reduced amplitudes, resulting in substantial success rates for the dog class. The study found that higher backdoor pattern amplitudes led to higher attack success rates, with 6% poisoning percentages resulting in substantial success. Image plausibility improved with reduced amplitudes, and an amplitude of 32 was chosen for further investigation. Data augmentation was used to prevent overfitting in deep learning models. The study found that higher backdoor pattern amplitudes led to higher attack success rates, with 6% poisoning percentages resulting in substantial success. An amplitude of 32 was chosen for further investigation to improve attack success when using data augmentation. The backdoor pattern duplication aims to ensure pattern visibility after cropping and remain invariant to horizontal flips. When training with data augmentation, the four-corner backdoor pattern attack shows significantly higher success rates compared to the original one-corner attack. The use of data augmentation results in improved performance, with the four-corner pattern achieving over 90% success rates for poisoning percentages of 6% and higher. This performance improvement is illustrated in FIG10. The four-corner backdoor pattern ensures visibility after data augmentation, leading to stark performance differences compared to the one-corner pattern. The four-corner attack achieves high success rates with data augmentation, while the one-corner attack results in near-zero success rates across classes and poisoning percentages. The four-corner attack showed similar results to the exploratory experiment, with varying degrees of success rates across different classes. The ship class performed poorly, while other classes were more stable in their poisoning. Gaussian noise was added to increase the difficulty of examples before applying the backdoor pattern. The backdoor pattern was used to increase the difficulty of examples, with some improvement at low standard deviations but substantial performance degradation at higher standard deviations. Poisoned images with high standard deviations of Gaussian noise do not contain meaningful information about the original image label, making them easily classified correctly using the backdoor. Additional runs of the final attack on all classes were conducted, with varied random seed used in training. Results comparing the performance of different attacks are presented. The GAN-based interpolation attack and adversarial perturbation-based attack were compared with the adversarial examples-based attack, which outperformed the other two attacks. The GAN-based attack showed some improvement over the baseline but not as much as the adversarial examples-based attack. Examples of image interpolation using a GAN were shown, with increasing distortion as the degree of interpolation increased. The right image in each pair is a perturbed version of the original CIFAR-10 dataset image, created using 2-norm adversarial perturbations with a bounded \u03b5 = 600 and a reduced amplitude backdoor pattern applied with an amplitude of 16. The poisoned images may not change enough to appear mislabeled."
}