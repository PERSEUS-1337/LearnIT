{
    "title": "BygMAiRqK7",
    "content": "Building on the success of deep learning, Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs) are two modern approaches to learning a probability model of observed data. VAEs compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function, while GANs minimize a distance between observed and generated probability distributions without an explicit model for the data. An optimal transport GAN with entropy regularization can be seen as a generative model that maximizes a lower-bound on average sample likelihoods, similar to VAEs. Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs) are two modern approaches to learning generative models. VAEs compute a generative model by maximizing a variational lowerbound on average sample likelihoods using an explicit probability distribution for the data, while GANs learn a generative model by minimizing a distance between observed and generated data. The importance of learning generative models is evident in various fields such as self-driving cars, robotics, natural language processing, domain transfer, and computational biology. In this paper, the limitations of GANs in computing sample likelihoods and posterior distributions of latent variables are addressed by providing a theoretically justified approach to compute sample likelihoods using GAN's generative model. This can open new possibilities for using GANs in statistical inference problems and massive-data applications. The main results of the study involve using GANs for model selection, sample selection, and hypothesis testing. The optimal generator and coupling between observed and latent variables are computed through training. The likelihood of a test sample is lower-bounded using three terms. The study focuses on using GANs for model selection, sample selection, and hypothesis testing. The optimal transport GAN minimizes an objective with a loss function and entropy regularization to maximize sample likelihoods. The OT GAN simplifies to WGAN or W2GAN based on the loss function used. The study explores the statistical justification for GAN's optimization, highlighting the connection between GANs and VAEs. The entropy regularization in GANs aims to improve computational aspects, while the coupling between observed and latent variables resembles the encoder-decoder framework in VAEs. Recent works have also investigated the relationship between GANs and VAEs. In recent works, GANs are seen as performing variational inference on a generative model in the label space. Different from previous methods, our approach considers an explicit probability model for data and shows that the entropic GAN objective maximizes a variational lower bound, allowing for sample likelihood computation similar to VAEs. This is relevant to the use of annealed importance sampling in evaluating the approximate likelihood of decoder-based generative models. Our approach connects GANs and VAEs by constructing a model for data distribution in GANs, enabling sample likelihood estimation. It addresses estimating the likelihood of new samples in GANs and computing sample likelihood estimates. Using an explicit probability model, we can lower-bound the likelihood of a sample in GANs, similar to VAEs. The optimal generator and coupling between variables can efficiently compute this likelihood, with approximations for different GAN architectures. The log likelihood of a new test sample y test can be lower-bounded by the distance to the generative model. This result combines three components: the distance between y test and the generative model, the entropy of the coupled latent variable, and the likelihood of the coupled latent variable. These components are illustrated in FIG2. In Section 3, computational methods for GANs and entropic GANs are explained, while Section 4 presents numerical experiments on benchmark datasets. GAN aims to find a generator function G that generates samples similar to real-data random variable Y. The generator is optimized by minimizing the distance between the observed distribution and the generative one. The paper focuses on GANs based on optimal transport distance, using measures like Wasserstein distance. The optimal transport GAN is formulated through specific optimization techniques. The optimal transport GAN formulation involves a min-min optimization with a non-smooth objective, which can be computationally expensive. To improve efficiency, a regularization term can be added to make the objective strongly convex. The entropic GAN formulation includes a negative Shannon entropy as a common regularization term. The entropic GAN formulation involves solving an optimization problem using two approaches: an iterative method and a min-max formulation. The latter, known as GAN formulation, maximizes a lower bound on average sample likelihoods using an explicit probability model. The entropic GAN maximizes a lower bound on sample likelihoods according to a well-defined normalization for a given X = x. The proof of this theorem is presented in Section A and is similar to VAEs, where a generative model is computed by maximizing a lower bound on sample likelihoods. Having a shift invariant loss function is critical for this theorem, making the normalization term independent from G and x. Standard OT GAN loss functions like L2 for WGAN satisfy this property. The entropic GAN aims to maximize a lower bound on sample likelihoods with a well-defined normalization for a given X = x. The optimal solutions can be efficiently computed, and the likelihood of a new observed sample can be approximated using the explicit probability model. Dual formulations for OT GAN and entropic GAN optimizations are discussed in this section, involving min-max optimizations. The dual formulations for OT GAN and entropic GAN optimizations involve min-max optimizations over the generator and discriminator function classes. Local search methods like alternating gradient descent are commonly used to find solutions. The optimization for OT GAN can be simplified using Kantorovich duality, providing a general formulation for OT GANs. If the loss function is 2, the optimal transport distance is known as the first order Wasserstein distance. The first order Wasserstein distance is the optimal transport distance in OT GAN. The optimization simplifies to min DISPLAYFORM4, known as Wasserstein GAN or WGAN. The OT GAN with quadratic loss is called quadratic GAN or W2GAN. The entropic GAN dual formulation can be written as a different optimization. The hard constraint in optimization 3.1 is replaced by a soft constraint in optimization 3.2, allowing for efficient computation of optimal coupling between real and generative. The entropic GAN provides an efficient way to compute optimal coupling between real and generative variables using the optimal generator and discriminators. This computational benefit is highlighted in experimental validations, aiming to compute sample likelihoods in GANs for statistical inference applications. The likelihood of a test sample can be lower-bounded with a trained entropic WGAN, offering valuable insights for various statistical analyses. In our implementation, we use the algorithm from BID24 to train the Entropic GAN, which provides a good approximation of stationary solutions. To compute surrogate likelihood estimates, we need to calculate the density P * X Y =y test (x). WGAN with entropy regularization offers a closed-form solution to the conditional density of the latent variable. If G * is injective, P * X Y =y test (x) can be obtained from the equation by a change of variables. In cases where multiple x can produce the same y test, a solution satisfying the equations can be found. One solution that satisfies equations 4.1 and 4.2 is DISPLAYFORM2. To compute the surrogate likelihood, samples are drawn from the distribution P * X Y =y test (x). Using a different estimator due to poor performance of MCMC in high dimensional X, we present our sampling procedure in Alg. 1. In the experiments, an entropic WGAN is trained on the MNIST dataset. 1,000 samples are randomly chosen from the test-set to compute surrogate likelihoods using Algorithm 1 at different training iterations. Discriminators D1 and D2 are updated before computing the likelihoods. Sample likelihoods are expected to increase as the generative model improves. Experiments are conducted across different datasets. An entropic WGAN is trained on a subset of samples from the MNIST dataset containing digit 1. Likelihood estimates are computed for samples from the entire MNIST dataset and the SVHN dataset. The likelihood distribution for the MNIST dataset is bimodal, with one mode peaking for digit 1 samples and another mode for the rest of the digits. The SVHN dataset has much smaller likelihoods due to its significantly different distribution from MNIST. The likelihood distribution of SVHN samples is more diverse with varying backgrounds and styles compared to MNIST. SVHN samples with high likelihood estimates are similar to MNIST digits, while those with low scores are different. Standard GAN architectures lack entropy regularization, but likelihood lower bounds hold with optimal coupling and generator from GAN training. Computation of optimal coupling is feasible with a quadratic BID3 loss function. The optimal coupling between Y and \u0176 in GAN architectures can be approximated by coupling Y = y test with a single latent sample x. This heuristic considers the likelihood of the latent variable and the distance between y test and the model. Likelihood estimates for various GAN architectures can be computed using this approach, as demonstrated with CIFAR-10 and LSUNBedrooms datasets. In this study, GANs were trained on CIFAR-10 and LSUN datasets, with likelihood estimates showing differences between datasets. The Office dataset exhibited higher likelihood values than MNIST, indicating similarity to CIFAR. The entropic GAN optimization was found to maximize a variational lower-bound on average log-likelihoods, connecting GANs with VAEs. Our study connects GANs with VAEs by constructing an explicit probability model for GANs based on log-likelihoods. The likelihood surrogate we propose can approximate the true likelihood function well across various datasets. This statistical framework can be applied in different statistical inference tasks, such as evaluating GAN architectures, quantifying domain shifts, and detecting outlier samples. The generator class balances bias vs. variance, detects outlier samples, and can be used in statistical tests like hypothesis testing. Using Baye's rule, the log-likelihood of an observed sample y can be computed. Joint density function P X,Y with matching marginal distributions P X and P Y is considered. Expectation of both sides with respect to distributions P X Y =y and P Y is taken, leading to an equation involving the Shannon-entropy function. Expectation over f X is used instead of P X, and the KL divergence is discussed. The Entropic GAN objective maximizes a lower-bound on average sample log-likelihoods, similar to VAEs. It aims to understand the tightness of the lower bound for some generative models, providing a connection between these two areas. The Entropic GAN objective aims to understand the tightness of the lower bound for generative models by quantifying the approximation gap, which is the KL divergence between the real data distribution and the likelihood surrogate function. Experimental results show that the approximation gap is very small for linear generative models with a quadratic loss function. Results in TAB0 are consistent with the presented results. Using Bayes rule, we have a closed-form for f X Y, allowing for efficient computation of KL P X Y =y f X Y =y. An entropic GAN with a linear generator and non-linear discriminators is trained on a randomly chosen matrix G to generate Y. P X Y =y is computed using equation 4.3. Average surrogate log-likelihood values and approximation gaps are reported in TAB0, showing the small approximation gap compared to log-likelihood values. Density functions of P X Y =y and f X Y =y are demonstrated in Figure 4, indicating a very small approximation gap. Architecture and hyper-parameter details are also provided. The generator network used 3 linear layers without non-linearities, while the discriminator architecture consisted of 2-layer MLP with ReLU non-linearities. Both were trained using the Adam optimizer with specific hyperparameters. Optimal coupling for the W2GAN can be computed using the gradient of the optimal discriminator. A modified version of the entropic GAN has shown improved computational properties. The modification of the entropic GAN is explained in this section, introducing the Sinkhorn loss function. Optimizing the GAN with the Sinkhorn loss is equivalent to optimizing the entropic GAN, allowing the likelihood estimation framework to be used with models trained using Sinkhorn loss. This is important for practical training purposes. Training models with Sinkhorn loss is more stable in practice. WGANs with entropic regularization are trained using alternating optimization. The smoothness introduced by the entropic regularizer is considered, and the generator problem is solved using first-order methods. Algorithm 1 of BID24 is used to train the GAN model in experiments on the MNIST dataset. Discriminator and Generator architectures are provided in Tables 2 and 3. Dual formulation of GANs involves two discriminators - D1 and D2. The dual formulation of GANs employs two discriminators - D1 and D2 with the same architecture. Trained models include a DCGAN on CIFAR dataset and a WGAN on LSUN-Bedrooms dataset. Hyperparameter details are provided in tables, and sample generations are shown in figures."
}