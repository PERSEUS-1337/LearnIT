{
    "title": "HylwpREtDr",
    "content": "Graph Neural Networks (GNNs) are gaining attention for prediction tasks on graphically structured data. Active learning with GNNs for node classification is investigated in this paper, proposing a method using node feature propagation and K-Medoids clustering for instance selection. Experimental results show that this approach outperforms other baseline methods on benchmark datasets. The proposed method using Graph Neural Networks (GNN) outperforms baseline methods in supervised and semi-supervised learning scenarios. GNN frameworks effectively fuse node feature representations and connectivity information, but there is a need to enhance learning efficiency with limited annotated nodes, especially in biological problems involving chemical structures. Active Learning (AL) provides solutions for selecting \"informative\" examples as the initial training set for Graph Neural Networks (GNN). While various methods have been proposed for active learning on graphs, active learning for GNN has received relatively little attention. Cai et al. (2017) and Gao et al. (2018) are two major works that study active learning for GNN, using three metrics to evaluate training samples. The papers by Cai et al. (2017) and Gao et al. (2018) evaluate training samples using uncertainty, information density, and graph centrality metrics. While the first two metrics rely on GNN representations, they may not be informative with limited label budgets or under-trained network weights. In contrast, graph centrality overlooks node features. Existing methods combine scores linearly, lacking a solution to these issues. Our proposed method for GNN selects nodes based on propagated node features, addressing these limitations. Our algorithm utilizes node features propagated through the graph structure, enhancing robustness to inaccuracies in model representation. We cluster nodes using K-Medoids clustering, which constrains centers to real nodes in the graph. Theoretical analysis and practical experiments demonstrate the strength of our method, outperforming Coreset and other active learning methods on benchmark datasets. Active Learning (AL) involves selecting data points interactively to improve model performance. Sener & Savarese (2017) proposed a Coreset method for neural networks, while earlier works focused on AL with graph-structured data using non-parametric classification models. Recent studies have explored active sampling in graph signal processing, primarily in denoising scenarios with noisy labels. Graph Neural Networks (GNNs) are a popular framework for modeling graph-structured data, utilizing a multi-layer paradigm with message passing for neighborhood aggregation. Recent studies have shown the effectiveness of GNNs in Active Learning (AL) settings. In the AL setting, GNNs have been effectively used. Different approaches have been proposed, such as linearly combining uncertainty, graph centrality, and information density scores. This paper focuses on clustering propagated node features for active learning in the node classification setting on large graphs. In the inductive learning setting for GNN, a loss function is used to minimize loss over the inputs for a model that maps graph G and features X to a prediction vector \u0176. An active learning algorithm selects subsets of nodes in each step to reveal labels, aiming to minimize the overall loss on the graph. Graph Neural Networks aim to minimize loss under a given budget by training a model using active learning strategies on a training set. The focus is on Graph Neural Networks and their multi-layer feature propagation process, similar to Multi-Layer Perceptrons. Graph Neural Networks (GNNs) use input node features X (k) and X (0) \u2208 R n\u00d7d. Different GNNs define the recursive function f for the next-layer representation with parameter \u0398 k. Graph Convolution Network (GCN) has a specific form of function f using ReLU activation function, parameter matrix \u0398 k, and normalized adjacency matrix S. The adjacency matrix A and diagonal degree matrix D are used to update node embeddings by aggregating neighbors. The identity matrix I acts similarly to residual links. Graph Convolution Network (GCN) utilizes self-loops to connect nodes in a multi-layer fashion, encouraging locally related nodes to share similar embeddings. For classification tasks, a linear transformation and softmax function are stacked in the final layer to predict class scores. The GCN structure is used as a unified model for active learning strategies. Traditional active learning algorithms label one instance at a time, but modern datasets may require labeling multiple instances simultaneously. The text discusses the use of Graph Convolution Network (GCN) for active learning strategies, focusing on batched one-step active learning. It introduces a node selection algorithm and compares it with a related algorithm. The framework uses distance-based clustering and aims to select the most representative nodes for labeling. The text discusses an active learning framework using distance-based clustering in Algorithm 1. It involves computing a distance matrix using node feature representations and graph structure, applying clustering with b centers, selecting nodes closest to the center of each cluster, and training a graph neural network (GCN) for node classification based on the received labels. Different options for the steps can impact downstream prediction tasks. Previous methods commonly use network representations to compute distance, but the text justifies different choices. The text discusses using distance-based clustering in an active learning framework. It proposes defining pairwise node distance using the L2 norm of propagated node features, removing the effect of untrained parameters. The K-Medoids clustering method is suggested over K-Means and K-Center for selecting real sample nodes as cluster centers, crucial for active learning. FeatProp is a method that utilizes active learning through node feature propagation in a graph. It approximates pairwise distances between nodes in a simplified GCN structure and shows that using K-Medoids with propagated features can lead to a low classification loss. Using K-Medoids with propagated features can result in a low classification loss. The classification loss of A 0 on the entire graph G is mainly dependent on the K-Medoids loss, which can be approximated using algorithms like Partitioning Around Medoids (PAM). The assumptions in Theorem 1 are standard in the literature. The results in Theorem 1 are detailed in the appendix and differ from Sener et al. (2017) by focusing on translated features. Using raw feature clustering for GCN may not yield the same bound due to the diffusion of raw features across nodes. Visualization in Section 5.2 provides a clearer comparison. In this subsection, the justification for using K-Medoids clustering over Coreset is provided. K-Medoids can achieve a better bound than K-Center, as shown in Figure 1. Experimental results also demonstrate the superior performance of K-Medoid clustering. The node classification performance of our selection method is evaluated on various network datasets. Budget sizes of 10, 20, 40, 80, and 160 nodes are chosen, and a two-layer GCN with 16 hidden neurons is trained for prediction. The Adam optimizer is used with specific hyperparameters, and the model is evaluated after 200 epochs. The model trained after 200 epochs is used to evaluate the metric on the whole set. Different methods were compared for node selection, including Random, Degree, Uncertainty, Coreset, and AGE. These methods involve various techniques such as choosing nodes uniformly, based on degrees, entropy, clustering, and combining multiple metrics. FeatProp is a method that combines graph centrality, information density, and uncertainty metrics to select nodes with the highest scores. It involves performing K-Medoids clustering on propagated features and starting with a small set of nodes sampled uniformly at random. The classification accuracy is averaged over 5 different random seeds, and the accuracy is plotted against the number of labeled points. Our method outperforms baseline methods in various settings, with AGE and ANRMAB showing better performance than Uncertainty. The method also has the second smallest standard deviation, indicating consistency in node selection. Our method outperforms baseline methods in various settings, with AGE and ANRMAB showing better performance than Uncertainty. The method also has the second smallest standard deviation, indicating consistency in node selection. Comparing time expenses, CoresetMIP takes significantly longer than Coreset-greedy, with similar performance on Citeseer but Coreset-greedy outperforming CoresetMIP on Cora. This difference in performance between graph node classification and traditional classification tasks is noteworthy. After outperforming baseline methods in various settings, the study explores the impact of distance function and clustering subroutine selection on FeatProp algorithm for graph node classification. Comparing different approaches, the K-Medoids choice with a K-Center replacement shows lower accuracy than the original FeatProp approach, attributed to tighter bounds on classification loss. This observation aligns with the analysis in Section 4.3, highlighting the importance of proper algorithm selection for optimal performance. The study introduces FeatProp, a node feature selection approach for Graph Convolution Networks (GCNs), which shows deteriorated performance compared to K-Center in terms of classification loss. The effectiveness of FeatProp is tested on other GNN frameworks like Simplified Graph Convolution (SGC), with similar observations. Detailed results are provided in the appendix. FeatProp is a node feature selection approach for Graph Convolution Networks (GCNs) that outperforms state-of-the-art AL methods on benchmark datasets. The method focuses on sampling representative points in a graph representation, while uncertainty-based methods select active nodes based on labels. Combining these methods with FeatProp remains an open problem. Assuming a ground truth GCN predicts class probabilities, A0 achieves high confidence on trained samples and low confidence on unseen samples. The loss function is Lipschitz with constant \u03bb and bounded in [\u2212L, L]. This assumption is widely used in DL theory. The text discusses assumptions in deep learning theory, including the sum of input weights of neurons being less than a constant \u03b1 and the ReLU function activating with probability 1/2. These assumptions are used to prove Theorem 1. The text discusses the proof of Theorem 1 under Assumptions 1-4, showing the expected classification loss of model A t satisfies certain conditions. The proof involves analyzing the terms in the loss function and utilizing properties of ReLU activation. The text discusses the proof of Theorem 1 under Assumptions 1-4, showing the expected classification loss of model A t satisfies certain conditions. It involves analyzing terms in the loss function and utilizing properties of ReLU activation. Regarding randomness in \u03c3 (ReLU) in M * , the algorithm A chooses a set s 0 to label, samples y j \u223c \u03b7(j) for j \u2208 s 0 to train model A 0 , and then samples y i \u223c \u03b7(i) to obtain loss l(A 0 |G, X, Y ). The text discusses the proof of Theorem 1 under Assumptions 1-4, showing the expected classification loss of model A t satisfies certain conditions. It involves analyzing terms in the loss function and utilizing properties of ReLU activation. The algorithm A chooses a set s 0 to label, samples y j \u223c \u03b7(j) for j \u2208 s 0 to train model A 0, and then samples y i \u223c \u03b7(i) to obtain loss l(A 0 |G, X, Y). The proof involves using Hoeffding's inequality and the max of distances instead of averaging."
}