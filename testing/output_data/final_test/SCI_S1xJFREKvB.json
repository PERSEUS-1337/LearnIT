{
    "title": "S1xJFREKvB",
    "content": "Stochastic Gradient Descent (SGD) with Nesterov's momentum is a widely used optimizer in deep learning, known for its excellent generalization performance. However, its large stochasticity makes it less robust. Amortized Nesterov's Momentum is proposed as a more robust variant with faster convergence and higher efficiency. Experimental results show similar or better generalization performance with minimal tuning. In the convex case, optimal convergence rates are provided, and theorems explain empirical results. Stochastic Gradient Descent (SGD) with Nesterov's momentum is a popular optimizer for neural network training, known for achieving substantial speedups. The momentum technique plays a key role in good generalization performance, especially for classification tasks. Adaptive methods are also becoming increasingly important in this field. In this work, Amortized Nesterov's Momentum is introduced as a special variant of Nesterov's momentum for training neural networks. It has only one additional hyper-parameter, the amortization length, making it easy for users to choose. Extensive studies show the benefits of this new variant compared to traditional methods like SGD with momentum. The advantages of Amortized Nesterov's Momentum include improved robustness, reduced iteration complexity, boosted convergence rate in early training stages, easy tuning of hyper-parameter m, and optimal convergence rate in general convex settings. Minor drawbacks include requiring one more memory. The proposed variant of Amortized Nesterov's Momentum addresses drawbacks and benefits large-scale deep learning tasks by using momentum based on past iterates for robust acceleration. In this paper, the authors review SGD and Nesterov's momentum, discussing implementation subtleties and notations. SGD iterates with a learning rate \u03b3, while Nesterov's momentum scheme involves acceleration with constant step. Nesterov's momentum scheme involves acceleration with a constant step, widely used in deep learning. The reformulation in Sutskever et al. (2013) and Tensorflow also utilize this scheme. PyTorch and Tensorflow track values {x k} for M-SGD implementation, while accelerated convergence rate is based on {y k}. In this work, momentum is studied as an add-on to plain SGD, with Nesterov's momentum scheme showing acceleration in convergence. Results from training ResNet34 on CIFAR-10 using SGD and M-SGD are presented, indicating that Nesterov's momentum initially slows convergence but speeds it up in the end. The importance of momentum for achieving high accuracy is verified through 60 epochs, with Nesterov's momentum slightly increasing uncertainty in the training process of SGD. Full-batch loss is more informative than train-batch loss in evaluating optimizers, but it is costly to measure and thus only done on small datasets. Test accuracy reflects optimization and generalization, with convergence similar to that of full-batch loss. In comparison, M-SGD and OM-SGD final accuracies are 94.606% \u00b1 0.152% and 94.728% \u00b1 0.111% with average deviations at 1.040% and 0.634%, respectively. AM1-SGD with Options I and II correspond to M-SGD and OM-SGD, respectively, with accelerated convergence rates. Option I and Option II offer accelerated convergence rates similar to M-SGD and OM-SGD. Option I involves SGD with amortized momentum, while Option II applies tail averaging on Option I. Ma & Yarats (2019) found that M-SGD and SGD perform similarly when fixed effective learning rates are used. Implementation of Option II requires maintaining another network for the shifted point or temporarily changing network parameters during evaluation. The efficiency of the algorithm can be enhanced by maintaining a running scaled. The efficiency of Algorithm 1 can be improved by maintaining a running scaled momentum instead of the running average, resulting in lower amortized cost compared to standard PyTorch. Tuning the parameter m in AM1-SGD can impact test accuracies. The parameter m in AM1-SGD impacts test accuracies by introducing a trade-off between final accuracy and robustness. Empirical results suggest that m = 5 is a good choice for this task, as it significantly increases robustness. The amortized momentum in Figure 2b shows a large gap between Option I and SGD, where Option I injects a very large momentum 6 into SGD every m iterations. AM1-SGD injects a large momentum 6 into SGD every m iterations, providing acceleration and robustness. The algorithm differentiates from M-SGD and SGD by using amortized momentum. Large decay factor and excessive momentum can lead to performance drops, resolved by restarting the algorithm after each learning rate reduction. Algorithm 2 AM2-SGD introduces a variant of Amortized Nesterov's Momentum to address efficiency limitations of AM1-SGD. It trades memory for extensibility by storing the most recent m iterations in the point table \u03c6, resulting in an m-iterations tail average output. This connects AM2-SGD to AM1-SGD, similar to the relation between SVRG and SAGA. AM2-SGD improves efficiency by storing the most recent m iterations in a point table, leading to an m-iterations tail average output. Comparison with SVRG and SAGA methods shows that Option II outperforms Option I, making Option I not recommended. In our implementation, we sample an index in [m] at each iteration and observe larger deviations in AM2-SGD compared to AM1-SGD due to random indexes {j k}. Analyzing convergence rates for AM1-SGD and AM2-SGD in the convex case helps understand the impact of amortization (m) on convergence behaviors. The analysis also guides tuning m for optimal performance. The text discusses the setting of stochastic variants where Nesterov's acceleration achieves optimal rates for a convex composite problem. Assumptions on the regularity of functions and stochastic oracle are imposed, covering various classes of convex problems. The notation and assumptions cover important cases of convex functions, including L-smooth and Lipschitz continuous functions. The text discusses the migration of Nesterov's momentum into deep learning through reformulated schemes AM1-SGD and AM2-SGD. The momentum parameter \u03b2 is treated as a variable in convex analysis. The reformulations are equivalent to Algorithm 1 and Algorithm 2 under certain conditions. The text also mentions the use of subgradients for non-differentiable functions when M > 0. The text discusses the convergence rates for AM1-SGD and AM2-SGD after migrating Nesterov's momentum into deep learning. The proofs are provided in Appendix B.2. Theorems are established for the reformulated AM1-SGD, with conditions on variance and compactness of X. The minimum of K0(m) is at m = 1 or m = K, with m constrained in {1, ..., K}. When m = K, AM1-SGD becomes modified mirror descent SA or SGD that outputs the average of the whole history. The convergence rate is discussed in terms of full-batch loss F(x) - F(x). The trade-off between increasing m in AM1-SGD and its impact on variance \u03c3 and acceleration is discussed. Theorem 1b explains how increasing m leads to smaller deviations in full-batch loss. Empirical results in Figure 2 show faster convergence with better control on \u03c3 but slightly lowered final accuracy. Experiments in Figure 4 & Table 2 are based on full-batch loss metric. The trade-off between increasing m in AM1-SGD and its impact on variance \u03c3 and acceleration is discussed. Theorem 1b explains how increasing m leads to smaller deviations in full-batch loss. Empirical results show faster convergence with better control on \u03c3 but slightly lowered final accuracy. Training a smaller ResNet18 with pre-activation on CIFAR-10 is used as a case study for AM2-SGD convergence results. Theorem 2 for AM2-SGD introduces an additional term in the upper bound compared to Theorem 1a, due to different restrictions on the choice of m. The optimal choice of m > 1 in Theorem 2 can be messy and is not discussed here. Comparing the rates, AM2-SGD shows improved performance when using the same m. The discussion compares the rates of AM1-SGD and AM2-SGD, showing that AM2-SGD has better dependence on \u03c3 and is slightly faster. Theorems establish optimal rates for m = O(1) in the convex setting, linking AM1-SGD and AM2-SGD to Nesterov's method. The effect of m is seen as trading acceleration for variance control, impacting convergence speed. The reduced final performance in CIFAR experiments may not always occur. AM1-SGD and AM2-SGD are evaluated on deep learning tasks as alternatives for M-SGD. Option I for AM1-SGD shows slightly better performance, while Option II is chosen for consistency. Both methods use the same values for (\u03b7, \u03b2) as M-SGD. For deep learning tasks, AM1-SGD and AM2-SGD use the same values for (\u03b7, \u03b2) as M-SGD. ResNet50 and ResNet152 were trained on the ILSVRC2012 dataset with specific parameters. A language model experiment was also conducted on the Penn Treebank dataset using LSTM model with modified learning rate and momentum. In a study comparing different optimization algorithms, the authors used a constant learning rate of 30 and chose \u03b2 = 0.99 for AM1-SGD and AM2-SGD. The learning rate was reduced by half when validation loss did not decrease for 15 epochs. The best validation perplexity was achieved with \u03b7 = 2.5 for M-SGD. Results were compared with a previous study, and additional experiments were conducted for robustness and comparison with classical momentum. Amortized Nesterov's Momentum, a variant of Nesterov's momentum using past iterates, was introduced in a CIFAR-100 experiment. Two realizations, AM1-SGD and AM2-SGD, were designed with faster early convergence and comparable final performance to M-SGD. AM1-SGD is lightweight and robust, suitable for large-scale tasks, while AM2-SGD is extensible and performs well in restrictive tasks. Both methods are optimal in convex cases, like M-SGD. The proposed methods, AM1-SGD and AM2-SGD, trade acceleration for variance control and are optimal in convex cases like M-SGD. Results of ResNet18 and CIFAR-100 experiments are provided in appendices. Table 4 shows test accuracy of training ResNet34 on CIFAR-10. Increasing m speeds up convergence in the early stage, as shown in Figure 6. In the experiment, AM2-SGD with Option II outperforms Option I, indicating Option I is not robust for AM2-SGD. Comparing M-SGD and AM1-SGD with large \u03b2, M-SGD is more affected by \u03b2, while CM-SGD shows similar performance but is less stable than M-SGD. Aggregated Momentum (AggMo) combines multiple momentum buffers inspired by passive damping from physics literature. AggMo is more stable and robust compared to CM-SGD, with T = 2 giving the best performance. QHM introduces an immediate discount factor \u03bd for the momentum scheme, resulting in faster convergence for AM1-SGD, AggMo, and QHM compared to CM-SGD. In terms of convergence speed, AggMo and QHM show faster early-stage convergence, while CM-SGD achieves the highest final accuracy. AM1-SGD is more efficient than QHM and AggMo, and as efficient as CM-SGD. Despite worse generalization performance, QHM and AggMo perform better in reducing train-batch loss. When \u03b2 is large, a performance drop is observed with a step learning rate scheduler. Restarting after each learning rate reduction resolves this issue. In experiments with different learning rate reduction strategies, a performance drop is observed with a large \u03b2 value. Restarting after each reduction resolves this issue. AM1-SGD and AM2-SGD perform well initially but have slightly lower final accuracies compared to M-SGD. SGD reduces train-batch loss quickly, but final accuracies are slightly lower compared to M-SGD. DenseNet model may be overfitting with M-SGD. AM1-SGD and AM2-SGD have lower STD error than M-SGD when m = 1. Maintaining iterates without scaling is more stable numerically. Without scaling, AM1-SGD and AM2-SGD have lower STD error than M-SGD when m = 1. The reformulated AM1-SGD and AM2-SGD eliminate the sequence {z k} and use inner loops similar to SGD. The reformulated schemes are based on Auslender & Teboulle (2006) and Nesterov. Intuition for the Auslender & Teboulle (2006) scheme can be found in Lan (2012). The reformulated schemes based on Auslender & Teboulle (2006) and Nesterov aim to accelerate iterations. Lemma 1 provides a cornerstone for convergence proofs of AM1-SGD and AM2-SGD. If \u03b1(1 \u2212 \u03b2) < 1/L, the update scheme satisfies a specific recursion. This lemma is also discussed in Lan (2012) and Ghadimi & Lan (2012) for completeness. The proof presented here utilizes convexity assumptions to bound various terms. Young's inequality is used to derive inequalities for different terms, leading to a final conclusion based on the chosen parameters. The discussion also involves the optimality condition of a proximal operator and the use of subgradients. The overall analysis relies on specific choices of parameters to ensure convergence. The proof utilizes convexity assumptions and Young's inequality to derive inequalities for different terms, leading to a final conclusion based on chosen parameters. The analysis involves the optimality condition of a proximal operator and the use of subgradients, ensuring convergence with specific parameter choices. The discussion in this section sheds light on the experimental results and reveals connections between AM1-SGD and Katyusha. Katyusha momentum acts as a \"magnet\" within an epoch of SVRG updates, stabilizing the iterates to enhance Nesterov's momentum effectiveness. Katyusha momentum stabilizes iterates to enhance Nesterov's momentum effectiveness and reduce variance in SVRG. The construction of Katyusha is similar to AM1-SGD, suggesting that amortized momentum can also reduce variance in SGD. However, a guaranteed reduction in variance in the worst case is not ensured. The Katyusha momentum stabilizes iterates to enhance Nesterov's momentum effectiveness and reduce variance in SVRG. The scheme presented eliminates the sequence {z k} and injects Katyusha momentum every m iterations, using Nesterov's momentum in the inner loops. By replacing the SVRG estimator and adjusting parameters, the scheme transforms into AM1-SGD (Algorithm 1). The scheme discussed involves adding amortized momentum to M-SGD by tuning the ratio of Nesterov's momentum and amortized momentum. However, the complexity increase is not considered worthwhile. A recent work shows that using only Katyusha momentum can lead to optimal rates and simplified algorithms. This scheme is structurally similar to AM1-SGD and is equivalent to the PyTorch formulation. The experiments in Ma & Yarats (2019) suggest that momentum's benefit comes from using sensible learning rates. In convex analysis, optimal convergence rate is achieved with a large effective learning rate. Setting \u03b1 = 2/3L for both M-SGD and SGD results in O(1/K) convergence rate for GD. Nesterov's method with \u03b2k = k/(k+2) has a different convergence rate. In the context of Ma & Yarats (2019), Nesterov's method achieves acceleration by using a large effective learning rate, breaking the 1/L limitation. This allows for stability and prevents potential divergence, acting as a \"stabilizer\". In a basic case study with ResNet34 on CIFAR-10, aligning the effective learning rate and setting \u03b3 = 1.0 for SGD improves final accuracy but results in highly unstable and non-robust performance, with 2.205% average STD of test accuracy over 5 runs. The significance of QHM (Ma & Yarats, 2019) lies in its ability to achieve improved performance with suitable tuning. Our work uses the convergence behavior of SGD as a reference to understand the features of our proposed momentum, setting \u03b3 = \u03b7. We used a step learning rate scheduler with a decay factor of 10 for experiments on CIFAR-10 and CIFAR-100. In ImageNet experiments, we tested ResNet50 and ResNet152 with a batch size of 256 and a starting learning rate of 0.1. The model used a batch size of 256, with a learning rate starting at 0.1 and decaying by a factor of 10 every 30 epochs. Weight decay with a rate of 0.0001 was applied during training. Data augmentation included random 224-pixel crops and horizontal flips with 0.5 probability. Experiments were run on 8 NVIDIA P100 GPUs for 90 epochs. The LSTM model had 3 layers with 1150 hidden units each, an embedding size of 400, and other default hyperparameters. The model's configuration includes dropout probabilities for various layers, weight drop, regularization values, and weight decay."
}