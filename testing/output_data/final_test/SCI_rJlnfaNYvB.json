{
    "title": "rJlnfaNYvB",
    "content": "Mixed precision training (MPT) is a technique that improves the speed and energy efficiency of training deep neural networks by leveraging fast hardware support for half-precision floating point. It is used with loss scaling to minimize numerical underflow during backpropagation. Adaptive loss scaling introduces layer-wise loss scale values that are automatically computed during training, eliminating the need to tune a model-specific loss scale hyperparameter. Our approach introduces layer-wise loss scale values computed during training to improve efficiency in training deep neural networks compared to existing methods. It aims to shorten convergence time and enhance accuracy by utilizing hardware-friendly numerical representations. Using FP16 for training can reduce memory footprint by half compared to FP32, improving runtime performance and power efficiency. However, numerical issues like overflow, underflow, and rounding errors are common in low precision training. Underflow rate is calculated by counting absolute gradients below 2^-24, while expected loss scale values are computed for each layer during training. Loss scale values for each layer are calculated based on the smallest absolute gradient, allowing for proper representation in FP16 training. Tuning the loss scaling parameter \u03b1 is necessary for optimal results, as it varies across different models. Loss scale values for each layer are calculated based on the smallest absolute gradient, allowing for proper representation in FP16 training. Tuning the loss scaling parameter \u03b1 is necessary for optimal results, as it varies across different models. Its value must be chosen large enough to prevent underflow issues from affecting training accuracy, but if chosen too large, it could lead to rounding errors or overflow. Different values of \u03b1 can result in varying ResNet-50 MPT convergence behavior due to the sensitivity to the choice of loss scale. Gradients can vary between layers and iterations, requiring a different scale for each. This variability, along with the time spent tuning \u03b1, can lead to longer training times for MPT compared to regular FP32 training. An adaptive loss scaling method is introduced to address these challenges. Loss scaling-based training method called adaptive loss scaling introduced for MPT to utilize existing hardware with fast FP16 operations. Improves usability by removing the need to tune model-specific loss scale hyperparameter while maintaining accuracy. Layer-wise loss scale values automatically computed and updated during training to handle underflow effectively. Experimental results show adaptive loss scaling achieves best model accuracy and shortest training time for deep models on large datasets. MPT utilizes FP16 for activations and gradients, with FP32 used for precision. FP16 has a limited dynamic range, causing numerical issues with gradients. Loss scaling addresses underflow, overflow, and rounding errors. Each layer has a single output, with specific functions for output and gradient calculation. The text discusses the use of loss scaling in a neural network training algorithm to address numerical issues with gradients in FP16 precision. The algorithm involves scaling the initial error gradients and weight updates by a factor \u03b1 to prevent underflow and overflow issues during training. The goal is to choose \u03b1 large enough to prevent underflow but small enough to prevent overflow. Many recent works focus on reducing rounding error to improve training performance. Wang et al. (2018) propose a chunk-based accumulation mechanism to mitigate the swamping issue, while Sakr et al. (2019) suggest finding lower precision for accumulation through variance analysis. Hoffer et al. (2018) address numerical issues caused by batch normalization by proposing a more stable alternative. These methods are independent of loss scaling. Loss scaling is a method to improve mixed precision training by reducing underflow in computed gradients. Two improved versions are proposed: backoff, which adjusts the loss scale if a numerical error occurs, and logmax, which estimates the proper loss scale value based on the maximal absolute gradient value. However, these solutions are not ideal as backoff is trial-and-error and logmax is risky to use. The backoff method for 8-bit floating point is being studied due to its trial-and-error nature and potential waste of training workload. An adaptive loss scaling approach is demonstrated in a 3-layer Multi-Layer Perceptron, where gradients are scaled before being consumed by the last linear layer to prevent the choice of scaling factor from affecting gradient magnitudes. This approach differs from the standard loss scaling method. In the adaptive loss scaling method, each layer calculates its own local loss scale value based on output gradients and weights statistics to prevent underflow in computed input gradients. This local scale value is used to scale weights before computing input activation gradients for that layer. The approach aims to minimize underflow in each layer by using layer-local loss scales \u03b2 i, which are automatically computed based on current layer statistics. This eliminates the need for model-specific hyperparameter tuning and enables layer-wise loss scaling. The 2-tuple notation \u03b1 i, \u03b4 i denotes the loss scale value and scaled gradient for layer i in a N-layer MLP. The approach minimizes underflow in each layer using layer-local loss scales \u03b2 i, computed based on current layer statistics. Algorithm 3 describes adaptive loss scaled backpropagation for single-output layers, with details on handling multiple outputs in Section 3.2.2. Starting with error gradients \u03b4 N +1 from the output loss value for the last layer N + 1, each previous layer i is visited in reversed topological order to update weights and produce \u03b1 i, \u03b4 i for the previous layer i \u2212 1. In a reversed topological order, the algorithm computes layer-wise loss scales \u03b2 i based on current layer statistics. These scales are used to scale error gradients before calculating activation gradients and updating weights. The process involves computing \u03b1 j, \u03b4 j for each layer i, and scaling gradients with \u03b1 j before weight updates. The loss scale value \u03b4 i is calculated as \u03b1 j \u03b2 i to pass to the next previous layer. This method supports general DNN operations like GEMM and element-wise operations. The GEMM computation for linear layers involves input activations X, weights W, and output activations Y. The backward pass includes weight gradient computation using normal random variables w and g for W and \u03b4, respectively. In weight initialization and low-precision floating-point training, W and \u03b4 are characterized by normal random variables. The product p = wg follows a zero-mean normal distribution. To reduce underflow, the probability of p being less than a threshold u is minimized by scaling w or g. The algorithm calculates the lower bound of loss scale for each GEMM-based layer by using a threshold for the probability of underflow for p. This is achieved by scaling either w or g before multiplication, with the expected outcome of loss scaling determined by the half-normal distribution of |p|. The lower bound term is taken as the loss scale value in practice, with details on the corresponding upper bound provided in Section 3.2.3. The computation of \u03c3 p requires statistics of w and g, necessitating the calculation of sample mean and variance of W and \u03b4. In the current implementation, statistics for loss scale calculation are computed on GPU and then transferred to CPU. For FP16-based training, u is set to 2^-24 and T uf is set to 1.0 x 10^-3. Element-wise operations can be unary or binary, including batch normalization. Loss scale update is not done for these operations. Element-wise operations like branching in networks with skip connections require special treatment, especially when gradients with different loss scales need to be summed during backpropagation. This ensures the correct computation of output gradient's loss scale for subsequent layers. Our solution addresses the issue of computing output gradient's loss scale in networks with skip connections. Input gradients are rescaled before summation to maintain correct gradient magnitudes. The rescaling process involves adjusting gradients based on the maximum loss scale among input tuples. If necessary, gradients are adjusted to prevent overflow. Raw loss scale values are rounded down to the nearest power-of-two number to ensure accuracy. The upper bound of layer-wise loss scale is determined by avoiding overflow in the GEMM case, calculated by choosing maximal numbers from operands and taking u max over the multiplication result as the largest possible loss scale. This loose bound is much larger than the lower bound, with the lower bound chosen as the loss scale value in practice. Only switch to the upper bound when it is smaller than the lower bound. Other operators do not update loss scale, except for branching. This section demonstrates the benefits of adaptive loss scaling in computer vision tasks using various experiments with different model topologies. The approach was implemented in Chainer v6.1.0, comparing it to FP32 training, fixed loss scaling, and dynamic loss scaling by backoff. Evaluation was done on CIFAR-10/100 image classification with ResNet models of varying depths. The study compares different loss scaling options for ResNet models trained on CIFAR-10/100, including fixed and adaptive scaling. Results show that adaptive scaling is not very beneficial for ResNet-20 and 56 due to overfitting, but it improves performance for ResNet-110 due to its depth. In comparing loss scaling methods for ResNet models, adaptive scaling outperforms fixed scaling in terms of training time efficiency. Results on ILSVRC2012 show that adaptive scaling performs the best among all training results, including fixed loss scaling and dynamic loss scaling using the backoff strategy. Loss scaling with a fixed scale of 128 reduces model test accuracy for ResNet-50 compared to no loss scaling. Adaptive loss scaling shows no hassle with hyperparameters. The calculated loss scale for ResNet-18 is much smaller than 128, indicating that 128 may cause rounding errors. A comparison of gradient standard deviations between adaptive and fixed scaling shows a 20 times higher ratio for fixed scaling, potentially leading to accuracy drops due to increased accumulation error. The Single-Shot Detector model (SSD512) with a VGG-16 backbone and 512 input resolution is used as the baseline for object detection training. SSD512 model with VGG-16 backbone and 512 input resolution is challenging for mixed-precision training due to its interleaved batch normalization layers and multi-branched topology. Adaptive loss scaling is effective in mitigating underflow, as shown in Figure 4. SSD512 cannot be properly scaled by a fixed loss scale, leading to varied test performance based on different loss scaling methods. The performance of SSD512 models trained with different loss scaling methods is compared in terms of mAP (%). Results show that fixed and dynamic loss scaling methods have varying effects on stability and performance. Dynamic loss scaling performs poorly with small batch sizes, while adaptive methods show promise in addressing these challenges. Our adaptive method for calculating layer-wise loss scale during runtime outperforms the FP32 baseline, reducing underflow rates significantly. The speed overhead of computing loss scale is minimal, with an update frequency of 100 iterations resulting in only 0.27% overhead. Compared to fixed loss scaling, our approach reduces total training time and improves performance in MPT. Our adaptive method for calculating layer-wise loss scale during runtime outperforms the FP32 baseline, reducing underflow rates significantly. It works better than existing loss scaling methods and even FP32 in some cases, improving model accuracy and convergence time. Future work includes evaluating adaptive loss scaling on different tasks and models, finding tighter upper bounds for each layer, and extending it to FP8. A detailed analysis on CIFAR results shows benefits for training ResNet-110, while being less advantageous for shallower models like ResNet-20 and ResNet-56. The underflowing ones is moderate and can be seen as a form of regularization. Training accuracy of ResNet models on CIFAR can reach 100%. Adaptive loss scaling may improve computed gradients accuracy but not always test accuracy. Test accuracy of shallower ResNet models on CIFAR-10 fluctuates before stabilizing. Initial low test accuracy is attributed to high underflow rate, mitigated by loss scaling. The relationship between loss scale and test accuracy is complex, with underflow acting as a form of regularization. However, as gradients become more accurate, the regularizing effect decreases, leading to a drop in test accuracy until a loss scale of around 128. Increasing the loss scale can lead to high rounding error and swamping issues, which can harm test accuracy. Empirical evaluation shows that the relationship between loss scaling schemes and test accuracy is intricate, especially when the model tends to overfit. Changing fixed loss scales can impact SSD training results, highlighting the benefits of adaptive loss scaling for improving both training time and model accuracy."
}