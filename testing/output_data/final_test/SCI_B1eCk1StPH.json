{
    "title": "B1eCk1StPH",
    "content": "Pruning neural network parameters can compress models and prevent overfitting. Various pruning approaches increase test accuracy despite reducing parameter counts. Pruning's effect on generalization depends on the instability it generates rather than the final model size. Pruning unimportant parameters can still cause instability, similar to regularizing by injecting noise. This mechanism explains the generalization improvements seen in over-parameterized models. Pruning weights and filters from deep neural networks can reduce parameters while maintaining accuracy, enabling broader applications. Many pruning methods improve generalization by preventing overfitting to small datasets. Concerns about over-parameterization have lessened due to recent studies showing the benefits of pruning. Recent studies have found that adding parameters to deep neural networks can actually reduce the generalization-gap. Experiments and bounds on generalization-gaps suggest less overfitting with increased parameter counts. This has implications for neural network pruning, raising questions about how pruning improves generalization despite larger parameter counts not increasing overfitting. Pruning neural networks involves reducing parameters during training to improve generalization. Tradeoffs exist between stability and potential benefits of pruning, with iterative pruning resembling regularization with noise. Different pruning algorithms can lead to instability and higher generalization. Various approaches to pruning neural networks exist, including post-hoc methods. Pruning neural networks involves reducing parameters to improve generalization. Different pruning methods identify unimportant parameters, with magnitude pruning using small-magnitude values. Pruned models show enhanced generalization, framing pruning as a regularization approach. Variational Bayesian approaches and VC dimension motivate pruning in neural networks to improve generalization by reducing model complexity and overfitting. The idea of flat minima and sharp minima also play a role in pruning strategies. Iterative DNN pruning may improve generalization by creating noisy versions of internal data representation, similar to noise-injection regularization. Dropout introduces noise by setting random subsets of outputs to zero, promoting robustness to post-hoc pruning of features. Iterative DNN pruning introduces noise to improve generalization, targeting important sets of weights to maintain accuracy. The instability of a pruning algorithm is measured by the accuracy drop before and after pruning. The stability of pruning algorithms is influenced by various factors such as pruning target, schedule, iterative rate, and model. Experimental results show the need for a new pruning target. Iterative magnitude pruning is used in experiments, targeting weights based on magnitude. Parameters are removed based on smallest, random, or largest magnitude. Pruning involves removing parameters with small magnitudes or low importance to the loss function. The correlation between parameter magnitude and importance weakens with batch normalization. Batch normalization normalizes feature map magnitudes, depending on BN affine-transformation parameters \u03b3 and \u03b2. In batch normalized networks, filter magnitude no longer directly scales the output. In batch normalized networks, the correlation between filter magnitude and importance weakens due to normalization with parameters \u03b3 and \u03b2. A novel approach incorporates \u03b3 and \u03b2 to determine filter importance, approximating feature map activations as N(\u03b2, \u03b3). This approximation is accurate early in training but less so as training progresses. The post-ReLU feature map R = max{0, M} has elements that are either 0 or samples from a truncated normal distribution. E[BN] pruning computes filter magnitude using an estimate of the expected value of R ij, avoiding the assumption that filter importance is tied to filter 2 norm in a batch-normalized network. This approach can provide better control of the neural network's output stability and simplifies the calculation complexity. The pruning method based on expected value of feature map activations provides better output stability control and simpler calculation complexity compared to filter 2 norm pruning. Three model classes were considered: Conv4, VGG11, and ResNet18, all trained using Adam optimizer with a learning rate of 0.001. The pruning algorithms considered in the study are iterative, with a defined pruning schedule and rate to steadily remove DNN parameters throughout training. The total pruning percentages for VGG11, ResNet18, and Conv4 were 42%, 46%, and 10% respectively. The goal of pruning studies is to compress pre-trained models that generalize well. The study focuses on understanding the mechanisms behind generalization improvements in pruned DNNs, questioning if parameter-count reduction alone or the pruning algorithm properties contribute to generalization. The study questions whether generalization improvements in pruned DNNs are solely due to parameter-count reduction or if the pruning algorithm itself plays a role. The variability in reported generalization benefits may be attributed to differences in pruning algorithms, particularly in terms of stability. More stable algorithms may provide a closer approximation to the loss changes with respect to each parameter, potentially influencing generalization outcomes. Pruning algorithms' stability may impact generalization in DNNs. Comparing various pruning algorithms, it was found that unstable algorithms led to higher test accuracies. This suggests a relationship between pruning algorithm stability and generalization outcomes. The study found that unstable pruning algorithms resulted in higher test accuracies, indicating a link between pruning algorithm stability and generalization outcomes. Pruning techniques that induce more instability may actually improve generalization. The precise pruning method used plays a critical role in this process. High iterative pruning rates can lead to substantial drops in performance, but the network can recover to higher performance within a few epochs. The study observed that ResNet18 adapted to pruning events more quickly than VGG11, leading to improved generalization by reducing shortcut connections. This simple improvement in pruning hyperparameters suggests potential for further optimization. See Appendix A.5.1 for experiment details. Pruning larger magnitude weights via the E[BN] algorithm can lead to increased test accuracy improvements, indicating a positive correlation between pruning target magnitude and regularization effect. The final generalization gap depends on the magnitude of the weights pruned during training, especially with unstructured pruning compared to structured pruning. The relationship between pruning effects may vary based on whether nodes/filters or individual parameters are pruned. Target weight magnitude was tested to further explore these effects. Pruning larger weights leads to better generalization gaps, especially with unstructured pruning compared to structured pruning. The 2 norm of pruned neurons did not vary dramatically in structured pruning. Target weight magnitude was tested to explore these effects. In structured pruning, the 2 norm of pruned neurons did not vary significantly, while unstructured pruning showed an exponential distribution. Pruning large magnitude weights may improve generalization, with structured pruning outperforming unstructured pruning in some cases. This difference could be due to structured pruning preserving important connections in convolutional filters. Pruning large magnitude weights can improve generalization, with structured pruning preserving important connections in convolutional filters. However, targeting the smallest magnitude or least important parameters can also lead to generalization improvements. Pruning more parameters per iteration may increase instability but could result in better generalization performance. Increasing the iterative pruning rate in VGG11 can lead to more instability but may allow methods to generalize better. E[BN] magnitude better approximates parameter importance than 2-norm magnitude. Pruning rate does not affect generalization when keeping the pruning target and total pruning percentage fixed. Iterative pruning rate plays a role in inducing instability, with larger rates leading to more instability. When targeting less important weights, higher iterative pruning rates induce more instability and improve generalization. Pruning random or small magnitude parameters works best at a 30% iterative rate. However, targeting the largest magnitude weights with 2-norm pruning does not benefit from higher iterative rates. This suggests that inducing additional instability during training can boost generalization. The regularizing effect of pruning is enhanced by pruning more often, leading to better generalization. Raising the iterative pruning rate may cause 2-norm pruning to target less important weights, potentially harming model generalization. Pruning more often can improve generalization by targeting less important weights, but it may also lead to instability and reduced model capacity. Iterative pruning can be seen as noise injection, and the benefits of pruning on generalization may be due to the reduction in model capacity. Improvements in generalization from pruning are similar to those from using temporary noise injection. Pruning can help isolate how iterative pruning regularizes through noise injection. Experimentation with zeroing weights temporarily removes a key aspect of pruning noise regularization. The study explores the impact of weight removal and noise injection on generalization in pruning algorithms. Weight removal and noise injection are compared to understand their effects on regularization through noise in pruning. Applying zeroing noise to filters generates similar accuracy to prune L in pruned DNNs. Regularization induced by forcing weights to adapt to noised representations is critical for improving generalization. Gaussian noise can increase generalization but still does not match the performance of pruning. Pruning induced generalization benefits are not solely due to weight removal but also depend on regularization from adapting to noised representations. Pruning algorithms causing more instability lead to better generalization in neural networks. Pruning algorithms causing instability can lead to better generalization in neural networks by injecting noise, providing similar benefits as permanent pruning without removing as much capacity. Pruning algorithm stability is important for recovery from damage and can impact final generalization. Factors affecting recovery include learning rate, regularization effects, preservation of biases, and network capacity. Understanding these factors can help design better pruning algorithms. The network could prune important weights occasionally to enhance generalization improvements without risking permanent damage. Pruning may reduce information stored in the network's weights and make the network more distributed, raising the possibility that pruning noise engenders helpful properties in DNNs. Further exploration is needed to understand the relationship between pruning and noise injection schemes. Further exploration is needed to understand the relationship between pruning and noise injection schemes. The study found that the approximation to standard normality of feature map activations in VGG19 degrades with training. The quality of feature map activations in VGG19 degrades with training, particularly in layers near the output. The E[BN] approach has drawbacks in approximating activations and does not consider connection strength to post-BN feature maps. Prune SE[BN] in VGG11 is more stable than prune S, with higher iterative pruning rates causing more instability. When training Conv4 on CIFAR10, unstable pruning can significantly improve the baseline's generalization. The correlation between instability and generalization is weaker with 2-norm pruning compared to E[BN] pruning. 2-norm pruning generates a narrower spectrum of instabilities due to its inability to accurately assess parameter importance. Top-1 Accuracy % shows how each pruning algorithm disturbs the neural network's output during training. Unstructured and structured magnitude pruning methods are used to remove individual weights or entire filters/neurons during training. Unstructured pruning does not allow previously pruned weights to reenter the network, while structured pruning is used for VGG11 and ResNet18. Pruning of n layers is specified by a series of epochs at which pruning starts. The approach involves specifying epochs for pruning start and end, fractions of parameters to remove, and a retrain period. The iterative pruning percentage is determined by the retrain period and fraction. The method aims to study the effects of changing factors in pruning but lacks certain practical features. Our study focused on iterative pruning in ResNet models using the CIFAR-10 dataset. We pruned incoming and outgoing shortcut connections associated with pruned feature maps. Batch size was 128, and data augmentation was only used in one experiment. Multi-step learning rate schedules were implemented in some experiments. E [BN] pruning was used in all models except for one that used 2-norm magnitude pruning. The models were trained on CIFAR-10 with Adam for 325 epochs with lr s = (150, 300). VGG11 Pruning targeted the final four convolutional layers during training with specific starting and ending epochs, and pruning fractions. Multiple iterative pruning percentages were used, adjusted by the number of inter-pruning retraining epochs. The models underwent iterative pruning with varying percentages, adjusting the number of retraining epochs. Different pruning methods were used, with ResNet18 targeting specific convolutional layers. The pruning rate of the output layer was increased to enhance adaptation. Each experiment in Figure 2 focused on different aspects. During iterative pruning experiments on the Conv4 network, different pruning targets were set for ten weight-magnitude deciles in the post-convolutional linear layer. The pruning methods targeted specific deciles, with one method (D10) focusing on removing the largest weights each iteration. This approach required creating eleven different pruning targets to cover all deciles effectively. During training, the Adam optimizer was used with specific parameters. Generalization gap was calculated on epoch 54 and average pruned magnitudes were sampled on epoch 35. Results were consistent regardless of training epochs or data augmentation. Figure 3 shows pruning applied to the final four convolutional layers of ResNet18 with varying pruning fractions and epochs. Multiple iterative pruning percentages were used due to layerwise pruning variations. The models underwent iterative pruning with different percentages, ranging from 1% to 13%, and the \"One Shot\" model pruned all targeted parameters at once on epoch 246. Unstructured pruning involved removing individual weights based on their magnitude, while structured pruning used E[BN] pruning. The models were trained on CIFAR-10 with Adam for 325 epochs. Error bars represent 95% confidence intervals for the means. In Figure 4, pruning targeted the final four convolutional layers of VGG11 with varying starting and ending epochs, and pruning fractions. To create different iterative pruning rates, models with inter-pruning retrain periods were used. The models were trained on CIFAR-10 with Adam for 325 epochs. Pruning targeted the final four convolutional layers of VGG11 with specific starting and ending epochs, pruning fractions, and inter-pruning-iteration retrain period. During training on CIFAR-10 with Adam for 325 epochs, pruning noise was injected using the same schedule and percentages as pruning, but applied to parameters instead of removing them. Gaussian noise with mean 0 and standard deviation equal to the empirical standard deviation of a noiseless filter was used. Prune L utilized 2-norm pruning. The experiments in Appendix A.4 focused on the post-convolutional linear layer of the Conv4 network, demonstrating that unstable pruning can improve performance at higher sparsity levels. At higher sparsity levels, unstable pruning can enhance generalization in the baseline model. Error bars represent 95% confidence intervals from 20 runs of each configuration."
}