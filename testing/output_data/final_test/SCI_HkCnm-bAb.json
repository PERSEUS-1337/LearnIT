{
    "title": "HkCnm-bAb",
    "content": "Deep reinforcement learning has seen recent successes, but our understanding of its strengths and limitations is limited by the lack of rich environments to fully characterize optimal behavior and diagnose individual actions. The Erdos-Selfridge-Spencer games offer a low-dimensional, parametrized environment for evaluating reinforcement learning approaches. They provide a linear closed form solution for optimal behavior and allow for tuning game difficulty. These games are used to compare algorithms, supervised and reinforcement learning approaches, analyze multi-agent performance, and evaluate generalization to new environments. Deep reinforcement learning has shown significant successes recently, but creating algorithms that are robust across tasks and policy representations remains a challenge. Standard benchmarks like MuJoCo and Atari offer rich settings for experimentation, but the differences in underlying environments make it difficult to determine the principles behind sub-optimal behavior. An ideal environment for studying reinforcement learning algorithms would be one with simply parametrized settings where optimal behavior is completely characterized. The text discusses the creation of a family of environments for studying reinforcement learning algorithms. These environments are based on two-player combinatorial games known as Erdos-Selfridge-Spencer (ESS) games, where optimal strategies can be defined by potential functions. The goal is to provide a range of instances that vary in difficulty for current algorithms. The text discusses creating environments for studying reinforcement learning using Erdos-Selfridge-Spencer (ESS) games. These games involve an attacker advancing pieces while a defender tries to stop them. Parameters like the number of levels and a potential function determine the game's difficulty and optimal strategies. The text discusses using combinatorial games as environments for studying reinforcement learning. It focuses on analyzing optimal play, learning by the defender or attacker, and the robustness of solutions in non-trivial environments. The main contributions include developing insights about the behavior of reinforcement learning in these games. The text explores using combinatorial games to study reinforcement learning algorithms, comparing deep RL and supervised learning agents' performance. It highlights that while the supervised learning agent is more accurate on individual moves, the deep RL agent plays the game better. Additionally, it investigates how the success of one player in training depends on the algorithm used for the other player. In this empirical paper, the focus is on exploring properties of multiagent learning and implementing algorithms for the other player in a combinatorial game environment. The study presents learning and generalization experiments using various model architectures and learning algorithms to demonstrate challenges for standard reinforcement learning approaches. The family of Attacker-Defender Games BID10 is introduced as a testbed for deep reinforcement learning due to its ability to vary the environment's difficulty and provide a closed form solution. An Attacker-Defender game involves two players: an attacker who moves pieces and a defender who destroys pieces. The game has levels numbered from 0 to K and N pieces distributed across these levels. The attacker's goal is to get a piece to level K while the defender aims to destroy all pieces before that. Each turn, the attacker proposes a partition of pieces, and the defender chooses one set to destroy. The game ends when a piece reaches level K or all pieces are destroyed, with varying levels and pieces changing the difficulty for the players. In the Attacker-Defender game, the difficulty is influenced by the number of levels and pieces. If all pieces start at level 0, the defender can always win if N < 2K by strategically destroying pieces. This reduces the number of pieces by at least half in each step, ensuring no piece reaches level K. In the more general case of the game, pieces placed at arbitrary levels make it unclear how to define the \"larger\" set A or B. A second proof of Theorem 1 uses Erdos's probabilistic method, showing that any piece has a 1/2 probability of advancing a level and 1/2 of being destroyed. The goal is for all pieces to advance K levels to reach the top, which happens with probability 2^(-K). The attacker cannot win with probability 1 against random play by the defender, as the defender can force a win by guaranteeing the destruction of all pieces. The game's state is described by a K-dimensional vector representing the number of pieces at each level. A piece at level l has a 2^(-l) chance of survival under random play. The potential function on states in the Attacker-Defender game determines the outcome based on the initial state's potential value. If the potential is less than 1, the defender can always win, while if it is greater than or equal to 1, the attacker can always win. This strategy is based on a linear function expressed as \u03c6(S) = w T \u00b7 S, where w l = 2^-(K-l). The defender's strategy in the Attacker-Defender game involves destroying the set with higher potential to ensure that subsequent states also have potential less than 1. If the initial potential is less than 1, the defender can always win by preventing any piece from reaching level K. If the initial potential is greater than or equal to 1, the attacker can devise a similar optimal strategy to win. The attacker picks two sets A, B with potential \u2265 1/2. Theorem 3 and BID10 show that regardless of which set is destroyed, the other doubles its potential, ensuring a win for the attacker. Various benchmark sets like Atari (BID5), MuJoCo (BID2), Deepmind Lab (BID0), and OpenAI Gym (BID1) offer different environments for testing algorithms. Physics-based environments allow for continuous variation through changing parameters or randomizing rendering. Our proposed benchmark merges properties of algorithmic tasks and physics-based tasks, allowing for difficulty adjustments through changes in input length or potential. Attacker-Defender games offer environments with adjustable difficulty levels based on start state potential and number of levels. Baseline results on Attacker-Defender games motivate further exploration in the paper, with game state represented by a vector for levels 0 to K. Defender agent input includes the concatenation of partition A, B. The defender agent's input is the concatenation of partition A, B, creating a 2(K + 1) dimensional vector. The game start state S 0 is randomly initialized from a distribution over start states of a certain potential. Training involves a defender agent facing an attacker who alternates between playing optimally and suboptimally using the Disjoint Support Strategy. This strategy creates uneven potential levels between A, B, leading to different states than the optimal strategy. When testing reinforcement learning, difficulty parameters can be adjusted based on potential levels. The difficulty parameters in reinforcement learning involve adjusting the potential of the start state and the number of levels (K). Changing these parameters affects the sparsity of the reward and the number of possible states and game trajectories. While theoretically expressive enough, RL struggles to learn the optimal policy for the defender agent in practice, except for DQN which performs relatively well across difficulty settings. The optimal policy can be represented as a linear network with weights determined by the potential function. The defender agent's optimal policy can be represented as a linear network with weights determined by the potential function. However, training a linear model for the defender agent was not as effective as using deeper networks like DQN, PPO, and A2C. Deeper networks showed improved performance and reduced variance, leading to the decision to use a fully connected neural network with two hidden layers of width 300 for the policy net. The study evaluated PPO, A2C, and DQN with two hidden layers of width 300. Each algorithm was tested on varying start state potentials and K with 3 random seeds. PPO showed more robustness to larger K, while A2C performed worse than PPO and DQN. Supervised learning was better at getting the ground truth correct move, but RL outperformed in playing the game. The Attacker-Defender game compared RL and Supervised Learning in training defender policies. RL significantly outperformed Supervised Learning, with results shown in FIG4. The trained supervised policy network has a higher proportion of correct moves, but reinforcement learning wins a larger proportion of games. Reinforcement learning focuses more on moves that matter for winning and makes fewer fatal mistakes compared to supervised learning. Supervised learning is prone to fatal mistakes, especially for larger K values, leading to a decrease in performance. RL outperforms supervised learning in the final moves and earlier parts of the game. Comparison to optimal play shows the robustness of the RL Defender Agent's learned policy. In FIG7, a defender agent trained on the optimal attacker shows a significant performance drop when tested against a different attacker strategy. Supervised learning makes more fatal mistakes compared to RL, leading to a collapse in performance. RL is more accurate in predicting actions towards the end game, while supervised learning struggles with accuracy even in earlier stages. The performance of PPO and A2C on training the attacker agent for different difficulty settings is shown in Figure 8. DQN performance was poor with best hyperparameters. There is greater variation in performance with changing difficulty levels, affecting reward sparseness and action space size. Performance is less varied with potential, but significant differences are seen. The performance of PPO and A2C on training the attacker agent for different difficulty settings is shown in Figure 8. There is high performance variance with lower potentials, suggesting the defender may be overfitting to a specific attacker strategy. To mitigate this, training the attacker first is proposed, but a tractable parametrization of the action space is needed. A naive implementation would be impractical due to exponential growth in complexity. To address the exponential growth in complexity of the action space in Attacker-Defender games with K levels, a theorem is proven to parametrize an optimal attacker with a smaller action space. The theorem states that there exists a partition A, B such that \u03c6(A) \u2265 0.5, \u03c6(B) \u2265 0.5, and for some level l, A contains pieces of level i > l, and B contains all pieces of level i < l. The theorem proves that an optimal attacker can be parametrized with a smaller action space to address the complexity of Attacker-Defender games with K levels. The potential of sets A and B must be at least 0.5, with A containing pieces of level i > l and B containing pieces of level i < l. Additionally, in a multiagent setting, the defender outperforms the attacker, with sharp changes in performance observed when switching agents during training. The theorem guarantees an optimal attacker parametrization with a smaller action space for Attacker-Defender games with K levels. Training an attacker agent against the optimal defender using PPO and A2C showed large performance variations with changes in K, affecting reward sparsity and action space size. DQN results were poor, leading to focus on PPO and A2C algorithms. The attacker's performance in multiagent play is worse compared to single agent settings, even losing to the defender with certain parameters. The single agent attacker succeeds against the optimal defender, while a multiagent defender shows varied results when tested against different values of K. The defender was tested against the disjoint support attacker for different values of K. The multiagent defender generalizes better than the single agent defender. Results show that the defender trained in a multiagent setting performs noticeably better. In a study on Erdos-Selfridge-Spencer games, researchers found that algorithms perform differently based on game difficulty. They compared supervised learning with reinforcement learning approaches and explored insights on overfitting, generalization, and multiagent learning. Further analysis in the Appendix includes catastrophic forgetting, generalization across different game parameters, and measuring model confidence. The games are seen as a valuable environment for research. The study on Erdos-Selfridge-Spencer games revealed that training on harder games leads to better performance in deep reinforcement learning. Performance is inversely proportional to the difference between train and test difficulty settings. The AttackerDefender game has an expressible optimal that generalizes across all difficulty settings. Training on harder games leads to better generalization in deep reinforcement learning. Performance is inversely related to the difference between train and test difficulty settings. The Attacker-Defender game has an optimal strategy that generalizes across all difficulty settings. Testing on different potentials is straightforward, but testing on different levels requires a reformulation due to the input size constraints of the neural network policy. Training on a smaller number of levels and testing on a larger number is not a fair test, but the opposite scenario is easily implementable and provides a legitimate test of generalization. Results show that when varying potential, training on harder games results in improved generalization. Performance when testing on a smaller number of levels than the one used in training is inversely related to the difference between the two. Recently, catastrophic forgetting in Deep Reinforcement Learning has been identified as a problem where switching between tasks leads to lower performance. In Attacker-Defender games, training on different difficulty levels can either improve play or interfere catastrophically. The model may have learned simple rules, such as the value of the null set, to navigate these challenges. The defender agent should choose to destroy B if A consists of only zeros. The model's confidence is tested based on potential value discrepancies. Training on different start state distributions affects generalization. The model's confidence is tested based on potential value discrepancies, showing an increase in potential difference with an increase in model confidence even when the model is wrong. The number of states with potential 1 for a game with K levels grows super exponentially. The optimal defender policy is expressed as a linear model, and depth is investigated empirically. Linear models perform worse than models with hidden layers, even with a temperature included."
}