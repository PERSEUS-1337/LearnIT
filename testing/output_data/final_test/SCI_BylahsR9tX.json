{
    "title": "BylahsR9tX",
    "content": "Large-scale Long Short-Term Memory (LSTM) cells are key in NLP tasks but are computationally inefficient due to the number of parameters and non-parallelizable recurrence. This paper proposes low-rank matrix factorization algorithms to compress network parameters and speed up LSTMs without performance loss. The method is tested on compressing core LSTM layers in Language Models and biLSTM layers of ELMo, showing effectiveness in Sentiment Analysis, Textual Entailment, and Question Answering tasks. The study explores the use of matrix factorization to compress network parameters in large pre-trained biLSTM Language Models. It finds that additive recurrence is more important than multiplicative recurrence, and there is a correlation between matrix norms and compression performance. Long Short-Term Memory (LSTM) networks have become essential for tasks requiring temporal dependency in NLP. They have shown significant improvements in various NLP tasks like Language Modeling, Semantic Role Labeling, Named Entity Recognition, Machine Translation, and Question Answering. However, scalability issues arise due to the temporal dependency in the computational graph and the large number of parameters. The LSTM networks face scalability issues due to temporal dependency and a large number of parameters. Recent empirical results challenge the belief that LSTM memory capacity is proportional to model size, suggesting that LSTMs are over-parameterized. This motivates the search for effective compression methods to address scalability concerns. Compression methods for large neural networks like parameter pruning, low-rank Matrix Factorization, and knowledge distillation have been applied to various network architectures, but there is limited focus on compressing LSTM architectures, especially in NLP tasks. Some approaches include parameter pruning in Seq2Seq architecture for Neural Machine Translation and techniques like Tensor-Train Decomposition and binarization for language modeling. These methods typically require some form of training or retraining. In this work, the focus is on compressing LSTM architectures in NLP tasks. The authors advocate for low-rank matrix factorization as a compression method for already compact and optimized models, which poses a more challenging but realistic approach compared to traditional compression techniques. The authors propose low-rank matrix factorization as a compression method for LSTM architectures in NLP tasks, achieving a 1.5x speedup in inference time while maintaining performance across various datasets. Fine-tuning very compressed models can further improve speed by 2x in PTB. The curr_chunk discusses how matrix factorization can be applied to compress LSTM parameters W i and W h, leading to a 2x speedup in inference time and potentially improving performance. The authors also highlight the importance of additive recurrence over multiplicative recurrence and the correlations between matrix norms and compression performance. The curr_chunk explains the application of Low-Rank Matrix Factorization to LSTM parameters W i and W h, emphasizing the role of input, output, and forget gates in controlling information flow within the network. LSTM aims to capture long-term dependencies and avoid gradient problems, with memory cells learning salient information through time. The LSTM dynamics are represented by recurrent equations involving input x t and hidden state h t at time t. The model parameters for LSTM, W_i and W_h, can be summarized as matrices with additive and multiplicative recurrence. Low-Rank Matrix Factorization is applied to these matrices to compute their rank, which is crucial for capturing long-term dependencies in the network. Low-Rank Matrix Factorization is a method used to compute the rank of matrices in LSTM models, crucial for capturing long-term dependencies. The nuclear norm is a convex surrogate function for the NP-hard problem of computing singular values. By finding U and V for low-rank representation, the number of parameters and computations can be significantly reduced. Various constrained versions of low-rank matrix factorization exist, with methods like SVD applying orthogonal constraints on factors U and V. Low-Rank Matrix Factorization methods like SVD apply orthogonal constraints on factors U and V to find optimal values for singular vectors and values. Semi-NMF relaxes sign constraints on negative values for U and W, making it preferable for Neural Networks. When the input matrix W contains mixed signs, a factorization is considered with V restricted to be non-negative. The optimization algorithm alternates between updating U and V using coordinate descent. A basic LSTM cell includes four gates: input, forget, output, and cell state, performing a linear combination on input and hidden state. Large scale data requires significant memory for the matrices W i and W h. The proposal suggests replacing W i and W h matrices with low-rank decomposition to reduce memory and computational costs for large-scale data. Evaluation is done on language modeling using different datasets, showing improved performance compared to ELMo. The study compares different low-rank approximation methods for matrix factorization in language modeling. It evaluates the efficiency of compressing weight matrices W i and W h, and compares additive and multiplicative recurrence types for compression. Additionally, it includes metrics such as Perplexity, Accuracy, F1 score, number of parameters, and efficiency E(r). The study evaluates the efficiency of compressing weight matrices in language modeling using different low-rank approximation methods. It compares various compression techniques such as Semi-NMF, SVD, and pruning on a 3-layer LSTM Language Model. The evaluation includes metrics like Perplexity, Accuracy, F1 score, number of parameters, and efficiency E(r). The study compares different low-rank approximation methods for compressing weight matrices in language modeling. Results show that compressing Wh is more efficient and performs better than Wi. SVD method has the lowest perplexity and best efficiency among others. All methods outperform previous results using Tensor Train for LSTM compression. In TAB1, results after fine-tuning show that MF methods and pruning work better than existing compression methods. By factorizing Wi with rank 10, a small improvement is achieved with a 2.13x speedup. Pruning with rank 10 also works comparably well. Timing information was not available from the SQuAD test set. The practicality of the proposed method is highlighted by measuring factorization performances with models using pre-trained ELMo. Factorization performances with models using pre-trained ELMo were measured. The ELMo layer was low-rank factorized to compress transferable knowledge effectively. Only the ELMo weights were compressed, while other layers of the models remained unchanged. The ELMo layer in different models has a large number of parameters, but the inference time is less affected compared to Language Modeling tasks. Compressing W h is generally more efficient and better performing than compressing W i for SST-5 and SNLI, except for SVD in SST-5. However, compressing W i constantly outperforms compressing W h for SQuAD. Even with low-rank compression, using highly compressed ELMo with BiDAF still performs better than without. BiDAF continues to outperform without compression, achieving similar results across all datasets even after compressing over 10M parameters. The comparison between Matrix Factorization (MF) and Pruning in different datasets reveals that MF works better in PTB and Wiki-Text 2, while Pruning is more effective in ELMo for W h. Additionally, factorizing W h is generally more beneficial than factorizing W i. The analysis includes L1 and Nuclear norm statistics to compare sparsity and rank approximation between W h and W i in PTB and ELMo. In comparing Matrix Factorization (MF) and Pruning for compression, MF outperforms Pruning for higher compression ratios in W i. MF retains more salient information at lower ranks, while Pruning loses some information. This is evident in the L1 norm statistics, where Pruning drops significantly more than MF. The explanation for why MF consistently outperforms Pruning for higher compression ratios in W i is consistent in both PTB and WT2 datasets. However, results from TAB1 show that Pruning works better than MF in W h of ELMo even in higher compression ratios. This behavior can be explained by the nature of compression and inherent matrix sparsity, where Pruning retains salient information while keeping the matrix sparse. In the context of matrix sparsity, pruning involves zeroing values close to zero to maintain L1 stability and increase std, while MF reduces noise by pushing lower values lower and keeping salient information by pushing larger values higher. Compression of W h shows better performance than W i, with consistently lower nuclear norm in both LM and ELMo. The compression of W h shows better performance than W i in both LM and ELMo, with a consistently lower nuclear norm. This difference is more pronounced in LM, indicating that W h is inherently low-rank compared to W i. In ELMo, the gap in norm is smaller, leading to smaller performance differences between W i and W h. Several studies have proposed modifications to LSTM structures to improve training and inference time. These include replacing GRUs with low-rank and diagonal weights, partitioning input and hidden weights, and zeroing out insignificant weights. Additionally, compression techniques such as low-rank factorization and pruning have been explored to reduce the number of parameters in Neural Machine Translation. In conclusion, various techniques have been proposed to compress LSTM gates, including low-rank matrix factorization and pruning. Experimental results show that low-rank matrix factorization generally outperforms pruning, but pruning is more effective for particularly sparse matrices. Future work aims to factorize all LSTMs in models like BiDAF and combine pruning with matrix factorization. The semi-NMF algorithm is an extension of NMF that remains unconstrained. It ignores the constraint in U and explores the relationship between matrix factorization and K-means clustering. The algorithm initializes U and runs k-means clustering, updating U by fixing V using a constraint. The NMF algorithm involves calculating derivatives to optimize the objective function. By iteratively updating U and V, convergence is achieved. The algorithm is similar to coordinate descent with modifications. In the latent space, each axis represents a cluster centroid, and documents are represented as combinations of centroids. Cluster membership is determined by the closest centroid. The K-means clustering algorithm involves finding cluster centroids and membership indicators. The NMF or semi-NMF is then performed on the data matrix to obtain non-negative matrices U and V. The algorithm complexity is shown in a table. The L1 norm of a matrix is defined as the maximum absolute column sum norm. The L1 norm of a matrix is the maximum of the L1 vector norms of all column vectors in W, with the standard deviation indicating the variance across column-wise L1 norms. The nuclear norm of a matrix is the sum of singular values, approximating the rank. Efficiency is evaluated using a formula involving evaluation metrics, parameters, and a ratio indicating loss in performance. In this section, the fine-tuning steps and hyper-parameter tuning process are explained to improve model performance. Different initial learning rates are applied for each rank to reach convergence quickly. The efficiency of compression is evaluated using a formula involving evaluation metrics, parameters, and a ratio indicating loss in performance. To quickly reach convergence, different initial learning rates are applied for each rank, especially after matrix factorization. Asynchronous Stochastic Gradient Descent Merity et al. FORMULA0 is used as the optimizer during every step. Lower perplexity is achieved after fine-tuning steps, as shown in Figure 5."
}