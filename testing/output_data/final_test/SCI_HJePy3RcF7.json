{
    "title": "HJePy3RcF7",
    "content": "There is a disparity between learning rate schedules in large scale machine learning and those prescribed in stochastic approximation theory. Recent results show that non-convex neural network training may benefit from different learning rate schedules, such as cutting the rate every constant number of epochs. This contrasts with the polynomial decay schemes recommended for convex optimization problems. The work highlights that different learning rate schedules, like cutting the rate every constant number of epochs, can be more effective than polynomial decay schemes even for simple cases like stochastic linear regression. The exponential improvement provided by this approach compared to polynomial decay is significant. The theoretical insights suggest that recent results on making the gradient norm small at an optimal rate apply to both convex and non-convex optimization problems. Theoretical insights suggest that optimizing the gradient norm at an optimal rate applies to both convex and non-convex optimization, offering valuable insights into learning rate schedules used in practice. The recent advances in machine learning and deep learning heavily rely on stochastic optimization methods like SGD and its variants. These methods require manual tuning for each specific problem, unlike non-stochastic optimization methods such as l-BFGS and non-linear conjugate gradient methods which have been highly effective without hyper-parameter tuning. In stochastic optimization, there are two dominant approaches that involve manual tuning of learning rate schedules. This work focuses on understanding learning rate scheduling methods for stochastic optimization, specifically examining manual tuning approaches and approximate preconditioning methods. Theoretical analysis and empirical studies are used to explore the disparity between theoretical admissibility and practical implementation of learning rate schedules for achieving optimal results. The focus is on polynomial decay schemes for learning rate scheduling in stochastic optimization. These schemes are square summable and aim to achieve optimal performance within a fixed time horizon. Practical implementations often involve cutting the step size by a constant factor after a certain number of epochs or when no progress is made on a validation set. These approaches are widely used in deep learning libraries like PyTorch. The emphasis in deep learning is on learning rate scheduling, with practitioners using constant and cut learning rate schemes for better performance. The theory often works with polynomial decaying schemes, but the practical results show that constant and cut schemes yield better quality local minima during neural network training. The primary contribution of this work is to challenge the belief that polynomial decay schemes are optimal for convex optimization problems. It is shown that constant and cut schemes produce better quality local minima. Even for simple cases like stochastic linear regression, polynomial decay schemes are suboptimal compared to statistical minimax rates. This work challenges the belief that polynomial decay schemes are optimal for convex optimization problems. It shows that constant and cut schemes can produce better quality local minima, even outperforming polynomial decay schemes in cases like stochastic linear regression. The convergence rate of the error in stochastic approximation is necessarily suboptimal by a factor of \u03a9(\u03ba) compared to statistical minimax rates. The paper challenges the belief that polynomial decay schemes are optimal for convex optimization problems. It demonstrates that constant and cut schemes can achieve better convergence rates, especially in cases like stochastic linear regression. The work also shows that for a fixed time horizon, these schemes are significantly more effective than polynomial decay schemes. The paper challenges the belief that polynomial decay schemes are optimal for convex optimization problems, showing that constant and cut schemes can achieve better convergence rates, especially in cases like stochastic linear regression. The results suggest a need to rethink learning rate schedules, as even for stochastic linear regression, constant and cut learning rate schedules are provably better than polynomial decay schemes. The paper challenges the belief that polynomial decay schemes are optimal for convex optimization problems and suggests rethinking learning rate schedules for stochastic approximation. Results show that constant and cut schemes can achieve better convergence rates, especially in cases like stochastic linear regression. The paper is organized into sections reviewing related work, describing notation and problem setup, presenting results on suboptimality of decay schemes, discussing infinite horizon setting, showcasing experimental results, and concluding. The paper challenges traditional beliefs on optimal learning rate schedules for stochastic approximation, suggesting constant and cut schemes may achieve better convergence rates, especially in stochastic linear regression. Previous works have focused on function value sub-optimality and asymptotic convergence to derive \"convergent stepsize schedules\". The curr_chunk discusses efforts to prove optimality of convergence rates in a worst-case sense, with works by various authors such as Raginsky & Rakhlin (2011) and Nesterov (2012). It also mentions the importance of gradient norm in measuring algorithm progress, with Allen-Zhu (2018) providing an improved rate for making the gradient norm small. The paper and other works operate in an oracle model assuming bounded variance of stochastic gradients. The curr_chunk discusses practical efforts in stochastic optimization diverging from classical theory, with deep learning libraries providing alternative learning rate schemes. These schemes, like cosine/sawtooth/dev set decay, are discretized variants of exponentially decaying schedules used in training convolutional neural networks. The curr_chunk discusses unconventional learning rate schedules like sgd with warm restarts and oscillating rates, which have gained attention. Exponential learning rates are also being considered in recent NLP papers. Theoretical results focus on additive noise stochastic linear regression. The section presents the setup for solving a minimization problem with a positive definite matrix and vector. It involves using a stochastic gradient oracle and updating the algorithm with independent random vectors. The suboptimality of a point is defined, and lower bounds on suboptimality are discussed. In this section, the text discusses the suboptimality of polynomial decay schemes compared to the statistical minimax rate in stochastic linear regression scenarios. The noise in the gradient is highlighted as a key factor, and the results can be extended to settings with arbitrary error covariance matrices. The text discusses the suboptimality of polynomial decay schemes in stochastic linear regression compared to the statistical minimax rate. It shows that certain schemes are suboptimal by a factor of \u2126(\u03ba) and introduces constant and cut schemes that achieve the minimax rate up to a multiplicative factor of log \u03ba log 2 T. The text discusses the suboptimality of polynomial decay schemes in stochastic linear regression compared to the statistical minimax rate. It introduces constant and cut schemes that achieve the minimax rate up to a multiplicative factor of log \u03ba log 2 T. Additionally, it shows that exponential decay schemes, dependent on the time horizon, are smoother versions of constant and cut schemes and may have better performance. The text introduces a new learning rate scheme that improves the rate to 1 T by using constant and polynomial schemes initially, then switching to constant and cut schemes later. This approach has not been explored in stochastic optimization literature before. Theorem 4 states that for any problem and fixed time horizon T / log T > 5\u03ba, a learning rate scheme can achieve DISPLAYFORM2 T. The text also highlights a fundamental limitation of the SGD algorithm, showing that it needs to query a point with suboptimality more than \u2126(\u03ba/ log \u03ba) \u00b7 \u03c3 2 d/T for infinitely many time steps T. The text discusses the limitations of the SGD algorithm, showing the need to query points with suboptimality more than \u2126(\u03ba/ log \u03ba) \u00b7 \u03c3 2 d/T for infinitely many time steps. Theorems 5, 6, and 7 present results on the fraction of query points with values exceeding certain thresholds, the existence of universal constants for SGD algorithms, and the achievement of statistical minimax rate with specific learning rate schemes. Learning rates above 2/\u03ba lead to algorithm divergence. The text discusses the limitations of the SGD algorithm and the benefits of non-convergent exponentially decaying and oscillating learning rate schemes. Experimental validation is provided through controlled synthetic experiments on a two-dimensional quadratic objective and a real-world non-convex optimization problem. In Appendix D, the problem of optimizing a two-dimensional quadratic objective with varying condition numbers \u03ba is considered. Results are obtained by averaging over two random seeds and a grid search is performed for the starting learning rate \u03b7 0 and decay factor b. The experiments validate the benefits of non-convergent learning rate schemes through controlled synthetic experiments. The search parameter falls at the grid edge to ensure all results are within the final grid. Results are presented for experiments on the error behavior of the final iterate using different learning rate schemes. Polynomially decaying schemes show linear scaling of excess risk with condition number, while exponentially decaying schemes exhibit nearly constant excess risk. The best error is not necessarily achieved in 50\u03ba or 100\u03ba steps. The study focuses on training a deep residual network for image classification using different learning rate decay schemes. Experiments are conducted with Nesterov's Accelerated gradient method, grid searching for the best decay scheme, and evaluating error behavior. The best error is not necessarily achieved in 50\u03ba or 100\u03ba steps. The study compares different learning rate decay schemes for training a deep residual network on image classification. The best exponential scheme outperforms polynomial schemes, and hyperparameter selection is done using truncated runs at different epochs. The results are presented in figures and tables for comparison. The study compares various learning rate decay schemes for training a deep residual network on image classification. The best performing hyperparameters at 33 and 66 epochs are not the best at 100 epochs, as shown by validation error. This challenges prior theoretical results, demonstrating that different learning rate schemes can be more effective than standard polynomial decay rates. The recent work by Allen-Zhu (2018) shows significant improvements in minimizing gradient norm for both convex and non-convex optimization problems, surpassing prior results. This includes results with only a logarithmic dependency on \u03ba for strongly convex cases, a substantial improvement over standard analyses. The recent work by Allen-Zhu (2018) has shown significant improvements in minimizing gradient norm for both convex and non-convex optimization problems, surpassing prior results. This includes results with only a logarithmic dependency on \u03ba for strongly convex cases, a substantial improvement over standard analyses. Allen-Zhu's algorithm provides a recursive regularization procedure that obtains an SGD procedure, with doubling regularization analogous to an exponentially decaying learning rate schedule. Further research in this direction holds promise for improved algorithms. The recent work by Allen-Zhu (2018) has shown significant improvements in minimizing gradient norm for both convex and non-convex optimization problems. The algorithm provides a recursive regularization procedure that converges to a stationary point equation. The variance in direction i eventually converges to a specific value, showing promising results for improved algorithms. The recent work by Allen-Zhu (2018) has shown significant improvements in minimizing gradient norm for both convex and non-convex optimization problems. The algorithm provides a recursive regularization procedure that converges to a stationary point equation. The variance in direction i eventually converges to a specific value, showing promising results for improved algorithms. The learning rate scheme for the theorem involves dividing the total time horizon into log(\u03ba) equal sized phases, with specific learning rates assigned in each phase. The learning rate scheme divides the total time horizon into equal-sized phases, with specific rates assigned based on strong convexity and smoothness. The proof involves bounding the variance in the k-th coordinate and showing progress along a specific eigen direction. The theorem is proven using a specific learning rate scheme and progress made along a direction. The proof involves bounding the variance in the k-th coordinate and showing progress along a specific eigen direction. The algorithm progresses from T + 1 to T by adding variance and considering bias contraction. The algorithm progresses from T + 1 to T by adding variance and considering bias contraction. To bound the variance of the process, a constant learning rate is used, leading to a monotonic function of the learning rate. The overall variance can be bounded using the variance of the process run with a learning rate of \u03b3T. The learning rate scheme involves breaking T into three equal parts and using different learning rates in each part. The algorithm progresses by breaking T into three equal parts and adjusting the learning rate accordingly. The variance is bounded by considering bias contraction and adding variance. The learning rate in each phase depends on strong convexity and smoothness. The variance in the k th coordinate is upper bounded, leading to a theorem proof for the first T/3 steps. The algorithm adjusts the learning rate by breaking T into three parts. Variance is bounded by bias contraction and variance addition. The learning rate depends on strong convexity and smoothness. The variance in the k th coordinate is upper bounded, leading to a theorem proof for the first T/3 steps. The lim sup is shown to be larger than \u03c4, with step size sequences considered. The second inequality is based on equation (6) and uses C1 to denote exp(3). By repeating the argument, it is shown that for i = 1, the sum of \u2206j can be lower bounded. As long as C2 is sufficiently large, the point T + \u2206 is also a point with a certain property. Eventually, either case 1 or case 2 will occur, leading to the conclusion that E[f(wT)] is greater than or equal to a certain value. This result is a corollary of Theorem 5. Theorem 7 is proven by running the constant and cut scheme for a fixed time horizon T and then increasing it to \u03ba \u00b7 T. The learning rate for the new time horizon is 1/\u00b5T = 1/\u00b5 \u00b7 \u03baT = 1/LT. The experiments consider three condition numbers \u03ba \u2208 {50, 100, 200} with 200\u03ba iterations, eigenvalues of the Hessian as \u03ba and 1, noise level \u03c3 2 = 1, and results averaged with two random seeds. Grid search is conducted on a 10 \u00d7 10 grid of learning rates \u00d7 decay factor, extending the grid if needed for the best run. The grid search is extended to find the best run in the interior of the gridsearch. Different learning rate schemes are considered with varying decay parameters and starting learning rates. Nesterov's Accelerated gradient method is used with specific parameters for training epochs and regularization. The grid search involves finding the best run within a specified parameter grid. Different learning rate schemes with varying decay parameters and starting learning rates are considered. Nesterov's Accelerated gradient method is used with specific parameters for training epochs and regularization. The learning rate is set to decrease additively over a range at the 80th epoch, with a minimum possible learning rate of 0.0001 to ensure optimization progress. The experiments aim to determine the best performing gridsearch parameter. The grid search involves finding the best run within a specified parameter grid by extending the grid for best performing parameters. A tenth of the training dataset is separated as a validation set, and the remaining dataset is used for training. Once the best grid search parameter is chosen, training is done on the entire dataset."
}