{
    "title": "BkgWahEFvr",
    "content": "Adversarial attacks on CNNs have sparked research on defense mechanisms. Stochastic input transformation methods aim to recover images from attacks through random transformations. However, this can reduce accuracy on clean images. The distribution of softmax induced by these transformations is studied, showing potential for correcting predictions. The text discusses a method to improve transformation-based defenses against adversarial attacks on CNNs. By training a distribution classifier on softmax outputs of transformed images, the proposed method outperforms majority voting for both clean and adversarial images. This approach can be integrated with existing defenses and shows promise in enhancing prediction accuracy. CNNs are vulnerable to adversarial attacks, where small changes in input images can lead to misclassification. Defense methods, such as stochastic transformation-based defenses, have shown success in recovering from these attacks. By transforming input images before feeding them into the CNN, the adversarial nature of the image can be eliminated. This approach involves accumulating a set of CNN softmax outputs and predictions by feeding samples of the transformed image through the network. Existing transformation-based defenses for CNNs accumulate CNN softmax outputs and predictions from randomly transformed images. These defenses do not require retraining the CNN model but suffer from performance deterioration on clean images. Increasing pixel deflections can improve performance on adversarial images but rapidly deteriorates performance on clean images. In these defenses, images are stochastically transformed and fed to the CNN. In a new defense method, the marginal distribution of softmax probabilities from transformed samples is built for each input image. The distribution classifier is trained on clean images and tested on both clean and adversarial images. The deterioration in performance on clean images is unclear, but the softmax distribution from random transformations contains valuable information not captured by majority vote. The study focuses on the effects of random image transformations on the distribution of softmax outputs to understand the accuracy deterioration on clean images. After image transformation, clean images show softmax distributions with modes at incorrect classes, impacting voting accuracy. Adversarial images exhibit similar shifts in softmax distributions. A method is proposed to enhance transformation-based defenses by training a distribution classifier to recognize unique features in softmax outputs. Our distribution classifier improves the performance of transformation-based defenses on clean and adversarial images without retraining the original CNN. The improvements in accuracy on MNIST dataset are 1.7% and 5.9% on clean and adversarial images, and on CIFAR10 they are 6.4% and 3.6% respectively. The defense method is attack-agnostic, as training on a specific attack may cause overfitting. Features of the distribution in the softmax can be used to enhance existing defenses. Our contributions include analyzing the effects of image transformation on softmax outputs for clean and adversarial images, proposing a method that trains a distribution classifier on transformed clean images to improve performance on both types of images. The method is attack-agnostic, does not require CNN retraining, and can be integrated with existing defense methods. The Fast Gradient Sign Method (FGSM) is a single-step attack using the gradient of the classification loss to perturb the image. Iterative Gradient Sign Method (IGM) is an iterative version of FGSM. DeepFool approximates the classifier with a linear decision boundary to generate minimal perturbations. Carlini & Wagner (C&W) L2 attack minimizes perturbation L2 norm and a loss function based on classifier's logit outputs. Black-box attacks involve unknown CNN models with only softmax output or final prediction given. Defense methods have been proposed to make classifiers more robust against adversarial attacks. These methods include adversarial training on generated examples, training auxiliary neural networks on clean and adversarial images, and transformation-based defenses aimed at recovering from perturbations. Two transformation-based defenses, Pixel deflection (PD) and Random resize and padding (RRP), are introduced to maintain accuracy on clean images while defending against adversarial attacks. Pixel deflection corrupts images by redistributing pixels based on class activation maps, with a denoising step using wavelet transform. Random resize and padding randomly resize and pad images to improve robustness. These defenses are implemented at test time, different from training-time data augmentation. In transformation-based methods, random resize and padding (RRP) is used to improve performance without retraining the CNN classifier. However, a weakness identified is that it increases accuracy on adversarial images at the expense of clean images. The exact mechanism of this performance deterioration on clean images is unclear. In this paper, the deterioration in accuracy on clean images is studied by analyzing the effects of random image transformations on the distribution of softmax outputs. Samples of transformed images have different softmax outputs, leading to a distribution of softmax probabilities obtained from multiple samples of the transformation for each input image. The study analyzes the effects of image transformations on the distribution of softmax probabilities from a CNN model. The softmax distribution is approximated by computing marginal distributions over each class using kernel density estimation. The LeNet5 CNN trained with MNIST is used for the analysis. In this study, a LeNet5 CNN trained with MNIST is analyzed for its performance on clean and adversarial images. Adversarial images are generated using FGSM and transformed using pixel deflection. The test accuracy is 100% for clean images and 0% for adversarial images without any defense. The analysis excludes misclassified images by the CNN. In Figure 2a, the image transformation affects the voting predictions on two MNIST classes. Class label 8 shows some recovery from the attack, with clean images misclassified to other classes. A similar pattern is observed for class label 6, where clean images are misclassified to classes 4 and 5. The relationship between clean and adversarial images is characterized through this analysis. The relationship between clean and adversarial images is characterized by analyzing the JS divergence of softmax distributions at increasing pixel deflections. Distance measures are calculated for clean-clean, adversarial-adversarial, clean-adversarial, and clean-clean (different class) distributions. The distance between softmax distributions of two input images is determined by the Jensen-Shannon divergence. The Jensen-Shannon divergence is used to compute distance measures between clean and adversarial image distributions. Results for two MNIST classes show that as the number of deflections increases, there is increased variability in the softmax outputs. Adversarial images of the same class are initially predicted as different incorrect classes. The adversarial-adversarial distance decreases with more transformation, while the clean-adversarial distance decreases as well. The clean-clean distance decreases less rapidly and retains information about the differences between classes. At d=800, all distance measures converge. The adversarial-adversarial distance decreases with more transformation, while the clean-adversarial distance decreases as well. The clean-clean distance decreases less rapidly and retains information about the differences between classes. At d=800, all distance measures converge. The number of pixel deflections is too large, causing differences between classes to no longer be retained. Visualizing the morphing of distributions with increasing pixel deflections shows evolving distributions due to randomness. The voting mechanism classifies images based on distribution mass. Clean and adversarial images show similar distributions after transformation, leading to incorrect predictions. Examples in Figure 4 illustrate voting predictions for clean and adversarial images at d=300. Adversarial images successfully recover correct class with transformation defense and voting. After successful recovery of the correct class with the transformation defense and voting, similarities in distribution shapes between clean and adversarial images were observed. The voting accuracy on clean images decreased post-transformation, but the resulting distributions had similar features. To enhance transformation-based defenses, a distribution classifier was proposed to train on clean image distributions only, improving performance on both clean and adversarial images. Additionally, a separate compact distribution classifier was trained to recognize patterns in the distributions of softmax probabilities of clean images. The distribution classifier, trained on clean image distributions, can recover correct class labels for misclassified images without retraining the original CNN. It is agnostic to attack methods and can be integrated with existing transformation-based defenses. Three distribution classification methods were investigated, including distribution-to-distribution regression. Classification methods were explored for a distribution classification task. The methods included adapting a distribution regression network (DRN), random forest (RF), and multilayer perceptrons (MLP). Different techniques were used for each method, such as cross entropy loss for DRN and MLP, and Gini impurity for RF. Hyperparameters were tuned using cross-validation. In the experimental setup, the maximum tree depth is tuned by cross-validation. Hyperparameter values are provided in Appendix D.4. The performance is evaluated on clean and adversarial images using the MNIST, CIFAR10, and CIFAR100 datasets with high test accuracies. Various attack methods are used, and transformation-based defenses like random pixel noise are employed. In the defense method, random pixel noise (RPN) is used along with two existing transformation-based methods: pixel deflection (PD) and image random resize and pad (RRP). Hyperparameter tuning is done on validation sets to select parameters that provide the best recovery from adversarial attacks. The effectiveness of transformation-based defenses is tested before integrating with the defense method by performing majority voting on transformed image samples. Test accuracies on clean images are reported. The study tested the effectiveness of transformation-based defenses in combination with a classifier defense method. Test accuracies were reported for clean and adversarial images, with the distribution classifiers requiring only 1000 training data out of the original 50,000. Results showed the test accuracies of the transformation-based defenses with majority voting and distribution classifiers. The study compared the recovery on adversarial images using iterative methods like IGSM, DeepFool, and C&W with single-step FGSM. Results showed that distribution classifiers outperformed majority voting on clean images, with a mean accuracy improvement of 1.7% for DRN. The distribution classifier method was found to be stronger than voting, as it considers properties like variance across classes. The distribution classifier method improved the recovery rate on adversarial images by 5.9% for DRN. It outperformed other classifiers in some cases, showing similarities and distinctive features in the distributions of softmax between clean and adversarial images. The classifier successfully classified all clean and adversarial images of class 6, even though it was only trained on clean images. The distribution classifier can recover correct class for adversarial images by learning distinctive distribution shapes from clean images. It can pick up subtle differences in distribution features and distinguish between similar shapes for different classes. Evaluation time will be longer with N=100 transformed samples. The study analyzes the impact of the number of samples on classification accuracies for voting and DRN models. Results show that more samples improve accuracies on clean images, with DRN's performance continuing to increase. However, accuracies on adversarial images remain relatively constant. The distribution classifier enhances the voting performance regardless of the number of samples used. The distribution classifiers in the study showed improved performance over voting for CIFAR10 and CIFAR100 datasets, except for some cases where MLP performed worse. Random forest may perform better than other classifiers for datasets with more classes. Clean images misclassified by CNN and failed attack images were excluded from the evaluation. The distribution classifier method still showed effectiveness on these images. Our distribution classifier method outperforms majority voting, as shown in Table 14 in the Appendix. End-to-end attacks were evaluated on the distribution classifier method with DRN using the Boundary Attack on MNIST and CIFAR10 datasets. Different scenarios were tested, including attacks on base CNN, CNN with pixel deflection and voting, and CNN with pixel deflection and distribution classifier. Additionally, a lightweight adversarial training scheme was implemented, and the attack was also tested on an adversarially-trained CNN by Madry et al. (2017). The Boundary Attack was implemented on various models, including CNN and adversarially-trained CNN, with different levels of perturbations observed. The distribution classifier and lightweight adversarial training extension proved to be more resilient to attacks compared to majority voting. Attacks on random transformation defenses, such as Expectation over Transformation (EOT), pose challenges for white-box attacks on distribution classifier methods. The method requires 50-100 transformation samples per image, making it time-consuming to attack with EOT. The method has been shown to work with different distribution classifier models, including non-differentiable random forest classifiers, but the feasibility of combining attacks on random forests with EOT remains unclear. In this paper, the effects of stochastic transformation-based defenses on convolutional neural networks are analyzed. The distributions of softmax outputs from clean and adversarial images after transformation share similar features. A method is proposed to train a distribution classifier on transformed clean images, resulting in improvements for both clean and adversarial images. In this study, the effects of stochastic transformation-based defenses on convolutional neural networks are examined. A distribution classifier method is proposed to train on transformed clean images, showing improvements for both clean and adversarial images. Distance metrics for softmax distributions and examples of pixel deflection recovery from adversarial attacks are also analyzed. The use of a state-of-the-art distribution regression network (DRN) is discussed for achieving higher prediction accuracies in distribution regression tasks. In this work, a distribution classifier is adapted from a distribution regression network (DRN) for distribution classification. The network consists of fully-connected layers with nodes encoding distributions, and the number of hidden layers and nodes are chosen by cross validation. The cost function is the cross entropy loss on the logits, and optimization is done using the Adam optimizer through backpropagation. The optimization of the distribution classifier is done using backpropagation with the Adam optimizer. Weight initialization follows a specific method, and hyperparameter settings for adversarial attacks are shown in tables. Image transformation parameters are also detailed for different datasets. Hyperparameter tuning is conducted on the validation set for each defense method, prioritizing recovery from attacks over accuracy on clean images. The pixel deflection defense involves class activation maps. The pixel deflection defense uses class activation maps for selecting pixels, but in our experiments, we randomly select pixels without using CAMs. CAMs are unsuitable for the MNIST dataset due to the architecture, while they are tested on the CIFAR10 dataset with the wide ResNet architecture. Performance comparison on clean and adversarial images using FGSM and IGSM attacks shows that using CAMs does not significantly improve results. Random pixel noise (RPN), pixel deflection (PD), and random resize and padding (RRP) are defense methods tested. RPN uses unnormalized noise magnitude, PD involves deflections with window size and denoising parameter, and RRP includes resizing ranges. The performance difference is not significant, possibly due to CAMs being more effective on larger images like those in ImageNet. A distribution classifier is used for defense, training on softmax probability distributions from transformed clean image samples. Marginal distributions for each class are built using kernel density estimation with a Gaussian kernel. The kernel width is optimized to be 0.05. The network architecture and optimization hyperparameters for DRN, MLP, and random forest are chosen by cross-validation. Detailed numerical figures for accuracies of majority voting and distribution classifier methods are provided. Clean and adversarial test accuracies, attack methods, and defense methods are shown in Tables 11 to 13. Model outputs for Vote, DRN, and DRN LAT are random due to image transformations. Boundary Attack involves querying the model once and taking transformed samples for voting or feeding to the MNIST and CIFAR10 datasets. The curr_chunk discusses different configurations of RPN, PD, and RRP models for various datasets like CIFAR10, CIFAR100, and MNIST with different parameters such as n=100, d=20. The curr_chunk discusses the comparison of clean and adversarial test accuracies using different defense methods and classifiers on CIFAR100 dataset. The three transformation-based defenses are random pixel noise (RPN), pixel deflection (PD), and random resize and padding (RRP). The criteria for an image being adversarial is that it is misclassified at least once out of 5 queries. Boundary Attack may return an image classified to the correct class due to the randomness of the model. Boundary Attack increases perturbation until image is misclassified, multiple queries at each attack step are computationally infeasible due to model's use of transformed samples."
}