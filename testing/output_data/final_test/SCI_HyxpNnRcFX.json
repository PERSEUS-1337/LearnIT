{
    "title": "HyxpNnRcFX",
    "content": "Learning-to-learn, or meta-learning, uses data-driven bias to improve learning efficiency on new tasks. Challenges arise when transfer is not beneficial due to task dissimilarity or changes over time. A mixture of hierarchical Bayesian models is proposed to enhance gradient-based meta-learning, allowing for better task diversity capture. Experiments show improved generalization on the miniImageNet benchmark for 1-shot learning. Our experiments show improved generalization on the miniImageNet benchmark for 1-shot classification. A novel non-parametric variant of our method captures the evolution of task distribution over time, demonstrated on few-shot regression tasks. Meta-learning algorithms aim to increase learning efficiency by generalizing from task-specific episodes to improve sample efficiency. Recent algorithms learn global hyperparameters to provide a task-general inductive bias, assuming related tasks benefit from inductive transfer. Meta-learning algorithms assume tasks are equally related for gradient descent, but real-world scenarios may require different degrees of transfer. Positive transfer benefits from strongly related tasks, while negative transfer worsens generalization. Non-stationary task distributions need graceful degradation to address catastrophic forgetting. Consolidating inductive biases into a single set of hyperparameters lacks flexibility. In contrast to consolidating inductive biases into a single set of hyperparameters, this work proposes a mixture of hierarchical models for adaptive task-switching. The method involves learning task-specific parameter clusters in a hierarchical Bayes model, improving upon the model-agnostic meta-learning algorithm. By treating task-specific parameters as latent variables, similarities between tasks can be directly detected. Our approach involves detecting similarities between tasks using task-specific likelihood in a probabilistic model with latent variables, eliminating the need for explicit modeling assumptions about model weights. We extend this model to a non-parametric setting and utilize stochastic point estimation in a Dirichlet process mixture model for scalable inference. This non-parametric extension is novel and has not been explored in previous work, especially in high-dimensional parameter spaces like deep neural networks. The non-parametric extension in meta-learning allows a meta-learner to adapt to changing datasets by adding or removing clusters, preserving performance on previous tasks. Meta-learning aims to extract task-general knowledge from solving related tasks, enabling quick adaptation to new tasks with limited data or computation time. Recent approaches consolidate training task information into a mapping for novel tasks, such as a learned metric space, a trained recurrent neural network, or a gradient-based optimization algorithm with learned parameters. Model-agnostic meta-learning (MAML) is a gradient-based approach that estimates global parameters shared among task-specific models for a few steps of gradient descent. It can be interpreted as parameter estimation in a hierarchical model, where the learned initialization acts as data-driven regularization for task-specific parameters. MAML is cast as posterior inference for task-specific parameters given task data and a prior induced by early stopping of an iterative descent procedure. The E-STEP and M-STEP for a finite mixture of hierarchical Bayesian models involve adapting task-specific parameters \u03c6 to a diversity of tasks by estimating relatedness between tasks. This aids in modulating transfer effects in meta-learning algorithms. Defining task relatedness in high-dimensional spaces like neural networks is challenging, but a probabilistic interpretation helps by assuming task-specific parameters are drawn from a mixture of base distributions. The algorithm for end-to-end meta-learning involves estimating task relatedness by assigning tasks to mixture components based on task loss. It jointly learns task-specific cluster assignments and network parameters, facilitating information transfer across tasks for better generalization. The direct maximization of the mixture model likelihood is a complex optimization problem due to intractability issues. The posterior distribution over cluster assignment variables and task-specific parameters in meta-learning is estimated using a sampler. Various models like Gibbs Model, LSTM, SNAIL, MAML, and others are compared based on their performance. Our method involves clustering all layers of a neural network and using a sampler to draw from the conditional distribution of latent variables. In the context of meta-learning, a scalable approximation involves representing the conditional distribution for each latent variable with either a MAP value or an expectation. This suggests an augmented EM procedure alternating between an E-STEP and an M-STEP to compute task-to-cluster assignments and hyperparameters, respectively. The approach involves using a minibatch variant of stochastic optimization to compute task parameters and cluster assignments. The E-STEP leverages gradient-based meta-learning and hierarchical Bayes to compute posterior probabilities of cluster assignment based on training data likelihood. The approach involves using a minibatch variant of stochastic optimization to compute task parameters and cluster assignments. The cluster means are updated by gradient descent on the validation loss in the M-STEP. The algorithm follows an episodic meta-learning setup for training and testing, with task-specific parameters for dealing with task heterogeneity. The full algorithm is detailed in Algorithm 2, with a high-level structure shared with the non-parametric variant of the method. In Algorithm 2, the E-STEP and M-STEP are applied to an infinite mixture of hierarchical Bayesian models with 5 components for few-shot classification on miniImageNet. The parametric meta-learner improves gradient-based meta-learning generalization, showing cluster differentiation even on a non-heterogeneous benchmark. The mixture of meta-learners addresses drawbacks of traditional meta-learning approaches. The mixture of meta-learners in Section 3 improves meta-learning by dynamically adding components to specialize in different tasks as the task distribution changes. A scalable stochastic estimation procedure is derived to compute task-to-cluster assignments for a growing number of components. The E-Step computes task-to-cluster assignments for a growing number of task clusters using a Dirichlet process mixture model. This approach adapts its complexity based on observed data without a fixed number of components. The CRP prior allows for new mixture components to be assigned probabilities sequentially, aiding in online and stochastic learning of the DPMM. The E-Step assigns tasks to clusters using a Dirichlet process mixture model with a CRP prior, allowing for online learning. A stochastic EM procedure is developed for estimation, with a focus on task-specific parameters and cluster assignment variables. The E-Step assigns tasks to clusters using a Dirichlet process mixture model with a CRP prior. The estimation of cluster assignment variables in the E-STEP involves revisiting Gibbs conditional distributions. The distribution over task-to-cluster assignments can be expressed using a mode estimate for task-specific parameters. Gradient-based optimization is used for local objective functions, similar to the parametric M-STEP. The prior term can be omitted as it arises from truncated gradient descent. The approximate inference routine in Subroutine 4 includes a penalty term to incentivize larger clusters and deter overspawning. A threshold on cluster responsibilities is used to account for mini-batch noise when spawning a cluster based on a single batch. This threshold ensures that a new cluster's responsibility must exceed a certain value before being permanently added to the set of components. The model proposed in BID19 introduces a sequential approximation for nonparametric mixtures using variational Bayes. Unlike traditional algorithms, this model incrementally infers parameters and adds components during episodic training based on noisy estimates. It is the first to consider scalable stochastic point estimation in a non-parametric mixture model and apply it to high-dimensional function approximators like neural networks. The non-parametric mixture model can adapt to changing task distributions without external signals, while current meta-learning methods saturate model parameters with inductive biases. The model uses variational Bayes for sequential approximation and scalable stochastic point estimation in high-dimensional function approximators like neural networks. Our non-parametric meta-learning algorithm can acquire new inductive biases without saturating model parameters. Experimental results show adaptability to alternating regression tasks like sinusoidal and polynomial functions. In synthetic regression tasks, polynomial regression is followed by sinusoidal regression. A feedforward neural network with 2 hidden layers is used, along with a meta-batch size of 25 tasks. The non-parametric algorithm starts with a single cluster and computes cluster sizes using a moving window of size 20. The non-parametric algorithm starts with a single cluster and computes cluster sizes using a moving window of size 20 to prevent accumulation of assignments. Storing all assignments for an entire dataset in memory would be costly, so a stochastic setting is used. Task assignments are preserved to potentially index training iterations. The dataset generates quadratic, sinusoidal, and logistic regression tasks sequentially. The algorithm consistently outperforms MAML in terms of validation loss on the three tasks. Our algorithm outperforms MAML in validation loss on three tasks and maintains performance on old tasks when transitioning to new training phases. It shows the ability to adjust capacity for new phases, improving preservation of learned knowledge essential for continual learning. Task differentiation is explored through cluster responsibilities, revealing clear distinctions between quadratic, sinusoidal, and logistic regression tasks. The responsibilities for different tasks are indicated by cluster responsibilities. The first cluster decreases for the sinusoid task but remains split for odd and even polynomial regression tasks. A second cluster is created to differentiate between odd and even polynomials, showing similarities between them. The difficulty of the sinusoidal task explains the higher losses and longer training period. Regression losses are unbounded, posing challenges for optimization in continual learning. Continual learning datasets focus on classification tasks with bounded cross-entropy error. Negative transfer can harm generalization performance, leading to research on hierarchical Bayesian transfer learning. Previous approaches differ from ours by placing a prior only on the output layer and not being applied to meta-learning. Some propose training a mixture model over output layer weights, but scalability issues arise with full passes on the dataset. Our nonparametric algorithm addresses the \"task-agnostic\" setting of continual learning, adapting to task distribution shifts without explicit task delineation. In contrast to task-aware techniques like EWC and SI, our approach does not require model size growth. Incremental clustering methods have been explored in the EM setting and minibatch K-means for online learning. Our algorithm leverages the connection between empirical Bayes and gradient-based metalearning to use the MAML objective as a log posterior surrogate, allowing for scalability and integration with minibatch stochastic gradient-based meta-learning. This approach differs from recent work on gradient-based clustering by focusing on episodic meta-learning for both training and testing, which presents challenges for clustering algorithms. The approach presented allows a gradient-based meta-learner to modulate transfer between tasks and adapt parameter dimensionality. It involves probabilistic inference in a mixture model for clustering task-specific parameters, ensuring scalability through hierarchical Bayes. The approach involves approximate MAP inference in finite and infinite mixture models with non-conjugate likelihoods parameterized by a deep neural network. It allows model complexity to adapt to evolving task complexity in few-shot regression and classification problems. Various models are compared in terms of performance. An evolving dataset of miniImageNet few-shot classification tasks is discussed, where different artistic filters are applied to simulate a changing distribution of tasks. The standard dataset is trained for 20k iterations, followed by \"pencil\" effect tasks for 10k iterations, and finally \"blurred\" effect tasks until 40k iterations. Algorithm 4 is applied to this evolving variant using the BID46 BID8 architecture. Responsibilities for each cluster are plotted over time, showing changes at iterations 20k and 30k. During training, the evolving dataset of miniImageNet tasks transitions from standard tasks to \"pencil\" effect tasks and then to \"radial blur\" effect tasks. Cluster responsibilities are shown in Figure 7. The number of clusters is restricted to 1 initially but is lifted when new datasets are introduced. Inductive biases learned in the first 20000 iterations can transfer to new tasks as more datasets are added. Biases learned in the first 20000 iterations can easily transfer to filtered datasets, more so than different regression tasks experimented with."
}