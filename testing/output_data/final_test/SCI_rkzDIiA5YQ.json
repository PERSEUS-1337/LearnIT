{
    "title": "rkzDIiA5YQ",
    "content": "Distributed optimization is crucial for solving large-scale machine learning problems. To address the issue of slow nodes, or stragglers, an online distributed optimization method called Anytime Minibatch is proposed. This method allows nodes to compute gradients of varying minibatch sizes within a fixed time frame, preventing stragglers from delaying progress. The gradients are then averaged through consensus rounds to update primal variables via dual averaging. Convergence analysis is presented to support the effectiveness of Anytime Minibatch. The paper focuses on distributed optimization methods for large-scale machine learning problems, specifically addressing slow nodes or stragglers. It presents the Anytime Minibatch method for online distributed optimization, which allows nodes to compute gradients of varying minibatch sizes within a fixed time frame to prevent delays. The approach is shown to be faster in Amazon EC2 and more efficient with greater variability in compute node performance. The study categorizes recent advances into synchronous and asynchronous approaches, with a focus on the synchronous method and different synchronization methods based on the computing system's topology. In distributed optimization for large-scale machine learning, synchronization is crucial in computing systems, whether master-worker or fully distributed. Stragglers, slow processing nodes, can cause significant delays. Equal data processing by all nodes before synchronization is a classical requirement in parallel computing. The proposed approach, Anytime MiniBatch (AMB), aims to address the issue of straggler nodes in cloud-based computing systems by fixing the computation time in each epoch instead of the minibatch size. This prevents a single straggler from delaying the entire network while still allowing nodes to benefit from parallel optimization techniques. The Anytime MiniBatch (AMB) method addresses straggler nodes in cloud-based systems by fixing computation time per epoch, allowing nodes to benefit from parallel optimization. It ensures all workers share gradient information efficiently, achieving optimal performance for gradient-based algorithms. The online regret of AMB is O( \u221am ), where m is the expected sum of samples processed across nodes, with an upper bound on the expected wall time for a specified regret. The Anytime MiniBatch (AMB) method accelerates distributed learning by being O( \u221a n \u2212 1) faster than fixed minibatch methods in terms of expected wall time for a specified regret. Numerical simulations on Amazon EC2 show significant speedup with AMB. This work contributes to the literature on distributed optimization, building on previous research on distributed first-order methods and stochastic learning in large networks. Recent works have focused on distributed online stochastic convex optimization over networks with communications constraints, but they do not consider the straggler effect. Methods proposed to mitigate stragglers either ignore them or use redundancy for faster convergence. Our approach, in contrast to previous studies, utilizes both fast and slow working nodes to achieve faster convergence. We outline our computation and optimization model, detailing the three phases of the AMB algorithm. The algorithm's pseudo code is provided in the appendix. The system consists of n compute nodes represented as vertices in a connected graph. The collaborative objective is to find the parameter vector that solves a specific equation. The text discusses finding the parameter vector w that solves an equation using an unknown probability distribution Q. Nodes approximate the solution using data points drawn from Q in an i.i.d. manner. The optimization process involves three phases: compute, consensus, and update, with each node having primal and dual variables. During each epoch, workers compute local minibatch gradients of a 1-strongly convex function. The global minibatch size is aggregated over all nodes, accounting for variability in work completed. Convergence and wall time analyses are provided in subsequent sections. In the Consensus Phase, nodes communicate to approximate a quantity involving dual variables and gradients through synchronous rounds of average consensus. The number of consensus rounds varies due to random network delays, with a fixed communication time Tc. During the Consensus Phase, nodes communicate to approximate a quantity involving dual variables and gradients through synchronous rounds of average consensus. The iterations converge to the true average if the graph is connected and the second-largest eigenvalue of P is less than one. Each node will have an error in its approximation, denoted by \u03be i (t). After distributed averaging of dual variables, each node updates its primal variable using a normalized version of the distributed average solution. In this section, the performance of AMB is analyzed in terms of expected regret. The analysis considers the number of epochs processed and the size of minibatches processed by each node. A probabilistic model on processing time is presented in Sec. 5 to illustrate the advantages of AMB. The feasible space of the primal optimization variable is assumed to be a closed and bounded convex set in Euclidean space. The feasible space W \u2208 R d of the primal optimization variable w is a closed and bounded convex set where D = max w,u\u2208W w \u2212 u. The objective function f (w, x) is convex and differentiable in w \u2208 W for all x \u2208 X. It is Lipschitz continuous with constant L. The gradient of f (w, x) is Lipschitz continuous with constant K. The global minimum is denoted w * := arg min w\u2208W F (w). Consensus errors are bounded, and z(t) is the exact dual variable without any consensus errors at each node. The consensus errors are bounded, with z(t) representing the exact dual variable without any errors at each node. A lemma bounds the consensus errors, and the regret after \u03c4 epochs is characterized. Each node's computation cycles could have been used more efficiently if the consensus phase was shorter. The undone work of computing additional gradients does not impact system performance. The undone work of computing additional gradients does not impact system performance but affects the regret characterization. The total potential data samples processed in each epoch is calculated, and the ratio of total potential computations to actual completions is important for bounding the regret. The skewness of minibatches across epochs is crucial for this analysis. The computation of a i (t) and b i (t) depends on latent effects. In practice, a i (t) and b i (t) depend on latent effects, modeled as random variables. The expected regret for a fixed sample path is bounded. The average regret after \u03c4 epochs is defined, with a bound on R(\u03c4) given certain parameters. The expected regret for a fixed sample path is bounded by certain parameters, with the regret bound depending on summary statistics and communication speed relative to processing speed. Theorem 2 is a sample path result that scales with the aggregate number of samples. The final term in the equation scales with the number of samples, while the first term scales with \u03b2 and c max. Imperfect consensus can be reduced by increasing rounds of consensus. Variability across c(t) is reflected in terms with c max, c avg, and \u03b4 parameters. Perfect consensus would eliminate terms scaling with \u221am except for minibatch skewness parameter \u03b3. In the case of constant minibatch size and \u03b4 = 0, the expected regret is bounded by certain parameters. The expected regret is bounded by certain parameters when averaging consensus has additive accuracy. The regret is averaged over the sample path, with joint distribution over summary statistics rather than sample path. The expected regret is also over i.i.d. choice of data samples and (b(t), c(t)) pairs. In the preceding section, the expected regret is bounded by certain parameters when averaging consensus has additive accuracy. The advantages of AMB include a reduction in wall time, achieving the same convergence in less time than fixed minibatch approaches. Each epoch in AMB corresponds to a fixed compute time T, contrasting with fixed minibatch approaches with variable computing times. The regret per unit time is analyzed for FMB methods, where each node computes b/n gradients. The FMB method involves computing gradients in each epoch with a time distribution assumption. The expected minibatch size of AMB is shown to be at least as big as FMB when fixing the computation time. Theorem 7 demonstrates that AMB achieves the same or better regret bound in less time. Theorem 7 states that AMB achieves the same or better regret bound as FMB in less time. The method is shown to be at most 1 + \u03c3/\u00b5 \u221a n \u2212 1 faster than traditional FMB methods. The choice of shifted exponential distribution for modeling finishing time strikes a balance between analytical tractability and practical behavior. Based on the assumption of shifted exponential distribution, AMB is O(log(n)) faster than FMB, as proven in App. H. Performance evaluation experiments on Amazon EC2 for linear and logistic regression tasks were conducted for both schemes. Error vs. wall time performance was compared in two experiments using synthetic and real datasets. Additional simulations are provided in App. I, with the aim of collaboratively learning the true parameter w*. The aim is to collaboratively learn the true parameter w* for logistic regression using MNIST images of numbers 0 to 9. The data dimension is d=10^5, with each image represented as a 784-dimensional vector. The system aims to minimize the cost function using the softmax function to predict probabilities for 10 classes while streaming inputs online. In a fully distributed setup with 10 nodes, AMB and FMB schemes were tested for minimizing the cost function while streaming inputs online. The processors showed faster task completion initially before entering a steady state with consistent processor speeds. The performance of both schemes was compared over a long duration, focusing on the steady-state behavior. In a fully distributed setting with 10 nodes, AMB outperformed FMB in solving the linear regression problem. Each worker in FMB computed 6000 gradients with an average compute time of 14.5 sec. AMB achieved a lower error rate in less time compared to FMB, with a 25% reduction in total time spent to finish all epochs. Ignoring inter-node communication times, this ratio increased to almost 30%. In logistic regression using 10 distributed nodes, AMB outperformed FMB with fixed compute and communication times. AMB was 1.7 times faster than FMB, achieving the same error rate earlier. The proposed Anytime MiniBatch method fixes computation time per node, leading to faster convergence. The Anytime Minibatch (AMB) scheme in a distributed setting ensures deterministic finishing time for all nodes, independent of the slowest processing node. Convergence rate was proven in terms of expected regret bound, with significant improvements over fixed minibatch schemes shown in numerical experiments using Amazon EC2. The pseudocode outlines initialization, computation, and consensus phases for distributed averaging of dual variables. The distributed averaging algorithm involves updating dual variable z and primal variable w in each node. For hub-and-spoke configuration, a single consensus round is needed where workers send gradients to master node for calculation of z and w, followed by an update communication to workers. The convergence of the scheme is impacted by factors such as gradient calculation with respect to f (w, x) instead of exact gradient computation. The distributed averaging algorithm updates dual variable z and primal variable w in each node. Factors affecting convergence include gradient calculation with respect to f (w, x), limited consensus rounds errors, and variable minibatch sizes over epochs. The expected regret bound is found by bounding these errors. The primal variable w(t) is computed using the exact dual z(t). The total set of samples processed by the end of each epoch is denoted as X(t). The distributed averaging algorithm updates dual variable z and primal variable w in each node, considering factors like gradient calculation, consensus errors, and variable minibatch sizes. The total regret can be obtained by bounding these errors. The primal variable w(t) is computed using the exact dual z(t), with X(t) denoting the set of samples processed by the end of each epoch. Equation 31 shows terms related to initialization, consensus errors, and noisy gradient calculation, while Lemma 8 in equation 30 helps in obtaining the total regret. Equation 32 is used to derive DISPLAYFORM13 by substituting equation 37 into equation 36 and rearranging terms. The proof of Theorem 2.C is completed by showing that \u00b5\u03c4 = m. Lemma 8 discusses errors in computing gradients, including errors related to the function f(w, x) and the use of g(t) instead of \u1e21(t). Lemma 9 presents four relations, with the proof given in Appendix E. Lemma 9 presents four relations regarding the optimization process, with the proof provided in Appendix E. The relations involve approximations and smoothness of the function F(w). The text discusses various equations and substitutions related to optimization processes, with a focus on bounding expectations and rearranging terms. Lemma 9 provides relations on approximations and smoothness of the function F(w). Lemma 8 is proven by taking expectations over data samples and using various equations. The proof of Lemma 9 involves bounding expectations and utilizing the Cauchy-Schwarz inequality. The proof of Theorem 7 involves analyzing an FMB method where nodes compute gradients per epoch and derive the relationship between variables. The FMB method involves nodes computing gradients per epoch, while the AMB method has a fixed epoch duration. The expected minibatch size in AMB is at least as large as in FMB, with a different finishing time calculation. The shifted exponential distribution models a minimum time to complete a job, with a balance of processing time thereafter. The \u03bb parameter indicates processing speed. The expected finishing time is calculated using order statistics. In this section, additional details are provided regarding the numerical results and new findings from Section 6 of the main paper. The network used in Section 6 is detailed in Appendix I.1, with computations implemented in a master-worker network topology for comparison. Appendix I.2 models node compute times as shifted exponential random variables, contrasting AMB and FMB performance for linear regression. Appendix I.3 presents an experimental methodology for simulating a wide range of scenarios. In Appendix I.3, an experimental methodology is presented for simulating various straggler distributions in EC2 by running background jobs to slow down the foreground job. Another experiment in Appendix I.4 induces stragglers by forcing nodes to pause between gradient calculations. Numerical results show the advantage of AMB over FMB in distributed logistic regression, with a diagram of the computation network in Section 6. In Section 6, results for distributed logistic regression in a hub-and-spoke network topology with 19 worker nodes and one master node were presented. The batch size used was 3990, with each worker calculating 210 gradients per batch. The average EC2 compute time per batch was 3 seconds, and the communication time was 1 second. The logistic error versus wall clock time was plotted for both AMB and FMB schemes. In the master-worker (hub-and-spoke) network topology, the workers implementing AMB outperform those implementing FMB. The speed of each worker is modeled probabilistically, with a shifted exponential distribution assumed for the time taken to calculate gradients. The simulation results use \u03bb = 2/3 and \u03b6 = 1 for the distribution parameters. In the simulation results, the AMB scheme outperforms the FMB scheme in a master-worker network topology. Each worker computes gradients based on a fixed computing time allocated, with T chosen according to a specific theorem. 20 sample paths are generated, and after each computing epoch, 5 rounds of consensus are conducted. AMB consistently outperforms FMB across all sample paths, with minimal performance variance. In the discussion, a single sample path is chosen to show the impact of imperfect consensus on AMB and FMB. Consensus error for 5 rounds and perfect consensus is plotted in Figures 5a and 5b. The error is compared in terms of computing epochs and wall clock time, showing that AMB outperforms FMB significantly in real time. AMB reaches an error rate of 10^-3 in less than half the time of FMB. In a new experimental methodology, stragglers are induced among EC2 micro.t2 instances by running background jobs on 10 compute nodes. The nodes are divided into three groups: bad stragglers, intermediate stragglers, and non-stragglers. Bad stragglers interfere with foreground jobs, intermediate stragglers run a single background job, and non-stragglers do not run any background jobs. The experiment induced stragglers among EC2 micro.t2 instances by running background jobs on 10 compute nodes divided into three groups: bad stragglers, intermediate stragglers, and non-stragglers. The background jobs and FMB were launched simultaneously, causing a slowdown in computing for the first two groups. The histogram showed the FMB compute times, with the fast group clustered around 10 seconds per batch, while the other two groups were clustered at roughly 20 and 30 seconds. AMB was also tested, with similar results in fixed compute time. In the AMB experiments, bad straggler nodes cluster around a batch size of 230, while faster nodes cluster to the right. The FMB histogram had a fixed per-worker batch size of 585, while the AMB histograms had a fixed compute time of 12 seconds. Empirical results confirm the deterministic aspects of the statistical model, showing non-straggler nodes taking about 10 seconds to complete a fixed-sized minibatch, and intermediate nodes taking twice as long. The AMB experiments confirmed that intermediate straggler nodes complete only 50% of the work non-straggler nodes do in a fixed amount of time. Results show AMB is now twice as fast as FMB, with a 50% reduction in time to hit a target error rate. Increasing variation among stragglers leads to a greater improvement in AMB over FMB. Another experiment on an HPC platform also showed promising results. In a separate experiment on a high-performance computing platform with dedicated nodes, inducing stragglers was challenging due to the inability to control job placement. Instead, a different approach was used to induce stragglers in the MNIST classification problem using 51 nodes. In a study involving 51 nodes, stragglers were induced in the MNIST classification problem. The workers pause their computation after each gradient calculation, with the duration of the pause modeled as independent and drawn from a normal distribution. Larger mean values represent worse stragglers, while larger variance values indicate more delay variability. If the remaining time to compute gradients is less than the sampled pause duration, the node will pause until the end of the compute phase before proceeding to consensus rounds. In an experiment with 51 nodes, stragglers were induced in the MNIST classification problem. Workers pause their computation after each gradient calculation, with pause duration modeled as independent and drawn from a normal distribution. Larger mean values indicate worse stragglers, while larger variance values suggest more delay variability. If the remaining time to compute gradients is less than the sampled pause duration, the node will pause until the end of the compute phase before proceeding to consensus rounds. In the experiment, different parameters were chosen for the FMB and AMB setups, resulting in distinct compute times and minibatch sizes. Performance comparison between AMB and FMB for the MNIST dataset is shown in FIG12. The logistic regression performance of AMB is compared to FMB for the MNIST dataset. AMB achieves its lowest cost in 2.45 sec, while FMB takes 12.7 sec to reach the same cost, making AMB's convergence rate more than five times faster than FMB."
}