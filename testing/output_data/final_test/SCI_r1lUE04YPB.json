{
    "title": "r1lUE04YPB",
    "content": "In this study, the behavior of a neural language generator in the pretrain-finetune framework is examined, focusing on the transformer encoder-decoder model for dialogue response generation. It is found that standard fine-tuning leads to the model forgetting language generation skills acquired during pre-training. A fine-tuning strategy called \"mix-review\" is proposed to address this issue, showing effective regularization and alleviation of the forgetting problem. The resulting dialogue model's behavior and implications are also discussed. Large-scale unsupervised pre-training has significantly improved the performance of natural language processing models. Despite its success, a fundamental question remains: is there a weakness in the standard NLP pretrain-finetune framework? This work explores the concept of data separation in language generation, revealing that there is some weakness in the traditional two-stage pretrain-finetune procedure. In this work, the focus is on the pretrain-finetune framework for neural language generation, specifically in the context of open-domain dialogue response tasks. The study aims to analyze how pretrain-finetuning affects the model's behavior, particularly in terms of context sensitivity and knowledge transfer. The main finding is the potential for effective transfer of language generation skills from pre-training to downstream tasks. In the fine-tuning stage, data separation leads to the model forgetting language generation skills acquired during pre-training. To address this, a mix-review fine-tuning strategy is proposed, combining pre-training and fine-tuning objectives. This approach effectively regularizes the process and alleviates the forgetting problem. The resulting dialogue model's behavior and implications are discussed in detail. The model architecture used in NLG applications like dialogue response generation and machine translation is the transformer model. It has 6 encoder/decoder layers, 16 attention heads, with an embedding dimension of 1024 and a feed-forward dimension of 4096. The Adam optimizer is used during baseline training to minimize the negative log-likelihood of the reference target sentence given the input sentence. In NLG applications like dialogue response generation, the transformer model is used with specific architecture details. The decoding method of choice is top-k sampling, proven to be more effective than traditional beam-search decoding in recent research. In NLG applications, the transformer model uses top-k sampling for decoding. The pretrain-finetune framework for encoder-decoder models is discussed, focusing on language generation skills and the mix-review fine-tuning strategy. Two pre-training strategies are compared: next-sentence (NS) and masked sequence-to-sequence (MASS) pre-training. In NLG applications, the transformer model uses top-k sampling for decoding. The pretrain-finetune framework for encoder-decoder models is discussed, comparing next-sentence (NS) and masked sequence-to-sequence (MASS) pre-training. MASS is an extension of BERT pre-training, focusing on generating masked words in sentences. MASS disadvantages include single-sentence focus and lack of context from previous sentences, crucial for dialogue response generation. The pre-training stage of the transformer model focuses on acquiring knowledge and utilizing contextual input for dialogue response generation. The model can learn from large-scale text data to make responses more informative and engaging, while also being sensitive to contextual input to avoid generic responses. During fine-tuning, the model shows potential issues of over-fitting and forgetting, as seen in the gap between training-set NLL and validation-set NLL, and the drop in performance on pre-training data. The model experiences over-fitting and forgetting during fine-tuning, leading to a drop in performance on pre-training data. To address forgetting, a fine-tuning strategy called \"mix-review\" is proposed, where target dialogue data is mixed with a random subset of pre-training data in each epoch. This introduces hyper-parameters like mix-ratio and mix-decay to control the amount of mixed data. In the mix-review fine-tuning process, pre-training utterances are gradually reduced in each epoch, with a mix-ratio of 4 and mix-decay of 0.7 being effective. Hyper-parameter tuning shows that a small mix-ratio of 4 works well, with performance gains not highly sensitive to tuning. The computational cost of mix-review is comparable to standard fine-tuning. In mix-review fine-tuning, a mix-ratio of 4 and mix-decay of 0.7 are effective in preserving performance on pre-training data. Comparison with L2 regularization (weight decay) shows mix-review has a stronger regularization effect. Tuning \u03bb in WD(\u03b8pre) with \u03bb=0.1 also shows a regularization effect, though not as strong as mix-review. Additional basic regularization techniques were also tested. For pre-training, the large-scale CCNEWS dataset is used, containing 1 billion sentences and 27 billion words. The first 10 percent of the data is used for experiments. Fine-tuning is done using three open-domain conversational dialogue datasets. Additional regularization techniques like increasing dropout rate and freezing bottom layers show little improvement due to the transformer model's already well-tuned nature. For fine-tuning, three conversational dialogue datasets are used: Dailydialogue, Switchboard, and Cornell Movie. Byte Pair Encoding is used to construct the vocabulary, resulting in a size of 62k. The evaluation includes Perplexity and AMT-Rating for different training processes. Fairseq toolkit and Adam optimizer are utilized, with a mini-batch size of 2048 and LR set to 0.0001 for pre-training MASS and NS models. The pre-training process involves using a mini-batch size of 2048 and a learning rate of 0.0001, with an \"inverse square root\" LR scheduler. Pre-training is done on 32 GPUs with half-precision speed-up. The pre-training stops after the CCNEWS data is swept 20 times, despite the perplexity still improving, to control experiment duration. A dropout rate of 0.1 is applied to the transformer model. Fine-tuning is conducted on 2 GPUs without float16 speed-up, with the learning rate halved when the PPL on the validation set does not improve. Early-stop is implemented to prevent over-fitting. In this section, over-fitting is addressed by implementing early-stop when validation set performance deteriorates. The learning rate is tuned and the best model is selected based on validation set perplexity. Results for standard dialogue model evaluation are presented, followed by a detailed behavior analysis to understand how different training strategies impact the model's behavior. The focus is on whether the model forgets language generation skills during fine-tuning and if mixreview helps in retaining these skills. Additionally, human evaluation using Amazon Mechanical Turk is conducted to assess different training processes on dialogue data-sets. Each turker rates model sample responses based on a given dialogue context. The study involves rating model sample responses for fluency, consistency, and engagingness on a scale of 1 to 9. Bayesian inference is used to remove annotator bias, and results show significant improvements in perplexity for pre-trained models compared to baseline models. NS pre-training outperforms MASS by over 7%, indicating better utilization of contextual input. The study shows that NS pre-training outperforms MASS, with a focus on its contextual input utilization. Mix-review also improves performance due to strong regularization. However, the gap between mix-review and WD(\u03b8 pre) is not significant, suggesting the need for more sophisticated regularization techniques. Comparing model samples on the Dailydialogue test-set, pre-trained models show responses more related to context than baseline models. For example, while the baseline model mentions \"fruit cake\", pre-trained models mention beer as the main subject. The pretrained models in the conversation focus on beer, using sophisticated words like \"belgian ale\" and \"medium-batch\" not present in the training data. Dialogue response models trained from scratch lack sensitivity to context, as shown by previous experiments. Distorting context input through word-drop and word-shuffle tests was conducted to evaluate pre-trained dialogue models. The study evaluated the sensitivity of pre-trained dialogue models by distorting context input through word-drop and word-shuffle tests. Results showed that pre-training effectively changes the model's behavior, with the NS pre-trained model exhibiting better utilization of context compared to MASS. Pokemon Go, a popular mobile game where players catch Pokemon, took the world by storm in the mid-90s and again with the release of Pokemon Go. Ryan Reynolds played the wisecracking antihero Deadpool in a movie that became the highest-grossing R-rated film of all time. The study compares the performance of a pre-trained model on news-style triggers and dialogue-style triggers, showing that fine-tuning with dialogue data improves sensitivity to context input. The mix-review fine-tuning strategy effectively addresses the issue of the model forgetting important generation skills during standard fine-tuning. The study discusses how fine-tuning with dialogue data improves sensitivity to context input. A process is designed to quantify the model's knowledge acquired from pretraining data, using Google trend data to test the model's knowledge of trending terms. Three news-style and three dialogue-style trigger templates are used to monitor the model's behavior changes during pretrain-finetune framework. The study evaluates the model's response generation using trigger templates. Comparing BLEU scores of news and dialogue triggers, the pre-trained model performs better with news triggers due to its training data. Fine-tuned model shows lower scores. The study compares the performance of a pre-trained model and a fine-tuned model in response generation using trigger templates. The pre-trained model outperforms the fine-tuned model, demonstrating the forgetting problem of standard fine-tuning. Mix-review and WD(\u03b8 pre ) methods help retain knowledge acquired during pretraining, resulting in higher BLEU scores. Samples from different models confirm that mix-review facilitates knowledge retention and expression in a dialogue context. In a dialogue context, the model trained mix-review outperforms WD(\u03b8 pre) on knowledge, consistency, and engagingness in multi-turn dialogue evaluation using the ParlAI platform. Ratings from around 600 dialogues are collected for each model, with the model fine-tuned on Dailydialogue data showing superior performance. Examples of dialogues are included in the evaluation. In Figure 2, the UMAP projection shows that standard fine-tuned models differ significantly from pre-trained models in generative behavior. Mix-review regularization helps maintain similarity to pre-trained models. These findings align with results in Section 5.2 and 5.3. Additional details and plots are provided in Appendix E. The observations in Figure 2 highlight potential limitations of mix-review and WD(\u03b8 pre) in regularizing model behavior. The model trained by mix-review shows the ability to generate interesting responses and develop its own \"opinions\" based on pre-training knowledge, suggesting the potential for a data-driven knowledgeable chat-bot. The pre-trained dialogue model has developed its own opinions and can give advice to users. Pre-trained models are more easily triggered to respond maliciously when provoked, making it a relevant issue to address. This contrasts with the catastrophic forgetting problem in sequential learning. In the context of pre-trained dialogue models, a mix-review strategy is proposed to address the forgetting of language generation skills during standard finetuning. This strategy draws on concepts from multi-task learning and sequential learning algorithms, aiming to mitigate performance drops on pre-training data. This work is the first to analyze the forgetting problem for NLG models under unsupervised pretrain-finetune frameworks. In the context of pre-trained dialogue models, a mix-review strategy is proposed to address forgetting of language generation skills during standard finetuning. This work aims to analyze the forgetting problem for NLG models under unsupervised pretrain-finetune frameworks and introduces the concept of data mixing to mitigate performance drops on pre-training data. In this study, the mix-review fine-tuning strategy is proposed to analyze the forgetting problem in NLP pretrain-finetune frameworks. It is found that large-scale pre-training changes the model's generative behavior, influenced by the nature of data itself. The model can discuss news even when fine-tuning data is not about news, showing a data-driven way to customize language. The study proposes a mix-review fine-tuning strategy to address the forgetting problem in NLP pretrain-finetune frameworks. Large-scale pre-training alters the model's generative behavior, allowing it to discuss topics not present in the fine-tuning data. Results show that top-k sampling produces more diverse responses compared to beam search, which often leads to generic or repetitive responses in multi-turn dialogues. The study adopts top-k sampling as the main decoding method due to its diverse responses in multi-turn dialogues. The data-sets used include Dailydialogue and a collection of movie scripts, with details on the number of dialogues and words in each split provided. In this section, interaction samples between turkers and the model discussing assigned topics from Google Trend are shown. Additional samples from various training procedures for three dialogue datasets are presented in Table 11. Results deferred in the main body due to space limitations are supplemented here. Fluency/Consistency/Engagingness scores from the AMT Rating are displayed in Table 12. Context sensitivity results for Switchboard and Cornell Movie datasets are shown in Table 13, while knowledge transfer results for the Cornell Movie dataset are presented in Table 14. The input to UMAP for function space projection should be the model's output distributions, collected from 10k words in the CCNEWS and Dailydialogue validation sets. The default hyperparameter setting of the python implementation of UMAP is used, with results depicted in Figure 2 in the main body. During pre-training of the CCNEWS data, 20 epochs constitute one data pass. Fine-tuning is done from epoch 100, 200, 300, 400, 500 of the pre-training checkpoints. Average BLEU-2/BLEU-3 scores for model samples are compared to reference descriptions. The parameter space UMAP projection shows that fine-tuned models are close to pretrained models but exhibit different behavior. This suggests that parameter-space regularization may not be very effective."
}