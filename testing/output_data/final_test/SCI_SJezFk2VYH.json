{
    "title": "SJezFk2VYH",
    "content": "Variational Bayesian Inference is used to approximate posterior distributions over Bayesian neural network weights. Experimental findings suggest that restricting the variational distribution to a more compact parameterization can improve performance. Posterior standard deviations in deep Bayesian neural networks show a strong low-rank structure, allowing for a more compact variational approximation without sacrificing performance. Factorized parameterizations also improve the signal-to-noise ratio of stochastic gradient estimates. Bayesian Neural Networks represent parameter uncertainty with a posterior distribution, offering improved predictive performance and robustness. Variational inference is used to approximate the posterior distribution, as exact computation is often intractable. This method improves the signal-to-noise ratio of stochastic gradient estimates for faster convergence. Variational inference approximates the posterior distribution in Bayesian Neural Networks to improve predictive performance and robustness. The method minimizes the negative Evidence Lower Bound using a fully factorized Gaussian distribution as the variational approximation. The text discusses the use of Gaussian Mean-Field posteriors in variational inference for Bayesian neural networks. These posteriors are simple but have limitations, yet they scale well for large models. However, compared to deterministic neural networks, GMFVI has double the parameters and can be harder to train due to increased noise in gradient estimates. Recent research explores richer parameterizations of the approximate posterior to enhance the performance of Bayesian neural networks. The text discusses simpler mean-field variational posteriors for Bayesian neural networks, contrasting with more complex structures. It shows that converged posterior standard deviations exhibit a strong low-rank structure, allowing for effective decomposition of variational parameters. By decomposing variational parameters into a low-rank factorization, the variational approximation becomes more compact without sacrificing model performance. Factorized parameterizations of posterior standard deviations improve signal-to-noise ratio and lead to faster convergence. Posterior standard deviation matrices exhibit a natural low-rank structure, motivating the introduction of the k-tied Normal variational family. The k-tied Normal variational family is introduced as a more compact version of the standard Gaussian mean-field variational distribution. Despite its compactness, it matches the fully parametrized GMFVI in terms of ELBO and predictive performance. The total number of standard deviation parameters is reduced from quadratic to linear in the dimensions of the layer, making it more efficient. The k-tied Normal variational family reduces the total number of standard deviation parameters from quadratic to linear in the layer dimensions, leading to higher signal-to-noise ratio during training and faster convergence. The distribution is similar to the matrix variate Gaussian distribution when k = 1, but cannot be represented by it for k \u2265 2. MNIST dataset, LeNet-type CNN trained on CIFAR-100 dataset, vanilla LSTM model trained on IMDB dataset. Experiments focus on comparing model types rather than state-of-the-art results. Scaling GMFVI to larger models is a challenging research problem. Posterior standard deviations of dense layers in LeNet CNN trained using GMFVI display strong low-rank structure. The posterior standard deviations of dense layers in the LeNet CNN trained using GMFVI show a strong low-rank structure. This structure allows for a post-training approximation with minimal loss in performance. Similar results were observed in MLP and LSTM models as well. The k-tied Normal posterior in GMFVI training achieves competitive performance with standard GMFVI posterior, reducing model parameters. It benefits models with dense layers like MLPs and CNNs for classification, increasing gradient signal-to-noise ratio and speeding up convergence of negative ELBO objective. Bayesian Neural Networks trained with standard Gaussian meanfield variational inference can have posterior standard deviation matrices approximated with low-rank decomposition. This insight led to a proposal for a simple yet effective variational posterior parametrization that speeds up training and reduces variational parameters without sacrificing predictive performance. Future work aims to scale up this approach to larger models and more complex problems. Improving signal-to-noise ratio of ELBO gradients using compact variational parameterizations may address challenges in neural network settings. Variational inference in neural networks has evolved since Peterson (1987) and Hinton and Van Camp (1993), focusing on expressive posterior distributions and structured variational parameters for easier learning and scalability. In the context of variational inference in neural networks, the discussion focuses on Gaussian variational distributions with full covariance matrices. Various schemes have been proposed to efficiently represent the dense covariance matrix in terms of variational parameters. In Bayesian neural networks, exploiting the natural structure of layers can simplify the covariance matrix by assuming independence across layers, resulting in a block-diagonal structure. Each layer's covariance matrix can be represented by a Kronecker product of smaller matrices, possibly with a parametrization based on rotation matrices. Zhang et al. (2017) proposed a block tridiagonal structure to approximate dense covariance behavior. The fully factorized Gaussian variational distribution is commonly used in Bayesian neural networks due to its simplicity and scalability. Despite limitations such as underestimated variance and robustness issues, it offers competitive performance and scalability. This approach has been widely adopted in various studies for its practicality. Our approach aims to reduce the number of parameters in neural networks by exploring tying strategies beyond rank-1 covariance matrices. This approach outperforms previous methods in terms of ELBO and predictive metrics. We also demonstrate that tying strategies with a rank greater than one cannot be represented in a matrix variate Gaussian distribution. The approach aims to reduce neural network parameters by using tying strategies beyond rank-1 covariance matrices. It outperforms previous methods in ELBO and predictive metrics. Tying strategies with a rank greater than one cannot be represented in a matrix variate Gaussian distribution. The k-tied Normal distribution is connected to the matrix variate Gaussian distribution MN. The k-tied Normal distribution, when k \u2265 2, cannot be represented as a matrix variate Gaussian distribution. This is proven by showing that there are no matrices Q and P that satisfy diag(vec(B)) = P \u2297 Q for a rank-2 matrix B in R r\u00d7c +. The k-tied Normal distribution cannot be represented as a matrix variate Gaussian distribution, as there are no matrices Q and P that satisfy diag(vec(B)) = P \u2297 Q for a rank-2 matrix B in R r\u00d7c +. If D = P \u2297 Q, then D = diag(p) \u2297 diag(q) with p \u2208 R c and q \u2208 R r, leading to a contradiction since qp has rank one while B is assumed to have rank two. The post-training analysis of GMFVI posterior standard deviations reveals a low-rank structure across different model types (CNN, MLP, LSTM). SVD decomposition shows per rank percentage of explained variance, indicating most variance is captured by low-rank approximation. The post-training analysis of GMFVI posterior standard deviations shows a low-rank structure. Most variance is captured by rank-1 approximation, with rank-2 covering the remaining variance. This behavior is not observed in posterior mean parameters. Low-rank approximations of the posterior standard deviation matrix maintain performance without decrease. The post-training analysis of GMFVI posterior standard deviations reveals a low-rank structure, with higher ranks achieving predictive performance close to full-rank matrices. This observation suggests potential network compression methods and exploration of low-rank structures in posteriors, such as the k-tied Normal posterior. The impact of post-training low-rank approximation on ELBO and predictive performance is shown in Table 2, while Figure 6 illustrates the impact of the k-tied Normal on GMFVI convergence speed. The impact of the k-tied Normal posterior on GMFVI convergence speed varies depending on the model type. It significantly increases convergence speed for MLP and LSTM models, but has a smaller effect on CNN models. This difference may be due to the selective use of the k-tied Normal posterior in CNN model layers compared to its broader application in MLP and LSTM models. In the CNN model, the k-tied Normal posterior is used only for the two dense layers, while the convolutional layers use the standard parametrization of the GMFVI. The impact of the k-tied Normal posterior with different ranks on convergence is shown in Figure 6 for MLP, CNN, and LSTM models. The three models used in the study are a MLP, CNN, and LSTM. The CNN model has two convolutional layers and two dense layers, trained on the CIFAR-100 dataset. The LSTM model consists of an embedding, LSTM cell, and a single unit dense layer, trained on an IMBD dataset. The study uses MLP, CNN, and LSTM models with GMFVI for weight approximation. Normal prior and variational posterior are used with specific initialization and hyperparameters. Selection of hyperparameter is based on validation data performance. The study uses MLP, CNN, and LSTM models with GMFVI for weight approximation. Optimization is done using an Adam optimizer. The optimal learning rate and batch size for each model are selected based on validation data performance. Low-rank structure analysis is conducted on posterior standard deviation matrices. Negative values in low-rank approximations are thresholded to meet constraints. The k-tied Normal variational posterior is used for all dense layers in the GMFVI models, including the MLP, CNN, and LSTM models. Parameters are initialized to ensure standard deviations have mean values of 0.01 before transforming to the log-domain. In the log domain, parameters u ik and v jk are initialized with a specific formula and white noise is added to break symmetry. KL annealing is recommended for training models with the k-tied Normal posterior, gradually increasing the contribution of the KL term during training. KL annealing is used for test performance results but not for SNR and negative ELBO convergence results to show the impact of the k-tied Normal posterior clearly."
}