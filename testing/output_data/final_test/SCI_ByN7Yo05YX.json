{
    "title": "ByN7Yo05YX",
    "content": "Deep neural networks and decision trees are typically separate in their approaches, with the former focusing on representation learning and the latter on hierarchies over pre-specified features. Adaptive neural trees (ANTs) combine these paradigms by incorporating representation learning into decision tree components, allowing for increased interpretability through hierarchical clustering. ANTs achieve high accuracy on classification and regression tasks, outperforming standard neural networks, random forests, and gradient boosted trees on various datasets. Neural networks and decision trees are powerful machine learning models with distinct benefits and limitations. Neural networks learn hierarchical representations of data without the need for feature engineering, while decision trees focus on hierarchies over pre-specified features. Neural networks are trained with stochastic optimizers, allowing for scalability to large datasets. ANT optimization adapts the architecture to the size and complexity of the training data, achieving high accuracy on classification and regression tasks. Neural networks are powerful models that require hand-designed architectures and heavy-weight inference, while decision trees learn hierarchical clusters of data and are optimized based on training data, making them advantageous in data-scarce scenarios with lightweight inference. The goal is to combine neural networks (NNs) and decision trees (DTs) to leverage their complementary benefits. Adaptive neural trees (ANTs) are proposed to overcome the limitations of single DTs and achieve state-of-the-art performance in various tasks. Adaptive neural trees (ANTs) combine neural networks (NNs) and decision trees (DTs) to benefit from hierarchical representation learning. ANTs adapt their architectures based on data, inheriting desirable properties from both DTs and NNs. Features can be learned end-to-end with gradient-based optimization, allowing for complex data partitioning. Adaptive neural trees (ANTs) combine neural networks and decision trees for hierarchical representation learning. ANTs adapt their architectures based on data, allowing for complex data partitioning. At inference time, ANTs perform conditional computation, selecting a single path on the tree per sample, activating only a subset of the model parameters. Experimental results show that ANTs outperform state-of-the-art methods on image-based classification datasets. The best performing methods on the SARCOS multivariate regression dataset are tree-based, with soft decision trees (SDTs), GBTs, and ANTs achieving the lowest mean squared error. ANTs can learn meaningful hierarchical partitionings of data and have reduced time and memory requirements during inference. ANTs achieve over 98% accuracy on MNIST using a similar number of parameters as a linear classifier on raw image pixels, showing the benefits of modeling a hierarchical structure for enhanced computational and predictive performance. The study demonstrates the benefits of architecture learning by training ANTs on subsets of CIFAR-10, leading to better generalization on small datasets. ANTs combine decision trees and neural networks for improved representation learning, subsuming prior work and addressing their limitations. The method includes reviews of conditional computation and neural architecture search. The hierarchical mixture of experts (HMEs) proposed by BID26 is a variant of SDTs using linear classifiers as routers. More recent SDTs in BID46 BID32 utilized multilayer perceptrons (MLPs) or convolutional layers in routers for complex partitioning of input space. However, integrating non-linear transformations into decision trees, like the neural decision forest (NDF) BID30, can enhance model performance. NDF, an ensemble of DTs, achieved cutting-edge performance on ImageNet in 2015. The GoogLeNet architecture is used as the root transformer for learning tree-structured classifiers with linear routers. Different approaches like BID57 and BID22 have been optimized to reduce compute cost and parameters. ANTs provide a general framework for learning tree-structured models with representation learning capabilities. The architecture growth in decision trees involves representation learning and learning its structure. Previous strategies include decision jungles merging input spaces and budding trees grown based on global optimization. The proposed training algorithm grows the architecture by choosing between going deeper or splitting the input space. Adaptive Neural Trees (ANTs) are a form of decision trees enhanced with deep, learned representations for supervised learning. The aim is to learn the conditional distribution p(y|x) from a set of labelled samples. ANT features are optimized for the target task and can be learned end-to-end, improving representational quality. Adaptive Neural Trees (ANTs) are tree-structured models for supervised learning, characterized by hierarchical partitions of the input space and separate predictive models in component regions. ANTs consist of a pair (T, O) where T defines the model topology as binary trees, and O denotes operations on it. The model topology T is defined as a set of finite graphs with internal and leaf nodes connected by edges. Adaptive Neural Trees (ANTs) are tree-structured models for supervised learning with hierarchical partitions of the input space. Each node j in the tree has two children, left(j) and right(j), connected by edges. Operations are assigned to nodes and edges, transforming samples as they traverse the tree based on a set of operations O. The tree is constructed using three primitive modules: routers, which send samples to left or right child nodes based on stochastic routing, and other differentiable operations. Adaptive Neural Trees (ANTs) are tree-structured models for supervised learning with hierarchical partitions of the input space. Each node in the tree has two children connected by edges. Transformers are used on the edges to transform samples from one module to the next. Solvers are assigned to leaf nodes to output estimates for the conditional distribution. Operations on the graph T include convolutional layers and linear classifiers for classification tasks. The ANT model uses a tree structure with operations on the graph to define a distribution over classes. Computational paths and routers are guided by CNNs, and the model addresses limitations of existing tree-structured models. Each node in the tree corresponds to a NN defining a root-to-leaf path. The ANT model utilizes a tree structure with operations on the graph to define a distribution over classes. Each input traverses the tree based on router decisions and undergoes transformations until reaching a leaf node for label prediction. The model differs from traditional HMEs by transforming inputs within the tree hierarchy. The ANT model uses a tree structure with router decisions and transformations to predict labels at leaf nodes. Parameters describe router, transformer, and solver modules. The mixing coefficient quantifies the probability of assignment to a leaf node, while leaf-specific conditional distribution estimates the target distribution. The ANT model uses a tree structure with router decisions and transformations to predict labels at leaf nodes. Parameters describe router, transformer, and solver modules. The mixing coefficient quantifies the probability of assignment to a leaf node, while leaf-specific conditional distribution estimates the target distribution. Inference in the model can be done using either the full predictive distribution or a more efficient approximation based on the leaf node with the highest confidence. Training of the model occurs in two stages: growth phase for learning the model architecture and inference phase for making predictions. The ANT model uses a tree structure with router decisions and transformations to predict labels at leaf nodes. Parameters describe router, transformer, and solver modules. The model proceeds in two stages: growth phase for learning the model architecture based on local optimization, and refinement phase for tuning parameters based on global optimization. Gradient-based optimization is used for learning the parameters, with backpropagation for gradient computation. The negative log-likelihood is minimized as the common objective function. Pseudocode for the joint training algorithm is included in the supplementary material. Our proposed method involves growing a tree architecture by adding computational modules at leaf nodes in breadth-first order. Three choices are evaluated at each leaf node: split data, deepen transform, or keep the current model. Parameters of the added modules are optimized locally via gradient descent. The model with the lowest validation NLL is selected, otherwise, the original model is kept. The proposed method grows a tree architecture by adding computational modules at leaf nodes in breadth-first order. The model evaluates three choices at each leaf node: split data, deepen transform, or keep the current model. The process is repeated level-by-level until no more operations pass the validation test. Splitting a node partitions the feature space, creating two new leaf nodes with identity functions as transformer modules. Deepening an edge seeks to learn richer representations via nonlinear transformations. Local optimization saves time, memory, and compute by computing gradients only for the parameters of the new parts. During the growth phase, a tree architecture is expanded by adding modules at leaf nodes in breadth-first order. Gradients are computed only for new parameters, saving time and memory. The model topology is finalized, followed by global optimization to refine parameters and improve generalization error. ANTs are evaluated using various datasets for object classification and regression tasks. In this study, ANTs are shown to learn hierarchical structures in data and achieve favorable classification accuracies compared to DT and NN models. The refinement phase of ANTs can automatically prune the tree, and the training procedure adapts the model size appropriately with varying amounts of labeled data. The models are constructed using the PyTorch framework and trained with a range of primitive modules, outperforming DT methods like RFs and GBTs on complex image data. In comparison to CNNs, ANTs show a balance between strong performance with fewer parameters. Primitive modules consist of convolutional, global-average-pooling, and fully-connected layers. Solver modules are linear classifiers with softmax output, while router modules are binary classifiers with sigmoid output. ReLUs are applied to convolutional and FC layers, with max-pooling after every d transformer modules. The number of parameters in routers is balanced. The number of parameters in the router and transformer modules is balanced to avoid favoring data partitioning or feature learning. 10% of training images are held out for validation, and the best model is selected based on performance. Two inference schemes are used for classification, with single-path inference being more efficient than multi-path inference due to confident splitting probabilities in the routers. In Tab. 3, the Error (Full) and Error (Path) show a 0.06% difference, with a reduction in parameters across all ANT models. Patience-based local optimization is used during the growth phase to prevent underfitting or overfitting. A patience level of 5 produced good performance on MNIST and CIFAR-10 datasets. ANT-MNIST-A outperforms in digit classification. In digit classification, ANT-MNIST-A outperforms state-of-the-art methods despite using a single tree. It is constructed in a data-driven manner and shows improvement in accuracy and parameter efficiency compared to NDF. Reducing convolution kernel size in ANT-MNIST-B decreases parameters by 25% and path-wise average by almost 40% with minimal increase in error. Comparisons are made against LeNet-5 CNN. Comparing against LeNet-5 CNN, ANT-MNIST-A and ANT-MNIST-B achieve better accuracy with fewer parameters. Ensembling ANTs can reach similar performance with significantly fewer parameters. ANT-MNIST-C achieves a low error rate with single-path inference. The ANT-MNIST-C model achieves a low error rate of 1.68% with single-path inference, outperforming the linear classifier. Increasing the number of kernels in the root-to-path CNN resulted in a higher error rate of 3.55%, highlighting the benefits of data partitioning. ANT variants in CIFAR-10 object recognition surpass the state-of-the-art method, gcForest BID61, with over 90% accuracy, showcasing the advantages of representation learning in tree-structured models. The ANT-CIFAR-A model achieves over 90% accuracy, demonstrating the benefits of representation learning in tree-structured models. It outperforms CNN models without shortcut connections and achieves higher accuracy with fewer parameters in single-path inference. The use of simpler primitive modules results in more compact models with a marginal compromise in accuracy. Initializing parameters from a pre-trained single-path CNN further reduces the error rate, indicating room for improvement in the proposed optimization method. The best network, ANT-MNIST-A*, has a comparable error rate and half the parameter count compared to the best-performing residual model. Shortcut connections could improve ANT performance, leading to better accuracy with fewer parameters. The refinement phase polarizes path probabilities, pruning a branch. Ablation study compares classification errors of different ANTs variants with transformer or router modules disabled. The resulting models are equivalent to SDTs or HMEs with locally grown architectures or standard CNNs grown adaptively layer by layer. Ablation consistently leads to improvements. The growth procedure of ANTs discovers hierarchical structures in data useful for the end task, displaying strong specialization of paths to certain classes on MNIST and CIFAR-10 datasets. Human intuitions on relevant hierarchies may not always lead to optimal representations, highlighting ANTs' ability to optimize data representation for performance. The model learns meaningful routing strategies to optimize end-task performance by sharing or separating data representations. Test accuracy drops significantly when using the least likely \"expert\" in the learned trees, which are mostly unbalanced. Global refinement phase improves generalization error in ANT models on CIFAR-10. During the refinement phase, models converge to higher test accuracy after an initial drop, indicating a remedy for suboptimal decisions made during the growth phase. Global optimization polarizes decision probabilities, leading to effective pruning of some branches. For example, decision probabilities become more concentrated near 0 or 1, reducing the probability of visiting certain leaf nodes. The pruning of branches in the model reduces the validation set error to 0.09%, leading to a lower generalization error. The ANT training method adapts model complexity based on varying amounts of labeled data, showing improved performance on small datasets. Classfication experiments on CIFAR-10 with different dataset sizes show the effectiveness of the proposed approach compared to the baseline All-CNN model. The ANT models outperform All-CNN and linear classifier on small datasets, with a margin of up to 13% in test accuracy. The number of parameters in discovered ANTs increases with dataset size, while All-CNN remains consistently larger and prone to overfitting. The proposed method constructs models of adequate complexity for better performance. Our method, Adaptive Neural Trees (ANTs), combines decision trees with deep neural networks through a training algorithm that optimizes both parameters and architectures for better generalization. ANTs outperform other models on small datasets and are validated on regression and object classification tasks with high performance. The training algorithm for Adaptive Neural Trees (ANTs) optimizes parameters and architecture by initializing a topology and optimizing parameters via gradient descent. The tree structure of ANTs naturally performs conditional computation and can be seen as a form of neural architecture search. Conditional computation in neural networks (NNs) involves activating only a small fraction of the model for each sample, as opposed to routing each sample through every parameter. This concept, advocated by Bengio, is gaining interest for its ability to adapt computation to input difficulty. The growth procedure of Adaptive Neural Trees (ANTs) shares a similar motivation, aiming to optimize architecture for computer vision tasks by adapting to the specific requirements of the input. The ANT growing procedure in current state-of-the-art architectures adapts complexity to the dataset, with routers determining computation on a per-sample basis. Neural Architecture Search, mainly through greedy layerwise training, has historically optimized NNs, but now NNs can be trained end-to-end. Lifelong learning still uses progressive growing, with BID58 introducing a method for accommodating new classes with a tree-shaped network. In a tree-shaped network, the method adapts complexity to the dataset by determining computation on a per-sample basis. The search space is constrained to simple tree-structured NNs for data-dependent computation and interpretable structures. Experiments are conducted on the MNIST digit classification and CIFAR-10 object recognition tasks. The MNIST dataset has 60,000 training and 10,000 testing examples of grayscale digit images. CIFAR-10 dataset has 50,000 training and 10,000 testing examples of colored natural images. Data augmentation is used for CIFAR-10 by zero-padding, random cropping, and horizontal mirroring. 10% of training images are held out for validation. The best model is selected based on validation accuracy during training, and its accuracy on the testing set is reported. The growth and refinement phase of ANTs take up to 2 hours on a single Titan X GPU for both datasets. The training protocol includes optimizing parameters using Adam BID28 with specific settings, employing early stopping during the growth phase, and training for a set number of epochs during the refinement phase. The time taken on a single Titan X GPU for the growth and refinement phase of various ANTs is compared against the training time of All-CNN. During the growth phase of ANTs, local optimization allows for model growth under 2 hours on a single GPU. Different patience levels affect model accuracy, with a level of 5 yielding the best results at 91% validation accuracy. Comparing routing strategies, using the least likely \"expert\" leads to a significant drop in classification accuracy. The ANT algorithm demonstrates a drop in classification accuracy for large trees, showing the capability to split the input space meaningfully. Most architectures learn a few levels of features before primarily resorting to splits, with all architectures being unbalanced. The ANT algorithm is general purpose and can be applied to problems other than image classification. The ANT algorithm, which can be applied to various problems, including regression on the SARCOS robot dataset, outperforms all other methods. ANT-SARCOS outperforms all other methods in mean squared error with the full set of parameters, highlighting the power of splitting the input space and conditional computation. Tree-based methods like ANT-SARCOS and SDT require fewer parameters than the best-performing GBT configuration, showing the benefits of representation learning. Deeper NNs can overfit on this small dataset, making tree-based methods ideal for good generalization. In an ablation study, the regression error of the ANT model is compared with and without transformer or router modules. The results show that disabling these components leads to higher regression errors across different module configurations. Additionally, ensembling ANTs can improve performance. Ensembling ANTs can improve performance, with results showing noticeable improvements in both full and single-path inference performance. In MNIST, close to state-of-the-art performance is achieved with significantly fewer parameters compared to single tree models."
}