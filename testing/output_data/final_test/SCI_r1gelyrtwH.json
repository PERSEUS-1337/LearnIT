{
    "title": "r1gelyrtwH",
    "content": "Sparsely available data points cause numerical errors in finite differences, especially when data is irregularly distributed on an unstructured grid. A novel architecture called Physics-aware Difference Graph Networks (PA-DGN) is proposed to handle physics-governing observations on such grids. PA-DGN utilizes neighboring information to learn finite differences inspired by physics equations and uses data-driven learning to uncover dynamical relations in observations. The superiority of PA-DGN is demonstrated in approximating directional derivatives and predicting graph signals on synthetic and real-world climate data from weather stations. Modeling real-world phenomena like climate observations is challenging, especially with limited data points. Deep learning has been successful in data-rich applications but faces difficulties in natural phenomena modeling. Sparsely available data causes numerical errors, requiring a more principled approach to redesign deep learning models. Many works propose using deep learning to model physics-simulated observations, such as Physics-informed neural networks (PINNs) by Raissi et al. (2017a) and methods for image restoration by Chen et al. (2015). de Bezenac et al. (2018) incorporated transport physics with deep neural networks for forecasting sea surface temperature. Neural networks for forecasting sea surface temperature by learning Lagrangian mechanics with physics-informed regularizer. Previous methods are not efficient for sparsely discretized input due to limited data points and difficulty in recovering continuous properties. Directly using continuous differential operators is challenging with sparse points. Equations are difficult to generalize for different types, leading to the proposal of PDE-Net by Long et al. (2018) and new CNNs by Ruthotto & Haber (2018) based on PDE theory. These models uncover hidden physics but are limited to regular grids. Graph-based neural networks have also been studied for handling sparsely located data points without explicit physics. Our method, Physics-aware Difference Graph Networks (PA-DGN), leverages differences in sparsely available data from physical systems. The proposed Spatial Difference Layer (SDL) efficiently learns local representations by aggregating neighboring information. This approach considers physics-inspired inductive bias to learn spatiotemporal data from the physics system. The Spatial Difference Layer (SDL) utilizes Graph Networks (GN) to learn localized representations by aggregating neighboring information in sparse data points. It is connected with Recurrent Graph Networks (RGN) to incorporate temporal differences in physics-related dynamic equations. PA-DGN addresses limitations of sparsely discretized data, reducing numerical errors in modeling physical systems. Incorporating SDL with Recurrent Graph Networks, PA-DGN learns spatiotemporal dynamics in graph signals effectively. It approximates directional derivatives and predicts graph signals accurately. PA-DGN outperforms other baselines in predicting climate observations from weather stations. The module used to learn spatial differences of graph signals is introduced, showing how it predicts signals in the physics system using difference operators as core tools for computing numerical solutions of differential equations. Operators are essential for solving PDEs in physics-related data like meteorological observations. Graph signals on nodes and edges are represented by functions on a graph, with gradients providing finite differences of signals. The Laplace-Beltrami operator is also discussed. The Laplace-Beltrami operator in the graph domain is defined as a matrix form, L = D - A, where A is the adjacency matrix and D is the degree matrix. The gradient and Laplacian operator on a triangulated mesh can be discretized by incorporating node coordinates. The gradient on each node is the area-weighted average of its neighboring faces, and the Laplacian operator can be discretized with Finite Element Method (FEM). The Spatial Difference Layer (SDL) is proposed to address limitations of difference operators in Riemannian manifolds, especially when nodes are far from neighbors or data points are sparse. SDL utilizes learnable difference operators like gradient and Laplacian to fully leverage neighboring information. The Spatial Difference Layer (SDL) utilizes learnable difference operators such as gradient and Laplacian to fully leverage neighboring information. It integrates local information by considering a larger view (h-hop) to represent the differences between nodes, making it a higher-order difference equation. Graph Networks (GN) are used to aggregate neighboring information efficiently, with w ij representing edge features from the output of GN. The Spatial Difference Layer (SDL) utilizes learnable difference operators like gradient and Laplacian to leverage neighboring information effectively. It extends to conventional kernels such as edge detection or sharpening filters by modulating parameters like w ij. This approach provides a physics-inspired inductive bias, effective for modeling physics-related observations. Increasing the number of channels for w (g) ij and w (l) ij can enhance expressiveness. The Spatial Difference Layer (SDL) uses learnable difference operators to leverage neighboring information effectively. It extends conventional kernels by modulating parameters like w ij, enhancing expressiveness. The difference graph includes all information for spatial variations, and recurrent graph networks predict next graph signals using learned spatial differences. The Spatial Difference Layer (SDL) utilizes learnable difference operators to enhance neighboring information. Recurrent Graph Networks (RGN) combine spatial differences with temporal variations to predict next graph signals. RGN returns a graph state and updates edge attributes related to nodes. Predictions are made through a decoder feeding the graph signal. Training objective involves minimizing a specific objective for multistep predictions. The Spatial Difference Layer (SDL) is trained to optimize parameters in SDL and RGN simultaneously for multistep predictions. It is used to approximate directional derivatives and predict synthetic graph signals. The proposed SDL aims to improve accuracy compared to standard difference forms on a graph. Training involves defining a synthetic function, sampling points, and evaluating the applicability of SDL for directional derivatives on a graph. The study defines a synthetic function and its gradients on 2D space, samples 200 points, and constructs a graph using the k-NN algorithm. Directional derivatives are computed by projecting gradients to connected edges. A comparison is made against four baselines: finite gradient, Multilayer Perceptron Layer, Graph Networks, and a different form of Eq 1. Each method is evaluated for its ability to approximate directional derivatives on the graph. The study compares different methods for approximating directional derivatives on a graph. The proposed spatial difference layer outperforms other methods, showing the effectiveness of utilizing learnable parameters. The study also highlights the importance of explicitly defining differences in order to accurately approximate directional derivatives. The study evaluates the effectiveness of the proposed spatial difference layer (SDL) compared to other methods for approximating directional derivatives on a graph. It is found that the SDL, with its adjustable parameters, outperforms other methods such as One-w and FinGrad due to its ability to learn whole linear combinations of functions. The study uses synthetic data to assess the predictive capabilities of the PA-DGN model on discrete nodes. In the study, an equation similar to Long et al. (2018) is used to predict signal values of points in the future. A graph is constructed using the k-NN algorithm with k=4. The proposed SDL is combined with a linear regression model for prediction, outperforming other methods like VAR and MLP. The prediction model with the proposed spatial differential layer (SDL) outperforms other baselines, showing that introducing spatial differences information helps prediction. However, spatial differential operators approximated with fixed rules can be inaccurate in cases where points with observable signal are sparse in space. Our proposed spatial differential layer improves prediction performance by bridging the gap between approximated difference operators and accurate ones using learnable coefficients. The model is evaluated on predicting climate observations (Temperature) from land-based weather stations in the United States, specifically focusing on stations in the Western and Southeastern states. The k-Nearest Neighbor algorithm is used to generate graph structures for the evaluation. The study uses k-Nearest Neighbor algorithm to create graph structures for predicting graph signals. The adjacency matrix is made symmetric for prediction tasks using PA-DGN and other baselines like VAR, MLP, and GRU. Recurrent Graph Neural Networks are also employed to assess the benefits of graph structure in prediction. The study evaluates the effectiveness of graph structures in prediction tasks using RGN and PA-DGN. Results show that PA-DGN outperforms RGN due to the beneficial spatial derivatives fed into the model. This suggests that the graph structure provides useful inductive bias for the task, as meteorological observations from closer stations are strongly related to each other. The study explores the importance of relative differences among local observations in understanding physics-related dynamics, such as the Diffusion equation. It investigates the effectiveness of modulated spatial derivatives compared to those defined in Riemannian manifolds in prediction tasks on graph signals. The model without spatial derivatives is first assessed, followed by the addition of StandardOP for prediction tasks on Western and Southeastern states graph signals. The study incorporates spatial differences and derivatives into the prediction model for graph signals. PA-DGN shows significant improvements in prediction accuracy compared to RGN without derivatives. However, RGN with StandardOP or MeshOP does not consistently outperform RGN. The findings support the effectiveness of modulated spatial derivatives in improving prediction accuracy. The study introduces a novel architecture (PA-DGN) that approximates spatial derivatives to represent PDEs for physics-aware modeling. It effectively learns modulated derivatives for predictions and reveals hidden physics interactions. Simulated data is used to discretize a linear variable-coefficient convection-diffusion equation on graphs. The study introduces a novel architecture (PA-DGN) that approximates spatial derivatives to represent PDEs for physics-aware modeling. It effectively learns modulated derivatives for predictions and reveals hidden physics interactions. Simulated data is used to discretize a linear variable-coefficient convection-diffusion equation on graphs. The model is trained on 700 sessions, validated on 150 sessions, and tested on 150 sessions using a dataset of 250 points in a 2D space with time series of u. The models used in this work include VAR, MLP, GRU, and RGN with specific architecture and hyper-parameter settings. All models have a hidden dimension of size 64 and utilize concatenated features from previous 2 frames. RGN, a recurrent graph neural network model, has 2 GN blocks with edge and node update blocks using a 2-layer GRU cell. Its hidden dimension is set to 73 to match the number of learnable parameters in the proposed model. Our proposed model PA-DGN utilizes a message passing neural network with 2 GN blocks and a recurrent graph neural network with 2 recurrent GN blocks. The hidden dimension is set to 73 to match the number of learnable parameters in the models. Evaluation runs were performed 3 times for each experiment to report mean and standard deviations. For experiments, models predict frames based on input frames. Training uses Adam optimizer with specific parameters and early stopping. Experiments are implemented in Python3.6 and PyTorch 1.1.0 on NVIDIA GTX 1080 Ti GPUs. Two graph structures, k-NN and TriMesh, are evaluated. The effect of different graph structures on prediction performance varies for different models. While k-NN graph benefits RGN and PA-DGN, it harms RGN(MeshOP) due to the reliance on neighboring information. PA-DGN under k-NN graphs consistently outperforms other baselines. Figure 7 shows the MAE distribution across nodes of PA-DGN in the west coast region for graph signal prediction. Nodes with high prediction errors for short-term prediction are concentrated in sparse observable areas, while for long-term prediction, nodes with limited observable points no longer have the largest MAE. This suggests that PA-DGN efficiently utilizes neighboring information even with sparse observations."
}