{
    "title": "ryx35Ehi84",
    "content": "Analysis methods using Representational Similarity Analysis (RSA) and Tree Kernels (TK) help understand neural models of language in NLP. Validated on a synthetic language, these methods correlate neural representations of English sentences with their syntax. Analysis methods are increasingly needed to understand neural models of language in NLP. Diagnostic models predict information from activation patterns, but are limited to simple target information. Decoding complex structures like constituency parse trees requires more advanced techniques. Our approach introduces a method based on correlating neural representations of sentences with structured symbolic representations commonly used in linguistics. This correlation is in similarity space, allowing for more flexible representation types. It is an extension of the Representational Similarity Analysis (RSA) method, applied to neural representations of language strings and structured symbolic representations. The text discusses using a tree kernel to compare neural and symbolic-linguistic representations, introducing RSA REGRESS for similarity analysis. Validation is done on neural models processing a synthetic language and English text models like Infersent BID9 and BERT BID13, showing the encoding of syntactic information. The dominance of syntax is noted in the intermediate layers of BERT. The dominance of deep learning models in NLP has led to increased interest in analyzing how they encode linguistic information. Diagnostic models, using internal activations of neural networks for predictive tasks, help understand complex models like BERT. The text discusses the limitations of deep learning models in NLP and introduces a method called edge probing to analyze complex structures. It also mentions the non-random activation patterns in neural networks and alternative approaches for analysis. In the context of diagnostic models in NLP, alternative approaches have been proposed to analyze neural models of language. Some modify the neural architecture to make it more interpretable, such as adapting layerwise relevance propagation to Kernel-based Deep Architectures to trace network decisions back to influential landmarks. The proposed general analytical framework, RSA, is a variant of pattern-information analysis used to understand neural activation patterns in human brains. It correlates representations of stimuli in neuroimaging, behavioral experiments, and computational modeling spaces via pairwise (dis)similarities. Kernels, like convolutional kernels for syntactic parse trees, are used to measure (dis)similarity in structured representation spaces. The text discusses the use of syntactic parse trees as a metric for measuring similarity between trees and introduces a polynomial time algorithm for computing these kernels. It also mentions the use of synthetic data from artificial languages in analyzing neural network models of language. The tradition of using synthetic language data dates back to the first generation of connectionist models of language. Recent studies have used context-free grammars to generate data and train RNN-based models to identify matching numbers of opening and closing brackets in Dyck languages. The text discusses the use of synthetic data from artificial languages to train neural network models. BID16 trained RNNs on nested arithmetic languages, showing good performance and generalization. RSA analyzes similarities between pairs of stimuli in different representation spaces. The text discusses applying RSA to neural representations of strings from a language and structured symbolic representations using tree kernels to measure similarity. The algorithm introduced by BID8 efficiently computes the number of shared tree fragments between two tree structures. The tree kernel calculation between trees T1 and T2 involves sets of tree fragments and a scaling parameter \u03bb to adjust the importance of fragment size. Lower values of \u03bb discount larger tree fragments in the kernel computation. The kernels are normalized using a function K that counts common tree fragments between the trees. The normalized tree kernel, computed using function K, measures the raw count of shared tree fragments between syntax trees T1 and T2. Dynamic programming formula for convolution kernel computation is shown in Figure 2 after BID8. RSA REGRESS compares global similarities in different representations, while diagnostic models focus on extracting specific information from a representation. For example, neural encoding may predict sentence length accurately. RSA REGRESS is a method that combines classic RSA and diagnostic model approaches to compare similarities in different representations. It uses similarity functions specific to two representations and embeds objects in a representation space using a representational similarity function. The RSA REGRESS method combines classic RSA and diagnostic model approaches to compare similarities in different representations by fitting a multivariate linear regression model between two views of objects. The success of the model is evaluated using cross-validated Pearson's correlation for an L2-penalized model. Evaluation of analysis methods for neural network models remains an open problem, often relying on qualitative assessment. In the context of evaluating analysis methods for neural network models, pre-existing intuitions may not be reliable. To simplify the process, a simple language of arithmetic expressions is used as a case study. The language consists of expressions encoding addition and subtraction modulo 10, with rules for evaluation provided. The language used for evaluating analysis methods for neural network models is a simple arithmetic expression language with rules for evaluation. The context-free grammar for this language is provided, along with rules for semantic evaluation. The language lacks ambiguity, has a small vocabulary, and simple semantics, requiring hierarchical structure processing for expression evaluation. The recursive function GENERATE is used to generate expressions in the language, with input parameters for branching probability and decay factor. Larger decay factor values result in smaller expressions being generated. The grammar is more complex than necessary for expression generation. The language used for evaluating analysis methods for neural network models is a simple arithmetic expression language with rules for evaluation. The context-free grammar for this language is provided, along with rules for semantic evaluation. The language lacks ambiguity, has a small vocabulary, and simple semantics, requiring hierarchical structure processing for expression evaluation. The recursive function GENERATE is used to generate expressions in the language, with input parameters for branching probability and decay factor. Larger decay factor values result in smaller expressions being generated. The grammar is more complex than strictly needed to facilitate the computation of the Tree Kernel. Three recurrent models process the arithmetic expressions from language L, each trained to predict different targets related to syntax or semantics. These models share the same recurrent encoder architecture based on LSTM BID15. The LSTM hidden layer at the last step represents the input expression. A model with an encoder passes the input representation to a multi-layer perceptron to predict the expression value. Another model predicts the syntax tree depth instead of the value, focusing on hierarchical information rather than semantics. The model uses an encoder to convert input expressions into prefix form. The decoder is an LSTM trained as a conditional language model. RSA is used to correlate neural encoders with syntactic and semantic information. The model utilizes cosine distance for neural representations. Dissimilarity metrics include semantic value, tree depth, and tree kernel. Dissimilarity measures are based on absolute differences or subtracting from 1 for normalized tree kernel metric. The text discusses the implementation of neural models in PyTorch 1.0.0 for structured RSA evaluation of a synthetic language. The model architecture includes encoder and decoder layers with specific sizes, and training is done in batches with varying decay values. Training follows a curriculum approach to optimize the model. The study implemented neural models in PyTorch 1.0.0 for structured RSA evaluation of a synthetic language. Training was done in batches with varying decay values, followed by optimization with Adam using a learning rate of 0.001. Results were reported on expressions sampled with d = 1.5, and the effects of learning on activation patterns were quantified. The trained model was selected based on the smallest loss on held-out validation expressions. Results were reported on separate test data, and TAB2 shows the experimental results for different encoder types and target tasks. The study compared diagnostic models and RSA techniques in evaluating encoder performance. Both approaches showed high scores when the objective function matched the reference representations. However, RSA focuses on overall similarity structure, while diagnostic models and RSA REGRESS focus on encoding target information. This difference explains discrepancies in scores between RSA and the other approaches. The study compared diagnostic models and RSA techniques in evaluating encoder performance. RSA and RSA REGRESS were used to explore if hidden activations encode structural syntax representation, with TK reference representations showing highest scores for INFIX-TO-PREFIX encodings. Unexpectedly, random encoder also showed non-random activation patterns, with high scores for diagnostic tasks. The random encoder has high scores for diagnostic regression on tree depth and substantial negative RSA score for the Tree Kernel. The scatter-plot in FIG3 shows the correlation pattern between the random encoder and Tree Kernel representations, with expression pairs showing opposite dissimilarities. The trained INFIX-TO-PREFIX encoder's dissimilarities are positively correlated with TK dissimilarities. The raw correlation value for the trained encoder is a biased estimate of the effect of learning, as learning has to overcome the initially substantial negative correlation. The difference between scores for the learned and random model provides a better estimate. The same approach would be less informative for the diagnostic model approach or for RSA REGRESS. For a regression model, the correlation scores will be positive, and when taking the difference between learned and random scores, they may cancel out. This is what we see for the RSA REGRESS scores for random vs. INFIX-TO-PREFIX, where the scores partially cancel out. Subtracting them is misleading. Overall, the results suggest that using RSA REGRESS in conjunction with RSA correlation scores provides a comprehensive understanding of how learning impacts neural representations. The RSA REGRESS method can address similar questions as the diagnostic model, with the added benefit of being applicable to structured symbolic representations. Applying RSA and RSA REGRESS to natural language sentences reveals insights into comparing tree-structure representations with neural representations captured by sentence embeddings. NLP systems are trained on unlabeled text using language modeling objectives. Different types of encoders are used, including bag of words, Infersent, and BERT models. Bag of words represents sentences with word counts, Infersent is a supervised model based on bidirectional LSTM, and BERT is an unsupervised model based on the Transformer architecture. The unsupervised model BID31, based on the Transformer architecture BERT, is trained on a cloze-task and next-sentence prediction. Data from the English Web Treebank (EWT) BID4 is used, with delexicalized constituency trees for computing Tree Kernel scores. The study compares different sentence encoders using RSA and RSA REGRESS with Tree Kernel reference. Results show high RSA correlation with parse trees, especially for trained Infersent and BERT models. Trained Infersent and BERT models show high correlation with syntactic trees in RSA and RSA REGRESS. Even untrained versions and Bag of Words exhibit moderate correlation. RSA REGRESS is less sensitive to \u03bb values compared to RSA. Scores peak between layers 15-22 in BERT for trained models, indicating later layers focus on aspects other than syntax. The study presents two RSA-based methods for correlating neural and syntactic representations of language using tree kernels. Results on arithmetic expressions confirm that structured RSA captures correlations between different representation spaces. The methods are applicable to various symbolic representations of linguistic structures. A toolkit with the implementation of these methods is available at https://github.com/gchrupala/ursa."
}