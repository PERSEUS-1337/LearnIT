{
    "title": "Byxpfh0cFm",
    "content": "Data augmentation is commonly used to encode invariances in learning methods. In this work, a novel set of subsampling policies based on model influence and loss is proposed to reduce the number of data points included in data augmentation by 90% while maintaining accuracy gains. Data augmentation is a crucial tool in modern machine learning pipelines, especially for image recognition tasks. It involves applying class-preserving transformations to the training set, such as rotations or crops, to improve accuracy. While data augmentation is popular due to its simplicity, it can be costly as it significantly increases the dataset size by applying multiple transformations. Data augmentation is essential for image recognition tasks, involving applying transformations to the training set to improve accuracy. However, augmenting the entire dataset can lead to increased data storage costs and training time, which scale with the dataset size. Selecting the optimal transformations for each data point is challenging and often requires domain expertise. Complex augmentations may need to be verified on a per-sample basis. In this work, the aim is to make data augmentation more efficient by identifying subsamples of the dataset that are good candidates for augmentation. Drawing inspiration from the virtual support vector (VSV) method used in SVMs, the approach focuses on augmenting samples close to the margin to create a more robust decision surface. Class-preserving data augmentations are applied to support vectors in the training set, and the SVM is retrained on the augmented support vector dataset. In this work, the aim is to develop policies for efficient data augmentation by reducing the augmentation set size while applying to a broader class of models. Two key metrics are used: measuring the loss induced by a training point and exploring the influence of a point for augmentation potential. Influence functions are utilized for this purpose. The paper demonstrates that high accuracy can be achieved by augmenting only a small subset of the dataset, using policies based on training loss or model influence. Various modifications like sample reweighting and online learning can further enhance performance. Our proposed policies for improving performance are simple to implement with just a few lines of code. Experiments are conducted on common benchmark datasets like MNIST, CIFAR10, and NORB. Data augmentation techniques, such as applying crops, flips, or affine transformations, are widely used in image classification pipelines. These strategies can significantly impact performance and require careful selection and tuning. The text discusses the challenges of selecting and tuning data augmentation techniques for improving performance in image classification pipelines. Various approaches have been proposed, such as adaptive data augmentation, learning composed transformations, and reinforcement learning. The focus of the current work is on selecting which data points to augment while keeping transformations fixed, offering complementary solutions to existing methods. Several recent works have proposed augmentation strategies based on adversarial training approaches, such as robust optimization frameworks or generative adversarial networks (GANs). These approaches generate artificial points from a target distribution, rather than directly transforming the original training points. The Virtual Support Vector (VSV) method is closely related, limiting transformations to support vectors to reduce the set of points for augmentation. The motivation behind downsampling candidate points for augmentation is to focus on points that have a significant impact on the model. This approach extends beyond SVMs by generalizing the concept of support vectors to measure loss at each training point and explore model influence. Model influence is crucial in determining the most impactful data points on the model, especially in non-differentiable and non-convex settings like deep networks. The work discusses metrics in non-differentiable and non-convex settings, related to subsampling for dataset reduction. Different methods like gradients and influence functions have shown better results than uniform sampling. The goal is to increase the dataset size through augmentation, unlike subsampling methods motivated by large dataset sizes. The work focuses on making data augmentation more efficient by providing effective policies for subsampling the original training dataset. It aims to retain the accuracy of a fully augmented dataset by performing translation augmentations. The study reports the impact of these augmentations on final test accuracy for datasets like MNIST, CIFAR10, and NORB. The study focuses on efficient data augmentation by proposing policies for subsampling the training dataset. It shows that augmenting just 25% of the data selected at random can yield significant accuracy gains, highlighting the effectiveness of partial augmentation. The study demonstrates that subsampling with optimal policies can achieve similar results to full augmentation when augmenting only 10% of the data. The proposed policies are detailed in Section 4, with full experiments and experimental details in Section 5. The study aims to find a subset S from the training set D that, when augmented, performs similarly to augmenting the entire dataset. The goal is to minimize the size of S while maintaining performance. Proposed policies include an augmentation score for each training point and a sampling policy based on these scores. Two metrics, loss, and model influence, are used to generate augmentation scores. In Section 4.1, two metrics, loss and model influence, are used to generate augmentation scores. Different policies for subset selection based on these scores are explored, including deterministic and random selection methods. The augmentation scores and policies may be adjusted iteratively to account for model updates. An overview of various augmentation policies is provided in TAB1. The augmentation policies in TAB1 include Policy Type Selection Function Update Scores Downweight Points DISPLAYFORM0 DISPLAYFORM1 S function corresponds to the inverse of an order statistic function. Two metrics, training loss and model influence, are proposed to determine augmentation scores. Training loss is obtained from the loss at a point in the training set, while model influence is also explored. The text discusses augmentation policies based on model influence, specifically focusing on Leave-One-Out (LOO) influence to measure the impact of training data points on the loss when removed. The influence of upweighting a point on the loss at a test point is calculated, and the magnitude of LOO influence is considered for augmentation scores. The potential of using training loss and model influence for scoring is explored through a histogram of model influence across CIFAR10 and NORB datasets. The text discusses model influence for scoring through a histogram across CIFAR10 and NORB datasets. The values show variability for ranking points by preference, with correlations before and after augmentation indicating reliability. Spearman's rank correlations range from 0.5 to 0.97, supporting the impact of data points after augmentation. Reweighting individual samples is motivated by considering an augmentation as the identity map, leading to duplicating selected samples in the training set. The augmentation policy duplicates selected samples in the training set, resulting in reweighted samples with twice the original weight. To normalize for this reweighting effect, weights of original and augmented samples are divided by the size of the set. More advanced policies, like reweighting based on sample trustworthiness, are suggested for future investigation. In some cases, reweighting can negatively impact performance, especially in class imbalance scenarios where duplicating minority class samples can alter class distribution. Updating scores for augmented data points can account for model behavior changes, but using a single influence estimate throughout the model's lifetime can reduce computation and enable parallelism. The modification of updating scores for augmented data points can reduce computation and enable parallelism, similar to reweightings. However, the extra cost required may not justify the technique's significant effect. The benefit is that many applications may only need to compute selection metadata once during the augmentation process. Detailed results on the performance of proposed policies for data subsampling using a Convolutional Neural Network (CNN) are provided. The study explores the impact of augmentation policies on three datasets using different architectures. Augmentations include translation, rotation, and crop, applied exhaustively in a deterministic manner. The LeNet architecture is used for MNIST, while ResNet50v2 is used for CIFAR10 and NORB. The effect of re-generating features for MNIST is also examined. The study examines the impact of augmentation policies on three datasets using various architectures. Augmentations like translation, rotation, and crop are applied exhaustively in a deterministic manner. Regularization is controlled through cross-validation, with negligible impact on observed trends. Augmented test points are added to the test set to make the effects of augmentation more apparent. In the study, augmentation policies are explored using Imgaug for various datasets. Different strategies for sampling points for augmentation are compared, including random, loss-based, and influence-based methods. The impact on test accuracy is analyzed, with all policies showing the same accuracy when no points are augmented. The code implementation details are provided in the appendix, and the code is publicly available online. The study explores augmentation policies using Imgaug for different datasets, comparing random, loss-based, and influence-based methods for sampling points. Policies based on loss and influence consistently outperform the random baseline, especially with rotation augmentation. Influence slightly outperforms loss, but both show similar behavior across datasets. The study compares augmentation policies using Imgaug for different datasets, showing that policies based on loss and influence outperform random sampling. Influence slightly outperforms loss, with better performance on the NORB dataset. Using a reduced set of points for augmentation can achieve higher accuracy, possibly due to a bias towards harder examples. Support vectors for augmentation are explored, finding VSV points through tuning a linear SVM on bottleneck features. The study explores augmentation policies using Imgaug, finding that policies based on loss and influence outperform random sampling. Support vectors for augmentation are obtained by tuning a linear SVM on bottleneck features, resulting in strong performance on some tests but not as reliable for finding optimal subsets of points. The augmentation set size is fixed to the number of support vectors, limiting flexibility based on data budget. The study examines augmentation policies using Imgaug, showing that policies based on loss and influence are more effective than random sampling. Reweighting samples and updating scores during augmentation can impact performance differently across datasets, with reweighting potentially hindering performance in CIFAR10 and NORB. In terms of score updating, slight positive impact seen for NORB-rotate, but performance similar to original policy. Simpler policies preferred due to extra expense in model updating. Examining 10 points with highest vs. least influence/loss for MNIST shows benefits of downsampling in promoting diversity. Additional results for CIFAR10 and NORB provided in Appendix E. In this paper, the importance of promoting diversity and removing redundancy in data augmentation for efficient learning of invariances is highlighted. Simple policies based on training loss and model influence are proposed to select the most useful subset of training points. Access to an augmentation score vector can be obtained in one training cycle, leading to potential superlinear improvements in augmented training. This approach allows for more efficient application of augmentations with fewer data points, potentially on a per-example basis. Future work could explore this further. Future work could explore subset selection policies that consider the entire subset rather than greedy policies. Including second-order information or encouraging subset diversity may improve performance. Histogram plots show that most points have low loss and influence, suggesting they can be augmented with low probability. Full implementation details are provided in the paper. The experiments in the paper focus on key architectural ideas such as data, augmentations, selection policy, featurization preprocessing, and logistic regression model in Python. The dataset is loaded into a NumPy array, processed through a CNN model like LeNet or ResNet50v2 to obtain a feature vector. Logistic regression is trained on this featurized dataset and tested on a \"poisoned\" test set, showing a performance gap due to distribution changes. Loss and influence are measured for each training point to use as scores. Augmentations are then exhaustively applied to the test set. The study focuses on closing the performance gap between original and \"poisoned\" test sets by augmenting the training data. Augmentations are applied in rounds, with a policy selecting points to augment based on scores. The CNN may be retrained, while the logistic regression model must be retrained for current test accuracy. Experiments are conducted in Python using Keras, Tensorflow, and Scikit-Learn, with logistic regression hyperparameter C fixed at 10. The study utilizes Python libraries such as Keras, Tensorflow, Scikit-Learn, AutoGrad, and Imgaug for implementing CNNs and classifiers. Augmentations are performed using Imgaug. The code is publicly available online. Bottleneck features from CNNs are used as inputs for logistic regression models. The study used a LeNet architecture for MNIST but faced issues with CIFAR10 and NORB. A ResNet50v2 model was used for CIFAR10 with good performance. Pretrained ImageNet ResNet50 model resulted in poor performance. Good performance was achieved on NORB without data augmentations. The ResNet model was retrained with random rotations, shifts, and flips applied to images after facing high prediction degradation with other augmentations. Datasets were converted into binary classification tasks with specific class splits for MNIST, CIFAR10, and NORB. Augmentations such as translate, rotate, and crop were applied over a range of parameters to generate multiple augmented images. Augmentations like translate, rotate, and crop are applied to generate multiple augmented images with specific parameters. For example, translate is applied for 2 pixels in all cardinal directions on MNIST, 3 pixels for CIFAR10, and 6 pixels for NORB. Rotate is applied for 15 rotations evenly spaced between \u00b130\u00b0 for MNIST, and \u00b15\u00b0, \u00b12.5\u00b0 for CIFAR10 and NORB. Crop excludes outer pixels on all sides for MNIST, while CIFAR10 and NORB exclude the outer 2 pixels. Zoom is applied to rescale resulting images back to their original dimensions. The augmentations discussed involve utilizing label information to induce changes in labels, which can be costly and challenging to validate. Full experiments are provided for randomized and deterministic policies, with Area Under the Curve (AUC) results presented. Tables separate augmentations and score functions, providing a single metric for policy performance evaluation. The VSV method uses SVM margin to score points, considering points farther from the margin less important to logistic regression. Results show that using margin or loss/influence scores from SVM and logistic regression leads to worse performance compared to an influence-based approach. Transfer between models poses interesting questions for future research. The curr_chunk discusses augmentation with cluster-based stratified sampling to explore diversity in the augmentation process. Two policies are compared: Baseline Clustered and Random Proportional Influence Clustered. The performance of CIFAR10 k-DPP policies using bottleneck features is evaluated. Only 250 augmented points were used due to computational constraints. The curr_chunk discusses using a k-DPP with influence and bottleneck features for augmentation. DPP results are shown in Figure 16, with different policies compared. The experiments ran up to 250 points augmentations and were repeated 5 times. The influence weighted DPP performance is competitive with the influence driven approach in augmentation experiments. Using bottleneck features alone for L resulted in poor performance. Computational limitations were encountered when sampling a DPP with a large dataset size, suggesting alternative approaches may be needed for larger subsets. Performance in deep learning is expected to scale linearly with increased dataset size. In experiments using ResNet50v2 on CIFAR10 data, linear scaling performance was observed. Subsampling resulted in a linear decrease in training time, as seen in MNIST experiments. The benefits of this improvement are expected to be greater in distributed training scenarios. Pretrained models can be used to bootstrap different models, reducing initial training costs. For the CIFAR10 translate task, SVM support vectors were used to augment training data for a ResNetv2 model, achieving close to baseline accuracy with a fraction of the data. Augmenting a subset of the training set has the potential to decrease training time without compromising performance."
}