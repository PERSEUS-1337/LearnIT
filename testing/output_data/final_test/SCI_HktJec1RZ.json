{
    "title": "HktJec1RZ",
    "content": "Neural Phrase-based Machine Translation (NPMT) utilizes Sleep-WAke Networks (SWAN) to model phrase structures in output sequences. It introduces a layer for local reordering of input sequences and does not rely on attention-based decoding mechanisms. NPMT outperforms strong NMT baselines on translation tasks and produces meaningful phrases in output languages. The goal of the paper is to explore the use of phrase structures for neural network-based machine translation systems. Traditional phrase-based statistical machine translation approaches have been shown to outperform word-based ones. The paper introduces a neural machine translation method that explicitly models phrases in target language sequences. The proposed Neural Phrase-based Machine Translation (NPMT) method builds upon Sleep-WAke Networks (SWAN) to explore advantages from both traditional and modern neural machine translation methods. NPMT introduces a new layer for local reordering to address the issue of monotonic alignments. Experimental results show that NPMT outperforms attention-based NMT baselines in terms of BLEU score on translation tasks. The paper introduces the Neural Phrase-based Machine Translation (NPMT) model, which combines neural machine translation and phrase-based SMT. It discusses the NPMT architecture, including SWAN and a soft reordering layer, and demonstrates its effectiveness on various language pairs. The overall architecture of NPMT is illustrated in Figure 1(a), showing the input sequence transformed into embedding representations before passing through a reordering layer. The NPMT model combines neural machine translation with phrase-based SMT. It utilizes a soft reordering layer and bi-directional RNN layers to output target language segments. Unlike other approaches, NPMT does not use attention-based decoding mechanisms. The SWAN model, proposed in BID25, utilizes segment-to-segment neural transduction for Chinese-to-English translation. It employs latent variables for monotonic alignments between inputs and outputs, with dynamic programming used to marginalize the variables during training. Symbol $ marks the end of a segment, and SWAN emits specific segmentations based on input sequences from a bi-directional RNN. The SWAN model, introduced in BID25, defines a probability distribution for output sequences based on input sequences. It considers all valid output segmentations and alignments with input segments, allowing for empty segments. No assumptions are made on sequence lengths. The input sequence x 1:T is from a bi-directional RNN, while the output sequence is y 1:T. Valid segmentations of y 1:T are denoted by S y, with the same number of segments as the input sequence length, T. Empty segments ensure correct alignment between input and output elements. The SWAN model defines a probability distribution for output sequences based on input sequences. It considers valid output segmentations and alignments with input segments, allowing for empty segments. The probability of the sequence y 1:T is defined as the sum of probabilities of all segmentations in S y. The segment probability p(a t |x t) is modeled using a recurrent neural network (RNN) with a softmax layer. SWAN can be understood via a generative model where words are sampled from RNN until an end of segment symbol is reached. The SWAN model defines a probability distribution for output sequences based on input sequences, considering valid output segmentations and alignments with input segments. The probability of the sequence y 1:T is obtained by summing over all possible ways, with an exact dynamic programming algorithm developed to tackle computation challenges. The feasibility of using dynamic programming is due to a property of segmentations where the number of possible segments is polynomial-O(T 2). The SWAN model defines a conditional probability for output sequences given input sequences, using a sequence encoder like a bi-directional RNN. BID25 discussed enforcing a maximum length for segments to reduce computational complexity. Greedy decoding for SWAN involves modeling p(a t |x t). Greedy decoding for SWAN involves running an RNN in parallel to produce output segments for each p(a t |x t), which are then concatenated to form the entire output sequence. The decoding complexity is O(T L) and satisfies the non-autoregressive property BID9. SWAN assumes a monotonic alignment between output segments, with only output segments being explicit while input segments are implicitly modeled. In SWAN, a monotonic alignment is assumed between output segments and input elements. While this works well for speech recognition, it is too restrictive for machine translation. Attention mechanisms have been proposed in neural machine translation literature to address alignment issues, but it's unclear how to apply them to SWAN due to its use of segmentations for output sequences. Empirical studies show that simply using a bi-directional RNN encoder is not enough to achieve optimal performance. To improve results, SWAN is augmented with a reordering layer for (soft) local reordering of input sequences, leading to promising results in translation tasks. SWAN, a model for machine translation, achieved promising results on translation tasks without using attention mechanisms. A local reordering layer was added to improve performance by reordering input sequences. This layer operates in parallel with linear complexity, enhancing translation accuracy. The local reordering layer in SWAN improves translation accuracy by selecting input elements from adjacent windows based on weights, without the need for a query like standard attention mechanisms. This reordering operation is bottom-up and captures relative positions of input elements with nonlinear transformation using the tanh(\u00b7) function. The reordering layer in SWAN improves translation accuracy by selecting input elements from adjacent windows based on weights. It performs similarly to a convolutional layer and encodes positional information with different parameters for each relative position. The weights for input elements are not normalized, allowing for reordering capability. The gate of each position in the window is determined by all input elements within the window. This reordering layer is different from Gated Linear Units (GLU) in controlling information flow in convolutional outputs. The reordering layer in SWAN improves translation accuracy by selecting input elements from adjacent windows based on weights, similar to a convolutional layer. It encodes positional information with different parameters for each relative position. The gate of each position in the window is determined by all input elements within the window. However, GLU lacks a mechanism to choose input elements from the convolutional window. Experimental results show that neither GLU nor traditional convolutional layers helped NPMT. Different window sizes in the reordering layer impact the context of each position. The model is evaluated on various machine translation tasks with small datasets to showcase its utility. The study evaluates a model for German-English machine translation using data from translated TED talks. The model includes one reordering layer, two bi-directional GRU encoder layers, and two unidirectional GRU decoder layers. Dropout is added to the GRU layer, and the maximum segment length is set to 6. Batch size is 32 per GPU, and the Adam algorithm is used. Future work will involve more large-scale experiments to demonstrate the method's usefulness. The model for German-English machine translation includes a reordering layer, bi-directional GRU encoder layers, and unidirectional GRU decoder layers. The maximum segment length is set to 6, batch size is 32 per GPU, and the Adam algorithm is used for optimization with a learning rate of 0.001. Greedy search and beam search with a beam size of 10 are used for decoding. Penalizing short candidate sentences was necessary for better results, and hyperparameters are chosen based on the development set. NPMT takes 2-3 days to converge on a machine with four M40 GPUs. Results show NPMT achieves state-of-the-art performance on the dataset. NPMT achieves state-of-the-art results on the dataset, outperforming supervised sequence-to-sequence models and actor-critic based methods. Adding a reordering layer to the sequence-to-sequence model does not improve performance, and removing the reordering layer from NPMT results in a higher BLEU score. The reordering layer in NPMT is crucial for its effectiveness, as shown by BLEU scores of 27.79 (greedy) and 29.28 (beam search). SWAN and the reordering layer play important roles in NPMT. The process of creating small DNA pieces involves potential errors, with longer pieces leading to more errors. Examples of German-English translation outputs with segmentations are provided, showing the emission of words during decoding. Visualization of the decoding process is also available. In NPMT, the reordering layer is essential for effectiveness, with BLEU scores of 27.79 (greedy) and 29.28 (beam search). The decoding process includes capturing phrase-level translation and knowing when to output phrases. Informative segments and mappings between phrases, words, and phrases are observed in the decoding results. The most frequent phrase mappings are shown in Appendix C. In SWAN, a 4th-order language model is used during beam search to improve segmentation modeling. The beam search score is calculated with \u03bb 1 = 1.2 and \u03bb 2 = 0.2, tuned on the development set. Results with the external language model are denoted by NPMT+LM in Table 1. If no external language models are used, \u03bb 2 = 0. In NPMT, a strong sequence-to-sequence attention model is created for English-German translation. The model outperforms the baseline by 2.46 BLEU in greedy and beam search cases. The training takes 2-3 days to converge on a machine with four M40 GPUs. The model for English-German translation improves performance with a 4th-order language model trained using KenLM for German target training data. Evaluation is done on the IWSLT 2015 English to Vietnamese machine translation task using TED talks data. The model architecture includes one reordering layer, bi-directional LSTM encoder, unidirectional LSTM decoder, and dropout with a rate of 0.4. The model uses LSTM with a dropout rate of 0.4 and Adam optimization. Greedy decoding and beam search with a beam size of 10 are used. Results show improvement over baseline models in BLEU score. A 4th-order language model for Vietnamese data further enhances performance. The model's reordering layer allows for decoding in linear time. The model uses LSTM with a dropout rate of 0.4 and Adam optimization, outperforming models with monotonic attention. Table 6 shows examples of translation. The model's reordering layer allows for decoding in linear time, with gates focusing on central words in the sentence. The model's reordering layer uses gating mechanism to achieve a reordering effect, with gates focusing on central words in the sentence. Evaluating different window sizes, the performance peaks with a window size of 7 on the IWSLT 2014 English-German translation task. The performance peaks with a window size of 7 in the reordering layer using a gating mechanism for the English-German translation task. Window sizes of 5 and 9-11 show a drop in performance, indicating the need for proper reordering. Input segments are analyzed without explicit input in NPMT, sorting based on frequency. Tables show phrases and translations. In the German-English translation task, the performance peaks with a window size of 7 in the reordering layer. Input segments are analyzed based on frequency without explicit input in NPMT. Tables display phrases and translations, with top 10 mappings shown in different categories. Phrases with 3 or 4 words are considered, and phrases with the word \"UNK\" are removed. The performance peaks in German-English translation with a window size of 7 in the reordering layer. Phrases with 3 or 4 words are analyzed based on frequency, and mappings are shown in different categories. The word \"UNK\" is removed from phrases. Top 5 input-output phrase mappings are displayed for both 3-word and 4-word phrases."
}