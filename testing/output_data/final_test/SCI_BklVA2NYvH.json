{
    "title": "BklVA2NYvH",
    "content": "Deep neural networks are vulnerable to adversarial perturbations. This paper connects adversarial robustness with Lyapunov stability of dynamical systems. Training neural nets is like optimizing a discrete dynamical system, using an optimal control algorithm to improve robustness. Constraints can be added to the optimization, enhancing the deep model's robustness. Experiments show this method effectively improves adversarial robustness. In this paper, a dynamical system view is proposed to address the vulnerability of neural networks to adversarial examples in image classification. Various methods have been developed to attack deep models, leading to low accuracy, despite the existence of defenses. The study of adversarial attacks is crucial as they pose a threat to real-life machine learning systems. The proposed approach aims to enhance the adversarial robustness of models. In this paper, a dynamical system view is proposed to enhance the adversarial robustness of models by treating neural networks as a discretization of an ordinary differential equation (ODE). Training neural nets involves finding an optimal control of the corresponding discrete dynamical system. The traditional approach treats training neural networks as an unconstrained non-convex optimization problem solved with gradient-descent based methods. The training process involves feeding the network with training data and computing gradients through forward and backward propagation. The propagation process involves computing gradients with forward and backward propagation. Adversarial defense methods include adversarial training, modifying networks, and adding external models. Various approaches have been proposed to improve models' adversarial robustness. Recent works have connected deep neural networks with ODEs and dynamical systems, aiming to make deep learning models robust against real-life threats. Deep residual networks can be seen as approximating an ODE, inspiring the design of effective network structures. Viewing the network as a dynamical system allows for an optimal control perspective, with Pontryagin's Maximum Principle being applied to train neural nets. Training neural nets involves controlling parameters to fit the training data, viewed as an optimal control problem. Deep residual networks can be seen as approximating an ODE, with the network represented as a dynamical system. The training optimization problem includes loss function, regularizer, and batch size considerations. The problem of adversarial examples in neural networks can be viewed as the forward Euler discretization of a continuous optimal control problem. Adversarial examples are clean images with small perturbations that lead to different outputs. Lyapunov stability is used to characterize the sensitivity of neural networks to these perturbations. Lyapunov stability ensures that a neural network is robust to perturbations, making it resistant to adversarial attacks. The stability of continuous ODEs is crucial for deep residual networks, with specific conditions outlined in Theorem 1. The stability conditions for stable ODEs and deep residual networks are outlined, emphasizing the importance of numerical stability. Theorem 2 discusses stable discrete networks and the conditions for stability in neural networks. The use of Pontryagin's Maximum Principle (PMP) and Minimum State Action (MSA) for deterministic systems is also highlighted for improving adversarial robustness in neural nets. The Pontryagin's Maximum Principle (PMP) and Method of Successive Approximations (MSA) are utilized for optimal control in deterministic systems. MSA has been used in deep learning to train neural networks. The necessary conditions for optimal control in training neural nets are discussed, along with the Hamiltonian function and Theorem 3 on Pontryagin's Maximum Principle for Discrete Systems. The Maximum Principle for Discrete Systems states that there exist co-states that satisfy certain conditions. These co-states can be viewed as Lagrangian dual variables and are crucial for optimizing the Hamiltonian function. By iteratively computing forward and backward propagation, the optimal control can be found using the Method of Successive Approximations. The Method of Successive Approximations (MSA) involves adding regularizer terms to prevent drastic steps during maximization. Training with MSA allows for decoupled optimization steps on different layers, leading to faster parallelization and reduced parameter space. This makes optimization easier compared to traditional gradient descent algorithms. Li & Hao (2018) discuss the connection between MSA and back-propagation-based gradient descent algorithms in their appendix. The Method of Successive Approximations (MSA) involves adding constraints for robustness in optimization. Decoupled optimization in MSA allows for easier handling of constraints, such as the spectral radius of parameters. This approach prevents divergence during training and is more efficient than traditional gradient descent algorithms. The spectral radius of parameters can be controlled by using special forms of matrices, like anti-symmetric matrices. This ensures stability during training by satisfying the constraint Re(\u03bb i (\u03b8 t )) \u2264 0. By replacing \u03b8 t with \u03b8 t \u2212 \u03b8, the optimization problem can be transformed into a semi-definite programming (SDP) problem, which can be efficiently solved using interior point methods. Our method utilizes MSA to train neural networks by iteratively computing states and co-states for each layer, adding a positive semi-definite constraint for stable dynamics control. Experimental evaluations on CIFAR10 show the effectiveness of our approach compared to adversarial training methods. The network model used was an 18-layer residual network. The network model used was an 18-layer residual network with 8 residual blocks. Perturbation size was set to 0.1 for FGSM and PGD attacks, and L0 metric was used for C&W. The model was trained for 150 epochs with a batch size of 200, and the learning rate was adjusted at specific epochs. Results are shown in Table 1, indicating improved adversarial robustness compared to the vanilla model. Figure 1 displays eigenvalues of the last fully-connected layer, showing effective bounding of spectral radius below 1. Our method offers advantages such as reduced gradient propagation compared to traditional adversarial training, allowing for different hyperparameters and training methods for each layer. Lyapunov stability provides a framework for analyzing adversarial robustness in deep models, potentially leading to theoretical advancements in understanding adversarial samples. This work connects the robustness of deep neural models with Lyapunov stability of dynamical systems, proposing a method using stable optimal control to enhance adversarial robustness. While not surpassing state-of-the-art defense methods, the approach suggests a new direction for training neural networks. Future work includes mathematical analysis on Lyapunov stability and the application of specific algorithms for optimization. Popular deep learning platforms like TensorFlow and PyTorch do not currently support optimal control frameworks."
}