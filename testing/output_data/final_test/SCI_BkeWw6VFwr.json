{
    "title": "BkeWw6VFwr",
    "content": "In this work, the authors aim to derive certified robustness for top-$k$ predictions using randomized smoothing with Gaussian noise. They find that generalizing the certified robustness from top-1 to top-$k$ predictions presents technical challenges and evaluate their method on CIFAR10 and ImageNet datasets. The authors address technical challenges in deriving certified robustness for top-k predictions using randomized smoothing with Gaussian noise. Their method achieves a certified top-5 accuracy of 62.8% on ImageNet with adversarial perturbations limited to an $\\ell_2$-norm of 0.5. Various empirical defenses have been proposed to counter adversarial perturbations in classifiers. Certified robustness against adversarial perturbations has been developed to defend classifiers, with a focus on top-k predictions. Existing results are limited to top-1 predictions, prompting the study of certified robustness for top-k predictions in this work. In this work, certified robustness for top-k predictions is studied using randomized smoothing. Randomized smoothing adds random noise to a base classifier to make it robust. Gaussian noise is chosen for its certified robustness guarantee. The smoothed classifier predicts the k labels with the largest probabilities for the example. Randomized smoothing is scalable and applicable to any base classifier, with a tight certified robustness bound for top-k predictions. Certified robustness for top-k predictions with randomized smoothing using Gaussian noise is studied. The certified radius for top-1 predictions is a special case of the certified radius for top-k predictions. Generalizing certified robustness from top-1 to top-k predictions presents new challenges and requires new techniques. The certified radius depends on \u03c3, p l , and the k largest probabilities p i 's. Computing the certified radius faces challenges in practice. Certified robustness for top-k predictions with randomized smoothing using Gaussian noise is studied. The certified radius for top-1 predictions is a special case of the certified radius for top-k predictions. To address challenges in computing the certified radius, simultaneous confidence intervals of label probabilities are estimated using statistical methods. An algorithm is proposed to obtain a lower bound of the certified radius, with results evaluated on CIFAR10 and ImageNet datasets. The method achieves approximate certified accuracies on ImageNet, with contributions including deriving the first certified radius for top-k predictions. Certified robustness for top-k predictions with randomized smoothing using Gaussian noise is studied. The method derives the first certified radius for top-k predictions and develops algorithms to estimate the certified radius in practice. Empirical evaluation is conducted on CIFAR10 and ImageNet datasets. The goal is to derive a certified radius R such that a certain label is within the set of k labels with the largest probabilities when adding Gaussian noise to an example. Theoretical results are summarized in two theorems regarding deriving a certified radius for top-k predictions. Given an example x, a base classifier f, a smoothed classifier g, and certain probability bounds, the certified radius Rl is the unique solution to an equation. The certified radius Rl is the unique solution to an equation involving the cumulative distribution function of the standard Gaussian distribution. Theorem 2 states the conditions for the tightness of the certified radius, ensuring the existence of a base classifier consistent with the perturbation constraints. Detailed proofs are provided in the appendices. The certified radius Rl is determined by the cumulative distribution function of the standard Gaussian distribution. It depends on \u03c3, pl, and the k largest probability upper bounds. The radius is larger when the bounds are tighter. If ||\u03b4|| 2 > Rl, then Pr(f*) > 1. The certified radius is applicable to any base classifier and when Rl < 0, label l is not among the top-k predicted labels. The certified radius Rl is determined by the standard Gaussian distribution function and depends on \u03c3, pl, and the k largest probability upper bounds. It is challenging to compute the top-k labels predicted by the smoothed classifier accurately, so a Monte Carlo method is used with a probabilistic guarantee. The PREDICT function in Algorithm 1 estimates the top-k labels predicted by the smoothed classifier using hypothesis testing results from previous work. The PREDICT function estimates top-k labels predicted by the smoothed classifier using hypothesis testing. The function SAMPLEUNDERNOISE randomly samples noise from a Gaussian distribution, while BINOMPVALUE calibrates the abstention threshold for correct label prediction. The certified radius Rl is computed based on the standard deviation \u03c3, label l, and Gaussian noise. The certified radius Rl is calculated using a lower bound of pl and an upper bound of pSt, related to f, x, and \u03c3. Two Monte Carlo methods are discussed to estimate pl and pSt with guarantees. An algorithm is designed to find a lower bound of Rl through binary search, with steps to estimate pl and pi for i = l, and then estimate pSt using pi for i = l. The text discusses estimating confidence intervals for labels in a multinomial distribution. Existing methods are deemed insufficient due to low expected counts for some labels. The goal is to estimate bounds with a confidence level of at least 1 - \u03b1. The text discusses two confidence interval estimation methods for labels in a multinomial distribution: BinoCP method uses the Clopper-Pearson method to estimate confidence levels, aiming for at least 1 - \u03b1. SimuEM is introduced to estimate label probabilities directly, aiming to avoid conservative estimations that result in smaller certified radii. The method leverages the Clopper-Pearson method and Bonferroni correction to obtain simultaneous confidence intervals for each label. SimuEM uses the Clopper-Pearson method and Bonferroni correction to estimate label probabilities directly, ensuring confidence intervals hold simultaneously. The method derives bounds for estimating probabilities and computes the certified radius Rl challenging. The method in SimuEM uses binary search to estimate a lower bound for the certified radius Rl, which can be tuned to be close to the exact solution. Algorithm 2 presents a method to estimate the certified radius for a given example x and label l. The algorithm utilizes functions to estimate probability bounds and perform binary search to find a solution satisfying certain conditions. With a probability of at least 1 - \u03b1, the algorithm guarantees that if a radius Rl is returned, then the label l is within a certain range of the example x. Experiments were conducted on CIFAR10 and ImageNet datasets. Experiments were conducted on CIFAR10 and ImageNet datasets using pre-trained ResNet models. Parameters such as k, confidence level, noise level, number of samples, and confidence interval estimation methods were studied to evaluate the certified radius. The CERTIFY algorithm was used to compute the certified radius for each testing example. The CERTIFY algorithm is used to compute the certified radius for testing examples, and the certified top-k accuracy is approximated based on this radius. The gap between the lower bound of the true certified top-k accuracy and the approximate accuracy is small when \u03b1 is small. The certified top-k accuracy increases as k increases, with results on CIFAR10 showing certified top-1, top-2, and top-3 accuracies of 45.2%, 58.8%, and... On CIFAR10 and ImageNet, certified top-k accuracies are achieved with small gaps between different k values. The influence of confidence level on certified accuracy is minimal. The estimated confidence intervals of probabilities shrink slowly as the confidence level increases. \u03c3 controls a trade-off between normal accuracy and robustness. BinoCP is better for large certified radii, while SimuEM is better for small radii. Defenses against adversarial perturbations have been proposed in recent years. Many empirical defenses have been proposed to show robustness against attacks, with adversarial training being the most effective. However, these defenses lack certified robustness guarantees and have been broken by adaptive attacks. Certified defenses have been developed to combat adaptive attacks and provide a lower bound on adversarial perturbations. These defenses include methods based on satisfiability modulo theories, mixed integer linear programming, abstract interpretation, and Lipschitz constants. Randomized smoothing is a scalable defense method for large neural networks that is applicable to arbitrary classifiers. It provides a certified robustness guarantee against adversarial attacks, unlike other defenses that are not scalable or make assumptions on neural network architectures. Our work provides the first certified robustness guarantee for top-k predictions using randomized smoothing with Gaussian noise. Existing defenses focus on top-1 predictions, but we derive a certified radius under 2-norm for top-k predictions. We also propose methods for computing the certified radius and suggest future directions for research. Our work focuses on studying certified robustness for top-k ranking by deriving a certified radius under 2-norm for randomized smoothing. We define random variables X and Y representing samples with isotropic Gaussian noise added to examples x and x + \u03b4. Additionally, we present Lemmas 2 and 3 based on previous work by Cohen et al. (2019) and discuss the implications for base classifiers and label sets. The text discusses certified robustness for top-k ranking using randomized smoothing. It introduces random variables X and Y with isotropic Gaussian noise, presents Lemmas 2 and 3, and defines regions A S and B S. Theorem 1 provides conditions for certified radius calculation. The text discusses certified robustness for top-k ranking using randomized smoothing, introducing Lemmas 2 and 3 and defining regions A S and B S. Theorem 1 provides conditions for certified radius calculation based on probabilities of base classifier predictions. The text discusses certified robustness for top-k ranking using randomized smoothing, introducing Lemmas 2 and 3 and defining regions A S and B S. Theorem 1 provides conditions for certified radius calculation based on probabilities of base classifier predictions. It involves bounding probabilities for labels in subsets of \u0393 k to obtain a certified radius R l. Equation 27 holds for any subset S of \u0393 k, where S t is the set of t labels in \u0393 k with the smallest probability upper bounds. Equation 30 is derived from Equation 29, with Pr(Y \u2208 B S ) decreasing as p S decreases. The maximum value of Equation 30 is reached when \u0393 k is the set of k labels in \u0393 with the largest probability upper bounds. Obtaining R l involves setting conditions based on probabilities of base classifier predictions. According to Lemma 2, the constraint on \u03b4 is ||\u03b4|| 2 < R l. A region A {l} is defined with Pr(X \u2208 A {l} ) = p l. Lemma 4 states conditions for perturbation \u03b4 2 > R l, leading to disjoint regions C bj satisfying certain criteria. The proof is based on mathematical induction. The proof of Theorem 1 is based on mathematical induction and the intermediate value theorem, with the detailed proof deferred to Appendix B.1. Theorem 2 discusses the tightness of the Certified Radius, stating conditions for perturbation \u03b4 > Rl and the existence of disjoint regions satisfying specific criteria. The idea is to construct a base classifier where l is not among the top-k labels predicted by the smoothed classifier for perturbed examples when ||\u03b4|| 2 > Rl. Based on the construction of a base classifier, the proof shows that l is not among the top-k labels predicted by the smoothed classifier for perturbed examples when ||\u03b4|| 2 > Rl. This is achieved by dividing the region into c\u2212k\u22121 regions and defining key notations and lemmas. Based on the construction of a base classifier, the proof shows that l is not among the top-k labels predicted by the smoothed classifier for perturbed examples when ||\u03b4|| 2 > Rl. This is achieved by dividing the region into c\u2212k\u22121 regions and defining key notations and lemmas. Given two values q 1 and q 2 that satisfy 0 \u2264 q 1 < q 2 \u2264 1, we define regions C (q 1 , q 2 ) and r x (q 1 , q 2 ), r y (q 1 , q 2 ). Lemma 2 defines the Gaussian random variable X, and pairs of (q 1 , q 2 ) define regions C (q 1 , q 2 ) and r x (q 1 , q 2 ), r y (q 1 , q 2 ). The probability densities h x and h y for random variables X and Y are compared, showing a key property of the defined functions r x (q. The text discusses equations and properties related to defined functions r x (q) and the Intermediate Value Theorem. It also introduces Lemma 7 regarding probabilities q x and q y. Lemma 7 states that given probabilities q x and q y, there exist q 1 and q 2 in the interval [q 1, q 2] such that certain conditions are met. This lemma is used to show the existence of q 1 and q 2 satisfying specific equations. Lemma 8 states that if q 1 is given, then certain conditions are met, leading to the conclusion that no region is excluded. This is proven by applying Lemma 5 and generalizing it to two regions. The proof uses Mathematical Induction to show that for each iteration, certain conditions are met. It splits B into two parts and shows that disjoint sets can be found for each part. The proof leverages Mathematical Induction to construct each C bj, showing the existence of disjoint sets for each part of B. It demonstrates finding C b\u03c4 satisfying specific requirements and then iteratively finding C b\u03c4\u2212m. The proof utilizes Lemma 7 and shows the steps to find C b\u03c4 satisfying Equation 85 and 86. Based on the definition of r x, r y, we find C b\u03c4 = C(q 1, q 2) \u2229 C(q \u03c4 1, q \u03c4 2). By leveraging Lemma 7, we can find q 1, q 2 such that r x(q 1, q 2) = p be \u2264 Pr(X \u2208 C w) = p bt. In two scenarios, we show the existence of q 1, q 2 satisfying specific equations. This is achieved by iteratively finding C b\u03c4\u2212m and utilizing Lemma 8. Based on Lemma 7 and the definition of r x, r y, we establish the intersection C e = C(q 1, q 2) \u2229 C(q e 1, q e 2). Through Mathematical Induction, we prove that for all j in [1, \u03c4], C be \u2229 C bj = \u2205. Additionally, we demonstrate the construction process for B S k \\ B S\u03c4 using similar methods but with subtle differences. Equation 150 is derived from C (q 1, q 2) = C(q 1, q 2) and Definition 1, leading to p b k \u2264 r x(q 1, q 2). Based on Lemma 7 and definitions of r x, r y, we establish C e = C(q 1, q 2) \u2229 C(q e 1, q e 2). Through Mathematical Induction, we prove C be \u2229 C bj = \u2205 for all j in [1, \u03c4]. Equation 154 is obtained from 155 using Equation 83, leading to p b k \u2264 r x(q 1, q 2). Two scenarios are considered: one where r y(q 1, q 2) > q 2, and another where r y(q 1, q 2) \u2264 Pr(Y\u2208B S\u03c4) \u03c4. By defining q 1 = \u03c4 j=1 p bj and q 2 = k j=1 p bj, we can find (q 1, q 2) such that p be \u2264 r x(q 1, q 2). Based on Lemma 7 and definitions of r x, r y, we establish C e = C(q 1, q 2) \u2229 C(q e 1, q e 2). Through Mathematical Induction, we prove C be \u2229 C bj = \u2205 for all j in [1, \u03c4]. Equation 154 is obtained from 155 using Equation 83, leading to p b k \u2264 r x(q 1, q 2). Two scenarios are considered: one where r y(q 1, q 2) > q 2, and another where r y(q 1, q 2) \u2264 Pr(Y\u2208B S\u03c4) \u03c4. By defining q 1 = \u03c4 j=1 p bj and q 2 = k j=1 p bj, we can find (q 1, q 2) such that p be \u2264 r x(q 1, q 2). In this scenario, we have q w 2 < q 2 because q w 2 = q 2 and q 1 > q w 1 cannot hold at the same time as long as r y(q 1, q 2) > 0. Thus, we have Pr(Y \u2208 C w ) = , we have q w 2 = q 2. As we have q 1 > q w 1, q 2 > q w 2 and r x(q 1, q 2) = p be \u2264 Pr(X \u2208 C w ) = p bw. Based on Lemma 7 and definitions of r x, r y, we establish C e = C(q 1, q 2) \u2229 C(q e 1, q e 2). Through Mathematical Induction, we prove C be \u2229 C bj = \u2205 for all j in [1, \u03c4]. The function SAMPLEUNDERNOISE(f, k, \u03c3, x, n, \u03b1) works by drawing random noise and computing values. The function BINOMPVALUE(n ct, n ct + n ct+1, p) returns the p-value of a hypothesis test. The function BINOMPVALUE(n ct, n ct + n ct+1, p) returns the p-value of a hypothesis test for n ct \u223c Bin(n ct + n ct+1, p). Proposition 1 states that if PREDICT returns a set T (not ABSTAIN), then g k (x) = T with probability at least 1\u2212\u03b1. Proposition 2 states that if CERTIFY returns a radius R l (not ABSTAIN), then l \u2208 g k (x +. In CERTIFY, if a radius R l is returned (not ABSTAIN), then l \u2208 g k (x + \u03b4), \u2200 \u03b4 2 < R l. The probability of certain inequalities holding is at least 1 \u2212 \u03b1. The robustness guarantee is obtained if the radius is larger than 0; otherwise, CERTIFY abstains. A lower bound of certified top-k accuracy is derived based on the approximate certified top-k accuracy, similar to previous work. The text discusses the certified top-k accuracy of a smoothed classifier at a given radius. It mentions using a test dataset and a confidence level to compute this accuracy. The text also refers to Proposition 2 and Lemma 11 to obtain lower bounds for the accuracy with a certain probability. The difference between certified top-k accuracy and approximate certified top-k accuracy is negligible when \u03b1 is small, showing the impact of randomness on CERTIFY."
}