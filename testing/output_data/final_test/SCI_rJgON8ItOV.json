{
    "title": "rJgON8ItOV",
    "content": "We present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. This framework involves identifying interpretable units related to object concepts, examining causal effects of these units, and exploring contextual relationships. Practical applications include comparing internal representations, improving GANs by removing artifact-causing units, and manipulating objects in scenes. The framework allows for analyzing internal GAN representations by decomposing featuremaps into unit channels and positions. Units can be manipulated to make objects disappear or appear, and contextual relationships can be studied by inserting object concepts into new images. This approach helps compare representations, debug GANs, and improve them by identifying artifact-causing units. The segmentation network uses a threshold to maximize information quality ratio. Units are intervened to identify semantic effects, generating images with units ablated or activated. The average causal effect of units on class is measured. The analysis of internal units in a GAN reveals new findings, with diverse objects emerging on more diverse models. Innovations in Progressive GANs lead to an increase in units matching semantic classes. Interpretable units are found in mid-level layers, unlike in classifier networks. Our framework can analyze failures and repair GAN artifacts by ablating artifact-causing units, resulting in improved generated results. Quality improvements are measured using Fr\u00e9chet Inception Distance and realism annotations from Amazon MTurk. A \"door\" intervention at layer4 has varying effects on the final convolutional feature layer. The effect of inserting door units in specific locations in scenes is analyzed, showing varying effects on the final convolutional feature layer. Brighter colors in the heatmap indicate stronger effects on layer14 features, with larger effects in building locations and smaller effects near trees and sky. Feature changes for interventions resulting in human-visible changes are separated from those that do not show noticeable output changes. The effects of inserting door units in specific locations vary widely depending on the context, with doors matching the visual attributes of buildings. Doors cannot be added in most locations, but are highlighted in yellow boxes where possible. The causal effects of door insertions are influenced by the object class at the intervention location, with windows being a particularly good location for doors. The insertion of 20 door-causal units in specific locations causes changes in later layer featuremaps at layer 4. Effects on downstream features are quantified and normalized, with a heatmap in FIG4 showing stronger effects on the final feature layer when doors are near buildings. Interventions provide insight into how a GAN enforces object relationships, with small interventions at layer 4 leading to amplified changes up to layer 12. Adding a door in layer 4 can be vetoed by later layers if it doesn't fit the context, showing how a GAN enforces object relationships."
}