{
    "title": "Hyet2Q29IS",
    "content": "The aim of this work is to combine handcrafted constraints with deep convolutional neural networks in wave-based least-squares imaging. The method is based on the expectation-maximization framework, where data is divided into batches and coupled with additional \"latent\" unknowns. The neural network controls the similarity between these additional parameters, resulting in a maximum-likelihood estimation of the network parameters. This approach is used for inverting inconsistent ill-conditioned problems in least-squares imaging. In least-squares imaging, the linear inverse problem involves an unknown image vector x, observed data y from N experiments, and linearized forward operators A. Adding constraints or penalties to the problem can improve solutions, but challenges remain due to large, expensive linear systems and inconsistencies caused by noise or errors. To address computational challenges in solving non-differentiable constraints, Bregman iterations are used with a batch size of one. The approach involves iterating on the dual variable and replacing sparsity-promoting thresholding with projections to ensure feasibility in each iteration. The approach involves using Bregman iterations with a batch size of one to address computational challenges in solving non-differentiable constraints. By replacing sparsity-promoting thresholding with projections, each model iterate remains in the constraint set. Stochastic gradient descent on the dual variable is achieved by randomly drawing different source experiments for each iteration, creating high-fidelity images with handcrafted priors encoded in the constraint set C. To enhance this approach, CNNs are proposed as deep priors on the model. The use of CNNs as deep priors in the model is proposed to reduce bias and improve image quality. By replacing the unknown model with a neural net, this approach is suitable for situations where data-image training pairs are not available. Solving the formulated problem can lead to good estimates for natural images within the range of the CNN. The CNN g(z, w) with w as the minimizer of problem 5 is suitable when data is limited. Using neural nets as constraints may have advantages, but physical feasibility of model iterates is not guaranteed. Pre-training the network is necessary to ensure feasibility in early iterations. Results from solving inverse problems with deep priors may benefit from additional regularization. Combining hard handcrafted constraints with a weak constraint for the deep prior reformulates problem 5. The deep prior in problem 5 is expressed as a penalty term weighted by \u03bb > 0. The formulation allows for constraints that can be relaxed during training by adjusting the TV-norm ball size or trade-off parameter \u03bb. While neural networks can regularize inverse problems deterministically, they do not fully utilize generative capabilities. Neural networks can generate realizations from learned distributions, posing challenges in inverse problems where objects of interest are unknown. Two options exist: access to an oracle with training images or assumptions on image statistics. Learned priors and inferred posteriors may be biased by limited understanding of the inversion process or statistical properties, leading to simplifications or poor choices in regularization. Our approach aims to learn the posterior through inversion using a combination of hard handcrafted constraints and weak deep priors, inspired by Han et al.'s use of the Expectation Maximization technique. We propose training a network to generate realizations from seismic data collected from the same Earth model, treating each source experiment as a separate dataset for inversion. This differs from assuming all data comes from the same source, avoiding biases and simplifications in regularization. Our approach involves solving an unsupervised training problem by pairing observed data, latent, and slack variables into tuples. Latent variables are initialized as zero-centered white Gaussian noise and sampled using Stochastic Gradient Langevin Dynamics. Slack variables are computed using Bregman iterations, working on randomized source experiments of each minibatch. The approach involves warm-starting latent variables while keeping network weights fixed, followed by unsupervised inference and supervised learning steps. Uncertainty in latent variables is addressed through SGLD iterations. The generative model is exposed to uncertainties by drawing samples from the posterior via Langevin dynamics. The training procedure involves sampling from the posterior distribution and updating network weights. Different solutions of the Bregman iterations are converged for each dataset, allowing room for deviation when \u03bb is not too large. The approach replaces the center variable with a generative network and conducts a survey with experiments containing noise and linearization errors. The training procedure involves updating network weights by sampling from the posterior distribution. The generative network produces smoother models compared to the primal Bregman variable. Variations among different generative models average out in the mean, reducing imaging artifacts. Statistical information can be computed from the trained generative model to assess uncertainty. A plot of pointwise standard deviation is included, computed from random realizations of the generative model. The text discusses the use of hard constraints and deep priors in an inverse problem framework, showing how pointwise standard deviations sharpen probabilities before and after training. High standard deviation areas coincide with difficult imaging regions due to errors and noise. Deep priors, enforced through neural networks, provide regularization. The algorithm is mathematically sound and supported by numerical results. The algorithm discussed combines stochastic optimization on the dual variable with on-the-fly estimation of the generative model's weights using Langevin dynamics. It leverages multiplicity in the data and Earth model consistency to extract information from the \"posterior\" distribution. Preliminary results show behavior consistent with expected distribution outcomes."
}