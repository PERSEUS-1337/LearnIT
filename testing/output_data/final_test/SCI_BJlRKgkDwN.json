{
    "title": "BJlRKgkDwN",
    "content": "We explore topic modeling in a weakly semi-supervised setting where the user provides a subset of topics and exemplar documents. The user does not need to identify all topics exhaustively in each document. Recent state-of-the-art topic models, such as Neural Topic Models (NTMs), are extended to the weakly semi-supervised setting by using informative priors in the training objective. A modification of the NVDM model using a logit-normal posterior achieves better alignment to user-desired topics. Topic models are probabilistic models that represent abstract topics in data generation processes and are useful for feature representations and exploring various types of data. Neural Topic Models (NTMs) leverage deep learning for topic modeling, fitting an approximate posterior using variational inference to predict document topics efficiently. While NTMs model documents well, user guidance may be needed to align topics with natural semantics. Supervision in NTMs, unlike classical LDA models, remains unexplored. The main contribution of our work is an NTM with the ability to leverage minimal user supervision to better align topics to desired semantics. Topic models describe documents as being generated by a mixture of underlying abstract topics represented as distributions over words in a vocabulary. A Neural Topic Model is constructed according to the variational autoencoder paradigm. The text discusses a topic model constructed using the variational autoencoder paradigm, where a document is generated based on a latent topic mixture. The model utilizes an approximate posterior and maximizes the evidence lower bound through stochastic optimization. The topics extracted from unsupervised topic modeling can be challenging to interpret, and users may have specific topics in mind that they want the model to align with. In a semi-supervised setting, a user guides a model to align with specific topics by labeling a subset of documents. Supervision through exemplar documents improves topic alignment, addressing challenges of limited labeled data and weak supervision. In a semi-supervised setting, a user guides a model to align with specific topics by labeling a subset of documents. The supervision is weak as the user may not identify all topics present. \"Ground truth\" word rankings for each topic are defined using a chi-square association metric. The top-10 words for each topic are those with the highest relative importance. Alignment of topics extracted by an NTM is evaluated using average normalized point-wise mutual information (NPMI) between words in the NTM's topics and the \"ground truth\" topics. Our approach focuses on extending the NTM to the semi-supervised setting by modifying the prior distribution over latent variables in a VAE to incorporate user label information. The user provides partial binary labels indicating topic existence in a document, encouraging the model's posterior samples to match the true presence of topics. In the semi-supervised setting, the model's posterior samples are encouraged to match the true presence of topics in a document by incorporating user label information. The probabilities of topic existence are recovered by sigmoiding the encoder outputs, and the generative distribution is conditioned on the logits of possible one-hot vectors to extract topics from the model. The text discusses different posterior sampling methods, including the Concrete/Gumbel-softmax approximation, to model the existence of topics in documents. It explores sampling techniques and prior probabilities to identify top words associated with each topic. In Equation (6), the log-density of Concrete(\u03c0, \u03c4) is detailed in BID4. The Bernoulli-D KL is used as an approximation for the posterior. A logit-normal distribution is also considered, with the logit(\u03c5) being normally distributed. Prior distributions are fixed for the Bernoulli model and NVDM in FIG1. The cross entropy loss and posterior variance are discussed, showing that NVDM D KL provides a strong loss signal for topic prediction, while the Bernoulli model achieves better topic coherence. The Bernoulli model achieves better topic coherence compared to NVDM due to cleaner decoding process. A new model, NVDM-\u03c3, is proposed with a logit-normal posterior to combine strong supervision signal of NVDM with clean decoding. Three multi-label datasets are used for training and evaluation: BibTeX, Delicious, and AAPD extracted from computer science ArXiv papers. Three experimental variables are varied in the study. In a study on computer science ArXiv papers, three experimental variables were varied: the percentage of topics supervised (10%, 50%, or 100%), the number of labeled samples per supervised topic, and the number of labels given per document. Different settings were explored independently based on these variables. The study varied three experimental variables independently, resulting in six different settings for each of three datasets. NPMI increased by 12% with weak labels compared to unsupervised, and by 15% beyond that to fully supervised. This shows weak labels can significantly boost performance. The study evaluated the performance of different models in supervised topic alignment. Only three documents were labeled for each topic, showing successful alignment. NVDM-\u03c3 performed the best in terms of all NPMI metrics. In this work, the study proposed supervising Neural Topic Models with weak supervision via informative priors and explored various model posteriors. An NTM with logit-normal posterior was found to align extracted topics to desired user semantics. The encoder used a feedforward neural network with 2 hidden layers, sigmoid nonlinearities, and a linear transformation. The final layer outputted a vector for the parameters of the Gaussian posterior, with the mean and standard deviation obtained accordingly. The study proposed supervising Neural Topic Models with weak supervision via informative priors and explored various model posteriors. The encoder outputted a vector for the parameters of the Gaussian posterior, with the standard deviation obtained from the last K entries. The final linear layer output was interpreted as logit(\u03c0), with a latent-space dimensionality of 50 for 20NG dataset. The decoder consisted of a linear layer followed by a softmax for multinomial probabilities. Training was done with Adadelta and rescaled gradients by 0."
}