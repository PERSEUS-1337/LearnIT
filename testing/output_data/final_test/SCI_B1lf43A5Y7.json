{
    "title": "B1lf43A5Y7",
    "content": "The memory network model can learn to distill and manipulate text representations without explicit supervision. It struggles with reasoning tasks without attention mechanism supervision but performs well with it. On the WikiHop dataset, it achieves near state-of-the-art performance but doesn't utilize its multi-hop capabilities. Recent advancements in question answering from text have led to the development of new datasets and models, such as BID22, BID9, BID8, BID21, BID24, and BID25. While some QA tasks like SQuAD rely on simple pattern matching, recent work has focused on multi-hop reasoning, particularly with memory networks. These networks iterate through a text passage using the question and a memory cell to gather information from different parts of the passage. Memory networks are used in reading comprehension models to iteratively gather information from a passage. This paper examines the effectiveness of memory networks and their behavior in selecting relevant passage excerpts. The study includes tasks in synthetic and realistic settings, such as the bAbI path-finding task and multi-hop reasoning datasets from Wikipedia. By applying memory networks to these tasks, the performance and behavior of the models are observed. The study evaluates memory networks in reading comprehension models, focusing on their reasoning abilities. Results show that memory networks do not always reason correctly but can improve with additional supervision. The hierarchical memory network's computation flow is illustrated in Figure 1, showing how queries and attention mechanisms are used to infer answers. In a masked version of the WikiHop task, the memory network performs poorly without explicit supervision, but improves when guided by pseudogold chains. Memory networks struggle to learn multi-hop reasoning from task supervision alone, even though they can achieve near state-of-the-art performance on the WikHop dataset. Additional supervision on attention can lead to significant improvements, bringing the model closer to more sophisticated state-of-the-art models. In this paper, a hierarchical version of the original memory network is used for text question answering tasks. The model utilizes word-level bidirectional GRUs to compute contextual representations of tokens in the query and sentences. The model in this paper uses a hierarchical version of the original memory network for text question answering tasks. It utilizes word-level bidirectional GRUs to compute contextual representations of tokens in the query and sentences. The passage representation is computed using sentence-level and word-level attention mechanisms. Multiple steps of reasoning involve shared GRU encoders and different attention weight matrices. The model proposed in this paper, called MemNet, utilizes attention weight matrices to compute sentence-level and word-level attention for text question answering tasks. It can predict answers from the last memory cell or a combination of multiple memory cells, which is a hyperparameter that can be adjusted for different datasets. By supervising the attention weights, the model can be trained to use the right mode of reasoning at each timestep. The model MemNet utilizes attention weights for text question answering tasks. It can predict answers from the last memory cell or multiple cells. By supervising attention weights, the model can be trained for the right reasoning mode. The model is trained with extra supervision initially and then with downstream supervision. Experiments are conducted on the bAbI dataset to test multi-hop reasoning. The MemNet model uses attention weights for text question answering tasks, achieving strong performance on most bAbI sub-tasks. However, it struggles with the Path Finding task. Supervising attention layers significantly improves performance. Experiments on the synthetic bAbI dataset show the effectiveness of the model, especially with 2-hop memory networks. The MemNet model struggles with the Path Finding task but improves with attention supervision. The model's behavior is compared to gold reasoning chains provided in the dataset. For the Path Finding task, the number of MemNet hops is fixed at 2, with a word embedding dimension of 100 and GRU implementation. The MemNet model's performance on Path Finding task improves significantly with attention supervision. Even with 10k examples, the model struggles to generalize without extra supervision. Comparing the model's attention values with gold reasoning chains shows alignment with human reasoning. The MemNet model struggles to generalize without extra supervision, even with 10k examples. Attention supervision improves performance significantly, helping the model learn the right pattern of reasoning. However, in a simple synthetic setting, the MemNet fails to learn multi-hop reasoning, indicating a limitation in its capacity. The MemNet model struggles to generalize without extra supervision, even with 10k examples. Attention supervision improves performance significantly, helping the model learn the right pattern of reasoning. However, in a simple synthetic setting, the MemNet fails to learn multi-hop reasoning, indicating a limitation in its capacity. This raises questions about the effectiveness of memory networks in more complex real-world problems and the potential for improvement in those settings. WikiHop BID30 is a reading comprehension dataset designed for multi-hop reasoning over multiple documents, where questions are posed as queries of a relation followed by a head entity, with the task of finding the tail entity from a set of candidates. Unlike bAbI, a gold reasoning chain is not provided in WikiHop, but a pseudogold chain can be derived based on heuristic reasoning. The Stanford Named Entity Recognition system BID6 is used to identify entities in documents, constructing a graph over sentences to find a pseudogold reasoning chain. This process involves searching for sentences containing the head entity in a query and using breadth-first search to find the answer. More advanced strategies are possible for reasoning chains. The study conducted graph search to find reasoning chains for pseudogold data, revealing that most chains require two or three steps of reasoning. When the NER system fails to recognize entities, examples are not supervised. The number of reasoning steps is fixed at 3, with final attention targets duplicated for remaining steps if the pseudogold chain is less than 3. The study used graph search to find reasoning chains for pseudogold data. When the pseudogold chain is longer than 3 entries, the first and last steps are supervised, with all other sentences as possible intermediate targets. Memory cells are combined in the second and third steps, and a dot product is done with each option to get probabilities. The vocabulary is limited to the most frequent 50k words, with other tokens mapped to unk. Word embedding is initialized with 100-dimensional pre-trained Glove embedding. GRU has a hidden size of 128 and a dropout rate of 0.5. Attention weights are initialized using Glorot initialization. The batch size is 24, and Adam is used as the optimization method with an initial learning rate of 0.0001. Our MemNet model outperforms other systems, including those using coreference resolution, on a dataset. Supervising attention improves performance by around 1%. The model may compensate for test-time preprocessing, and its success may be due to a more straightforward approach. The study found that a \"no text\" baseline model achieved 59.7% performance on the development set, indicating that the task can be solved without using the document text. This suggests that the MemNet model may not rely heavily on multi-hop reasoning for its high performance. The dataset was corrected using a masked version to address this issue. The NoText model was able to pick up on correlations between questions and options without needing to reason over the text. The dataset was masked to remove correlations, forcing the model to use multiple hops to find answers. The basic memory network struggled to learn reasoning without supervision, achieving 48.7% accuracy with guidance. Capturing correct reasoning is within the model's capacity, but supervision is necessary for learning. The model requires additional supervision to learn in the masked setting, leading to performance improvements even in the unmasked setting. Two experiments were conducted to understand the attention mechanism's behavior, focusing on the fraction of attention on pseudogold sentences and the model's accuracy based on attention weight on the final step of the pseudogold chain. Results showed a more concentrated distribution of attention on the correct chain. The attention distribution of MemNet is flatter compared to bAbI, with most attention weight on the pseudo gold path being less than 0.1. Despite this, MemNet can still achieve 58.0% accuracy, indicating a focus on lexical overlap rather than correct reasoning. Performance seems independent of attention identifying the right sentence, suggesting a reliance on learning lexical patterns. Comparing MemNet+Sup and MemNet Masked+Sup shows a correlation between additional supervision and improved performance. The study compares MemNet+Sup and MemNet Masked+Sup, finding that attention focused on the pseudogold correlates with strong model performance. The masked model performs well when confident but struggles when unsure, indicating reliance on lexical matching. Despite additional supervision, MemNet still struggles to learn correct reasoning directly from data. The study identifies major categories of errors in the model's predictions, including attention split and wrong tail entity. These errors suggest that the model lacks the power to perform correct reasoning, possibly due to ambiguity in natural language or the need for improved global reasoning abilities. Other errors are attributed to wrong attention or unknown reasons. Memory networks define a model class for multi-hop reasoning over passages. Many models have incorporated memory and multi-hop computation, showing improvement in various tasks. Attention mechanisms are widely used in NLP tasks like machine translation, question answering, and summarization. In this paper, the memory network is explored for multi-hop reasoning tasks. Experimental results show that additional supervision is needed for generalizable multi-hop reasoning, but with this supervision, the memory network model achieves strong results on the WikiHop dataset."
}