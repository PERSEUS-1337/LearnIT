{
    "title": "S1xf-W5paX",
    "content": "Identifying hypernym relations between words is crucial in NLP. A new method is proposed to learn hierarchical word embeddings that capture hypernymy by considering both hypernym relations in a taxonomy and contextual information in a text corpus. Experimental results demonstrate the method's ability to encode hierarchy and outperform previous methods in learning word embeddings. Hypernymy relation is essential in NLP tasks. Hypernymy relation is crucial for NLP tasks like question answering, taxonomy construction, and text generation. It can be identified through lexical-syntactic patterns or distributional representation of word pairs. For example, a hypernymy relation can be inferred from a sentence like \"a bird such as a falcon\". Pattern-based approaches for identifying hypernymy relations, like \"a bird such as a falcon,\" can suffer from low precision and coverage issues. Unlike lexical-pattern based approaches, distributional approaches rely on co-occurrence statistics in a large text corpus to deduce hypernymy. These approaches work on the assumption that taxonomically related words tend to occur in a similar context, with a hypernym word having a broader context than its hyponym. Recent studies have proposed methods for learning hypernymy-specific word embeddings, utilizing neural networks to perceive hypernymy and extracting pairwise training data from a web corpus. These approaches aim to encode taxonomic relations in the learned word embeddings, overcoming limitations of traditional word embeddings models. Recent studies have proposed methods for learning hypernymy-specific word embeddings, utilizing neural networks to perceive hypernymy and extracting pairwise training data from a web corpus. Anh et al. [2016] and Nguyen et al. [2017] have introduced approaches that incorporate contextual information and utilize Skip-gram with Negative Sampling (SGNS) to learn word embeddings subject to pairwise hypernymy constraints. Additionally, BID27 proposed a model embedding symbolic data into hyperbolic space, while the LEAR model of BID39 adjusts word vectors to emphasize hypernym relations. These methods focus on pairwise hypernymy relations rather than hierarchical paths in a taxonomy. The proposed method in this paper introduces a way to learn hierarchical word embeddings (HWE) by embedding words into low-dimensional vectors and updating them to encode hierarchical structures from a taxonomy. It utilizes contextual information and taxonomic relations to train the model, benefiting from both aspects to learn the embeddings. The hierarchical word embeddings are evaluated on various tasks such as supervised hypernymy, graded lexical entailment prediction, and hierarchical path completion. The proposed method in this paper introduces a way to learn hierarchical word embeddings by considering full hierarchical hypernymy paths instead of only pairwise relations. It shows improvement in supervised hypernymy identification and ability to encode hypernymy in learned embeddings. The NLP community typically follows pattern-based and distributional approaches for identifying hypernymy relations between words. The Hearst's patterns BID13 demonstrate hypernymy relations, while distributional methods rely on the Distributional Inclusion Hypothesis (DIH) BID11 to identify hypernymy relationships based on word distribution in text corpora. The Distributional Inclusion Hypothesis (DIH) assumes that most features of a hyponym word are a subset of the hypernym word features. Unsupervised methods for hypernymy identification include pattern-based and distribution-based approaches, with some hybrid approaches combining their strengths. Methods using pre-trained word embeddings can predict hypernym relations between word pairs, combining unsupervised and supervised techniques. Unsupervised word embeddings are used as input in hybrid approaches for hypernymy identification, combining unsupervised and supervised methods. Various unsupervised operators have been studied to represent the relationship between two words using their embeddings, such as concatenation, difference, addition, and hybrid combinations. While supervised methods have shown good hypernym detection accuracies, doubts have been raised about their effectiveness. The effectiveness of supervised methods for learning hypernymy relations is questioned due to concerns about memorization of prototypical hypernyms. A new approach using hypernymy-specific word embeddings has gained popularity, with a focus on encoding hypernymy properties in the learned embeddings. Previous work has shown that considering co-occurrence contexts in sentences can provide useful semantic clues for detecting taxonomic relationships. Anh et al. [2016] and Nguyen et al. [2017] introduced methods for learning hypernymy-specific word embeddings. Anh et al. utilized pairwise hypernymy data from WordNet and related sentences from Wikipedia, while Nguyen et al. developed the HyperVec model that learns from hypernym pairs and contextual data. Their approaches outperformed previous methods in hypernymy relation identification tasks. BID39 introduced the LEAR model for learning word embeddings that encode hypernymy, while Nickel and Kiela proposed the Poincar\u00e9 model for hierarchical embeddings in a hyperbolic space. These methods leverage WordNet hypernymy data but have different approaches in learning. The proposed method learns word embeddings by encoding the hierarchy of hypernymy relations between words, focusing on the entire hierarchical path rather than just pairwise relations. This approach aims to capture both direct and indirect hypernymy relations for a better understanding of the problem. The proposed method learns hierarchical word embeddings by encoding hypernymy relations in a taxonomy, focusing on the full hierarchical path rather than just direct relations. The proposed method learns hierarchical word embeddings by encoding hypernymy relations in a taxonomy, focusing on the full hierarchical path rather than just direct relations. In the case of WordNet, multiple paths are considered for inferring semantic representations using compositional operators like recurrent neural networks (RNN). The embedding of a leaf node is represented as the sum of its parents' embeddings for simplicity and computational efficiency. The method focuses on learning hierarchical word embeddings by encoding hypernymy relations in a taxonomy, considering the full hierarchical path rather than just direct relations. It introduces a discounting term to assign weights for each hypernym word in the path, updating the objective function accordingly. The objective function learns word embeddings purely from the taxonomy, without considering contextual co-occurrences between hyponym and hypernym words. The method focuses on learning hierarchical word embeddings by encoding hypernymy relations in a taxonomy, considering the full hierarchical path rather than just direct relations. It introduces a discounting term to assign weights for each hypernym word in the path, updating the objective function accordingly. The objective function learns word embeddings purely from the taxonomy, without considering contextual co-occurrences between hyponym and hypernym words. To address this limitation, a co-occurrence matrix is created between hyponym and hypernym words within a context window in the corpus, using the Global Vector (GloVe) BID29 objective to consider their co-occurrence for learning embeddings. The proposed method focuses on learning hierarchical word embeddings by encoding hypernymy relations in a taxonomy. It introduces a joint linearly-weighted objective to minimize and evaluates the embeddings on various tasks. Experimental setup and comparison with prior works are discussed before conducting experiments on supervised hypernymy identification. The experiments focus on predicting graded lexical entailment and a newly proposed evaluation task called hierarchical path completion using WordNet. Top-level hypernyms are excluded in the path to avoid noise during learning, with words like 'entity' serving as the root for all nouns in WordNet. The study focused on extracting hierarchical paths from WordNet, limiting the number of words in each path to 5 hypernyms. A total of 256,442 relations were obtained, forming 59,908 distinct paths. The corpus used was ukWaC with 2 billion tokens. The context window was set to 10 tokens on both sides of the hyponym word. The study compared the Hierarchical Word Embeddings (HWE) with other word embedding models, including CBOW, SGNS, GloVe, Retrofitting, and JointReps. The text discusses various word embedding models such as CBOW, SGNS, GloVe, Retrofit, and JointReps, which utilize different approaches to learn word embeddings from text corpora and taxonomies. CBOW and SGNS focus on local co-occurrence, while GloVe considers global co-occurrences. Retrofit and JointReps combine taxonomy and text corpus for learning word embeddings. The text discusses word embedding models like CBOW, SGNS, GloVe, Retrofit, and JointReps. These models learn word embeddings from text corpora and taxonomies using different approaches. JointReps learns word embeddings post-processing, while HyperVec, Poincar\u00e9, and LEAR are hypernymy-specific embeddings. The same ukWaC corpus is used to train these models, with a context window of 10 words and words appearing less than 20 times removed. Negative sampling rate is set to 5 in SGNS, and embeddings from SGNS and CBOW are retrofitted into the Retrofit model. Hypernym relations from WordNet are used for Retrofit, JointReps, and HyperVec. In the experiments, various word embedding models like HyperVec, Poincar\u00e9, and LEAR were used with 300-dimensional word embeddings. The task involved binary classification to identify hypernymy relations between word pairs using pretrained embeddings and different approaches to combine them into a single feature vector. In the experiments, different word embedding models were used for binary classification to identify hypernymy relations between word pairs. The preferred operator for creating feature vectors was concatenation, and a binary SVM classifier was utilized with specific parameters set through validation. In the experiments, different word embedding models were used for binary classification to identify hypernymy relations between word pairs. The classifier memorizes prototypical hypernyms rather than learning the relation. Disjoint versions with no lexical overlap between test and train portions were adopted for benchmark datasets. Evaluation measures include F1 score and Area Under the ROC Curve (AUC) for unbalanced datasets. In the experiments, different word embedding models were used for binary classification to identify hypernymy relations between word pairs. Evaluation measures include F1 score and AUC. HWE reports the best scores in Levy and Baroni datasets, outperforming other methods significantly. This result is particularly noteworthy as the Baroni dataset is considered the most appropriate for evaluating hypernym identification. The Baroni dataset is deemed the most suitable for evaluating hypernym identification methods. LEAR performs best in Kotlerman and Bless datasets, while even models trained solely on a text corpus show good results in the Bless dataset. HWE and HyperVec also show strong performance. HWE embeddings can encode hierarchical structures from taxonomies, as demonstrated in the HyperLex BID40 dataset. The HyperLex BID40 dataset is used to test how well HWE embeddings capture graded lexical entailment. It consists of 2616 word pairs annotated with scores indicating the strength of lexical entailment relations. An asymmetric distance function is used to compute entailment scores between word pairs, taking into account vector norm and direction. HWE embeddings were evaluated using the HyperLex dataset, showing strong correlation with human ratings. HWE demonstrated hierarchical structure encoding, outperforming other models except for LEAR. LEAR, trained with larger resources, provided pre-trained embeddings. Future experiments aim to use the same resources and data with all models, including LEAR. The study evaluated HWE embeddings on the HyperLex dataset, showing strong correlation with human ratings and hierarchical structure encoding. However, it remains unclear how well the word embeddings encode hierarchical information. Previous experiments focused on hypernymy relation, ignoring taxonomic structure. A novel dataset was created to address this gap in evaluating hierarchical structure over a taxonomy. To address the gap in evaluating hierarchical structure over a taxonomy, a novel dataset was created by sampling paths from WordNet connecting hypernyms to hyponyms within a maximum path length. The paths include unigrams, bigrams, and trigrams with different frequency ranges, enabling a fair evaluation of hierarchical word embeddings. This dataset contains 330 paths categorized as unigram, bigram, or trigram paths. The dataset created from WordNet includes 150 unigram, 120 bigram, and 60 trigram paths. A hierarchical path completion task is proposed, where the goal is to predict a hyponym word given its hypernyms. Each hierarchical path in WordNet represents an average of 8 hyponym words. If a method predicts a candidate hyponym that belongs to any of the hyponyms with that path, it is considered correct. To solve a hierarchical path completion problem using WordNet data, prediction methods like ADD and SUB were used to predict hyponym words based on hypernyms. The ADD method computes cosine similarity between hypernyms and selects the word with the highest similarity as the prediction. The SUB method considers additional prediction methods like DISPLAYFORM0. The subtraction method (SUB) uses variants like b + c + d \u2212 e, which outperformed other variations with all embedding models. The choice of hypernyms in the subtraction method affects prediction accuracy, with closer hypernyms yielding better results. The direct hypernym method (DH) uses only the direct hypernym b for prediction, with similar cosine similarity computations as ADD and SUB. The preferred SUB variant b + c + d \u2212 e is used for the remaining experiments in the paper. The accuracy of prediction methods using word embeddings is measured as the percentage of correctly answered test instances. The proposed HWE outperforms all other word embedding learning methods on the hierarchical path completion dataset. Poincar\u00e9 embeddings show improved performance on hierarchical completion tasks compared to other methods like HyperVec and LEAR. HWE method reports a significant 28% accuracy improvement over Poincar\u00e9 using the ADD prediction method. This suggests that hierarchical word embeddings are effective for evaluating hierarchies and embeddings. The proposed HWE method outperforms other word embedding methods, except for Poincar\u00e9. Adding bigram and trigram hypernyms slightly decreases performance due to data sparseness. Qualitative analysis of HWE and Retrofit(CBOW) predictions shows promising results. The HWE method outperforms other word embedding methods, except for Poincar\u00e9. The hyponym column in TAB6 represents correct hyponym words. HWE accurately predicts correct words where Retrofit(CBOW) fails. Retrofit(CBOW) tends to predict the average nearest neighbor of a word's hypernyms as the missing word. Considering the hierarchical path reduces prediction ambiguity. For example, HWE correctly predicts \"hat\" in a path, while Retrofit(CBOW) incorrectly predicts \"dress\". The Retrofit(CBOW) model predicts words based on hierarchical paths, reducing ambiguity in predictions. It correctly predicted \"nature\" in a path related to publication, while HWE predicted \"textbook\". However, HWE struggled with certain words like \"voyage\" and \"cruise\", which Retrofit(CBOW) accurately completed. Retrofit(CBOW) may sometimes rely on synonyms like \"ghost\" and \"apparition\" without considering hierarchical information. The study analyzes the impact of hypernym words in hierarchical paths on prediction accuracy. By excluding different hypernyms in the path, the strongest effect on predicting the correct word is determined. Experiments show accuracy results on hierarchical path completion using the ADD method with various hypernym exclusions. Excluding the direct hypernym (b) has a notable impact on prediction accuracy. Excluding different hypernyms in the hierarchical path significantly affects prediction accuracy. The performance varies depending on which hypernym is excluded, with the farthest hypernym (e) showing the least impact. The dimensionality of the method also plays a role, with accuracy improving as the number of dimensions increases, reaching a peak of 88% accuracy with 200 dimensions. Adding more dimensions does not negatively affect performance. The study introduced a method for learning Hierarchical Word Embedding (HWE) to identify hypernymy relations between words using a joint objective that utilizes a taxonomy and a large text corpus. Evaluation on hypernymy identification tasks showed that HWE encoded hypernymy relations effectively, outperforming previous methods. The study introduced a method for learning Hierarchical Word Embedding (HWE) to identify hypernymy relations between words, outperforming previously proposed methods."
}