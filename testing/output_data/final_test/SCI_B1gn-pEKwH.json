{
    "title": "B1gn-pEKwH",
    "content": "The text discusses new methods for inferring models, predicting future symbols, and estimating the entropy rate of continuous-time, discrete-event processes using Bayesian structural inference and neural networks. These methods show promise in competing with current state-of-the-art methods for prediction and entropy rate estimation. The dynamic nature of scientific data provides a richer understanding of systems, but processing this data effectively remains a challenge. The text discusses the challenge of processing rich scientific data to gain new insights into complex systems, such as predicting earthquakes, understanding neural codes, and modeling organism behavior. These problems require a unified framework for inferring continuous-time, discrete-event models to make predictions and estimate outcomes. The paper presents a unified framework for inferring discrete-event models using the advantages of unifilarity and neural networks. It focuses on inferring unifilar hidden semi-Markov models from data using Bayesian information criterion. The paper introduces a unified framework for inferring discrete-event models by leveraging unifilarity and neural networks. It focuses on inferring unifilar hidden semi-Markov models using Bayesian information criterion, allowing for more powerful modeling compared to semi-Markov models. The inferred model can be used to estimate entropy rate and predict future input. New algorithms are competitive when the true model is within the same class. In Sec. 3, unifilar hidden semi-Markov models are introduced. Sec. 4 describes new algorithms for model inference, entropy rate estimation, and time series prediction tested on memoryful synthetic data. Sec. 5 discusses potential extensions and applications of the research. Various methods for studying discrete-time processes are mentioned, including autoregressive processes like AR-k and generalized linear models (GLM). The Baum-Welch algorithm and Bayesian structural inference have also been used in previous work. Inference methods like Bayesian structural inference and recurrent neural networks can be used to predict future symbols in hidden Markov models. Continuous-time, discrete-event predictors have received less attention compared to discrete-time predictors. Reservoir computers and recurrent neural networks can recreate the output of dynamical systems. In a new approach, continuous-time hidden Markov models are inferred to predict using the model's internal state as useful features from a sequence of symbols and durations. The data consists of symbols and their durations, such as seismic time series with magnitude and time between earthquakes. The last observed symbol has been seen for a certain duration, which may increase with more observation time. The possible symbols are assumed to be finite. The observed time series consists of symbols {x i } i from a finite set A and interevent intervals {\u03c4 i } i from (0, \u221e), assuming stationarity. A description of unifilar hidden semi-Markov models generating such a time series is provided. The minimal model consistent with observations is the -Machine, involving a finite-state machine with states g, dwell-time distribution \u03c6 g (\u03c4 ), emission probability p(x|g), and function + (g, x) determining the next hidden state. The observed time series involves symbols and interevent intervals from a finite set A. A hidden state g is randomly chosen, and a dwell time \u03c4 is selected based on the distribution \u03c6 g (\u03c4). An emission symbol is chosen according to the probability p(x|g), and the process repeats with a new hidden state determined by + (g, x). This model is unifilar, making tasks like model inference, entropy rate calculation, and future symbol prediction easier. A discrete-alphabet, continuous-time stochastic process where dwell times are drawn upon transitions between states and represented by symbols. A conveyer belt representation shows the time since the last symbol. An example time series is generated from the model with inverse Gaussian distributions. Unifilar hidden semi-Markov models can be parameterized with models, parameters, and density of dwell times defined by neural networks. The Bayesian framework for inferring the best-fit model and parameters in discrete-time unifilar hidden Markov models was described by Strelioff and Crutchfield. They calculated the posterior analytically using the unifilarity property to simplify the process. Approximations, such as the Bayesian inference criterion (BIC) by Bishop, were used due to the complexity of continuous-time calculations. In the Bayesian framework for inferring models in hidden Markov models, the use of the Bayesian inference criterion (BIC) by Bishop simplifies the process. The goal is to choose parameters that maximize the log likelihood and select a model that maximizes the likelihood while considering the number of parameters. The parameters of a model include the emission probability and transition distribution. In the Bayesian framework for inferring models in hidden Markov models, the use of the Bayesian inference criterion (BIC) by Bishop simplifies the process by choosing parameters that maximize the log likelihood and selecting a model that maximizes the likelihood while considering the number of parameters. The parameters of a model include the emission probability and transition distribution. The dwell-time distribution can be represented by an artificial neural network to enforce nonnegativity and normalization. The estimated density function from varying numbers of samples using an artificial neural network (ANN) with nonnegativity and normalization enforced. The inferred density function becomes closer to ground truth as the amount of data increases. Mean-squared error between estimated and true density compared for different estimation techniques, including ANN and k-nearest neighbors algorithm. Our new method, competitive with standard density estimation methods, utilizes ReLus as activation functions and nonnegative weights. The neural network's output, determined by the log likelihood cost function, can estimate interevent interval density function accurately with enough samples. Outside the data interval, the estimated density function may not be reliable. The neural network approach for density estimation, referred to as ANN, is a new method that interpolates well within the data interval but may not be reliable outside of it. This method is compared to k-nearest neighbor (kNN) and Parzen window estimation techniques. The neural network uses ReLus as activation functions and nonnegative weights to accurately estimate interevent interval density functions with enough samples. The new method for density estimation involves using a mixture of inverse Gaussians and training a properly normalized ANN. The BIC is used to infer the correct model, with a larger BIC indicating a higher posterior. A two-state model is deemed most likely with very little data. With little data, a two-state model is initially favored, but as data increases, a four-state model becomes more likely. The six-state model is never preferred. Unifilar hidden semi-Markov models allow for unique identification of dwell times to states. They can be used to calculate explicit formulae for the differential entropy rate, a measure of process randomness. Various algorithms have been developed to calculate entropy rates for complex processes. Estimating entropy rates from finite data is challenging. A better estimator involves calculating the slope of a graph or using the k-nearest-neighbor entropy estimator. Various algorithms exist for estimating entropy rates for complex processes. Estimating entropy rates from finite data is challenging. Two estimators are discussed: a model-free estimator using plug-in entropy estimators, and a model-based estimator where a model is inferred to calculate the estimate. The differential entropy rate is calculated using a plug-in estimator based on a formula, with the distribution over internal states solved using linear equations. The interevent interval density functions are estimated using a Parzen window estimate. The interevent interval density functions are estimated using a Parzen window estimate with a chosen smoothing parameter to maximize pseudolikelihood. A comparison is shown between the model-free method and the model-based method for estimating entropy rates. The model-based method uses the inferred model for estimation. The correct four-state model is used for the plug-in estimator, leading to a lower variance compared to the model-free method. Efficiently estimating excess entropy requires models of the time-reversed process, with future research focusing on retrodictive representations of unifilar hidden semi-Markov models. Based on the experiments, a predictive ANN method called PANN is developed using a feedforward neural network with six layers and 25 nodes. The network is trained to predict the emitted value x at a later time T based on a mean-squared error loss function. The method requires the neural network to guess the hidden state g from the observed data. The neural network must guess the hidden state g from observed data, with increases in n improving prediction accuracy. The second method, labeled as \"RNN,\" uses LSTM to predict x at time T with a mean-squared error loss function. The third method, labeled as \"uhsMm,\" preprocesses input data using an inferred unifilar hidden semi-Markov model for prediction. The optimal predictor for discrete-time applications has an explicit formula in terms of -M, while for continuous-time applications, a k-nearest neighbor estimate is used. Different entropy estimators are compared, showing that the model-based methods have lower bias compared to the model-free method with higher variance. The method uses a four-state model with lower bias. It compares different methods for predicting symbols at time T using varying amounts of data points and epochs. The uhsMm method infers the internal state of the model, while PANN and RNN methods use different input data for prediction. The method estimates x T by averaging future data points in the training set. The method uses a four-state model with lower bias to identify the correct model and output an optimal predictor through cross-validation. Synthetic datasets are generated with specific distributions to aid in implicit inference of hidden states. Experimental results show that the uhsMm method outperforms feedforward and recurrent neural networks in predicting symbols. Different network architectures, learning rates, and epochs are explored in the experiments. The study compared different network architectures, learning rates, and epochs to predict future symbols using a k-nearest neighbor estimate on causal states. A new algorithm for inferring causal states was introduced, along with a new estimator of entropy rate. The predictor based on causal states was found to be more accurate and less computationally intensive than other methods, showing potential for predicting complex processes like animal behavior. The study introduced a new algorithm for predicting complex processes like animal behavior by using a predictive model of continuous-time, discrete-event processes. Future research could focus on improving estimators of time series information measures and exploring more accurate methods for calculating MAP models."
}