{
    "title": "B1eHgu-Fim",
    "content": "Modern deep neural networks often have a large number of weights, making them challenging to deploy on devices with limited computational resources like mobile phones. To address this issue, one common approach is to use low-rank factorization to approximate weight matrices. However, using a small rank in standard low-rank factorization can reduce model expressiveness and performance. In this study, a mixture of multiple low-rank factorizations is proposed to model large weight matrices, with dynamically computed mixture coefficients based on input. The approach is shown to improve computation efficiency while maintaining or even outperforming accuracy compared to full-rank counterparts in tasks like language modeling and image classification. Model compression techniques, such as low-rank factorization, are used to reduce the size of neural networks for deployment on mobile devices with limited resources. By factorizing a large weight matrix into smaller matrices, the model can be efficiently run on general-purpose CPUs and GPUs without requiring specialized hardware support. However, using a small rank in low-rank factorization may limit model expressiveness and performance. The limitations of low-rank factorization in model compression are due to the conflict between rank d and model expressiveness, leading to a loss of information. To address this, an adaptive input-dependent factorization approach is proposed using a mixture of low-rank factorizations with computed mixing weights based on the input. This adaptive linear projection significantly improves performance with minimal additional cost. The proposed method introduces an unnormalized learned mixture of low-rank factorizations with adaptive mixing weights based on the input, adding a small amount of extra parameters and computation. The mixture function \u03c0 can be implemented efficiently, reducing to low-rank factorization if the coefficients are constant. The proposed method introduces adaptive mixing weights \u03c0(h) to enhance the expressiveness of the low-dimensional space. The mixing weights can be generated flexibly using non-linear transformations, such as sigmoid or hyperbolic tangent functions. Strategies to reduce parameters and computation in the mixing weights include pooling before projection. In the mixing weights \u03c0, strategies include pooling before projection and using random projection to reduce parameters in the linear projection of h. Adaptive mixing weights introduce a non-linear transformation for increased expressiveness in the low-dimensional space. Our method introduces a non-linear transformation into the high-to-low-dimensional projection by adaptively adjusting mixing weights for the linear bottleneck. It assigns weights into groups and dynamically controls them at the group level. This approach is applied to recurrent neural networks for language modeling using LSTM models on Penn Tree Bank and Text8 datasets. The study introduces adaptive mixing weights for linear bottleneck in LSTM models for language modeling on Penn Tree Bank and Text8 datasets. Three variants of the proposed model are tested against regular low-rank factorization, showing a reduction in FLOPs and improved performance. The study demonstrates the effectiveness of adaptive mixtures in improving performance compared to regular low-rank factorization. Pooling before projection is recommended for computing mixing weights, reducing computation and parameter size while capturing global information for better accuracy in compressing CNN models. The study shows that the proposed method using adaptive mixtures outperforms regular low-rank factorization in compact convolutional models like MobileNet. It achieves significantly better results with minimal additional computational cost."
}