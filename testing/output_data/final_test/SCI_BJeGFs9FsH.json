{
    "title": "BJeGFs9FsH",
    "content": "The trade-off between accuracy and adversarial robustness is not clearly understood. It is important to consider varying magnitudes of perturbations, use different methods to generate adversarially perturbed samples, and prioritize adversarial accuracies with different magnitudes for training a robust classifier. The Lexicographical Genuine Robustness (LGR) of classifiers combines accuracy and adversarial robustness. A candidate oracle classifier called \"Optimal Lexicographically Genuinely Robust Classifier (OLGRC)\" prioritizes accuracy on meaningful adversarially perturbed examples. The training algorithm for estimating OLGRC requires lexicographical optimization using Gradient Episodic Memory (GEM) for neural networks. Deep learning models have shown promising performances in image classification tasks but are vulnerable to mis-classifying imperceptibly perturbed images, known as adversarial examples. The vulnerability of classifiers to adversarial attacks can occur even when the attacks are applied before printing images, which can be read through a camera. Adversarial examples can be transferable to other models, enabling black-box attacks. Adversarially perturbed samples result from perturbation methods that increase adversarial loss, but may not always be classified as adversarial examples. Adversary generation methods aim to effectively increase adversarial loss. Adversary generation methods aim to increase adversarial loss using available information. Methods include FGSM, BIM, PGD, DAA, and Interval Attack. FGSM generates adversarial results using a linear classifier hypothesis. Classifiers can lead to adversarial susceptibility of models. The formula for generating strong attacks involves applying local linearization of the cost function and finding the optimal perturbation. Projected gradient descent (PGD) is used to generate adversarial results through iterative steps. Basic Iterative Method (BIM) follows similar steps as PGD but starts from a fixed point. Adversarial training aims to reduce vulnerability of classifiers by combining standard and adversarial loss functions. It uses a hyperparameter \u03b1 to balance the two losses, with \u03b1 = 0.5 commonly used. By considering both clean and adversarially perturbed samples, adversarial training improves accuracy on both types of data. Inner maximization involves generating adversarial attacks, while outer minimization focuses on reducing the adversarial loss of the model. Adversarial training aims to reduce vulnerability of classifiers by balancing standard and adversarial loss functions. Research shows trade-offs between accuracy on clean data and adversarial robustness. Classifiers trained with adversarial training may be more susceptible to invariance-based adversarial examples. The objective of this paper is to design and train a classifier that is more robust like a human than a model trained with standard adversarial training. The paper introduces Lexicographical Genuine Robustness, emphasizing human-like classification properties such as robustness against varying magnitudes of adversarial perturbations and prioritizing adversarial accuracies with smaller perturbation norms. The paper introduces Lexicographical Genuine Robustness (LGR) to improve classifier training by considering three properties. LGR helps prevent lower accuracy on clean data and pseudo adversarial examples. It also introduces the Optimal Lexicographically Genuinely Robust Classifier (OLGRC) and discusses the trade-off between accuracy on clean data and adversarial robustness. The paper discusses the trade-off between accuracy on clean data and adversarial robustness in classifier training. It argues that simply increasing adversarial robustness can lead to a classifier's behavior that differs from humans. The problem setting involves a clean input set where each sample belongs exclusively to one class, with a classifier assigning class labels based on input. This exclusive belonging assumption simplifies analysis but may not reflect real-world scenarios accurately. The paper discusses the trade-off between accuracy on clean data and adversarial robustness in classifier training. It defines adversarial examples as perturbed samples that deviate from clean samples by a maximum permutation norm. The analysis focuses on using l p norm for measuring distance, but the concept of adversarial examples is not limited to these metrics. The study considers the classification task and the potential application of ideas to adversarial examples based on relaxed metric versions. The analysis discusses adversarial examples in classifier training, focusing on using l p norm for measuring distance. It considers the classification task on the MNIST dataset and the impact of perturbations on classifier accuracy. The study highlights the importance of robustness in classifiers when faced with perturbations larger than a certain norm threshold. The discussion revolves around the need for a robust classifier when dealing with adversarial examples, particularly focusing on the l2 norm for measuring perturbations. It questions the classifier's ability to correctly classify images with perturbations and emphasizes the importance of robustness in classifying MNIST data. The analysis raises the question of whether we truly desire classifiers with the ability to understand the history of images and highlights the challenges in achieving such psychometric power. The discussion questions the need for robust classifiers in handling adversarial examples, focusing on the l2 norm for perturbations. It highlights the challenges in achieving classifiers with the ability to understand image history and emphasizes the importance of adversarial robustness. The discussion distinguishes between two types of class for clean and perturbed samples, emphasizing the need for l0 and l2 norm robustness in addition to l\u221e norm. De facto class refers to the current perceptual class, while de jure class represents the original class of the perturbed example. The discussion differentiates between de facto class (current perceptual class) and de jure class (original class of perturbed example). Adversarial examples can be pseudo (de facto class differs from de jure class) or genuine (de facto class is undefined). The classifier determines if a perturbed sample is an adversarial example. The classifier determines if a perturbed sample is an adversarial example, while the de facto class distinguishes between genuine and pseudo adversarial examples. The history of a sample determines if it is clean or perturbed. The de facto class of a sample determines if it is a pseudo adversarial example, regardless of its perturbation status. Classifier robustness against pseudo adversarial examples is not necessary, as humans only focus on the most likely class of a sample. The classifier aims to assign de jure classes even for pseudo adversarial examples, leading to perceptually wrong classifications. Increasing robustness against pseudo adversarial examples may decrease accuracy on clean data without improving human-like robustness. It is important to avoid increasing robustness against pseudo adversarial examples to achieve a model with human-like robustness. Comparing training tasks with only clean samples versus perturbed samples reveals the derivation process. Human classification is robust against adversarially perturbed samples generated from varying magnitudes of perturbations, not just fixed maximum norm. Clean samples have more information than perturbed samples, leading to a preference for using clean samples when training a model. This preference prioritizes natural accuracy over accuracy on perturbed samples and emphasizes adversarial accuracy on smaller perturbation norms. These properties contribute to human-like robustness in classification tasks. The current sample determines the classification result in human-like classification, avoiding assigning de jure class for pseudo adversarial examples. Human classifiers prioritize robustness for smaller perturbation norms over larger ones. The challenge lies in distinguishing between pseudo and genuine adversarial examples without full knowledge of data distribution. We introduce a method to estimate if a perturbed sample has the actual class, avoiding pseudo adversarial examples in training. Mathematical definitions of natural and adversarial accuracies are provided based on perturbation norms. The text discusses genuine adversarial accuracy based on maximum and exact perturbation norms, with a focus on avoiding pseudo adversarial examples in training. Genuine adversarial accuracies can be undefined depending on the dataset. Genuine adversarial accuracies can be undefined when specific conditions are met. Adversarial accuracy functions are defined for classifiers by measuring accuracies with varying perturbation norms. The genuine adversarial accuracy function uses a modified formula and focuses on the allowed regions of perturbed samples. The genuine adversarial accuracy function focuses on allowed regions of perturbed samples, using a modified formula that considers continuously increasing perturbation norms. It ignores already considered points for calculation of adversarial accuracy with smaller perturbation norms. The text discusses the change in adversarial accuracy for different classifiers based on perturbation norms. It uses a toy example with predefined classes to illustrate the concept. The text explores the impact of perturbation norms on adversarial accuracy for different classifiers, using a toy example with predefined classes. It shows how changing the predicted class for certain values of x affects the adversarial accuracy. The text discusses the differences in adversarial accuracy for different classifiers based on perturbation norms. It explains how changing the predicted class for specific values of x impacts the adversarial accuracy. The text discusses the calculation of genuine adversarial accuracies for different classifiers based on perturbation norms. It emphasizes the importance of considering specific points for accurate measurement of classifier robustness. The text introduces Lexicographical Genuine Robustness (LGR) as a total preorder based on adversarial accuracy functions, emphasizing the need for a human-like classifier. It also suggests a candidate oracle classifier for optimal performance. The text introduces Lexicographical Genuine Robustness (LGR) as a total preorder based on adversarial accuracy functions, emphasizing the need for a human-like classifier. It suggests a candidate oracle classifier called \"Optimal Lexicographically Genuinely Robust Classifier (OLGRC)\" and defines Lexicographical Standard Robustness (LSR) as a total preorder of classifiers based on adversarial robustness against varying magnitudes of perturbations. The text discusses the prioritization of robustness for smaller perturbation norms in classifiers to avoid loss of information in samples. It also explains the use of adversarial accuracy by exact perturbation norm rather than maximum perturbation norm. The text discusses the prioritization of robustness for smaller perturbation norms in classifiers to avoid information loss in samples. It explains the use of adversarial accuracy by exact perturbation norm and introduces Lexicographical Standard Robustness (LSR) to compare classifiers. LSR does not have an antisymmetric property, making it not a partial order. Lexicographical Genuine Robustness (LGR) is introduced to handle the second property incompletely addressed by Lexicographical Standard Robustness (LSR). It involves using genuine adversarial accuracy functions to compare classifiers based on perturbation norms. LGR determines if one classifier is genuinely more robust or equivalently robust compared to another. The perturbations needed to change predicted classification results involve positive and negative perturbations to switch between classes. The direction of perturbations explains the change in classes for different classifiers. The perturbations needed to change predicted classes involve positive and negative perturbations. For classifier f2, negative perturbation x \u2208 (\u22126, \u22121) changes class 1 to class \u22121, while positive perturbation x \u2208 (1, \u221e) or negative perturbation x \u2208 (\u2212\u221e, \u22122) changes class \u22121 to class 1. The direction of perturbations does not fully explain the change in de facto classes for f2 compared to f3. Considering Occam's razor principle, f3 is preferred over f2 due to similar standard adversarial robustness for x \u2264 4 and f2 being more complex with an additional decision boundary point. The Optimal Lexicographically Genuinely Robust Classifier (OLGRC) is defined as the maximal classifier based on Lexicographical Genuine Robustness (LGR). The Optimal Lexicographically Genuinely Robust Classifier (OLGRC) is determined by expanding explored regions and behaves similarly to the support vector machine (SVM) in maximizing distance to data points. Linear SVM is limited to linearly separable problems, while Kernel SVM maximizes distance based on feature space norms, making it potentially vulnerable to adversarial attacks. The Optimal Lexicographically Genuinely Robust Classifier (OLGRC) maximizes distance based on input set norms to enhance adversarial robustness. In a toy example, classifier f3 represents OLGRC. Loss functions l1, l2, etc., are used instead of lp norms. A discriminator is trained to differentiate between clean and adversarially perturbed samples to avoid pseudo adversarial examples in training. The discriminator in the training method plays a role similar to the discriminator in Generative Adversarial Nets. It assigns classes for different magnitudes of perturbations to avoid generating samples in the allowed perturbation region. Lexicographical optimization is used to prioritize avoiding generating samples in the perturbation region. Gradient Episodic Memory (GEM) is a method developed to prevent catastrophic forgetting in neural networks. It minimizes the loss for a new task without increasing losses for previous tasks. While lexicographical optimization was previously used to avoid catastrophic forgetting in continual learning, it is argued that it is also necessary for traditional multi-task learning and single task optimization. Lexicographical optimization is essential not only for traditional multi-task learning but also for single task learning. It prioritizes reducing the main loss over regularization terms to prevent over-fitting. This approach can be applied to progressively growing generative adversarial networks to ensure correct learning of simple structures first. Lexicographical Genuine Robustness (LGR) also requires this optimization due to considering multiple accuracies with preference. Lexicographical optimization prioritizes reducing losses with lexicographical preferences in a neural network. It aims to reduce multiple losses simultaneously without forgetting previous tasks. The approach involves parameter updates and projection to ensure lexicographical improvement. The \"Onestep method\" suggests applying a combined weights update for each lexicographical training step to reduce computational complexity. This approach involves using weighted parameter updates to achieve the same training effect. Additionally, considering adversarial robustness for different perturbations is not a new concept, as standard accuracy can be seen as adversarial accuracy with zero perturbation. Recently, research has focused on the model's robustness against multiple perturbation types and suggested adversarial training schemes. However, existing adversarial training methods did not consider the varying importance of adversarial accuracy with different magnitudes. Pseudo adversarial examples and the challenges of using them for training have also been studied. The concept of pseudo adversarial examples is similar to invariance-based adversarial examples, but with a different approach that is easier to implement. The definition of pseudo adversarial examples is similar to invalid adversarial examples. Some methods like MixTrain and TRADES aim to balance accuracy on clean data and perturbed samples by adjusting hyperparameters dynamically. In this study, the authors explore the importance of adversarial accuracies of different magnitudes and prioritize training methods for adversarial robustness. They compare 5 different training methods, including standard training, standard adversarial training, TRADES, OLSRC, and OLGRC.OLSRC and OLGRC refer to models trained with different methods for generating adversaries. The study compared different training methods for adversarial robustness, including standard training, standard adversarial training, TRADES, OLSRC, and OLGRC. OLGRC utilized the Onestep method with an adversary generation method to avoid perturbation in a specific region. PGD method was used to generate adversarially perturbed samples, and ADAM algorithm trained the discriminator. Lexicographical optimization in OLGRC required full batch training due to different weight updates, as mini-batch training could lead to catastrophic forgetting. In order to avoid weight update issues, full batch training was used in all experiments. Genuine adversarial accuracy changes were not plotted for toy example 2 and MNIST due to data distribution uncertainties. 100 training and test samples were randomly generated for the toy example, using a fully connected neural network with one hidden layer. The hidden layer with 256 neurons and leaky-ReLU non-linearity was used for experiments with full batch training and a learning rate of 0.015 for 1000 epochs. Gradient descent was used for weight updates. The first experiment focused on the effect of lexicographical optimization in adversarial training, using perturbation norm 4 for attacks and specific parameters for different methods. Standard training and OLGRC were not tested. Comparing training processes, standard adversarial training and TRADES are unstable due to lack of loss prioritization. OLSRC, prioritizing natural cross-entropy loss, shows more stability. Final classifiers show varying natural and adversarial accuracies, with TRADES achieving higher natural accuracy but fluctuating training accuracy. In the second experiment, exact perturbation norms were used for adversarial attacks on OLSRC and OLGRC. The perturbation process avoids already explored regions, moving samples in the right direction without errors. Estimated regions needing exploration are captured by p(x \u2208 X C |x). The trained OLGRC and its adversarial accuracy changes resemble the theoretical OLGRC. Using a discriminator, perturbed samples were generated to prevent catastrophic forgetting during mini-batch training. Only 2000 randomly sampled samples were used for training data. In mini-batch training, 2000 randomly sampled samples were used with a learning rate of 0.001 for 2000 epochs. The results may not be comparable due to the smaller training data. The common architecture used had two convolution layers and two fully connected layers. The ADAM algorithm was applied after projections for faster training. The Projected Gradient Descent method with 40 iterations was used for generating adversarial attacks with only l2 norm 4 attacks. Results from different training methods were compared using l2 norm attacks for training and testing. OLSRC and OLGRC showed better performance than standard adversarial training. However, OLSRC was not significantly more robust than OLGRC, even on trained data. The study compared different training methods using l2 norm attacks. OLSRC and OLGRC outperformed standard adversarial training, with OLSRC not significantly more robust than OLGRC. TRADES achieved the best natural accuracy, possibly due to different loss formulations. The study explained why existing methods struggle to achieve human-like robustness in classification. The study introduced the Optimal Lexicographically Genuinely Robust Classifier (OL-GRC) as a candidate oracle classifier for adversarial training. It suggested using a method to generate adversarially perturbed samples using a discriminator and proposed the use of Gradient Episodic Memory (GEM) for training. In the experiment, lexicographical optimization with GEM enabled stable training, even when other adversarial methods failed. Using a discriminator, adversarially perturbed samples were generated to train a classifier similar to the theoretical OL-GRC. Results on MNIST data showed that OLSRC and OLGRC outperformed standard adversarial training methods in natural and adversarial accuracy. In this work, the GEM method was applied to adversarial training, utilizing multiobjective optimization to train a single ensemble model for different datasets. This approach can also be used for efficient black-box attacks and to smoothen model interpretations. GEM with standard gradient descent is slow and requires adaptive gradient update algorithms for improved performance. Future work needs to address the limitation of GEM with mini-batch training in deep learning applications. The assumption of exclusive belonging in finding a human-like classifier needs to be analyzed when violated. One approach is to ease the lexicographical preference to account for accuracy less than 1. Another method involves estimating the original data by adding noise to the current data. Our model aims to find a classifier that is robust against various forms of adversarial attacks, including shift, rotation, and spatial transformation. To achieve this, a combined metric needs to be defined. The adversarial accuracy of the classifier is shown to not satisfy a specific property for certain perturbation norms. This highlights the need for a more comprehensive approach to adversarial robustness. The text discusses the definition of a Negative Adversarial Remover (NAR) and the interpretation of a classifier's decision boundary. It introduces the concept of a neutral boundary (NB) and emphasizes the importance of considering prior probabilities in adversarial accuracy functions. The Negative Adversarial Remover (NAR) and Nearest Decision Boundary Point (NDBP) are defined for a classifier f, a sample x, and a class c. NAR can be an interpretation of x as it perturbs a point on the decision boundary to x. NDBP is similar to a baseline in interpretation methods but is dependent on x. If f is derived from a differentiable function g, DeepFool or FAB-attack can estimate NAR. The Negative Adversarial Remover (NAR) and Nearest Decision Boundary Point (NDBP) are defined for a classifier f, a sample x, and a class c. NAR can be estimated using algorithm or Fast Adaptive Boundary (FAB)-attack when c = f (x), while Boundary attack or HopSkipJumpAttack can be used when only f is available."
}