{
    "title": "rJ1RPJWAW",
    "content": "This paper examines the simplicity of learned neural networks in various settings, such as real vs random data, different sizes/architectures, and large vs small minibatch sizes. The focus is on learnability, which refers to how accurately a neural network's prediction function can be learned from labeled samples. The study suggests a strong correlation between small generalization errors and high learnability. This paper explores the differences between shallow and deep neural networks, aiming to understand the reasons behind the success of neural networks in various tasks like image classification and machine translation. The study highlights the need for a better understanding of neural network performance and success factors, which are currently major open questions in the field. This paper delves into the performance and success factors of neural networks, focusing on the challenges of optimization and generalization. It explores how neural networks can achieve zero training error on random labels despite being highly overparametrized. Neural networks can achieve zero training error on random labels, indicating poor generalization. However, they can still obtain good test accuracy on real data even with massive label noise. This raises the question of whether neural networks learn simple patterns on random training data. Increasing the size of a neural network can help improve test error. Increasing the size of a neural network can lead to better generalization error, contrary to traditional beliefs in statistical learning theory. Larger networks are found to learn simpler patterns and achieve better test accuracy, even with massive label noise. Additionally, using large minibatches for stochastic gradient descent can result in poor generalization. This paper explores the impact of minibatch sizes on neural network generalization, focusing on the sharpness of minimizers and learnability. The study connects sharpness of minimizers to generalization performance but lacks a clear quantification of sharpness. The approach taken in this paper is complementary, looking at the questions through the lens of learnability. The learnability of a neural network N is defined as the ease of learning N from data, which is closely related to PAC learnability. Learnability has been well studied theoretically in the context of neural networks. This paper empirically investigates the learnability of neural networks of varying sizes and architectures, as well as minibatch sizes, on real and random data. The results suggest a strong correlation between generalizability and learnability, indicating that neural networks that generalize well are more learnable. Experiments show that neural networks do not learn simple patterns on random data, larger networks achieving higher accuracies are more learnable, and networks trained with small minibatch sizes are also more learnable. The paper discusses the learnability of neural networks based on varying sizes, architectures, and minibatch sizes. It highlights the correlation between generalizability and learnability, showing that networks achieving higher accuracies are more learnable. The organization of the paper includes sections on related work, experimental setup and results, and conclusions with future directions. The paper discusses the learnability of neural networks, showing that there exist families of single hidden layer neural networks that are hard to learn for statistical query algorithms. The results hold for log-concave distributions and a wide class of activation functions. Additionally, recent work has shown that the learnability of neural networks is close to 1 under certain assumptions. Recently, BID6 introduced an efficient algorithm for learning one hidden layer neural networks with sigmoids. Their approach differs from practical methods, and the output hypothesis is not in neural network form. Training one neural net with another has been used in practice for distillation, where a large network is first trained accurately, and then a smaller network is trained using the large network's classification probabilities. This research differs from our goal. Our experiments and results were conducted on CIFAR-10 dataset. Experiments were conducted on CIFAR-10 BID17 dataset with 60,000 training examples divided into three subsets. Vanilla stochastic gradient descent was used with specific parameters for learning rate adjustment and training duration. The network N1 was trained in the experimental setup. The experimental setup involves training network N1 on labeled data D1, using N1 to predict labels for unlabeled data D2, and then training another network N2 on the combined data. Learnability of a network is calculated and experimental results are presented to address specific questions. Three types of data are used: true data from CIFAR-10, random labels, and random images. The experiment involved training networks N1 and N2 with different architectures, including VGG, GoogleNet, ResNet, PreActResnet, DPN, and DenseNet. Shallow convolutional networks with varying numbers of filters were also tested. Learnability values for true data, random labels, and random images were compared across different networks. The complexity of learned neural networks heavily depends on training data, impacting generalizability. Results show higher learnability for networks trained on random data compared to true data, especially for shallow networks. Class imbalance affects output skewness, with deeper networks closer to 10% learnability. The learnability of shallow networks is slightly higher than deeper networks, with class imbalance impacting output skewness. Results show that networks trained on true data make simpler predictions even on misclassified examples. The experiments focus on the impact of network size and architecture on the learnability of neural networks. Results show that larger networks are simpler and have higher learnability. Test accuracy increases with network size, suggesting that deeper networks learn information not present in the data. The experiments analyzed the impact of network size and architecture on learnability. Results indicated that larger networks are simpler and exhibit higher learnability. Test accuracy rises with network size, implying that deeper networks learn additional information not present in the data. The experiment results are presented in TAB1, showing lower accuracies due to reduced data size. Positive correlation between test accuracy and learnability was observed, except for specific cases. The effect of minibatch size on the learned model was also explored in a separate set of experiments. In a separate set of experiments, the impact of minibatch size on learnability was analyzed. Results showed that increasing minibatch size led to reduced learnability, indicating that larger minibatches result in more complex neural networks. The study also delves into whether neural networks learned with different initializations converge to the same functions. The study explores if different SGD solutions between neural networks with different initializations correspond to the same function. Confusion matrices show that while the solutions are not identical, they agree on a common subset of examples. VGG-11 exhibits behavior where off-diagonal entries are close to test accuracy, similar to other popular architectures. The study examines if different SGD solutions in neural networks with various initializations correspond to the same function. Figures 1 and 2 display histograms of distinct predictions for shallow network and VGG-11. The correlation between learnability and generalizability of neural networks is evident in the experimental results. The study explores the relationship between learnability and generalizability in neural networks trained with SGD. Initial random networks generalize well and are simple, but as SGD reduces training error, networks become more complex and generalization error increases. Results show a correlation between low generalization error and high learnability. Shallow and deep networks exhibit qualitative differences in learnability. The paper discusses qualitative differences between shallow and deep neural networks, raising questions about optimization algorithms, hyperparameters, and initialization schemes' impact on learnability. Characterizing efficiently learnable networks via backprop could improve generalization by driving networks to converge to learnable states."
}