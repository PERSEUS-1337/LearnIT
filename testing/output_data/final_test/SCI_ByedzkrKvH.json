{
    "title": "ByedzkrKvH",
    "content": "Counterfactual regret minimization (CFR) is a technique for solving Imperfect Information Games (IIG). The original CFR algorithm is limited to discrete states and action spaces, hindering its application to large games. A double neural representation is proposed in this paper, with one network for cumulative regret and another for average strategy. This allows for end-to-end optimization without manual game abstraction. Novel techniques like robust sampling and mini-batch Monte Carlo CFR are also developed. Empirically, neural strategies trained with this algorithm show promise on games suitable for tabular approaches. Neural strategies trained with the proposed algorithm show strong performance on large games, outperforming deep reinforcement learning. In head-to-head matches of texas hold'em, the neural agent beats a strong opponent by $9.8\\pm4.1$ chips per game, showcasing the success of neural CFR in large games. Solving imperfect information games remains a challenge, with players having partial knowledge about opponents. Imperfect Information Games (IIGs) offer realistic modeling for real-world applications. Nash equilibrium is a key concept for two-player perfect-recall IIGs, with CFR being an effective approach to converge strategies to a Nash equilibrium. Original CFR works for discrete states and action spaces. To address the limitations of CFR in large games, one can simplify the game by grouping similar states together and using tabular CFR. Function approximation, such as Regression CFR (RCFR), can replace tabular representation, but full traversals of the game tree make it impractical for large games. DeepStack proposes using fully connected neural networks to tackle this challenge. DeepStack uses fully connected neural networks for players' counterfactual values, while Jin et al. (2017) apply deep reinforcement learning for regret minimization in single-agent settings. Heinrich et al. (2015) and Heinrich & Silver (2016) introduce eXtensive-form Fictitious Play (XFP) and Neural Fictitious Self-Play (NFSP) for learning approximate Nash equilibrium in end-to-end manner. NFSP updates neural strategies by selecting best responses to opponents' average strategies, allowing continuous strategy improvement without abstracting the game. In the paper, a double neural counterfactual regret minimization (DNCFR) algorithm is proposed to address the question of whether a purely neural-based approach can achieve performance comparable to tabular-based CFR methods. A novel recurrent neural network with attention is used to model imperfect information games, and a new sampling technique is developed to improve convergence efficiency. A set of ablation studies is conducted to evaluate the effectiveness of each novelty introduced in the experiment. The experiments conducted on DNCFR showed convergence to results comparable to its tabular counterpart, outperforming NFSP. DNCFR was also tested on HUNL, achieving strong neural strategy with few parameters and beating ABS-CFR. The notation and definitions for histories, actions, and player functions were provided in the experiment. The CFR Algorithm is a strategy profile for players in imperfect information games, where utility functions define payoffs. It involves assigning action distributions to information sets and iterating through strategies for convergence. The CFR Algorithm involves assigning action distributions to information sets in imperfect information games. For iterative methods like CFR, the reach probability of a history is denoted by \u03c0 \u03c3 (h) and can be approximated using double deep neural networks in DNCFR to solve large games efficiently. The Counterfactual Regret Minimization (CFR) is an iterative method for finding a Nash equilibrium in zero-sum perfect-recall imperfect information games. It involves updating strategy profiles based on counterfactual values and regrets at information sets. The cumulative strategy is adjusted over iterations to approach a Nash equilibrium. More details can be found in the appendices. The Counterfactual Regret Minimization (CFR) strategy after iterations approaches a Nash equilibrium. Monte Carlo CFR (MCCFR) computes unbiased estimations by sampling subsets of infosets, requiring less working memory than standard CFR. MCCFR maintains values for visited nodes and estimates sampled counterfactual values. The Double neural CFR algorithm uses two neural networks for cumulative regret and average strategy estimation. The RegretSumNetwork computes the current strategy based on the regret sum. The Double neural CFR algorithm utilizes two neural networks for cumulative regret and average strategy estimation. The current strategy is computed from the cumulative regret, and a neural network is designed for this purpose. The approximate Nash equilibrium is calculated using the weighted average of previous behavior strategies. The Double neural CFR algorithm uses two neural networks for cumulative regret and average strategy estimation. The deep sequential neural network representation for infosets involves modeling action sequences with recurrent neural networks. Different variants of RNNs like GRU and LSTM can be used for this purpose. In later experiments, different neural architectures including GRU and LSTM will be compared with a fully connected network representation. An attention mechanism is added to the RNN architecture to enhance decision making based on the importance of information in the sequence. The attention mechanism helps DNCFR achieve better convergence rates. In fashion interleaving with CFR iterations, samples are stored for all players in each iteration using robust sampling and mini-batch MCCFR methods. The cumulative regret neural network is optimized using a loss function to update parameters. The design of the loss function minimizes bias by considering all players' samples, not just a specific player. Techniques can be applied to reduce variance in the estimation process. In practice, minimizing loss based on observed samples can help achieve a converged strategy. Concerns about forgetting values for unobserved infosets due to mini-batch training are addressed by using neural network parameters from the previous iteration as initialization for online learning. In practice, minimizing loss based on observed samples can help achieve a converged strategy. Concerns about forgetting values for unobserved infosets due to mini-batch training are addressed by using neural network parameters from the previous iteration as initialization for online learning. The double neural approach can converge to an approximate Nash equilibrium even with a small proportion of infosets used for updates. See Appendix F for implementation details. The loss function and cumulative regret are key components in generating behavior strategy for stable training. To optimize the average strategy network, we can save values within multiple iterations into memory. Using a uniform reservoir sampling method can provide unbiased estimations of each strategy. This approach helps address memory-expensive limitations of maximum likelihood optimization for the average strategy. In practice, the maximum likelihood method for unbiased estimation of strategies has high variance and struggles to reach a less exploitable Nash equilibrium. Minimizing squared loss leads to a fast convergent average strategy profile with lower memory usage. Unlike prior methods like Libratus and DeepStack, DNCFR is not limited by abstracted cards or actions, allowing for the use of continuous variables to represent bet money. Double Neural Counterfactual Regret Minimization (DNCFR) can clone existing representations and continually improve strategies. Cumulative regret and strategy can be learned using behavior cloning techniques. Warm starting DNCFR from tabular methods allows for improvement beyond the initial strategy profile. If no warm start, parameters are randomly initialized in RSN and ASN. The algorithm for Double Neural Counterfactual Regret Minimization (DNCFR) involves randomly initializing parameters in RSN and ASN, then using sampling methods to obtain infosets and values. These samples are used by the NeuralAgent algorithm to optimize RSN and ASN. A robust sampling method (RS) is introduced to improve efficiency, sampling k actions in one player's infosets and one action in the other player's infosets. The robust sampling method involves defining a sampled profile by selecting k actions according to a sampled strategy. It is efficient and shows comparable convergence with external sampling, using less memory. The paper introduces a mini-batch Monte Carlo technique for sampling in MCCFR algorithms, which randomly samples blocks in each iteration. This technique provides an unbiased estimation of CFV and can be extended to a mini-batch MCCFR+ algorithm by incorporating regret matching plus. The mini-batch MCCFR+ algorithm samples blocks in parallel and converges faster than original MCCFR on multi-core machines. Ablation studies are conducted to understand the contributions of various components in the DNCFR algorithm. DNCFR is compared with tabular CFR and NFSP, a deep reinforcement learning method. Experiments on HUNL show the scalability of DNCFR algorithm. Ablation studies are performed on Leduc Hold'em poker. In experiments, DNCFR is tested on Leduc Hold'em instances with different stack sizes. A neural agent is developed to solve HUNL, a challenging benchmark in IIGs. Exploitability is used as a standard win rate measure, with lower exploitability indicating better performance. Nash equilibrium has an exploitability of zero. Head-to-head performance is used in extremely large games to measure differences. The implementation details of the neural agent for solving HUNL are provided in Algorithms 2, 3, and 4, along with parameters used in experiments. Developing a HUNL solver is challenging due to lack of open source codes and high variance. Validation approaches for the implementation are discussed in Appendix G, including ablation studies on mini-batch training and neural architecture choice on Leduc Hold'em. The experimental results in the context of solving HUNL show that larger batch sizes lead to better strategy profiles. The proposed robust sampling method outperforms outcome sampling, with increasing k resulting in even better convergence rates. The proposed robust sampling method in experiments for Leduc Hold'em shows that k = 3 and 5 perform similarly to k = max(|A(I i )|), requiring less memory than external sampling. LSTM with attention achieves slightly better performance, leading to its selection as the default architecture. Neural networks demonstrate generalization by using fewer parameters compared to tabular counterparts, with LSTM plus attention networks containing 1048 and 2608 parameters. In experiments for Leduc Hold'em, LSTM with attention outperforms external sampling with k = 3 and 5, requiring less memory. The LSTM plus attention networks contain 1048 and 2608 parameters, demonstrating generalization ability with fewer parameters compared to tabular counterparts. DNCFR converges with small mini-batch sizes, showing exploitability less than 0.1 within 1000 iterations. The neural network can estimate values for unseen infosets with fewer parameters compared to tabular methods. DNCFRs slightly outperform tabular MCCFR due to generalization. Ablation study on RSN and ASN networks show slight improvements over DNCFR. Continual improvement from warm starting leads to better performance. Starting from either full-width or sampling based CFR can lead to continual improvements. The first 10 iterations are learned by tabular based CFR and RS-MCCFR+, with DNCFR improving the remaining iterations. DNCFR is tested on large games like Leduc(10) and Leduc(15), showing convergence even with a small proportion of nodes sampled. Comparisons are made with NFSP, a leading function approximation method for solving IIG, which requires large memory sizes. The experiment compares DNCFR with NFSP and XFP in terms of convergence and memory usage. DNCFR achieves better performance with fewer nodes touched. It also shows a significant improvement over reinforcement learning methods. Additionally, the comparison of runtime and memory usage between DNCFR and tabular CFR on Leduc(15) demonstrates that DNCFR requires significantly less memory. DNCFR achieves a better trade-off between relative runtime and memory compared to tabular CFR on Leduc(15). It shows that for every one-fold increase in relative runtime, DNCFR may lead to a five-fold decrease in relative memory consumption. This demonstrates a significant improvement in memory usage over traditional methods. DNCFR achieves a better trade-off between relative runtime and memory compared to tabular CFR on Leduc. It shows a fivefold decrease in relative memory consumption for every one-fold increase in relative runtime. The research on neural CFR is deemed important for future work, with potential for improvement through acceleration techniques and distributed computation. To test the scalability of the DNCFR on large games like HUNL, a neural agent was developed. Solving HUNL directly is challenging, even with abstraction techniques like ABS-CFR using k-means clustering. The agent has about 2 \u00d7 10 10 infosets and requires 80GB memory for strategies. Training ABS-CFR needs over 200GB memory for cumulative regrets and other variables. To solve HUNL using deep learning, ideas from DeepStack and Libratus were combined to train flop and turn networks for predicting counterfactual values. The flop and turn networks estimate values after dealing public cards. Blueprint strategies in our settings are learned by DNCFR with value networks. Testing DNCFR on HUNL(1) shows convergence, with over 2\u00d710^8 infosets and 3\u00d710^11 states. Figure 7 (a) illustrates DNCFR convergence on different embedding sizes. The convergence of DNCFR on different embedding sizes is shown in Figure 7 (a), ranging from emd=8 to 128. Larger neural networks with more parameters tend to achieve better performance by representing complex patterns. The neural agent is compared against its tabular version and ABS-CFR on HUNL, a strong agent requiring 80GB memory for storing strategies. ABS-CFR utilizes card and action abstraction techniques. The text discusses using DNCFR to learn blueprint strategies on HUNL(2), a game with a large number of infosets. Real-time strategy computation is done using continual resolving techniques. When variance reduction techniques are applied, the neural agent outperforms ABS-CFR in terms of chips per game. The curr_chunk discusses the efficiency of neural networks in solving imperfect information games (IIGs) compared to traditional methods like tabular agents and ABS-CFR. It introduces Neural Fictitious Self-Play (NFSP) as a leading method in this area but highlights its slower convergence compared to CFR-based approaches. The text also mentions Regression CFR (RCFR) as a function approximation method based on CFR but notes its limitations in traversing the full game tree in large games. The curr_chunk discusses the differences between various deep learning approaches in solving imperfect information games, focusing on DNCFR using LSTM with attention for representing extensive-form games. DNCFR updates cumulative regret based on current iteration samples, uses squared-loss for average strategies, divides regret by \u221a T, and collects data with unbiased mini-batch sampling. Experimental evaluations include ablation studies in various settings. In our method, we propose a novel double neural counterfactual regret minimization approach for large imperfect-information games. Evaluations show that the techniques used are of independent interest and successful in applying deep learning to these games. This opens up a promising direction for future work. In Leduc Hold'em, a two-player IIG poker game introduced in 2012, players are dealt one card from a deck of 6 cards with two suits of three ranks. Players can pass, bet, call, or fold, with different outcomes based on card values. There are no limits on betting amounts or raises, and the game consists of two rounds. In Leduc Hold'em, players are dealt one card from a 6-card deck in two rounds. The game has no limit on raises or bets. In Heads-Up No-Limit Texas Hold'em, there are four betting rounds - preflop, flop, turn, and river. Players start with 20000 chips each in ACPC. The small blind puts in 50 chips, the big blind 100 chips, followed by the first round of betting. If no player folds in the preflop round, three public cards are revealed for the flop. In Heads-Up No-Limit Texas Hold'em, players go through four betting rounds - preflop, flop, turn, and river. Players can fold, call, or bet, with the bet amount ranging from one big blind to the number of chips they have left. The game follows an extensive-form structure with defined components and player functions assigned to each non-terminal history. In Heads-Up No-Limit Texas Hold'em, players go through four betting rounds - preflop, flop, turn, and river. Players can fold, call, or bet with varying amounts. The game follows an extensive-form structure with player functions assigned to each non-terminal history. Player actions are based on information partitions and utility functions, aiming to achieve maximum payoff in a Nash equilibrium strategy profile. In games, a Nash equilibrium is a strategy profile where each player's strategy is a best response to the opponent. An \u03b5-Nash equilibrium is an approximation of a Nash equilibrium with unexploitable strategies. In large two-player zero-sum games like poker, computing the value of a strategy is difficult. However, if players alternate positions, the value of the games is zero. The exploitability of a strategy profile \u03c3 is defined as the sum of the exploitabilities of each player's strategy. Figure 1 illustrates a partial game tree in One-Card Poker with different nodes and terminal nodes. In an extensive-form game, nodes represent histories of actions. Each player's private information is captured in infosets, grouping undistinguished states. For example, h7 and h8 are indistinguishable for player 0. Similarly, h1 and h2, as well as h3 and h5, h4 and h6, are in the same infosets. Players can only remember information observed, including hidden variables and public actions. In an extensive-form game, nodes represent histories of actions. Players' private information is captured in infosets, grouping undistinguished states. For example, h7 and h8 are indistinguishable for player 0. Players can only remember information observed, including hidden variables and public actions. A strategy profile \u03c3 is a collection of strategies for all players, where each player has a set of possible strategies. \u03c3 \u2212i refers to strategies of all players except player i. In iterative methods like CFR, \u03c3 t refers to the strategy profile at the t-th iteration. The state reach probability of history h is denoted by \u03c0 \u03c3 (h) if players follow the strategy \u03c3. The reach probability in an extensive-form game can be decomposed into infoset reach probabilities. Robust sampling with a larger batch size shows better performance, leading to faster convergence. This method is efficient for parallel sampling on distributed systems. Choosing a suitable mini-batch size based on computation and memory constraints is crucial. Comparing robust sampling with Average Strategy (AS) sampling on Leduc game demonstrates the benefits of larger batch sizes. The proposed robust sampling method outperforms Average Strategy (AS) sampling on Leduc Hold'em with a stack size of 5. Using a mini-batch size of b=100 and k=2 in robust sampling, after 1000 iterations, the exploitability of robust sampling is lower than AS. Robust sampling samples a minimum of player i's actions while AS samples a random number of player i's actions, leading to higher variance in AS. Parameters for AS are within specific ranges according to Gibson (2014). The parameter scopes of AS are defined by Gibson (2014) as \u2208(0,1], \u03c4 \u2208[1,\u221e), \u03b2 \u2208[0,\u221e). Bayes' Theorem can be used to infer the posterior probability of opponent's private cards. Robust sampling, outcome sampling, and external sampling methods are discussed for online regret minimization. Robust sampling is equivalent to external sampling when k = max. Robust sampling is more flexible and memory-efficient than external sampling, especially for large games where one player must take all actions in her infosets. By specifying a suitable k, we can achieve a similar convergence rate to external sampling. Additionally, robust sampling converges more efficiently than outcome sampling in certain cases. In our experiment, we use robust sampling as the default policy when k=1, which converges more efficiently than outcome sampling. Mini-Batch MCCFR provides an unbiased estimation of counterfactual value. We represent infosets in extensive-form games using R and S networks, modeled as a recurrent neural network. The recurrent cell structure includes LSTM with a gating mechanism for better performance. Schmidhuber (1997) introduced LSTM with a gating mechanism for learning long-term dependencies. An attention mechanism is added to enhance the LSTM architecture, considering different positions in a sequence. For example, in a poker game, the importance of information may vary based on revealed public cards. Each action is represented by a LSTM cell, capable of modifying the cell state. The LSTM cell has three gates to modify cell state: forgetting gate, input gate, and output gate. Attention weights are learned for each cell, and the final output is predicted by a value network with defined parameters. The proposed RSN and ASN share the same neural architecture, using different parameters. Algorithm 2 outlines the double neural counterfactual regret minimization method, starting with tabular-based methods and utilizing techniques in Section 3.4 for initialization. Sampling methods like robust sampling are used to collect training samples for RSN and ASN. The proposed mini-batch robust sampling MCCFR algorithm utilizes samples saved in memories M t R and M t S for optimizing RSN and ASN using the NeuralAgent algorithm. The implementation allows for parallel collection of training samples on multi-processors or distributed systems, ensuring unbiased estimation. The comparison between DNCFR and tabular CFR experiments is conducted on a single processor. The optimization of the Deep Neural Network involves parameters like learning rate, loss criteria, and iteration bounds. The optimization technique for both RSN and ASN involves optimizing neural networks using the Adam optimizer with momentum and adaptive learning rate techniques. Other optimizers like Nadam and RMSprop were tested but did not yield better results. The optimization technique for neural networks involves using a novel scheduler to reduce learning rate when loss stops decreasing. Gradient clipping is used to limit parameter gradient magnitude. Best parameters are updated after each epoch, and early stopping is implemented based on a specified criteria. The feature encoding for observed actions in poker games involves using vectors to represent private and public cards, current actions, and betting chips. Private and public cards are encoded using a one-hot technique, while betting chips are represented by normalized cumulative spent. In HUNL, cards are encoded with vectors for ranking and suit. Public sequence actions are represented by one-hot encoding. The proposed mini-batch robust sampling method in poker games involves using one-hot encoding for private and public cards, normalized cumulative spent for betting chips, and robust sampling for actions. Algorithm 4 outlines the application scenario, where the function MCCFR-NN traverses the game tree starting from the root. Player i samples k actions based on robust sampling, updating the strategy using cumulative regret predicted by RSN. The mini-batch sampling method in poker games involves updating strategy profiles and storing counterfactual regrets. The function Mini-Batch-MCCFR-NN uses parallel sampling to achieve unbiased estimation of CFV. There is a trade-off between unbiased estimates, convergence, and data efficiency, with stochastically-weighted averaging as a feasible solution. In poker games, the Mini-Batch-MCCFR-NN method updates strategy profiles and stores counterfactual regrets using parallel sampling. Stochastically-weighted averaging (SWA) is a solution for unbiased estimation of CFV, but it leads to high variance. External sampling (ES) solves this variance issue by updating the average strategy for one player only. In neural CFR, collecting samples for the average strategy at the other player is inefficient. Empirically, it was found that not using SWA leads to more efficient convergence. Experimental hyperparameters were set for Leduc Hold'em with different numbers of infosets. In poker games, the Mini-Batch-MCCFR-NN method updates strategy profiles and stores counterfactual regrets using parallel sampling. The default parameters for the provable robust sampling method are set at k = 3 for games with different numbers of infosets. For Leduc(5), b=100 is selected as the default parameter, sampling only 5.59% infosets in each iteration. For larger games like Leduc(10) and Leduc(15), default b=500 is chosen, visiting only 2.39% and 0.53% infosets in each iteration. The default embedding size for RSN and ASN neural networks is set at 16, 32, and 64 for Leduc(5), Leduc(10), and Leduc(15) respectively. Adam optimizer and LSTM with attention are the default choices for neural architecture. The neural networks have 2608, 7424, and 23360 parameters respectively. The neural agent uses Adam optimizer with a default learning rate of 0.001. A scheduler adjusts the learning rate based on epochs and loss convergence, reducing it by 0.5 after 10 epochs without improvement. The learning rate has a lower bound of 10^-6 to prevent convergence to local minima. The best parameters are saved as \u03b8 T best after T epochs, with early stopping if the loss is below a specified criteria. The optimizer is updated for a maximum of 2000 epochs. In the experiment, hyperparameters were set for different algorithms such as ASN and NFSP. NFSP utilized a neural network with specific architecture and learning rates. The exploration in \u03b5-greedy policies started at 0.06 and decayed over time. For HUNL, a small percentage of infosets were sampled in each iteration to solve different scenarios. In the experiment, hyperparameters were set for different algorithms such as ASN and NFSP. NFSP utilized a neural network with specific architecture and learning rates. The exploration in \u03b5-greedy policies started at 0.06 and decayed over time. For HUNL, a small percentage of infosets were sampled in each iteration to solve different scenarios. To solve HUNL(1) and HUNL(2), 0.01% and 0.001% infosets were sampled, with a batch size of 100000 for training the neural network. Larger batch sizes were preferred to reduce the number of gradient descent updates. DNCFR was performed under different embedding sizes and gradient descent update steps, with results compared to Go and analyzed for computational complexity. The proposed method was evaluated by re-implementing DeepStack. In this section, the implementation of DeepStack in Heads-up No-Limit Texas Hold'em is discussed. The ABS-CFR agent, an enhanced version of HITSZ_LMW_2pn, is introduced with 2\u00d710 10 information sets. The agent abstracts the full game into smaller abstract games using action and card abstractions. The implementation of DeepStack in Heads-up No-Limit Texas Hold'em involves action and card abstractions. Action abstraction uses a discretized betting model, while card abstraction collapses strategically similar states. Different abstractions are used in preflop, flop, turn, and river stages to simplify the game. The algorithm was implemented based on the original article since the source code was not released by Alberta University. The released example code for Leduc Hold'em cannot be directly used for Heads-up No-Limit Texas Hold'em due to differences in game structure and complexity. The implementation of DeepStack in HUNL requires a large-scale distributed computing cluster and specific acceleration techniques. The example code lacks necessary algorithms for Texas Hold'em, while our implementation follows key ideas from the original DeepStack article. The implementation of DeepStack in HUNL requires a large-scale distributed computing cluster and specific acceleration techniques. To optimize the counterfactual value network on different subgames, millions of samples are generated, costing significant computational resources and time. The overall DeepStack algorithm involves computing strategies for various game rounds and extensive code implementation. The DeepStack algorithm involves computing strategies for the current public state, using depth-limited Lookahead with a counterfactual value network, and action abstraction to reduce the game tree size. Exploitability is used to evaluate strategy in imperfect information games, but it is computationally expensive in large games like Heads-Up No-Limit Texas Hold'em. Verification of the implementation was done by analyzing hand histories against professional players. Our implementation of DeepStack was verified using hand histories against professional players, achieving strategies close to the original DeepStack and a huber loss similar to the original paper. Additionally, our agent outperformed an enhanced version of HITSZ_LMW_2pn, winning 120 mbb/g."
}