{
    "title": "SJgMHcrs3E",
    "content": "Catastrophic forgetting in neural networks is a well-known issue in continual learning. A new meta-learning algorithm is proposed in this paper to address this problem by reconstructing gradients of old tasks and combining them with current gradients to enable continual learning and backward transfer learning. Experimental results demonstrate the effectiveness of the algorithm in preventing catastrophic forgetting and supporting backward transfer learning. The importance of continual learning in artificial general intelligence (AGI) is highlighted, with a focus on addressing catastrophic forgetting in artificial neural networks (ANNs). Various algorithms like Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapse (MAS) aim to prevent important parameters from deviating from their old values to avoid catastrophic forgetting. These algorithms compute the importance of each parameter for each task in the sequence and add a regularization term to prevent forgetting. The regularization term in EWC-like algorithms controls the importance of each task's parameters. It aims to prevent changes to important parameters to avoid catastrophic forgetting. However, this approach does not support backward transfer to leverage information from previous tasks for improved performance. The regularization term in EWC-like algorithms controls the importance of each task's parameters to prevent catastrophic forgetting. However, these algorithms are not scalable to a large number of tasks due to the linear growth of old parameter vectors. To address these limitations, a meta learning algorithm is proposed that approximates task gradients from current parameters and combines them with old task gradients to improve network performance on all tasks. Our algorithm combines task gradients to enable backward transfer learning, reducing memory requirements. The learning to learn algorithm for continual learning is introduced, with experiments in section 3 and conclusions in sections 4 and 5. Consider a continual learning problem with a learner f(x; \u03b8) and T loss functions. In joint learning settings, data from all tasks is available to the learner for parameter updates. In continual learning settings, the learner updates parameter \u03b8 using gradients from all tasks. At task t+1, the learner cannot access data from previous tasks and updates \u03b8 based on the current task's gradient only. This can lead to improved performance on the current task but may damage performance on previous tasks. To address this issue, a meta learning algorithm is proposed to reconstruct gradients for previous tasks using a meta-network. The meta-learning algorithm proposes using a meta-network h (i) to reconstruct gradients for previous tasks without accessing their data. This approach prevents catastrophic forgetting and enables backward transfer learning, even when using optimizers other than SGD. Training h would require a large number of samples due to the high dimensionality of neural networks. The meta-learning algorithm uses a neural network h to process high-dimensional inputs and outputs independently. The network is trained to minimize the Euclidean distance between its output and the gradients of the parameters. Different variations of h are possible, such as processing the position of parameters or values of \u03b8. Updating f with gradients of opposite signs can improve the network's performance. When updating the network with gradients of different signs, it can improve performance on one task while damaging performance on another. If the gradients have the same sign, performance on both tasks can improve. The update vector \u03b4 is created by applying a rule to pairs of gradients, resulting in a sparse vector for large t. Updating the network with \u03b4 improves performance on the current task and likely on previous tasks as well, transferring common information between them. Our algorithm utilizes \u03b4 for backward transfer learning, preventing catastrophic forgetting on old tasks and even improving performance. Forward transfer learning is also observed, with later tasks starting with higher accuracy. Our algorithm for continual learning prevents catastrophic forgetting and supports backward transfer learning. Task boundaries are automatically detected using the loss of the meta-networks, which is more effective than looking at the loss of the main network."
}