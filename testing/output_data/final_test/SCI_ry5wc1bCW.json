{
    "title": "ry5wc1bCW",
    "content": "CGNN is a framework for learning functional causal models using generative neural networks. It utilizes backpropagation to minimize the maximum mean discrepancy to observed data, incorporating conditional independences and distributional asymmetries for discovering bivariate and multivariate relationships. CGNN is a framework for learning causal structures and generative models of data. It competes well with state-of-the-art alternatives in causal discovery tasks, such as cause-effect inference and v-structure identification. Deep learning models excel in predictive abilities but lack explanatory power, leading to potential mistakes in causation versus correlation. The generative process underlying the variables X1, X2, and Y is described by equations with additive noise variables. The asymmetric relations between the variables show that X1 causes Y, and Y causes X2. However, X2 provides a stronger signal-to-noise ratio for predicting Y, leading to a least-squares solution of Y = 0.25X1 + 0.5X2. This explanation is incorrect as X2 does not cause the computation of Y. Observational causal discovery is crucial for understanding how manipulating variables affects outcomes. Experiments are the gold standard for discovering causal relations, but in some cases, observational causal discovery is necessary due to cost, ethics, or feasibility constraints. The literature on observational causal discovery lacks a unified solution. The literature on causal discovery lacks a unified solution, with some approaches using distributional asymmetries to find bivariate causal relations, while others rely on conditional independence for structures on three or more variables. Different algorithms and causal graphs are used to model generative processes. Observational causal discovery is essential for understanding variable manipulation effects. Intervening in a Functional Causal Model (FCM) allows for manipulation of equations to obtain new distributions. Understanding interventions requires knowledge of the causal structure, which is the focus of this work. Conditional independence and v-structures are key concepts in causal discovery from data. The causal structure of a random vector is determined by variables and their relationships. The DAG skeleton is created by replacing directed edges with undirected ones. Causal inference relies on assumptions like causal sufficiency, causal Markov, and causal faithfulness. Markov equivalence class refers to graphs with the same d-separations. When using causal faithfulness and conditional independence, we can recover the Markov equivalence class of the causal structure from a random vector. This class includes DAGs with undirected edges. Score-based methods are used to learn FCMs from data, estimating the causal DAG and mechanisms. Score-based methods use a score-function to measure the fit between a candidate set and observed data, selecting the DAG with the maximum score. The Bayesian Information Criterion penalizes model complexity, and edges in the graph are associated with importance scores. However, enumerating all possible DAGs is not feasible due to the super-exponential number of combinations. In this paper, the focus is on learning FCMs from data using generative neural networks. The proposed framework involves estimating causal graphs and mechanisms from known graph skeletons, reducing causal discovery to selecting edge orientations. The goal is to tackle the intractability of brute-force search for the best DAG over d nodes. The estimated noise variables \u00ca = (\u00ca 1 , . . . ,\u00ca d ) are sampled from a fixed distribution Q. Observational samples D are drawn from the estimated FCM and parametrized as generative neural networks. A score-function is proposed to measure the fit between a candidate structure \u011c and data D using the Maximum Mean Discrepancy statistic. This statistic scores a graph \u011c by measuring the discrepancy between the data observational distribution P and the estimated observational distribution P. CGNN implements Occam's razor to prefer simpler models as causal, leveraging distributional asymmetries and conditional independences to score both bivariate and multivariate graphs. It uses a differentiable kernel like the Gaussian kernel for backpropagation training. CGNN is a trainable directed acyclic graph of conditional generator networks used for searching causal graphs. A greedy approach is proposed to orient the graph and remove cycles. The algorithm assumes causal sufficiency but can be adapted to address hidden confounders. In a variant of the algorithm, the presence of confounders is addressed by extending each equation in the FCM to account for possible unobserved variables. The performance of CGNN in discovering causal structures is evaluated by considering different types of causal relationships. The performance of CGNN is evaluated in discovering various causal structures, including cause-effect relations, v-structures, and multivariate causal structures with or without hidden variables. Experiments are conducted on an Intel Xeon CPU and NVIDIA 1080Ti GPU. CGNN uses one hidden-layer neural networks with ReLU units and is trained with the Adam optimizer. Training involves using all data to combat overfitting, and the best results are obtained when using the entire data as a minibatch. CGNN is trained for 1000 epochs and evaluated on 500 generated samples, with training ensemble over 32 random initializations. The number of hidden units (n h) in CGNN is a crucial hyperparameter that should be cross-validated for each application. It determines the model's flexibility in capturing causal mechanisms, with a balance needed to avoid missing patterns or over-complicating explanations. Occam's razor principle is illustrated in a comparison of bivariate CGNNs with different complexities, showing that a smaller n h (2 in this case) can explain the data effectively. The most discriminative value for n h = 2 is highlighted in an illustrative case. Under the causal sufficiency assumption, CGNN evaluates the performance to determine causal relations X \u2192 Y or X \u2190 Y using observational data. Five cause-effect inference datasets are used, covering a wide range of associations. The dataset includes 300 artificial cause-effect pairs with random linear and polynomial causal mechanisms, simulating additive or multiplicative noise. CE-Tueb contains 99 real-world cause-effect pairs from various domains. The CGNN algorithm is compared to several other causal inference algorithms. The Conditional Distribution Similarity statistic, CDS BID6, and Jarfo BID6 method are used for causal direction preference. A random forest classifier trained on ChaLearn Cause-effect pairs is utilized with 150 features extracted. Hyperparameters are selected using a leave-one-dataset-out scheme, with specific values chosen for different datasets. The code for various methods is available at a specific GitHub repository. For ANM, Gaussian kernel bandwidth \u03b3 is searched in {0.01, 0.1, 0.2, 0.5, 0.8, 1, 1.2, 1.5, 2, 5, 10}. LiNGAM and IGCI have no parameters. PNL searches for independence test significance level \u03b1 in {0.0005, 0.005, 0.01, 0.025, 0.04, 0.05, 0.06, 0.075, 0.1, 0.25, 0.5}. GPI uses default parameters. CDS looks for best discretization of cause variable into {1, . . . , 10} levels. Jarfo trains random forest with 4,000 cause-effect pairs. TAB1 shows AUPRC for binary classification of \"X \u2192 Y \" or \"X \u2190 Y \", computational time, and complexity for methods. The computational time and complexity of different methods are compared. Linear regression performs poorly, while CDS and IGCI show good performance on some datasets. ANM works well with additive noise assumption but not otherwise. PNL, a generalization of ANM, performs favorably. Jarfo performs well on artificial data but poorly on real examples. Generative methods like GPI and CGNN show good performance, but are computationally expensive. CGNN's approximation of MMD with random features does not degrade performance. The performance of CGNN is competitive compared to the state-of-the-art on cause-effect inference, identifying causal structures of three random variables (A, B, C) with skeleton A \u2212 B \u2212 C. The chain, reverse chain, and reverse v-structure are indistinguishable using certain methods. The goal is to use CGNN to determine if P (A, B, C) follows an FCM with causal graph: A \u2192 B \u2190 C. A causal discovery method must reason about conditional independences between the three random variables. CGNN assigns the lowest MMD to the v-structure. The CGNN assigns the lowest MMD to the v-structure hypothesis on datasets generated by v-structures. It leverages distributional asymmetries and conditional independences. Experiments provide algorithms with the true graph skeleton to compare their ability to orient edges fairly. 500 samples are drawn from artificial causal graphs for orientation tasks. The study compares CGNN to other algorithms like PC, GES, ANM, LiNGAM, and Jarfo on artificial causal graphs with polynomial mechanisms and noise. PC-Gaussian and PC-HSIC use different conditional independence tests. GES uses a penalization parameter of \u03bb = 3.11. CGNN sets n h to 20 for experiments. The study compares PC, GES, CGNN, ANM, LiNGAM, and Jarfo algorithms on artificial causal graphs. PC-HSIC performs best for denser graphs due to its voting rule, while CGNN can orient all edges and provide a generative model. CGNN achieved an AUPRC of 85.5 \u00b1 4 on 5 graphs with 100 variables in 30 hours. In real applications, some variables may be hidden in causal graphs. The goal is to orient edges based on direct causal relations and remove those due to confounding. CGNN is compared to the RFCI algorithm, which accounts for hidden variables. Different variants of RFCI using Gaussian or HSIC tests are evaluated, along with the data-driven method Jarfo. CGNN is evaluated for its performance in classifying relations and handling hidden confounders. It outperforms RFCI-HSIC on certain graphs and is the only approach providing a generative model of the data. The model learns functional causal models based on generative neural networks by minimizing the discrepancy between generated samples and observed data. The CGNN approach learns functional causal models using generative neural networks with shallow neural networks. It allows for simulating interventions on variables and evaluating their impact. Future work includes reducing computational costs, handling categorical data, improving causal graph search heuristics, and adapting methods for temporal data. The Maximum Mean Discrepancy (MMD) statistic measures the distance between two probability distributions using kernel mean embeddings. In practical applications, the empirical MMD is calculated from sets of samples. The empirical MMD statistic is calculated using a characteristic kernel, such as the Gaussian kernel. The MMD tends to zero as n \u2192 \u221e if and only if P = P, making it a good measure of how close observational distributions are. Computationally, evaluating MMD takes O(n^2) time, which is impractical for large n. Shift-invariant kernels, like the Gaussian kernel, can be used with Bochner's theorem. Learning FCMs from data is a vast field, with bivariate and multivariate algorithms available. Bivariate and multivariate algorithms for orienting cause-effect relations between random variables include the Additive Noise Model (ANM), Post Non-Linear (PNL) model, IGCI method, LiNGAM method, and CURE method. These algorithms use different approaches such as nonlinear regression, monotone functions, independent component analysis, and cause distribution independence to determine causal directions."
}