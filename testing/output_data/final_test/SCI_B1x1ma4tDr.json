{
    "title": "B1x1ma4tDr",
    "content": "Most generative models of audio focus on generating samples in either the time or frequency domain, but these approaches are inefficient as they do not leverage existing knowledge of sound generation and perception. A new approach called Differentiable Digital Signal Processing (DDSP) library integrates classic signal processing elements with deep learning methods for high-fidelity audio synthesis without the need for large autoregressive models or adversarial losses. DDSP allows for the utilization of strong inductive biases while maintaining the expressive power of neural networks. DDSP enables an interpretable and modular approach to generative modeling, allowing manipulation of separate model components for various applications like pitch and loudness control, extrapolation to unseen pitches, dereverberation, room acoustics transfer, and timbre transformation. The library is available at https://github.com/magenta/ddsp for contributions from the community and domain experts. Neural networks rely on structural priors like convolution, recurrence, and self-attention for practical success. End-to-end learning depends on these constraints for scalability, limited to differentiable functions. The Differentiable Digital Signal Processing (DDSP) library expands this toolbox by integrating signal processing elements into TensorFlow for increased interpretability and applicability. The text discusses the potential of neural synthesis models in audio synthesis, highlighting the importance of phase-coherent oscillation in human hearing. Current neural synthesis models do not fully utilize this periodic structure for generation and perception. Neural synthesis models in audio generation struggle with aligning waveforms due to varying frequencies and phase variations, impacting the quality of generated waveforms. Fourier Transform (STFT) represents wave packets over windows and deals with spectral leakage. Autoregressive waveform models like WaveNet, SampleRNN, and WaveRNN generate waveforms one sample at a time, avoiding issues with Fourier basis frequencies. However, they require larger networks and suffer from exposure bias during generation. Autoregressive waveform models like WaveNet, SampleRNN, and WaveRNN suffer from exposure bias during generation and are incompatible with perceptual losses such as spectral features and pretrained models. Instead of predicting waveforms or Fourier coefficients, vocoders or synthesizers directly generate audio with oscillators, using expert knowledge and hand-tuned heuristics. In this paper, the limitations of using neural networks for modeling synthesis parameters are discussed. The DDSP library is introduced to implement fully differentiable synthesizers and audio effects, overcoming the need for hand-tuning analysis parameters and allowing for end-to-end learning. This approach improves the realism of vocoders by enhancing the expressivity of analysis/synthesis pairs. Models using DDSP components can generate high-fidelity audio without autoregressive or adversarial losses. These models offer independent control over pitch and loudness, realistic extrapolation to unseen pitches, dereverberation of audio, transfer of room acoustics to new environments, timbre transfer between sources, and smaller network sizes compared to other neural synthesizers. Audio samples are available for reference. Vocoders come in various types: source-filter/subtractive models mimic the human vocal tract, while sinusoidal/additive models create sound using time-varying sine waves. Additive models are more expressive but have more parameters. This work introduces a differentiable synthesizer based on the Harmonic plus Noise model, combining harmonic ratios of a fundamental frequency with filtered noise. Recent research has focused on estimating parameters for synthesizers using gradient-free methods. A variational autoencoder was used to model synthesizer outputs as a \"world model\" for approximate gradients during learning. DDSP is a toolkit of differentiable DSP components for end-to-end learning, different from black-box approaches. The Neural Source Filter (NSF) is a specific DDSP model that uses convolutional waveshaping for harmonic content generation. Promising speech synthesis results were achieved with a differentiable waveshaping synthesizer. Both works generate audio in the time domain and impose multi-scale spectrograms losses. The curr_chunk discusses the implementation of differentiable DSP components, such as oscillators, envelopes, and filters, using automatic differentiation software. A key contribution is the release of a modular library that allows for easy mixing and matching of components. An example model, DDSP, implements a differentiable version of Spectral Modeling Synthesis (SMS) for sound generation. The curr_chunk discusses the use of Spectral Modeling Synthesis (SMS) for sound generation, which combines additive and subtractive synthesis. SMS is known for its expressive capabilities and has been widely used in various audio tasks. The Harmonic plus Noise model is used for monophonic sources in the experiments. SMS is more expressive due to its numerous parameters, making it a versatile tool for audio processing. The synthesis techniques explored in this paper involve using coefficients with more dimensions than the audio waveform itself, making them suitable for neural network control. The core of the technique is the sinusoidal oscillator, which outputs a signal over discrete time steps. The oscillators have time-varying amplitudes and phases, with harmonic frequencies being integer multiples of a fundamental frequency. The harmonic oscillator output is determined by the time-varying fundamental frequency and harmonic amplitudes. The amplitudes are factorized into a global amplitude for loudness control and a normalized distribution for spectral variations. Amplitudes and harmonic distribution components are constrained to be positive. Bilinear interpolation is used for instantaneous frequency upsampling in neural networks operating at a slower frame rate. Frequency upsampling in neural networks can be achieved using bilinear interpolation. To smooth amplitudes and harmonic distributions in additive synthesis, a smoothed amplitude envelope with overlapping Hamming windows is applied. A 4ms hop size and 8ms frame size with 50% overlap effectively remove artifacts. Linear filter design is crucial in DSP, with convolutional layers equivalent to LTI-FIR filters. To maintain interpretability and prevent phase distortion, the frequency sampling method is used to convert network outputs into impulse responses of linear-phase filters. A neural network is designed to predict the frequency-domain transfer functions of a FIR filter for each output frame. The neural network predicts frequency-domain transfer functions of FIR filters for each output frame. Time-varying FIR filters are implemented by dividing audio into non-overlapping frames and performing frame-wise convolution in the Fourier domain. The hop size for conditioning frames is 256 for 64000 samples and 250 frames. The filtered audio is obtained by overlap-adding frames with rectangular windows. The neural network predicts frequency-domain transfer functions of FIR filters for each output frame with a hop size of 256. A window function is applied to compute the filter, defaulting to a Hann window of size 257 for time-frequency resolution control. The Harmonic plus Noise model combines additive synthesis with filtered noise to create natural sounds. In 2007, a differentiable filtered noise synthesizer was created using an LTV-FIR filter applied to uniform noise. Room reverbation is explicitly modeled by factorizing room acoustics into a post-synthesis convolution step. Reverb is implemented by performing convolution in the frequency domain, scaling as O(n log n) for large kernel sizes. The supervised and unsupervised DDSP autoencoder variants are tested on NSynth and solo violin datasets. Components of the neural network architecture are color-coded, with some components not used in all experiments. DDSP components do not restrict the choice of generative model. In this study, the focus is on a deterministic autoencoder to evaluate the effectiveness of DDSP components in improving autoencoder performance in the audio domain. The use of convolutional layers in autoencoders has shown better results in image processing, and similarly, DDSP components enhance autoencoder performance. The potential benefits of introducing stochastic latents in models like GAN, VAE, and Flow are left for future exploration, as the current focus is on investigating the performance of DDSP components. Our architecture uses DDSP components and a decomposed latent representation to reconstruct input audio. Encoders extract loudness, fundamental frequency, and a time-varying latent encoding. MFCC coefficients are transformed into 16 latent variables per frame. The unsupervised autoencoder replaces the CREPE model with a Resnet architecture for extracting fundamental frequency. The Resnet architecture extracts features from audio spectrograms and is jointly trained with the network. The decoder maps these features to control parameters for synthesizing audio, with a focus on DDSP components for quality. The latent representation is fed directly to the synthesizer for structural meaning. The disentangled representation in DDSP models allows for interpolation and extrapolation outside the data distribution. DDSP models have fewer parameters compared to other models like GANSynth and WaveNet Autoencoder. Initial experiments with smaller models show promising results for low-latency applications. The NSynth dataset consists of 70,379 audio examples of strings, brass, woodwinds, and mallets with pitch labels within MIDI range 24-84. The dataset is split into 80/20 train/test sets for experiments with supervised and unsupervised variants. Additionally, 13 minutes of expressive, solo violin performances from the MusOpen library were collected to focus on performance aspects not captured by NSynth. The solo violin experiments used a consistent room environment to focus on performance. Audio was converted to mono 16kHz and split into 4-second training examples. A reverb module was added to account for room reverberation, using a fixed variable for the impulse response. The primary goal was to minimize reconstruction loss in the autoencoder. In the experiments, a multi-scale spectral loss was used to compare original and synthesized audio waveforms at different resolutions using FFT sizes ranging from 2048 to 64. The total reconstruction loss was calculated as the sum of all spectral losses. The DDSP autoencoder efficiently resynthesizes audio with a simple model and L1 spectrogram loss, outperforming large autoregressive models. It demonstrates the ability to exploit DSP bias while maintaining neural network expressive power. The quality of DDSP resynthesis is compared quantitatively with WaveRNN on the NSynth dataset. The supervised DDSP autoencoder outperforms the WaveRNN baseline in loudness and fundamental frequency metrics. Despite using the same data and conditioning, the DDSP model excels, especially in F0 L1. By incorporating a perceptual loss with a pretrained CREPE network, the unsupervised DDSP model improves optimization and generates sounds accurately, although not as precise as the supervised version. The supervised DDSP model outperforms the WaveRNN model in generating sounds with correct frequencies without supervision. Interpolation allows for independent control over generative factors, with each component of latent variables altering samples along perceptual axes. Conditioning independently controls characteristics of the audio, with loudness and pitch explicitly controlled by latent variables. The model uses residual z(t) to encode timbre, demonstrating smooth timbre changes by varying z. It can extrapolate to new conditions not seen during training, as shown in Figure 7 with a solo violin clip shifted down an octave. The audio remains coherent and resembles a cello. The modular approach to generative modeling allows for complete separation of source audio from room effects. By bypassing the reverb module during resynthesis, the audio becomes completely dereverberated, similar to recording in an anechoic chamber. The quality of this approach is limited by the underlying generative model, which is high for the autoencoder used in the study. The study demonstrates the ability to transfer acoustic environments and timbre using a DDSP autoencoder. By adjusting fundamental frequency and transferring room acoustics, the singing voice is transformed into a violin sound. The resulting audio captures the subtleties of the singing with the timbre of the violin dataset. The DDSP library combines classical DSP with deep learning to preserve expressive power while leveraging strong inductive biases. Contributions from domain experts are encouraged to expand the library's applications. Figure 5 shows the decomposition of a solo violin clip, highlighting the extraction of loudness and fundamental frequency signals for prediction by the DDSP autoencoder. The Additive Synthesizer component uses a neural network to predict amplitudes, harmonic distributions, and noise magnitudes for audio synthesis. The synthesizer generates audio by summing sinusoids at harmonic multiples of the fundamental frequency, with the network controlling parameters like frequency and amplitude. The extracted impulse response is then applied to combine audio from synthesizers for full resynthesis. The Additive Synthesizer component utilizes a neural network to predict amplitudes, harmonic distributions, and noise magnitudes for audio synthesis. It involves three encoders: f-encoder for fundamental frequency, l-encoder for loudness, and z-encoder for residual vector. The f-encoder uses a pretrained CREPE pitch detector to extract ground truth fundamental frequencies, while the l-encoder extracts loudness using similar computational steps. The z-encoder in Table 2 calculates MFCCs from audio using a log-mel-spectrogram with specific parameters. The MFCCs are normalized and passed through a GRU and linear layer to obtain z(t) with 16 dimensions across 250 time-steps. The model architecture for the f(t) encoder uses a Resnet on log mel spectrograms with 16 dimensions across 250 time-steps. Spectrograms have specific parameters and are upsampled for the final output, which is a probability distribution over 128 frequency values. The decoder's input is a latent tuple (f(t), l(t), z(t)) over 250 timesteps, and its outputs are parameters for synthesizers like harmonic and filtered noise setups. The decoder in the model architecture outputs amplitudes for harmonics and transfer functions for synthesizers like harmonic and filtered noise setups. It uses a shared-bottom architecture with separate MLPs for each input, concatenated outputs passed through a GRU, and final MLP and Linear layer for decoder outputs. The MLP architecture includes layer normalization before RELU nonlinearity. The model architecture includes a shared-bottom design with separate MLPs for each input, utilizing layer normalization before RELU nonlinearity. The model is differentiable end-to-end, allowing for training with any SGD optimizer. Additional perceptual loss is incorporated using pretrained models like CREPE pitch estimator and WaveNet autoencoder encoder. The best results were achieved by using the L1 distance between activations of the small CREPE model's fifth max pool layer. The small CREPE model's fifth max pool layer is compared using L1 distance with a weighting relative to spectral loss. Harmonic synthesizer parameters are adjusted for amplitude and distribution, with fixed initial phases. Non-negative constraints are enforced using a sigmoid nonlinearity on network outputs for stability during training. Improvement in training stability by modifying the sigmoid function, larger slope by exponentiating, and threshold at a minimum value for filtered noise synthesizer. Comparison of model parameters for different models, with autoregressive models having the most parameters and GANs requiring less. DDSP models require 2 to 3 times less parameters than GANSynth, with unsupervised model having more parameters due to the CREPE encoder. Initial experiments with small models show slightly less realistic outputs but still relatively high quality. Evaluation details include loudness L1 distance and F0 L1 distance for model performance assessment. Pitch tracking using CREPE may have errors such as sudden octave jumps at low volumes. F0 outliers account for these imperfections, with samples below a confidence threshold labeled as outliers. Lower outlier scores indicate better model performance as the number of outliers decrease with better quality audio. In reconstruction tasks, the model is supplied with standard inputs for loudness and F0. Different interpolation tasks involve using different inputs as ground truth for calculating Loudness (L1) and F0 (L1). While harmonic synthesizers are currently used, the technique is not limited to this and can handle perturbations like inharmonicity. The modular structure of synthesizers allows for the incorporation of perturbations like inharmonicity to handle phenomena such as stiff strings. Additional losses can be defined based on different synthesizer outputs and parameters, such as imposing an SNR loss to penalize noisy outputs. These engineered losses can potentially improve training efficiency, although they deviate from the end-to-end training paradigm."
}