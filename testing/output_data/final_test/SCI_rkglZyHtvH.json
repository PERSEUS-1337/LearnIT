{
    "title": "rkglZyHtvH",
    "content": "Variational inference (VI) is a popular approach for approximate Bayesian inference, especially for complex models like deep neural networks. The method proposed in this work trains flexible variational distributions by iteratively refining a coarse approximation. This refinement process involves making cheap, local adjustments and optimizing simple variational families. The method consistently outperforms recent variational inference methods for deep learning in terms of log-likelihood and the Evidence Lower BOund (ELBO) in experiments. The gains in uncertainty estimation are further amplified on larger scale models, significantly outperforming standard VI and deep ensembles on residual networks on CIFAR10. Uncertainty is crucial in various machine learning applications, from weather prediction to drug discovery, to avoid potentially poor outcomes in domains like medical diagnosis or autonomous vehicles. Deep neural networks often struggle to accurately quantify model uncertainty, being overconfident in their predictions even when incorrect. Bayesian inference provides a principled approach to capturing uncertainty by marginalizing over a posterior distribution. Variational inference approximates the true posterior with a simpler distribution, such as the mean-field approximation for neural networks. This method reduces inference to an optimization problem, minimizing the discrepancy between the true posterior and the variational posterior. This paper introduces a novel method for training flexible posterior approximations by iteratively refining a coarse mean-field approximation using additive auxiliary variables. The model parameters are expressed using auxiliary variables that leave the marginal distribution unchanged, and the refinement process involves sampling values of these auxiliary variables to update the variational approximation. The method involves iteratively refining a mean-field approximation using auxiliary variables to update the variational posterior distribution. Through local adjustments, samples are drawn to approximate the true posterior distribution, resulting in improved ELBO without significant computational burden. The method involves iteratively refining a mean-field approximation using auxiliary variables to update the variational posterior distribution. It guarantees an improvement in ELBO without adding significant computational overhead. A novel algorithm increases the flexibility of the variational distribution, improving its quality under mild conditions. The effectiveness of the method is demonstrated on Bayesian neural networks, achieving a new state-of-the-art in uncertainty estimation using variational inference at ResNet scale on CIFAR10. Bayesian neural networks use variational inference to approximate the true posterior distribution of the weights. The method optimizes the Evidence Lower Bound (ELBO) to minimize the Kullback-Leibler divergence between the approximate and true posteriors. Variational inference optimizes the Evidence Lower Bound (ELBO) to approximate the posterior distribution of weights in Bayesian neural networks. The ELBO is a lower bound to the log marginal likelihood, and maximizing it is equivalent to minimizing the KL divergence. The main issue with variational inference is the inflexibility of the posterior approximation, often approximated with independent Gaussians in mean-field variational inference. The posterior approximation in Bayesian neural networks is often too simplistic, using independent Gaussians across all dimensions. To address this, a refinement procedure is proposed to train a detailed posterior in regions used for prediction while relying on a coarse-grained approximation elsewhere. This approach involves augmenting the graphical model with auxiliary variables to ensure conditional independence and not affecting the prior distribution. This method distinguishes itself from hierarchical variational models. The model in Bayesian neural networks uses auxiliary variables to aid inference, ensuring the prior distribution remains unchanged. Iteratively refining the approximate posterior involves sampling auxiliary variables and approximating the posterior based on these values. This iterative process improves the posterior approximation for better predictions. In Bayesian neural networks, iterative refinement of the approximate posterior involves sampling auxiliary variables to improve the posterior approximation for better predictions. Using a toy example with a single weight and a complicated posterior with four modes, a Gaussian approximation fails to capture the true multimodal nature of the posterior. Sampling is done using q \u03c6 to initialize the refinement process. The iterative refinement of the approximate posterior in Bayesian neural networks involves optimizing a lower bound to the ELBO by sampling auxiliary variables. The refined posterior, generated from samples drawn from q \u03c6 (a 1 ) and optimized q \u03c61 (w), provides a much better approximation to the true posterior than the initial Gaussian approximation. The iterative refinement of the approximate posterior in Bayesian neural networks involves optimizing a lower bound to the ELBO by sampling auxiliary variables. The refinement process ensures that the refined posterior approximation cannot be worse than the initial mean-field approximation in terms of ELBO. The ELBO aux serves as a lower bound to ELBO ref and is increased by both the initial training and refinement steps. The KL divergence of the joint distribution is guaranteed to be greater than or equal to that of the marginals, ensuring improvement in the ELBO. This improvement is guaranteed under two assumptions: the conditional variational posterior is within the variational family, and the optimization process does not worsen the initial value. For Gaussian families, the posterior is Gaussian and can be computed in closed form. Comparing initial and final values ensures the optimization process does not decrease the ELBO. The optimization process ensures that the Evidence Lower Bound (ELBO) improves through refining steps, with any amount of optimization increasing the ELBO. In practice, exact inference is often computationally infeasible, leading to the use of approximate inference methods like variational inference. The ELBO improvement is shown on real-world datasets, forming a staircase pattern with each sampling of auxiliary variables. Variational inference approximates the true posterior distribution over parameters using a simple family of distributions. Mean-field VI employs independent Gaussian distributions to capture the posterior, allowing for efficient training and proper density over the parameter space. Normalizing flows can further increase the flexibility of the variational posterior. Recent advances in VI are detailed in a survey by Zhang et al. (2018a). Our method is a novel variant of the auxiliary variable approaches to VI that increase the flexibility of the variational posterior through the use of auxiliary variables. Instead of training a complex variational approximation over the joint distribution, we iteratively train simple, mean-field approximations at the sampled values of the auxiliary variables. The introduction of every new auxiliary variable increases the flexibility of the posterior approximation. Our algorithm refines the posterior starting at large scale and iteratively moves towards smaller scale refinements using auxiliary variables. Unlike other methods that only refine the posterior at the scale of the MCMC steps, we do not add parameters at training time but instead refine the samples through the introduction of auxiliary variables. These approaches have not been applied to Bayesian multi-layer neural networks. Methods like deep ensembles and bootstrapping are non-Bayesian strategies for estimating uncertainty in deep learning. Deep ensembles have shown strong performance in regression, classification, and uncertainty benchmarks. They rely on independently trained models to estimate uncertainty. The text discusses the use of deep ensembles and bootstrapping to estimate uncertainty in deep learning models. Deep ensembles rely on independently trained models to estimate uncertainty, while bootstrapping subsamples the training set to induce diversity. Experiments were conducted to confirm improvement in ELBO and quantify uncertainty estimates. In the study, 10 ensemble members with 5 auxiliary variables were refined, with fixed means and variances forming a geometric series. Three baselines were used: mean-field variational inference, multiplicative normalizing flows (MNF), and deep ensemble models. Variational inference was conducted over all weights and biases with a Gaussian prior. In the study, Gaussian posterior approximation with a Gaussian prior centered at 0 is used for inference over all weights and biases. The variance of the prior is tuned through empirical Bayes. Multiplicative Normalizing Flows (MNF) and Deep ensemble models are also explored for posterior approximations and quantifying uncertainty. Deep ensemble models are shown to be effective at quantifying uncertainty, although less principled than Bayesian methods. The study utilized adversarial training with a small learning rate for regression benchmarks and image classification benchmarks. Ensemble members were trained on datasets with a feed forward neural network model. The refinement step consistently improved ELBO and uncertainty estimates over VI, with Refined VI performing well on most datasets. Image classification benchmarks were also evaluated using a specific architecture. The LeNet5 architecture was used for image classification benchmarks, showing improvement in ELBO and uncertainty estimates over VI. Refined VI did not outperform Deep Ensembles in accuracy but outperformed in MLL on CIFAR10 dataset. The performance of the refining algorithm was demonstrated on Residual Networks with 20 layers, showing mean values and standard deviations for metrics like MLL, accuracy, and ELBO. Two models were examined: one with inference over all residual blocks and a hybrid model where inference is only done over the final layer of each block. The batch size was 256, with a decayed learning rate and 10 auxiliary variables reducing prior variance. The effect of Batch Normalization was also investigated. The study investigated the impact of Batch Normalization on variational inference, confirming its benefits. The refined hybrid model showed superior performance in terms of MLL compared to previous research. The results on CIFAR10 with the ResNet architecture were presented, highlighting the importance of Batch Normalization in improving VI outcomes. The study introduces a novel algorithm for variational inference, discussing computational costs and training times for different models. Deep ensembles require fewer training iterations and parallelize well, while Refined VI can split refinement iterations into threads. The algorithm refines a coarse variational approximation to the Bayesian posterior, showing improved performance over the initial distribution. Our method improves uncertainty estimation and computational efficiency compared to baseline variational approximations. It achieves state-of-the-art results in uncertainty estimation using variational inference on CIFAR10 at ResNet scale. Analytic solutions are found for additive Gaussians in the sampling and initialization processes. The proof concludes by showing that the ELBO initialization method improves uncertainty estimation and computational efficiency compared to baseline variational approximations."
}