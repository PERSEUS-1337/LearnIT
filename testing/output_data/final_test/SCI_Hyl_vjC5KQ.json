{
    "title": "Hyl_vjC5KQ",
    "content": "Hierarchical reinforcement learning (HRL) is a method that leverages the hierarchical structure of tasks in reinforcement learning (RL). A new HRL approach proposed in this paper learns a latent variable of a hierarchical policy using mutual information maximization. This method aims to create a discrete and latent representation of the state-action space. Additionally, advantage-weighted importance sampling is introduced to learn option policies corresponding to modes of the advantage function. Our HRL method uses a gating policy to select option policies based on an option-value function, optimized with the deterministic policy gradient method. It enhances RL performance in continuous control tasks by learning diverse options. HRL leverages task hierarchy, with past studies showing success in solving challenging tasks. In this study, an HRL method called adInfoHRL is presented, utilizing mutual information maximization with advantage-weighted importance. The goal is to identify a hierarchical policy structure that efficiently learns a variety of behavior types to solve complex tasks. Previous studies have shown that HRL, using deep learning, can achieve impressive performance in tasks such as video games and robotic manipulation. The adInfoHRL method aims to learn a latent variable in a hierarchical policy by using advantage-weighted importance. It divides the state-action space into regions based on an information maximization criterion and learns option policies for each region. This approach is based on deterministic option policies trained using an extension of the deterministic policy gradient. The proposed adInfoHRL method learns latent variables in a hierarchical policy using advantage-weighted importance. It optimizes option policies based on the deterministic policy gradient and a gating policy selects the option with the highest expected return. Experimental results demonstrate the method's ability to learn diverse options in continuous control tasks and improve performance on tasks like Walker2d and Ant in OpenAI Gym with MuJoco simulator. The reinforcement learning framework consists of state space, action space, reward function, initial state distribution, and transition probability. The goal is to learn a policy that maximizes the expected return by defining the Q-function and hierarchical policy. The reinforcement learning framework involves defining the Q-function and hierarchical policy to maximize expected return. Hierarchical policies in HRL often use a structure with a latent variable o, which can be discrete or continuous. The objective function in HRL involves multiple policies that can yield equivalent returns, requiring additional constraints to determine the optimal solution for o. In hierarchical reinforcement learning (HRL), additional constraints are needed to determine the optimal solution for the latent variable o. This study proposes learning the latent variable by maximizing mutual information (MI) between latent variables and state-action pairs. The deterministic policy gradient (DPG) algorithm is used for learning a monolithic deterministic policy, and has been extended to deep deterministic policy gradient (DDPG) for continuous control problems. The Twin Delayed Deep Deterministic policy gradient algorithm (TD3) is an extension of DDPG for continuous control problems, outperforming on-policy methods like TRPO and PPO in certain domains. Recent studies have shown that an interpretable representation can be learned by maximizing mutual information (MI) through regularized information maximization (RIM). RIM involves learning a conditional model to predict a label y, with the objective of minimizing a specific function. In this study, a novel HRL method based on advantage-weighted information maximization is proposed for learning a latent representation of the state-action space. The concept of maximizing mutual information (MI) through regularized information maximization (RIM) is applicable to various problems requiring a hidden discrete representation. The HRL framework is based on deterministic option policies, focusing on learning the latent variable o of a hierarchical policy. The study proposes a novel HRL method based on advantage-weighted information maximization for learning a latent representation of the state-action space. The approach focuses on dividing the state-action space using I(s, a), o and finding modes of the probability density of state-action pairs. The policy is based on the advantage function with a state value function and a partition function. The study introduces a novel HRL method that utilizes advantage-weighted information maximization to learn a latent representation of the state-action space. It focuses on clustering samples induced by the policy \u03c0 Ad through MI maximization using a neural network called the option network. The learning of the latent variable o is formulated as minimizing a specific function with a regularization term. The study introduces a novel HRL method that utilizes advantage-weighted information maximization to learn a latent representation of the state-action space. In practice, the regularization term involves approximating the advantage function and learning a discrete variable o. The regularization method used is a simplified version of virtual adversarial training (VAT) to penalize dissimilarity between original and perturbed state-action pairs. This regularization has been shown to improve the performance of learning latent discrete representations. The probability density of (s, a) induced by the policy \u03c0 Ad is necessary for computing mutual information for the purpose of estimating the probability. To estimate the probability density of (s, a) induced by policy \u03c0 Ad, an advantage-weighted importance sampling approach is employed. The importance weight is approximated to account for the change in state distribution induced by the policy update. This method eliminates the need to compute the partition function Z. The advantage-weighted importance is used to compute the entropy terms for computing mutual information. Empirical estimates of entropy and conditional entropy are calculated from samples drawn from a behavior policy. The option network is trained using samples collected from recent behavior policies stored in the onpolicy buffer. The algorithm utilizes samples from the replay buffer, with a preference for on-policy buffers for better performance in latent representation learning. The implementation is \"semi\" on-policy, using samples from recent behavior policies. Deterministic option policies are considered, modeled with separate neural networks. The objective function of off-policy HRL with deterministic option policies involves replacing stochastic option policies with deterministic ones in the Q-function equation. The objective function in off-policy HRL with deterministic option policies involves learning the gating policy \u03c0(o|s) to maximize the conditional expectation of the return. The option-value function for deterministic option policies is approximated using the softmax gating policy. In this study, the Q-function is trained using a softmax gating policy and state-value function. Two neural networks are used to estimate the Q-function, with the target value computed based on a replay buffer. The gating policy determines the option periodically, and option-policy networks are trained separately for each option. The Q-function is trained using a softmax gating policy and state-value function. Option-policy networks are trained separately for each option, with samples assigned to the option with the highest probability. The adInfoHRL algorithm is evaluated on the OpenAI Gym platform using the MuJoCo Physics simulator, showing improved performance compared to PPO. The performance of adInfoHRL was compared with PPO and TD3, with a focus on TD3 due to its implementation basis. The analysis included on-policy and off-policy methods, using the same network architectures for actor and critic in adInfoHRL and TD3. A variant of adInfoHRL without advantage-weighted importance was also evaluated. The performance of adInfoHRL was compared with PPO and TD3, focusing on TD3 due to its implementation basis. The analysis included on-policy and off-policy methods, using the same network architectures for actor and critic in adInfoHRL and TD3. A variant of adInfoHRL without advantage-weighted importance was also evaluated. The gating policy updated variable o once every three time steps. The option policies learned on the Walker2d task are visualized in FIG1, showing different phases of locomotion. AdIfoHRL yielded the best performance on Ant 1 and Walker2d tasks. AdInfoHRL outperformed infoHRL and showed sample efficiency on Ant and Walker2d tasks by dividing the state-action space to deal with multi-modal advantage functions. The advantage-weighted importance enhanced learning options' performance, making adInfoHRL superior on tasks with unstable dynamics. AdInfoHRL's superiority lies in tasks with multi-modal advantage functions, as shown in the outputs of the option network on Walker2d. By considering mutual information between latent variables and state-action pairs, it efficiently divides the state-action space for improved performance. InfoGAIL, proposed by Li et al. (2017), learns an interpretable representation of the state-action space through mutual information maximization. It divides the space based on density induced by an expert's policy, similar to our method, although their focus is on imitation learning. Previous studies have used importance weights based on the value function, with our method using a similar approach for learning a latent variable in a hierarchical policy. Our method, adInfoHRL, divides the state-action space based on the MI criterion in hierarchical reinforcement learning. It is related to the Divide and Conquer concept but allows for switching between option policies. Further exploration is needed for applying this approach to stochastic policy gradients. In our adInfoHRL method, we introduce a novel HRL approach where the latent variable of a hierarchical policy is learned as a discrete representation of the state-action space. Results show that adInfoHRL can learn diverse options on continuous control tasks and improve TD3 performance in certain domains. The mutual information between the latent variable and state-action pair is defined, with an empirical estimate modified to include importance weight. The empirical estimate of mutual information (MI) with respect to the density induced by a policy \u03c0 is calculated using importance weights. The estimate of conditional entropy with respect to the density induced by a policy \u03c0 is also determined. The empirical estimates of mutual information (MI) can be computed using equations in the context of a policy \u03c0. Evaluation was done using benchmark tasks in OpenAI Gym with Mujoco physics simulator. Hyperparameters for reinforcement learning methods were specified, including exploration strategies for adInfoHRL and TD3. Specific tasks like Walker2d, HalfCheetah, and Hopper were used with corresponding environments. The Ant task was performed using the AntEnv in rllab BID2. Training policies with AdInfoHRL, infoHRL, and TD3 involved training critics once per time step and actors once every two updates of the critics. Experiments were conducted five times with different seeds, reporting averaged test return computed every 5000 time steps. The learned policy on HalfCheetah task with adInfoHRL showed the best performance with two options. The distribution of options on HalfCheetah0v1 after one million steps is displayed in FIG6, showing uneven activation despite an evenly divided state-action space. The activation of options in the state-action space is not evenly distributed, as shown in FIG7 and FIG8. One option corresponds to stable running, while the other is for recovering from unstable states. The Ant-rllab task with four options displays uneven activation in different domains of the state space, as visualized in t-SNE reduced dimensionality in FIG6. The Ant-rllab task displays uneven activation of options in the state space, with two options mainly activated during stable locomotion. The Ant-v1 task in OpenAI gym showed that adInfoHRL performed best with two options, comparable to TD3. Ant-v1 task does not require a hierarchical policy structure, unlike Ant-rllab where a hierarchical policy improves learning performance. The performance of adInfoHRL, IOPG, and SAC-LSP on various tasks was compared after 1 million steps. AdInfoHRL outperformed IOPG on tasks in terms of return, while SAC-LSP showed superiority on different tasks. SAC-LSP used reward scaling for its results. Further experiments are needed to compare adInfoHRL and SAC-LSP under the same conditions, as SAC-LSP used reward scaling which was not used in the evaluation of adInfoHRL."
}