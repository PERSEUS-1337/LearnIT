{
    "title": "S1giWPGsjQ",
    "content": "Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), excel in supervised tasks like classification and feature learning. However, CNNs may experience a performance drop when the data distribution changes between training and testing. The representations of CNNs show higher variance for unseen data, especially from shifted distributions. This variance can lead to mis-classification as samples cross decision boundaries. Nearest neighbor classification reveals that high variance embeddings perform significantly worse, despite end-to-end classification results. This issue is addressed by... Deep Within-Class Covariance Analysis (DWCCA) is a deep neural network layer that reduces within-class covariance, improving performance on unseen test data from shifted distributions. Empirically evaluated on two datasets for Acoustic Scene Classification, DWCCA significantly enhances the network's internal representation. Adding DWCCA to a VGG neural network improves end-to-end classification accuracy by around 6 percentage points, especially with a slight distribution shift in the test set. Convolutional Neural Networks (CNNs) are powerful in supervised learning tasks like classification, learning superior features, and achieving high performance. CNNs can even perfectly learn from data with random labels. However, a model must generalize to unseen data and cope with distribution mismatch. In this paper, the robustness of a DNN architecture (VGG BID28) adapted for audio classification is evaluated on data with and without distribution mismatch. While VGG performs well on data without mismatch, its performance drops significantly on data from a shifted distribution. Internal representations of the network show that the spread in the embedding space is greater for unseen data compared to training data. The study evaluates the robustness of a DNN architecture for audio classification on data with distribution mismatch. Internal representations of the network reveal higher variance in unseen data compared to training data, especially from a shifted distribution. This variance impacts the generalization quality of the network's representations. The study proposes Deep Within-Class Covariance Analysis (DWCCA), a DNN layer that reduces within-class covariance in internal representations to address performance degradation in cases of distribution mismatch. DWCCA is trainable end-to-end and can be integrated at any position in a DNN. DWCCA significantly reduces within-class covariance in DNN representations, improving generalization quality and classification results on an acoustic scene task with distribution shift. The authors in BID19 studied how architecture topologies impact representation robustness and generalization in CNNs. Different architectures transfer representations better to other datasets and tasks. BID8 proposed a loss to learn separable latent representations, while BID0 suggested creating maximally correlated representations from different modalities. Stand-alone CNNs have limitations in representation quality. Representation from different modalities is a challenge for stand-alone CNNs, especially in tasks like Acoustic Scene Classification (ASC) where distributional shifts are common. While CNNs have shown promise in ASC with similar training and test data distributions, they struggle with generalization to shifted distributions. To address this, CNN manifold data augmentation using Generative Adversarial Networks has been explored. In BID11, data augmentation was done using Generative Adversarial Networks, while in BID15, BID31, and BID27, an ensemble of CNNs was used as feature extractors with Support Vector Machines for processing deep features and late fusion of models. CNNs struggle with generalization in distribution shift cases but can learn useful features for new classifiers. This paper aims to understand these issues by analyzing internal representations in CNNs and proposing DWCCA to address performance drops. Within-Class Covariance Normalization (WCCN) is introduced as a classic machine learning method. Within-Class Covariance Normalization (WCCN) is a classic machine learning method that compensates for within-class variability. It scales the feature space opposite to its within-class covariance matrix, making decision boundary identification easier. Deep Within-Class Covariance Analysis (DWCCA) is proposed as a DNN-compatible extension of WCCN. Deep Within-Class Covariance Analysis (DWCCA) is a DNN-compatible formulation of WCCN. Parameters are optimized with Stochastic Gradient Descent (SGD) and trained with mini-batches. A DWCCA Layer is proposed to address the issue of estimating within-class covariance matrix on mini-batches instead of the entire training set. This approach involves computing a mini-batch projection matrix and maintaining a moving average projection matrix. The DWCCA Layer computes a mini-batch projection matrix using a hyperparameter \u03b1 to control data influence. Parameters are not learned via SGD but computed through previous layer activations. Covariance normalization is applied during optimization using Theano for gradient flow establishment. In this section, we present our experimental setup and results using the TUT Acoustic Scenes 2016 (DCASE2016) and TUT Acoustic Scenes 2017 (DCASE2017) datasets, which consist of 15 different acoustic scenes. The audio material was collected as 3-5 minute recordings and then segmented into 30-second segments. The DCASE2017 dataset uses the same recordings as DCASE2016 for its development set. The DCASE2016 and DCASE2017 datasets were used for training and validation, with DCASE2017 having a new unseen test set. There is a distribution mismatch between the development and test data in DCASE2017 due to different recording environments. This information is detailed in the DCASE2017 BID25 report. The DCASE2017 BID25 report discusses the use of DCASE2016 as a scenario without distribution shift and DCASE2017 as a distribution shift case. The experiments include cross-validation splits and performance on the unseen test set using a VGG-Style CNN model. The study compares different models for audio classification, including a vanilla model and a model with a DWCCA layer. Baselines include an ensemble CNN model from DCASE2016 and a ResNet ensemble model from DCASE2017. Classification results are reported for all models on various datasets. The study compares different models for audio classification, including a vanilla model and a model with a DWCCA layer. Baselines include an ensemble CNN model from DCASE2016 and a ResNet ensemble model from DCASE2017. Classification results are reported for all models on various datasets. The report presents classification results of end-to-end CNNs on all folds and the unseen test set. Late fusion using a linear logistic regression model is applied for calibration. The setup includes an initial learning rate of 0.0001, ADAM optimizer, \u03b1 value of 0.1, and model selection with max patience of 20. The models are trained with stratified batch iterators and a batch size of 75. Spectrograms of the audio segments are fed to the models, extracted using the madmom library. The DWCCA Layer is implemented as a Lasagne-compatible layer in Theano. Experiments are conducted on DCASE2016 and DCASE2017, with visualization of VGG representations in 2D via PCA. The within-class covariance of representations is analyzed using eigenvalue decomposition on the covariance matrix. High eigenvalues indicate high covariance within a class, while a large difference between the highest and lowest eigenvalues suggests uneven variance distribution. K-nearest-neighbor classification experiments are conducted to assess the learned internal representation spaces. Results are visualized in Figures 2 and 3. After analyzing the within-class covariance of representations, a knearest-neighbor classification experiment was conducted in the learned representation spaces. The results are shown in Figure 4. End-to-end classification results for various methods on both datasets can be found in TAB2. Additionally, class-wise f-measure of DWCCA and baseline VGG are provided in TAB3. Internal representations of the network in each class are projected into 2D via PCA in FIG2, showing embeddings of unseen data spread less after applying DWCCA. After applying DWCCA, unseen embeddings are projected closer to training embeddings, reducing variance and improving generalization. DWCCA significantly reduces within-class variability in datasets with and without distribution shift, as shown in Figure 3. Eigenvalues of covariances of representations have a smaller range on unseen data. The eigenvalues of covariances of representations have a significantly larger range on unseen data compared to training data. Adding DWCCA improves KNN classification accuracy in both cases. Test accuracy drops in the mismatch case for DCASE2017 but improves with DWCCA, showing a 6 percentage point improvement. The performance of vanilla on DCASE 2017 consistently improves with DWCCA, showing a 6 percentage point improvement. Results on all folds were enhanced by adding DWCCA, but the test set results did not significantly change. Test data in DCASE2016 may be more similar to the training set than the validation set. The performance of vanilla on DCASE 2017 improves with DWCCA, showing a 6 percentage point enhancement. Test data in DCASE 2016 may be more similar to the training set than the validation set. DWCCA could not help due to a small generalization gap between training and test data. Single models were used in the experiments, achieving comparable performances to ensemble models. Class-wise f-measures on the unseen test set show a boost of 13 percentage points on the \"train\" class. Overall, DWCCA shows a significant impact on improving the performance of different classes in the dataset with distribution shift. The \"train\" class saw a boost of 13 percentage points, while the \"beach\" class, which initially had the lowest f1 score, improved by 24 and 37 percentage points for non-calibrated and calibrated models, respectively. On the other hand, the \"forest path\" class only dropped by 2 and 3 percentage points for non-calibrated and calibrated models. DWCCA seems to have less impact on classes with high f1 scores like \"office\" and \"beach\". The DWCCA layer improves DNN performance by reducing within-class covariance, leading to more robust performance and better generalization. It helps normalize representations and enhances CNN performance on data with shifted distributions. The VGG network saw a 6 percentage point improvement with DWCCA on shifted test data. DWCCA also boosts generalization of DNN representations. DWCCA improves generalization of DNN representations by reducing within-class covariance. It can significantly reduce WCC and enhance the robustness of CNN performance on data with distribution mismatch. The study was supported by various organizations and the authors express gratitude for the support received."
}