{
    "title": "S1g_N7FIUS",
    "content": "The fields of artificial intelligence and neuroscience have a history of bi-directional interactions. Inspiration for AI systems comes from studying natural intelligence, especially the neocortex. Models of the brain have also been influenced by AI research. A key question is how neocortex learns, similar to deep networks' back-propagation. Matching neocortical learning efficiency in deep learning is an active research area. Recent advances in understanding neocortical physiology suggest new approaches for unsupervised representation learning. Recent advances in understanding neocortical physiology propose new methods for unsupervised representation learning, potentially using local learning rules with implicit objectives. These approaches could enhance the efficiency of deep networks in leveraging unlabelled data for improved downstream supervised learning, while also reducing vulnerability to adversarial attacks. The ventral stream, responsible for visual object recognition, learns hierarchically organized representations similar to CNNs but with sparse labeling. This property of cortical learning is attractive for deep learning models as it allows for the use of unlabelled data, leading to superior generalization and transfer properties. The monkey's paw effect illustrates the problem of numerical optimization algorithms yielding solutions that match the objective but not the intended outcome. This phenomenon can be seen in the short story \"The Monkey's Paw\" by W. W. Jacobs, where a wish for $200 is granted through a tragic event. The Monkey's Paw effect, as seen in the short story by W. W. Jacobs, relates to the unintended consequences of optimization algorithms in deep neural networks. Adversarial examples highlight the need to carefully manage learning processes and avoid injecting harmful data back into the system, especially in critical applications like self-driving cars. The need to address the Monkey's Paw effect in deep learning systems, particularly in critical applications like self-driving cars, has been emphasized. Adversarial examples offer a way to tackle this issue by designing deep learning systems to disentangle underlying factors of variation for better generalization and transfer. The challenge lies in expressing the objective of learning good representations, with potential clues from the neocortex's local processes of synaptic plasticity. The original CNN architecture (Fukushima, 1980) learned visual features through self-organization using local rules. Recent insights into neocortex plasticity offer inspiration for deep representation learning paradigms that learn \"disentangled representations\" from unlabelled datasets. A new dendrite-centric view of synaptic plasticity is emerging with the discovery of the NMDA spike, associating co-activated synapses through potentiation or structural changes. Recent insights into neocortex plasticity reveal the formation of co-coding clusters of synapses and small cliques of all-to-all connected neurons driving co-coding. Plasticity mechanisms involving dendritic clustering by NMDA spikes and inhibition by Martinotti neurons limit clique size and suppress competing cliques, forming basic building blocks for representation learning in the neocortex. The neocortex utilizes local learning rules for representation learning in the feedforward pathway, incorporating dendritic dimensions. Feedback pathways trigger dendritic calcium spikes and bursting, activating WTA dynamics in neurons. VIP neurons can also modulate these dynamics. The feedback pathways in the neocortex implement predictive coding and error back-propagation for supervised learning algorithms. While their importance for rapid object recognition has been demonstrated, their computational role remains inconclusive. Supervised learning, back-propagation, and gradient descent are likely to remain key components of AI, but unsupervised approaches are becoming increasingly important for sparse data, generalization, and transfer learning. Unsupervised approaches are crucial for learning good representations from observation in AI. Sparse labels, adversarial examples, transfer learning, and few-shot learning are key success criteria for further development. Neocortical plasticity inspires advancements, but there are limitations to unsupervised learning compared to supervised learning. Neocortical unsupervised learning algorithms rely on specific spatial and temporal scales in input streams to learn representations. Evolution has optimized sensory organs to provide signals that the neocortex can interpret, tailored to the animal's ecological niche. Cortical unsupervised learning algorithms cannot cluster random noise images, as they require a neocortical problem to solve. It is essential to work within the limitations of the neocortex when drawing inspiration from it."
}