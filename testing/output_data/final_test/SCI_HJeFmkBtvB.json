{
    "title": "HJeFmkBtvB",
    "content": "Energy-based models output unnormalized log-probability values for data samples, essential for various applications. Standard maximum likelihood training is computationally expensive, but denoising score matching can help alleviate this issue. Previous attempts at high-quality sample synthesis failed due to limitations in denoising score matching. To overcome this, a new approach learns an energy function using all noise scales, resulting in high-quality samples comparable to state-of-the-art techniques when sampled using Annealed Langevin dynamics and single-step denoising jump. Our model achieved high-quality sample generation and likelihood estimation, setting a new baseline in likelihood-based models. It demonstrates strong generalization in image inpainting tasks and the importance of learning sample distributions for various applications like error correction, outlier detection, and Bayesian reasoning. Energy-Based Models play a key role in assigning energy values to data samples for these tasks. Energy-Based Models assign energy to data points, defining a probability distribution. They offer advantages over other generative models like GANs, flow-based models, and auto-regressive models. EBMs provide explicit density information, better mode coverage, and flexibility without requiring special model architecture. Energy-based models have been trained successfully with maximum likelihood, but training is computationally demanding due to sampling model distribution. Truncated sampling procedures like contrastive divergence have been proposed for faster learning, although they may not explore the state space thoroughly. Score matching avoids sampling the model distribution by minimizing the expected L2 norm difference between model and data score functions. One application is learning the energy function for a Gaussian kernel Parzen density estimator of the data. The Parzen density estimator of the data involves evaluating the data score and defining the objective function. Vincent (2011) established the equivalence between Denoising Score Matching (DSM) and the objective function of score matching. DSM replaces the Parzen density score with the derivative of the log density of the noise kernel, making evaluation easier. In the case of Gaussian noise, the objective function aligns the energy gradient with the data. The interpretation of objective (3) aligns the energy gradient with the vector from noisy to clean data samples. Double backpropagation is used to optimize objectives involving neural network derivatives. Recent work has focused on training energy-based models to match Parzen density estimators with noise magnitudes for denoising tasks and generating high-quality data samples. Our work introduces Multiscale Denoising Score Matching (MDSM) as a novel method for training energy-based models. It addresses the limitations of learning with a single noise level and provides empirical results comparing MDSM with other models. The authors introduced Multiscale Denoising Score Matching (MDSM) as a method for training energy-based models, addressing the need for large noise perturbations in low-data density regions. Analyzing the learning process in denoising score matching, they focus on measure concentration properties of high-dimensional random vectors. The assumption of a high-dimensional data distribution with support around a low-dimensional manifold poses challenges for score matching, as the density gradient becomes undefined outside the manifold. The authors proposed smoothing the data distribution with a Gaussian kernel to address challenges in denoising score matching when data lie on a high-dimensional manifold. Random Gaussian vectors in high-dimensional spaces exhibit concentrated length distribution and proximity to orthogonal vectors, aiding in visualizing noisy and noiseless data points in the learning process. The tangent space T intersects X at x. Distance vectors between pairs have similar length \u221a d\u03c3. Noisy data points concentrate on setX \u221a. Denoising score matching with fixed noise level \u03c3 matches score in setX \u221a d\u03c3, enabling denoising. Little information provided about density outside this set. Models based on denoising score matching encounter difficulties. To address the difficulties faced by models trained with a single noise level, a learning procedure involving samples with different noise levels is proposed. Gaussian noise and a Gaussian scale mixture are used to generate noisy data samples for training. Additionally, denoising score matching suggests that learned features have larger spatial scale with increasing noise levels. In the experiment, it was observed that training a model with denoising score matching using a single noise scale is not sufficient for high-quality sample synthesis. Different noise levels result in models learning different spatial correlations, indicating the need to capture data structure at all scales. An Energy-Based Model (EBM) based on denoising score matching is proposed to address this limitation by training with noisy samples of varying noise levels. The Multiscale Score Matching (MSM) objective aims to train a model with noisy samples drawn from a distribution to approximate the Parzen density estimator of the data. It minimizes the difference between the derivative of the energy and the score of the model under a new expectation, covering the signal space more evenly. This objective is compared to denoising score matching, with the main difference being the expectation used. The Multiscale Score Matching (MSM) objective aims to train a model with noisy samples to approximate the Parzen density estimator of the data. Proposition 2 shows that Equation 4 is equivalent to a denoising score matching objective. By choosing Gaussian noise kernels, the equation is transformed into Multiscale Denoising Score Matching (MDSM) for better evaluation. In practice, adding a monotonically decreasing term for balancing noise scales is convenient. The noise range should be broad enough to encourage learning of data features over all scales. Instead of sampling \u03c3, a series of fixed \u03c3 values is chosen for the final objective. The importance of \u03c3 0 as a hyperparameter diminishes in the model approximation. The model approximation focuses on the range of noise levels used during training, with a single target distribution and multiple distributions smoothed by different kernels. Noise magnitude is not required as input, and Langevin dynamics is used for sampling from neural network energy functions. Simulated annealing is proposed to improve mode exploration in Langevin dynamics by sampling at high temperature and gradually cooling down. This method has been successful in combinatorial optimization problems. The Langevin sampling process is defined by an annealing schedule for temperature and a fixed step length. During sampling, particles behave like physical particles under Brownian motion in a potential field, exploring the state space at different distances from the data manifold depending on temperature. A single step gradient denoising jump can be applied to improve sample quality, justified by the Empirical Bayes interpretation. The proposed energy-based model is trained on standard image datasets, including MNIST, Fashion MNIST, CelebA, and CIFAR-10. Training involves setting \u03c3 0 = 0.1 and training over a noise range of \u03c3 \u2208 [0.05, 1.2]. Different noise levels are uniformly spaced on the batch dimension, with geometrically distributed noise used for MNIST and Fashion MNIST in the range [0.1, 3]. The weighting factor l(\u03c3) is always set to 1/\u03c3 2. The energy-based model is trained on image datasets like MNIST, Fashion MNIST, CelebA, and CIFAR-10. Batch size is fixed at 128, Adam optimizer with learning rate 5 \u00d7 10 \u22125 is used. Different ResNet architectures are employed with specific filters. Output layers are designed in a quadratic form to enhance performance. Training method without sampling results in a significant speedup compared to Langevin dynamics. Our method allows training of energy-based models with limited computational resources, showing an order of magnitude improvement over maximum-likelihood training using Langevin dynamics. The choice of noise level has little impact on learning, as long as it encourages learning of long-range features in the data. Linearly spaced noise for sampling the learned energy function was found to be more robust than geometrically spaced noise. An empirically optimized annealing schedule is used for sampling the learned energy function, with an initial temperature of T = 100 and step length of 0.02. After annealing, a single step denoising process is performed to enhance sample quality. After an empirically optimized annealing schedule, a denoising process is performed to enhance sample quality. Inception Score of 8.31 and FID of 31.7 were achieved, comparable to modern GAN approaches. Visual assessment is essential as visually impressive samples may not have the highest Inception Score. No overfitting was found in the model. Mode coverage experiments were repeated with the model. Additional samples and training images are provided in the Appendix for visual inspection. In the Appendix, mode coverage experiment with a 3-channel MNIST model was conducted, achieving results comparable to GAN approaches. Training was successful, but some modes were not accessed during sampling. Image inpainting can be achieved by clamping part of the image to ground truth and using annealed Langevin and Jump sampling on the missing part. In the context of image inpainting, annealed Langevin and Jump sampling are used on the missing parts. Noise is added to the inputs based on the sampling temperature. Log likelihood estimation is done using Annealed Importance Sampling or Reverse AIS. There is a gap in estimation between AIS and Reverse AIS even with significant computational effort. Density of 1.21 bits/dim is reported on the MNIST dataset. More details on the experiment can be found in the Appendix. Outlier detection was conducted to investigate the behavior of high dimensional density models on out of distribution samples. The energy function was trained outside the data manifold, leading to lower energy values for some out of distribution samples. Noise was added before measuring the energy value, showing similarities to previous likelihood models. The denoising performance was also used for outlier detection. In this work, the denoising performance was explored for outlier detection, showing similarities to using the energy value. The study found that denoising performance is more correlated with the variance of the original image rather than the image content. The limitations of learning high-dimensional data with denoising score matching were analyzed, revealing that the objective function restricts learning to a small set due to the measure concentration phenomenon in random vectors. Sampling the learned distribution outside this set does not yield good results, but using samples corrupted by different noise levels during learning can help. The paper's main contribution is investigating how to effectively utilize denoising performance for outlier detection. The paper introduces the Multiscale Denoising Score Matching (MDSM) model for energy-based models (EBMs), which can denoise, generate high-quality samples, and perform image inpainting. The model learns faster than maximum likelihood-based models and combines denoising autoencoder and annealing techniques. Previous efforts in learning EBMs with score matching were either computationally intensive or unable to produce high-quality samples. The NCSN model proposed by Saremi et al. (2018) and Saremi and Hyvarinen (2019) can produce high-quality samples, unlike previous energy-based models. It approximates the score of distributions by smoothing data with kernels of different widths, improving sample synthesis. In contrast, our method learns an energy-based model for a fixed \u03c3 0, enhancing score matching in high-dimensional space by matching gradients to avoid measure concentration issues. Our novel EBM model achieves high-quality sample synthesis and is more parsimonious than the NCSN model. However, it performs slightly worse, possibly due to the approximation used in Equation 6 and the different output structures between the models. The MDSM objective is discussed as an improved score matching formulation in high-dimensional space. It suggests replacing the expectation with a distribution that avoids the measure concentration problem. The multiscale score matching objective aims to minimize L M SM = 0 by using a Gaussian scale mixture distribution. This approach is different from the MDSM objective, which replaces the expectation with a distribution to avoid measure concentration issues. The importance sampling procedure involves sampling from empirical distribution and Gaussian scale mixture, weighting the sample, and using Bayes rule to justify the ratio close to 1. The weighting factor is ignored in practice, and future work aims to improve this approximation. Comparisons with previous methods involve training an energy-based model with denoising score matching on MNIST dataset. Denoising score matching was performed on MNIST using one noise level, with sampling initialized using Gaussian noise and Langevin dynamics. A 12-layer ResNet was used for training, and model samples were compared with nearest neighbors in the training set using Fashion MNIST. An 18-layer ResNet with ELU activation function was used for this comparison. More details on the network structures can be found in the code release. During training, the energy model's gradient scales linearly with noise, leading to a quadratic scaling of the energy function. The output layer of the energy-based network was modified to a flexible quadratic form, improving performance. For CIFAR and CelebA, training involved 300k weight updates with checkpoints saved every 5000 updates. MNIST and Fashion MNIST were trained for 100k updates, with the last checkpoint used. During training, the energy model's gradient scales linearly with noise, leading to a quadratic scaling of the energy function. The output layer of the energy-based network was modified to a flexible quadratic form, improving performance. For CIFAR and CelebA, training involved 300k weight updates with checkpoints saved every 5000 updates. MNIST and Fashion MNIST were trained for 100k updates, using the last checkpoint. The annealing sampling schedule is optimized for CIFAR-10 dataset with 2700 steps. Log likelihood estimation is done by initializing reverse chain on test images and sampling 10000 intermediate distributions using 10 steps HMC updates each. Temperature schedule is roughly exponential shaped with an isotropic Gaussian reference distribution. Variance of estimation was generally less than 10% on the log scale. The variance of estimation was generally less than 10% on the log scale. To avoid being dominated by outliers, the average of the log density is reported instead of the log of average density. More inpainting examples and samples are provided for visual judgment of sample generation quality. Energy values for CIFAR-10 train, CIFAR-10 test, and SVHN datasets are included. The network does not overfit to the training set but assigns lower energy to SVHN images. Annealing schedule and a typical energy trace during Annealed Langevin Sampling are shown, indicating sampling proximity to a proportional temperature. The energy of the sample is proportional to the temperature, suggesting sampling is close to a quasi-static process."
}