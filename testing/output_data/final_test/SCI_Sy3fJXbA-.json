{
    "title": "Sy3fJXbA-",
    "content": "Recent studies have shown that branching in convolutional networks, splitting computation along parallel threads and aggregating outputs, can lead to significant performance improvements. Prior work has simplified multi-branch architectures with fixed branching factors, same input to all branches, and additive output combination at aggregation points. In this work, an algorithm is proposed to learn the connections between branches in a network without predefined choices. The multi-branch connectivity is learned simultaneously with the network weights by optimizing a single loss function for the end task. This approach is demonstrated on multi-class image classification with higher accuracy compared to the state-of-the-art \"ResNeXt\" network. Deep neural networks have become prominent for learning complex functions with large training data, but designing deep architectures remains challenging and time-consuming. Deep architectures in image categorization have been studied extensively, focusing on the impact of depth, filter sizes, and nonlinearities on performance. Simplifying architecture design with basic building blocks like VGG networks and ResNets has been proposed to improve efficiency. Residual networks (ResNets) and ResNeXt models have shown the benefits of arranging building blocks in parallel to build multi-branch convolutional networks. ResNeXt consists of C parallel branches with identical topology but distinct parameters, leading to better results than single-thread ResNets. The modularized design simplifies architecture building, but the combination and aggregation of computations still require human design input to avoid a combinatorial explosion. Recent studies have shown that branching in convolutional networks, splitting computation along parallel threads and aggregating outputs, offers significant performance improvements. Prior work has used simple strategies to handle the complexity of multi-branch architectures, such as fixed branching factors and additive combination of outputs. In this work, an algorithm is proposed to learn the connections between branches in a network for multi-class image classification. The multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function. The approach yields absolute improvements of over 3% with respect to the state-of-the-art \"ResNeXt\" architecture. Recent studies have shown that branching, splitting computation along parallel threads and aggregating outputs, can significantly improve performance in multi-branch architectures. Prior work has used fixed branching factors and additive combinations, but this work proposes an algorithm to learn connections between branches in the network. This approach aims to optimize performance without predefined design choices. Recent studies have shown that branching, splitting computation along parallel threads and aggregating outputs, can significantly improve performance in multi-branch architectures. This work proposes an algorithm to learn connections between branches in the network, aiming to optimize performance without predefined design choices. The complexity of design choices in multi-branch architectures has led to the proposal of a new algorithm to learn connections between branches in the network. This approach aims to optimize performance without predefined choices. Our approach replaces fixed aggregation points in RexNeXt with learnable masks defining input connections for each residual block. We remove predefined choices and propose an algorithm that learns to combine building blocks in a neural network, allowing network connectivity to arise naturally through training optimization. Our approach utilizes residual blocks in a multi-branch architecture with learnable masks to optimize connectivity for improved performance compared to predefined networks like ResNeXt. Our approach involves automatically identifying and pruning superfluous residual blocks, inspired by ResNeXt, to optimize connectivity in multi-branch architectures. This strategy, introduced by He et al., helps cope with the challenges of deep model optimization. The authors introduced residual blocks to address the optimization challenges posed by increasing depth in neural networks. These blocks learn residual functions with reference to the layer input, allowing for easier optimization in deeper models. Shortcut connections aggregate the output of the residual block with its input, simplifying the learning process at each depth in the network. The module introduces convolutional residual blocks using a bottleneck architecture for image categorization. It projects input feature maps, applies filters, and restores original dimensionality. Batch Normalization and ReLU are applied after each layer, with a ReLU used after aggregation. The multi-branch architecture of ResNeXt is also discussed. The multi-branch architecture of ResNeXt involves arranging residual blocks in parallel threads of computation, with outputs summed up and passed to the next module. Parameters include cardinality (number of branches) and depth of the network. The ResNeXt module implements a split-transform-merge strategy with parallel threads of computation, each block having its own parameters. The output of each module is computed as a projection into separate lower-dimensional embeddings, a transformation within each embedding, and a final aggregation via addition. The ResNeXt module implements a split-transform-merge strategy with parallel threads of computation, each block having its own parameters. It was experimentally shown that increasing the cardinality C is more effective for accuracy improvement than increasing depth or number of filters. ResNeXt multi-branch networks consistently outperform single-branch ResNets with the same learning capacity. However, ResNeXt modules have embedded limitations such as implementing C parallel feature extractors on the same input and a constant number of active branches at all depth levels. Our proposed masked multi-branch architecture removes restrictions on network design without adding significant burden. It consists of L multi-branch modules with C parallel feature extractors, where each branch can take a different input controlled by a binary mask vector learned with the network weights. The proposed masked multi-branch architecture consists of L modules with C parallel feature extractors. Each branch can take a different input controlled by a binary mask vector learned with the network weights. The mask determines selectively which branches from the previous module will be aggregated and provided as input to the block, leading to more diverse features in the parallel branches. The masked multi-branch architecture consists of L modules with C parallel feature extractors. The binary mask vector controls input selection for each branch, leading to diverse features. Constraints on the mask vector can create different models, with the best results achieved by connecting each block to K branches where 1 < K < C. The mask vector is learned simultaneously with other network weights. The MASKCONNECT algorithm optimizes the network by pruning unused residual blocks to reduce parameters and speed up inference. It allows for adaptively learning a variable branching factor for different depths in the network. The algorithm jointly optimizes the learning objective with network weights and masks. In MASKCONNECT, the weights of the network and masks are optimized for image categorization using multi-class cross-entropy loss. The branch masks have binary values, making optimization more challenging. A modified backpropagation method is used to learn these binary parameters. During training, a real-valued version of the branch masks is stored and updated. Training via backpropagation involves forward propagation, backward propagation, and weight updates. The training via backpropagation in MASKCONNECT involves three steps: forward propagation, backward propagation, and parameter updates. Real-valued branch masks are binarized for forward and backward propagation, while the method updates the real-valued masks during parameter updates. The weights of convolutional and fully connected layers are optimized using standard backpropagation, with constraints on the number of active entries in the binary branch masks. During forward propagation in MASKCONNECT, real-valued branch masks are normalized to sum up to 1 for proper multinomial distribution. Binary branch masks are stochastically generated by sampling from the multinomial distribution. The input activation volume to the residual block is computed from the sampled binary branch masks. Sampling ensures connections with the largest values are selected. During backward propagation, the algorithm updates the real-valued branch masks by computing gradients with respect to the binary branch masks and using gradient descent with a clipped interval of [0, 1]. Fine-tuning is beneficial after joint training over \u03b8 and m. After joint training over \u03b8 and m, fine-tuning the weights \u03b8 with fixed binary masks has been found beneficial. The approach was tested on image categorization tasks using CIFAR-100 and ImageNet datasets. CIFAR-100 dataset consists of 50,000 training images and 10,000 test images categorized into 100 classes. The effect of fan-in (K) was studied in the experiments. The effect of fan-in (K) on model performance was studied using a model with L = 6 multi-branch residual modules and C = 8 branches per module. The total depth of the network was D = 20, and different fan-in values (K = 1 to 8) were tested without affecting the number of parameters. MASKCONNECT learns sparse connections, with squares without in/out edges deemed superfluous and pruned at the end of learning. The best accuracy is achieved by connecting each residual block to 4 branches out of the total 8 in each module. Varying the fan-in value yields lower accuracy. In experiments, setting K = 4 yields the best accuracy, while K = 1 results in high sparsity and parameter savings. Different architectures are tested with fan-in values of 1 and 4, showing the benefits of using MASKCONNECT for learning sparse connections. Learning the connectivity with MASKCONNECT leads to higher accuracy compared to fixed connectivity, with gains of up to 2.2% over the state-of-the-art ResNeXt model. The training time overhead for learning masks and weights is about 39%, but the accuracy improvements justify this cost. Models trained with sparse random connectivity (Fixed-Random) show lower accuracy compared to MASKCONNECT models, despite having the same connectivity density. Our proposed approach improves over ResNeXt by learning connectivity rather than relying on sparser connectivity. It automatically identifies unnecessary residual blocks during training, allowing for parameter savings through pruning. Using fan-in K = 4 yields models with the same number of parameters as ResNeXt but higher accuracy. Using fan-in K = 1 results in models with the same number of parameters as ResNeXt but higher accuracy. Real-valued masks were also explored, but found to be less effective compared to binary masks. Different architectures were trained using predefined full connectivity of ResNeXt (Fixed-Full) versus connectivity learned by the algorithm (Learned), with models trained using random, fixed connectivity (Fixed-Random) as well. Our method reports the best test performance and mean test performance from 4 runs, using K = 1 and K = 4. The connectivity learned with K = 4 shows up to 2.2% accuracy gains compared to ResNeXt, while K = 1 yields similar results but with a 40% reduction in parameters at test time. Real-valued masks yield lower accuracy compared to binary masks, with a difference of 1.93% for a specific model. Visualization of the connectivity learned by MASKCONNECT for K = 1 versus the fixed connectivity of ResNeXt for model {D = 29, w = 8, C = 8}. Our algorithm learns different input pathways for each block, resulting in a branching factor that varies along depth. Evaluation on ImageNet dataset shows improved connectivity learning with stochastically-sampled binary masks. In experiments on different architectures, learned connectivity by MASKCONNECT outperforms fixed connectivity of ResNeXt BID31, with accuracy gains of up to 3.8%. Deep networks often require laborious model search for good results, prompting research on automatic model selection algorithms. Most prior work focuses on hyper-parameter optimization rather than architecture. Our method focuses on learning connectivity within a predefined architecture efficiently through gradient descent optimization, unlike evolutionary search or reinforcement learning. Prior methods involve pruning unimportant weights from the network in stages. Our method focuses on learning connectivity within a predefined architecture efficiently through gradient descent optimization. Unlike prior approaches that involve pruning connections based on importance criteria or evolutionary search, our algorithm directly optimizes the loss function for the problem at hand. This approach is similar to the \"Shake-Shake\" regularization method, where tensors produced by parallel branches are randomly scaled during training iterations. However, our algorithm learns an optimal binary scaling of parallel tensors to achieve sparse connectivity in the resulting network at test time. Our algorithm efficiently learns connectivity within a predefined architecture through gradient descent optimization, optimizing over a larger family of connectivity models. It differs from prior methods by optimizing connections at all depths in the network. Adams et al. proposed a nonparametric Bayesian approach for searching over an infinite network, while Saxena and Verbeek introduced learnable 3D trellises for locally connecting response maps in a CNN. In this paper, an algorithm was introduced to learn the connectivity of deep multi-branch networks through joint optimization over weights and branch connections. The approach led to significant accuracy improvements in image categorization benchmarks compared to the state-of-the-art ResNeXt model. It can also automatically identify unnecessary blocks for more efficient testing and parameter reduction. The benefits of this approach are expected to extend to other network structures beyond ResNeXt. The paper introduces an algorithm to learn connectivity in deep multi-branch networks, improving accuracy in image categorization benchmarks. The approach can identify unnecessary blocks for efficiency and parameter reduction. Future work will explore learnable masks for architecture discovery. The paper presents an algorithm for learning connectivity in deep multi-branch networks, enhancing accuracy in image categorization benchmarks. Results show that models with learned connectivity outperform those with fixed connectivity on CIFAR-10. The Mini-ImageNet dataset consists of 100 classes with 600 images each, used for training and testing. Models trained on Mini-ImageNet show improved accuracy with learned connectivity compared to fixed connectivity, with a significant accuracy gain of 3.87% for a 20-layer network. The accuracy improvement with learned connectivity over fixed connectivity is evident in both 20-layer and 29-layer networks. The number of active branches increases in deeper modules, indicating the importance of parallel computation in deep layers. The setting K = 4 produces a uniform number of active branches across modules, close to the maximum value C. Little parameter saving occurs with K = 4 as there are rarely unused blocks. The plot in FIG10 shows active branches vs module depth for model {D = 50, w = 4, C = 32} trained on ImageNet with K = 16. Residual block details are specified inside brackets. The models used in experiments on CIFAR-10 and CIFAR-100 have a common architecture with a convolutional layer, global average pooling, and softmax. Data augmentation includes padding, cropping, and flipping of images. Training involves 8 GPUs, mini-batch size of 128, weight decay of 0.0005, and momentum of 0.9 over 320 epochs in four phases. In the training phases, the model is trained over 320 epochs in four phases with different learning rates and connectivity settings. The architectures used in ImageNet experiments are based on the ResNeXt paper BID31, with varying depth and bottleneck width. For the ImageNet experiments, the model is trained over 120 epochs in four phases with different learning rates and connectivity settings. The input image size is 224x224, with data augmentation and a mini-batch size of 256 on 8 GPUs. Training includes four incremental phases with specific learning rates for different layers and masks. In the ImageNet experiments, the model is trained over 120 epochs in four phases with different learning rates and connectivity settings. The input image size is 224x224, with data augmentation and a mini-batch size of 256 on 8 GPUs. Training includes four incremental phases with specific learning rates for different layers and masks. In phase 1, the model uses convolutional and fully-connected layers with a learning rate of 0.2 for the masks. Subsequent phases involve finetuning the model with different learning rates for weights and masks. The experiments on the Mini-ImageNet dataset involve randomly sampling a 64x64 crop from the scaled 84x84 image or its horizontal flip for training, and using the center 64x64 crop for testing. The model specifications are similar to CIFAR-100 models, with the addition of a max pooling layer after the first input convolutional layer. The model is trained on 8 GPUs with a mini-batch size of 256 (32 per GPU), using a weight decay of 0.0005 and momentum of 0.9. Training involves four incremental phases with a total of 320 epochs, similar to the CIFAR-100 dataset."
}