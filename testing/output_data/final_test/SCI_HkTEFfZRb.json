{
    "title": "HkTEFfZRb",
    "content": "Neural networks with low-precision weights and activations offer efficiency advantages like reduced memory consumption and faster forward pass. A third benefit is improved robustness against adversarial attacks, with performance comparable to full-precision models. Stochastic quantization of weights can reduce the impact of attacks. Binary neural networks show similarities to defensive distillation, leading to gradient masking and a false sense of security. Black-box and white-box experiments are conducted to address this issue. The ability to deceive machine learning models by making small changes to their input limits their safe use in real-world scenarios. Threat models include black-box and white-box assumptions, representing different levels of adversary information. In a black-box threat model, adversaries interact with a system without knowing model details or training data. In a white-box threat model, an adversary has complete access to the model architecture and parameters. White-box attacks on neural networks rely on gradient information to craft strong adversarial examples, causing the model to yield incorrect outputs. Targeted attacks, like making a '7' look like a '3', are more potent than simple misclassification. Machine learning systems face stronger attacks like adversarial examples that can compromise secured models. Adversarial attacks can generalize between different models, requiring consideration of threat models. Progress has been made in training low-precision neural networks for deployment on hardware. The current motivation for extreme quantization is to deploy models under hardware constraints, with 32\u00d7 compression possible using 1-bit for single-precision floating point parameters. Very low-precision models are designed for deployment in embedded systems, including critical decision-making in applications like autonomous navigation and robotics. Understanding how these models behave in adversarial settings is crucial for security. The study evaluates the robustness of binary neural networks to adversarial attacks on MNIST and CIFAR-10 datasets. It compares low-precision neural networks for defense strategies and recommends an optimal defense strategy. The research aims to generalize the suitability of low-precision neural networks against various attack classes. The study focuses on defending against adversarial attacks by improving the robustness of the model through a proactive approach. This involves adding an extra class for malicious inputs, aiming to make reliable decisions consistently. The measurement of distance between perturbed and natural inputs is crucial, especially in the visual domain where human perceptual similarity is important. The study evaluates different distance metrics for adversarial attacks in the visual domain, focusing on the linearity hypothesis as a possible explanation for the existence of adversarial examples in neural networks. The study explores the linearity hypothesis in neural networks, showing that small weights and activations can lead to vulnerability to adversarial attacks. Adversarial training is a successful defense mechanism based on the universal approximation theorem, aiming to learn functions that resist such attacks. The fast gradient sign method (FGSM) is a key technique in this defense strategy. The fast gradient sign method (FGSM) is a procedure for crafting damaging noise to create adversarial examples. Binarized neural networks are considered a natural defense against such attacks due to their non-linearity and difficulty to attack with iterative procedures. Black-box attacks have relaxed assumptions on the information required by an effective adversary. Adversarial attacks have become more damaging with just a small set of input-output pairs. Adversarial examples generalize well between models with different architectures and datasets. Successful black-box attacks involve training a substitute model to mimic the Oracle's decision boundary without access to original training data. BID16 demonstrated the ability to compromise machine learning models 80% of the time on small datasets like MNIST using shallow MLP-based substitute models. Despite not achieving great results on the original task, a simple substitute model can compromise the Oracle. This technique can overcome gradient masking approaches and attacks generated using the substitute model do not transfer well with strong adversarial training of the model to be defended. The text discusses the use of iterative attacks in adversarial examples, specifically focusing on an attack using the Adam optimizer that outperforms other optimization-based approaches. The quantization scheme applied to a convolutional neural network is also depicted, with weights and activations retained in single-precision floating point in the first layer. The text discusses the quantization scheme applied to a convolutional neural network, where weights are binarized either deterministically or stochastically. Stochastic quantization of weights at test time is used as a defense against iterative attacks. The weights are sampled from a Bernoulli distribution and mapped to \u00b11. The training process is not significantly slowed down, but takes 3-4\u00d7 more epochs to converge compared to deterministically quantized networks. The straight through estimator is used to back-propagate gradients through the quantization step. The text discusses the quantization scheme applied to a convolutional neural network, where weights are binarized either deterministically or stochastically. Gradients are back-propagated through the quantization step using a small tunable scalar after ReLU in hidden layers. Convolution kernels are initialized from a truncated normal distribution and quantized to \u00b11 in the forward pass. Batch normalization is applied before quantizing activations to ensure they are centered around zero. Test error rates for these models on MNIST with varying capacity are reported in Table 6 of Appendix A. Models were trained for 15 epochs unless indicated otherwise. The text discusses the quantization scheme applied to convolutional neural networks, where models with full-precision weights and activations under-fit the data less than binary equivalents. Different types of adversarial training were experimented with, including FGSM and PGD. The model's own best prediction is used as the true label in adversarial training. In adversarial training, experiments were conducted using single step attacks in a whitebox setting and scaling up to stronger attacks. All experiments were done in TensorFlow using CleverHans BID15 or Foolbox BID18. Attacks were clipped to the input range, and accuracy was reported on perturbed test sets after a fixed number of iterations. The FGSM attack is a simple but effective single-step attack that approximates the gradient of the loss used to train the model. To confer robustness, adversarial training involves sampling unique values for each training example from a truncated normal distribution with a standard deviation set to \u03c3 = ceil(max * 255/2). The standard deviation for adversarial training is set to \u03c3 = ceil(max * 255/2), with max typically up to 0.3. Table 1 shows accuracy results for different models on adversarial examples generated with a FGSM attack on the MNIST test set. A plain binary network without adversarial training achieves the best robustness, with nearly 90% accuracy for = 0.1. The results show that the binary models did not perform well when trained with an adversary and tested on larger values of epsilon. The binary weight implementation leads to polarization of real valued weights, resulting in fewer sign changes during convergence. Regularization schemes encouraging weights to polarize around \u00b11 were not found to be helpful after testing various settings. The binary models did not benefit much from adversarial training, especially after testing different regularization settings. Adversarial training with binary models is a delicate balance, as introducing a strong adversary too early can hinder convergence for natural inputs. However, the scaled binary model showed significant benefits from adversarial training and achieved accuracy comparable to full-precision models. The scaled binary model outperforms its full-precision equivalent in robustness against attacks. Adversarial training with binary models is challenging, but the scaled binary model benefits significantly from it, achieving accuracy comparable to full-precision models. The Carlini-Wagner L2 attack produces strong adversarial examples by minimizing distortion and manipulating logits. In the study, binary models outperformed full-precision models in robustness against attacks. The best full-precision model achieved 1.8\u00b10.9% accuracy, while stochastically quantized binary models had test error rates of 8\u00b11%, 5\u00b12%, and 3\u00b11% for different scaling factors. Binary and full-precision models performed similarly in the early iterations of the attack, but full-precision models quickly dropped in accuracy after 10-20 iterations. The study found that binary models outperformed full-precision models in robustness against attacks. The binary model benefited significantly from adversarial training at 10 to 100 attack iterations, while the full-precision model did not. However, after 1000 iterations, both models had similar accuracy levels. The targeted attack to 1000 iterations was much more time-consuming than training the models from scratch. The study found that binary models outperformed full-precision models in robustness against attacks. A targeted attack to 1000 iterations was time-consuming compared to training models from scratch. The substitute model training procedure was run using CleverHans v2.0.0 on MNIST and CIFAR-10 datasets with and without FGSM adversarial training. The substitute model used a two-layer MLP with 200 hidden units and ReLU activations, trained on 150 images withheld from the test set. Data augmentation with \u03bb = 0.1 and 10 substitute model training epochs were used after each augmentation step. The oracle was trained for 15 epochs for MNIST and 20 epochs for CIFAR-10. Results for the black-box experiment on the MNIST dataset are shown in TAB3. Full-precision networks had a moderate advantage over undefended binary models B and C in the black-box experiment on the MNIST dataset. PGD was more effective than stochasticity in learning an optimal decision boundary for both A and C models. BNNs operate with larger range and variance due to convolving inputs with greater magnitude compared to 'normal' networks. The logits in a 64 kernel CNN were 4\u00d7 larger than in full-precision networks, leading to gradient masking that causes FGSM and JSMA to fail. This defense can be defeated with a close guess for the softmax temperature or a black box attack. The gradient masking effect in scaled BNNs gives them an advantage over full-precision models in targeted attacks. Regularization effect of binary units in MLPs helps in better representing the actual function and reduces susceptibility to adversarial examples. Models trained on clean inputs show a significant performance gap when attacked, with sub-optimality of the attack contributing to the difference in accuracy for adversarial vs. clean inputs. The attack's sub-optimality is due to using logits instead of softmax probabilities. Manipulating pairs of pixels in a noisy process in a binarized model hinders achieving the L0 goal. Model 'S' with stochastically quantized weights in its third convolutional layer is successful against iterative attacks. Adversarial examples are not random noise and neural networks are robust to benign noise. An iterative attack faces a unique model at each step, making it difficult to fool the stochastically quantized model. For binarized neural networks, training difficulty hinders attacks. Stochastic quantization reduces impact of strong attacks. Adversarial accuracy on MNIST against CWL2 is 71\u00b12% (S64+). Blackbox results competitive between binary and full-precision models. Binary models were slightly more robust for CIFAR-10 due to improved regularization. Future work will explore other low-precision models and adversarial attack methods. Table 6 shows error rates on clean MNIST test set for models with varying capacity and precision. Adversarially trained models used 20 iterations of PGD BID11. An experiment was conducted on learning the two-input logical AND function with a simple MLP. In an experiment on learning the two-input logical AND function, a 3-hidden-layer MLP was trained with the Adam optimizer for 1k epochs. The middle layer was quantized without affecting the input or output, using a straight through estimator BID1 for backpropagation. MLPs with a single quantized hidden layer showed highly non-linear forward gradients during training. Increasing the number of hidden units provided more capacity for learning. When the MLP was given more capacity by doubling hidden units, the forward derivative was mostly destroyed. Comparing full-precision and binary networks under FGSM perturbation, a flip in adversarial example direction was observed for binary networks with softmax temperature between 0.6-0.7. Full-precision logits respond linearly to scaling, while binary logits show little change except for a 180 degree flip. The piecewise linear effect from the range of actual attacks conducted in the paper is still present for values with large absolute value."
}