{
    "title": "HklJV3A9Ym",
    "content": "This paper extends the proof of density of neural networks in the space of continuous functions on Euclidean spaces to functions on compact sets of probability measures. The work has practical implications for multi-instance learning, supporting some recently proposed constructions. The universal approximation theorem for tree-structured domains, such as JSON, XML, YAML, AVRO, and ProtoBuffer, allows for the automatic creation of neural network architectures for processing structured data. This has practical implications for AutoML paradigms, as demonstrated by a library for JSON format. The structure of data in tree formats like JSON, XML, YAML, Avro, or ProtoBuffer presents challenges, especially with unordered lists of records. Handling the variability in length of these lists is crucial for classifiers, resembling Multiinstance learning (MIL) problems. In Multiinstance learning (MIL) problems, a sample is defined as a collection of feature vectors, with each collection having the same dimension but varying in the number of vectors. The order of instances in the bag is not important, and the classifier output should be the same for any permutation of instances. MIL was introduced as a solution for learning a classifier on instances from labels available at the bag level. Various approaches have been proposed to address this problem, with the assumption that a bag is considered positive if at least one instance is positive. In Multiinstance learning (MIL) problems, a sample is viewed as a probability distribution observed through a set of realizations of a random variable. Independent works have proposed an adaptation of neural networks to MIL problems, where two feed-forward neural networks are used to classify probability measures. The MIL NN algorithm has been widely used in various applications such as causal reasoning, computer vision, medicine, and network traffic analysis. It can handle data with a hierarchical structure and has a universal approximation property. The MIL NN algorithm is widely used in various applications and has a universal approximation property. MIL NNs with two non-linear layers, a linear output layer, and mean aggregation are proven to be dense in the space of continuous functions from probability measures to real numbers. The proof is extended to data with a tree-like schema. The paper formalizes the adaptation of kernel embedding tools to neural networks for MIL and tree-structured data. Theoretical results are provided, with a proof of concept demonstration for processing JSON data. The section also discusses extensions of the universal approximation theorem and solutions to multiinstance learning problems. The text discusses solutions to multiinstance learning problems in the context of continuous functions and metric spaces. It introduces the space of probability measures on a compact metric space and explores different metrics to define compact metric spaces. The results presented are general and not tied to any specific topology. The main result states that neural networks can approximate continuous functions. Any non-polynomial measurable function can be approximated by neural networks in the space of continuous functions. In Multi-instance learning, samples consist of multiple vectors of fixed dimension. Labels are provided at the sample level rather than individual instances. To adapt neural networks to MIL problems, a specific construction has been proposed. In Multi-instance learning, samples consist of multiple vectors of fixed dimension. To adapt neural networks to MIL problems, a construction has been proposed in BID19; BID5. The network consists of two feed-forward neural networks \u03c6. The output is calculated based on the mean aggregation function, with the option to use other functions like maximum. The MIL problem is viewed as instances of a random variable with distribution p \u2208 P X. The main result of Section 3 shows that neural networks with specific non-linear layers can be used. Neural networks with specific non-linear layers can be used in Multi-instance learning problems. The theoretical analysis assumes functions of a certain form, while in practice, observations are limited. The sample can be interpreted as a mixture of probability measures, allowing for estimation of the function. If the non-linearities are continuous, the function is bounded, and Hoeffding's inequality can be applied to estimate the function. The function f is bounded, and Hoeffding's inequality implies that the probability of the difference between f(p) and f(x) being greater than t is bounded. A set of functions is introduced to embed probability measures into R, extending Theorem 1. This set can be viewed as an analogy of affine functions for probability measures on X. By using this set, \u03a3-networks can be extended to probability spaces. The main theorem states that three-layer neural networks with non-linear layers and an integration layer can approximate continuous functions on a compact set of Borel probability measures. This class of networks is dense in the space of continuous functions. Additionally, a corollary shows that a similar result holds for measurable functions, where the set of functions is dense in the space of finite Borel measures on X. The proof of Theorem 2 involves the Stone-Weierstrass theorem, which states that a collection of functions is an algebra if it is closed under multiplication and linear combinations. Since \u03a3(\u03c3, A F ) is not closed under multiplication, the density of the class of \u03a3\u03a0 networks is first proven, which does form an algebra, before extending the result to \u03a3-networks. The proof of Theorem 3 involves showing that a set of functions is dense in a space of probability measures. By using a corollary of a lemma, it is shown that the set of functions satisfies the separation and non-vanishing properties required by the Stone-Weierstrass theorem. The proof of Theorem 3 involves demonstrating the density of a set of functions in a space of probability measures. By satisfying the separation and non-vanishing properties, the set of functions meets the requirements of the Stone-Weierstrass theorem. This is supported by a lemma corollary. The proof of Theorem 2 relies on Theorem 3 and the density of \u03a3-networks in C(R^k, R) for any k. The function h acts as a feature extraction layer embedding probability measures into a Euclidean space, similar to the meanmap in kernel machines. The curr_chunk discusses an extension of the universal approximation theorem to product spaces, focusing on cases where positive and negative samples differ. It covers scenarios where X i are compact sets of probability measures, subsets of Euclidean spaces, or general compact spaces. The theorem is general and not limited to algebraic polynomials. The theorem extends the universal approximation theorem to product spaces, considering cases with different positive and negative samples. It applies to compact sets of probability measures, subsets of Euclidean spaces, and general compact spaces. The proof involves using a corollary of the Stone-Weierstrass theorem to show that a set of functions is dense in C(X i , R). The corollary of Theorem 2 justifies the embedding paradigm of BID28, BID5, BID19 to MIL problems by stating that any function in C(P X, R) can be approximated closely by a three-layer neural network. This network consists of two non-linear layers with an integral aggregation layer between them, and a linear output layer. The theorem also shows that for every f in C(P X, R) and a given epsilon, there exists a representation for f in a specific form, where bias vectors are omitted, and matrices are used for calculations. Theorem 5 states that in a class of spaces containing compact subsets of R^d, closed under finite cartesian products, and with continuous functions, every function on X can be approximated by a neural network with three non-linear layers and one linear layer. Theorem 5 shows that functions on certain spaces can be approximated by neural networks. The proof involves showing that a class of spaces is contained within another class where neural network functions are dense. This work is related to kernel mean embedding, which embeds probability measures into high-dimensional spaces. The MIL problem has been studied in BID26 proposing to use a LSTM network augmented by memory for problems with tree structures, typically solved by recurrent neural networks. The reduction from sets to vectors is indirect by computing a weighted average over elements in an associative memory, making aggregation an integral part of the architecture. The paper lacks approximation guarantees, and the difference lies in the assumption that the tree can have an arbitrary structure. The authors propose a novel approach that generalizes the universal approximation theorem of neural networks to compact sets of probability measures over Euclidean spaces. This adaptation of the mean-map framework to neural networks is important for comparing probability measures and multi-instance learning. The universal approximation theorem is extended to inputs with a tree schema, justifying applications of neural networks in this setting. The proof relies on the Stone-Weierstrass theorem, restricting non-linear functions to be continuous in all but the last layer. Generalizing the result to non-continuous non-linearities would be interesting."
}