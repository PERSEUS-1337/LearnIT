{
    "title": "rywHCPkAW",
    "content": "NoisyNet is a deep reinforcement learning agent with parametric noise added to its weights, aiding efficient exploration. By replacing conventional exploration heuristics with NoisyNet, higher scores in Atari games are achieved, advancing the agent's performance. Optimism in exploration is a common heuristic in reinforcement learning, but traditional methods have limitations in complex environments. A structured approach involves adding intrinsic motivation to the reward signal to encourage novel behaviors. Adding intrinsic motivation to the reward signal can encourage novel behaviors, but methods like learning progress and prediction gain may separate generalization from exploration. The weighting of intrinsic reward relative to environment reward must be chosen by the experimenter, potentially altering the optimal policy. Dithering perturbations and exploration in the policy space may also be necessary for robust exploration. NoisyNet is a simple alternative approach to exploration in reinforcement learning that uses perturbations of network weights to drive exploration. Unlike traditional algorithms, NoisyNet induces consistent, state-dependent changes in policy over multiple time steps by sampling perturbations from a noise distribution. The variance of the perturbation can be seen as the energy of the injected noise. The approach of NoisyNet involves learning variance parameters for injected noise using gradients from the reinforcement learning loss function. Unlike other schemes like variational inference and flat minima search, NoisyNet does not maintain an explicit distribution over weights but injects noise in parameters. This randomised value function uses a neural network and provides efficient exploration without necessarily converging to a posterior distribution. NoisyNet approach involves injecting noise into network parameters to enable efficient exploration in reinforcement learning. Unlike other methods, NoisyNet does not maintain a weight distribution but adapts noise injection over time, allowing for flexibility beyond Gaussian noise. NoisyNet introduces noise injection into network parameters for efficient exploration in reinforcement learning. It differs from other methods by adapting noise over time, offering flexibility beyond Gaussian noise distributions. NoisyNet can be applied to various deep RL algorithms, such as DQN and A3C, showing significant performance improvements with minimal computational cost and fewer hyperparameters to tune. This section provides a mathematical background on Markov Decision Processes (MDPs) and deep reinforcement learning (RL) with Q-learning, dueling, and actor-critic methods. MDPs model stochastic, discrete-time, and finite action space control problems. An MDP is defined by a tuple M = (X, A, R, P, \u03b3), where X is the state space, A is the action space, R is the reward function, \u03b3 is the discount factor, and P is a stochastic kernel modeling the one-step Markovian dynamics. A stochastic policy \u03c0 maps states to action distributions, and the quality of a policy is assessed by the action-value function Q\u03c0. The quantity Q \u03c0 (x, a) represents the expected cumulative reward collected by executing a policy starting from x and a. An optimal policy maximizes the return, with the action-value function Q (x, a) determining the best policy. Value function V \u03c0 represents the expected return from executing a policy starting from state x. Deep Reinforcement Learning utilizes deep neural networks for RL methods like Deep Q-Networks (DQN), Dueling architecture, Asynchronous Advantage Actor-Critic (A3C), Trust Region Policy Optimization, and Deep Deterministic Policy Gradient. In our experiments, we consider DQN, Dueling, and A3C algorithms for reinforcement learning. DQN uses a neural network to approximate the action-value function Q(x, a) by minimizing a loss function with respect to network parameters \u03b8. The target network's parameters \u03b8- are updated regularly to stabilize learning. The Dueling DQN BID24 is an extension of the DQN architecture, using Dueling network architecture with two parallel sub-networks to estimate the action-value function. The algorithm utilizes the double-DQN update rule to optimize parameters \u03b8. A3C is a policy gradient algorithm that directly learns a policy \u03c0 and a value function V. It optimizes the advantage function and the entropy of the policy using a hyperparameter \u03b2. The advantage function is the difference between observed returns and estimates produced by the value network. The parameters of the value function match on-policy returns. NoisyNets are neural networks with weights and biases perturbed by noise, adapted with gradient descent. The noisy parameters are represented as a vector of learnable parameters plus noise, optimizing the policy loss relative to the baseline value function loss. The neural network is optimized with respect to parameters \u03b6, using noisy linear layers with Gaussian noise distributions. Two options explored are Independent Gaussian noise per weight and Factorised Gaussian noise per input and output. The neural network is optimized using factorised Gaussian noise for DQN and Duelling to reduce compute time, while independent noise is used for distributed A3C. Independent Gaussian noise is applied to each weight and bias independently, while factorised Gaussian noise uses unit Gaussian variables for noise of the inputs. In our experiments, we factorise \u03b5 w i,j to use Gaussian variables for noise in inputs and outputs. Gradients for noisy networks are obtained easily as an expectation over noise. We apply a Monte Carlo approximation for these gradients. Noise is crucial for exploration in deep reinforcement learning methods. In deep reinforcement learning, noisy networks are used to automatically adjust the level of noise for exploration in agents. This approach involves sampling new parameters after each optimization step to ensure the agent acts according to the current noise distribution.\u03b5-greedy is replaced with a policy that greedily optimizes. NoisyNet-DQN and NoisyNet-Dueling are adaptations of DQN and Dueling that use noisy networks to parameterize the value network. The noisy network parameters are re-sampled before every action, and factorized Gaussian noise is used for replay. The loss function minimized by this variant of DQN involves replacing linear layers with noisy layers in the network. NoisyNet-DQN and NoisyNet-Dueling use noisy networks to parameterize the value network in DQN and Dueling. The loss function involves replacing linear layers with noisy layers, generating independent noises to avoid bias, and acting greedily with respect to the output action-value function. The loss function for NoisyNet-Dueling and A3C involves modifying the policy network with noisy layers for exploration. A3C does not use an explicit exploratory action selection scheme and instead samples noisy parameters for policy updates. NoisyNet-A3C is a modification of A3C that replaces linear layers with noisy linear layers for exploration. Gradients in A3C are unbiased when noise is consistent throughout the rollout. Additional details can be found in the appendices. NoisyNet-A3C replaces linear layers with noisy linear layers for exploration in A3C. Parameters \u00b5 and \u03c3 are initialised for noisy networks. Each element \u00b5 i,j is sampled from uniform distributions, and each element \u03c3 i,j is set to 0.017. This initialisation was chosen based on previous successful supervised learning tasks. No tuning was done for this parameter, but different values on the same scale should yield similar results. Factorised noisy networks have a different initialisation method. Performance of noisy network agents was evaluated on 57 Atari games and compared to baselines using original exploration methods. The study evaluated the performance of agents using original exploration methods like \u03b5-greedy and entropy bonus. Three baseline agents were considered: DQN, Dueling algorithm BID24, and A3C. DQN and A3C agents were trained for 200M and 320M frames respectively, using neural network architectures from original papers. NoisyNet variants used the same hyperparameters as the baselines. Performance was compared using human evaluation. The study compared agent performance using human-normalized scores across 57 Atari games. Maximum scores were computed per game by averaging raw scores over three seeds. Results were reported in tables and showed percentage improvement over the baseline. NoisyNet agents (DQN, Dueling, and A3C) outperform corresponding baselines in most games, with significant improvements in some cases. Learning curves demonstrate superior performance throughout the learning process. NoisyNet agents show order of magnitude improvement in some games compared to vanilla agents. Detailed breakdown of individual game scores and learning curves provided in the appendix. In this subsection, the study analyzes the impact of noisy networks on the learning process and exploratory behavior of agents. The evolution of noise weights \u03c3 w and \u03c3 b is examined, showing that factorized noise does not decrease A3C performance significantly. Positive effects are observed in terms of improving median score and speeding up learning. The existence of a deterministic optimizer for the loss function L(\u03b6) is highlighted. The study examines the impact of noisy networks on agent learning and behavior. It is noted that factorized noise does not significantly decrease A3C performance. The existence of a deterministic optimizer for the loss function is highlighted, with the neural network learning to discard noise entries by pushing \u03c3 w s and \u03c3 b towards 0. The mean-absolute of noisy layer weights, \u03a3, is tracked to measure stochasticity. Results show a decrease in \u03a3 for the last layer but mixed results for the penultimate layer in NoisyNet-DQN across Atari games. The study explores the impact of noisy networks on agent learning and behavior in deep reinforcement learning. NoisyNet-DQN shows varied evolution and problem-specific exploration strategies across different Atari games, outperforming standard DQN, Dueling, and A3C in games like Beam rider, Asteroids, and Freeway. The agent does not necessarily evolve towards a deterministic solution, indicating a significant performance improvement with the use of noisy networks. NoisyNet-Dueling and NoisyNet-A3C achieve superhuman performance, with the uncertainty in network parameters serving as the sole exploration mechanism. The amount of noise in the network is automatically adjusted by the RL algorithm, eliminating the need for hyperparameter tuning. This contrasts with other methods that may destabilize learning or alter the optimal policy. Further analysis is required to separate exploration and optimization effects. The NoisyNet approach utilizes intrinsic motivation signals to adjust exploration levels based on per-weight variances. The computational overhead is minimal due to an efficient affine function linking mean and variance gradients. Implementation is straightforward with automatic differentiation. This exploration strategy can be applied to various deep RL algorithms, not limited to the baselines discussed. The NoisyNet approach uses intrinsic motivation signals for exploration in deep RL algorithms like A3C, which parameterizes the policy directly and updates parameters through gradient ascent on the mean value-function. The A3C algorithm uses roll-outs to improve the policy and train the value-head by minimizing the error between estimated return and value. Network parameters are updated after each roll-out using hyper-parameters. An entropy term is recommended to be added to the policy in the original A3C algorithm. The A3C algorithm recommends adding an entropy term to encourage exploration in policy updates. Noisy layers can be used to estimate returns via roll-outs, with network noise fixed for consistency in policy updates. In this Appendix, a graphical representation of a noisy linear layer is provided, where parameters \u00b5 w, \u00b5 b, \u03c3 w, and \u03c3 b are the learnables of the network and \u03b5 w, \u03b5 b are noise variables. The noisy layer perturbs both the weights vector and bias with parametric zero-mean noise. A3C algorithm includes factorised and non-factorised variants, showing percentage improvement on the baseline in terms of median human-normalised score."
}