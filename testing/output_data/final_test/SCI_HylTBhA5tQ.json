{
    "title": "HylTBhA5tQ",
    "content": "The effectiveness of adversarial training in defending against adversarial examples in deep neural networks is strongly correlated with the distance between test points and the manifold of training data. Test examples far from this manifold are more vulnerable to attacks, leading to \"blind-spot attacks\" where input images are in low-density regions of the training data distribution. These blind-spots can be easily identified in MNIST by scaling and shifting image pixel values. Blind-spots in adversarial training can be found by scaling and shifting image pixel values, making defending against valid test examples difficult for large datasets like CIFAR and ImageNet. Blind-spots also exist in provable defenses due to limited training data optimization. Adversarial training is effective in defending against adversarial examples in deep neural networks without relying on obfuscated gradients. Adversarial training, using projected gradient descent (PGD), effectively defends against adversarial examples on small datasets like MNIST and CIFAR-10. Results show significant defense on MNIST, reducing accuracy from 98% to 88% against white-box attacks. However, on CIFAR-10, a simple 20-step PGD can decrease accuracy from 87% to less than 50%. The effectiveness of adversarial training is measured by the robustness on the test set, but the process is done on the training set. If the empirical distribution of the training dataset differs from the true data distribution, a test point from the true distribution might not be covered by the adversarial training procedure. Adversarial training is more successful on simple, low-dimensional datasets like MNIST and Fashion MNIST compared to high-dimensional datasets like CIFAR and ImageNet. The effectiveness of adversarial training is limited, as shown by recent attacks using generative models to find \"blind-spots\" in the input space not covered by training data. This results in only moderate robustness, especially for higher dimensional datasets like CIFAR and ImageNet. The distance of test images to the training dataset plays a role in the success of adversarial training. Our paper explains the success and limitations of robust optimization based adversarial training. We show that the effectiveness of adversarial training is highly correlated with the distance from test images to the training data manifold. A new class of attacks called \"blind-spot attacks\" is identified, where the input image is far from any training data distribution. Blind-spots in the data distribution can be exploited by adversarial attacks, even on strong defense models like BID32 and BID24. Simple transformations like changing contrast and background can make adversarially trained models vulnerable to attacks without affecting accuracy. This poses a challenge for robust learning and highlights the limitations of current adversarial training methods. Adversarial training struggles with datasets of large intrinsic dimensions, leading to limited success on ImageNet. Adversarial examples pose threats to AI applications like autonomous driving and face recognition. Various defense methods have been proposed but some are proven ineffective against strong attacks. Some defense methods against adversarial attacks have been proven vulnerable. Gradient masking and randomization in DNNs are used but have limitations. Adversarial training, introduced by Goodfellow et al., shows state-of-the-art defending performance. The text discusses a robust optimization problem achieving state-of-the-art defending performance on MNIST and CIFAR datasets. Various attacks have been proposed, with the best attack reducing test accuracy from 98% to 88%. Adversarial training framework increases model robustness without relying on obfuscated gradient. Attacks using generative models have also been explored. Certified defense methods have been proposed to increase model robustness. In addition to adversarial training, certified defenses are considered in the paper to improve model robustness. BID24 and BID32 propose robustness certificates for ReLU networks, ensuring training increases robustness. Certified defense methods guarantee robustness during training but struggle to generalize to unseen test examples. The arms race between attacks and defenses has led to insights into adversarial examples. Recent research has focused on understanding adversarial examples, showing that robust generalization requires more samples than standard generalization. The PAC learning theory has been extended to include adversaries, with the adversarial VC-dimension being a key concept. It is conjectured that finding a robust classifier is computationally hard, and there are computational hardness results under different models. Additionally, the prevalence of adversarial examples is linked to the \"concentration of measure\" phenomenon. The BID27 and BID30 experiments on ImageNet show a negative correlation between robustness and accuracy. Adversarial training focuses on finding robust features that correlate strongly with labels. Certified defenses improve robustness on training data, but the generalization property of trained models is still unknown. Evaluation of model robustness involves computing an upper bound of error on the test set. Some test images may still be correctly classified by DNNs and recognized by humans. Our paper shows a strong correlation between the effectiveness of adversarial defenses and the distance between training data and test points. A small shift in input distribution can easily compromise the robustness of a model. Defining a meaningful distance metric for high dimensional image data remains a challenging problem. Defining a distance metric for high dimensional image data is challenging. Using methods like (kernel-)PCA, t-SNE, or UMAP to reduce dimensionality works for simple datasets like MNIST, but for complex datasets like CIFAR, it's difficult. Deep Neural Networks (DNN) perform better in capturing image data manifold for distance measurement compared to traditional methods like PCA or t-SNE. Proposing a distance metric using deep feature embeddings and k-nearest neighbor for high dimensional image data. This non-parametric metric averages the embedding space distance of k nearest neighbors in the training dataset. The study proposes a non-parametric distance metric using deep feature embeddings and k-nearest neighbor for high dimensional image data. Results show that the metric is not sensitive to the selection of k and reveals similar correlations with the effectiveness of adversarial training for different feature extractors. The research also explores the \"distance\" between training and test datasets to understand how adversarial training performs on the entire test set. This involves computing a divergence between two empirical data distributions and using t-SNE for non-linear projection to a low dimensional space. The study explores a non-parametric distance metric using deep feature embeddings and k-nearest neighbor for high dimensional image data. It investigates the correlation between the effectiveness of adversarial training and the distance between test images and the training dataset. This involves computing KDE density functions and K-L divergence for each class separately. The research identifies a new class of adversarial attacks called \"blind-spot attacks\" where input images are far from any existing training examples but still drawn from the ground-truth data distribution and classified correctly by the model. Blind-spot images, not adversarial themselves, can be exploited to find adversarial examples with small distortions despite adversarial training. These blind-spots are prevalent and easily found without complex generative models. A simple transformation involving scaling and shifting pixel values can reveal blind-spots in defense models like BID32 and BID24 for the MNIST dataset. By scaling and shifting pixel values, blind-spot images are transformed to adjust contrast or add a gray background. Carlini & Wagner's attacks are then performed on these transformed images to find adversarial examples. The models maintain high accuracy with appropriate scaling and shifting parameters. Blind-spot attacks can easily find adversarial examples even with models that have good generalization capability. Adversarial training may not scale well to high dimensional datasets, leading to blind-spots in about 50% of test images in CIFAR-10. Data augmentation can help reduce blind-spots, but it is impossible to eliminate all potential inputs due to the curse of dimensionality. In this section, experimental results on adversarially trained models are presented for MNIST, Fashion MNIST, and CIFAR-10 datasets. Different models are used for each dataset, with specific parameters to ensure model accuracy. Blind-spot attacks are utilized to find blind-spot images, and Carlini & Wagner's attack is used to identify adversarial examples. In experiments on adversarially trained models for MNIST, Fashion MNIST, and CIFAR-10 datasets, Carlini & Wagner's attack is used to find blind-spot images and adversarial examples with smaller perturbations than projected gradient descent. Attacks are initialized using two schemes to avoid gradient masking, and successful attacks are defined by changing the model's classification with a distortion less than a specified threshold. The connection between attack success rate and the distance between test examples and the training set is explored. The distance between test examples and the training set is measured using a defined metric. Different neural feature extractors are used for MNIST, Fashion-MNIST, and CIFAR datasets. The results are shown in figures, with test data points binned based on their distances to the training set. Attack success rates are calculated for correctly classified images only. The success rates of attacks on correctly classified images are analyzed based on the distance between test examples and the training set. The trend shows that attacks on adversarially trained networks tend to concentrate on the right side of the distance distribution, with higher success rates as the distance increases. This correlation supports the hypothesis that adversarial training is less effective on test points that are farther from the training dataset. The K-L divergence between KDE distributions of training and test sets is calculated to measure the distance. Adversarially trained networks show higher failure rates on test points far from the training data distribution. The average normalized distance between test points and training set is also considered, with feature representation normalized for comparison across datasets. Fashion-MNIST dataset shows the strongest defense against attacks, with the smallest K-L divergence. CIFAR has a significantly larger divergence between training and test sets, with limited success in adversarial training. MNIST falls between Fashion-MNIST and CIFAR in terms of robust model training difficulty. Adversarial training effectiveness is not dependent on accuracy, with Fashion-MNIST being harder to classify but easier to train robust models due to a more concentrated data distribution. The proposed blind-spot attack is applied to MNIST and Fashion-MNIST in this section. The blind-spot attack was proposed for MNIST and Fashion MNIST datasets. The attack involved scaling and shifting test set images to create new natural images, and then using the C&W \u221e attack to craft adversarial images. Different scaling factors were tested for both datasets, with varying success rates reported. The attack success was determined by the \u221e perturbation being less than the scaling factor. Blind-spot attacks on Fashion-MNIST and MNIST data involved scaling and shifting test set images to create adversarial examples. The transformation did not affect the models' test accuracy, with the adversarially trained model classifying the slightly altered images accurately. The scaled and shifted images performed well in tests, with high attack success rates compared to original images. Adversarial perturbations were minimal, making it hard to distinguish transformed images from originals. The proposed scale and shift transformations move test images into blind spots, making it difficult to detect blind-spot attacks by observing distances to the training dataset. Despite tiny differences in distance histograms, the robustness property drastically changes on transformed images. The effectiveness of adversarial training is highly correlated with dataset characteristics. The effectiveness of adversarial training is closely tied to dataset characteristics. Blind-spot attacks, which target data points far from the training distribution, pose a challenge even with adversarial training. A scale-and-shift scheme was proposed for successful blind-spot attacks on MNIST and Fashion MNIST datasets. The choice of k in k-nearest neighbor analysis did not significantly impact the results. In Figures 6, 7, and 8, attack success rates on the CIFAR dataset show a correlation with the distance from test points to the training dataset. The adversarially trained CIFAR model's attack success rates and distance distribution are analyzed for k = 10, 100, and 1000. The German Traffic Sign (GTS) BID12 dataset is also studied with similar model training. The German Traffic Sign (GTS) BID12 dataset is analyzed using the same model structure and parameters as the adversarially trained CIFAR model. Attack success rates are higher when the distances between test examples and the training dataset are larger. Experimental results on other defense methods are also demonstrated. The methods BID32 and BID24 provide formal certification on model robustness and improve robustness on the training dataset, but do not guarantee robustness on test data. Results on CIFAR and blind-spot attacks on MNIST and Fashion-MNIST are shown for these defenses. Table 5 shows blind-spot attacks on MNIST and Fashion-MNIST for robust models by BID24, using 2 distortion as the threat model. Different scaling and shifting parameters are applied to input images, with corresponding adversarial examples found. Adversarial perturbations have small distortions for transformed images."
}