{
    "title": "HJx7uJStPH",
    "content": "Source separation for music involves isolating stems from different instruments recorded individually to form a song. The best models use masks on the magnitude spectrum, like Demucs, which utilizes a convolutional autoencoder with a bidirectional LSTM and skip-connections. Compared to Wave-U-Net, Demucs achieves superior performance. Our approach utilizes transposed convolution layers, gated linear units, and careful weight initialization. Results on the MusDB dataset show our model achieves a higher signal-to-distortion ratio compared to competitors, matching state-of-the-art performance. This bridges the gap between spectrogram-based models and end-to-end approaches. Cherry and Bregman's work on auditory signal analysis and segmentation is also referenced. The framework for auditory scene analysis by Bregman led to computational models for source separation in music production. Source separation aims to recover individual stems like drums, bass, vocals, and others from mixed music tracks. This differs from the cocktail party problem as it involves separating a variety of tones and timbres playing together. The SiSec Mus evaluation campaign categorized stems into 4 groups, and the goal is to separate them from the mixed signal. The goal of music source separation is to generate waveforms for individual stems like drums, bass, vocals, and others from a mixed music track. Current approaches use spectrograms from the short-time Fourier transform to create masks for each source, but newer architectures aim to directly synthesize the waveforms. However, these newer methods have not yet reached the performance level of existing techniques. In music source separation, newer models like Conv-Tasnet aim to directly synthesize waveforms for individual stems like drums, bass, and vocals. However, their performance is still below existing techniques using spectrogram masks. The best model achieves an average signal-to-noise ratio of 3.2, compared to 5.3 for spectrogram mask-based approaches. Ideal masks like IRM or IBM set an upper bound on performance. Conv-Tasnet, a model proposed by Luo & Mesgarani (2019), surpasses IRM and IBM oracles in speech separation by learning masks jointly with a convolutional front-end. Adapted for stereophonic music source separation at 44.1 kHz, Conv-Tasnet outperforms previous methods with an SDR of 5.7. However, artifacts like constant noise and hollow instrument attacks are observed in the generated audio. Conv-Tasnet, a model for music source separation, uses an over-complete linear representation with a masking operation from a deep convolutional network. However, artifacts like constant noise and hollow instrument attacks are observed in the generated audio. To address these limitations, Demucs is proposed as a new architecture inspired by music synthesis models, utilizing a U-net architecture with wide transposed convolutions. The approach involves a convolutional encoder and decoder based on wide transposed convolutions inspired by recent work on music synthesis. It includes a bidirectional LSTM, gated linear units as activation function, and a new initialization scheme. Experiments on the MusDB benchmark show that Conv-Tasnet and Demucs outperform other methods on the spectrogram, with Conv-Tasnet performing better in terms of SDR. Human evaluations indicate that Demucs has better perceived quality despite lower SDR due to more contamination from other sources. In an in-depth study of the Demucs architecture, additional experiments with 150 songs added to the training set show that Demucs and TasNet achieve an SDR of 6.3, narrowing the gap between the two models. This new SDR sets a state-of-the-art record, surpassing the previous best result on the MusDB test set. The Demucs architecture, ConvTasnet model, experimental protocol, and results are detailed in subsequent sections. The text discusses methods for supervised music source separation, including traditional blind source separation techniques like non-negative matrix factorization and independent component analysis. It also mentions learning masks over power spectrograms using HMM-based prediction or segmentation techniques. With the development of deep learning, fully supervised methods for music source separation have gained momentum. Initial work was done on speech source separation, followed by works on music using various neural network architectures. Wiener filtering has been shown to be an efficient post-processing step for spectrogram-based models. Top performing models in this category have utilized these methods and performed well in recent evaluations. Recent advancements in music source separation have led to the development of models like MMDenseLSTM and Wave-U-Net, which have shown promising results in the SiSec campaign. While models operating in the waveform domain have shown potential, spectrogram-based models like Demucs and Conv-Tasnet still achieve higher Signal-to-Distortion Ratio (SDR). Additionally, a Wavenet-inspired model has been adapted for source separation tasks, showcasing its versatility in different applications. Our model outperforms Wave-U-Net in speech denoising and source separation tasks. Spectrogram masking methods have shown good performance in monophonic speech source separation. Waveform domain methods using masking have also achieved high accuracy. Improvements have been made in spectrogram methods by incorporating phase reconstruction algorithms. Superposition of dilated convolutions has further improved source separation accuracy. The Conv-Tasnet architecture outperforms spectrogram-based methods for music source separation but has more artifacts compared to the Demucs architecture. It is designed to process waveforms of different sources in the music track and aims to train a model for source separation. The Conv-Tasnet architecture, trained with a scale-invariant source-to-noise ratio loss, uses a front-end to transform input waveforms for source separation. The architecture includes an encoder and decoder with specific kernel sizes and strides, followed by a separation network. The Conv-Tasnet architecture utilizes a separation network with stacked residual blocks for high dimensional representation masking. Each block consists of various convolutional layers and normalization techniques. The final mask estimation is obtained by summing the outputs of all blocks and applying ReLU before being multiplied with the encoder output for music source separation. The Conv-Tasnet architecture was adapted for stereophonic music source separation by increasing kernel size and stride, leading to state-of-the-art performance on the MusDB dataset. When evaluating on entire tracks, splitting input into 8-second chunks yielded the best results due to global layer normalization. The Demucs architecture, designed for stereo music source separation, utilizes global layer normalization to handle quiet and loud parts in entire songs. The architecture includes a convolutional encoder, bidirectional LSTM, and convolutional decoder with skip U-Net connections. Reconstruction loss is discussed in Section 4.2. The architecture for stereo music source separation, Demucs, includes a convolutional encoder with stacked convolutional blocks and skip U-Net connections. Batch normalization is not used as it was found to be detrimental to model performance. The encoder consists of convolutional blocks with specific parameters and gated linear units as activation functions. Convolutions with kernel width 1 increase model depth efficiently. The architecture for stereo music source separation, Demucs, utilizes convolutional encoder with stacked blocks and skip U-Net connections. Kernel width 1 increases model depth efficiently. GLU activations after convolutions boost performance. The number of channels in the input mixture is C=2, with C1=100 for the first encoder block. Channels double at each subsequent block, reaching CL=3200. A bidirectional LSTM with 2 layers and hidden size CL is used. The decoder is the inverse of the encoder, with L blocks starting with a convolution with kernel width 3, input/output channels Ci, and ReLU activation. The final layer of the stereo music source separation architecture Demucs uses a transposed convolution with kernel width 8 and stride 4, C i\u22121 outputs, and ReLU activation. Skip connections between encoder and decoder blocks allow direct access to the original signal, preserving phase information. Unlike Wave-U-Net, the final layer is linear with S \u00b7 C 0 output channels for each source. The approach in the current section discusses using transposed convolutions with a large number of channels and strides for music note synthesis. This method is inspired by previous works and utilizes U-Net architecture with skip connections and gating for improved results. The Demucs approach utilizes U-Net architecture with skip connections and gating for expressive representation of masks on input signals. It offers multi-scale representations and non-linear transformations for improved results in music note synthesis. Reconstruction loss is calculated using either average mean square error or average absolute error between waveforms, addressing challenges related to initial phases of signals in generative models for audio. In generative audio models, the problem of phase shift in signals can lead to high losses in L1 or L2 metrics. To address this, some approaches focus on generating spectrograms or using power spectrograms for loss comparison. Source separation models have an advantage as they can access the original phase of the signal, making L1/L2 losses valid for training. Methods for generating masks on power spectrograms are valid for source separation, with L1/L2 losses commonly used. Experiments showed no significant difference between L1 and L2 losses. Different initialization schemes, like fixup, have been found to greatly impact the performance of deep neural networks. Fixup initialization, although not designed for U-Net-style skip connections, has shown positive results compared to standard initialization methods. The initialization scheme proposed in the study had a significant positive impact on model performance compared to standard initialization methods. By adjusting the weights based on a reference scale, the scheme produced weights with a specific order of magnitude and non-trivial scale. The chosen value for the reference scale was 0.1, which was found to be effective for both regular and transposed convolutions. Experimental results showed that the scheme maintained the standard deviation of features across layers in a randomly initialized model applied to audio data. The Conv-Tasnet model maintains standard deviation of features across layers in audio data without initial rescaling. It is time equivariant due to dilated convolutions, ensuring output shifts with input. Spectrogram-based methods also exhibit this property, reflecting input shifts in phase. The model architecture shows experimental verification of time equivariance. Randomized equivariant stabilization is proposed as a workaround to ensure shifts in input mixture x are accounted for in model predictions. This technique, using S = 10, resulted in a 0.3 SDR gain without changing training or network architecture. Evaluation of the model becomes S times slower, but still significantly faster than real-time processing. MusDB dataset is utilized for training, consisting of 150 songs with full supervision in stereo and sampled at 44100Hz. The MusDB dataset consists of 150 songs with full supervision in stereo and sampled at 44100Hz. Each song includes waveform data for drums, bass, vocals, and other parts. The dataset is split into train, valid, and test sets. Additionally, raw stems for 150 tracks were collected and manually assigned to sources in MusDB, forming the stem set. Source separation models' performances were evaluated using blind source separation metrics. The study focused on blind source separation using the SDR metric, comparing different models including Open Unmix. The evaluation was done using the MusDB dataset with full supervision in stereo and sampled at 44100Hz. Source separation models' performances were assessed using blind source separation metrics. The study compared source separation models including Open Unmix and MMDenseLSTM using the MusDB dataset. Metrics were downloaded from the SiSec submission repository for Wave-U-Net and MMDenseLSTM, while Open Unmix metrics were provided by the authors. The Ideal Ratio Mask oracle (IRM) metrics were also included. Table 1 compares Conv-Tasnet and Demucs to state-of-the-art models operating on waveform (Wave-U-Net) and spectrograms (Open-Unmix, MMDenseLSTM) using the MusDB test set. Demucs metrics are averaged over 5 runs, with statistically state-of-the-art values highlighted. The study defines one epoch and includes augmentation techniques. The training setup for the models involved using 16 V100 GPUs with 32GB of RAM, a batch size of 64, and the Adam optimizer with a learning rate chosen from [3e-4, 5e-4]. Demucs models were trained for 240 epochs, while Conv-Tasnet was trained for 360 epochs on MusDB and 240 epochs with extra data. Various data augmentation techniques were employed during training. The optimal learning rate of 3e-4 and 100 channels were selected based on validation set performance. Initial weight rescaling reference level of 0.1 was chosen. Confidence intervals were computed using 5 random seeds. Experimental results on MusDB dataset for Conv-Tasnet and Demucs were compared with baselines. Ablation study on Demucs was conducted, showing metrics for a single run. State-of-the-art baselines were compared in terms of training data usage and confidence intervals were considered for the first time. Quality of the separation is highlighted in the results, with Demucs and Conv-Tasnet outperforming previous methods for music source separation. Conv-Tasnet shows higher SDR at 5.73, improving over Open-Unmix. Demucs architecture has room for improvement compared to the IRM oracle. Human evaluations noted artifacts in Conv-Tasnet audio, especially in drums and bass sources. Audio samples and additional details can be found in the ICLR link code. The presence of artifacts in the output of Conv-Tasnet degrades the user experience, as confirmed by a mean opinion score survey. Participants rated the quality and absence of artifacts, as well as contamination by other sources, with Conv-Tasnet receiving a lower rating compared to Demucs and Open-Unmix. The output of Conv-Tasnet has lower quality and more artifacts compared to Demucs and Open-Unmix. Conv-Tasnet samples have less contamination by other sources, but training speed is slower than other models. The ablation study for Demucs shows that using L1 loss, BiLSTM, and randomized equivariant stabilization leads to significant improvements in SDR. Noise levels below 0.06 are considered insignificant. Validation loss was not reported, and stabilization was only applied during SDR computation on the test set. The study introduced extra convolutions in the encoder and decoder, improving model expressivity, especially with GLU activation. Using a kernel size of 3 in the decoder further enhances performance by providing consistent output through time. Conv-Tasnet, a state-of-the-art architecture for speech source separation, achieves top performance for music source separation, surpassing previous methods by 0.4 SDR. Demucs is an alternative approach to Conv-Tasnet for audio source separation, combining masking with stronger decoder capacity for audio synthesis. It produces higher quality audio and matches Conv-Tasnet's performance when trained with extra tracks. This makes it a promising alternative to masking-based methods."
}