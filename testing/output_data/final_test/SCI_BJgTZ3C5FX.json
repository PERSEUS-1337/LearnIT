{
    "title": "BJgTZ3C5FX",
    "content": "Generative Adversarial Networks (GANs) are powerful for generative modeling but can be hard to train. Wasserstein GAN (WGAN) addresses instability by evaluating the Wasserstein distance in the primal domain. This method shows stable convergence and achieves the lowest Wasserstein distance among WGAN variants, although it may sacrifice sharpness in generated images. Our method shows improved gradients for the generator in experiments on the 8-Gaussian toy dataset, enabling more flexible generative modeling compared to WGAN. GANs are a powerful framework for generative modeling, but training stability remains a challenge. Various methods have been proposed to stabilize training, but consistently stable training of GANs is still an open problem. Consistently stable training of GANs is a challenge. WGAN improves on original GANs by using Wasserstein distance. The critic in WGAN must be a 1-Lipschitz function. Weight clipping is used to ensure the critic satisfies this condition, but it can limit the critic's function space and cause gradient issues. In the latest work, a new WGAN variant was proposed to evaluate the exact empirical Wasserstein distance without the need for a critic network. The model evaluates the Wasserstein distance between real and fake data distributions efficiently using linear programming, addressing the issue of approximation error by the critic network. In this paper, a new generative model is proposed that learns by directly evaluating the gradient of the exact empirical optimal transport cost in the primal domain. The method corresponds to stochastic gradient descent of the optimal transport cost, addressing the training difficulty of GANs by using the Wasserstein-1 distance. The WGAN objective function is constructed using the Kantorovich-Rubinstein duality to minimize the set of all 1-Lipschitz functions. Minimization of the objective function with respect to G is equivalent to minimizing W1 (Pr, Pg). Weight clipping in the critic D ensures the desired Lipschitz condition, but may limit the function space. BID3 introduced gradient penalty to encourage the critic to have gradients with magnitude equal. The WGAN-GP learning scheme enforces unit-norm gradient constraints along straight line segments, connecting real and fake data points. However, it can destabilize due to changes in the generator distribution support. The SN method bounds the Lipschitz norm of a critic neural network to optimize the Wasserstein distance. The proposed method in this paper evaluates optimal transport costs between probability distributions efficiently using equal-size sample datasets of real and fake data points. It is based on the fact that the optimal transport cost can be calculated when distributions are uniform over finite sets of the same cardinality. The optimal transport cost between two datasets D and F is calculated as a linear-programming problem, where P r and P g are the probability distributions, and D and F are datasets of real and fake data points, respectively. The cost is determined by the joint probability distributions of P r and P g. The linear-programming problem in optimal transport can be simplified to the linear sum assignment problem by using permutation matrices. The most efficient algorithm for solving this problem has a time complexity of O(N 2.5 log(N C)). The objective is to find the optimal transport plan to minimize the average cost, as shown in a two-dimensional example in FIG0. In optimal transport, the goal is to minimize the average cost by finding the optimal transport plan. Evaluating the optimal transport cost and its derivative is necessary for learning the generator with backpropagation. This involves estimating the derivative of the optimal transport cost with respect to the generator's parameter. The optimal transport plan is difficult to differentiate with respect to the generator output. By assuming equal cardinalities of sets D and F, the optimal transport plan remains unchanged under small perturbations. This allows for tractable differentiation and updating of the generator using the loss function of the optimal transport cost. Specializing in the Wasserstein distance yields desirable properties for the framework. The Wasserstein-p distance between real-data distribution Pr and generator distribution Pg is defined in terms of optimal transport cost. Empirical Wasserstein distance \u0174p(D, F) is a consistent estimator of true Wasserstein distance Wp(Pr, Pg), with an upper bound on the error of the estimator. It metrizes the space of probability measures with finite moments of order p, making it an asymptotically unbiased estimator. The proposed method provides an asymptotically unbiased estimator of the empirical Wasserstein distance without using a critic, making it no longer a GAN. It is applicable to any optimal transport cost and is summarized in Algorithm 1. Experimental results on the MNIST dataset show the use of convolutional neural networks to generate images and optimize the model using Adam. The experimental setup for evaluating variants of WGAN involved a batch size of 64 and 30,000 iterations for training the generator. The Wasserstein-1 distance was used, and performance was compared using the empirical Wasserstein distance (EWD). EWD is considered a reliable estimator of the Wasserstein distance and was evaluated with 256 samples for each method. The proposed method achieved a small EWD and low computational cost compared to WGAN variants, as it does not use the critic. WGAN-SN failed to learn, with the critic's loss function diverging towards -\u221e. The critic's loss function diverged towards -\u221e in EWD trials, while WGAN training remained stable. Gradient penalty in WGAN-GP caused EWD deterioration in all trials. WGAN-TS with weight scaling outperformed WGAN, but without scaling, EWD decreased at the expense of training stability. The proposed method achieved stable training without failure in 5 trials. The proposed method achieved stable training without failure in 5 trials by directly solving the optimal transport problem in the primal domain. Weight scaling in WGAN-TS showed a trade-off between training stability and regression error of the critic. The proposed method directly solves the optimal transport problem in the primal domain to address trade-offs in the empirical version. Fake-data images generated by different methods show that minimizing Wasserstein distance alone may not produce realistic images. The proposed method produces less sharp images but avoids fake-data images that closely resemble real-data images. The proposed method compares with WGAN variants in terms of gradients. Generator distribution on 8-Gaussian dataset converges to real-data distribution. Comparison in FIG4 shows diverse sample updates in proposed method, unlike WGAN and WGAN-SN. The proposed method shows diverse sample update directions aligned with optimal gradients, aiding efficient learning of real-data distribution. In contrast, WGAN variants exhibit less diversity and alignment, hindering generator distribution spread and slowing training due to poor critic quality. Increasing critic iterations per generator iteration improves training but at higher computational cost. The proposed method aims to improve learning by directly minimizing the exact empirical Wasserstein distance between real-data and generator distributions. This approach offers more flexibility in generative modeling compared to WGAN variants, which are constrained by transport cost and Lipschitz constraints. The proposed method aims to improve generative modeling by minimizing the Wasserstein distance more effectively than other WGAN variants. The network architecture for the experiment on the MNIST dataset is detailed, with the generator receiving a 100-dimensional noise vector as input. The network architecture for the experiment on the MNIST dataset includes four transposed convolution layers with 5 \u00d7 5 kernels, stride 2, and no biases. The critic network is the reverse of the generator network, using a convolution layer instead of a transposed convolution layer. Batch normalization is employed in all intermediate layers, with ReLU as the activation function except for the last layers. The last layer uses hyperbolic tangent for the generator and identity function for the critic. In the experiment on the 8-Gaussian toy dataset, the generator network receives a 100-dimensional noise vector and maps it to a two-dimensional space through four fully-connected layers. The critic network architecture is the reverse of the generator network. Default parameter settings were used for the methods, with specific learning rates and weight clipping parameters for the critic and generator in WGAN. In the experiment on the 8-Gaussian toy dataset, the generator network maps a 100-dimensional noise vector to a two-dimensional space. The dataset contains real-data sampled from a Gaussian mixture distribution with 8 centers. Various parameter settings were tested, but WGAN and WGAN-SN did not perform well. The batch size was 64, and the maximum number of generator iterations was set to 1,000. The experiments used Adam with specific parameters for WGAN-GP, WGAN-TS, and the proposed method. The computer setup included an Intel Core i7-6850K CPU, 32 GB RAM, and four GeForce GTX 1080 graphics cards. The experiments were coded in tensorflow 1.10.1 on python 3.6.0 with eager execution enabled. Evaluation was done using FID and IS metrics, with FID measuring the distance between real and fake data points. The FID metric evaluates the quality of fake data points by comparing multivariate Gaussian distributions. IS is a metric for assessing fake data points based on label probabilities obtained from the inception model. The proposed method in WGAN-GP, WGAN-SN, and WGAN-TS* showed deteriorated training in some trials, leading to the use of early stopping based on EWD. Results in TAB3 with early stopping indicated the proposed method performed the worst in FID but the best in IS. The fake data generated by the proposed method lacked sharpness and resemblance to real data points but were easily distinguishable with diversity as digit images. To improve FID results using the proposed method, transport cost should be considered in the desired space corresponding to FID."
}