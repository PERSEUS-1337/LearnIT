{
    "title": "rke2P1BFwS",
    "content": "Most algorithms for representation learning and link prediction in relational data are designed for static data, but real-world data often evolves over time. This includes knowledge bases with time-sensitive facts. To address link prediction under temporal constraints, a solution inspired by tensor decomposition is proposed. In this work, new regularization schemes are introduced, and an extension of ComplEx is presented to achieve state-of-the-art performance in link prediction. A new dataset for knowledge base completion from Wikidata is proposed, serving as a reference for evaluating temporal and non-temporal link prediction methods. The focus is on temporal link prediction in relational data, where missing links in graphs at specific time points are identified. Temporal link prediction in knowledge bases involves finding missing links in graphs at specific time points. It focuses on answering incomplete queries by accurately ranking potential objects with temporal metadata attached to the facts. This task is essential for providing precise information about events or facts that occurred at certain points in time. Temporal Knowledge Base Completion involves using tensor factorization methods to complete a binary tensor holding facts with timestamps. A ComplEx decomposition is used to generate embeddings for each timestamp, with a regularization method to ensure slow evolution over time. This approach helps in dealing with heterogeneous temporal knowledge bases. The method proposed in the study introduces a non-temporal component to the decomposition of a nuclear p-norm, improving performance compared to existing methods. Additional experiments with larger models show improvements in Mean Reciprocal Rank. A dataset of 400k entities with temporal validity information is also introduced, aiming to bridge the gap between methods and web-scale applications in Knowledge Base completion. The tensor product of vectors is denoted by \u2297 and the hadamard product by \u2299. Standard tensor decomposition methods have shown good results in Knowledge Base completion. The Canonical Polyadic (CP) Decomposition is the tensor equivalent of low-rank matrix decomposition. The Distmult model, successful in representing symmetric score functions, sets U = W. To go beyond symmetric relations, complex parameters are used by Trouillon et al. Regularizing the algorithm with the tensor nuclear norm and adjusting the learning objective is also discussed. The tensor nuclear norm and a modified learning objective proposed by Kazemi & Poole (2018) have led to state-of-the-art results in Lacroix et al. (2018). Other methods like TransE (Bordes et al., 2013) model score as distance, but have limitations in modeling relation systems. Schlichtkrull et al. (2018) suggest generating entity embeddings using a Graph Neural Network, but did not outperform the Distmult decomposition in link prediction. Temporal relations in link prediction have been explored using various methods such as Bayesian models, tensor decompositions, and entity embeddings that change over time. These approaches aim to improve prediction performances by considering temporal dynamics in the data. Temporal relations in link prediction have been explored using various methods such as Bayesian models, tensor decompositions, and entity embeddings that change over time. ConT learns a new core tensor for each timestamp, while Garc\u00eda-Dur\u00e1n et al. use recurrent neural nets to transform embeddings of standard models to accommodate temporal data. Lacroix et al. study and extend a regularized CP decomposition of the training set as an order 4 tensor, proposing and studying several regularizers suited to their decompositions. They discretize timestamp ranges to obtain a training set indexing an order 4 tensor, minimizing for each train tuple. In this setting, a new factor T is added to extend the ComplEx decomposition, creating TComplEx. Timestamps are embedded to modulate the multi-linear dot product, allowing for time-dependent representation of objects, predicates, and subjects. In TNTComplEx, timestamps are used to modulate objects, predicates, and subjects for time-dependent representation. The model decomposes the tensorX into two tensors, one temporal and one non-temporal, to handle predicates that may or may not be affected by timestamps. The model proposed by Goel et al. (2020) introduces a non-temporal component by modulating a fraction \u03b3 of embeddings in time, sharing parameters between temporal and non-temporal parts to remove a hyperparameter. Unfolding modes of an order 4 tensor allows it to be considered as an order 3 tensor. Decompositions can be viewed as order 3 tensors by unfolding temporal and predicate modes together, leading to weighted regularizers. The regularizer in the model weights objects, predicates, and pairs based on their marginal probabilities. It includes a variational form of the weighted nuclear 3-norm on an order 4 tensor. The second regularizer penalizes the norm of the discrete derivative of temporal embeddings, aiming for smoothness in neighboring timestamps. The use of tensor nuclear p-norms as regularizers is discussed, leading to a new tensor atomic norm formulation. The nuclear p-norm of a tensor of order D is NP-hard to compute. A practical solution is to use the variational form of the nuclear p-norm, fixing the rank of the decomposition. This approach leads to state-of-the-art results for order-3 tensors and allows for easy introduction of weighting. In subsection 3.3, a penalty is added in Equation (5) to change the norm of atoms for learning under non-uniform sampling distributions. Another variational form is introduced in Equation (4) in subsection 3.2 to easily penalize the nuclear 3-norm of an order 4 tensor. This regularizer allows for weighting based on the joint marginal of timestamps and predicates, rather than the product of the marginals. The impact of regularization on the ICEWS05-15 dataset for the TNTComplEx model is studied, with a grid search for the strength of regularizers \u039b p and \u2126 3. The effect of the regularizer \u039b p and \u2126 3 on the TNTComplEx model is studied through a grid search. Smoothness along the time mode shows an improvement of over 2 MRR points. Comparisons are made between different regularizers (\u2206 4, \u2206 3, \u2206 2, \u2126 3) with a temporal regularizer of 0. The non-convexity of the penalty for tensors is highlighted, with a focus on the minimization of \u2126 3. The differences between \u2206 4 and \u2126 3 lie in their variational forms of nuclear norms, with \u2206 4 being closer to the nuclear 4-norm and \u2126 3 being closer to the nuclear 3-norm. Weighting in \u2206 4 is separate for timestamps and predicates, while in \u2126 3 it is joint. The impact of weighting on guarantees is discussed in Foygel et al. (2011). The contribution of regularizers in Foygel et al. (2011) is shown in Table 3, with a 0.05 MRR increase. Garc\u00eda-Dur\u00e1n et al. (2018) proposed a dataset based on Wikidata containing numerical data not suitable for link prediction. Their dataset had limitations like unwieldy temporal information and insufficient entities. To address these issues, a new dataset was created from Wikidata, available at dataseturl, after removing irrelevant entities. The dataset was created from Wikidata, removing irrelevant entities like scholarly articles and proteins. Entities with degree less than 5 and predicates with less than 50 occurrences were filtered out. The dataset consists of 432715 entities, 407 predicates, and 1724 timestamps. The training set contains 7M tuples, with 10% partially specified temporal tuples. Validation and test sets are of size 50k each. Timestamps are sampled uniformly at random for each datum at train and test time. We sample data uniformly at random within a specified temporal range and rank objects for a partial query. Models from Garc\u00eda-Dur\u00e1n et al. (2018) and Goel et al. (2020) are used as baselines. Results are reported using filtered Mean Reciprocal Rank (MRR) and ranks 10 times higher for comparison. Our models are optimized with Adagrad, using a learning rate of 0.1 and a batch-size of 1000. Results are reported on 3 datasets: ICEWS14, ICEWS15-05, and Yago15k. ICEWS datasets are samplings from the Integrated Conflict Early Warning System. The Yago15K dataset is a modification of FB15k, adding timestamps to triples. Evaluation involves completing incomplete triples with specific formats. Unfolding occursSince, occursUntil, and predicate modes together is chosen over dealing with tensors of order 5. Highly unbalanced relations in Wikidata are addressed by computing Mean Reciprocal Rank for missing right-hand sides. ComplEx model's performance remains stable with a rank of 100, capturing static information well. Temporal models show improved performance with more parameters. Separate modeling of non-temporal predicates benefits performance, as seen in TNTComplex. The model matches or surpasses the state of the art in all cases. TNTComplEx outperforms ComplEx on temporal triples, with comparable performance on non-temporal triples. It can minimize temporal cross-entropy and is more flexible in answering queries. Training on Wikidata with a rank of d = 100, TNTComplEx achieves a speed of 5.6k triples per second, slightly slower than ComplEx at 5.8k triples per second, with minimal impact on runtime. The TNTComplEx model, despite its additional complexity, does not significantly impact runtime. It enforces correct rankings along object tubes and proposes a cross-entropy loss along temporal tubes to improve temporal consistency. This results in a slight decrease in MRR but enhances the model's ability to answer queries along the time axis. In this work, an extension of tensor methods to Temporal Knowledge Bases is proposed. The methodology adapts well to various forms of datasets and achieves higher performances than the state of the art with a similar number of parameters. The model is able to learn correct rankings along time intervals, showing success in Knowledge Base completion. The study proposes an extension of tensor methods for Temporal Knowledge Bases, achieving high performance with fewer parameters. A large scale temporal dataset is introduced to address challenges in knowledge base completion. Unfolding along modes 3 and 4 leads to a tensor decomposition, justifying the regularizers used. The regularizers used in the study are justified by unfolding the tensor along modes 3 and 4, leading to a tensor decomposition. The penalty function is shown to be a variational form of an atomic norm, with the equivalence to a nuclear 3-norm of an order 4 tensor. The study also includes statistics of all datasets used and results for TA. The datasets in Table 6 show the best results for TA and DE-SimplE from previous papers."
}