{
    "title": "HylvlaVtwr",
    "content": "Recent progress in physics-based character animation has led to breakthroughs in human motion synthesis using deep reinforcement learning. However, current methods struggle with generalizing to interactive tasks that require flexible motion patterns. To address this, a hierarchical reinforcement learning framework is proposed for tasks like sitting on a chair. This approach involves subtask controllers trained on simple motions and a meta controller to execute these subtasks. Experimental results show the effectiveness of this method over traditional baselines, and its potential for motion prediction from image inputs. A hierarchical reinforcement learning framework is proposed for tasks like sitting on a chair, with subtask controllers trained on simple motions and a meta controller to execute these subtasks. Experimental results show the method's effectiveness over traditional baselines and its potential for motion prediction from image inputs. Video highlight: https://youtu.be/XWU3wzz1ip8/. The capability of synthesizing realistic human-scene interactions is crucial for simulating human living space and training robots to collaborate with humans. Motion capture data and deep learning algorithms have advanced motion synthesis, but the lack of physical interpretability in synthesized motion remains a major limitation. The problem with motion synthesis lies in unrealistic interactions without physics modeling, limiting its use to non-interactive motions or highly controlled virtual scenes. Recent advancements in physics-based character animation through deep reinforcement learning have shown promise in generating realistic motions in simulated environments, improving generalization for interaction-based motions like walking on uneven terrain or performing stunts. However, challenges still exist in these approaches. In this paper, the focus is on high-level interactive tasks like sitting onto a chair, which require flexible motion patterns. Different human-chair configurations may require different sequences of actions to accomplish the goal, highlighting the limitations of fixed motion patterns. The paper proposes a hierarchical reinforcement learning method for generalizing high-level interactive tasks like sitting onto a chair. The approach involves decomposing the main task into subtasks and training a meta controller to execute them effectively. This strategy mimics human motion skills and allows for flexibility in adapting to different human-chair configurations. The paper introduces a hierarchical approach for generalizing high-level interactive tasks like sitting on a chair. It involves decomposing the main task into subtasks and training a meta controller. This method demonstrates strength over single level and hierarchical baselines and can be applied to motion synthesis in human living space with 3D scene reconstruction. Kinematic modeling of human motions using deep learning from mocap data is also discussed. Recent work in motion prediction has focused on predicting future poses from mocap data sequences, with some even attempting to predict motions directly from static images. In the graphics community, the emphasis has been on motion synthesis to create realistic motions from mocap examples. However, a common challenge faced by these approaches is the lack of physical plausibility in the synthesized motion, such as foot sliding and obstacle penetrations. Physics-based models have a long history in computer graphics for simulating character animation. Our work is related to recent studies by Peng et al. and Clegg et al. in motion imitation using deep reinforcement learning. While Peng et al. focused on training a virtual character for various skills like locomotion, acrobatics, and navigating on irregular terrain, we address a more complex task of sitting onto a chair, involving diverse subtasks like walking, turning, and sitting. Clegg et al. also used a hierarchical model for dressing tasks. Our meta controller in hierarchical reinforcement learning allows for the selection of subtasks at any time point, unlike previous models with pre-defined orders. This approach is crucial for tasks where a fixed order of subtasks may not always lead to completion. The model is inspired by recent work on hierarchical control in deep reinforcement learning, focusing on tasks with high-dimensional action spaces that can be decomposed. Our work focuses on learning object affordances in the vision domain, specifically in generating a sequence of skeletal poses for a human interacting with a chair in 3D space. Unlike previous methods that detect affordances using static poses, we use limited mocap examples and reinforcement learning in a physics simulated environment. This approach allows for the decomposition of high-dimensional tasks into reusable subtasks, aiding in generalization to new high-level tasks. Our system utilizes a physics simulated environment with a humanoid and chair model to generate a sequence of skeletal poses for a human sitting on a chair. The goal is to learn a policy that controls the humanoid to successfully perform the sitting task. The policy consists of subtask controllers trained on mocap data for realistic motions, and a meta controller for higher-level coordination. The system uses mocap data to train subtask controllers for realistic human motions. A meta controller coordinates subtasks to achieve the main task, generating control input at different timescales. Each subtask and the meta task are treated as independent reinforcement learning problems, approximated using neural networks. Subtask controllers map state vectors to actions at each timestep, with state representations varying for different subtasks. The system uses mocap data to train subtask controllers for realistic human motions. Each subtask is treated as an independent RL problem, with a humanoid model having 21 degrees of freedom. A multi-layer perceptron with two hidden layers of size 64 is used for network architecture. The output parameterizes the probability distribution of actions, modeled by a Gaussian distribution. Each subtask generates actions at each timestep by sampling from the distribution. The reward function in training subtask controllers for human motion encourages imitation of mocap reference and task objectives. It consists of terms r S and r G, promoting similarity to reference motion and goal achievement. Angular differences between joint angles and velocities are weighted to shape the humanoid's movements. The state representation for target-directed locomotion includes proprioceptive and goal features. The goal feature specifies an intermediate walking target using azimuth angle in humanoid-centric coordinates. Walking in random directions is replaced by target-directed locomotion for high-level tasks. The training strategy for target-directed walking involves two stages with distinct task objectives. In the first stage, the focus is on steering patterns similar to a reference motion, while in the second stage, the reward is for motion towards the target. The rotation of the root is encouraged to match between the humanoid and reference motion for left/right turns. The sit subtask involves the humanoid lowering and seating in front of a chair. The state includes pose information of the chair and rewards the pelvis for moving towards the seat surface. The meta controller, which shares the same architecture as the subtask controllers, controls the execution of subtasks for navigating the humanoid to sit on a chair. The input state encodes pose information of the chair, while the output action consists of a switch to select a subtask and a target for the walk subtask. The policy network outputs the desired actions for the meta controller. The policy network outputs probability distributions for a switch and a target in the walk subtask. The reward function encourages the pelvis to be in contact with the seat surface. Subtasks and meta task are trained independently using standard RL. The controllers for subtasks and meta task are trained independently using standard RL algorithms like PPO. Subtasks involve imitating reference motion with early termination based on humanoid height and yaw angle deviation. The two-stage training strategy involves target-directed walking and sitting with reference state initialization. The initial pose is set to facilitate training for subtasks, but fine-tuning is needed for successful transitions between subtasks. Initial poses for turn and sit are sampled from the ending poses of walk and turn, respectively. The training strategy involves starting from easier states and progressively increasing difficulty to facilitate learning for the humanoid to sit down. This approach is inspired by curriculum learning and aims to improve training efficiency. The training strategy involves starting from easier states and progressively increasing difficulty for the humanoid to sit down. Mocap data is collected from the CMU Graphics Lab Motion Capture Database and retargeted to a humanoid model. The simulation environment is based on OpenAI Roboschool using the Bullet physics engine. The PPO algorithm for training is implemented from OpenAI Baselines with specific hyperparameters used. The training strategy involves starting from easier states and progressively increasing difficulty for the humanoid to sit down. Mocap data is collected from the CMU Graphics Lab Motion Capture Database and retargeted to a humanoid model. The simulation environment is based on OpenAI Roboschool using the Bullet physics engine. The PPO algorithm for training is implemented from OpenAI Baselines with specific hyperparameters used. The qualitative results of individual subtask controllers are shown, including walking, following a target, turning left and right, and sitting on a chair. Two metrics are used to evaluate the main task: success rate and minimum distance. Success is defined as continuous contact with the seat surface for 3.0 seconds, and the success rate is reported over 10,000 trials. Additionally, the per-trial minimum distance is computed. The humanoid's performance is evaluated by computing the per-trial minimum distance between the pelvis and the seat surface over 10,000 trials. Two initialization settings, Easy and Hard, are considered for the task. In the Easy setting, the humanoid starts 2 meters away from the chair and simply walks forward to sit down. In the Hard setting, the humanoid starts from different positions and needs to walk around the chair to sit down successfully. In the Easy setting, the humanoid simply walks forward to sit down, while in the Hard setting, the humanoid needs to walk around the chair to sit down successfully. The approach is benchmarked against various baselines, including a kinematics-based method and a physics-based approach where a controller imitates the motion sequence. The study compares a kinematics-based method and a physics-based approach for a humanoid robot to perform a holistic subtask involving walking, turning, and sitting. The results show that both baselines struggle to approach the chair, with the kinematics baseline generating unrealistic motions and the physics baseline failing to get close to the chair. The study compares kinematics-based and physics-based approaches for a humanoid robot to perform a holistic subtask involving walking, turning, and sitting. Hierarchical baselines outperform single level approaches, showing that breaking a task into reusable subtasks leads to better generalization. Our approach outperforms pre-defined order baselines by breaking tasks into reusable subtasks for better generalization. The success rate is low due to failures in subtask execution, transitions, and an insufficient subtask repertoire. Individual subtask success rates vary, with some subtasks like right turn having a 67.59% success rate. The success rate for subtask execution varies, with some tasks like right turn having a 67.59% success rate. Fine-tuning can improve success rates after transitions, but not perfectly. A diverse subtask skill set is needed for better performance. Analysis of the meta controller's behavior shows certain transitions are favored based on the starting position of the humanoid. The success rate for subtask execution varies, with some tasks like right turn having a 67.59% success rate. Fine-tuning can improve success rates after transitions, but not perfectly. A diverse subtask skill set is needed for better performance. Analysis of the meta controller's behavior shows certain transitions are favored based on the starting position of the humanoid. Starting areas, e.g. walk\u2192left turn is favored over walk\u2192right turn when started from the left side. The task's difficulty is increased by initializing the humanoid in Zone 2 and 3, showing the effect of curriculum learning strategy. Results show a drop in success rate without CL, but improvement with CL, indicating a tailored curriculum can enhance training outcomes for challenging tasks. The humanoid's success rate varies for different tasks, with fine-tuning improving success rates after transitions. Starting position affects task difficulty, with certain transitions favored based on starting areas. Curriculum learning enhances training outcomes, as shown by improved success rates with CL. Vision-based application synthesizes sitting motions from a single RGB image, aligning observed scenes with simulated environments for realistic motion. The synthesized humanoid motion looks physically plausible in examples, but not always accurate due to the lack of modeling other objects in the scene. Future work includes learning motion by simulating cluttered scenes and synthesizing motions based on observed humans in images."
}