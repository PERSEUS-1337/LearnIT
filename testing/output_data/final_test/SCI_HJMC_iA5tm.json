{
    "title": "HJMC_iA5tm",
    "content": "NeuroSAT is a message passing neural network that learns to solve SAT problems by predicting satisfiability. It can handle larger and more difficult problems than those seen during training by running for more iterations. Additionally, NeuroSAT can generalize to various problem distributions, such as graph coloring, clique detection, dominating set, and vertex cover problems. The propositional satisfiability problem is fundamental in computer science, proven to be NP-complete by BID4. NeuroSAT is a message passing neural network that learns to solve SAT problems by predicting satisfiability with minimal supervision. It can handle complex problems and generalize to various distributions, such as graph coloring and clique detection. NeuroSAT is an end-to-end SAT solver that can handle complex problems and generalize to various distributions. It guesses unsatisfiable with low confidence until finding a solution, then converges to guess satisfiable with high confidence. Despite not being competitive with state-of-the-art solvers, NeuroSAT can solve larger and more difficult SAT problems by performing more message passing iterations. NeuroSAT is a procedure that searches for solutions to varying difficulty problems indefinitely. It predicts whether SAT problems are satisfiable with only a single bit of supervision. It can handle larger, more difficult problems at test time and generalize to new domains. Since SAT is NP-complete, NeuroSAT can be queried on any search problem with certificates that can be checked in polynomial time. NeuroSAT is a procedure that can solve SAT problems by encoding graph coloring, clique detection, dominating set, and vertex cover problems. It can also help construct proofs for unsatisfiable problems. Another model, NeuroUNSAT, is trained to detect contradictions in unsatisfiable problems. By extracting variables involved in the contradiction, a resolution proof can be constructed more efficiently. A formula in propositional logic is satisfiable if boolean values can be assigned to its variables to evaluate it to 1. Every formula has an equisatisfiable form in conjunctive normal form (CNF), represented as a conjunction of disjunctions of variables. Each conjunct in CNF is a clause, and each variable within a clause is a literal. This CNF form allows for more concise representation of the formula. A formula in CNF is satisfiable if every clause has at least one literal mapped to 1. A SAT problem aims to determine if a formula is satisfiable and find a satisfying assignment. Using n for variables and m for clauses, the goal is to learn a classifier approximating satisfiability. Datasets are created by sampling problems and computing satisfiability. The task is to predict if a given problem is satisfiable. The goal is to predict if a given problem is satisfiable by determining \u03c6(P). A SAT problem can be encoded into a vector space using methods like an RNN, but the semantics of propositional logic introduce invariances like permutation and negation invariance. Permuting variables, clauses, or literals within a clause does not affect satisfiability, nor does negating literals corresponding to a variable. NeuroSAT is a neural network architecture that enforces permutation and negation invariance in solving SAT problems. It encodes the problem as an undirected graph and iteratively refines node embeddings by passing messages along the graph edges. The architecture updates embeddings for literals and clauses in two stages, considering messages from neighboring nodes and complementary literals. The NeuroSAT model updates embeddings for literals and clauses by passing messages and considering neighboring nodes and complementary literals. The model is parameterized by vectors and multilayer perceptrons, with hidden states initialized to zero matrices. NeuroSAT updates embeddings for literals and clauses by passing messages and considering neighboring nodes and complementary literals. The model enforces permutation and negation invariance, operating on nodes and edges without specific ordering. The network is trained to minimize sigmoid cross-entropy loss between logit y (T) and true label \u03c6(P). Each literal is paired with its complement in a matrix for voting. The network in NeuroSAT updates embeddings for literals and clauses by passing messages and considering neighboring nodes and complementary literals. It enforces permutation and negation invariance, operating without specific ordering. The model is trained to minimize sigmoid cross-entropy loss between logit y (T) and true label \u03c6(P). Each literal is paired with its complement in a matrix for voting, where a phase transition occurs leading to all literals voting sat with high confidence. The learned parameters are independent of the SAT problem size, allowing the model to be trained and tested on problems of varying sizes. The neural network in NeuroSAT updates embeddings for literals and clauses by passing messages and considering neighboring nodes. It enforces permutation and negation invariance without specific ordering. The model is trained to minimize cross-entropy loss between logit y (T) and true label \u03c6(P). To ensure substantive learning, a distribution SR(n) is created over pairs of random SAT problems on n variables where one is satisfiable and the other is not. The distribution SR(n) generates pairs of random SAT problems on n variables, where one is satisfiable and the other is unsatisfiable. It samples variables and negates them with a certain probability to create clauses, adding them to the SAT problem until it becomes unsatisfiable. The ultimate goal is to solve SAT problems from various domains. NeuroSAT is trained as a classifier on SR(40) problems, which are small and efficiently solvable by SAT solvers. The classification task is challenging due to the complexity of the problems, with each having 40 variables and over 200 clauses. LSTM models struggled to predict satisfiability accurately, even with specialized encoding. MiniSAT BID23 requires multiple backjumps and logical inferences to solve each problem. The NeuroSAT architecture used had a dimension of d = 128. NeuroSAT architecture with d = 128 dimensions for embeddings and hidden units, 3 hidden layers, and linear output layers for MLPs. Regularized with 2 norm scaled by 10^-10, trained with ADAM optimizer, learning rate of 2 x 10^-5, and batched problems with up to 12,000 nodes. Sampled variables n from 10 to 40 during training. Trained on millions of problems. NeuroSAT, trained on SR(U(10, 40)), achieves 85% accuracy on the test set after training on millions of problems. The model computes a 2n-dimensional vector of literal votes during each iteration, visualized in Figure 3 for a satisfiable problem from SR(20). Each literal is paired with its complement in a matrix format for clarity. The ith row shows scalar votes for literals x i and x i. Initially, most literals vote unsat with low confidence, then a phase transition occurs where all literals start voting sat with high confidence. NeuroSAT displays similar behavior on satisfiable problems, while guessing unsat problems with low confidence for many iterations without a phase change. NeuroSAT searches for a certificate of satisfiability and only guesses sat once it has found one. Literal votes encode one bit for each variable, representing a satisfying assignment in this case. The votes are derived from higher dimensional literal embeddings using an MLP. NeuroSAT uses PCA embeddings to find satisfying assignments in satisfiable problems. Blue and red dots represent literals set to 0 and 1 in the assignment. Clustering of literal embeddings helps decode solutions with 70% accuracy. Literal votes encode satisfying assignments by chance. NeuroSAT uses clustering to decode solutions with 70% accuracy in satisfiable problems. Literal embeddings form distinct clusters at the phase transition. NeuroSAT uses clustering to decode solutions with 70% accuracy in satisfiable problems. Literal embeddings form distinct clusters at the phase transition. At training time, NeuroSAT is given minimal supervision for each SAT problem and has learned to search for satisfying assignments based on this. It can solve SAT problems with larger n by running more iterations of message passing. NeuroSAT can solve larger and harder SAT problems by running more iterations, even though it was only trained on smaller instances. Its success rate increases significantly when running for more iterations, showcasing its ability to encode procedural knowledge that can operate effectively over a wide range of time frames. The graph from the Forest-Fire distribution demonstrates various properties that are not easily perceptible and require deliberate computation. NeuroSAT can solve larger and harder SAT problems by running more iterations, showcasing its ability to encode procedural knowledge effectively. Problems in NP can be reduced to SAT in polynomial time, with different domains having different properties. To assess NeuroSAT's ability to extrapolate, problems from various domains were encoded into SAT problems, including graphs from six different random graph distributions. NeuroSAT can solve larger and harder SAT problems by encoding procedural knowledge effectively. Problems from various domains were encoded into SAT problems, including graphs from six different random graph distributions. Parameters were found for random graph generators to create graphs with specific characteristics. Graph coloring, dominating-set, clique-detection, and vertex cover problems were generated for each graph. Minisat BID23 was used to determine satisfiability, resulting in 4,888 satisfiable problems out of 7,200 generated. These problems contained over two and a half times as many clauses as problems in SR(40). NeuroSAT successfully decodes solutions for SAT problems with more clauses than SR(40). It outperforms Survey Propagation (SP) BID3 in finding satisfying assignments. NeuroSAT synthesizes a different algorithm and can find satisfying assignments but struggles with proofs of unsatisfiability. Training on datasets with unsat cores improves its performance on unsatisfiable problems. NeuroSAT can detect unsat cores in unsatisfiable problems, aiding in constructing resolution proofs efficiently. A new distribution SRC(n, u) is introduced, where every unsatisfiable problem contains a small unsat core. Sampling from SRC(n, u) involves initializing a problem with u and sampling clauses until it becomes unsatisfiable. NeuroUNSAT is a trained model that can detect unsat cores in unsatisfiable problems. It was trained on datasets created from unsat cores ranging from three to nine clauses. The model can identify unsat cores with high confidence, aiding in constructing resolution proofs efficiently. NeuroUNSAT can predict satisfiability with 100% accuracy by recognizing unsat cores. It finds unsat cores and votes unsat with high confidence. The model clusters literals involved in unsat cores, achieving 98% accuracy on the test set. NeuroUNSAT can predict satisfiability with 100% accuracy by recognizing unsat cores and clustering literals involved in unsat cores with 98% accuracy on the test set. Previous attempts to apply statistical learning to the SAT problem did not use neural networks and aimed to assist existing solvers rather than solving SAT problems independently. The closest related work used an MPNN to predict unique solutions of Sudoku puzzles. Neural networks like NeuroSAT and BID6 can predict solutions to Sudoku puzzles and propositional formula entailment. BID6's network does not use heuristic search and works on simple problems. Neural networks have shown success in learning heuristics for combinatorial optimization problems, with a focus on finding low-cost solutions. The main goal is to understand the capabilities of neural networks in solving constraint satisfaction problems. Neural networks like NeuroSAT can perform discrete search without hard-coded procedures, showing surprising capabilities in logical reasoning. While still less reliable than state-of-the-art SAT solvers, this work contributes to understanding neural networks' limitations and potential for practical SAT solving improvements. NeuroSAT can be trained as an end-to-end solver on challenging problems or used to guide decisions in traditional SAT solvers. Training on different data can lead to learning different procedures. In a separate experiment, the architecture predicted satisfying assignments accurately for individual literals. Unlike NeuroSAT, it made errors, had no phase transition, and could make predictions quickly. Descendants of NeuroSAT may learn diverse mechanisms. NeuroSAT's descendants may learn diverse mechanisms and heuristics based on training data and objective functions, potentially leading to advancements in the field."
}