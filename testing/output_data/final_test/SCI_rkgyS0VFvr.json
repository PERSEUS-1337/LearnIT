{
    "title": "rkgyS0VFvr",
    "content": "Backdoor attacks manipulate training data with adversarial triggers to cause incorrect predictions on the test set. Federated learning aggregates data from different parties but can introduce vulnerabilities. A distributed backdoor attack (DBA) exploits the distributed nature of FL by decomposing a global trigger pattern into local patterns embedded in different parties' training sets. This approach is more effective than standard centralized backdoors. DBA is a distributed backdoor attack that is more persistent and stealthy against federated learning compared to standard centralized backdoors. Extensive experiments show that DBA has a higher attack success rate and can evade robust FL algorithms. Explanations for DBA's effectiveness are provided through feature visual interpretation and importance ranking. Our proposed DBA explores attack performance variations in federated learning, shedding light on FL's robustness. FL addresses training ML models without direct data access, with applications in privacy-sensitive tasks like loan prediction and health assessment. Federated Learning (FL) allows for better model training by aggregating information from different parties, but its distributed methodology and non-i.i.d. data distribution can lead to new attacks. Privacy concerns may enable backdoor attacks on shared models trained with FL, making them vulnerable to manipulation. Recent studies have focused on backdoor attacks on FL, highlighting the need for robustness in federated learning. Recent studies have focused on backdoor attacks on Federated Learning (FL), with current attacks not fully exploiting FL's distributed methodology. A proposed distributed backdoor attack (DBA) leverages FL's aggregation of information from local parties to train a shared model, using a local trigger instead of a global one. The distributed backdoor attack strategy DBA decomposes centralized attacks into local patterns, embedding them to different adversarial parties. Extensive experiments show that DBA is more persistent and effective than centralized attacks, with the global trigger achieving better performance on the global model. This paper introduces distributed backdoor attacks (DBA) in federated learning (FL), which are more effective and stealthy compared to centralized backdoor attacks. The effectiveness of DBA is explained through feature visual interpretation and importance ranking. Comprehensive analysis and ablation studies are conducted on various trigger factors in DBA, including trigger size, gap, location, scaling effect, poisoning interval, ratio, and data distribution. In federated learning, the central server sends the shared model to selected parties for local computation using optimization algorithms like stochastic gradient descent. The parties then send their model updates back to the central server for aggregation to generate a new global model. This process continues iteratively until the final global model is achieved. The attacker in this scenario has full control over their local training process. The attacker in federated learning has full control over their local training process, including backdoor data injection and updating hyperparameters. The objective of a backdoor attack is to manipulate local models to predict a target label chosen by the attacker. The attack aims to fit the main task and backdoor task simultaneously, allowing the global model to behave normally on untampered data while achieving a high attack success rate. The attacker in federated learning can manipulate local models with backdoor data injection to predict a target label. The attacker designs a trigger pattern using parameters like trigger location, size, and gap, to achieve a high attack success rate. The poisoned dataset transforms clean data into backdoored data, allowing the global model to behave normally on untampered data. In federated learning, attackers manipulate local models by injecting backdoor data with trigger patterns. Each attacker uses parts of a global trigger to poison their local models, aiming to attack the shared model. The local trigger is defined for each attacker, while the combined trigger is the global trigger for fair comparison. In federated learning, attackers manipulate local models by injecting backdoor data with trigger patterns. The Distributed Backdoor Attack (DBA) utilizes distributed learning and local data opacity to break a centralized attack into multiple sub-attacks. Each DBA attacker independently performs the backdoor attack on their local models using a geometric decomposing strategy and trigger decomposition rule based on a global trigger. DBA attackers poison with a poison round interval and manipulate their updates before submitting to the aggregator. In federated learning, attackers use the Distributed Backdoor Attack (DBA) to break centralized attacks into multiple sub-attacks. Attackers manipulate local models with trigger patterns and poison them with a poison round interval. They use a scale factor \u03b3 to manipulate updates before submitting to the aggregator. The framework of DBA on FL introduces new trigger factors, such as trigger size and trigger gap, which are critical for the attack. Gap TG represents the distance between local triggers, Trigger Location TL is the offset of trigger pattern, Scale \u03b3 is used to scale up malicious model weights, Poison Ratio r controls backdoored samples fraction, and Poison Interval I determines round intervals between poison steps. Data Distribution: Federated Learning (FL) assumes non-i.i.d. data distribution across parties. A Dirichlet distribution with different hyperparameters is used to generate different data distributions for evaluation on four classification datasets. The model is trained using SGD for E local epochs with a local learning rate and batch size. A shared global model is trained by all participants, with 10 selected for aggregation in each round. Refer to Appendix A.1 for more details. In Federated Learning, a shared global model is trained by all participants, with 10 selected for aggregation in each round. Attack scenarios include multiple-shot attack (Attack A-M) and single-shot attack (Attack A-S), with different strategies for successful attacks. In Federated Learning, attackers perform complete attacks in every round, with DBA or centralized attackers consistently selected. Attack A-S requires only one shot for successful embedding of backdoor triggers, achieved by scaling malicious updates to overpower benign updates. DBA attackers embed local triggers in rounds 12, 14, 16, 18, while the centralized attacker implants a global trigger in round 18. In experiments evaluating DBA and centralized backdoor attacks, the success rates were compared using the same global trigger. The ratio of DBA pixels to centralized pixels varied across different datasets. To ensure fair comparison, the total number of backdoor pixels for DBA attackers was controlled to be close to or less than that of centralized attackers. Test data with the same true label as the backdoor target was removed to avoid label influence. The attack success rate of DBA is higher than centralized attack in all cases, with DBA also converging faster and yielding a higher success rate in MNIST. The global trigger in DBA performs better than local triggers, even when the global trigger is not present in any local training dataset. Additionally, the global trigger converges faster in attack performance than local triggers. The centralized attacker embeds the whole pattern, resulting in a low attack success rate for local triggers. Continuous poisoning increases the attack rate on local triggers for LOAN but not for MNIST and Tiny-imagenet. DBA leads to high attack success rate for the global trigger even when some local triggers have low success rates. In Attack A-S, DBA and centralized attack both achieve high success rates with a scale factor \u03b3 = 100. The backdoor injected into the global model weakens over consecutive rounds due to benign updates, gradually decreasing the attack success rate. The attack success rate of centralized attack weakens over time due to benign updates, with an initial drop followed by a slow rise. DBA yields a more persistent attack, maintaining an 89% success rate in MNIST after 50 rounds compared to centralized attack's 21%. DBA's global trigger lasts longer than any local triggers, making it more resilient to benign updates. FoolsGold and RFA are robust FL aggregation algorithms that can detect outliers beyond the worst-case scenario. RFA replaces the weighted arithmetic mean with an approximate geometric median to be robust to outliers. DBA and centralized backdoor attacks are evaluated against RFA and FoolsGold under Attack A-M setting, with DBA meeting conditions for convergence to a solution. The outliers in RFA are kept below 1/2 to ensure convergence despite outliers. RFA converges rapidly in about 4 iterations, outperforming centralized attacks in backdoor attacks. DBA attackers have higher success rates and faster convergence in Tiny-imagenet, MNIST, CIFAR, and LOAN datasets. The Euclidean norm between attacker's updates and geometric median shows DBA's effectiveness with lower distances. In the Tb.2 in Appendix, DBA attackers submit malicious updates with lower distances than centralized attackers in all datasets, aiding in bypassing defenses. FoolsGold reduces aggregation weights for similar gradient updates, favoring diverse updates. DBA outperforms centralized attacks in Fig.5, with higher success rates and faster convergence in image datasets like MNIST. However, FoolsGold struggles with LOAN due to its inability to differentiate between malicious and clean updates, leading to quick backdoor success. In Appendix, FoolsGold's weights on adversarial parties in Tb.2 show DBA's effectiveness in backdoor attacks. Despite assigning smaller weights to DBA attackers, the distributed approach is more successful than centralized attacks. Feature importance can be determined using various classification tools, with consistent top features identified in LOAN. Grad-CAM and Soft Decision Tree are used to explain DBA's stealthiness. Soft Decision Tree trained on the datasets is discussed in Appendix A.7. Using the Grad-CAM visualization method, the stealthy nature of DBA is explained by analyzing the interpretations of original and backdoor target labels. Locally triggered images alone are weak attacks, but when combined as a global trigger, the backdoored image is classified differently. Grad-CAM results show the attention being drawn to the trigger location, highlighting the stealthiness of DBA. In the soft decision tree of MNIST, poisoning significantly impacts decision making. The importance of features changes after poisoning, with previously insignificant features becoming crucial for prediction. DBA trigger factors are studied under Attack A-S, with one factor changed in each experiment. The attack success rate and accuracy are measured in Attack A-S. Enlarging the scale factor increases both DBA-ASR and DBA-ASR-t, narrowing the gap between them. For CIFAR, DBA-ASR reaches over 90% and remains stable with a gamma value larger than 40, but larger gamma values have a more positive impact on DBA-ASR-t. The more complex the model architecture, the more noticeable the decline in main accuracy as gamma increases, due to scaling affecting more model parameters in complex neural networks. The main accuracy of LOAN remains stable despite these changes. Enlarging the scale factor increases both DBA-ASR and DBA-ASR-t, narrowing the gap between them. The main accuracy of Tiny-imagenet in attacking round drops to 2.75% when \u03b3 = 110. Larger scale factor alleviates the averaging impacts of central server for DBA, leading to a more influential attack performance. However, it also causes the main accuracy of global model to descend in the attacking round for three image datasets. There is a trade-off in choosing the scale factor as using a large scale factor results in an anomalous update that is easy to detect. The global trigger pattern is moved across different corners for three image datasets. The U-shape curve between TL and DBA-ASR is observed in MNIST and Tinyimagenet, indicating that DBA is harder to succeed in the middle part of images where the main object is located. In LOAN, DBA using low-importance features has a higher success rate compared to high-importance triggers. Additionally, local trigger patterns in the four corners of an image show varying levels of DBA-ASR. In image datasets, the DBA-ASR and DBA-ASR-t are low in the four corners due to local convolution operations and large trigger distances. Using zero trigger gap in CIFAR and Tiny-imagenet still succeeds but the backdoor is forgotten faster. Larger trigger sizes result in higher DBA-ASR, but little gain is seen with oversized triggers. In MNIST, DBA-ASR is low when trigger size is 1. In MNIST, DBA-ASR is low when trigger size is 1, indicating that small triggers are ineffective in global models. Centralized attacks with 4-pixel global patterns also have low success rates. Scaling updates simultaneously or having long poison intervals result in poor attack performance. Optimal poison round intervals exist for LOAN and MNIST datasets. In CIFAR and Tiny-imagenet, varying the interval from 1 up to 50 does not lead to remarkable changes in DBA-ASR and DBA-ASR-t, showing the robustness of distributed attacks. Increasing the number of poisoned samples initially improves backdoor performance, but too high a poison ratio can lead to failure of the global model. The weight of a local model with low accuracy can cause the global model to fail in the main task. Poisoning the full batch leads to different outcomes in CIFAR, Tiny-imagenet, and MNIST datasets. It is crucial for Distributed Backdoor Attacks (DBA) to maintain stealthiness in local training by using a reasonable poison ratio. DBA-ASR shows stability under various data distributions, demonstrating its practicality and robustness. Federated Learning (FL) was introduced by McMahan et al. (2017) to address distributed machine learning challenges without sharing training data with the server. In this paper, experiments in standard Federated Learning (FL) settings are discussed and analyzed, focusing on improving communication efficacy by compressing updates. Previous studies have looked into backdoor attacks on FL, where malicious local models replace the global model to disrupt convergence. Strategies to evade these attacks include alternating minimization and estimating benign updates. Robust Federated Learning aims to train models while mitigating attack threats. Fung et al. proposed a defense based on updating diversity to prevent label-flipping and centralized backdoor attacks. Pillutla et al. suggested using an approximate geometric median for aggregation to minimize the impact of outlier updates. Our proposed Diversity-based Aggregation (DBA) is shown to be more persistent and effective than centralized backdoor attacks in Federated Learning (FL). Through experiments on various datasets, DBA achieves higher attack success rates, faster convergence, and better resiliency in different attack scenarios. It is also more stealthy and can evade robust FL approaches. Feature visual interpretation is used to explain DBA's effectiveness, and an analysis of its unique factors is conducted. DBA is identified as a powerful new attack on FL, providing insights for evaluating FL's adversarial robustness. The financial dataset LOAN contains loan status and payment information for loan status prediction. Data samples are divided by 51 US states, each representing a participant in FL. Training data is split with 80% for training and the rest for testing. In image datasets, training images are divided using a Dirichlet distribution for 100 parties. Each party uses SGD optimizer with local epochs and learning rate. A shared global model is trained by participants, with 10 selected in each round for aggregation. White pixels are assigned for the pixel-pattern backdoor. In the financial dataset LOAN, data samples are split by 51 US states for loan status prediction. Training data is divided with 80% for training and the rest for testing. In image datasets, white pixels are assigned for the pixel-pattern backdoor. For MNIST, CIFAR, and Tiny-imagenet, triggers are set with specific values. In Tiny-imagenet, the row number of the local trigger is set to 2 due to larger image size. In the preprocessed LOAN dataset, low importance features are chosen and split by DBA attackers, with new values assigned to trigger features. In Attack A-M, attackers use backdoored data with poison ratio to maximize performance and remain stealthy. They start attacking when the global model converges, around round 10 for MNIST, 200 for CIFAR, and 20 for Tiny-imagenet. It is recommended to attack late in Attack A-S to minimize the impact on the backdoor. In evaluating DBA on irregular shape triggers, the logo 'ICLR' and physical pattern glasses were decomposed into smaller parts. Results show DBA is more effective than centralized attack, even with different colors. Experiment setup includes distributed and centralized attackers. LOAN features were preprocessed before normalization. The mean value of 91 features is below 10, with different poison ratios for centralized and distributed attacks on various datasets. The poison ratio is scaled by a factor of f. The poison ratio is scaled by a factor of f for centralized and distributed attacks on various datasets. The setting of f times scaling for centralized attack has a larger impact on complex neural networks like Resnet used in CIFAR and Tiny-imagenet. DBA is more effective for CIFAR and Tiny-imagenet, while LOAN and MNIST do not perform well under both attacks. The Bulyan aggregation rule is used for LOAN and image datasets, with specific parameters set for each. DBA is more effective for CIFAR, while other attacks fail. It is suggested to explore distributed versions of new attack algorithms to counter Krum and Bulyan defenses. The proposed Soft Decision Tree distills a trained neural network by training with data and soft targets. Trained with gradient descent, each inner node has a learned filter and bias for binary decisions, while leaf nodes have learned distributions. Soft decision trees achieve 90% test accuracy on main and backdoor tasks. In a specific example, the third node in the fourth layer distinguishes between two digits based on filter values. The Soft Decision Tree model uses inner nodes to make binary decisions based on learned filters and biases. Clean images trigger specific filters, while poisoned images show significant changes in decision-making areas. This model provides explainable classification decisions for both clean and poisoned input data. The poisoned model misbehaves from the top node of the tree. 10000 poisoned and clean samples are run to study sample-wise importance based on filter value multiplied by input feature value. The original low importance feature becomes salient in the poisoned model with poisoned input. Shif t y is increased while keeping Shif t x = Shif t y until the rightmost pixel reaches the image edge. TL is the max value among Shif t x and Shif t y. Increasing the hyperparameter \u03b1 in the Dirichlet distribution simulates from non-i.i.d to i.i.d distributions for image datasets. Data distribution significantly impacts the performance of DBA under robust aggregation algorithms when calculating distance between benign and malicious updates. Non-i.i.d. training data makes it easier for DBA to succeed against certain attacks in CIFAR and Tiny-imagenet datasets. Six low importance features are named in Fig. 7. The six high importance features in the data include out prncp, total pymnt inv, total rec prncp, last pymnt amnt, and all util, along with public records of bankruptcies, delinquencies, and tax liens."
}