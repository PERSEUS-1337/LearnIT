{
    "title": "r1e8qpVKPS",
    "content": "Model-agnostic meta-learning (MAML) is a powerful method but difficult to train due to two learning rates. This paper derives conditions for inner learning rate $\\alpha$ and meta-learning rate $\\beta$ for MAML convergence. The upper bound of $\\beta$ depends on $\\alpha$, and the threshold of $\\beta$ increases as $\\alpha$ approaches its upper bound. Experiments on few-shot tasks and architectures validate these findings. A guideline is proposed for determining the learning rates: first, find the largest possible $\\alpha$, then tune $\\beta. The ability to learn quickly with limited data is a key aspect of human intelligence. Meta-learning, such as Model-agnostic meta-learning (MAML), aims to develop models that can adapt to new tasks efficiently. MAML has been extensively studied and applied in various areas like continual learning and reinforcement learning. MAML is widely used in meta-learning due to its simplicity and efficiency across different tasks and model architectures. However, it is challenging to train MAML because of the two learning rates involved: the inner learning rate \u03b1 and meta-learning rate \u03b2. The relationship between these rates and how to tune them remains unclear, making it difficult to optimize training. In this paper, the MAML algorithm is investigated for selecting learning rates. A guideline is proposed for choosing these parameters, specifically focusing on the relationship between the inner learning rate \u03b1 and meta-learning rate \u03b2. The study derives conditions for local convergence and highlights the dependency of the upper bound \u03b2c on the inner learning rate \u03b1. The study investigates the relationship between inner learning rate \u03b1 and meta-learning rate \u03b2 in MAML. The results suggest selecting the largest possible \u03b1 first, then tuning \u03b2 for rapid adaptation to new tasks with minimal data. MAML updates parameters \u03b8 for tasks \u03c4 using stochastic gradient descent, aiming to quickly reach optimal parameters \u03b8*\u03c4. The update equation involves inner learning rate \u03b1, meta-learning rate \u03b2, and loss L(\u00b7) gradient with respect to \u03b8. Multiple steps and data separation are considered in Appendix A. If \u03b1 is small, the gradient approximation is similar to the true gradient. The first-order approximation of MAML, as discussed by Finn et al. (2017) and studied by Nichol et al. (2018) and Fallah et al. (2019), does not significantly degrade performance in practice. Fallah et al. (2019) also proved that this approximation does not affect convergence when tasks are similar or \u03b1 is small. Simplifying to consider only one task during training, the loss L(\u03b8 \u2032 ) is used instead of the MAML loss. The simplified MAML can be seen as optimization with a negative gradient penalty, leading to potential instability during training. This bias towards larger gradients can cause training instability, as shown in Fig. 1. The gradient norm and inner products increase during training, as discussed by Guiroy et al. (2019). In this section, the authors derive a condition for learning rate \u03b1 and \u03b2 for local convergence from any point near the local minima. The analysis assumes one step for update using steepest GD, with continuous use of the same training data. The focus is on a single task initially, followed by considerations for multiple tasks. The authors derive a condition for learning rate \u03b1 and \u03b2 for local convergence from any point near the local minima. The analysis focuses on single task initially, followed by considerations for multiple tasks. The sufficient condition for \u03b1 for \u03b8 * to be a local minimum is derived. The magnitudes of Tg and H 2 are calculated numerically, with Tg being much smaller than H 2 in practice. The sufficient condition for the simplified MAML to locally converge to local minima is derived by analyzing the learning rates \u03b1 and \u03b2. The condition for \u03b1 to be a local minimum is determined by the inverse of the largest eigenvalue of matrix H. Additionally, the condition for \u03b2 is established for all i. The simplified MAML locally converges to local minima based on the learning rates \u03b1 and \u03b2. The condition for \u03b2 depends on \u03b1, especially when \u03b1 is close to a critical value \u03b1c. This is different from vanilla GD where \u03b2 is independent of \u03b1. Experiments in Section 5 validate this finding. In Section 3.1, upper bounds of \u03b1 and \u03b2 for multiple tasks \u03c4 \u223c P (\u03c4 ) are derived. The simplified meta-objective is defined as a sum of task-specific objectives, with conditions for local minima based on eigenvalues of the Hessian matrix. This provides a sufficient condition for \u03b8 * to be a local minimum forL(\u03b8). The analysis for \u03b2 is similar to that in Section 3.1.2. P \u03c4 varies between tasks, leading to an upper bound of \u03b2 c. If Eq. 23 holds for any eigenvalue \u03bb i of any task \u03c4, \u03b2 satisfies the condition for local convergence. This guarantees local convergence of simplified MAML to local minima from any point in the neighborhood of the local minima for multiple tasks. Several papers have investigated MAML and proposed various algorithms. Nichol et al. (2018) studied the first-order MAML family in detail, showing the decomposition of the MAML gradient into two terms related to joint training and increasing the inner product between gradients for different tasks. Guiroy et al. (2019) explored the generalization ability of MAML, finding a correlation between generalization and average gradient inner product. Loss surface flatness, often indicating strong generalizability in normal neural network training, was not necessarily related to MAML's performance. The authors proposed an algorithm considering the importance of inner product between task-specific gradient and average gradient in MAML. They introduced a regularization term involving negative squared gradient norm to simplify MAML optimization. Antoniou et al. (2019) identified challenges in training MAML and proposed an algorithm to address them, making training easier. In response to challenges in training MAML, various algorithms have been proposed to improve stability and ease of training. Behl et al. (2019) addressed the issue of tuning learning rates by introducing an algorithm for automatic tuning during training. Fallah et al. (2019) focused on convergence theory and proposed a method for selecting the meta-learning rate. In contrast, the current study explores the relationship between inner and meta-learning rates, showing that a larger meta-learning rate can be achieved when the inner learning rate is close to its upper bound. The study explores the relationship between inner and meta-learning rates, indicating that a larger meta-learning rate is possible when the inner learning rate is close to its upper bound. Linear regression results are shown with simplifications, matching theoretical expectations. Practical results without simplifications include sinusoid regression and classification tasks on Omniglot and MiniImagenet datasets using neural networks. The meta-objective used averages task-specific objectives, and the true function has the same architecture as the model. Steepest gradient descent is employed to minimize mean squared loss with one step taken during updates, focusing on a single task during training. The study focused on training a single task with specific parameters and meta parameters. Training loss was computed after 500 iterations with different values of \u03b1 and \u03b2. The eigenvalues of the Hessian matrix were analyzed, with \u03b1 = 5e-2 and \u03b2 = 7e-1 chosen as the closest to minima. Results were compared with theoretical bounds in linear regression, showing good agreement. Sinusoid regression experiments were also conducted. In Section 3.1, a sinusoid regression was conducted using a multilayer perceptron with specific parameters. Training loss was computed after 500 iterations with different values of \u03b1 and \u03b2. The results showed that a larger \u03b2 can be used when \u03b1 is close to the value causing loss divergence. No simplification was applied in the experiment. The experiment involved using different data for updating task-specific parameter and meta parameter, considering multiple tasks, and using SGD as an optimizer. The study confirmed that MAML allows larger \u03b2 if \u03b1 is close to \u03b1 c. Classification tasks were performed on Omniglot and MiniImagenet datasets, following a five-way one-shot classification setup with specific parameters. Training losses were computed with various values of \u03b1 and \u03b2 for both datasets. The experiment confirmed that larger \u03b1 is beneficial for stabilizing MAML training, with \u03b2 values varying depending on \u03b1. The training losses for classification tasks on Omniglot and MiniImagenet datasets were analyzed with different \u03b1 and \u03b2 values, showing that larger \u03b1 leads to smaller training loss. The results validate the applicability of the theory in practice. The practical implication of tuning learning rates is highlighted, with the importance of identifying the largest possible \u03b1 for MAML. The inner learning rate \u03b1 plays a crucial role in quickly adapting to new tasks, determining the step size for fine-tuning the model. Identifying the critical \u03b1 c not only helps in robustifying the model but also in finding a good initial parameter. Simplified MAML is likened to training with a negative gradient penalty, with the inner learning rate \u03b1 being a key factor in the process. Based on the formulation of gradient penalty, the sufficient conditions for local convergence of simplified MAML were derived. The upper bound of the meta-learning rate \u03b2 depends on the inner learning rate \u03b1, with a guideline proposed for determining \u03b1 and \u03b2. Experimental validation confirms the theory's practical applicability. In this section, we analyze cases of waiving simplifications in MAML loss. When training and test data differ, the simplified loss is interpreted as a Taylor series expansion. If task-specific parameters are updated with k-step SGD, the loss can be written accordingly. Conducting a sinusoid regression, the magnitude of T g and H is explored. In Section 5.2, the total number of iterations is 50000 with fixed learning rates \u03b1=1e-2 and \u03b2=1e-3. Tg is calculated numerically with the training error. Large eigenvalues of H are crucial for learning rate bounds. The condition for simplified MAML to converge to local minima is derived considering Tg. The Hessian matrix of the loss function can be diagonalized when Tg is considered. The simplified MAML loss function can be diagonalized when considering the Hessian matrix H. The relationship among eigenvalues of H and Tg is important for determining local minima. Majorization of vectors x and y is defined by their relationship in sorting order. The curr_chunk contains a series of numbers ranging from 0.1 to 0.8 in descending order."
}