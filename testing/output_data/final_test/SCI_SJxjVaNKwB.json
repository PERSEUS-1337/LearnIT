{
    "title": "SJxjVaNKwB",
    "content": "The recent development of Natural Language Processing (NLP) has led to the success of large pre-trained models like BERT. However, these models are too large and slow for deployment on mobile devices. To address this issue, MobileBERT is proposed as a compressed and accelerated version of BERT. MobileBERT maintains task-agnostic capabilities and achieves competitive results while being 4.3x smaller and 4.0x faster than the original BERT-BASE model. MobileBERT is 4.3x smaller and 4.0x faster than BERT-BASE, achieving competitive results on NLP benchmarks. It shows performance degradation of 0.6 GLUE score on natural language inference tasks and achieves a 90.0/79.2 dev F1 score on SQuAD v1.1/v2.0 question answering tasks. The NLP community has seen a revolution in pre-training self-supervised models like BERT, which has substantial accuracy improvements compared to training from scratch. MobileBERT is proposed as a task-agnostic lightweight pre-trained model to address the issues of heavy model size and high latency in BERT. It aims to enable deployment on resource-limited mobile devices for tasks like machine translation and dialogue modeling. Task-specific compression methods have been explored, but MobileBERT fills the gap for a model that can be fine-tuned on various NLP tasks similar to BERT. MobileBERT is designed to be a task-agnostic lightweight model that addresses the issues of heavy model size and high latency in BERT. It involves fine-tuning the original large BERT model into task-specific teachers and then distilling, which is more complex and costly than directly fine-tuning a task-agnostic compact model. Simply creating a narrower or shallower version of BERT and training it with prediction and distillation losses results in significant accuracy loss, as shallow networks lack representation power and narrow, deep networks are hard to train. MobileBERT aims to be as deep as BERT LARGE but with narrower layers using bottleneck structures and balancing self-attentions and feed-forward mechanisms. MobileBERT is a task-agnostic lightweight model that transfers knowledge from IB-BERT using bottleneck structures. It is storage and computationally efficient for mobile environments. Despite being smaller and faster, MobileBERT achieves competitive results compared to BERT BASE on NLP tasks. It shows minimal performance degradation on GLUE tasks and obtains high F1 scores on the SQuAD question answering task. MobileBERT achieves a 90.3/80.2 dev F1 score in question answering, outperforming BERT BASE by 1.5/2.1 points. BERT utilizes Multi-Head self-Attention and Feed-Forward Network modules in its architecture for enhanced representational power. In pre-training, BERT predicts masked tokens and next sentence relationships, while in fine-tuning, it is trained on task-specific data. In the fine-tuning stage, BERT is trained on task-specific annotated data. Knowledge transfer to compress model size was first proposed by Bucilu et al. (2006) and adopted in knowledge distillation by Hinton et al. (2015). Various methods like Fitnets and Luo et al. (2016) have been used to mimic the teacher network. Yeo et al. (2018) proposed a sequential knowledge transfer scheme, while Zagoruyko & Komodakis (2016) focused on transferring attention maps and hidden states. In contrast to previous works on knowledge transfer for BERT, this study focuses on transferring similarity of hidden states and word alignment from a Transformer teacher to a non-autoregressive student. The approach does not require a fine-tuned teacher for task-specific knowledge in downstream tasks, and only uses knowledge transfer in the pre-training stage. In this study, a novel progressive knowledge transfer approach is proposed to ease the pre-training of a compact MobileBERT model. The approach involves training a wider teacher network first and then progressively training the student network to mimic the teacher layer by layer. This method is applicable to multi-head attention encoders like Transformer, BERT, or XLNet, with BERT used as an example in the description. The progressive knowledge transfer is divided into stages based on the number of layers. The progressive knowledge transfer approach involves training a wider teacher network first and then progressively training the student network to mimic the teacher layer by layer. This method is applicable to multi-head attention encoders like Transformer, BERT, or XLNet. The knowledge transfer objectives include feature map transfer and attention transfer to train the student network. The feature maps of each layer should be optimized to match those of the teacher network. The approach involves training a wider teacher network first and then progressively training the student network to mimic the teacher layer by layer. The objective is to optimize the feature maps of each layer to match those of the teacher network. Additionally, minimizing statistics discrepancies on mean and variance in feature map transfer is found to be helpful in reducing inference latency. The attention mechanism plays a crucial role in NLP performance and is a key component in Transformer and BERT models. The approach involves training a wider teacher network first and then progressively training the student network to mimic the teacher layer by layer. The training process includes minimizing the KL-divergence between the per-head self-attention distributions of the teacher and the student. The progressive knowledge transfer loss for each stage is a linear combination of the objectives. The student's layers are trained progressively by minimizing the knowledge transfer loss, with lower layers being tuned with a small learning rate rather than entirely frozen. The approach involves training a wider teacher network first and then progressively training the student network to mimic the teacher layer by layer. The training process includes minimizing the KL-divergence between the per-head self-attention distributions of the teacher and the student. After the progressive knowledge transfer, MobileBERT is further pre-trained until convergence using a combination of original masked language modeling (MLM) loss, next sentence prediction (NSP) loss, and a new knowledge distillation loss. Knowledge distillation is not performed on the NSP task as it is deemed unimportant. MobileBERT is a slimmed-down version of BERT LARGE that utilizes bottleneck modules to align its feature maps with the teacher's. Inverted bottleneck modules are used in the BERT LARGE teacher to align with the student's feature maps, referred to as IB-BERT. Through these modules, the dimensions of block outputs and inputs are adjusted, while maintaining the intra-block hidden size. The inverted bottleneck modules in IB-BERT align feature maps of the teacher and student models, allowing for a more compact student model without compromising performance. MobileBERT's bottleneck structure disrupts the balance between self-attentions and feed-forward networks, affecting parameter ratios and input sizes. MobileBERT proposes to use stacked feed-forward networks to re-balance the model due to narrower bottlenecks in the self-attentions. Layer normalization and gelu activation are replaced with new operations to reduce latency. NoNorm is used for linear transformation and relu activation replaces gelu activation. In MobileBERT, gelu activation is replaced with simpler relu activation for efficiency. Extensive experiments are conducted to optimize model settings for IB-BERT teacher and MobileBERT student. The original embedding table is replaced with a smaller 3-convolution table to maintain parameter consistency. Training is done for 125k steps with 2048 batch size to save time and resources. Shrinking the inter-block size helps compress the model while preserving its power. The design philosophy for the teacher model is to use a small inter-block hidden size without accuracy loss. Experiments show that decreasing the inter-block hidden size doesn't affect BERT performance until it's smaller than 512. The IB-BERT LARGE with an inter-block hidden size of 512 is chosen as the teacher model. Additionally, experiments on shrinking the intra-block hidden size show potential for bridging the gap between student and teacher models. The intra-block hidden size is crucial for BERT performance, unlike the inter-block hidden size. Reducing the number of heads from 16 to 4 does not harm performance. A compression ratio of 4\u00d7 for BERT BASE is sought, with MobileBERT models designed for selection based on different parameter ratios in MHA and FFN. Experimental results in Table 1 show varying balances between parameters. The experimental results in Table 1 show varying balances between self-attentions and feedforward networks. The model performance peaks when the parameter ratio in MHA and FFN is 0.4 \u223c 0.6, justifying the original Transformer's choice of 0.5. The student model has 128 intra-block hidden size and 4 stacked FFNs for accuracy and training efficiency. The teacher model has 4 attention heads for progressive knowledge transfer. Table 2 displays the model settings of IB-BERT LARGE teacher and MobileBERT student, using BooksCorpus and English Wikipedia for pre-training data. To achieve the same accuracy as original BERT LARGE, IB-BERT LARGE was trained on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer. MobileBERT underwent progressive knowledge transfer over 24 layers in 240k steps. Fine-tuning for downstream tasks involved searching for optimal hyperparameters including batch sizes, learning rates, and number of epochs. MobileBERT typically requires a larger learning rate and more training epochs compared to original BERT for fine-tuning. Model selection for testing was based on performance on the development data. The model selection for testing is based on performance on the development set. MobileBERT is compared with BERT BASE and other pre-BERT models on the GLUE leaderboard, as well as with BERT-PKD. Test results are obtained by submitting predictions to the GLUE online evaluation system. MobileBERT is competitive with BERT BASE, outperforming it slightly on QNLI and RTE tasks. It also outperforms OpenAI GPT with a smaller model size. Operational optimizations slightly impact performance, but without them, MobileBERT can even outperform BERT BASE. SQuAD1.1 contains answerable questions, while SQuAD2.0 includes unanswerable questions. MobileBERT is fine-tuned on SQuAD2.0 following BERT's approach. MobileBERT outperforms BERT BASE and DocQA on SQuAD dev datasets and QNLI task. Ablation study shows components contributing to performance on diverse GLUE tasks. In a set of ablation experiments, Attention Transfer (AT), Feature Map Transfer (FMT), and Pre-training Distillation (PD) were conducted without operational OPTimizations (OPT). MobileBERT was compared with compact BERT models from Turc et al. (2019) and a BERT baseline model. The performance of MobileBERT on mobile devices was verified using Tensorflow Lite APIs, with results showing the effectiveness of Feature Map Transfer. The results in Table 5 show that Feature Map Transfer significantly improves MobileBERT performance, along with positive contributions from Attention Transfer and Pre-training Distillation. Operational OPTimizations slightly decrease performance but provide a crucial speedup. MobileBERT outperforms BERT SMALL and BERT MEDIUM on various tasks, but still lags behind IB-BERT LARGE. The attention transfer method helps the student mimic the teacher's attention distributions well. Multi-head attention is crucial in BERT's non-linearity, explaining minor improvements in the ablation table. MobileBERT is a task-agnostic variant of BERT with a progressive knowledge transfer method and unique architecture design. Layer-wise pre-training of neural networks, dating back to Deep Belief Networks (DBN) and stacked auto-encoders, helps mitigate the optimization problem by initializing weights. Model compression techniques like quantization and pruning can further reduce MobileBERT's size and inference latency. There is potential for compressing the embedding table significantly. In this paper, a progressive knowledge transfer scheme is proposed to combine the benefits of layer-wise pre-training and end-to-end learning for neural networks. The method involves using a wider teacher to guide the pre-training of a narrower student, optimizing the use of labels and rewards. This approach addresses the challenges of training neural networks efficiently from scratch, especially for mobile vision applications. Popular lightweight operations like depth-wise convolution cannot be directly applied to Transformer or BERT. Group LSTMs, incorporating group convolution into RNNs, have been explored in NLP literature. Recent research focuses on compressing Transformer and BERT models, with approaches like Block-Term Tensor Decomposition and structured memory layers to replace feed-forward networks. These methods aim to achieve model compression and improved efficiency in tasks like machine translation. Mobile-BERT reduces overheads in self-attentions and feed-forward networks by using a bottleneck structure. Operational optimizations like replacing LayerNorm with NoNorm and gelu activation with relu activation effectively reduce latency without decreasing FLOPS. This highlights the difference between real-world inference latency and theoretical computation overhead. In pre-training MobileBERT, hyperparameters (\u03bb, \u00b5, \u03b2, \u03b3) are used to balance loss terms. Specific values are set for these hyperparameters in experiments. For fair comparison with original BERT, the same preprocessing scheme is followed. Distillation in pre-training uses hyperparameter \u03b1 to balance original masked language modeling loss and distillation loss. Another task-agnostic compressed BERT model has been proposed in unpublished work. In a recent unpublished work, a task-agnostic compressed BERT model called DistilBERT, which is a 6-layer truncated version of BERT BASE, has been proposed. The distillation process is similar to pre-training distillation. This paper introduces conjugate architectures for knowledge transfer and a progressive scheme to transfer knowledge from teacher to student. Additionally, a brief overview of tasks in the GLUE benchmark, including CoLA, is provided. The task involves predicting grammaticality of English sentences using Matthews correlation coefficient. SST-2 is about sentiment prediction in movie reviews, evaluated by accuracy. MRPC focuses on determining if sentence pairs are equivalent, evaluated by accuracy and F1 score. The Semantic Textual Similarity Benchmark assesses similarity in sentence pairs. The task involves predicting similarity scores for sentence pairs annotated with human scores from 1 to 5, evaluated by Pearson and Spearman correlation coefficients."
}