{
    "title": "S1fcY-Z0-",
    "content": "Bayesian hypernetworks are proposed as a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork transforms a noise distribution to a distribution over the parameters of another neural network using variational inference. This method allows for efficient estimation of the posterior and can represent a complex multimodal approximate posterior with correlations between parameters. In practice, Bayesian hypernetworks show better defense against adversarial examples than dropout and perform well on tasks evaluating model uncertainty. Bayesian DNNs model the full posterior distribution of a model's parameters given the data, providing better calibrated confidence estimates compared to the traditional MAP estimate approach. This helps in addressing parameter uncertainty and reducing predictive uncertainty in deep learning applications. Bayesian DNNs use variational inference to maintain a distribution over parameters, offering better calibrated confidence estimates and defense against adversarial attacks. However, variational approximations may underestimate uncertainty, especially when using a restricted family of distributions. Bayesian hypernetworks (BHN) propose a flexible posterior parametrized by a DNN for sampling from the approximate posterior q(\u03b8) for another DNN. An invertible hypernet enables Monte Carlo estimation of the entropy term in variational inference training. The paper reviews Bayesian DNNs, explains the components of the approach, and details the design choices for efficient training of BHNs. Experiments validate the expressivity of BHNs. The text discusses the validation of Bayesian hypernetworks (BHNs) through experiments, showcasing their competitive performance. It also reviews prior work on Bayesian neural networks and different methods for learning an approximate posterior distribution. The focus is on variational inference as the most popular approach. Notable recent work includes interpreting dropout as a variational inference method. The text discusses Bayesian hypernetworks (BHNs) and their advantages over other methods like Bayes by Backprop and multiplicative normalizing flows. BHNs offer a unimodal approximate posterior and a flexible distribution for parameter dependencies. They address issues with scaling and use a hypernet to generate scaling factors for a factorial Gaussian distribution. The variational autoencoder (VAE) family of generative models is a well-known application of variational inference in deep neural networks. VAEs approximate the posterior over latent variables, while Bayesian DNNs approximate the posterior over model parameters. Hypernetworks are neural nets that output parameters of another neural net, forming a single model trained by backpropagation. The number of parameters in a DNN scales quadratically with the number of units per layer. The text discusses the challenge of parametrizing a large primary neural network due to the quadratic scaling of parameters with the number of units per layer. Various methods like Conditional Batch Norm, Conditional Instance Normalization, and Feature-wise Linear Modulation address this issue by using hypernetworks to output scale and shift parameters for each neuron. The proposed Bayesian hypernetworks utilize weight normalization and a differentiable directed generator network as a generative model. The text discusses using a differentiable directed generator network (DDGN) as a generative model for primary net parameters. DDGNs transform noise into samples from a complex distribution, commonly used in deep generative models like VAEs and GANs. Techniques for invertible DDGNs and normalizing flows are employed to reduce the cost of matrix determinant computations. Variational inference is applied to Bayesian deep nets to maximize a lower bound on the marginal log-likelihood of the data. This involves estimating model parameters and approximating the posterior distribution over unobserved random variables. The goal is to produce Bayesian hypernets by composing methods to learn an approximation to the true posterior distribution. In variational inference for Bayesian deep nets, the evidence lower bound (ELBO) is maximized to approximate the posterior distribution. The focus is on modeling conditional likelihoods and using stochastic gradient methods for optimization. Monte Carlo sampling is used to estimate expectations in deep nets, allowing for training by backpropagation. The entropy term and likelihood of test data-points can be evaluated efficiently. Bayesian hypernets use a DDGN to transform noise into samples from q(\u03b8), enabling efficient Monte Carlo estimation of expectations. However, computing the entropy term of the ELBO requires evaluating the likelihood of generated samples, which is challenging for popular DDGNs like VAEs and GANs. In this work, invertible h is used to compute q(\u03b8) by applying the change of variables formula. Techniques like RealNVP (RNVP) and Inverse Autoregressive Flows (IAF) are employed for training invertible DDGNs efficiently. Weight normalization reparametrization is used to scale Bayesian hypernets to large primary networks. The weight normalization reparametrization is used to scale Bayesian hypernets to large primary networks, overcoming computational limitations. The hypernet is used to generate weight matrices of the primary net, with shaded regions representing posterior uncertainty. This parametrization restricts approximate posteriors but allows for multimodality. The hypernet employs weight normalization and recommends small initializations for stability. Clipping softmax outputs is crucial for numerical stability. Experiments on MNIST, CIFAR10, and regression tasks are conducted to evaluate uncertainty estimation. Active learning and anomaly detection benefit from uncertainty estimates. The natural use of uncertainty estimates in anomaly detection and active learning provides regularization benefits and helps in weighing evidence for different hypotheses. Adversarial examples pose a challenge in deep learning, and finding defenses against them remains an open issue. The hypernet architecture experiments with different models and finds that IAF performs better. In experiments with different models, the IAF performs better. An isotropic standard normal prior is used on the scaling factors of the weights. Adam with default hyper-parameter settings BID22 and gradient clipping are used. Mini-batch size is 128, and the same noise-sample is used for all examples in a mini-batch. Independent noise was experimented with but did not show benefits. Baselines for comparison include Bayes by Backprop, MC dropout, and non-Bayesian DNN baselines. The network behavior is demonstrated on a toy 1D-regression problem, showing increased uncertainty away from observed data. The effects of scaling BHNs via weight norm parametrization are evaluated. The proposed method for scaling Bayesian Hypernets (BHNs) via weight norm parametrization is compared with a model that generates the full set of parameters, showing similar results. BHNs demonstrate the ability to learn multi-modal, dependent distributions. They act as a regularizer, outperforming dropout and traditional mean field methods. The experiment involves training an over-parametrized linear network on a dataset and showing that BHNs can capture different modes effectively. In experiments, Bayesian Hypernets (BHNs) act as a regularizer, outperforming dropout and traditional mean field methods. Results show BHNs perform similarly to dropout on full datasets of MNIST and CIFAR10. Increasing flexibility of the posterior with more coupling layers improves performance, especially compared to models with 0 coupling layers. Experiments with 8 coupling layers on a subset of MNIST and CIFAR10 show promising results. In active learning experiments, Bayesian Hypernets outperform other approaches after acquiring sufficient data. Warm-starting improves stability but may hurt performance compared to randomly re-initializing parameters. The baseline model without dropout is competitive with MCdropout and outperforms the Dropout baseline. The experiments replicate the MNIST architecture and training procedure, starting with 20 examples and acquiring 10 new examples at a time. The network is re-initialized after each acquisition. In active learning experiments, Bayesian Hypernets outperform other approaches after acquiring sufficient data. Warm-starting improves stability but may hurt performance compared to randomly re-initializing parameters. For anomaly detection, Bayesian DNNs outperform their non-Bayesian counterparts in determining out-of-distribution inputs. Bayesian DNNs outperform non-Bayesian models in anomaly detection using MC to estimate predictive posterior and score datapoints. Acquisition functions from active learning literature are effective for scoring anomalies, with the \"variation ratio\" function giving the best performance. BHN and MCdropout show significant performance gains over non-Bayesian models. In anomaly detection tasks, Bayesian DNNs like BHN and MCdropout outperform non-Bayesian models. Using scores reflecting dispersion in posterior samples improves performance. The detection procedure can also be used for identifying adversarial examples. Bayesian hypernets (BHNs) are introduced as a new method for variational Bayesian deep learning, using an invertible hypernetwork as a generative model of parameters. BHNs outperform dropout in detecting adversarial examples and errors, especially when constructed with IAF. The BALD values computed by BHNs provide a better-than-random acquisition function. Li & Gal (2017) and BID28 used different numbers of model samples to estimate gradient, with results showing increased uncertainty measures with more perturbation added to the data. When more perturbation is added to the data, uncertainty measures increase, particularly for BALD and Mean STD scores in Bayesian hypernets (BHNs) compared to dropout. Results for adversary and error detection are shown in terms of AUC of ROC with increasing perturbation. BHNs offer benefits over simpler methods for Bayesian deep learning and can address issues of overconfidence in variational approximations. Future work could explore different methods of parametrizing BHNs. In this paper, weight normalization is employed in the primary network, treating only the scaling factors as random variables. An isotropic Gaussian prior is chosen for the scaling factors, resulting in an L2 weight-decay penalty. The direction and bias parameters of the primary network are denoted as v and b, while \u03c6 represents the parameters of the hypernetwork. During training, the bound is optimized with respect to {v, b, \u03c6}, where v and b are the direction and bias parameters of the primary network, and \u03c6 represents the parameters of the hypernetwork."
}