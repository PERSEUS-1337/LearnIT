{
    "title": "HyFaiGbCW",
    "content": "We investigate how a Reservoir Computing Network (RCN) learns concepts like 'similar' and 'different' between image pairs using a small dataset and generalizes to unseen data types. The RCN trained on image-pairs from MNIST or depth maps of visual scenes can generalize to new images or scenes. Through Principal Component Analysis, we find that high dimensional reservoir states converge to unique relationships, allowing the reservoir to perform well with minimal training examples. Reservoir Computing Networks (RCNs) can identify and generalize linear and non-linear transformations, performing better than deep Siamese Neural Networks (SNNs) in generalization tasks on datasets like MNIST and depth maps. This work bridges explainable machine learning with biological learning using small datasets, pointing to new directions in learning processes. Different types of Artificial Neural Networks (ANNs, such as convolutional neural networks and stacked auto encoders) have been extensively studied for object recognition and classification. While feed-forward structures are the state of the art for classification, biological systems like the visual cortex primarily have recurrent connections, which Reservoir Computing Networks (RCNs) explain how accurate computations can be carried out with noisy physical substrate. Biological systems learn visual concepts through analogies, using only a handful of examples. Bees trained to fly towards visually similar images also showed a preference for scents that were very similar. This suggests that the brain comprehends through analogies or concepts of similarity across sensory inputs. In our framework, we aim to develop a biologically plausible learning technique inspired by nature, focusing on generalization and the ability to recognize transformations between image pairs. Feed-forward networks have not succeeded in creating an explainable model for this type of learning, as learning without comparisons is not biologically plausible. Humans, on the other hand, learn effectively with few training examples, such as a child learning about horses and donkeys by observing just a few examples. In the groundbreaking work of BID12, Recurrent Neural Networks (RNNs) rely on attractors for success. Training RNNs can be challenging due to issues like vanishing gradient, but BID3 demonstrated that slower dynamics can be achieved by using a random network of neurons with short term plasticity. This allows the system to train only the output weights, as seen in Echo State Networks (ESN) BID13 and Liquid. Echo State Networks (ESN) and Liquid State Machine (LSM) are part of Reservoir Computing (RC), known for their dynamical properties and easy scalability. RC is used in various applications such as weather prediction, stock market analysis, self-driving cars, speech processing, and more. RCNs and RNNs excel in generating chaotic dynamics, which is also observed in spontaneously active neural circuits. This study involves training RCNs on the MNIST handwritten digit database and depth maps of visual scenes to explore generalization capabilities. The study explores the generalization capabilities of Echo State Networks (ESN) and Liquid State Machine (LSM) in learning relationships between pairs of images from a moving camera. The reservoir activity reveals underlying features responsible for classification, showing that the reservoir learns relationships between images rather than individual image features. This allows for generalization to all image pairs, seen and unseen. The reservoir outperforms a pair-based deep siamese neural network (SNN) in a generalization task. The study demonstrates that the reservoir in Echo State Networks (ESN) performs well in recognizing linear combinations of learned transformations in images. This can be beneficial in computer vision for identifying similar transformations between images, even in non-linear scenarios like a moving camera. The Echo State Network (ESN) used for training and classification consists of a hidden layer of recurrently interconnected non-linear nodes and an output layer. The reservoir is described as a dynamical system with a reservoir state vector. The reservoir in Echo State Networks (ESN) is a dynamical system with a state vector that transforms input data into a higher dimensional space using randomly initialized recurrent connection weights. The reservoir operates in a chaotic regime with a spectral radius less than 1 and uses hyperbolic tangent as the activation function. It converts input images into a 'time-series' for analysis of time-series data, and achieves optimal performance even with sparse output and reservoir weights. The MNIST database contains 70000 images of digits 0-9, each 28x28 pixels in size. Input images are converted to a time series for classification using Echo State Networks. The input is processed column by column, with the reservoir state formed by concatenating the state at each timestep. The MNIST database contains 70000 images of digits 0-9, each 28x28 pixels in size. Input images are converted to a time series for classification using Echo State Networks. Images are classified into 5 labels based on similarity and difference, with transformations like rotation, zoom, and blur. Very similar images undergo small non-linear transformations, demonstrating robustness to noise. In the experiment, different transformations are applied to images from the MNIST database, including rotation, zoom, and blur. Pairs of images are created with non-linear relationships between the base and transformed image. The readout layer consists of a vector with five elements. In the experiment, pairs of images with different transformations are classified into five labels based on the differential reservoir state. The readout layer representation assigns specific values to similar, rotated, zoomed, blurred, and different pairs. During testing, the reservoir assigns probabilities to each label, and the pair is classified based on the highest probability. The weight matrix W out is optimized to minimize the mean squared error between the reservoir output and the target signal. The reservoir output is computed using Ridge Regression to minimize squared error and regularize the weights, making the system robust to overfitting and noise. Ridge regression calculates W out by minimizing squared error J(W out ) while regularizing the norm of the weights. The reservoir output is computed using Ridge Regression to minimize squared error and regularize the weights. The performance of the reservoir in identifying relationships between test image pairs of digits 6-9 from the MNIST dataset is presented, showing that a training set size of \u223c250 image pairs gives a reasonable trade-off between performance and computational efficiency. The reservoir performance is analyzed with a training data size of 250 pairs, showing improved efficiency with a reservoir size up to 750 nodes. Varying \u03b3 does not show a clear pattern for optimal performance. Sparse reservoirs can generalize transformations accurately with few training pairs. Reservoir and single node activity for all labels are shown in FIG2, with individual nodes lacking decipherable information. The reservoir converts input images to reservoir space through a non-linear operation, allowing it to learn relationships between image pairs and generalize to unseen images. This process is explained by dynamical systems theory, where the reservoir activity converges onto an attractor representing the relationship between input image pairs. The reservoir activity converges onto an attractor representing the relationship between input image pairs. The study focuses on differential reservoir states for image-pairs with common relationships, analyzing convergence over time. The study analyzes the correlation between differential reservoir activity for different image transformations over time. It shows that the overlap in principal component direction is higher for the same transformation compared to different transformations, with slightly higher values for the same digit pairs. The reservoir state converges onto specific attractors in the reservoir space, with image pairs sharing the same transformation clustering together. The RCN trains these attractors instead of the entire reservoir space, allowing for generalization with a smaller training set. The reservoir can identify all transformations in an image composed of multiple transformations. The RCN trains on individual transformations for digits 0-5 and tests on combined transformations for digits 6-9. The reservoir categorizes image-pairs with simultaneous rotations and blurs as rotated with the highest probability, showing better learning of rotation. The reservoir learns rotation better than blurring, with a high performance of 0.986 in classifying blurred or rotated images. Performance remains high at 0.989 when tested on purely rotated images of digits 6-9. By thresholding the difference in probabilities, significant transformations can be identified. The reservoir can distinguish individual transformations in combined images. The reservoir can generalize learning of transformations to unseen image pairs of digits and consistently performs well in identifying combinations. The model is validated by comparing its performance with a deep SNN using contrastive loss for image pairs. The reservoir has 1000 nodes with \u03b3 = 0.5 and sparsity 0.9, outperforming an equivalent SNN with 8 layers of 128 nodes each. Training is done for 40 epochs on the SNN and once on the reservoir with 300 image pairs. The reservoir outperforms the SNN in identifying combinations of image pairs of digits. Training is done for 40 epochs on both models with 300 image pairs. The SNN struggles with untrained digits (6-9) while the reservoir performs well on both trained (0-5) and untrained digits, showing it learns the underlying transformation in the pairs. The superior performance of the reservoir in identifying combinations of image pairs of digits is attributed to the convergence of the dynamical reservoir state for all rotated images. In contrast, the deep SNN struggles with untrained digits and training occurs explicitly on the images, leading to poorer generalization. The reservoir learns and generalizes relationships between images from a moving camera, which is important in computer vision. The method is implemented on depth maps from 6 visual scenes recorded indoors. The systems are trained to identify pairs of depth maps as similar or different scenes. The reservoir performs better on untrained scenes compared to the SNN, which classifies randomly. Both systems have high performance on trained scenes. The reservoir system shows high performance in identifying frames with similar depth maps from unseen scenes, with potential applications in scene or object recognition using a moving camera. This method utilizes RCNs for image classification, offering a biologically plausible approach that allows for analytical interpretation through a dynamical systems lens. Differential reservoir states from input image-pairs with common transformations exhibit aligned principal components, indicating the presence of attractors in reservoir space. The reservoir system allows for training on smaller datasets by utilizing attractors in reservoir space corresponding to image transformations. It preserves relationships learned and generalizes to unseen images. The reservoir maps image space to preserve common transformations, outperforming deep SNNs in generalization tasks. The method is computationally efficient and biologically plausible. Our method, Reservoir Computing Networks (RCNs), outperforms state-of-the-art machine learning techniques like SNNs in generalization tasks due to its ability to function as a dynamical system with 'memory'. The strength of our work lies in its ability to generalize to untrained images and explain this through reservoir dynamics and PCA. This aligns with the concept of explainable Artificial Intelligence, with potential for exploring different reservoir architectures that model the human brain. One interesting direction for future research is to explore different reservoir architectures that model the human brain better. Another direction is to use RCNs to study videos and investigate how the reservoir generalizes in the action domain. It is predicted that as image complexity increases, a more sophisticated reservoir will be required to maintain performance."
}