{
    "title": "BkgnhTEtDS",
    "content": "In this work, a method is proposed to interpret and enhance predictions of black-box recommender systems by extracting feature interactions from a source model and incorporating them into a target model. The approach is applicable in general settings and was tested on ad-click prediction, showing superior performance compared to existing models. Additionally, the method can offer insights beyond recommendation domains. Interpreting interactions in recommender systems can provide new insights beyond recommendation domains. State-of-the-art systems are becoming more complex and inscrutable, with models using intricate components for optimal accuracy. Efforts have been made to understand feature relationships, but there is still no method to interpret feature interactions in generic recommender systems. In this work, a novel approach called Global Interaction Detection and Encoding for Recommendation (GLIDER) is proposed to detect and encode feature interactions in recommender systems. GLIDER utilizes LIME for feature interaction detection and sparse feature crossing to improve prediction performance in ad-click recommendation experiments. GLIDER is proposed to detect and encode global feature interactions in blackbox recommender systems, improving recommendation accuracy. It uses interaction detection and LIME to interpret feature interactions learned by a source model, identifying consistent interactions across multiple data samples. Interactions in black-box recommender models can be detected and encoded using various methods like RuleFit, Additive Groves, Tree-Shap, and Neural Interaction Detection. Some methods like Anchors, Agglomerative Contextual Decomposition, and Context-Aware methods attempt to interpret feature groups in black-box models, but not specifically for identifying feature interactions. There are increasing methods for explicitly representing interactions in models. Representation: Various methods have been developed to explicitly represent interactions in models, such as incorporating multiplicative cross terms in neural network architectures and using attention as an interaction module. These approaches aim to enhance the neural network's function approximation and improve predictive performance. Recent work has focused on detecting interactions in data and encoding them via feature crossing. Our work emphasizes the importance of interaction interpretations for explicit encoding, particularly in model interpretation. Global interpretations are increasingly desired to summarize model behavior. LIME and Integrated Gradients are popular methods for local interpretation, while shuffle-based feature importance, submodular pick, and visual concept extraction are used for global interpretations. The paper discusses local and global interaction interpretations, with notations for vectors and dataset features. Feature interactions in a black-box model are defined by statistical non-additive relationships between features, where the model cannot be decomposed into subfunctions that do not depend on specific interaction variables. This is crucial for understanding the behavior of the model in classification tasks. GLIDER focuses on interpreting interactions in recommender systems, involving dense numerical and sparse categorical features represented through embeddings. The methodology includes local and global interaction detection, applicable beyond recommender systems. Recommender systems have pervasive applications in real-world systems with sparse features, making interaction detection challenging. LIME proposes perturbing data instances to detect feature interactions in a black-box model. LIME perturbs data instances by switching random features to 0 or \"off\" state to detect feature interactions in a black-box model. Feature interaction detection is crucial for identifying interactions in a dataset, but it requires expensive pre-processing. Neural Interaction Detection (NID) is a method that efficiently detects generic nonadditive and arbitrary-order statistical feature interactions in a dataset. It uses a lasso-regularized multilayer perceptron (MLP) to identify features with high-magnitude weights to common hidden units. NID is effective for detecting interactions in black-box models without the need for expensive pre-processing. Neural Interaction Detection (NID) efficiently detects feature interactions in black-box models using a lasso-regularized multilayer perceptron (MLP). A function called MADEX(f, x) identifies top-k feature interactions, regardless of model type. Global interactions are approximated using linear regression with multiplicative terms. GLIDER's first step involves using MADEX to detect global interactions in a source recommender system by counting occurrences of detected interactions in a batch of data instances. The algorithm in Alg. 1 detects interactions in a batch of data instances by counting their occurrences. It ranks global interactions and serves as a sanity check to ensure detected feature combinations are not random. The process is efficient, taking less than 3 hours for 1000 samples with 40 features on a 32-CPU server. The algorithm is parallelizable and generates synthetic features for a target recommender system by crossing sparse features indexed in the interactions. If dense features are involved, they are bucketized before crossing. When detecting interactions in data instances, the algorithm ranks global interactions efficiently. It generates synthetic cross features for a recommender system by crossing sparse features. If dense features are present, they are bucketized before crossing to handle dense features. The cross feature is an n-ary Cartesian product among sparse features, with a truncated form generated to reduce cardinality. GLIDER is a method for model distillation or enhancement in a target recommender system. It reduces the cardinality of sparse ID cross features by truncating them. It can be used as a teacher-student distillation process or to enhance the model's ability to represent interactions. The effectiveness of GLIDER is studied on real-world data in experiments. In experiments, GLIDER's effectiveness on real-world data is studied. Hyperparameters for local interaction interpretation include using 5000 perturbation samples for training models. NID is used as the interaction detector, with specific MLP architectures and regularization. LIME perturbations involve establishing what a binary 0 maps to in raw data instances. GLIDER is tested on real-world data with hyperparameters for local interaction interpretation. Perturbations are made in the DNA experiment by using a random nucleotide. In the graph experiment, nodes near a test node are perturbed by zeroing each node. Experiments are conducted on recommender models for CTR prediction, including Wide&Deep, DeepFM, Deep&Cross, xDeepFM, and AutoInt. AutoInt is considered the state-of-the-art model, and its settings and data splits are used. Other recommender models use public data. For other recommender models, public implementations with original architectures are used, setting embedding sizes to 16 and tuning the learning rate and optimizer to match AutoInt's performance. Adagrad optimizer with learning rates {0.01, 0.001} is utilized. Benchmark CTR datasets Criteo and Avazu are employed, containing 40+ million user records on ad clicks. Test prediction performance on Criteo and Avazu datasets is evaluated by encoding top-K global interactions in baseline recommender systems. The \"Setting\" column is labeled relative to the source of detected interactions: AutoInt. Global interaction detection is performed on Criteo and Avazu datasets, with significant speed-ups achievable through parallelization. Detection results are visualized in Figure 2, with the top-10 interactions in the Avazu dataset shown in Table 2. The top-10 global interactions detected in the Avazu dataset are shown in Table 2. These interactions are detected frequently across data instances, with some appearing in more than half of the batch. For example, the interaction between \"hour\" and \"device ip\" is explained by users' ad-click behaviors being dependent on their time zones. The detected global interactions are considered informative for modeling and are encoded in target baseline models using truncated feature crosses. The interactions detected in the Avazu dataset are encoded in baseline models, with final cross feature IDs occurring more than T = 100 times over a training batch of one million samples. Prediction performance results for different models with and without interactions are shown in Table 5. Using GLIDER can often reach or exceed the 0.001 significance level in CTR prediction tasks, especially for the main Criteo benchmark dataset. Performance gains are achieved with limited extra model parameters, thanks to feature cross truncations. Test performance of AutoInt varies with different K on the Criteo dataset, showing potential for improving baselines' performance. AutoInt's detected interactions can enhance performance on Criteo, suggesting the benefit of learning interactions explicitly. GLIDER performs well on industry production models with large private datasets. GLIDER's interaction interpretations are not limited to recommender systems, as demonstrated on general black-box models like ResNet152. The curr_chunk discusses various models such as image classifier pretrained on ImageNet, Sentiment-LSTM, DNA-CNN, and GCN, along with their validation and interpretation of interactions. It emphasizes the importance of encoding feature interactions for black-box models. The section discusses using neural network function approximators to test the significance of detected interactions in black-box models. Starting with linear regression at k=0, MLPs are incrementally added until validation performance no longer improves. Test prediction performances are compared for different values of k, with the average number of features ranging from 16 to 189. The average number of features among black-box models ranges from 16 to 189. Adding feature interactions for DNA-CNN, Sentiment-LSTM, and ResNet152, and adding node interactions for GCN result in significant performance gains. Qualitative results show interactions detected by MADEX and selected features by LIME's linear regression on ResNet152 and Sentiment-LSTM. Interpretations from MADEX and LIME selection are complementary. Interactions with majority overlap are merged if the overlap coefficient is greater than 0.5. MADEX columns display selected features from interactions between Quickshift superpixels. These interactions can form single or multiple regions in the image, complementing LIME's feature selection. For example, in the \"desktop computer\" classification, interaction detection finds one computer while feature selection finds the other. MADEX's interactions also complement LIME's selected features for Sentiment-LSTM interpretations, showing salient word combinations like \"science fiction\" and \"I like.\" In experiments on DNA-CNN, interactions between \"CACGTG\" nucleotides were consistently detected, forming a canonical DNA sequence. GLIDER was proposed to detect and encode global feature interactions in recommender systems, improving CTR prediction accuracy. Validation was done on image, text, and graph classifiers, aiming to understand the predictive nature of feature interactions. Future research will focus on the role of feature interactions in model integrity. In future research, the focus is on understanding how feature interactions impact automatic recommendations. The study compares the performance gains of increasing embedding size with GLIDER encoding in recommender models. Results show that directly increasing embedding size did not provide the same level of performance gains as GLIDER. Table 6 compares model parameters between baseline models with enlarged embeddings and original baselines + GLIDER. Test prediction performance for Criteo and Avazu datasets is shown in Table 7. The study examines the impact of dense feature bucketization on parameter efficiency and prediction performance of AutoInt using the Criteo dataset with 13 dense features. Figures 5 and 6 illustrate the effects of varying the number of dense feature buckets on parameters and test logloss, as well as on the embedding sizes of cross features involving dense features. The study analyzes the impact of dense feature bucketization on parameter efficiency and prediction performance of AutoInt using the Criteo dataset. The effects of varying the number of dense feature buckets on parameters and test logloss, as well as on the embedding sizes of cross features involving dense features, are illustrated. The requirement for a valid cross feature ID to occur more than T times restricts parameter growth, leading to a decrease in the number of cross feature IDs in some cases. Prediction performance degrades beyond 100 buckets, potentially due to overfitting. Ranked results for interactions discovered by MADEX in Sentiment-LSTM are also presented. The study presents ranked results for interactions discovered by MADEX in Sentiment-LSTM. The top-1 interactions are provided on random sentences from the SST test set after preprocessing to remove stop words. Occurrence counts of word interactions are obtained by detecting the same word interactions across different sentences. Interaction candidates are collected by running MADEX over all sentences in the SST test set and identifying word interactions that appear multiple times. The assumption is made that word interactions are ordered. After preprocessing to remove stop words, MADEX identifies word interactions in sentences from the SST test set. The interactions are ordered but not necessarily adjacent, and occurrence counts are obtained by detecting the same interactions across different sentences in the IMDB dataset. The interactions with the highest detection counts are compared between MADEX and baselines on identifying feature interactions learned by complex models like XGBoost and Multilayer. In evaluating interaction detection performance, MADEX uses synthetic data with known ground truth interactions. The baselines include Tree-Shap for tree-based models, MLP-ACD+ for Multilayer Perceptron, and LSTM-ACD+ for Long Short-Term Memory Network. The performance is assessed by generating synthetic data samples with continuous features. MADEX evaluates interaction detection performance using synthetic data with known ground truth interactions. Complex models like XGBoost, MLP, and LSTM are trained on the data, and MADEX is compared to baselines like Tree-Shap, ACD+ on 10 trials of 20 data instances. Results show that MADEX competes well with Tree-Shap on tree-based models and outperforms ACD+ on MLP and LSTM, especially in the LSTM setting. The detection performance of different models like Tree-Shap, MADEX, MLP, and LSTM is compared in terms of higher-order interactions. MADEX shows competitive performance with Tree-Shap on tree-based models and outperforms ACD+ on MLP and LSTM, especially in the LSTM setting. The occurrence counts of higher-order interactions are plotted, with 3rd-order interactions being the most common. Higher-order interactions are common in ResNet152, with an 8th-order interaction appearing 13 times in the Avazu dataset. The average number of features for ResNet152 and Sentiment-LSTM are 67 and 19 respectively."
}