{
    "title": "rkEfPeZRb",
    "content": "Due to the high computational cost of training deep neural networks on large datasets, distributed training with multiple computation workers is necessary. However, communication bottlenecks arise due to frequent gradient updates. Existing methods for compressing gradients either have low compression ratios or reduce model accuracy, especially for convolutional neural networks. To address this, a method is proposed to reduce communication overhead by delaying gradient updates until a clear gradient is calculated. An efficient algorithm to compute variance with minimal additional cost is also presented. Experimental results show the effectiveness of this approach. Deep neural networks have high computational costs for training, limiting the number of model trials possible. A method is proposed to achieve high compression ratios while maintaining model accuracy, enabling distributed deep learning in commodity environments. This approach addresses the bottleneck of communication in distributed training by delaying gradient updates until a clear gradient is calculated. When creating models, distributed training using multiple computation workers is necessary due to the limited number of trials possible. However, communication bottlenecks arise when workers need to frequently exchange gradients, especially with lower bandwidth connections. This limitation hinders scalability, making multiple nodes impractical with standard Ethernet connections. Expensive high-performance interconnections like InfiniBand and Omni-Path are not widely accessible, restricting deep learning research to a small group of researchers. While methods to compress gradients for efficient communication exist, they often compromise model accuracy. To address communication bottlenecks in distributed deep learning, a new gradient compression algorithm is proposed to reduce overhead. This method focuses on sparsification approaches to improve scalability and efficiency in exchanging gradients among computation workers. The text chunk discusses a new gradient compression algorithm that focuses on the variance of the gradient for each parameter point over iterations. This method aims to improve scalability and efficiency in distributed deep learning by combining sparsification and quantization approaches. The algorithm is shown to achieve high compression ratios while maintaining model accuracy, enabling distributed deep learning in commodity environments. In Section 2, definitions and notations are provided. Section 3 reviews related work, while Section 4 presents the proposed method. Section 5 analyzes performance, and Section 6 shows experimental results. The focus is on distributed deep learning and parameter updates with compressed gradients in data parallel distributed Stochastic Gradient Descent. The communication of gradients is done synchronously in this paper. The communication of gradients in distributed deep learning poses a bottleneck due to the large size of exchanged data, impacting training time significantly. Workers in a synchronous approach experience idle computing resources while waiting for communication completion, leading to performance loss. In distributed deep learning, communication of gradients is a bottleneck due to large data size, causing performance loss. SGD updates model parameters using gradients and a loss function. Gradient compression techniques like quantization and sparsification reduce communication costs. Quantization-based approaches express gradients with fewer bits, reducing communication by up to 32 times. BID9 showed neural networks can be trained using only one sign bit per parameter. Gradient compression techniques like quantization and sparsification are used to reduce communication costs in distributed deep learning. Quantization involves encoding and decoding gradient elements with different thresholds for each weight matrix column, while sparsification sends only a small fraction of gradients, significantly reducing transmission requirements. TernGrad by Wen et al. (2017) encodes gradients with 2 bits per parameter and has been experimentally verified for speech models. BID11 proposed sending only gradients with absolute values greater than a user-defined threshold, sending only sign bits to further reduce communication costs. The algorithm sends sign bits and encoded indexes of parameters based on absolute values exceeding a threshold. BID4 extended Strom's method by introducing an adaptive threshold and repeated sparsification of gradients. Alistarh et al. (2017) proposed QSGD, which stochastically rounds gradients to quantized values with controllable trade-offs between accuracy and compression. Strom's method is limited by threshold sizes. The proposed method in this section diverges from previous works by using approximated variances of gradient elements instead of magnitudes. It reduces communication while maintaining accuracy and allows for a balance between accuracy and compression. Additionally, it can be combined with sparsity-promoting quantization methods like QSGD and Strom's method. Our method combines work with sparsity-promoting quantization methods like QSGD and Strom's method. The key idea is delaying sending ambiguously estimated gradient elements based on their variance. Parameters are updated with approximated gradient elements using fewer bits, interpreting delays as updates rather than approximation errors. Gradient elements are sent only when meeting a certain criterion. Our method combines sparsity-promoting quantization methods like QSGD and Strom's method by delaying sending gradient elements based on their variance. Parameters are updated with approximated gradient elements using fewer bits, treating delays as updates rather than errors. Gradient elements are sent only when meeting a specific criterion. The size of the mini-batch and hyperparameter \u03b1 \u2032 represent required estimation accuracy. The lemma supports our formulation for descent direction vectors. Our method delays sending gradient elements based on their variance, updating parameters with approximated gradient elements using fewer bits. Gradient elements are sent only when meeting specific criteria, with mini-batch size and hyperparameter \u03b1 \u2032 determining estimation accuracy. The formulation for descent direction vectors is supported by the lemma. Our method delays sending gradient elements based on their variance, updating parameters with approximated gradient elements using fewer bits. Each worker sends pairs of a value of a gradient element and its parameter index after deciding which gradient elements to send. The elements are quantized to 4-bit for efficient representation. The quantization process involves truncating or rounding the gradient elements based on their magnitude. If the integer value calculated exceeds 7, it is not sent, otherwise, it is encoded using 3 bits. The text discusses a method for delaying the sending of gradient elements based on their variance to reduce communication costs. It involves quantizing gradient elements to 4 bits, with values exceeding 7 not being sent and those below 7 encoded using 3 bits. The method is shown to be effective in achieving sparsity without the need for additional quantization methods. In distributed deep learning, the key operation is \"allreduce,\" which involves collecting local arrays, reducing them through summation, and broadcasting the result to all workers. This process benefits data parallel applications with optimized implementations. The proposed method converts local gradient elements to a sparse data structure, making allreduce operation no longer applicable. Instead, allgatherv communication is adopted to send calculated elements to other workers, avoiding repetitive encoding and decoding. This approach allows for high compression ratios without significant communication cost increase. The algorithm allows for efficient computation and optimization of gradient elements without the need for additional floating point operations. Variance decay is achieved by multiplying a hyperparameter to the sum of squared gradient elements. This approach minimizes communication costs while supporting large numbers of workers. The algorithm efficiently optimizes gradient elements by applying a hyperparameter to the sum of squared gradients. It compresses more with larger alpha values, ranging from one to two. The quantization of parameters can be implemented using binary operations. Modifying variance when exchanging gradient elements is solved by adjusting the formula. The algorithm optimizes gradient elements by modifying the sum of squared gradients. It shows effectiveness through experiments and compares with other methods like QSGD and TernGrad. The speedup of each iteration is estimated by gradient compression, and the total speedup of the training process is the sum of each iteration. The communication part involves broadcasting quantized values and parameter indexes to all nodes. The communication part of the algorithm involves broadcasting quantized values and parameter indexes to all nodes. An MPI function called allgatherv is used for this operation. Collective communication algorithms like the ring algorithm are efficient for large neural network models with a high number of parameters. The pipelined ring algorithm helps mitigate the unacceptable time cost of the naive ring allgatherv algorithm. The pipelined ring algorithm is used for large input data in MVAPICH's allgatherv implementation. The variance of gradients calculation dominates the computation cost, with a leading term of 2N |B| multiply-add operations. The focus is on the communication part, discussing the compression ratio's impact on speedup. Ignoring latency, the baseline is ring allreduce for uncompressed gradients with an elapsed time of 2. Linear speedup is expected in the c > p/2 range. In this section, the proposed method is experimentally evaluated to reduce communication costs while maintaining test accuracy. The method, when combined with other sparsification techniques, further reduces communication costs and can even improve test accuracy in certain scenarios. Gradient compression algorithms are evaluated based on accuracy and compression ratio using CIFAR-10 and ImageNet BID8 datasets. The hyperparameter \u03b6 is fixed at 0.999 for all experiments. For experiments on CIFAR-10, a convolutional neural network similar to VGG BID10 was used. The network was trained for 300 epochs with a weight decay of 0.0005. A total of 8 workers with a batch size of 64 each were employed, with no data augmentation applied to training images. The study trained a convolutional neural network on CIFAR-10 for 300 epochs with a batch size of 64 workers. Two optimization methods, Adam and momentum SGD, were used with specific parameters. The implementation included QSGD with compression ratios reported in the results table. The method achieved slight accuracy gain with Adam and 2-3% accuracy degradation with momentum SGD. The study compared different compression methods for communication cost reduction in distributed training. The hybrid algorithm showed significantly higher compression ratios than existing methods, enabling feasible computation with a large number of nodes on commodity infrastructure. While QSGD achieved higher accuracy, the hybrid algorithm aggressively reduced communication cost without significant accuracy degradation. Strom's method caused accuracy degradation, while the hybrid algorithm improved accuracy while reducing communication. The hybrid algorithm in distributed training showed higher compression ratios and improved accuracy compared to existing methods. Hyperparameter tuning in Strom's method was challenging, as lower thresholds did not always lead to higher accuracy. The algorithm was free from unstable behaviors observed with other thresholds. Our method for gradient compression significantly reduces communication cost with minimal accuracy degradation. It outperforms quantization-based approaches and does not cause the same accuracy degradation as other methods. The contributions of our work include a novel measurement of ambiguity. The text chunk discusses a novel measurement of ambiguity for determining gradient updates, reducing update requirements while maintaining accuracy, and combining with other compression approaches to reduce communication cost. The criterion for the measurement corresponds to an estimated variance of gradients in mini-batches. The text chunk discusses the compression of gradient elements in a matrix, using 3 bits to represent values and sending them with sign bits and index. Scatter plots show the superiority of the variance-based compression algorithm. The VGG-like network architecture used for experiments on CIFAR-10 is detailed in Table 3, with convolutional layers followed by batch normalization and ReLU activation. The code is available on GitHub."
}