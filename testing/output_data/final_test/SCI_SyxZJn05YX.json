{
    "title": "SyxZJn05YX",
    "content": "A new perspective is introduced in this paper to address the problem of classifying objects with unanimous scores for every category. It involves utilizing two sets in the feature space - one with more reliable information guiding the feature learning of the other set. This mutual learning approach aims to push towards a more compact class centroid in the high-dimensional space, benefiting both sets by making samples closer within the same category. In this approach, a feature intertwiner is used to guide object detection by minimizing distribution divergence between sets of high and low resolution objects. A historical buffer is employed to guide feature learning, and the optimal transport algorithm aligns less reliable samples with the reliable set for improved classification. Incorporated with a plug-and-play intertwiner, an evident improvement is achieved on the COCO object detection benchmark. Previous methods focused on feature representation using cross-entropy loss or regularization constraints, aiming for a more compact representation in the feature space. The proposed approach groups samples into two sets for better alignment and classification. The reliable set of samples guides feature learning for the less reliable set in a mutual learning process called feature intertwiner. This plug-and-play module is applied for object detection, improving classification and localization accuracy. The detection accuracy drops as object resolutions decrease. Samples with high resolution are reliable, while low-resolution samples are less reliable. The feature intertwiner helps the feature learning of less reliable samples by leveraging the reliable set. It reduces scattered samples and potential classification mistakes. The intertwiner ensures compact centroids by bringing lower resolution features closer to higher resolution features. The proposed feature intertwiner improves detection accuracy by bringing lower resolution features closer to higher resolution features. It incorporates class-dependent historical representatives and optimal transport divergence for better performance. The InterNet detection system utilizes optimal transport divergence to align less reliable sets with reliable sets, improving object detection accuracy. The feature intertwiner incorporates class-dependent historical representatives for better performance. The code suite is available at https://github.com/hli2020/featureintertwiner. The development of deep networks and modern GPU hardware acceleration has greatly improved performance and efficiency in computer vision tasks. Small object detection is addressed through surrounding context and multiscale strategies to handle scale problems effectively. The proposed feature intertwiner offers a new perspective on detecting small objects by leveraging high-resolution reliable samples and designing loss functions for better feature learning. Previous works have focused on adding constraints to intra-class regularization, such as the angular softmax loss and center loss approaches. The proposed feature intertwiner focuses on improving classification accuracy by utilizing a reliable set to guide a less reliable set. It involves learning centroids for each class to penalize distances within categories. This work is different from previous approaches like the center loss method and angular softmax loss. The Faster RCNN pipeline is used for object detection in this study. The proposed InterNet framework utilizes region proposals of varying sizes to extract features for object detection. The network is divided into levels based on feature map sizes, with region proposals categorized as large or small sets. This approach aims to improve classification accuracy by utilizing reliable and less reliable sets within the object detection framework. The InterNet framework uses region proposals of varying sizes for object detection, divided into levels based on feature map sizes. The reliable and less reliable sets are utilized to improve classification accuracy. Feature map P l at level l is processed through RoI and a make-up layer to enhance information and compensate for small resolution instances. Blue blobs represent the less reliable set (small objects) and green for the reliable set (large ones). The refined high-level semantics are robust to factors like pose, lighting, and appearance. The module uses the reliable set for optimal transport divergence, aligning information between levels. The intertwiner unit optimizes the make-up unit with features from the large object set. The feature intertwiner measures data distribution between two sets, with inputs from the RoI layer and make-up layer. The critic module further extracts representations of the two sets for evaluation. The feature intertwiner module optimizes the make-up unit by aligning information between levels using the reliable set for optimal transport divergence. It compares two sets with a simple l2 loss and combines detection losses with intertwiner loss. The network structure is detailed in the appendix. Challenges include sets not occurring simultaneously and choosing input sources. The goal is to have samples from the less reliable set. The feature intertwiner module aims to align information between levels using a reliable set for optimal transport divergence. A buffer is used to store category representatives, which are mean feature representations from large instances. This helps address the challenge of calculating intertwiner loss when samples from different sets are not present simultaneously. The feature intertwiner module utilizes a buffer to store class representatives for optimal transport divergence. These representatives guide the learning process and can be used to calculate intertwiner loss between sets. Different options for designing the mapping are empirically discussed. The feature intertwiner module uses class representatives stored in a buffer for optimal transport divergence. It guides the learning process and calculates intertwiner loss between sets. Through mutual learning, small-region object features gradually encode details from large-region counterparts, ensuring similarity within categories despite visual appearance variation. This mechanism compensates for resolution imperfections in small instances by mimicking a more reliable set, resembling a teacher-student guidance in self-supervised learning. The buffer statistics from all levels guide the learning process by stabilizing the model to converge. The level-agnostic buffer receives semantic features for large instances, leading to improvements across all levels in the experiments. The input source, denoted as P (large,l), is crucial for guiding the learning process in the experiments. There are different options for acquiring the input source, such as using feature maps from different stages of ResNet. Option (a) suggests using features from the current level, while option (b) proposes using features from a different stage. Option (b) recommends using higher level feature maps in ResNet for large objects, which have lower resolution but higher semantics compared to lower level feature maps. However, some large instances may be considered small on higher levels, requiring up-sampling during the RoI operation. This can lead to insufficient information for updating the buffer on the current level. Option (c) suggests up-sampling P m to match the size at P l before RoI-pooling to optimize information recovery for large objects. This approach shows a 0.8% AP boost compared to option (b), indicating the importance of converting P m back to the feature space of P l. The appendix provides further analysis on why using P l directly may not be ideal. Option (c) is recommended for utilizing the reliable feature set of large-region objects. Introducing a better alternative to connect P l and P m|l, the intertwiner guides feature learning for the less reliable set. By optimizing the distribution transfer between P l and P m|l using optimal transport (OT), the input source for large instances can align better for learning. Incorporating OT between P l and P m|l before RoI-pooling enhances the feature map inputs effectively. The RoI-pooling operation is enhanced by employing a discretized version of the OT divergence as regularization to the loss. The Sinkhorn algorithm is used iteratively to compute W Q, resulting in a differentiable loss function referred to as Sinkhorn divergence. The generator network up-samples feature maps from higher levels to match the size of P l and outputs P m|l. The critic unit reduces spatial dimensionality while maintaining the channel dimension. The Sinkhorn algorithm is utilized to compute the Sinkhorn divergence, which is a differentiable loss function used in object detection. The OT module reduces the spatial dimensionality of input while keeping the channel dimension unchanged. The output is the ground cost Q x,y, calculated using cosine distance between manifolds. The total loss for the detector includes classification and regression losses. The Sinkhorn algorithm is used to compute the Sinkhorn divergence in object detection. The OT module reduces input dimensionality while maintaining channel dimensions. OT metric converges better than other alternatives like KL or JS divergence. It provides sensible cost functions for learning distributions on low-dimensional manifolds. This property helps in training to distinguish between positive and false samples. The OT metric maps distribution comparison in high-dimensional feature space to a lower dimension. Euclidean distance can improve AP by 0.5% but does not fully replace OT metric. InterNet is evaluated on the object detection track of the COCO benchmark using ResNet models. The framework is based on Mask-RCNN without the segmentation branch. Ablative analysis is conducted with austere settings, including training and test image scale at 512 and no multi-scale or data augmentation. InterNet is evaluated on the COCO benchmark for object detection using ResNet models. The training and test image scale is fixed at 512, with no multi-scale or data augmentation. Results show a 2-point improvement in mAP compared to the baseline, with a noticeable enhancement in detecting small objects. The method also improves detection of large objects by 0.8%, utilizing an assignment strategy based on region proposal allocations. The proposals in TAB6 are categorized as small or large based on their area, with a focus on placing more proposals on higher levels to balance workload. However, this may lead to misalignment with RPN training, affecting the distribution of anchor templates. The intertwiner loss in TAB3 shows different factors affecting the total loss, with the l2 loss performing slightly better than KL divergence and l1 loss. The intertwiner module affects learning by measuring divergence between small proposals and large references, optimizing the make-up layer to recover details. Combining features after the critic improves AP by 1, showing the generalization ability of the method in different loss options. The method combines features after the critic to improve AP by 1 point, demonstrating generalization ability in different loss options. A window size of K can be used to keep recent features, with larger sizes showing performance improvement. Using 'all history' data with a decayed scheme weights recent features more, providing a comprehensive view of the data. In experiments, it was found that averaging features equally leads to better performance in network evolution compared to a decayed scheme. Unified buffer approach was favored over level-based buffer. Detaching buffer transaction from gradient update showed improvement in performance. The update in the model led to an improvement in performance. Attempts to diversify critic outputs with stronger supervision did not work as expected. Our InterNet outperforms other detectors without multi-scale technique. A comparison with previous state-of-the-arts is provided in TAB7. The per-class improvement after adopting feature intertwiner is showcased in FIG3, highlighting improvements in the 'couch' class. The feature intertwiner in the model increases conv. layers at specific units, impacting accuracy. Training on 8 GPUs with batch size of 8 takes around 3.4 days, slower than Mask-RCNN. The memory cost is higher due to the 'all-history' strategy. The proposed feature intertwiner module in the model increases convolutional layers at specific units, impacting accuracy. It takes 3.4 days to train on 8 GPUs with a batch size of 8, which is slower than Mask-RCNN. The memory cost per card is 9.6 GB compared to the baseline of 8.3 GB. Inference runs at 325ms per image on a Titan Pascal X, showing a 5% increase in time compared to the baseline. The intertwiner helps generate a more compact centroid representation in high-dimensional space by leveraging features from a more reliable set to guide the feature learning of another less reliable set. The feature intertwiner module is applied to improve object detection by using a historical buffer to address sample missing issues. Optimal transport theory enforces similarity between sets, with reliable set features guiding feature learning for small objects. Different options for the large set are considered, with optimal transport chosen. The feature intertwiner significantly enhances detection performance, especially for small instances, serving as a general alternative for feature learning. The feature intertwiner module utilizes a historical buffer to improve object detection by addressing sample missing issues. Optimal transport theory enforces similarity between sets, guiding feature learning for small objects. Self-supervised learning is applied to supervise feature learning in high-dimensional space. Knowledge distillation framework is used to learn compact and accurate object detectors. Center loss is formulated to penalize samples with larger distances from the class centroid, aiming to increase inter-class resemblance. In our work, the feature intertwiner module aggregates statistics from a meta-subset to improve face recognition by optimizing a more compact class centroid. Inspired by object detection mechanisms, our framework involves two sets that mutually help and interact with each other during training. The goal is to enlarge inter-class resemblance and narrow down inner-class divergence for better recognition accuracy. In this work, optimal transport (OT) is utilized for transfer learning and estimating generative models. Previous studies have shown the effectiveness of OT in bridging the gap between target and source predictions, as well as in training large-scale generative models. The OT metric is competitive in measuring divergence between distributions on low-dimensional manifolds. The ResNet model with feature pyramid dressings is adopted in this study. The ResNet model BID3 with feature pyramid dressings generates five levels of feature maps for RPN and detection branches. Region proposals are divided into different levels, with most focusing on small objects allocated at shallow level l = 2. The ResNet model BID3 with feature pyramid dressings generates five levels of feature maps for RPN and detection branches, with a focus on small objects at shallow level l = 2. Proposals below a certain threshold are up-sampled due to the inherent design flaw in RoI operation, leading to the loss of information on small regions. This highlights the need for a meta-learner to guide feature learning for small objects. The RoIs generated by the ResNet model are used in the RoI-pooling layer to create output feature maps for object detection. Leaving out proposals from the last level can improve performance, but including them can also enhance detection of large objects. Individual samples are processed to low-dimensional vectors for analysis. The OT metric calculates the divergence between two joint probability distributions supported on spaces U. It involves a linear program denoted as BID2. The continuous form of OT divergence is defined with couplings and ground cost. The biased Sinkhorn divergence is a variation used in analysis. Options (a) to (d) were discussed in Table 1, with (a) being inferior due to inappropriate feature maps. Option (b) serves as the baseline, while options (c) and (d) show the preference for up-sampling feature maps and imposing a supervision signal for better alignment. OT outperforms other variants, and a biased version of Sinkhorn divergence did not bring significant gains and could burden system efficiency during training. The OT module may have issues with the critic and generator update, potentially causing gradient flow to iterate twice. OT divergence is applied to image classification, aligning feature maps in CIFAR-10 and reducing test error by 1.3%. This suggests OT's potential in vision tasks. Unlike OT in generative models, channel dimensions are treated as different samples, and F and H optimization is unified. Experiments on PASCAL VOC 2007 dataset show the effectiveness of the feature intertwiner. Our method outperforms others on the PASCAL dataset using ResNet-101 and VGG-16 backbones. Stochastic gradient descent is used with an initial learning rate of 0.01, momentum of 0.9, and weight decay of 0.0001. The models train for 13 epochs with a 90% learning rate drop at epoch 6 and 10. Warm-up strategy BID8 is not adopted. Gradient clip with a maximum norm of 5 is used to prevent training loss explosion. Batch size is 8 on 8 GPUs with an object detector based on Mask-RCNN. The object detector is based on Mask-RCNN with RoIAlign for better performance. The model is initialized with a ResNet pretrained on ImageNet. A new feature intertwiner module is trained from scratch. The backbone structure includes five ResNet blocks with up-sampling layers. The region proposal network has one convolutional layer for classification and regression. Non-maximum suppression is used during RPN generation and detection test phase. The object detector is based on Mask-RCNN with RoIAlign for better performance. The model includes a feature intertwiner module trained from scratch and a backbone structure with five ResNet blocks. The region proposal network has one convolutional layer for classification and regression, with non-maximum suppression used during RPN generation and detection test phase. The network architecture includes unique anchor sizes for each level and specific layers in the make-up and critic modules. The critic unit network structure includes convolutional and batch normalization layers, as well as activation functions. The input for the large set is the RoI output of the large-set feature map, while for the small set, it is the output of the make-up layer. The batch size in one mini-batch is denoted as B, and C l represents the number of channels in ResNet blocks."
}