{
    "title": "SJeLIgBKPS",
    "content": "In this paper, the implicit regularization of the gradient descent algorithm in homogeneous neural networks is studied, focusing on fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. The study involves gradient descent optimizing logistic or cross-entropy loss of any homogeneous model, showing that a smoothed version of the normalized margin increases over time when the training loss decreases below a certain threshold. Additionally, a constrained optimization problem related to margin maximization is formulated, proving convergence of the normalized margin and its smoothed version to the objective value at a KKT point. These results extend previous findings for logistic regression with linear networks and offer more quantitative convergence results. In this study, the implicit regularization of gradient descent in homogeneous neural networks is explored, focusing on fully-connected and convolutional networks with ReLU or LeakyReLU activations. The research provides quantitative convergence results for one-layer or multi-layer linear networks, with experiments on MNIST and CIFAR-10 datasets. The relationship between margin and robustness is discussed, along with the bias of gradient descent towards solutions with good generalization performance. The importance of the direction of the weight vector in linear logistic regression on linearly separable data is highlighted. The direction of the weight vector in linear logistic regression is crucial for prediction, as shown by previous studies. Gradient descent converges to the direction maximizing the L2-margin while the norm of the weight vector diverges. This convergent direction aligns with any regularization path, indicating a similar implicit bias in deep neural networks trained with gradient descent. Theoretical analysis focuses on the implicit bias of gradient descent in deep neural networks, particularly homogeneous networks. These networks exhibit a specific property where the output is proportional to the input raised to a power. Regularization paths in homogeneous networks converge to the max-margin direction, indicating a similarity in implicit bias with linear logistic regression. Theoretical analysis shows that gradient descent in homogeneous neural networks converges to the max-margin direction, similar to linear logistic regression. Studies have proven this for linear fully-connected networks and smooth homogeneous models. In this paper, a minimal set of assumptions for proving theoretical results for homogeneous neural networks on classification tasks is identified. The assumptions include an exponential-type loss function and separability, where the neural network can achieve 100% training accuracy. Recent theoretical work on over-parameterized neural networks shows that gradient descent can fit the training data if the width is large enough. The margin \u03b3(\u03b8) scales linearly with \u03b8 L 2 for homogeneous models, and studying the normalized margin \u03b3(\u03b8) := \u03b3(\u03b8)/ \u03b8 L 2 is important for comparing margins among different directions of \u03b8. In this paper, the focus is on the training dynamics of neural networks after a certain time t 0 when the training loss is below a threshold. The study examines how the normalized margin changes during training, showing that it generally increases over time. Additionally, the research addresses the size of the normalized margin at convergence by formulating an optimization problem to maximize the margin. The study focuses on the training dynamics of neural networks, showing that gradient descent implicitly maximizes the margin in deep homogeneous networks. This generalizes previous works from linear to homogeneous classifiers. Tight asymptotic convergence rates of loss and weights are derived, with the normalized margin increasing slowly over time. The study shows that after fitting the model, the growth rate is slow. Training CNNs on MNIST with and without bias using SGD results in exponential decrease in training loss and rapid increase in normalized margin. The weight norm grows logarithmically over time. Theoretical results suggest that training longer can increase the normalized margin. Training CNNs on MNIST and CIFAR-10 with SGD leads to an increase in the normalized margin. By adjusting the learning rate based on training loss, the margin grows significantly faster. The normalized margin is closely linked to L2-robustness in feedforward neural networks with ReLU activation. A Lipschitz constant is a lower bound for the L2-robustness on fully-connected networks with ReLU activation. Training longer may improve model robustness. In linear logistic regression, full-batch gradient descent converges towards the max L2-margin solution of the hard-margin SVM. In linear logistic regression, various optimization methods and loss functions have been considered, with results generalized to deep linear networks. Non-smooth analysis involves locally Lipschitz functions and Clarke's subdifferential. Clarke's subdifferential at x is the convex set \u2202 \u2022 f (x) defined as the limit of gradients of f at x. An arc is an absolutely continuous function on an interval. In binary classification, a neural network \u03a6 outputs a real number for an input x, with the sign indicating the classification result. The dataset D consists of input-output pairs. The training loss of \u03a6 on D is defined as the sum of losses over the dataset. The training loss of a neural network \u03a6 on dataset D is defined as L(\u03b8) := \u03a3(y_n \u03a6(\u03b8; x_n)). Gradient descent and gradient flow are methods used for training the network. Gradient descent involves updating \u03b8 using \u03b8(t + 1) = \u03b8(t) \u2212 \u03b7(t)\u2207L(\u03b8(t)), while gradient flow involves continuous changes in \u03b8 over time following d\u03b8(t)/dt = \u2212\u2207L(\u03b8(t)). The text discusses gradient flow and gradient descent for training neural networks. Gradient flow involves continuous changes in \u03b8 over time following d\u03b8(t)/dt = \u2212\u2207L(\u03b8(t)). The results for gradient flow on homogeneous models with exponential loss are presented, with more general results deferred to the appendix. Technical assumptions about the regularity and separability of the network output are also discussed. The network admits a chain rule based on definable pieces in an o-minimal structure like ReLU, sigmoid, LeakyReLU. Assumptions (A2) and (A3), (A4) are introduced, with a focus on the exponential loss in (A3). Gradient Descent assumptions include (A2), (A3), (A4), and (S1) and (S5) for learning rate conditions. The paragraph discusses the smoothness of \u03a6 in neural networks and the definition of margins for data points. It also mentions the normalized margin and -additive approximation for it. The text discusses the concept of margin in neural networks, specifically focusing on -additive and multiplicative approximations for the normalized margin. It also introduces the idea of a smoothed version of the normalized margin that is non-decreasing during training, with a specific theorem for gradient flow. The function\u03b3(\u03b8) is defined with a modification involving the LogSumExp function. The LogSumExp function is used to approximate q min in the analysis of margin in linear models. Theorem 4.2 states that there exists a multiplicative approximation function \u03b3(\u03b8) for the normalized margin in gradient descent, ensuring non-decreasing behavior during training. The LogSumExp function approximates q min in linear models. Theorem 4.2 introduces a multiplicative approximation function \u03b3(\u03b8) for the normalized margin in gradient descent, ensuring non-decreasing behavior during training. Convergence rates are discussed, showing tight bounds for training loss and weight norm under specific assumptions. Theorem 4.4 discusses the convergence of gradient flow towards KKT points in constrained optimization problems, extending the result to finite time cases. Theorem A.9 and Theorem E.4 in the appendices provide further details on this topic. The concept of KKT points and approximate KKT points is briefly reviewed in Appendix C.1. Theorem 4.4 states that for gradient flow or descent under certain assumptions, any limit point \u03b8 is towards a KKT point of a constrained optimization problem. The proof involves maximizing the normalized margin over all possible directions. The minimum objective of (P) is ensured to satisfy the Mangasarian-Fromovitz Constraint Qualification (MFCQ), making KKT conditions first-order necessary for global optimality. While linear models adhere to KKT conditions for global optimality, deep homogenuous networks may have highly non-convex q n (\u03b8). Without additional assumptions on q n (\u03b8), gradient descent may not lead to a globally optimal normalized margin. Proving convergence to KKT points is crucial, and further research is needed to establish stronger convergence results with natural assumptions. Additionally, the optimality of the normalized margin using SVM with Neural Tangent Kernel (NTK) can be characterized. The normalized margin using SVM with Neural Tangent Kernel (NTK) is characterized at limit points. The optimal solution for hard-margin SVM with kernel is discussed, along with the extension to other settings. The results can be generalized to gradient descent and other binary classification loss functions. The results on exponential loss can be generalized to a broader class of binary classification loss, including logistic loss. The smoothed normalized margin for cross-entropy loss is defined similarly to logistic loss. See Appendix A for details. The text discusses multi-homogeneous models in neural networks, which have a stronger property than homogeneity. These models have a normalized margin defined differently from homogeneous models. The smoothed approximation of \u03b3 for binary classification loss can also be defined similarly. The results hold for gradient flow, with a slightly different definition of (P). In this section, a proof sketch is presented for gradient flow on a homogeneous model with exponential loss. The main theorems on gradient flow and gradient descent are deferred to appendices. Notations are introduced for a L-homogeneous neural network, with \u03b8 being a function of time. Key lemma 5.1 is crucial in the proof. Lemma 5.1 decomposes the growth of the smoothed normalized margin into radial and tangential velocity components of \u03b8. It is crucial in proving the theorems on gradient flow and gradient descent for a homogeneous model with exponential loss. The proof sketch will be provided later in this section. Lemma 5.1 decomposes the growth of the smoothed normalized margin into radial and tangential velocity components of \u03b8. By analyzing the landscape of training loss, it shows that there are many \"approximate\" KKT points. Every convergent sub-sequence of {\u03b8(t) : t \u2265 0} can be modified to converge to the same limit, which is a KKT point. The proof sketch for Lemma 5.1 derives the formula of \u03b3 step by step, using the chain rule and Euler's theorem extensively. The proof sketch of Lemma 5.1 focuses on decomposing the growth of the smoothed normalized margin into radial and tangential velocity components of \u03b8. It utilizes the chain rule and Euler's theorem extensively to derive the formula of \u03b3 step by step. The analysis in this paper examines the dynamics of gradient flow/descent of homogeneous neural networks under minimal assumptions. The technical contribution of the work is proving that the normalized margin increases and converges to a KKT point for gradient flow/descent on neural networks. Further questions include generalizing results to nonsmooth networks and exploring stronger results with additional structural assumptions. The normalized margin at convergence is a key property of modern neural networks, with potential research directions to explore its optimality. The study aims to extend results to neural networks with bias terms, showing an increase in normalized margin during training. The results apply to a broad class of binary classification loss functions, including the popular logistic loss. The focus is on gradient flow and assumptions are made for exponential loss. The assumptions (A1) and (A2) are made for exponential loss. To replace (A3) and (A4), weaker assumptions (B3) and (B4) are introduced. These assumptions ensure the separability of data and are related to the smoothness and monotonicity of the function used for binary classification. The assumptions (B3) characterize the properties of (q) for binary classification. The inverse function g is defined in (B3.4), ensuring its existence and smoothness. This technical assumption allows for asymptotic comparison of loss or gradient length at different data points. It simplifies the analysis and is satisfied by many loss functions. Exponential loss (q) = e \u2212q and logistic loss (q) = log(1 + e \u2212q) satisfy the properties in (B3). The proof for exponential loss is trivial, while for logistic loss, it is shown through simple calculations. The smoothed normalized margin \u03b3(\u03b8) is defined for loss functions satisfying (B3) as \u03b3 where \u22121 (\u00b7) is the inverse function of (\u00b7) and \u03c1 := \u03b8 2. The smoothed normalized margin \u03b3(\u03b8) can be approximated using the LogSumExp function as a (log N)-additive approximation for max. Lemma A.5 asserts that \u03b3 is a good approximation of \u03b3 under certain assumptions. Additionally, there exists a constant B0 such that \u03b3(\u03b8m) \u2264 B0 for all m. The normalized margin \u03b3(\u03b8) can be approximated using the LogSumExp function as a (log N)-additive approximation for max. The main theorems state the monotonicity of the normalized margin at convergence, with proofs provided in the appendices. The constrained optimization problem (P) is defined, and the directional convergence of \u03b8(t) to a KKT point is shown. The text discusses gradient flow under specific assumptions and presents theorems related to KKT points and loss convergence. Tight rates for loss convergence and weight growth are provided, along with a corollary for exponential and logistic loss functions. The proof for these theorems is included in the appendices. The text discusses generalizing key lemma to prove propositions for loss convergence. It reviews properties of homogeneous functions and generalizes Clarke's subdifferential for locally Lipschitz functions. The text discusses the generalization of key lemma to prove propositions for loss convergence and reviews properties of homogeneous functions. It also generalizes Clarke's subdifferential for locally Lipschitz functions, providing an exact formula for weight growth during training. The text discusses using Corollary B.3 and chain rules to prove loss convergence. It defines \u03bd(t) and L, and shows that L \u2192 0 implies \u03c1 \u2192 +\u221e. In this section, the text analyzes the convergent direction of \u03b8 and proves Theorem A.8 and A.9, assuming certain conditions. It defines \u03c1 := \u03b8^2 and \u03b8 := \u03b8/|\u03b8| in S^(d-1), and discusses the functions of \u03b8. The proof involves showing that L \u2192 0 implies \u03c1 \u2192 +\u221e. The text defines \u03c1 := \u03b8^2 and \u03b8 := \u03b8/|\u03b8| in S^(d-1) to review the KKT conditions for non-smooth optimization problems. A feasible point x of the optimization problem satisfies KKT conditions with \u03bb 1 , . . . , \u03bb N \u2265 0. The KKT conditions are necessary for global optimality under regularity assumptions like the non-smooth version of MFCQ. The Mangasarian-Fromovitz Constraint Qualification (MFCQ) is defined for a feasible point x of the optimization problem. An approximate version of KKT point is introduced, known as ( , \u03b4)-KKT point, which can converge to a KKT point. The definition is slightly stronger than the modified -KKT point defined in a previous paper. Theorem C.4 states that if a sequence of feasible points converges to a point where MFCQ holds, then that point is a KKT point. KKT points and approximate KKT points for the optimization problem are defined accordingly. The text discusses the concept of ( , \u03b4)-KKT points of an optimization problem, showing that KKT conditions are necessary for global optimality. Lemma C.7 proves that the problem satisfies MFCQ at every feasible point. Lemma C.8 introduces conditions for approximate KKT points, emphasizing the importance of certain limits approaching zero. The text discusses the construction of ( , \u03b4)-KKT points for an optimization problem, proving the necessity of KKT conditions for global optimality. Lemmas C.7 and C.8 establish the satisfaction of MFCQ and conditions for approximate KKT points, emphasizing limits approaching zero. The chain rule is used to derive h(t) and bound the integral of \u03b2(t) in Lemma C.9. Lemma C.9 bounds the integral of \u03b2(t) using the chain rule. Corollary C.10 provides an upper bound for the minimum \u03b2 2 \u2212 1 within a time interval. The proof involves showing a contradiction using Lemmas B.1 and C.9. Additionally, an auxiliary lemma, Lemma C.11, gives an upper bound for the change of \u03b8. Lemma C.11 provides an upper bound for the change of \u03b8 by bounding the derivative with respect to time. It involves replacing q n with g(log 1 L) and constructing a series of approximate KKT points converging to \u03b8/q min (\u03b8) 1/L. Additionally, Lemma C.12 ensures the existence of a sequence {t m} converging to a limit point \u03b8 with \u03b2(t m) approaching 1. The proof involves constructing a sequence {t m} converging to a limit point \u03b8, ensuring the existence of approximate KKT points. The theorem is then proven by applying relevant lemmas and conditions. Proof of Theorem A.9 involves assuming certain conditions and finding a time interval where an optimal point exists. The proof utilizes Lemmas and Corollaries to show that a specific point satisfies KKT conditions. The construction of a sequence converging to a limit point \u03b8 is crucial in proving the theorem. The KKT conditions for optimization problem (Q) are derived from those of optimization problem (P). Every limit point \u03b8 is along the max-margin direction of (Q). For smooth models, (Q) corresponds to SVM with a specific kernel. For non-smooth models, an arbitrary function is constructed to define the optimization problem for SVM. The proof for Theorem A.10 provides bounds for loss convergence and weight growth under certain assumptions. Lemma D.1 establishes relationships between functions f(\u00b7) and g(\u00b7). The proof for Theorem A.10 utilizes Lemma D.4 to obtain tight asymptotic bounds for function G(\u00b7) and its inverse. The key idea is to use Lemma B.6 to bound L(t) from above, which in turn helps in lower bounding G \u22121. The functions f and g grow at most polynomially, as shown in Corollary D.3. Lemma D.4 provides tight asymptotic bounds for function G(\u00b7) and its inverse G \u22121 (\u00b7). The proof shows that G(x) and G \u22121 (y) have finite values for finite x and y. By deriving upper bounds for the gradient at each time t in terms of L, we can bound L(t) from below. This allows us to establish both lower and upper bounds for L(t). The proof for Theorem A.10 involves deriving upper and lower bounds for L(t) and \u03c1(t) by plugging in tight bounds and using Lemmas B.6 and D.4. The relationship between \u03c1, L, and g(log 1/L) is established, along with bounding \u03c1 in terms of t using Corollary D.2. In this section, the proof is discretized to analyze gradient descent on smooth homogeneous models with exponential loss. The update rule for gradient descent is defined, and challenges arise from the less smooth nature of the normalized margin as \u03c1 approaches infinity. To ensure monotonicity, a smoothed version of the normalized margin is needed. To ensure monotonicity in gradient descent on smooth homogeneous models with exponential loss, a new smoothed normalized margin \u03b3 is defined to cancel out discretization error. This margin increases slightly slower than \u03b3 during training. Assumptions (A2), (A3), (A4), (S1), and (S5) are made, with (A4) stating the existence of a time t0 where the loss is less than 1. The function H(L) is a function of the current training loss. The function H(L) is defined as a function of the current training loss, with specific formulas for C \u03b7, \u03ba(x), and \u00b5(x). The learning rate \u03b7(t) is set to be the inverse of the smoothness multiplied by a factor \u00b5(x). The smoothed normalized margins are defined, with \u03b3(\u03b8) and \u03c6(x) also introduced in the analysis. The function H(L) is defined based on the current training loss, with specific formulas for C \u03b7, \u03ba(x), and \u00b5(x). \u03b3(\u03b8) and \u03c6(x) are introduced in the analysis, where \u03b3(\u03b8) is well-defined for L(\u03b8) \u2264 L(t 0 ) and has certain properties.\u03c6(x) is constructed as the first-order derivative of \u03c6(x), and the relationship among \u03b3, \u03b3, and \u03b3 is established. The function H(L) is defined based on the current training loss, with specific formulas for C \u03b7, \u03ba(x), and \u00b5(x). \u03b3(\u03b8) and \u03c6(x) are introduced in the analysis, where \u03b3(\u03b8) is well-defined for L(\u03b8) \u2264 L(t 0 ) and has certain properties.\u03c6(x) is constructed as the first-order derivative of \u03c6(x), and the relationship among \u03b3, \u03b3, and \u03b3 is established. Theorems E.2 and E.3 discuss the monotonicity of the normalized margin and convergence to KKT points under specific assumptions for gradient descent and gradient flow. Theorems E.4 and E.5 provide tight rates for loss convergence and weight growth under specific assumptions for gradient descent. Tight rates for training loss and weight norm are also discussed, along with the definition of \u03bd(t) and a lower bound for it using Lemma B.5. Lemma B.5 is used for exponential loss. The first two propositions in Theorem E.2 are proven by Lemma E.7, which provides bounds for \u03b3, weight growth, and training loss decrement. The monotonicity of \u03b3 is shown in (P4). Lemma E.7 interpolates between \u03b8(t) and \u03b8(t + 1) for all t, and proves the lemma by induction. Lemma E.8 is proven to hold for t = T and \u03b1 \u2208 [0, 1], ensuring that (P1), (P2), (P3), (P4) are satisfied. This is achieved by showing that A = 1 and utilizing Corollary E.6 to establish \u03bd(t) > 0. The continuity of \u03b3(T + \u03b1) with respect to \u03b1 is used to derive the contradiction that leads to A = 1. By applying (P2) on (t, \u03b1) \u2208 {t 0 , . . . , T \u2212 1} \u00d7 1, we can derive \u03c1(t) \u2265 \u03c1(t 0 ). Utilizing Corollary E.6 and the continuity of \u03b3, we establish \u03b3(t + A) \u2265 \u03b3(t 0 ). Through a series of calculations and proofs, we show the validity of propositions (P1'), (P3), and (P4) in the context of gradient flow analysis. The proof involves showing the convergence of L(t) to 0 and \u03c1(t) to +\u221e as t approaches infinity. By utilizing propositions (P1), (P3), and (P4), we establish lower bounds for \u2207L(t) 2 and demonstrate the decreasing speed of loss. This leads to the conclusion that L(t) approaches 0 and \u03c1(t) approaches +\u221e as t increases. The proof involves demonstrating the convergence of L(t) to 0 and \u03c1(t) to +\u221e as t approaches infinity. By making certain adjustments in the proof process, such as replacing \u03b3(t 0 ) with \u03b3(t 0 ) and allowing for approximate equality in log \u03c1(t 1 ) and log \u03c1(t 2 ), the convergence to KKT points can be established. The proof involves showing the convergence of L(t) to 0 and \u03c1(t) to +\u221e as t approaches infinity. By allowing for approximate equality in log \u03c1(t), the minimum time t can be found such that log \u03c1(t) \u2265 R, and log \u03c1(t) \u2212 R \u2192 0 as R \u2192 +\u221e. This is essential for proving Theorem E.3. The proof involves showing the convergence of L(t) to 0 and \u03c1(t) to +\u221e as t approaches infinity. For every limit point \u03b8 of \u03b8(t), there exists a sequence of {t m : m \u2208 N} such that t m \u2191 +\u221e, \u03b8(t m ) \u2192 \u03b8, and \u03b2(t m ) \u2192 1. The choices of s m, s m, t m are changed in the proof for Lemma C.12. By a similar analysis as Lemma D.4, we can bound the inverse function E \u22121 (y) by \u0398 1 y(log y) 2\u22122/L and use it to prove Theorem E.5. In this section, the analysis is extended to multi-class classification with cross-entropy loss. Inspired by a theorem from Zhang et al. (2019), the gradient lower bound in terms of loss L is considered for neural networks with multiple outputs. Notations are redefined for C classes, where the output of a neural network \u03a6 is a vector \u03a6(\u03b8; x) \u2208 R^C. The dataset D = {x_n, y_n}^N_n=1 = {(x_n, y_n) : n \u2208 [N]} is defined, with x_n \u2208 R^dx as input and y_n \u2208 [C] as the label. The loss function of \u03a6 on dataset D is then determined. The loss function of \u03a6 on the dataset D is defined as \u2212 log e \u2212\u03a6y n (\u03b8;xn) C j=1 e \u2212\u03a6j (\u03b8;xn). The margin for a single data point (x n , y n) is defined as q n (\u03b8) := \u03a6 yn (\u03b8; x n) \u2212 max j =yn {\u03a6 j (\u03b8; x n)}, and for the entire dataset as q min (\u03b8) = min n\u2208[N] q n (\u03b8). The normalized margin is \u03b3(\u03b8) := q min (\u03b8)/\u03c1 L. The cross-entropy loss can be rewritten in different ways. The separability of training data is ensured by (M4). The smoothed normalized margin \u03b3(\u03b8) for cross-entropy loss is defined using a new function \u03bd(t). The proof involves showing that Lemma B.5 still holds for this new definition. Extending the results to multi-homogeneous models, the smoothed normalized margin is redefined. The generalized version of Lemma 5.1 is proven in this context. The proof for the smoothed normalized margin \u03b3(\u03b8) of \u03b8 involves redefining the margin for cross-entropy loss and showing that Lemma G.2 holds with a new definition. The proof also involves proving L in a similar way as in Lemma F.2. In this section, we provide background on the chain rule for non-differentiable functions, focusing on Clarke's subdifferential for locally Lipschitz functions. The chain rule holds as an inclusion rather than an equation, which is crucial for analyzing gradient flow. The chain rule is essential for analyzing gradient flow. For locally Lipschitz functions, the function value may not always decrease along the gradient flow. A generalized version of the chain rule holds for C1-smooth functions, and locally Lipschitz functions that are subdifferentially regular or Whitney C1-stratifiable also admit a chain rule. The class of functions that admits chain rules is closed under composition. Theorem H.4 states that locally Lipschitz functions admitting chain rules also have chain rules for their compositions. This is proven by showing that the composition of Lipschitz and absolutely continuous functions is absolutely continuous. In this section, an example is given to show that gradient flow may not converge directionally, even for smooth homogeneous models. The \"Mexican Hat\" function is a famous counterexample where gradient flow may not converge to any point, even when optimizing a smooth function. This function is not homogeneous, and directional convergence was not considered in previous studies. The function discussed is not homogeneous, and directional convergence was not considered in previous studies. To make it homogeneous, an extra variable z is introduced, and the parameter is normalized before evaluation. Theorem I.1 states that under certain conditions, the gradient flow does not converge to any point. The proof involves showing that a certain angle remains constant and that the radius approaches 1 as time goes to infinity, leading to non-convergence. The function discussed is not homogeneous, and directional convergence was not considered in previous studies. An extra variable z is introduced to make it homogeneous, and the parameter is normalized before evaluation. The gradient flow does not converge to any point under certain conditions, as shown in the proof involving constant angles and the radius approaching 1. Experimental validation was done on the MNIST dataset using two Tensorflow models. The first Tensorflow model is a 4-layer CNN with bias terms in each layer, trained with SGD and batch size 100. The model architecture includes conv-32, max-pool, conv-64, max-pool, fc-1024, fc-10 layers. Bias terms are removed except in the first layer to maintain homogeneity in parameters. All layer weights are initialized using He normal initializer. Images are normalized to [0, 1] 32x32. In the experiments, the CNN model is evaluated with batch size 100 without momentum. The normalized margin is computed for both CNN with and without bias terms, with the bias term in the first layer considered as part of the weight for convenience. The L2-norm of layer weights is used to calculate the normalized margin, with bias terms ignored in layers other than the first. Plots for the normalized margin using the original definition are included for completeness. After training CNNs with constant learning rate 0.01 for about 100 epochs, normalized margins increase slowly. Different learning rates were tested, showing similar results. A heuristic method called loss-based learning rate scheduling can speed up training by adjusting learning rates based on training loss. See Figure 1 for results. SGD with loss-based learning rate scheduling decreases training loss exponentially faster than SGD with constant learning rate. Rapid growth of normalized margin observed for CNNs. Training loss can be as small as 10^-800, leading to numerical issues. Re-parameterization and numerical tricks applied to address these issues. Experiments conducted on CIFAR-10 using modified VGGNet-16 model. The experiment results show that the normalized margin increases over time, indicating potential for better generalization. Test accuracy remains stable after training with loss-based learning rate scheduling for 10000 epochs. Further investigation is needed to study the implications of these findings. In experiments on VGGNet without bias on CIFAR-10, training with a loss-based learning rate scheduler for a longer time improves L2-robustness by enlarging the normalized margin. This enhancement in robustness is observed in both the training and test sets. The gap between generalization bound and generalization error is left for future study. The relationship between normalized margin and robustness is discussed theoretically. Normalized margin is a lower bound of L2-robustness for fully-connected networks. Improving the normalized margin on the training set can enhance robustness. Theoretical analysis suggests that improving the normalized margin during training can enhance model robustness. Experimental results confirm this, showing monotone increase in normalized margin with epochs. Model robustness was measured for different epochs, with the final model after 10000 epochs exhibiting high robustness. The experimental results show noticeable robust accuracy improvements from model-1 to model-4, with marginal or nonexistent improvement from model-5 upon model-4. The evaluation method used was the standard L2-robustness evaluation. The robust accuracy for the training set is plotted in Figure 8, showing the relative order of robust accuracy for different models. The experimental results demonstrate increasing robust accuracy from model-1 to model-4, with marginal improvement from model-5. Training longer improves L2-robust accuracy on both training and test sets. Different hyperparameter settings were explored for evaluation. In this section, additional details of the experiments are provided, focusing on loss-based learning rate scheduling. The learning rate is parameterized based on the average training loss and relative factors. The shapes and positions of the curves in Figure 8 remain stable across different hyperparameter settings. Loss-based learning rate scheduling involves adjusting the learning rate based on the training loss at each epoch. The learning rate is initialized at a certain value and updated according to whether the current training loss is lower or higher than the previous epoch. Specific hyperparameters like the initial learning rate and update ratios are set, but other choices can also be made without affecting the overall trend. The current Tensorflow implementation faces numerical issues due to extremely small loss values. To address this, a stable algorithm is proposed involving forward and backward passes to compute relative training loss in a numerically stable way. The proposed algorithm addresses numerical stability issues in the Tensorflow implementation by using a stable approach for forward and backward passes to compute relative training loss. The proposed algorithm in Tensorflow addresses numerical stability by using a stable approach for forward and backward passes to compute relative training loss. To perform the backward pass, a computation graph is built for the training loss using automatic differentiation. The learning rate is parameterized as \u03b7 = \u03b7 \u00b7 e^F, where F is chosen as logL(t-1) in experiments. Maintaining logL(t) during training involves adding F(t) and log R(t) together after evaluating the relative training loss R(t) on the training set. This choice ensures numerical stability for gradient descent on R B (\u03b8). The choice of F in the algorithm ensures numerical stability for the backward pass by adding F(t) and log R(t) together, with \u03b1(t) always between 10^-9 and 10^0 for stability."
}