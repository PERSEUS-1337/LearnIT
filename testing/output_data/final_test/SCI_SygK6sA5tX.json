{
    "title": "SygK6sA5tX",
    "content": "Deep learning has made significant contributions to structured signal classification, particularly in image classification using ConvNets. These networks have inspired the development of deep filter banks known as scattering transforms, which extract invariant features. Recent advances in geometric deep learning aim to generalize ConvNets to graph data by applying graph signal processing. A proposed geometric scattering transform uses graph wavelets defined by random walks on the graph to extract features efficiently. Deep neural networks outperform traditional models in various machine learning applications. Deep networks have advantages in computer vision with 2D convolutions enabling cascades of convolutional filters. Convolutional neural networks (ConvNets) are the current state of the art in image classification and are widely used for processing structured-signal data like audio and video. Pretrained ConvNet layers can be used as feature extractors by freezing early layers and retraining later ones for specific tasks. This transfer learning approach shows that deep filter banks can extract semantic information from data, similar to how human visual and auditory systems operate. The scattering transform, a deep filter bank, is designed based on predetermined patterns to extract informative representations with invariance to operations like rotations and translations. It provides Lipschitz stability to small diffeomorphisms and has shown effectiveness in audio and image processing applications. The scattering transform is a deep filter bank designed for applications with low data availability, such as quantum chemistry. It has advantages over learned features and is now being generalized from Euclidean domains to graphs using graph signal processing notions. The construction involves a cascade of bandpass filters and complex moduli, with a focus on graph wavelet filters defined by lazy random walks. These filters are related to diffusion geometry and geometric harmonic analysis. The constructed cascade geometric scattering, following geometric deep learning terminology, generalizes the scattering transform to graphs. Previous works have focused on theoretical properties and invariance to graph isomorphism. The defined geometric scattering transform also exhibits this invariance property and is expected to have similar stability properties. This paper emphasizes the practical applicability of geometric scattering transforms for analyzing graph-structured data. The text discusses geometric scattering transforms for graph-structured data analysis, specifically in graph classification tasks. It focuses on learning a model that associates labels to graphs in supervised classification problems. This is relevant in fields like biochemistry and social networks. The traditional kernel-based methods have been commonly used for this purpose. The Weisfeiler-Lehman graph kernel is a successful approach for graph similarity. Recent deep learning algorithms learn graph filters from training data for tasks, without relying on labels. A new method, the geometric scattering classifier (GSC), uses graph-dependent scattering transforms to map graphs to scattering features extracted from characteristic signals. The geometric scattering classifier (GSC) uses graph-dependent scattering transforms to extract scattering features from characteristic signals. These features are then used for classification, achieving state-of-the-art results on two datasets and competitive performance on others. The approach extends to multiple signals by concatenating their scattering features and is evaluated against various graph kernel and deep learning methods on 13 datasets commonly studied in related literature. The proposed Geometric Scattering Classifier (GSC) outperforms other methods in social network data and biochemistry datasets, and competes with state-of-the-art results in graph kernels and graph recurrent neural networks. The universality of graph features extracted by geometric scattering is highlighted, providing an effective representation of analyzed graphs. Additionally, unsupervised qualities are demonstrated by inferring emergent patterns of enzyme evolution using geometric scattering features extracted from enzyme data. The text discusses emergent patterns of enzyme commission exchange preferences in enzyme evolution, validated with established knowledge. It introduces graph wavelets as a way to mimic classical wavelet constructions in graph signal processing, focusing on the properties of the graph Laplacian. The graph Laplacian is a symmetric, real valued positive semi-definite matrix with nonnegative eigenvalues. If the graph is connected, then the second eigenvalue is greater than zero. The eigenvectors of the Laplacian can be viewed as Fourier modes. The eigenvectors of the graph Laplacian can be seen as Fourier modes, with a frequency magnitude \u221a \u03bb k. Graph signal processing involves defining wavelet operators on the graph using random walks on the graph to control the spatial graph support of the filters. The transition matrix of a lazy random walk is defined as P = 1/2D^-1A + I, ensuring row sums of P are all one. The transition matrix P = 1/2D^-1A + I is used for random walks on the graph, with P acting as a diffusion operator. Powers of P move the random walk forward, affecting the response of P t x most significantly at zero frequency x(0) while depressing non-zero frequencies. The value P t x(v) is a weighted average of x(v) with neighboring values x(v m). The value P t x(v) is a weighted average of x(v) with neighboring values x(v m) within t steps in the graph G. Wavelet transforms group non-zero frequencies of G into dyadic bands, providing stable operators in the Euclidean domain. The diffusion wavelet matrix at scale 2j partially recovers x(k) for k \u2265 1. The diffusion wavelet matrix at scale 2j recovers signal information from neighboring vertices, responding to sharp transitions or oscillations within a radius of 2j. Higher frequencies are recovered with smaller j values. The wavelet scattering transform on graphs leverages a graph wavelet transform, with wavelet coefficients denoted up to scale 2J. The selection of J is based on the graph diameter, controlling the maximum scale of the wavelet. The graph scattering transform is discussed in relation to other graph scattering constructions and its desirable properties compared to other geometric deep learning algorithms. Invariant graph features are obtained through summation operators acting on a signal defined on any graph. The geometric scattering transform follows this approach by computing the sum of the signal responses. The invariant graph features are obtained through summation operators computing the sum of signal responses. Higher order statistical moments, referred to as \"capsules,\" are used to capture additional information about the signal x. The unnormalized moments are discussed for simplicity, with Q values chosen via cross validation for optimal classification performance. Higher order moments are excluded due to instability, and results are presented for both normalized and unnormalized moments. The unnormalized moments are discussed for classification purposes, complemented by summary statistics from wavelet coefficients to form the graph ConvNet structure. Higher order moments capture the full range of frequencies in the signal x. The wavelet coefficients \u03a8 (J) x capture high frequencies of x, but are not invariant to permutations of vertex indices. Before summing the coefficients, a pointwise nonlinearity is applied. For regular graphs, the dominating coefficient in the expansion is |\u03a8 j x \u00b7 1| 1, leading to the use of absolute value nonlinearity for nonlinear covariant coefficients. The absolute value nonlinearity is applied to obtain nonlinear covariant coefficients, which are invariant to vertex permutations and nonexpansive. These coefficients, extracted from wavelet transforms, yield a stable scattering transform for q = 1. The first order geometric scattering moments provide multiscale statistics that partition the frequency responses of x, offering a finer description compared to traditional wavelet transforms. The second order geometric scattering moments, obtained by iterating the graph wavelet and absolute value transforms, lead to the structure of a graph ConvNet. These moments involve reapplying the wavelet transform operator to each coefficient and computing summary statistics. They couple different scales within the graph, creating features that connect patterns of smaller subgraphs with larger ones. The transform can be iterated further for higher-order moments. The collection of graph scattering moments provides a rich set of multiscale invariants of the graph, which can be used for graph classification or regression models. The scattering features aim to provide stability and capacity for representing graphs, ensuring robustness to noise and deformations. The diffusion scattering transform in BID17 and BID49 yields stable features to graph structure deformations. The scattering transform provides a rich feature space for representing graph data without losing informative variance. The stability and capacity of scattering transforms are typically examined empirically in machine learning tasks. In graph processing settings, the capacity of geometric scattering features is examined through their discriminative power in graph data analysis tasks. Extensive numerical experiments for graph classification show that SVM classification over scattering features achieves state-of-the-art results on social network data, outperforming feed-forward neural network methods. Additionally, FCL classification over scattering features outperforms other neural networks for biochemistry data. The geometric scattering feature space is examined for data representation and exploration, particularly in analyzing biochemistry data with enzyme graphs. It enables graph embedding in a low-dimensional Euclidean space while preserving data properties. The results suggest the viability of graph scattering transforms as universal feature extractors on graph data. The geometric scattering transform, represented by scattering coefficients Sx, is compared with other graph ConvNets. It is covariant to vertex permutations and propagates information through wavelet and absolute value layers. This feature extraction method is effective for analyzing biochemistry data with enzyme graphs and preserving data properties in a low-dimensional Euclidean space. The scattering transform extracts invariant statistics through wavelet and absolute value layers, aggregating covariant responses via summary statistics. This method eliminates needless complexity in classification or regression by providing important coarse geometric invariants. The graph wavelet transform decomposes the geometry of a graph through different scales, providing multiscale representations. While random walk operators act at different scales on the graph, they may lose high frequency responses useful in distinguishing similar graphs. In contrast, the graph wavelet coefficients respond strongly within specific frequency bands. The graph wavelet coefficients \u03a8 (J) x respond strongly within specific frequency bands, incorporating macroscopic graph patterns in every layer/order. This is advantageous compared to graph ConvNets which require many layers to aggregate macroscopic patterns due to fixed size filters. Geometric scattering transforms offer designed, multiscale wavelet filters for easier training on limited graph classification databases. The proposed geometric scattering features are evaluated for graph classification on thirteen datasets, including biochemistry graphs (NCI1, NCI109, BID45, BID14) and social network data (COLLAB, IMDB-B, IMDB-M, REDDIT-B, REDDIT-5K, REDDIT-12K). The biochemistry graphs have vertex features representing chemical properties, while the social network data lacks inherent graph signals. In graph classification, general node characteristics are computed over graphs with no inherent signals. Scattering features of available graph signals are computed and concatenated, then passed to a classifier (neural network, SVM, or logistic regression) to infer the class for each graph. Scattering features are based on normalized or unnormalized moments over the entire graph. The classification results of geometric scattering classification settings are evaluated using ten-fold cross validation and compared to 14 prominent graph classification methods, including graph kernel methods like Weisfeiler-Lehman, propagation kernel, and deep graph kernels. Technical design choices were made during cross validation. The classification results of geometric scattering classification settings are evaluated using ten-fold cross validation and compared to 14 prominent graph classification methods, including deep graph convolutional neural networks, diffusion convolutional neural networks, and recurrent neural network autoencoder for graphs. Reported classification performances are in the form of average accuracy \u00b1 standard deviation over the ten crossvalidation folds. The classification results of geometric scattering classification settings are compared to 14 prominent graph classification methods using ten-fold cross validation. The scattering transform provides universal graph features that offer stable classification results across various datasets. Previous methods may excel in specific cases but none achieve the best results in all datasets. The overall classification quality is compared by considering average accuracy aggregated over all datasets and within each field. The study compares geometric scattering classification settings with 14 graph classification methods using ten-fold cross validation. Results show that GSC (with SVM) outperforms all other methods on social network data, achieving state-of-the-art results on two datasets. The study compares geometric scattering classification with 14 graph classification methods using ten-fold cross validation. GSC (with FCL or SVM) outperforms other feed forward methods on biochemistry data and overall in terms of universal average accuracy. GSC provides a task independent representation of graphs in a Euclidean feature space, suitable for exploratory graph-data analysis. The focus is on biochemistry data, particularly the ENZYMES dataset. In biochemistry data, geometric scattering features serve as \"signature\" vectors for enzymes, allowing exploration of interactions between enzyme classes. The focus is on linear operations like PCA for feature extraction, emphasizing scattering-based embedding for dimensionality reduction of graph data. Applying PCA to scattering coefficients in the ENZYMES dataset results in a 16-dimensional subspace capturing 90% explained variance. The application of PCA to scattering coefficients in the ENZYMES dataset results in a 16-dimensional subspace, effectively reducing dimensionality. SVM classification on the resulting low-dimensional vectors shows a slight drop in accuracy compared to the full feature space. The scattering feature space is expected to reduce variability within each class. In the ENZYMES dataset, PCA is applied to scattering coefficients to create a 16-dimensional subspace, reducing dimensionality. Classes have PCA dimensionality ranging from 6 to 10, lower than the full 16 dimensions. The scattering feature space aims to reduce variability within each class. Further analysis explores relations between enzyme classes using the scattering feature space. The mean distance of enzymes from each EC-j class is computed in the scattering feature space. 48% of enzymes select their true EC as the nearest subspace, with 19% as the second nearest. These distances are used to infer EC exchange preferences during enzyme evolution, validated against established preferences. The ENZYMES dataset differs in enzyme distribution compared to BID3. The scattering features in ENZYMES dataset show EC \"incoherence\" in enzyme classes, correlating with evolutionary exchanges in BID12. Using these as EC weights in FIG5, the results demonstrate the capacity of geometric scattering to uncover insights in graph data analysis beyond supervised classification. The geometric scattering transform serves as a deep filter bank for feature extraction on graphs, augmenting the theoretical foundations of geometric deep learning. The evaluation results demonstrate the potential of scattering features as universal representations of graphs, achieving high accuracy in graph classification. The scarcity of labeled big data in this field may contribute to the success of scattering features. Geometric scattering features offer a new approach to computing global graph representations. Scattering features provide a new way for computing global graph representations, allowing for embedding entire graphs in Euclidean space and computing distances between them. This can be used for supervised and unsupervised learning, as well as exploratory analysis of graph-structured data. Datasets used include NCI1 with 4,110 chemical compounds and NCI109 with 4,127 compounds, both labeled for cancer cell activity. MUTAG consists of 188 mutagenic compounds. The MUTAG dataset contains 188 mutagenic compounds, while PTC dataset includes 344 chemical compounds categorized based on carcinogenicity in rats. The PROTEINS dataset comprises 1,113 proteins classified as enzymes or non-enzymes. D&D dataset consists of 1,178 protein structures classified similarly. ENZYMES dataset contains 600 protein structures categorized into six enzyme classes. COLLAB dataset involves 5K graphs for scientific collaboration analysis. COLLAB dataset contains 5K graphs for scientific collaboration analysis. IMDB-B dataset has 1K graphs for predicting genres like Action and Romance. IMDB-M has 1.5K graphs with genres Comedy, Romance, and Sci-Fi. REDDIT-B dataset includes 2K graphs for predicting Q&A-based or discussion-based communities. REDDIT-5K has 5K threads from different subreddits, while REDDIT-12K has 11,929 graphs from 12 subreddits. Table 4 summarizes the size of available graph data in various datasets, including the number of graphs and characteristics of vertices. Social network datasets lack ready-to-use node features, so characteristic graph signals like eccentricity, degree, and clustering coefficients are used. The software and hardware environment for implementing geometric scattering and classification code involved Python with TensorFlow on an HPC environment using an intel16-k80 cluster with Nvidia Tesla k80 GPUs."
}