{
    "title": "rylBK34FDS",
    "content": "In seeking efficient neural network models, previous works explored L1 and L0 regularizers for weight sparsity. L0 lacks useful gradients, requiring complex optimization, while L1 is easily optimized but inefficient in increasing sparsity. DeepHoyer introduces scale-invariant, differentiable regularizers inspired by the Hoyer measure, producing even sparser models than previous methods. Regularizers like DeepHoyer can create sparser neural network models while maintaining accuracy. These models are used in various real-world applications but often have a large number of parameters, making deployment on edge devices challenging. Pruning methods are studied to increase weight matrix sparsity and reduce memory consumption and computation cost. Previous studies have focused on reducing memory consumption and computation cost of DNNs by utilizing sparsity-inducing regularizers. The 1 regularizer, proposed by Tibshirani, is commonly used in DNN pruning to achieve element-wise or structural sparsity. However, the 1 regularizer scales down all elements in weight matrices at the same speed, which is not efficient. The 0 regularizer directly reflects the real sparsity of weights and is scale invariant, but lacks useful gradients. Some recent works integrate 0 regularization with stochastic approximation or more complex optimization methods, which adds overheads to the optimization process. To achieve sparser neural networks, moving beyond 0 and 1 regularizers is suggested. To achieve sparser neural networks, moving beyond 0 and 1 regularizers is suggested. Seeking a sparsity-inducing regularizer that is almost everywhere differentiable and scale-invariant is proposed. Various non-convex sparsity measurements have been used in feature selection and compressed sensing. Popular regularizers like SCAD, MDP, and Trimmed 1 use a piece-wise formulation to address the scaling problem of 1. The transformed 1 regularizer smoothly interpolates between 1 and 0. The Hoyer regularizer is a scale-invariant sparsity-inducing regularizer that outperforms others in various fields. DeepHoyer is proposed as the first Hoyer-inspired approach. DeepHoyer introduces Hoyer-Square (HS) regularizer for element-wise sparsity and Group-HS regularizer for structural sparsity in DNNs. The regularizers outperform state-of-the-art methods in weight pruning, generating sparser DNN models. In weight pruning of modern DNNs, pruning methods aim to remove unimportant weights. Some methods simply remove small value weights, leading to sparse models with long training times. Other approaches involve sparsity-inducing optimization problems like 1 regularization or 0 regularization, which require special optimization techniques. DeepHoyer regularizers, introduced in this work, belong to the sparsity-inducing optimization research line. The proposed Hoyer-Square regularizer for element-wise pruning is scale-invariant and serves as a differentiable approximation to the 0 norm. It achieves significant sparsity improvements on various models compared to previous methods, including the highest sparsity on AlexNet without accuracy loss. Structurally sparse DNNs aim to create hardware-friendly sparse patterns by removing filters with small norms and applying group Lasso regularization to remove various structures in DNNs. The DeepHoyer method introduces structured sparsity in DNNs with the Group-HS regularization, improving computation reduction in models like LeNet-5. Experiments on ResNet models show that Group-HS maintains a favorable accuracy-speedup tradeoff compared to previous methods. Sparsity measures offer manageable constraints for enforcing sparsity in problem-solving, extensively studied in compressed sensing. The Hoyer measure, a sparsity measure in non-negative matrix factorization research, maps vectors to a real number between 0 and 1. It satisfies five out of six heuristic criteria for sparsity measures and has been successfully applied in optimization problems like blind deconvolution and image deblurring. The Hoyer regularizer, used in image deblurring, is compared to the 1 regularizer. It has minima along axes similar to the 0 norm and is scale-invariant. The gradients are radial, leading to better performance on various tasks compared to the 1 regularizer. The Hoyer regularizer has a better guarantee on recovering sparse solutions. The Hoyer regularizer has a better guarantee than the 1 norm for recovering sparse solutions from coherent and redundant representations. Two types of DeepHoyer regularizers are proposed: Hoyer-Square regularizer (HS) for element-wise pruning and Group-HS regularizer for structural pruning. The proposed HS regularizer behaves as a differentiable approximation to the 0 norm. The Hoyer-Square regularizer is a differentiable approximation to the 0 norm, with minima structure similar to the Hoyer regularizer. It induces a trimming effect during gradient descent, turning small weights to zero while protecting large weights. The HS regularizer gradually prunes weights close to zero, extending the scope of pruning. Structural pruning, empowered by group lasso, constructs sparsity in a structured way for higher computation speed-up. The Hoyer-Square regularizer replaces the 1 regularizer to induce a trimming effect during gradient descent. In Section 4.1, the Hoyer-Square regularizer is used to replace the 1 regularizer in the group lasso formulation, defining the Group-HS regularizer. The Group-HS regularizer can be used when groups overlap, with a similar gradient and descent path to the Hoyer-Square regularizer. The deployment of DeepHoyer regularizers in DNN training follows a layer-based regularization approach. For element-wise pruning, the Hoyer-Square regularizer is applied to layer weight matrices for all layers. For structural pruning, the Hoyer-Square regularizer is applied to layer weight matrices in DNN training. The focus is on pruning columns and rows of fully connected layers, and filters and channels of convolutional layers. Group-HS regularizer is used to group layers in filter-wise and channel-wise fashion, with specific optimization objectives formulated. The recent advance in stochastic gradient descent (SGD) method allows for optimizing DeepHoyer regularizers in DNN training. Pruning is done by training with DeepHoyer, pruning small weights, and finetuning without the regularizer. The proposed DeepHoyer regularizers are tested on various models and datasets, consistently outperforming previous works in element-wise and structural pruning. The models are implemented in PyTorch, matching previous works for fair comparison. The proposed Hoyer-square regularizer outperforms previous works in element-wise and structural pruning. Training with this regularizer significantly reduces the number of nonzero weights on different models, achieving the highest sparsity compared to state-of-the-art methods. Additional results in Appendix C.1 further illustrate the impact of the regularizer on weight distribution during training. The Hoyer-Square regularizer improves compression rate by 21.3\u00d7 on the AlexNet model without losing testing accuracy. It outperforms the ADMM method and achieves sparse DNN models at a lower cost. A detailed layer-by-layer sparsity comparison is available in Appendix C.2. Ablation study results show the superiority of the Hoyer-Square regularizer over the original Hoyer regularizer. The Hoyer-Square regularizer achieves higher compression rates than the original Hoyer regularizer, emphasizing layers with more parameters. This validates its effectiveness for DNN compression. The Group-HS regularizer is effective in structural pruning tasks, focusing on the number of remaining neurons and required FLOPs for inference. Training with the Group-HS regularizer reduces FLOPs significantly for models like LeNet-300-100 and LeNet-5, outperforming existing methods in speedup while maintaining testing accuracy. The Group-HS regularizer shows promise in reducing FLOPs for structural pruning tasks, surpassing other methods like Bayesian compression in speedup on the LeNet-5 model. The Group-HS regularizer is effective for reducing FLOPs in models like LeNet-300-100 and LeNet-5, surpassing Bayesian compression in speedup. It can be extended to deeper models like ResNet on CIFAR-10 and ImageNet datasets, demonstrating its versatility. In this work, the tradeoff between accuracy and FLOPs is explored by adjusting the Group-HS regularizer strength in training. The proposed DeepHoyer regularizers are scale-invariant and differentiable, effectively measuring and regulating sparsity in DNN weight matrices. They outperform previous methods and can be optimized with standard gradient-based methods. The Hoyer-Square regularizer is tested in element-wise pruning experiments. The proposed DeepHoyer regularizers effectively regulate sparsity in DNN weight matrices, outperforming previous methods. The Hoyer-Square regularizer achieves significant sparsity increases without accuracy loss on various models. The Group-HS regularizer further reduces computation load and improves accuracy-FLOPs tradeoff on different datasets. The DeepHoyer regularizers are effective in achieving sparsity in deep neural networks, producing even sparser models than previous methods. Detailed derivations of the Hoyer-Square and Group-GS regularizers are provided. The MNIST dataset is used for experiments, consisting of greyscale images of handwritten digits. In experiments using the MNIST dataset, greyscale images of handwritten digits are utilized. The training set consists of 60,000 images, while the testing set has 10,000 images. The accuracy results are based on the testing set, with normalization applied to both sets. The LeNet-300-100 and LeNet-5 models achieve testing accuracies of 98.4% and 99.2% respectively before further training with DeepHoyer regularizers. The regularizers lead to sparser models, with weight decay parameters chosen manually. The weight decay parameters are manually selected to optimize results. Each layer's weight is pruned based on a threshold/std ratio to achieve high sparsity without accuracy loss. The pruned model is finetuned for 100 steps without DeepHoyer regularizers to achieve the best testing accuracy. The detailed parameter choices and dataset information are provided in Table 6. The ImageNet dataset, containing 1.2 million color images of 1000 categories, is used as a benchmark for image classification tasks. In this paper, the \"ILSVRC2012\" dataset is used for training and evaluating models. Data preprocessing follows the PyTorch ImageNet example, including random cropping, flipping, and normalization. Different input sizes are used for experiments on AlexNet and ResNet-50. Models are optimized with the SGD optimizer. Experiments on ResNet-50 and AlexNet models were conducted using the SGD optimizer. Batch sizes of 256 were used, with multiple GPUs for training. The AlexNet model from the \"torchvision\" package was re-implemented for fair comparison. Pretraining the model for 90 epochs resulted in a 19.8% top-5 error rate. The Hoyer-Square regularizer with a decay parameter of 1e-6 was applied in the AlexNet experiment. Initial training with the regularizer and a learning rate of 0.001 was done before pruning the model. The ResNet-50 experiments on ImageNet utilized the model architecture and pretrained model from the \"torchvision\" package, achieving 23.85% top-1 error and 7.13% top-5 error. The training process involved pruning convolution and FC layers with specific thresholds, finetuning the model until reaching the best accuracy, and decaying the learning rate every 30 epochs. The reported results were achieved with 90 epochs of training using the Group-HS regularizer. The Group-HS regularizer was used for training ResNet models on CIFAR-10 dataset with 90 epochs. Pruning was done with a threshold of 1e-4 and models were finetuned for best accuracy. The decay parameter of the regularizer was tuned to balance accuracy and FLOPs tradeoff. Standard preprocessing techniques were applied during training, and models were pretrained for 164 epochs with a learning rate of 0.1. The pretrained ResNet-56 and ResNet-110 models achieved testing accuracies of 93.14% and 93.62% respectively after 164 epochs of training. The Group-HS regularizer was then applied for further training and pruning with a threshold of 1e-4. Additional experiments were conducted to observe the weight distribution changes during the pruning process. After applying the H S regularizer during training, most weights in the LeNet-300-100 and LeNet-5 models concentrate near zero, with the remaining weights spread out. Zero weights are then fixed, resulting in a significant pruning of weights. The Hoyer-Square regularizer achieves high pruning rates on the largest layers of AlexNet, consistent with observations on MNIST models. In this section, data used to plot Figure 3 is listed along with the results of pruning ResNet models on different datasets. Tables show the pruning results of ResNet-50 on ImageNet, ResNet-56 on CIFAR-10, and ResNet-110 on CIFAR-10. Results achieved with the Group-HS regularizer are also included, marked with the regularization strength used for training."
}