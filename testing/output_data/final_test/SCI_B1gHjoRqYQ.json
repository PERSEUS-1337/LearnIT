{
    "title": "B1gHjoRqYQ",
    "content": "MarginAttack is a zero-confidence attack framework that computes the margin of an input feature with improved accuracy and efficiency compared to existing methods. It outperforms state-of-the-art zero-confidence attacks and matches fix-perturbation attacks, while running faster than the Carlini-Wagner attack. In this paper, the focus is on white-box attacks, specifically on finding small perturbations to cause misclassification in neural network classifiers. Two paradigms of adversarial attacks are discussed: fix-perturbation attacks aim to find perturbations within a given norm constraint, which may fail for inputs far from the norm. Zero-confidence attacks aim to find the smallest perturbations that guarantee misclassification, often riding on decision boundaries. These attacks focus on minimizing the perturbation norm, also known as the margin of an input feature to the decision boundary. The two paradigms of adversarial attacks discussed are essentially constrained optimization problems, with fix-perturbation attacks having a simple convex constraint but a non-convex target, and zero-confidence attacks having a non-convex constraint but a simple convex target. The fix-perturbation attack problem is easier than the zero-confidence attack problem. State-of-the-art algorithms like PGD and distributional adversarial attack can achieve high efficiency and success rates. Existing methods for zero-confidence attacks are either weak or slow. Linearization errors can lead to suboptimal solutions in some cases. MARGINATTACK is a zero-confidence attack framework that computes the margin with improved accuracy and efficiency. It iterates between a restoration move and a projection move to measure the margin of each individual token, providing insight into data distribution and adversarial robustness. MARGINATTACK is an efficient attack framework that computes smaller margins than other zero-confidence attacks, matching fix-perturbation attacks. It outperforms DeepFool and FGSM in accuracy and speed, with a convergence guarantee. Other works like Jacobian-based saliency map attack BID13 also explore margin manipulation using gradient information. The MARGINATTACK framework is inspired by Rosen's algorithm for constraint optimization problems, aiming to perturb input features to achieve the adversarial goal. It outperforms DeepFool and FGSM in accuracy and speed, with a convergence guarantee. Other white-box attack algorithms like One-pixel attack and CLEVER are also relevant in this field. The MARGINATTACK algorithm improves upon Rosen's algorithm by having a convergence guarantee with more realistic assumptions and a simpler step size scheme. It also offers a large class of attack algorithms depending on parameter settings, unlike Rosen's algorithm which is limited to one setting. The convergence guarantee of MARGINATTACK holds for all settings that meet moderate assumptions. The MARGINATTACK algorithm introduces the algorithm and its convergence properties, denoting scalars, vectors, matrices, and sets. It computes for non-targeted adversarial attacks with constraints defined by an offset parameter \u03b5. The MARGINATTACK algorithm involves performing restoration and projection moves to generate adversarial samples. The restoration move aims to move the point closer to the constraint boundary with a short hop. The MARGINATTACK algorithm involves restoration and projection moves to create adversarial samples. The restoration move aims to move the point closer to the constraint boundary with a short hop, while the projection move tries to move closer to x 0 without drastic changes to c(x). The MARGINATTACK algorithm uses projection moves to balance reduction in distance and constraint. Two designs for a (k) and b (k) are discussed, ensuring constraint values remain similar or perturbation norm reduces. The algorithm works with a wide range of bounded a (k) and b (k) values, providing a general and flexible framework. MARGINATTACK is a flexible framework for zero-confidence adversarial attacks, with Eq. (8) working better for 2 norm and \u221e norm. The algorithm uses restoration and projection moves to approach the decision boundary, as shown in FIG1 and FIG2. The MARGINATTACK-2 algorithm uses restoration and projection moves to approach the decision boundary, as illustrated in FIG2 with convergence curves showing how constraint value and perturbation norm change. The nonconvex constraint function limits convergence analysis to a unique local optimum, as per Theorem 1. The neighborhood B = {x : 2 \u2264 X, |c(x)| \u2264 C} satisfies assumptions including differentiability and unique optimality. The convergence guarantee is provided with a proof in the appendix. Assumption 1 allows for jump discontinuities in gradient, which is practical for deep neural networks. Assumption 3 requires constraint gradient to be lower bounded. The neighborhood B satisfies assumptions for unique optimality and convergence guarantee. Assumption 5 stipulates convexity of c(x) in B for local minimum implication. Implementation details include box constraint for input features. The implementation details include a box constraint for input features to impose boundaries on each dimension of the input features. The box constraint is incorporated in a principled way in the restoration move problem, leading to faster convergence. Target Scan involves approaching the adversarial class with the highest logit during each restoration move. Target scan is a method used in restoration moves to approach the adversarial class with the highest logit by introducing target-specific constraints. This approach helps in choosing the move with the shortest distance, especially in the initial restoration moves. The MARGINATTACK procedure involves target scan in the initial restoration moves to find the closest adversarial class. The initialization of x can be deterministic or random. Final iterations consist of restoration moves only to ensure misclassification. Each restoration move contributes to the complexity of the algorithm. Alg. 1 summarizes the MARGINATTACK procedure and compares it with other adversarial attack algorithms in terms of perturbation norm and computation time on image classification benchmarks like MNIST, CIFAR10, and ImageNet. The MARGINATTACK algorithm is compared with other adversarial attack algorithms on image classification benchmarks like MNIST, CIFAR10, and ImageNet. Key details of baseline algorithms and their settings are provided, including CW, DeepFool, FGSM, and PGD. The MARGINATTACK algorithm outperforms other adversarial attack algorithms on image classification benchmarks like MNIST, CIFAR10, and ImageNet. Different versions of MARGINATTACK with target and evaluation norms are implemented, showing higher success rates at all perturbation levels and datasets compared to CW, DeepFool, FGSM, and PGD. CW is close to MARGINATTACK on MNIST and CIFAR10 but MARGINATTACK maintains a 3% advantage on MNIST and 1% on CIFAR10. CW struggles to converge within 2,000 iterations on ImageNet despite tuning the learning rate. MARGINATTACK converges more efficiently. Running PGD for different perturbation levels is time-consuming, so four levels are chosen for comparison based on MARGINATTACK margins. TAB1 compares success rates under these quantiles among attacks. MARGINATTACK outperforms PGD and dominates FGSM in success rates under different perturbation levels. It is evaluated in the MNIST Adversarial Examples Challenge with 50 random starts, 500 moves, and a perturbation range of 0.3. MARGINATTACK has the second-best success rate among fix-perturbation attacks. MARGINATTACK, the only zero-confidence attack algorithm, performs competitively against state-of-the-art fix-perturbation attacks. It converges quickly within 20 moves, making it efficient. The number of moves can be adjusted based on priority - 200 for accuracy and 30 for efficiency. The running time comparison on a single NVIDIA TESLA P100 GPU is provided in Tab. 3 for attacking one batch of images. MARGINATTACK is a zero-confidence adversarial attack algorithm that converges quickly within 20 moves, making it efficient. The running time of MARGINATTACK is much shorter than CW, comparable to DeepFool and PGD. CW is significantly slower due to multiple trials for the best Lagrange multiplier. Early stop is enabled for DeepFool and CW, but not for MARGINATTACK. MARGINATTACK is an efficient zero-confidence adversarial attack algorithm that outperforms existing methods. It has room for improvement with different settings, as long as assumption 5 is met. The authors encourage exploring novel settings for MARGINATTACK and promote it as a new robustness evaluation measure in the field of adversarial attack and defense. If assumption 3 in Thm. 1 holds, then \u2200x \u2208 B, according to Eq. (5) for 2 norm and \u221e norm. Lemma 1.2 states that under all assumptions in Thm. 1, A and B are defined. The lemma proves that restoration moves bring c(x) closer to 0, while projection moves do not change c(x) much. The proof involves the Mean-Value Theorem and specific equations and assumptions. Lemma 1.3 proves that under the assumptions in Thm. 1, restoration moves bring c(x) closer to 0, while projection moves do not change c(x) much. The proof involves specific equations and assumptions, such as Eq. (4), assumptions 4 and 7, and Eq. (19). The lemma also discusses the implications of different norms and concludes with the proof. Proof. The proof involves equations derived from Lemmas 1.1 and 1.2. By combining equations, we show that certain inequalities hold. Additionally, Lemma 1.5 is discussed under the assumptions in Theorem 1. The proof concludes with the implications of different solutions to the optimization problem. The proof involves equations derived from Lemmas 1.1 and 1.2, showing certain inequalities hold. Lemma 1.5 is discussed under the assumptions in Theorem 1, concluding with implications of different solutions to the optimization problem. Thm. 1 is then proven using Lemmas 1.2, 1.3, and 1.4, establishing that certain equations hold. The proof involves showing that Eqs. FORMULA0 and FORMULA2 imply lim k\u2192 x (k) \u2212 x 0 2 = 0. By assuming c(x) is monotonic and solving for y, we can determine \u03bb. Thm. 2 applies to optimization problems in Eqs. FORMULA2 and FORMULA1. Assuming x satisfies certain conditions, we can derive equations that show the optimal solution to a problem. By combining these equations, we can confirm that a certain assumption holds with strict inequality. This concludes the proof. The proof concludes by showing that under certain conditions, there exists a continuous function that satisfies assumption 5."
}