{
    "title": "Hkg_8xBYDS",
    "content": "In a large connected learner network, evaluating strategy profiles is crucial for machine learning applications. The $\\alpha$-Rank algorithm was proposed for ranking joint policy profiles in multi-agent systems, claiming scalability but actually exhibiting exponential complexity in the number of agents. A new protocol, $\\alpha^{\\alpha}$-Rank, combines evolutionary dynamics and stochastic optimization for truly scalable ranking with linear time and memory complexities. Our contributions enable large-scale evaluation experiments of multi-agent systems, showing successful results on joint strategy profiles with sizes of approximately 33 million strategies. Scalable policy evaluation and learning in multi-agent reinforcement learning face challenges due to exponentially growing joint-strategy spaces and cyclic game dynamics. Previous work focused on game-theoretic treatments like fixed-points, such as Nash equilibrium. Recently, work in multi-agent systems has highlighted the limitations of using fixed-point equilibria, such as Nash equilibrium, as evaluation metrics due to the unrealistic assumption of perfectly rational agents. Game dynamics often lead to limit cycles instead of converging to fixed points. Solving for Nash equilibrium, even in simple settings like two-player games, is computationally demanding. To address these challenges, \u03b1-Rank has been proposed as a graph-based game-theoretic solution for multi-agent evaluation. \u03b1-Rank is a graph-based game-theoretic solution for multi-agent evaluation that uses Markov Conley Chains to identify cycles in game dynamics. It struggles with scalability, exhibiting exponential time and memory complexities. A new method, \u03b1\u03b1-Rank, is proposed as a scalable alternative with linear time and memory demands. This method combines numerical optimization with evolutionary game theory to handle large joint spaces with millions of strategy profiles. The text discusses the introduction of an oracle mechanism for handling large strategy profiles in multi-agent systems. It also mentions the computation advantages of \u03b1\u03b1-Rank in evaluating stochastic matrices and conducting experiments unsolvable by current techniques. In evaluating multi-agent systems with large strategy spaces, \u03b1-Rank uses an evolutionary process of mutation and selection to rank strategies. The approach is capable of handling complex domains like self-driving and Ising model scenarios. The agent faces a probabilistic choice between switching to the mutation policy, continuing with its current policy, or randomly selecting a new policy. This process aims to determine an evolutionary strong profile that spreads across the population of agents. Mathematical formalization of the process is done to understand the limitations of \u03b1-Rank and propose a solution. The joint strategy profile for all agents involves policies from a strategy pool. Each agent has a payoff function to evaluate performance within the joint strategy pool. After receiving rewards, agents decide whether to switch policies based on rewards. Agent i makes a probabilistic choice to adopt a policy \u03c0 based on rewards, exploration parameter \u00b5, policies of other agents \u03c0i, and intensity ranking parameter \u03b1. The switching process can be related to a random walk on a Markov chain with transition probabilities determined by payoff functions. The transition probability matrix T represents the likelihood of an agent switching policies based on attained payoffs. The \u03b1-Rank model focuses on establishing an evolutionary stable ordering of policy profiles based on the likelihood of policy changes among agents in a population. The transition probabilities in the Markov chain are determined by payoff functions, with the goal of ranking strategies based on their stability and prevalence. The \u03b1-Rank model aims to rank strategies based on stability and prevalence in populations. It uses a Markov chain to determine transition probabilities and establish an evolutionary stable ordering of policy profiles. However, \u03b1-Rank is limited by scalability issues, which this paper addresses by proposing a solution to the problem. The \u03b1-Rank model aims to rank strategies based on stability and prevalence in populations using a Markov chain. However, it faces scalability issues due to exponential complexity in determining rankings for a large number of agents. Traditional approaches are not suitable, leading to the proposal of an efficient evaluation algorithm called \u03b1-Rank based on stochastic optimization. Additionally, a search heuristic named \u03b1-Oracle is introduced to further scale up the method. The problem of computing stationary distributions is addressed using various techniques from linear algebra. Various techniques from linear algebra, such as the power method, PageRank, eigenvalue decomposition, and mirror descent, can be used to solve the classical problem in Eqn. 3. Implementations of these techniques scale exponentially with the number of learners. The power method, for example, exhibits exponential memory complexity in terms of the number of agents. Time complexity analysis requires linking convergence rates with the resulting graph topology of the Markov chain. The convergence rate of the power method in solving the eigenvalue problem is determined by the second-smallest eigenvalue of the normalized Laplacian of the graph associated with the Markov chain. The time complexity of the power method is exponential and can be computed using PageRank. The memory and time complexity of eigenvalue decomposition methods like \u03b1-Rank and Mirror Descent are exponential, limiting their applicability to a small number of agents. Mirror Descent requires a projection on a standard n-dimensional simplex, which is computationally expensive. The memory and time complexity of eigenvalue decomposition methods like \u03b1-Rank and Mirror Descent are exponential, limiting their applicability to a small number of agents. Mirror Descent requires a projection on a standard n-dimensional simplex, which is computationally expensive. In contrast, approximate solvers can be used for \u03b1-Rank when dealing with a large number of agents, focusing on determining an approximate vector x that minimizes distance while lying on an n-dimensional simplex. This relaxed optimization problem can be solved using a barrier-like technique. The relaxed optimization problem in Eqn. 5 can be solved using a barrier function approach, leading to an unconstrained finite sum minimization problem. By introducing logarithmic barrier-functions with a penalty parameter \u03bb > 0, Eqn. 6 can be solved using stochastic optimization algorithms like stochastic gradients or ADAM. This involves sampling a strategy profile at each iteration and executing a descent step to find the solution. The relaxed optimization problem in Eqn. 5 can be solved using a barrier function approach with a penalty parameter \u03bb > 0. Stochastic optimization algorithms like stochastic gradients or ADAM can be used to solve Eqn. 6 by sampling a strategy profile at each iteration and executing a descent step. The convergence theorem of the barrier method states that after T iterations, the output x \u03bb converges with a decay-rate \u03b3 > 1. The algorithm involves initializing strategy pools, setting outer iteration count, and updating the solution x in a scalable policy evaluation phase. The algorithm involves solving an optimization problem using a barrier function approach with a penalty parameter \u03bb > 0. Stochastic optimization algorithms like stochastic gradients or ADAM are used to update the solution x. The proof of the theorem highlights the memory and time complexity implications of the algorithm, showing exponential reduction in the number of agents. Our algorithm achieves exponential reduction in memory and time complexities for multi-agent evaluations through stochastic optimization. Scalability can be further enhanced by introducing an oracle mechanism, originally used in solving large-scale zero-sum matrix games. The oracle helps create restricted sub-games and find best responses before expanding strategy pools. The worst-case scenario involves solving the original problem in full size, with the oracle providing best responses. The algorithm achieves exponential reduction in complexities for multi-agent evaluations through stochastic optimization. An oracle mechanism is introduced to create restricted sub-games and find best responses before expanding strategy pools. The oracle provides best responses in the worst-case scenario of solving the original problem in full size. In this paper, the \u03b1-Oracle algorithm is discussed, which aims to converge to the minimax equilibrium in two-player zero-sum games. The effectiveness of the approach is demonstrated in a large-scale empirical study, showing the scalability properties of \u03b1-Rank 5 in recovering optimal policies in self-driving car simulations and the Ising model with millions of possible strategies. This method outperforms state-of-the-art methods like \u03b1-Rank and AlphaStar in handling larger strategy spaces. During implementation, a novel data structure for sparse storage was tailored to exploit the sparsity pattern in the transition probability of the Markov chain, leading to significant speed-ups. The correctness of ranking results was validated on simple cases before conducting large-scale experiments. Results from the \u03b1-Rank algorithm were consistent with those from Algorithm 1's Phase I. Time and complexity results on random matrices were also measured. The study measured time and memory requirements for computing stationary distribution on random matrices, comparing results with \u03b1-Rank, eigenvalue decomposition, and PyTorch optimization tools. \u03b1-Rank showed significant time reduction compared to eigenvalue decomposition, with performance gap increasing with matrix size. Additionally, a ranking experiment was conducted in an environment simulating self-driving scenarios with social vehicles. In a study involving 5 agents with 5 strategies each, optimal profile consists of five rational drivers. Cars were trained using value-iteration with rewards from 200 test trials. Results show that both \u03b1-Rank and \u03b1-Oracle can recover the correct highest ranking strategy profile. The study also considered the size of the strategy space and used \u03b1-Rank and the power-method for analysis. The Ising model describes ferromagnetism using magnetic spins with up or down orientations. Energy is determined by coefficients h and \u03bb, with equilibrium being difficult to find due to the need to compute all configurations. Traditional methods like Markov Chain Monte Carlo are used. Phase changes occur as temperature increases, affecting spin equilibrium. The Ising model describes ferromagnetism using magnetic spins with up or down orientations. Energy is determined by coefficients h and \u03bb, with equilibrium being difficult to find due to the need to compute all configurations. Traditional methods like Markov Chain Monte Carlo are used. Phase changes occur as temperature increases, affecting spin equilibrium. In this study, the phase change is observed through multi-agent evaluation methods, treating each spin as an agent. The system becomes chaotic as temperatures decrease, and the approach demonstrates exponential time and memory complexity. In this paper, a scalable solution called \u03b1-Rank is proposed for multi-agent evaluation with linear time and memory demands. The method is demonstrated to be truly scalable in handling large strategy spaces. Future research includes analyzing convergence properties of the oracle algorithm and conducting multi-robot experiments. The second-smallest eigenvalue of the normalized Laplacian of the graph associated with the Markov chain is given by a joint and transition probability matrix. The underlying graph for the Markov Chain is a Cartesian product of N complete graphs. The spectral properties of the graph can be described in terms of the spectral properties of the complete graphs. The minimum non-zero eigenvalue of the graph is determined by the degree of each node. The smallest non-zero eigenvalue of the normalized Laplacian of a regular graph is determined by the degree of each node. The Power Method has a time complexity bounded by O(log n) and memory requirements similar to the PageRank algorithm. However, it scales exponentially with the number of agents N, making it impractical for large N. The convergence of the Barrier Method in gradient algorithms is determined by a decay-rate parameter \u03b3, with Algorithm 2 outputting a vector x after a certain number of iterations. Algorithm 2 outputs vector x \u03bb 0 after 1 2 iterations, approximating a stationary distribution vector \u03bd [k]. It starts with a uniform distribution vector x 0 = 1 n 1 and updates iteratively. The transitional probability matrix T in \u03b1 \u03b1 -Rank is sparse, requiring a new data structure for storage and computation. Our data structure efficiently stores (defaults, positions, biases) for sparse vectors, improving computational efficiency for operations like addition, scalar multiplication, dot product, and L1 norm. The algorithm provides expected rankings in normal-form games and shows the example of addition in Fig. 5. In the Battle of sexes game, our method suggests long-term survival rates for all three populations. The method involves using gradient updates with warm-up and Adam phases for faster convergence. Column sampling is done for the stochastic matrix with a momentum term to stabilize learning. Infinite \u03b1 is implemented for calculating transition matrix with a noise term of 0.01. Termination conditions are set based on gradient norm for different experiments. Learning rate is adjusted accordingly. In the Random Matrix experiment, termination conditions were set with a gradient norm of 10^-2, learning rate between 15-17, alpha between 1-2.5, and population between 25-55. Adam experiments involved decaying \u03b4 and \u03bb by 0.999 after warm-up steps, with Collision Reward calculated for collisions. Value iteration agents were based on Leurent (2018) environment discretization, running for 200 steps with a discounting factor of 0.99. Default speed for controllable cars was randomized between 10 to 25. The default speed for controllable cars is randomized between 10 to 25, while social cars have speeds between 23 to 25. Five types of driving behaviors are defined by different ego reward functions for controlled cars during training to ensure rational driving."
}