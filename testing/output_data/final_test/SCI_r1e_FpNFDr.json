{
    "title": "r1e_FpNFDr",
    "content": "The generalization error bounds of convolutional networks are proven in terms of training loss, parameters, Lipschitz constant, and weight distance. These bounds are independent of input size and feature map dimensions. Experiments with CIFAR-10 and varying hyperparameters of deep networks are presented to compare the bounds. Recent progress has been made in analyzing the generalization of deep learning models, focusing on the limited set of models computable using weights with small magnitude. Training algorithms tend to produce small weights if the weights start small, leading to induction leverage. Implicit bias in deep learning has also been explored theoretically. Recent studies have shown that generalization bounds can be determined by the distance from the initial weight setting rather than the size of the weights. It is crucial to choose initial weights that maintain a strong signal to prevent vanishing gradients. Theoretical analyses suggest that well-behaved functions can be found by training a large network initialized in this way, traveling a short distance in parameter space. This implies that the distance from initialization is expected to be much smaller than the weight magnitude. Recent studies suggest that the distance from initialization plays a crucial role in determining generalization bounds, rather than the size of the weights. Convolutional layers, commonly used in deep neural networks for image processing, have been analyzed for their distance from initialization. Weight-tying in convolutional layers restricts the functions computed, potentially aiding generalization. New generalization bounds for convolutional networks that consider this effect have been proven in this paper. The study focuses on deriving new generalization bounds for convolutional networks that consider the distance from initialization. The bounds are independent of input size and layer dimensions, with a linear dependency on depth, aligning more with practical observations compared to earlier exponential dependencies. The study derives new generalization bounds for convolutional networks, focusing on the distance from initialization. The bounds are independent of input size and layer dimensions, with a linear dependency on depth. The central technical lemmas involve bounds on covering numbers, borrowing techniques from previous analyses. Covering bounds can be applied to obtain various generalization bounds, with examples provided. The covering bounds are polynomial in the inverse of the granularity of the cover, particularly useful for bounding relative error. The study introduces new generalization bounds for convolutional networks, focusing on distance from initialization. The bounds are independent of input size and layer dimensions, with a linear dependency on depth. Covering bounds are applied, with a focus on bounding relative error and the dependence of estimation error on the Lipschitz constant. Related work includes size-free bounds for CNNs and analysis of network generalization. Arora et al. (2018) and Zhou & Feng (2018) analyzed generalization of compressed CNNs and provided guarantees based on kernel rank constraints. Li et al. (2018) studied generalization under different parameter constraints. Lee & Raginsky (2018) offered a size-free bound for CNNs in unsupervised learning. The text introduces bounds for convolutional networks focusing on distance from initialization, independent of input size and layer dimensions, with linear depth dependency. In a clean and simple setting, a deep convolutional network with 1-Lipschitz and nonexpansive activations is considered. The network has convolutional layers with zero-padding and the last layer consists of fixed weights. The total number of trainable parameters in the network is denoted by W = Lk^2c^2. The kernels of the convolutional layers are -tensor obtained by concatenating the kernels for the various layers. The text discusses the initialization of kernels in a deep convolutional network, providing a generalization bound in terms of distance from initialization. It introduces the set of kernel tensors within a certain distance of the initialization and defines the set of functions computed by CNNs with these kernels. The text also mentions a loss function that is \u03bb-Lipschitz and presents basic bounds for training examples drawn from a joint probability distribution. The text discusses the initialization of kernels in a deep convolutional network, providing a generalization bound in terms of distance from initialization. It introduces the set of kernel tensors within a certain distance of the initialization and defines the set of functions computed by CNNs with these kernels. The text also mentions a loss function that is \u03bb-Lipschitz and presents basic bounds for training examples drawn from a joint probability distribution. Theorem 2.1 provides an upper bound on misclassification probability on test data, and the algorithm from Sedghi et al. (2018) can efficiently compute || \u00b7 || \u03c3. The text discusses the initialization of kernels in a deep convolutional network, providing a generalization bound in terms of distance from initialization. Lemma 2.3 states conditions for functions in a set G. Theorem 2.1 is proven by showing F \u03b2 is \u03b2\u03bbe \u03b2, W-Lipschitz parameterized through a series of lemmas, including Lemma 2.5 which involves functions in the network. The text discusses the initialization of kernels in a deep convolutional network, providing a generalization bound in terms of distance from initialization. Lemma 2.6 provides a bound for changing layers in the network, and Theorem 2.1 is proven by showing the mapping \u03c6 is \u03b2\u03bbe \u03b2-Lipschitz. Lemma 2.6 proves the \u03b2-Lipschitz mapping, completing the proof with Lemma 2.3. Theorem 2.1, along with model selection techniques, yields a bound with consequences for convolutional networks. The bound from Bartlett et al. (2017) translates to a proportional term related to matrix norms. Comparing this bound in a concrete case shows its relation to the network's convolutional layers. In a neural network setting, when the initialization is close to the identity and parameter changes are on a similar scale, a more general effect is observed. The network's input is a d \u00d7 d \u00d7 c tensor with Euclidean norm at most \u03c7, and the output is an m-dimensional vector. This scenario is independent of d and grows more slowly with \u03bb, c, and L. The specific case illustrates a broader effect that holds true in similar situations. The network for predicting a one-hot encoding of an m-class classification problem consists of convolutional and fully connected layers. The convolutional layers include a convolution with a kernel, non-linearity, and optional pooling. The fully connected layers have weight matrices. The functions computed by CNNs are within a certain distance of the parameters. Theorem 3.1 provides a general bound for training sets drawn from a joint probability distribution over certain parameters. The proof uses a specific parameterization method and discusses the effect of changing a single layer in the network. The proof of Theorem 3.1 involves transforming parameters in a neural network one layer at a time, bounding the distance traversed for each layer replacement using Lemmas 3.2 and 3.3, and applying the triangle inequality to derive the general bound. The mapping \u03c6 from the ball centered at \u0398 0 to F \u03b2,\u03bd is shown to be Lipschitz, completing the proof. Lemma 2.6 implies the mapping is Lipschitz. Theorem 3.1 applies to fully connected networks. The generalization gap is proportional to DL + D L log(\u03bb) + log(1/\u03b4) \u221a n. A 10-layer all-convolutional model was trained on CIFAR-10 dataset similar to VGG architecture with dropout regularization. The generalization gap is analyzed by scaling up the number of network parameters while keeping other elements fixed. Results show that the generalization gap increases with the number of parameters, but remains almost flat with increasing overparametrization. This phenomenon is consistent with the role of overparametrization on generalization. Increasing the number of network parameters leads to a decrease in the generalization gap, which remains almost flat with overparametrization. The fluctuations in training neural networks are due to its unstable nature. Lemma 2.3 and covering numbers are defined for metric spaces, with a standard bound from Vapnik-Chervonenkis theory provided as an example. Lemma A.3 states that for any set of functions G from a domain Z to [0, M], if certain conditions are met, then with high probability, the functions in G will be close to their expected values when sampled independently. Lemma A.4 provides a similar result with slightly different conditions. Lemma A.5 provides a bound for small B by combining Talagrand's Lemma with the standard bound on Rademacher complexity. It states that for an arbitrary set of functions G from a domain Z to [0, M], if certain conditions are met, then with high probability, the functions in G will be close to their expected values when sampled independently. This bound is useful for Lipschitz-parameterized classes. Definition A.6 introduces the concept of packing in a metric space. Lemma A.7 provides a result for metric spaces regarding the size of subsets with pairwise distances greater than a given value. Lemma A.8 discusses covering a ball in R^d with smaller balls, providing a specific method for achieving this. Lemma A.7 states that for a ball B of radius r in R^d, the /2-balls centered at the members of any -packing of B are disjoint. Applying Lemma A.8 and other Lemmas completes the proof. In this section, indices are numbered from 0, and \u03c9 is defined as exp(2\u03c0i/d). The tensor J is padded to a d \u00d7 d \u00d7 c \u00d7 c tensor. The lemma from Sedghi et al. (2018) is used to show that || op(J)|| 2 = ck 2. The network is shown to be \u03bb-Lipschitz, and functions g up and g down are defined for the network's inputs and convolution outputs. The network is \u03bb-Lipschitz, and functions g up and g down are defined for the network's inputs and convolution outputs, leading to the conclusion that |(f\u0398(x), y) - (f\u0398(x), y)| \u2264 \u03bb|f\u0398(x) - f\u0398(x)|. The network is \u03bb-Lipschitz, with functions g up and g down defined for inputs and convolution outputs. The proof shows that u \u2264 \u03c7 due to non-expansive operations, completing the proof by maximizing the expression subject to certain constraints."
}