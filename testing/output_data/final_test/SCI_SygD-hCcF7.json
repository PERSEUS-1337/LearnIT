{
    "title": "SygD-hCcF7",
    "content": "Most deep learning models rely on high-dimensional representations for good performance, but these are hard to interpret and prone to overfitting. A dimension reduction framework based on soft probabilistic interpretation of deep models improves visualization accuracy and generalization in zero-shot learning. This method also provides a finite sample error upper bound guarantee. Dimensionality reduction is crucial for enhancing classification performance, computational efficiency, and visualization in machine learning tasks. In the context of visualization, high-dimensional representations are often converted to two or three-dimensional representations for better understanding. Deep neural networks generate complex representations that are difficult to interpret, requiring techniques like PCA or t-SNE for visualization. However, these techniques do not fully utilize the soft probabilistic interpretations of deep models, such as softmax regression. In this paper, the authors experimentally demonstrate that soft probability representations learned by neural networks reveal key structure about the model. They propose a dimensionality reduction framework to visualize probability representations in a low-dimensional space, improving generalization. This is particularly useful in zero-shot learning scenarios where deep learning models often over-fit to training categories. The paper introduces a dimensionality reduction approach that utilizes soft probability representations from a high-dimensional pre-trained model to train a lower-dimensional student model. This method aims to improve generalization by considering inter-class similarities and is beneficial for zero-shot learning scenarios. The proposed method, Dimensionality Reduction of Probabilistic Representations (DRPR), utilizes soft target probabilistic representations from a pre-trained model to improve generalization performance in zero-shot learning. The approach aims to learn a low-dimensional representation that reflects the learned semantic structure better than standard visualization methods. The method also provides a theoretical analysis of its statistical properties and guarantees a finite sample error upper bound. The DRPR method utilizes soft target probabilistic representations from a pre-trained model to improve generalization in zero-shot learning. It aims to learn a low-dimensional space reflecting semantic relationships between categories. The algorithm for soft clustering in the low-dimensional space is detailed in Banerjee et al. (2005). The goal is to partition examples into soft clusters with corresponding probability densities. The BSCP involves learning maximum likelihood parameters for a mixture model to partition examples into clusters using the EM algorithm. The density decreases as the divergence between the example and center increases. The BSCP aims to maximize the likelihood parameter estimation for mixture models using the EM algorithm. It involves partitioning examples into clusters based on a soft assignment matrix. The local maximum condition is met when the algorithm converges to a local maximum of the likelihood parameter. The E-step of the EM algorithm computes p(Cc|fi) = \u0176ic when parameters F and \u0393 are given. The M-step, corresponding to Eq. (2), is computationally expensive for some exponential family distributions. Optimal conditions do not depend on the function b used for pc(fi), and soft clustering on a fixed representation F is explained in Section 2.1. Learning F to match soft clustering scores predicted by BSCP involves target probability representations y_i \u2208 [0, 1]^k concatenated into a matrix. DRPR learns F \u2208 V^n to match the target matrix. DRPR learns the representation of F in V^n such that the soft assignment matrix obtained from applying the BSCP on F is close to Y. The prediction function \u03c8(F, M, \u03c0) = \u03a8 \u2208 Y^n\u00d7k is defined, and the optimization problem aims to make the predicted assignment matrix similar to Y. The optimal values of M and \u03c0 depend on \u03a8, forming variables in the dimensionality reduction problem. The function \u2206 n (\u03a8, Y ) is an empirical discrepancy loss between predicted and target assignment matrices, formulated as average KL-divergence. The discrepancy loss is independent of Bregman divergence choice and the number of classes impacts clusters in low-dimensional space. Each class is represented by a cluster prototype. The target matrix Y contains probability scores from a pre-trained neural network. The goal is to learn F, M, and \u03c0 to reach optimality conditions and make \u03a8 similar to Y. DRPR can be used for visualization with target matrix Y provided. The input includes training examples in X and their target probability scores, nonlinear mapping g \u03b8, and number of iterations. The algorithm updates parameters \u03b8 using gradient descent iterations to compute optimal values for M and \u03c0 based on F and Y. The learning algorithm modifies matrix F at each iteration, with priors \u03c0 c being adjusted accordingly. The learning algorithm updates parameters \u03b8 using gradient descent iterations to compute optimal values for M and \u03c0 based on F and Y. DRPR can improve zero-shot learning generalization by preventing overfitting to training categories. In zero-shot learning, F concatenates image representations while M concatenates category representations extracted from text. Both F and M are computed by concatenating outputs of distinct neural networks. In experiments, the algorithm uses squared Euclidean distance for divergence calculation. Convergence and scalability are achieved by optimizing one variable using gradient descent, without multiple EM algorithm iterations. Our approach in DRPR utilizes the optimal properties of BSCP to efficiently minimize the optimization problem, resulting in linear complexity in n and k. Compared to t-SNE, our method is scalable and completes visualization experiments in under 5 minutes for 10^5 iterations. The algorithm generalizes to various Bregman divergences and simplifies the optimization problem by assuming equal priors and a fixed matrix M in certain tasks like zero-shot learning. The paper introduces a dimensionality reduction method for zero-shot learning tasks. The gradient of the algorithm depends on target scores and predicted responsibilities to move the vector closer to centroids while separating it from others. Theorem 1 provides an upper bound guarantee on the quality of the minimizer of the empirical discrepancy. The method aims to represent relations in a dataset efficiently. The paper introduces a dimensionality reduction method for zero-shot learning tasks that represents dataset relations with a probabilistic interpretation. It focuses on metric learning approaches for hard clustering problems, optimizing regression problems and using exponential families for probability scores. The paper introduces a dimensionality reduction method for zero-shot learning tasks using metric learning approaches for hard clustering problems. It optimizes regression problems and utilizes exponential families for probability scores. The approach generalizes previous methods to soft clustering without added complexity. The paper introduces a dimensionality reduction method for zero-shot learning tasks using metric learning approaches for soft clustering without added complexity. It optimizes regression problems and utilizes exponential families for probability scores, allowing for a representation with different dimensionality from the number of training categories. The approach in the current chunk utilizes probability scores from a pre-trained model for dimensionality reduction in zero-shot learning. It helps visualize examples that the teacher model struggles to distinguish, preserving inter-cluster distances unlike t-SNE. Our approach utilizes probability scores from a pre-trained model for dimensionality reduction in zero-shot learning, preserving inter-cluster distances unlike t-SNE. This method is evaluated in experiments focusing on low-dimensional representations for visualization and improving performance on novel categories using probability scores as supervision. DRPR can be learned with neural networks and compared to t-SNE on a toy dataset with 6 clusters. Soft clustering probability scores are generated in 3D space using Gaussian models. DRPR, a method learned with neural networks, is compared to t-SNE on a toy dataset with 6 clusters. Soft clustering probability scores are generated in 3D space using Gaussian models. In the 2D representation, DRPR preserves the global structure better than t-SNE and maintains relative distances between clusters more accurately. t-SNE may not provide meaningful distances between clusters as it focuses on local neighborhoods rather than global similarities. In comparing DRPR to t-SNE on a toy dataset with 6 clusters, t-SNE's use of probability scores leads to bad artifacts in visualizations. DRPR, on the other hand, preserves global structure better and maintains accurate relative distances between clusters. The metric evaluates how images with similar probability scores are close to each other in the learned low-dimensional space. CDPR measures how well inter-cluster distances are preserved in the representation. The approach is evaluated on various datasets including MNIST, STL, and CIFAR 10. The study evaluates the performance of DRPR on datasets like MNIST, STL, CIFAR 10, and CIFAR 100 using pre-trained models optimized for cross entropy. The goal is to visualize teacher representations in 2D using probability scores as targets. DRPR outperforms standard visualization techniques like t-SNE, ISOMAP, LLE, and PCA. DRPR outperforms dimensionality reduction baselines in preserving inter-cluster distances and similarity of probability-based representations. It utilizes as much supervision as unsupervised visualization methods. Qualitative results show spiked groups in DRPR representations for high confidence examples. Visualizations for MNIST and STL are illustrated in FIG6, while CIFAR 10 and 100 visualizations are in supplementary material. The DRPR model shows high confidence in assigning examples to categories based on their features. It reflects the semantic structure between categories, with visually grouped clusters on MNIST and STL datasets. Categories with similar features are placed close together, such as animals on the right and machines on the left. This differs from t-SNE representations, particularly in the distance between categories on the STL dataset. The DRPR model demonstrates a strong ability to assign examples to categories based on their features, showing a clear semantic structure between categories. In the STL dataset, the model struggles to differentiate between the ship and airplane clusters due to similar objects with blue backgrounds. This ambiguity is not present in the t-SNE representation. The DRPR visualization also reflects a semantical structure, grouping categories that belong to the same superclass together. For a detailed analysis of the CIFAR 100 and STL datasets, refer to the supplementary material. The quantitative results show that the representations of DRPR are meaningful, preserving cluster structure and observing ambiguities between categories. The goal is to learn mappings for image and category representations in a common space V, with image representations closer to their category representatives. Unseen categories are considered during testing. At test time, unseen categories are considered, and their representatives are obtained using a second mapping. Training datasets include Caltech-UCSD Birds (CUB) and Oxford Flowers-102 datasets. CUB has 11,788 bird images from 200 species categories, while Flowers has 8,189 flower images from 102 species categories. A GoogLeNet model with an output dimensionality of 1,024 is used to represent images. Text annotations are extracted for each category. The models use various techniques such as CNN-RNN, Prototypical Networks, and DS-SJE for image and text representation learning. They aim to generate representative vectors for images and categories, ensuring each image is more similar to its own category vector. The supervision/target for the model is generated from pre-trained models provided by BID1 and BID5. Prototypical Network BID5 represents images and categories with vectors, ensuring similarity within categories. Network BID5 represents images and categories with vectors, ensuring similarity within categories. The model trains two neural network models, g\u03b81 for images and g\u03b82 for categories, using representations learned by BID1. The target soft assignment matrix of DRPR is obtained following BID5's approach, achieving 58.3% accuracy on CUB. The study focuses on optimizing g \u03b81 and g \u03b82 alternately, using learned representations from BID1 and Snell et al. (2017). The models have the same architecture with MLP and tanh activation functions. Performance is evaluated on CUB and Flowers datasets, measuring classification accuracy across unseen classes. Our approach, utilizing DS-SJE and Prototypical Networks as supervision, achieves state-of-the-art results on CUB and Flowers datasets by significantly enhancing classification performance. It improves zero-shot learning performance by 2% to 4.3% across different classifiers. Additionally, reducing dimensionality, except for linear models, enhances generalization and improves zero-shot learning performance. The study shows that improving zero-shot learning performance can be achieved by using prediction scores as supervision for the model. Dimensionality reduction alone does not guarantee good generalization. Learning representations that group similar examples together acts as a semantic regularizer, leading to improved test accuracy. The study proposes a dimensionality reduction approach to improve generalization performance in zero-shot learning on challenging datasets. This approach can also be used for visualization and has real-world applications such as distillation when the teacher model is too large to store on a device with small memory. The study proposes a dimensionality reduction approach for zero-shot learning and distillation tasks. It aims to improve generalization performance by using a smaller student model when the teacher model is too large for devices with limited memory. Future work includes applying this approach to standard classification tasks. Funding was provided by Samsung, IARPA, and the Canada CIFAR AI Chairs Program. The Canada CIFAR AI Chairs Program provides statistical guarantees for the algorithm presented in Section 2.2, showing that the minimizer of the empirical discrepancy is close to the minimizer of the true expected discrepancy measure. The setup involves independent and identically distributed data points mapped by a fixed function provided by the teacher. The function space G maps points in X to a k-dimensional simplex, providing target probability distributions. The goal is to find a function g in G and define a k-dimensional probability distribution \u03c8 g (x). The empirical cluster centre is defined as \u015d. The priors \u03c0 c are assumed to be exact for simplicity of analysis. The student aims to find a function g in G that minimizes the distorted empirical discrepancy, defined based on the KL divergence between \u03c8 and \u03c6. The algorithm evaluates g based on its performance on new points x \u2208 X using the expected KL divergence w.r.t. distribution \u03bd. The output of the algorithm is the minimizer \u011d of the empirical discrepancy. The output of Algorithm 1 is the minimizer \u011d of \u2206(\u03c8, \u03c6). We also define the minimizer of the discrepancy as g*. Our assumptions include i.i.d. samples drawn from \u03bd(X). Statistical guarantees can be provided for learning algorithms under various mixing conditions. Statistical guarantees for learning algorithms can be provided under various mixing conditions. Assumption A3 (Teacher) states that the output of the teacher is a probability distribution, ensuring the algorithm receives well-defined inputs. The assumption also requires that the prior probability for each class is bounded away from zero. The assumption for statistical guarantees in learning algorithms requires prior probability for each cluster to be bounded away from zero. Assumptions about the function space complexity are made using covering number as a characterizer. The covering number at resolution \u03b5 is the minimum number of balls with radius \u03b5 needed to cover the space according to a specific metric. The text discusses a linear function approximator with a constraint on its magnitude. It defines a norm for the function space G using a mixed-norm approach. The covering number of G is characterized by this norm, showing faster growth compared to linear models. This behavior is suitable for capturing the complexity of large function spaces like Sobolev space and reproducing kernel Hilbert spaces. The use of a mixed-norm with a supremum norm is considered conservative. The text discusses defining a norm for the function space G using a mixed-norm approach and the implications for the covering number of G. It introduces the pointwise loss function and defines various functions and integrals. The main theoretical result states conditions under which a certain function \u011d satisfies a given inequality with high probability. The theorem provides a finite sample error upper bound on the true discrepancy of \u011d in relation to the function space G, number of samples n, and other properties. The estimation error decreases with more training data, showing a n^-1/2 dependence on the number of samples. The upper bound increases with the range L of the function space G. The upper bound on the true discrepancy of \u011d in relation to function space G, number of samples n, and other properties increases with the range L of the function space G, dimension d, and number of clusters k. The distorted empirical discrepancy shows a linear dependence on k in the upper bound. The proof relates \u2206(\u011d) to \u2206(g * ) and the distortion of the empirical loss. The upper bound on the true discrepancy of \u011d in relation to function space G, number of samples n, and other properties increases with the range L of the function space G, dimension d, and number of clusters k. The distorted empirical discrepancy shows a linear dependence on k in the upper bound. To upper bound the supremum of the empirical process and distortion caused by using\u03c8 in minimizing \u2206 n instead of \u03c8, we use Lemma 7 to relate the supremum of the empirical process to the Rademacher complexity of L. This lemma provides an upper bound on l(x) and DISPLAYFORM22, leading to the application of Proposition 4 to obtain a bounded l(x; g). The text discusses upper bounding the true discrepancy of a function space G, based on properties like range L, dimension d, and number of clusters k. By using Lemmas and Propositions, the text shows how to relate Rademacher complexity to covering numbers, ultimately obtaining bounded empirical discrepancies with high probability. The text introduces technical tools needed to prove Theorem 1, including Lipschitz constants and upper bounds on function magnitudes. These tools are crucial in relating the covering number of function spaces to the true discrepancy of G, leading to bounded empirical discrepancies with high probability. The text introduces technical tools such as Lipschitz properties and loss functions to prove Theorem 1, crucial for relating function spaces' covering numbers to the true discrepancy of G. The text discusses the Lipschitz property of a function \u03c1(u) to bound a term in the right-hand side of an equation, utilizing derivatives and probability distributions. It also applies Taylor's theorem and H\u00f6lder inequality to analyze the function's behavior. The text discusses the Lipschitz property of a function \u03c1(u) to bound a term in the right-hand side of an equation, utilizing derivatives and probability distributions. It also applies Taylor's theorem and H\u00f6lder inequality to analyze the function's behavior. For some \u0169 = (1 \u2212 \u03bb)u + \u03bbu with 0 \u2264 \u03bb \u2264 1, by H\u00f6lder inequality, for any H\u00f6lder conjugate where max over u \u2264 \u0169 \u2264 u should be understood as the maximum over the line segment between u and u. In particular, we used the fact that qc(u) is a probability distribution and its sum is equal to 1, for any choice of \u0169. The lemma relates the covering number of the function space L to the covering number of the function space G. The text discusses the Lipschitz property of a function \u03c1(u) to bound a term in the right-hand side of an equation, utilizing derivatives and probability distributions. It also applies Taylor's theorem and H\u00f6lder inequality to analyze the function's behavior. The proposition upper bounds the magnitude of l(x; f ) when certain assumptions hold. The terms on the right-hand side of the equation are analyzed, showing how they are bounded by various properties of the functions involved. The section provides an upper bound on the distortion of the empirical discrepancy, with a proposition stating that under certain assumptions, there exists a constant such that the distortion is bounded. The text also discusses the upper bounds of certain functions and their properties. The section presents a proposition that upper bounds the 2 distance between cluster centers under certain assumptions. It shows that with a fixed \u03b4 > 0, there exists a constant c1 > 0 such that the upper bound holds with probability at least 1 - \u03b4. The proof involves decomposing the difference between cluster centers and providing upper bounds for specific terms. The text presents a probabilistic upper bound on a function with fixed bounds using Hoeffding's inequality. It aims to upper bound the supremum of a term by defining a function and a function space. The text discusses using Hoeffding's inequality to obtain a probabilistic upper bound on a function with fixed bounds. It involves relating the covering number of function spaces to control the expected risk. The text discusses using Hoeffding's inequality to obtain a probabilistic upper bound on a function with fixed bounds. It involves relating the covering number of function spaces to control the expected risk. An \u03b5-cover of G induces an \u03b5-cover of F s, leading to an upper bound with probability at least 1 - (\u03b4 1 + \u03b4 2) by setting \u03b4 1 = \u03b4 2 = \u03b4/2. Various auxiliary definitions and results are collected for convenience. The paper discusses function spaces and probability distributions in the domain X. It defines the empirical measure Pn and the Lp norms for measurable functions. The text also mentions the empirical norm fn and quotes the definition of it. The text discusses the covering number of a set of real-valued functions defined on X with respect to a probability measure \u03bdX. It defines the \u03b5-cover of F w.r.t. p,\u03bdX and the metric entropy of F. The empirical covering number of F w.r.t. the empirical norm p,x1:n is also introduced. The Rademacher complexity is a measure of the extent to which a function from a function space F can fit a noise sequence of length n. It is closely related to the behavior of the empirical process and appears in the analysis of the supremum of an empirical process after the symmetrization technique is applied. The Rademacher complexity measures how well a function from a function space F can fit a noise sequence of length n. Implementation details of experiments, model performance, and generalization to hard clustering are discussed in Sections C.1 to C.4. The method was coded in PyTorch and experiments were conducted on a Nvidia GeForce GTX 1060. In PyTorch, experiments were conducted on a Nvidia GeForce GTX 1060 with 6GB of RAM. The gradient calculation in the mini-batch representation is automatic. The prediction function \u03c8(F, M, \u03c0) depends on F when centers are implicit. In zero-shot learning experiments, F and M are computed from different sources and optimized alternately. Mini-batch sizes for CUB and Flowers datasets are set at 421 images. In PyTorch experiments, mini-batch sizes of 421 and 735 were set for CUB and Flowers datasets, respectively. A temperature of 10 stabilized optimization, while a starting temperature of 50 was used for Bregman divergence. Temperature was decreased by 10% every 3000 epochs until training stopped at 10k epochs on CUB and 1k epochs on Flowers. Visualizations were done with two-dimensional embeddings instead of neural networks. We learn two-dimensional embeddings directly instead of using neural networks. The mini-batch size is the same as the test set size. We use the RMSprop optimizer with specific parameters and formulate the empirical discrepancy loss. The target assignment vector is defined for each test example. The initial temperature of the model is fixed at 1. The algorithm stops after 8000 iterations. Tuning t-SNE is also mentioned. The algorithm stops after 8000 iterations. Tuning t-SNE involves testing different scaling and perplexity ranges to find the best quantitative results. The model's architecture is based on the input and output dimensionality, with hyperparameter cross-validation on the validation set. Detailed accuracy performance is provided in TAB5, showing test performance on CUB using features from BID1 for supervision. TAB6 and Table 7 show validation and test performance on CUB with varying numbers of hidden layers and output dimensionality. The validation and test performance of the model on CUB and Flowers using features from BID5 as supervision is reported in Tables 7 and 8 respectively. Dimensionality reduction improves performance, with the optimal dimensionality being dataset specific. Increasing the number of hidden layers also enhances performance. Additionally, the model's ability to perform hard clustering similar to BID5 but with implicit centers is validated on MNIST using DRPR. The model achieves 98% accuracy for d=2 and 99% for d=3 on MNIST by assigning test examples to the closest centroid. DRPR is learned for hard clustering with implicit centers based on mini-batch matrix representation and target hard assignment matrix Y. The model achieves high accuracy on MNIST using supervised hard clustering. Visualization results show artifacts in the representation learned by t-SNE with different perplexity values. The use of logits is preferred due to these artifacts. In FIG12, t-SNE using KL or JS divergences produces worse representations than the original 3-dimensional ones. Comparing pairs of examples with t-SNE is less effective than our method that considers similarities between examples and clusters. Additional results in FIG13 and FIG14 show DRPR and t-SNE representations of CIFAR 10 and CIFAR 100, respectively. Our approach better defines groups with similar colors. The 2D representation of DRPR groups categories from the same superclass together more effectively than t-SNE. The model's representations for various datasets show clear objects on simple backgrounds towards the end of spikes. In a high-level analysis, classes in visualizations appear to be organized by their backgrounds. For example, deer and horses are close together due to their presence in green vegetation, while boats and planes are far apart with solid blue backgrounds. The ordering of classes makes sense, with planes near boats and birds, and trucks near cars and horses. In the MNIST visualization, written characters in spikes are easily recognizable, corresponding to examples where the model performs well. Ambiguous examples are located between spikes in the MNIST visualization, such as the characters 0 and 6."
}