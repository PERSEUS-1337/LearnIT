{
    "title": "ByGOuo0cYm",
    "content": "Few-Shot Learning aims to address the limitations of traditional machine learning by using meta-learning to learn an effective model with limited labeled data. The proposed novel meta-learning paradigm seeks to overcome the assumption of training and test tasks being drawn from the same distribution, allowing for more practical applications in the real world. In this paper, a novel meta-learning paradigm is proposed for few-shot learning to overcome domain shift between train and test tasks. The method is demonstrated through extensive experiments. Few-Shot Learning aims to learn a prediction model from limited labelled data, with a small number of examples for each class. This topic has gained significant interest in recent years, with modern techniques using meta-learning to solve the problem. The few-shot learning paradigm involves using a labeled training dataset to acquire prior knowledge for novel tasks. Episodes are simulated from the training data to tailor the prior knowledge for few-shot tasks. This knowledge is stored in a model and used for few-shot classification tasks drawn from unknown task distributions. The few-shot learning paradigm involves using a labeled training dataset to acquire prior knowledge for novel tasks. Existing meta-learning approaches assume train and test tasks are drawn from the same distribution, limiting real-world applications where ample labeled data is unavailable. This hinders the use of meta-learning strategies for scenarios with limited labeled data. It is challenging to train effective few-shot models without a large corpus of labeled data. One approach is to use domain adaptation techniques to address the domain shift between training and test data. However, existing approaches focus on single-task scenarios, while meta-learning deals with learning new tasks from test data with a domain shift at a task-level. In response to the challenge of domain shift in few-shot learning, a novel meta-learning paradigm called Meta-Learning with Domain Adaptation (MLDA) is proposed. Unlike existing approaches that only use training data, MLDA incorporates unlabeled instances from the few-shot test tasks' domain to address domain shift. The model is trained under the episodic-learning paradigm with the goal of being effective at few-shot tasks. The model aims to achieve two goals: excel in few-shot learning and be resilient to domain shift. Few-shot learning is improved by updating the model based on the loss from a given episode, while domain adaptation is achieved through an adversarial approach. The model is trained using Prototypical Networks BID28, where embeddings are generated for support and query instances to create prototypes for each class. The model utilizes Prototypical Networks for few-shot learning and domain adaptation through an adversarial approach. It updates the embedding function based on the loss from query instances, aiming for task-level domain adaptation rather than data-level adaptation. Previous approaches focused on obtaining invariant feature embeddings but were outperformed by GAN-based methods. To address this, a mapping function is used to style the embeddings. The model uses a mapping function to style training tasks for image translation and optimize it with a GAN loss. It meta-learns a few-shot model for tasks from a different distribution. Meta-learning approaches have been successful for few-shot learning by simulating tasks on labeled data. Some approaches in few-shot learning involve non-parametric principles and differentiable K-nearest neighbor solutions. These approaches aim to learn an embedding space tailored for effective K-nearest neighbor. Other meta-learning methods focus on quickly adapting a model in few gradient steps for a few-shot learning task. Optimization-based approaches aim to learn an initialization from training tasks that can be quickly adapted for novel few-shot tasks. Some approaches also use a \"memory\"-based approach, while others enhance meta-learning performance with additional information like unlabeled data for semi-supervised few-shot learning. Our approach of meta-learning with domain adaptation overcomes task-level domain shift to perform few-shot learning on tasks in a different domain. Domain adaptation, extensively studied for computer vision applications, involves exploiting labeled data in one domain to predict in another domain without labels. In the era of deep learning, approaches for domain adaptation aim to align feature distribution between domains using statistical measures and adversarial loss. Efforts have focused on learning a feature embedding that confuses a domain classifier, reconstructing instances in the target domain from the source data, and training a model in the source domain using a GAN. Approaches for domain adaptation in deep learning involve training a model in the source domain and using a GAN loss to embed the target domain to the same feature distribution. Other methods include image-to-image translation using GAN loss and cycle consistency for domain adaptation. These approaches focus on aligning feature distributions between domains and have shown application in domain adaptation tasks. Our approach focuses on task-level domain adaptation, allowing for solving new tasks with different label spaces. Previous efforts in few-shot and meta-learning domain adaptation have limitations in addressing novel tasks with different label spaces. Our approach focuses on task-level domain adaptation for solving new tasks with different label spaces. Unlike related works that consider different domains for training and testing data, we address scenarios where tasks are drawn from different distributions with limited labeled data for test tasks. During meta-training, the meta-learner samples a K-shot N-class classification task from a large labeled dataset. The meta-learner updates the model by backpropagating the gradient of the loss for updating the model. In the meta-testing phase, the resulting meta-learner is used to solve novel K-shot N-class classification tasks generated from an unknown task distribution. In meta-learning, the algorithm outputs class probabilities for task distribution \u03c4 test. Real-world scenarios may have different task distributions for training and testing tasks. Researchers may need to find labeled data from the same task distribution for few-shot classification, or adapt tasks from a different distribution for domain adaptation. In meta-learning, the goal is to learn a meta-learner that can utilize tasks from \u03c4 train to acquire a good prior for few-shot learning and overcome task-level domain shift to learn unobserved few-shot tasks from \u03c4 test. The setting involves tasks drawn from \u03c4 train for meta-learning and tasks drawn from a different distribution \u03c4 test for meta-testing. In meta-learning, the goal is to learn a feature extractor for few-shot learning on novel tasks while ensuring invariance to train and test task distributions. The proposed learning paradigm is Meta Learning with Domain Adaptation (MLDA), aiming to optimize two objectives simultaneously. The feature extractor F is trained for few-shot learning and domain adaptation, with G aiming for task-level domain invariance. G is trained using an adversarial loss for domain adaptation, and a mapping G is used for cyclic consistency. The overall objective function involves optimizing Lfs for few-shot learning. The proposed method focuses on few-shot learning under task-level domain shift using adversarial domain adaptation. It aims to achieve task-level domain invariance through the use of GAN loss and cycle-consistency loss. The overall objective function involves optimizing Lfs for few-shot learning, with the framework illustrated in FIG3. Our proposed paradigm for few-shot learning with domain adaptation follows the Prototypical Networks approach. It uses a feature extractor to compute embeddings for each instance and prototypes for each class. The Prototypical Network produces a probability distribution over classes for a given query instance based on the distance between embeddings. The few-shot loss is evaluated to optimize few-shot learning. In few-shot learning with domain adaptation, the feature extractor F is updated using the few-shot loss L f s on the query set S query i. To address domain shift, a mapping G is introduced to incorporate domain shift information. The GAN loss BID6 is used to learn the mapping G : X train \u2192 X test and its discriminator D. The objective is to generate instances similar to X test while distinguishing between translated instances and real samples. This leads to translating tasks from \u03c4 train to be similar to \u03c4 test. The domain adaptation objective involves using a cycle-consistency loss to prevent overfitting in adversarial networks. This loss aims to invert the mapping from the test domain to the train domain, ensuring that translated instances can be accurately reversed to their original form. The domain adaptation objective in the proposed framework includes L GAN, Lcycle, identity loss, reverse direction mapping, and cycle loss to improve performance in few-shot learning tasks. These enhancements are inspired by image-to-image translation techniques and aim to achieve task-level domain adaptation. The study focuses on domain adaptation for meta-learning tasks, utilizing unlabeled data in the target domain. Existing techniques in Meta-Learning and Domain Adaptation are adapted for this new problem setting. State-of-the-art domain adaptation baselines and meta-learning baselines are considered for training and prediction. The study combines Prototypical Networks BID28 with Gradient Reversal BID4 to create Meta-RevGrad, a method that addresses domain-shift issues in meta-learning tasks. By optimizing PN-loss and a domain-confusion loss, Meta-RevGrad aims to improve few-shot performance and domain adaptation. Different values of \u03bb are tested, with the best result reported. Additionally, the proposed method MLDA includes three variants: basic MLDA, MLDA+idt, and MLDA with equation 1 as the basis. MLDA has three variants: basic MLDA, MLDA+idt, and MLDA+idt+revMap. The code was implemented in PyTorch, following the model size and parameter settings from prior work. Optimizing the objective in equation 1 can be noisy, so a two-step optimization procedure is used. GAN-based training faces issues with lack of randomness in task generation. During GAN training, storing intermediate models and generating tasks using each model increases robustness, similar to Snapshot Ensembles. The Omniglot dataset is used for few-shot classification, with a new benchmark designed for domain-shift learning. Meta-learning approaches like Meta-RevGrad and MLDA are evaluated on Omniglot-M dataset for few-shot classification tasks. MLDA outperforms basic domain adaptation and meta-learning methods, showing superior performance in handling domain-shift challenges. MLDA outperforms baselines in handling domain-shift challenges for few-shot classification tasks on Omniglot-M dataset. Identity loss and reverse mappings boost performance, showing better task-level domain adaptation. Experiments on Office-Home Dataset with Clipart and Product domains also demonstrate promising results. In a study involving 65 classes, data was split into meta-train (25 classes), domain adaptation (20 classes), and meta-test (20 classes) sets. Images were resized to 84x84x3, and models were trained from scratch. Results for Clipart to Product and Product to Clipart tasks in 1-shot and 5-shot scenarios showed basic meta-learning approaches were poor, with Meta-RevGrad and MLDA offering improvements. Performance was consistent across tasks. In this paper, the study focused on Meta-Learning for few-shot learning under task-level domain shift. The existing meta-learning paradigm for few-shot learning assumes training and test tasks are from the same distribution, which may not be true in real-world applications. To address this, a meta-learning with domain adaptation paradigm was proposed, combining few-shot learning and task-level domain adaptation into a single meta-learner. The few-shot model used Prototypical Networks with an adversarial approach for task-level domain adaptation. The study focused on Meta-Learning for few-shot learning with task-level domain adaptation using Prototypical Networks. Experiments were conducted to validate the proposed ideas, including dataset construction details and model configuration for MLDA. Parameters for CycleGAN were adjusted to maintain image size consistency across different domains. The generative networks used in the experiments had specific configurations with two stride-2 convolutions and two fractionally-strided convolutions. The learning rate was set to 0.0002 and kept constant for 100 epochs. Different loss functions were applied for MLDA, MLDA+idt, and MLDA+idt+revMap settings. The Adam solver was used with a batch size of 1, and weights were initialized with a Gaussian distribution. The generative networks in the experiments had specific configurations with convolutions. For Prototypical Networks, hyperparameters were set following BID28. The embedding architecture included four convolutional blocks with 64-filter 3 \u00d7 3 convolutions, batch normalization, ReLU activation, and max-pooling. Different learning rates and iterations were used for Omniglot/Omniglotm and HomeOffice Clipart/Product experiments. Adam solver was used for optimization. In the experiments, Adam solver was used to optimize the networks. The kNN classification utilized squared Euclidean distance, chosen for its superior performance. Baselines were run with default hyperparameters, adjusted for image resolution and class numbers. Classification accuracy was computed by averaging over 600 randomly generated episodes from the Meta-test set."
}