{
    "title": "S1l6ITVKPS",
    "content": "The novel end-to-end neural network architecture presented aims to bridge the gap between deep learning and symbolic AI by learning propositional representations with a relational structure from raw pixel data. Evaluation on visual relational reasoning tasks shows that the architecture, when pre-trained on a curriculum of tasks, generates reusable representations that enhance subsequent learning on new tasks compared to baseline architectures. The PrediNet architecture utilizes shared and local weights across heads for improved performance. The PrediNet architecture aims to create representations with explicit structure for propositions, relations, and objects, unlike other architectures. It focuses on simple tasks to compare with existing models and uses small visual datasets for evaluation. The PrediNet architecture creates explicit structures for propositions, relations, and objects in high-dimensional data. It aims to improve data efficiency, generalization, and transfer learning by learning reusable representations. The architecture does not perform logical inference but extracts relational structures for downstream processing. The PrediNet architecture aims to transform high-dimensional data into propositional representations through a simple multi-layer perceptron output module. This approach helps in understanding the architecture before applying it to more complex problems. The foundation of symbolic AI lies in the idea that propositions are the building blocks of knowledge, which can be combined using logical connectives for inference processes like deduction. The PrediNet module transforms high-dimensional data into propositional representations useful for downstream processing. It consists of three stages: attention, binding, and evaluation, with k heads computing j relations between pairs of objects. The input is a matrix L from a convolutional neural network outputting n feature vectors. The PrediNet module uses a CNN to output feature vectors with xy coordinates. Each head computes relations using shared weights and dot-product attention. The outputs of all heads are concatenated to form the final output representing elementary propositions. The PrediNet module with k heads and j relations represents elementary propositions using vectors Q h 1 and Q h 2 in a 1D space defined by weight matrix W S. Equation 1 provides semantics for the final output vector R * mapping elements to logical formulas. The module can be extended to deliver an additional output in propositional form. The PrediNet architecture, with its pared-down vector form, allows for experimental investigation. The output can be used with various systems like Prolog interpreter, inductive logic programming, or statistical relational learning. It is important to first understand the architecture's properties and behavior before applying it to complex data. Small, simple datasets are needed to examine the architecture's operation and assess its design premises. The experimental goals are to test if PrediNet learns general-purpose representations and to investigate why. The Relations Game tasks involve classifying images based on relationships among objects in a grid. These tasks are designed to be simple and configurable, unlike existing datasets that include complexities like occlusion and shadows. The aim is to label images as True or False based on the relationships present. Relations Game tasks involve learning compound relations among objects drawn from training and held-out sets. The training set includes pentominoes with 25 colors, while the held-out sets contain hexominoes with 25 colors and squares with a striped pattern. Each task is tied to a specific relation, with countless possible combinations of objects. The study defined meaningful relations among objects and created labeled datasets with 50% positive and 50% negative examples. These relations include 'between' for three objects in a line, 'occurs' for matching objects in different rows, and 'same' for identical objects. Negative examples were balanced to include tricky images with similar colors but different shapes or vice versa. The architecture considered in this paper includes a CNN input layer, a central module (PrediNet or baseline), and an output MLP. Each CNN feature vector has xy coordinates and a task identifier. The PrediNet was compared to MLP and other baselines for evaluation. The final output of the MLP is a one-hot label for True or False. The PrediNet and baselines have the same top-level schematic with differences in the central module. MLP1 has a single fully-connected layer, MLP2 has two layers, RN computes all possible pairs of feature vectors, and MHA has multiple heads generating mappings. PrediNet used has 32 heads and 16 relations. The PrediNet utilized 32 heads and 16 relations in its structure. Experiments were conducted using stochastic gradient descent and results were averaged over 10 runs. The network was pre-trained on a curriculum of tasks, followed by retraining on new tasks with only the output MLP weights changing. The PrediNet was pre-trained on a curriculum of tasks, with only the output MLP weights changing for new tasks. Assessing transfer and reusability, the final performance on the target task after pre-training indicates how well the network has learned useful representations. The more different the target task is from the pre-training curriculum, the more impressive the network's ability to learn the target task. The PrediNet architecture was studied for data efficiency in a single-task Relations Game setting. Results on five tasks were summarized, including 'same', 'between', 'occurs', 'xoccurs', and 'colour/shape'. The 'colour/shape' task involves four labels and requires assigning one of the labels to two randomly placed objects in each image. The accuracy of each architecture on these tasks is shown in Table 1. The PrediNet architecture achieves over 90% accuracy on all tasks with both held-out object sets after 100,000 batches. It out-performs baselines by more than 10% on the 'xoccurs' task and by 25% or more on the 'colour/shape' task. Comparisons were made with baselines on re-usable representations using a pre-training curriculum, showing detailed findings for specific target tasks. The PrediNet architecture achieves over 90% accuracy on all tasks with held-out object sets after 100,000 batches, outperforming baselines by more than 10% on the 'xoccurs' task and by 25% or more on the 'colour/shape' task. The learning curves for each architecture at different stages of the experimental protocol show the rapid reusability of representations learned by both the MHA baseline and PrediNet. The PrediNet learns faster than baselines and achieves 90% accuracy, demonstrating its effectiveness in learning target tasks. The PrediNet architecture achieves high accuracy on various tasks, outperforming baselines significantly. The learning curves demonstrate the rapid reusability of learned representations. In a specific case, PrediNet performs well with column patterns compared to baselines, showing its effectiveness in learning target tasks. The PrediNet architecture shows high accuracy on tasks, outperforming baselines. It can learn orientation invariant representations, aiding transfer. Baselines struggle to learn reusable representations, while PrediNet achieves 85% accuracy. A PCA analysis compares PrediNet and MHA performance on a specific task. The PrediNet architecture demonstrates superior accuracy on tasks compared to baselines, achieving 85% accuracy and learning orientation invariant representations. Visualizations show that most attention focuses on single objects, with some heads attending to pairs or parts of objects. Some heads in the PrediNet architecture exhibit diffuse attention, enabled by a soft key-query matching mechanism. Principal component analysis (PCA) on output vectors reveals that differences in color and shape align along separate axes for certain heads. This contrasts with the MHA baseline, where heads do not individually cluster. The PrediNet architecture shows heads that cluster labels meaningfully, while the MHA baseline does not. PCA on output vectors does not produce interpretable results for other baselines. An ablation study demonstrates that PrediNet is more robust than MHA to pruning heads at test time. The PrediNet architecture demonstrates relational disentangling by generating explicitly relational representations through Prolog programs. This involves assigning symbolic identifiers to PrediNet attention masks and object ids, showcasing the network's ability to capture performance with just a few heads. The PrediNet architecture generates relational representations through Prolog programs by assigning symbolic identifiers to attention masks and object ids. Clustering is used to group attention masks before assigning identifiers. The output in Prolog form allows for deductive inference, showcasing the network's ability to capture relationships. The PrediNet architecture generates relational representations through Prolog programs by assigning symbolic identifiers to attention masks and object ids. It emphasizes the importance of good representations in AI and deep learning, advocating for feature sets that are invariant to irrelevant features. The novelty of PrediNet lies in incorporating architectural priors that favor disentangled representations at relational and propositional levels. The PrediNet architecture generates relational representations through Prolog programs by assigning symbolic identifiers to attention masks and object ids. It complements existing networks by producing explicitly relational, propositional structures, addressing the problem of acquiring structured representations. This work is motivated by the importance of curricula in lifelong learning for future RL agents to exhibit more general capabilities. Continual learning in future RL agents is crucial for exhibiting general intelligence, similar to human children. Curriculum pre-training has a long history in deep learning. Transfer learning is also essential for general intelligence and has gained recent attention. The PrediNet offers a unique perspective on curriculum learning and transfer, focusing on incremental accumulation of propositional knowledge. This requires a different architecture supporting the acquisition of relational representations. The PrediNet offers a unique perspective on curriculum learning and transfer, focusing on incremental accumulation of propositional knowledge. Asai's architecture differs from PrediNet in assuming input representation in symbolic form, while the present architecture learns what constitutes an object through the input CNN and PrediNet's attention mechanism. This neural network architecture supports predicate logic's powers of abstraction without hand-crafting representations, addressing the symbol grounding problem in AI. The PrediNet architecture emphasizes learning propositional knowledge incrementally and efficiently. It utilizes a unique approach to information flow organization, enhancing data efficiency, generalization, and transfer properties. The network's design limits interactions between information chunks, promoting independent representation learning. The PrediNet architecture focuses on acquiring propositional representations with potential for logic-based processes like deduction and reasoning. The architecture allows for recombination and re-use of information chunks, showing promise for novel tasks. Further exploration is needed to fully understand the architecture's capabilities. One potential direction for further research is to develop reinforcement learning agents using the PrediNet architecture, specifically focusing on model-based prediction for look-ahead and planning abilities. This approach aims to enhance RL agents with propositional, relational representations to improve data efficiency, generalization, and transfer properties. Expanding the Relations Game datasets into the temporal domain could serve as a precursor to implementing these advanced RL agents. The Relations Game datasets could be extended into the temporal domain to develop multi-task curricula for RL agents. Default hyperparameters for experiments are shown in Table S2. Dimensionality reduction analysis was performed on the central module outputs of each architecture trained on the 'colour / shape' task. PCA was used to visualize the representations produced by the networks. Dimensionality reduction analysis was conducted on the central module outputs of architectures trained on the 'colour / shape' task. PCA was used to visualize the representations, showing no clear clustering for the PrediNet and MHA models. However, some heads in the PrediNet architecture individually clustered different labels, with colour and shape projected along separate axes in certain heads. The clustering of objects with different colours and shapes was observed in certain heads of the PrediNet architecture. The clustering was preserved when new images were passed through the PrediNet. Lower resolution content masks were created to assess the attention of PrediNet heads to actual objects in the images. The attention masks of PrediNet heads were tested on 1000 images from the training set, showing some heads consistently attending to objects while others to a combination of object and background. The heads that clustered labels meaningfully were the ones where both attention masks attended to objects. Additionally, a similar analysis was done with a position mask, revealing different attention patterns. The attention masks of PrediNet heads were analyzed on 1000 training images, revealing some heads consistently attending to objects. The mean absolute values of per-head input weights showed heads focusing only on objects had higher weighting. The extent of attention masks in different heads varied in attending to specific locations in the image. The attention masks of PrediNet heads focus on specific locations in the image. Experimental results include variations in hyper-parameters and test accuracy curves for different object sets. Using Adam optimizer with a learning rate of 10^-4 yielded similar performance to SGD with a learning rate of 10^-2 in multi-task experiments. The number of heads and relations were also assessed. The experiments assessed the impact of the number of heads and relations on performance. Results show that more heads lead to better performance due to increased stability during training and richer propositional representation. The learning curves and results for different models are presented in figures and tables. In a study on multi-task curriculum training, different target/pre-training task combinations were tested on various architectures using SGD with a learning rate of 0.01. The experiments used the pentominoes object set for training and the 'stripes' object set for testing. The architectures included MLP1, MLP2, relation net (RN), multi-head attention (MHA), and PrediNet. Adam optimizer with a learning rate of 10^-4 was used for training. Figure S15 shows the reusability of representations learned with different target and pre-training tasks using the 'stripes' object set. All architectures were trained using Adam with a learning rate of 10^-4. Figure S16 demonstrates multi-task curriculum training with different task combinations and architectures using SGD with a learning rate of 0.01. Training was done with the pentominoes object set and testing with the 'stripes' object set. Increasing the number of heads for PrediNet improves stability during training and overall performance. Figure S18 also illustrates multi-task curriculum training. Figure S18 demonstrates multi-task curriculum training with different target/pre-training task combinations and architectures using SGD with a learning rate of 0.01. Training was conducted with the pentominoes object set and testing with the 'stripes' object set. Having fewer heads leads to a decrease in performance, even with an increased number of relations to maintain network size. Figure S19 shows the reusability of representations learned with various target and pre-training tasks using the 'stripes' object set."
}