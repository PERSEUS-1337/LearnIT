{
    "title": "B1eYGkBKDB",
    "content": "State-of-the-art neural machine translation methods use large parameters, but reducing computational costs without affecting performance has been a challenge. A quantization strategy tailored to the Transformer architecture is proposed in this work, achieving state-of-the-art results without loss in BLEU scores. The model can be further compressed by removing nodes in the encoder post-training without affecting translation quality. Neural networks for machine translation have shown impressive results, with a key element being the decoder's ability to attend to specific parts of the input. The Transformer network, based on self-attention, revolutionized machine translation and natural language processing tasks. However, the models require a large number of parameters, making inference on resource-limited hardware impractical. To address this issue, quantization methods can reduce computational burden by representing numerical values with fewer bits. In this work, a custom quantization strategy for the Transformer architecture is proposed, aiming to improve computational speed and enable deployment on constrained devices. The method involves applying quantization during the training process, resulting in state-of-the-art quantization results on translation tasks like WMT14 EN-FR and WMT14 EN-DE. The quantized models achieve equal or higher BLEU scores compared to full-precision models, without impairing translation quality. In this section, various quantization and pruning methods for compressing neural networks are reviewed. Different approaches such as combining pruning, quantization, weight sharing, and Huffman coding have been explored. Additionally, methods like using quantization with knowledge distillation for higher compression rates have been studied. Researchers have also shown that pruning a fully trained model and then retraining it can improve performance, along with gradually pruning in tandem with training. Some approaches involve pruning nodes instead of weights by applying a penalty in the loss on the \u03b3 parameters of batch normalization layers. In the context of compressing neural networks, various methods such as pruning, weight decay, and quantization have been explored. Different approaches have been proposed to minimize the number of loaded weight matrix chunks and improve efficiency in architectures like CNNs and RNNs. The quantization methodology chosen is uniform, ensuring a constant step size between quantized values. The uniform quantization scheme simplifies computations during inference by maintaining a constant step size between quantized values. Quantization intervals are defined by x min and x max, with values updated via exponential moving averages during training. During training, max variables are updated using an exponential moving average with a momentum of 0.9. The value k represents the bit precision, such as k = 8 in 8-bit quantization. Quantization is simulated by first quantizing and then rescaling to the original domain. The clamp function handles values outside the [x min , x max ] range by rounding to the nearest integer. The straight-through estimator is used during backpropagation, except for LayerNorm's denominator. Once training is complete, s and x min are frozen along with the weights. All operations that can provide computational speed gains at inference are quantized, including matrix multiplications with inputs and weights both being k-bit quantized. During training, max variables are updated using an exponential moving average with a momentum of 0.9. Quantization is applied to matrix multiplications with k-bit inputs and weights, divisions for second or higher rank tensors, and positional embeddings. Biases are excluded from quantization due to minimal computational efficiency gain. During training, quantization is applied to various components such as input embeddings, LayerNorm weights, activations, and attention mechanisms. Different parts of the network are quantized including input, softmax, ReLUs, and LayerNorm outputs. Subsets of tensors can have their own quantization parameters. This approach adds more scalars but has insignificant memory cost. The bucketing method is used for weight matrices and activations during quantization. It helps alleviate precision loss by allowing flexibility in fitting values into different domains. Zero values from padding, ReLU layers, and dropouts are ignored during quantization. Quantization is applied before dropout operations. Recent solutions for Transformer quantization include k-means quantization and binarization with two centroids for weights. Fan (2019) compares binary, 4 and 8-bit uniform quantization of Transformer weights. Quantizing only weights still requires full-precision operations. Quantization of Transformer weights has been explored using various methods such as 8-bit quantization and binarization. While some approaches focus on specific operations like MatMul, others aim to optimize parameters for each quantization range using KL divergence. However, none of these methods quantize the entire Transformer architecture, leading to suboptimal results in translation accuracy. Our method achieves optimal computational efficiency and translation quality by quantizing the entire Transformer architecture. Results are presented on machine translation tasks, ablation studies, and language model tasks using the base and big Transformer models. The models were evaluated on English-to-German and English-to-French translation datasets. The study evaluated models on WMT 2014/2017 English-to-German and English-to-French translation tasks, using beam search with a beam size of 4 and length penalty of 0.6. Results were compared with the original Transformer and other quantization methods. BLEU scores were computed on the test set using the checkpoint with the highest validation accuracy. In Table 2, the performance of the method on WMT14 EN-DE and WMT14 EN-FR is shown for a fixed number of training steps. The quantized models were trained about twice as slow as the baselines. Comparison was made with full-precision Transformers and other quantization approaches. BLEU scores were computed on the test set using the best validation accuracy checkpoint. Validation epochs were run every 100 training steps, and post-training quantization involved 20 trials with a variation of about 0.2 BLEU. The best results for big Transformer variants were achieved by not bucketing the Scaled Dot-Product Attention's output and summing the decoder's input embeddings with positional encoding. Fully quantizing the Transformer did not result in a loss of translation accuracy, possibly due to lower numerical precision acting as a regularization effect. All models use full-precision biases, resulting in different file sizes for base and big models. In the context of fully quantizing the Transformer models, the text discusses the impact of bucketing on model size and performance. It mentions that the increase in model size due to bucketing is minimal and worth it for improved translation. The text also highlights that 6-bit quantization may not always offer a significant compression advantage over 8-bit quantization, especially without dedicated hardware support. The comparison between bucketing and quantizing to 8-bit single operations of the Transformer is evaluated to understand the sensitivity to quantization. Quantizing the operation of a module for all Transformer layers in the WMT14 EN-FR translation task resulted in BLEU scores close to the full-precision baseline. However, underperforming elements like LayerNorm's numerator and denominator posed challenges. Quantizing only the denominator without bucketing led to poor results, suggesting that quantizing other network elements may be necessary for successful quantization. Validation was done on four 8-bit quantized base Transformers models, showing the need for careful design choices to improve computational efficiency. Our method aims to increase computational efficiency in Transformer inference by using a quantization scheme that only requires learning s and x min. Quantization can be applied during training or post-training, with results compared at different starting points. Models were evaluated on WMT14 EN-DE and WMT14 EN-FR tasks, with BLEU scores measured on the test set. From our observed results, quantizing the model early on seems preferable as it helps with regularization during training. Learning quantization parameters adds significant computational cost, but delaying quantization allows for more training steps in the same time frame. Post-quantization is quick and allows for easy iteration to find the best performing candidate. This strategy can be useful when training time is limited. To evaluate the generalization of our quantization scheme, we tested it on a language modeling benchmark using PyTorch's example on WikiText-2 and WikiText-103 corpus. Four Transformer models were trained, one full precision and three with quantization. Models had two Transformer encoder layers, with specific settings for embedding and hidden size, Multi-Head Attention, and other parameters. Training details can be found in the PyTorch example. The study evaluated a quantization strategy for the Transformer model on language modeling tasks using WikiText-2 and WikiText-103 datasets. Results showed higher BLEU scores compared to other quantization methods, with 8-bit quantization performing equally or better than full-precision in 36 out of 41 experiments. The focus was on maximizing computational efficiency while ensuring compatibility with various hardware types. The study evaluated a quantization strategy for the Transformer model on language modeling tasks, with 8-bit quantization performing equal or better than full-precision in 36 cases. The researchers plan to apply their method to other tasks and explore variations of the Transformer for further compression. They also proposed an additional compression method independent of quantization, which can be used together to compress the Transformer model further. The model is fully trained and quantized, and further compression is achieved by pruning useless nodes. Pruning nodes results in shrunken models by removing corresponding weights. Nodes between feed-forward network layers can be removed without affecting other components, making up a significant portion of the model's weights. In the case of the base Transformer, a large percentage of nodes can be removed without altering the network. In the fully trained and quantized model, further compression is achieved by pruning nodes in the feed-forward networks. The percentage of weights owned by these networks increases in the big Transformer model. Running estimates of maximum values for ReLU nodes are computed to identify nodes that can be safely pruned without impacting translation quality. After fully training and quantizing the model, compression is achieved by pruning nodes in the feed-forward networks based on running estimates of maximum values for ReLU nodes. Pruning is done without retraining the model, using a threshold of z = 0.025 to determine which nodes to prune. The approach is adaptive, with varying numbers of nodes pruned per layer, resulting in compression of the Transformer model without affecting BLEU scores. The method involves pruning nodes in the decoder based on a threshold to find the right ratio of nodes to prune per layer. Two comparison methods were used, one sorting nodes using L1-norm and the other using x max. Results varied per trial due to x max being a running estimate. The method only takes a few hundred training steps to perform, allowing for multiple trials. Evaluation on the validation set showed increased accuracy over non-pruned models, with perplexity and BLEU computed on the test set. Reported results were averaged over multiple trials, with BLEU either staying the same or decreasing slightly when accuracy did not improve."
}