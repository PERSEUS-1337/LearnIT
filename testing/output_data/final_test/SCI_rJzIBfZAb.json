{
    "title": "rJzIBfZAb",
    "content": "Recent work has shown that neural networks are susceptible to adversarial examples, which are inputs that closely resemble natural data but are misclassified by the network. To tackle this issue, the adversarial robustness of neural networks is examined through robust optimization. This approach offers a comprehensive view of previous research in this area and allows for the development of reliable methods for training and attacking neural networks. These methods provide concrete security guarantees against specific types of adversaries, leading to networks with enhanced resistance to various adversarial attacks. The focus is on achieving robustness against first-order adversaries as a fundamental security measure. Recent breakthroughs in computer vision and speech recognition are making trained classifiers crucial for security-critical systems like autonomous cars, face recognition, and malware detection. Ensuring resistance to adversarially chosen inputs is now a key design goal as recent work shows adversaries can manipulate models to produce incorrect outputs, especially in deep neural networks. This has sparked a growing body of research on adversarial robustness in computer vision. The growing body of work in computer vision highlights the challenge of small changes in input images fooling neural networks. This raises the question of how to create models robust to adversarial inputs. Various defense mechanisms have been proposed, such as defensive distillation and feature squeezing, but more work is needed in this area. While existing defense mechanisms are important first steps in exploring adversarial robustness, they lack a comprehensive understanding of the guarantees they provide. Recent research has shown that many defenses can be bypassed by adaptive adversaries. This paper examines neural network robustness using robust optimization, aiming to achieve precise security guarantees against a broad class of attacks. The paper explores neural network robustness through robust optimization to provide security guarantees against a broad class of attacks. It shows that the optimization problem is tractable, allowing first-order methods to reliably solve it. The paper discusses the use of projected gradient descent (PGD) as a strong first-order adversary for adversarial attacks on neural networks. It emphasizes the importance of network architecture in achieving robustness against such attacks, highlighting the need for larger model capacity. Training networks on MNIST and CIFAR10 datasets to be robust against various adversarial attacks is also explored. The study focuses on developing robust neural networks on MNIST and CIFAR10 datasets against a wide range of adversarial attacks. Their approach, based on optimizing a saddle point formulation, achieves high accuracy levels, with the MNIST model surpassing 89% against strong adversaries and even white box attacks. The CIFAR10 model achieves 46% accuracy against the same adversary. Additionally, their networks show high accuracy levels against black box attacks, with MNIST over 95% and CIFAR10 over 64%. These results indicate the possibility of secure neural networks. The findings suggest that secure neural networks are achievable. An open challenge was issued for attacks on MNIST and CIFAR10 networks, with limited success in breaking the defenses. The proposed defense methods increased robustness against attacks. The discussion focuses on an optimization view of adversarial robustness. The text discusses the need for robust models in the face of adversarial attacks, highlighting the limitations of empirical risk minimization. It references a standard classification task with a data distribution and loss function, aiming to find model parameters that minimize risk. The discussion also mentions the challenge of crafting adversarial examples and the importance of developing defenses against such attacks. To train robust models against adversarial attacks, it is essential to augment the ERM paradigm by defining a threat model and specifying allowed perturbations for each data point. This approach focuses on achieving concrete guarantees for adversarial robustness rather than directly improving resistance to specific attacks. In image classification, capturing perceptual similarity between images is crucial for robustness against adversarial attacks. The definition of population risk is modified to incorporate an adversary that perturbs the input before computing the loss. This leads to a saddle point problem that is central to the study of robust optimization. The formulation of the adversarial loss in the context of robust optimization provides a unifying perspective on prior work. It involves an inner maximization problem to find adversarial versions of data points and an outer minimization problem to train a robust classifier. The saddle point problem clarifies the process. Our paper investigates the structure of the saddle point problem in deep neural networks, aiming to produce models resistant to adversarial attacks. Attaining small adversarial loss ensures no allowed attack will fool the network, providing robustness. Our paper focuses on producing robust models in deep neural networks by investigating the saddle point problem. This approach aims to prevent adversarial attacks by minimizing the adversarial loss, making it impossible for any perturbations to fool the network. The use of Stochastic Gradient Descent (SGD) is crucial in solving this optimization problem, but computing gradients for the outer minimization problem requires a different approach due to the nature of the adversarial loss function. Danskin's theorem states that gradients at maximizers of the inner problem correspond to descent directions for the saddle point problem. Finding a maximizer of the inner maximization problem corresponds to finding an attack on the neural network, allowing known attacks like the Fast Gradient Sign Method (FGSM) to be used as inner maximization algorithms. The inner maximization problem in adversarial attacks is well-behaved, with a tractable structure of local maxima, as demonstrated in practice. This structure allows for effective exploration of the non-concave loss landscape. The inner maximization problem in adversarial attacks has a tractable structure of local maxima, pointing towards projected gradient descent as the \"ultimate\" first-order adversary. Despite deviations from Danskin's theorem assumptions, experiments show that using gradients to optimize the problem is effective. Applying SGD with the gradient of the loss at adversarial examples consistently reduces the loss during training, optimizing the saddle point formulation and training robust classifiers. Solving Equation (2.1) is not enough for robust classification; the final loss against adversarial examples must be small for classifier performance guarantees. Network capacity is crucial for robustness, requiring a larger network than for high accuracy on natural examples. Our approach trains robust classifiers using projected gradient descent on MNIST and CIFAR10 datasets. The experiments on MNIST and CIFAR10 datasets involve training against a projected gradient descent adversary, which efficiently produces examples of maximal loss. The training procedure shows a steady decrease in adversarial loss, indicating successful optimization. The sharp drops in the CIFAR10 plot correspond to decreases in the learning rate. The experiments involve training against various adversaries, including white-box attacks with PGD, black-box attacks from different networks, and different convolution architectures. The trained models are evaluated for robustness, with results shown in TAB0 for CIFAR10. The experiments involve training against various adversaries using PGD with a step size of 0.01. The network consists of two convolutional layers with 32 and 64 filters, followed by max-pooling and a fully connected layer. The model reaches 99.2% accuracy on natural examples but drops to 6.4% accuracy on perturbed examples. Investigation into the learned parameters for adversarial robustness is presented in Appendix E. For the CIFAR10 dataset, two architectures were used - the original Resnet and its 10\u00d7 wider variant. The network was trained against a PGD adversary with varying steps and \u03b5 values. Results show significant adversarial robustness but room for improvement. Further progress is sought through understanding adversarial training and complementary techniques for robust models. Additional experiments were conducted to evaluate resistance against different attacks. The models were tested against different types of attacks, including \u221e-bounded and 2-norm-bounded attacks. The MNIST model showed resistance to perturbations, even reaching 100% adversarial accuracy on the training set. Adversarial examples were provided in the appendix, showing perturbations that could confuse humans. The landscape of the optimization problem is tractable, as shown by the performance of adversarially trained networks against PGD adversaries. Adversarial training incurs a computational overhead compared to standard training. Adversarial training with a k-step PGD adversary increases running time by a factor of (k + 1). Future research may address this drawback. Loss landscape exploration shows that local maxima found by PGD have similar loss values, indicating robustness against first-order adversaries. The experimental evidence in Section 4 validates the hypothesis that training against PGD adversaries makes a network robust against various attacks. While better local maxima may exist, they are hard to find with first-order methods. Incorporating the adversary's computational power into the threat model is akin to a polynomially bounded adversary in cryptography. The optimization-based view on the power of the adversary in machine learning suggests that attacks relying on first-order information are universal for deep learning. Training networks to be robust against PGD adversaries can provide robustness against a wide range of attacks, leading towards machine learning models with guaranteed robustness. In the context of transfer attacks, the adversary lacks direct access to the target network and relies on less specific information. Even in zero-order attacks, the network gradient can be estimated using finite differences. Increasing network capacity and training against stronger adversaries improves resistance against transfer attacks. In the context of adversarial examples, robust optimization has been studied for decades. The min-max optimization formulation first appeared in related papers, focusing on stronger adversaries than previous works. Recent work on adversarial training on ImageNet has shown that training against iterative adversaries results in a more robust model compared to training only against single-step attacks like FGSM. This highlights the importance of considering a wider range of possible attacks during training to enhance the classifier's resilience. Recent experiments and research suggest that while adversarial training against black-box attacks can increase network robustness, it may not be effective against white-box attacks like PGD adversaries. However, evidence shows that deep neural networks can be made resistant to adversarial attacks through reliable training methods, thanks to the regular structure of the optimization task. Our findings show hope for adversarially robust deep learning models, especially on the MNIST dataset where our networks are very robust. While our experiments on CIFAR10 have not reached the same level of performance yet, our techniques have already significantly increased the network's robustness. Further exploration in this direction could lead to adversarially robust networks for this dataset. The inner problem of maximizing a non-concave function has been explored using projected gradient descent (PGD) on MNIST and CIFAR10 models. Restarting PGD from various points around data points reveals surprising insights into the loss landscape. Our experiments show that the inner problem of maximizing a non-concave function is tractable with first-order methods. Local maxima within x i + S have well-concentrated loss values, similar to the belief that neural networks have many local minima with similar values. Adversarial loss plateaus quickly with projected \u221e gradient descent, especially on CIFAR10. The final loss values on adversarially trained networks are significantly smaller than on naturally trained networks. Investigating the concentration of maxima, the loss of the final iterate follows a well-concentrated distribution without extreme outliers. Distances between maxima are close to the expected distance between random points, and angles are close to 90 degrees. The loss is convex along the line segment between local maxima, with a higher value than a random point. The distribution of maxima in adversarial examples suggests that the subspace view may not fully capture the richness of attacks. The architectural capacity of the classifier plays a significant role in handling adversarial examples, requiring a stronger classifier to navigate the more complex decision boundaries. The architectural capacity of the classifier is crucial for handling adversarial examples with complex decision boundaries. Increasing the network size improves robustness against strong adversaries, as shown in experiments with the MNIST dataset using a convolutional network. The initial network has 2 convolutional layers with 2 and 4 filters, followed by a fully connected hidden layer with 64 units. Data augmentation was done for the CIFAR10 dataset using Resnet model BID10 TFM (2017) with wider layers. The network achieved 95.2% accuracy with natural examples and 8 adversarial examples. Increasing network capacity improves robustness against adversaries. Increasing network capacity improves robustness against adversaries, especially for smaller \u03b5 adversarial examples. However, training with FGSM adversaries can lead to label leaking and overfitting, resulting in poor performance on natural examples and no robustness against PGD adversaries. Increasing network capacity improves robustness against adversaries, especially for smaller \u03b5 adversarial examples. Training with FGSM adversaries can lead to label leaking and overfitting, resulting in poor performance on natural examples and no robustness against PGD adversaries. Weak models may fail to learn non-trivial classifiers, converging to always predict a fixed class instead of learning accurate classifiers through natural training. The small capacity of the network sacrifices performance on natural examples to provide robustness against adversarial inputs, with the value of the saddle point problem decreasing as capacity increases. Fixing an adversary model and training against it shows a decrease in the value of the model's ability to fit adversarial examples as capacity increases, indicating a trade-off between capacity, adversarial robustness, and transferability. Increasing network capacity improves robustness against adversaries, especially for smaller \u03b5 adversarial examples. Training with FGSM adversaries can lead to label leaking and overfitting, resulting in poor performance on natural examples and no robustness against PGD adversaries. Weak models may fail to learn non-trivial classifiers, converging to always predict a fixed class instead of learning accurate classifiers through natural training. The small capacity of the network sacrifices performance on natural examples to provide robustness against adversarial inputs. The correlation between gradients from the source and the transfer network becomes less significant as capacity increases. The goal is to minimize the value of the saddle point problem, computed using sampled input points. The text discusses utilizing the classical theorem of Danskin to compute a descent direction for \u03b8. It also shows how natural and adversarial accuracy changes with network capacity in different training regimes. The final plot/table displays the cross-entropy loss on adversarial examples. Theorem C.1 by Danskin states conditions for a max-function to be locally Lipschitz continuous and directionally differentiable. The max-function is differentiable at \u03b8 if \u03b4 * (\u03b8) = {\u03b4 * \u03b8} is a singleton. Due to non-concavity, global maximizers cannot be computed, leading to convergence to local maxima with PGD. By considering a subset S where the local maximum is global, the gradient corresponds to a descent direction for the saddle point problem when the adversary is constrained in S. SGD using the gradient at the inner maximum decreases the loss value for true adversarial examples. The conclusions of the theorem regarding solving the saddle point problem with SGD are valid, as confirmed by experiments. Adversarial examples transfer between differently trained networks, indicating vulnerability to attacks. The transferability decreases with increased network capacity or adversary power during training. The transferability phenomenon can be reduced by using high capacity networks and strong oracles for inner optimization. Loss functions of trained models are analyzed to understand transferability, with angles between gradients and loss values compared. Experimental findings on MNIST dataset for \u03b5 = 0.3 are presented, showing the distribution of loss functions for naturally trained large networks. After analyzing loss functions and angles between gradients in trained models, experiments were conducted on MNIST dataset with \u03b5 = 0.3. Results showed that pairs of gradients for random inputs in one architecture are uncorrelated, while pairs in two networks with different initializations are somewhat correlated. A mild increase in classification accuracy for transferred examples was observed in a naturally trained large network. When trained against an FGSM adversary, gradients between architectures became significantly less correlated, leading to a notable increase in classification accuracy for transferred examples. The classification accuracy for transferred examples increases significantly compared to naturally trained networks. The loss function on the transfer network tends to start increasing later and less aggressively for stronger models. Transfer accuracies for the FGSM and PGD adversaries between simple and wide architectures on the CIFAR10 dataset were investigated. Transfer accuracies for FGSM and PGD adversaries between different model architectures show that stronger adversaries decrease transferability. Capacity also affects transferability, with wide networks being less successful in transfer attacks compared to simpler counterparts. The study found that changing the architecture to a wider network improves transferability across adversarial datasets. Results showed that PGD attacks outperform FGSM attacks in increasing loss, and transferred PGD attacks are more effective than white-box FGSM attacks. Wide networks are less successful in transfer attacks compared to simpler counterparts. The study compared the accuracy and resilience of networks against different types of input perturbations. It also analyzed the angles between gradients and how loss functions change with perturbations. The results showed that wider networks improve transferability across adversarial datasets, with PGD attacks being more effective than FGSM attacks. The study analyzed the accuracy and resilience of networks against input perturbations. It compared different networks and observed that robust models only utilize a few filters in the convolutional layer, resulting in a thresholding filter. The study found that robust models utilize thresholding filters in the convolutional layers, which are immune to perturbations on pixels below a certain threshold. The second convolutional layer interacts with only three channels from the first layer, leading to sparse filter weights with a wider value range. The softmax/output layer of the network was also examined for its weights. The study identified thresholding filters in convolutional layers of robust models, immune to perturbations below a certain threshold. The second convolutional layer interacts with three channels from the first layer, resulting in sparse filter weights. Significant differences in class biases were observed in the softmax/output layer of the network, with adversarially trained networks utilizing biases in a similar way. These modifications were learned through adversarial training, not hard-coded, to increase adversarial robustness. The study found that adversarially trained networks have more concentrated weights in their convolutional layers compared to natural models. The first convolutional layer in robust networks degrades into thresholding filters, while the second layer interacts with three channels from the first layer, resulting in sparse filter weights."
}