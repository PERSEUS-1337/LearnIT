{
    "title": "HkeryxBtPB",
    "content": "Max-Margin Adversarial (MMA) training aims to enhance neural network adversarial robustness by maximizing margins, which are distances from inputs to the decision boundary. This approach minimizes adversarial loss on the decision boundary at the \"shortest successful perturbation\", highlighting the connection between adversarial losses and margins. Max-Margin Adversarial (MMA) training improves adversarial robustness by adaptively selecting the correct margin for each datapoint. It maximizes margins to minimize adversarial loss, demonstrating efficacy on MNIST and CIFAR10 datasets. Neural networks are vulnerable to imperceptible perturbations, causing a drop in performance. The level of distortion in neural networks caused by imperceptible perturbations is measured by the magnitude of the perturbations. Adversarial robustness is linked to the margins of data points, defined as the distance from a data point to the classifier's decision boundary. Larger margins indicate a more robust classifier. Adversarial training aims to minimize the maximum loss within a fixed neighborhood around the training data using projected gradient descent. Despite recent advancements, a fundamental issue remains - the perturbation length must be set and remains constant throughout training. Recent work has shown that the perturbation length set for adversarial training is crucial, as setting it too small leads to less robust models and setting it too large results in decreased accuracy. Instead of using a fixed perturbation magnitude, a new approach called Max-Margin Adversarial (MMA) training is proposed, focusing on the margin perspective for improved adversarial robustness. Max-Margin Adversarial (MMA) training maximizes margins for each data point to optimize current robustness individually, overcoming technical difficulties by minimizing classification loss. Gradient descent is used for margin maximization, making adversarial training more effective. In Section 3, the text discusses how MMA training improves adversarial training by maximizing the margin for each training example. It shows that MMA training automatically balances accuracy and robustness, achieving higher robustness accuracies on average compared to standard adversarial training. In this paper, the focus is on maximizing the input space margin through the Margin Maximization Adversarial Training (MMA) algorithm. The MMA trained models match the performance of the best adversarially trained models and ensembles. Many defense methods aim to increase the margin, with some using regularization to constrain the model's Lipschitz constant. Samples with small loss would have large margin due to Lipschitz constant regularization. Local enforcement of Lipschitz constraint is not accurate, while global enforcement harms accuracy. First-order approximation methods for margin estimation have not achieved strong robustness. The focus is on K-class classification and maximizing input space margin through MMA training. Appendix B compares MMA training and SVM for K-class classification problems. The classifier assigns scores to classes based on a score function parametrized by \u03b8. The margin w.r.t. the classifier is defined for input (x, y). The margin's gradient w.r.t. \u03b8 is proportional to the loss' gradient at \u03b4 * according to Proposition 2.1. Margin maximization for non-smooth loss and norm can still be achieved by decreasing the loss w.r.t. the parameter \u03b8 at a specific perturbation \u03b4 * , even when the loss function and norm are not C 2 . This method can be applied to deep ReLU networks, where the loss can be reduced in a certain direction in the parameter space. Margin maximization can be achieved by reducing the loss with respect to the parameter \u03b8 at a specific perturbation \u03b4 * , even for non-smooth loss functions and norms. This method is applicable to deep ReLU networks, allowing the loss to be decreased in a specific direction in the parameter space. Margin maximization involves reducing the loss with respect to the parameter \u03b8 at a specific perturbation \u03b4 * to increase the margin. The \"logit margin loss\" can be unstable due to its piecewise nature, causing gradient discontinuities. In the MMA algorithm, the \"soft logit margin loss\" (SLM) is used as a smooth and convex surrogate loss. Gradient descent is then performed on model parameters using the cross-entropy loss L CE \u03b8 (x + \u03b4 * , y) for simplified learning. In the MMA algorithm, gradient descent is used on model parameters with the cross-entropy loss L CE \u03b8 (x + \u03b4 * , y). An adaptation of the projected gradient descent attack, AN-PGD, is proposed to find an approximate solution for \u03b4 * in margin maximization. The MMA algorithm uses gradient descent on model parameters with cross-entropy loss to maximize the margin. AN-PGD is an adapted projected gradient descent attack to approximate \u03b4 * for margin maximization. Previous adversarial training methods face non-convex optimization problems, but MMA training achieves desired behavior in practice. The MMA algorithm uses gradient descent with cross-entropy loss to maximize margin. Adding a clean loss term to the objective function improves efficiency in finding perturbations for training. The model trained with this combined loss is MMA-32. The MMA algorithm uses gradient descent with cross-entropy loss to maximize margin. The model trained with combined loss and d max = 32 is MMA-32, which shows improved efficiency in finding perturbations for training. The MMA training algorithm involves separating minibatches, finding \u03b4 * for correct predictions, and calculating gradients based on specific equations. The MMA algorithm utilizes gradient descent to maximize margin and improve efficiency in finding perturbations for training. It involves separating minibatches, determining optimal perturbation length \u03b4 * for correct predictions, and updating gradients based on specific equations. Adversarial training focuses on margin maximization by minimizing worst-case loss under a fixed perturbation magnitude. Update steps may not always increase the margin, leading to different scenarios for loss functions. Theorems connect adversarial training with margin maximization. Theorems connect adversarial training with margin maximization, showing that it maximizes the margin under certain conditions. Adversarial training with different loss functions can lead to different outcomes in terms of margin maximization. In adversarial training, using the SLM and CE loss results in the same approximate \u03b4 for the margin. When the margin is too large, adversarial training may maximize an upper bound of the margin, suggesting starting with a smaller margin and gradually increasing it could help maximize the lower bound of the margin. Results in Sections 4.1 and 4.2 confirm theoretical predictions. MMA training is compared with other adversarial training algorithms on MNIST and CIFAR10 datasets. MMA training shows stability to hyperparameter d max and better balance among attack lengths. It is a better choice for defense when perturbation length is unknown. Measuring Adversarial Robustness: The robust accuracy is determined using multiple projected gradient descent (PGD) attacks. Models are tested with whitebox PGD attacks and transfer attacks to assess robustness. N randomly initialized whitebox PGD attacks are performed on each model, followed by transfer attacks from other models. The total number of attacks is determined by the number of models considered. The study evaluates the robustness of models using whitebox PGD attacks and transfer attacks. A total of 320 attacks are conducted for CIFAR10-\u221e. Margin distribution changes during training processes are tracked in two models under the CIFAR10-24 case by measuring margins of 500 training points after each epoch. During training epochs, the margin distribution of two models is tracked using a 1000-step DDN attack. Margins initially concentrate near 0 and enlarge on average as training progresses. In PGD training, some margins remain close to 0 while others exceed 2.5, reflecting the focus on maximizing lower bounds. MMA training shows a different pattern, not giving up on margins. The PGD-2.5 model is trained to focus on maximizing lower bounds, resulting in larger margins for \"easy data\" but smaller margins for \"hard data.\" On the other hand, MMA training pushes the margin of every data point, achieving a better balance between small and large margins. When the attack magnitude is unknown, MMA training is more effective in handling adversarial attacks overall. PGD training does not increase the margin, as confirmed by experiments. PGD-24 fails at larger magnitudes, with accuracies around 10%. To improve, a variant called PGD with Linear Scaling (PGDLS) gradually increases perturbation magnitude to avoid exceeding the margin. This approach aims to maximize the lower bound of the margin, contrasting with MMA training that schedules magnitudes individually. The models' performances are shown in Table 1. PGDLS models, including PGDLS-24, show successful training compared to PGD-24. PGDLS performs similarly or better than PGD at perturbation levels of 8 or 16. MMA training is stable with varying hyperparameters, achieving good robustness with smaller values of the hinge threshold d max. MMA-32 achieves high clean accuracy and robust accuracy at lower attacking magnitudes, even with a high d max setting. MMA training achieves high accuracy at lower attacking magnitudes with large d max values. In contrast, PGD trained models are more sensitive to perturbation magnitude. Ensemble of PGD trained models shows similar performance to MMA training. MMA training achieves similar performance to ensembled PGD models, with MMA-32 slightly outperforming PGD-ens in robustness. Ensembling requires higher computation costs and attacking ensembles is relatively unexplored. MMA trained models significantly outperform PGD ensemble models on MNIST-\u221e / 2. SPSA attack does not successfully compute adversarial examples on MMA trained models. In this paper, a new training algorithm called MMA is proposed to maximize margins for improved adversarial robustness. The MMA training algorithm optimizes margins through adversarial training with perturbation magnitude adapted for each data point. Empirical experiments on CIFAR10 and MNIST show that MMA training outperforms traditional adversarial training in sensitivity to hyperparameters and robustness to variable attack lengths. MMA is recommended for defense when the adversary is unknown. The gradient for d \u03b8 (x, y) is computed in its general form for an optimization problem with Lagrangian L(\u03b4, \u03bb). The optimizer \u03b4 * and \u03bb * satisfy the first-order conditions (FOC) in vector form. By the implicit function theorem, \u03b4 * and \u03bb * can be expressed as functions of \u03b8. A simple way to compute this gradient is presented, involving the inverse of a matrix. One way to calculate the gradient involves differentiating with respect to \u03b8 on both sides. For a K-layers fully-connected ReLU network, the function f(\u03b8; x) is a piecewise polynomial function of \u03b8 with finitely many pieces. The directional derivative of a function g along the direction of v is defined. The directional derivative of a function is well defined for every direction v, with a polynomial restricted to a line segment. The existence of v and t for l(\u03b8) is shown, with a negative derivative along a specific direction v. The Danskin theorem is used to compute the directional gradient, with constraints on the compact set and piecewise Lipschitz continuity. The directional derivative of a function is well defined for every direction v, with a polynomial restricted to a line segment. The existence of v and t for l(\u03b8) is shown, with a negative derivative along a specific direction v. The Danskin theorem is used to compute the directional gradient, with constraints on the compact set and piecewise Lipschitz continuity. The function f \u03b8 is continuous on both \u03b4 and along the direction v, allowing the application of Theorem 1 in Yu (2012). Proposition A.2 provides an alternative method to increase the margin of f \u03b8, showing that if \u03b8 0 is not a local minimum, there exists a direction d for which f \u03b81 has a larger margin than f \u03b80. The lemma relates the objective of adversarial training to MMA training by showing that for a given (x, y) and \u03b8, if L(\u03b4, \u03b8) is continuous in \u03b4, then for \u03c1 \u2265 0, there exists a \u03b4 such that \u03b4 < \u03c1 and L(\u03b4, \u03b8) = \u03c1. This contradicts the assumption that min L(\u03b4, \u03b8) \u2265 \u03c1. The lemma shows that for a given (x, y) and \u03b8, if L(\u03b4, \u03b8) is continuous in \u03b4, then there exists a contradiction when min L(\u03b4, \u03b8) \u2265 \u03c1. Previous works have explored first-order large margin methods in machine learning. Previous works have explored first-order large margin methods in machine learning. Works have attempted to use first-order approximation to estimate the input space margin. MMA's margin estimation is exact when the shortest successful perturbation \u03b4 * can be solved, enabling more accurate margin estimation for improved training performance. Our method uses adversarial attacks to estimate the margin to the decision boundary, unlike (Cross-)Lipschitz Regularization which places a strong constraint on the model. Our method utilizes adversarial attacks to estimate the margin to the decision boundary, providing a more precise estimate in the data point's neighborhood. This approach is more flexible compared to relying on a global Lipschitz constraint. The objective of hard-margin SVM in the separable case is to maximize the average margin, unlike the minimum margin in traditional SVM formulations. The use of MMA aims to improve adversarial robustness by maximizing the average margin. To improve adversarial robustness, MMA focuses on maximizing the average margin, while delaying the minimization of the minimum margin for future work. Comparisons are made with models trained on the DDN attack, which aims for successful perturbation with minimal 2 norm. Evaluations were conducted on 7 DDN trained models, including a larger ConvNet for MNIST and a wideresnet-28-10 model for CIFAR10. DDN training differs from MMA as it uses clean images when no successful adversarial example is found. In MMA training, successful adversarial examples are treated as perturbations with large magnitude, ignored by hinge loss. There is no maximum norm constraint in MMA training unlike in DDN training. Despite differences in hyperparameters, MMA training achieves similar performance. MMA training framework shows significant improvements over PGD training in the \u221e case. In the experiments, LeNet5 models are trained for MNIST and wide residual networks are used for CIFAR10. The validation set is used to monitor training progress, but not in training. Models are trained and tested under the same norm constraints. LeNet5 architecture includes conv filters, ReLU activation, max pooling, and fully connected layers. No preprocessing is done on MNIST images before feeding into the model. For training LeNet5 on MNIST, Adam optimizer with initial learning rate of 0.0001 is used for 100000 steps with batch size 50. WideResNet-28-4 is used for CIFAR10 with stochastic gradient descent, momentum 0.9, and weight decay 0.0002 for 50000 steps with batch size 128. Learning rate schedule is 0.3 at step 0, 0.09 at step 20000, 0.03 at step 30000, and 0.009 at step 40000. In experiments, different learning rates were tested for PGD and MMA training, with 0.3 chosen for later experiments. A longer training schedule did not show improvement, so 50000 steps were used. For MNIST, a 40-step PGD attack with SLM loss was used, while CIFAR10 used a 10-step PGD attack with the same loss. AN-PGD included a 10-step binary search after PGD, with a maximum perturbation length of 1.05 times the hinge threshold. In experiments, different learning rates were tested for PGD and MMA training, with 0.3 chosen for later experiments. Trained models use various PGD/PGDLS models with different perturbation magnitudes. Ensemble models make predictions by majority voting on label predictions. MMA training is performed with different hinge thresholds and with/without additional clean loss. Two models are trained for each d max value with different random seeds. In experiments, different learning rates were tested for PGD and MMA training, with 0.3 chosen for later experiments. Trained models use various PGD/PGDLS models with different perturbation magnitudes. Ensemble models make predictions by majority voting on label predictions. MMA training is performed with different hinge thresholds and with/without additional clean loss. Two models are trained for each d max value with different random seeds. The MMA trained models are named as OMMA/MMA-d max -seed for transfer attacks. Whitebox PGD attacks are conducted using both cross-entropy (CE) loss and Carlini & Wagner (CW) loss. At test time, all models undergo N whitebox PGD attacks, with N/2 being CE-PGD attacks and the other N/2 being CW-PGD attacks. Step sizes are manually tuned for different models, with specific values for MNIST and CIFAR10. An ensemble model uses a majority vote for prediction and softmax scores as tiebreakers. Two strategies are used for CW-PGD and CE-PGD attacks on the ensemble models. The second strategy for attacking ensemble models involves a customized PGD attack with a focus on classifiers in the ensemble. The attack calculates the loss based on the majority of classifiers giving wrong classifications. If less than half are wrong, the CW losses from correct classifiers are summed. If more than half are wrong, the most frequent wrong prediction is identified, and a specific loss function is maximized during the attack. This approach allows for whitebox PGD attacks on ensemble models, with 50 repeated attacks on MNIST and CIFAR10 datasets. In attacking ensemble models, a customized PGD attack focuses on majority wrong classifications. For MNIST, 50 repeated attacks are performed, and for CIFAR10, 10 attacks. Adding a clean loss term to the MMA loss is examined for effectiveness. Models trained with the MMA loss show TransferGaps, representing the additional attack success rate transfer attacks bring. Adding clean loss to MMA-32 reduces TransferGap from 7.39% to 3.02%. MMA trained models show robustness to gradient free attacks, indicating TransferGaps are not due to gradient masking. TransferGaps are almost zero for MNIST cases, suggesting they are not solely due to the MMA algorithm or data distributions. Adding additional clean loss to MMA trained models on CIFAR10 leads to a decrease in clean accuracy and an increase in average robust accuracy. This counter-intuitive result confirms the motivation behind the clean loss, making the input space loss landscape steeper for stronger adversaries during training, emphasizing robustness over clean accuracy. Empirical results are presented in Tables 2 to 13, showing model performances under combined attacks. In Tables 2 to 13, model performances under combined attacks are shown. Whitebox PGD attacks are presented in Tables 6 to 9, while TransferGaps are displayed in Tables 10 to 13. The robust accuracy of PGD-Madry et al. models for MNIST and CIFAR10 is evaluated, with results slightly stronger than reported on the website. The study compares the performance of PGD attacks with 100-step iterations to those with 20-step iterations, showing that DDN training is similar to MMA training when d max is low. The results for CIFAR10-2 exhibit stable performance for MMA training, while PGD/PGDLS trained models sacrifice clean accuracies for good AvgRobAcc. Additionally, adding clean loss enhances model robustness and reduces TransferGap, albeit at the expense of slightly lower clean accuracies. The study compares the performance of different adversarial training algorithms on MNIST datasets. PGD training is ineffective on large perturbations for MNIST-\u221e, while PGDLS training performs well. MMA training does not provide additional benefits on top of PGDLS due to the simplicity of the task on MNIST. In Section 3, MMA training on MNIST-2 shows high robustness without sacrificing clean accuracy. Models with d max \u2265 4.0 perform similarly in terms of clean and robust accuracies. PGD/PGDLS models achieve higher robustness at = 3.0 with a robust accuracy of 44.5%, but with some loss in clean accuracy. Unlike CIFAR10, PGD(LS)-ens model performs poorly on MNIST datasets. TransferGaps are small for OMMA/MMA trained models on MNIST compared to CIFAR10. TransferGaps are small for OMMA/MMA trained models on MNIST compared to CIFAR10, indicating dataset properties play a significant role. Previous literature discusses differences in adversarial robustness between MNIST and CIFAR10 but does not directly address this observation. Further exploration of this topic is left for future work."
}