{
    "title": "B11bwYgfM",
    "content": "We propose a novel task clustering algorithm for deep learning-based multi-task and few-shot learning in settings with diverse tasks. Our method estimates task similarities using a matrix completion approach, addressing the limitations of the full transfer performance matrix. This algorithm efficiently works on partially-observed similarity matrices, ensuring robustness and accuracy in task clustering. The paper introduces a task clustering algorithm for multi-task and few-shot learning, utilizing a matrix completion approach to estimate task similarities. The algorithm efficiently partitions tasks, benefiting both multi-task learning and few-shot learning setups for sentiment classification and dialog intent classification tasks. MATL tackles hundreds or thousands of tasks with unknown relatedness, introducing challenges like task diversity and model inefficiency. These scenarios are common in machine learning applications, such as reinforcement learning for game playing and enterprise AI cloud services. Multi-task learning (MATL) involves handling diverse tasks for various clients, including companies seeking customer feedback, agencies monitoring public reactions, and financial analysts analyzing news for stock market impact. Challenges arise in dealing with heterogeneous tasks, as traditional approaches may not effectively transfer knowledge between tasks with varying labels and characteristics. In Multi-task learning (MATL), tasks can have varying numbers of labels and may not always help each other due to negative transfer. For example, in dialog services, different tasks like restaurant recommendations and park guides may have conflicting label assignments for similar sentences. In Multi-task learning, tasks can hinder each other when trained jointly with a single representation function. To address this issue, a Task Clustering Based Solution is proposed to partition tasks into clusters, reducing negative transfer problems. Task clustering algorithms for convex models have been proposed, but they assume tasks have the same number of labels. To handle tasks with varying label numbers, a similarity-based task clustering algorithm is adopted. The proposed task clustering algorithm aims to partition tasks into clusters based on their similarities, allowing for handling tasks with varying numbers of labels. However, directly using cross-task transfer performance for clustering may be inefficient and inaccurate due to the large number of tasks involved. The proposed task clustering algorithm utilizes matrix completion theory to address challenges in evaluating cross-task performance. By randomly sampling task pairs and keeping only consistent scores, a partially-observed matrix is constructed to accurately partition tasks into clusters based on similarities. The proposed approach utilizes matrix completion theory to recover a full similarity matrix from a partially-observed matrix, allowing for task partitioning through spectral clustering. The method guarantees perfect recovery with a sufficient number of observed correct entries, reducing computation and improving clustering robustness by filtering out uncertain task pairs. The proposed algorithm utilizes task clusters to handle diverse MTL and FSL problems, improving clustering performance by reducing noise sensitivity. Results show that combining task clustering with MTL and FSL strategies enhances deep learning algorithms for sentiment and intent classification tasks. Task clustering methods measure task relationships based on model parameter similarities, leading to improved clustering robustness and computation efficiency. The proposed algorithm uses task clusters to improve clustering performance in diverse MTL and FSL problems. Task clustering methods measure task relationships based on model parameter similarities, enhancing clustering robustness and computation efficiency. Deep neural networks can overfit with training loss alone, requiring additional regularization methods. Few Shot Learning aims to classify new classes with minimal training data. Bayesian Program Induction uses simple programs to explain examples. Siamese neural networks measure similarity between inputs. Matching Networks map support sets to labels without fine-tuning. LSTM-based meta-learner adapts to diverse tasks efficiently. Our FSL approach can handle diverse tasks with varying class labels. We train classification models on labeled examples using CNNs for text classification tasks. Our FSL approach utilizes CNNs for text classification tasks, with a state-of-the-art architecture shown in FIG0. The model includes a convolution layer and max-pooling operation, divided into an encoder and classifier part. A task-clustering framework is proposed for MTL and FSL settings, serving as the initial step in both algorithms as outlined in Section 3.3 & 3.4. FIG1 provides an overview of the idea, demonstrating how the task-clustering algorithm aids MTL. Performance scores are computed using single-task models. The algorithm aids MTL by computing performance scores using single-task models. It forms a pair-wise classification performance matrix S, indicating the similarity between tasks. Task Pair Sampling is used to evaluate transfer performance efficiently. The algorithm aids MTL by computing performance scores using single-task models and forming a pair-wise classification performance matrix S. Task Pair Sampling is used to efficiently evaluate transfer performance by considering reliable task relationships and constructing a symmetric similarity matrix. Algorithm 1 details the process of generating a partially-observed similarity matrix Y using reliable task pairs. A dynamic threshold is defined based on the mean and standard deviation of target task performance to determine high and low performance. Pairwise similarity is set to 1 or 0 based on whether S ij and S ji are high or low enough. Given a partially observed symmetric matrix Y, we reconstruct the full similarity matrix X by decomposing Y into two matrices: X, a low-rank matrix storing task pair similarities, and E, a sparse matrix capturing observed incorrect entries. The matrix completion problem involves a low-rank matrix for task pair similarities and a sparse matrix for errors. It is formulated as a convex optimization problem with a matrix nuclear norm. The theorem guarantees perfect recovery for a rank k matrix X*. The matrix completion problem involves recovering a low-rank matrix X* with errors, where a small fraction of observed entries can lead to perfect recovery. The theorem guarantees perfect recovery for a rank k matrix X*. The completed similarity matrix X is symmetric, enabling analysis by clustering algorithms like spectral clustering. Each cluster has a model \u039b k for parameter sharing. In the MTL setting, encoder sharing is done with distinct task-specific classifiers. Limited training samples in few-shot learning make training task-specific classifiers impractical, so predictions are made by linearly combining learned predictions. In few-shot learning, predictions for new tasks are made by combining predictions from learned clusters. Cluster-models are trained using alternatives that better suit the method. When tasks have identical label sets, a single classification model is trained on all tasks. For tasks with different label sets, a metric-learning model is trained to make each example closer to examples with the same label. The methods are tested through experiments using various datasets. In the data-preprocessing step, NLTK toolkit 3 was used for tokenization. For multi-task learning, tasks are clustered and model training is conducted. In the few-shot learning setting, tasks are divided into training and testing tasks for evaluation. The Amazon Review Sentiment Classification dataset consists of product reviews for 23 types of products, with three binary classification tasks constructed for each domain based on different rating thresholds. The dataset includes 23 domains with 69 tasks based on review ratings. For evaluation, 12 tasks from 4 domains are selected. The second dataset involves user intent classification for dialog systems using recorded conversations. The dataset consists of conversations between users and dialog systems in various domains. Intent-labels are assigned to user utterances, with tasks randomly sampled for training, validation, and testing. To adapt to Few-Shot Learning (FSL), one example for each label plus 20 randomly picked labeled examples are used for training. Matching networks BID20 are chosen for handling various numbers of labels in the FSL setting. In the FSL setting, matching networks BID20 are used to handle various numbers of labels. A large number of real-world tasks are collected for intent classification, with 100,000 task pairs sampled to verify task clustering method's robustness. Only 4.5% of the transfer-performance matrix entries are observed, with each entry modified based on a threshold value. In the FSL setting, various baselines are compared for MTL and FSL settings, including single-task CNN, holistic MTL-CNN, single-task FastText, and Matching Network. The modified Eq. 1 is used to determine entries in Y based on threshold values, with observed entries corresponding to sampled pairs in Y. The proposed method involves training different models for each target task, including MTL-CNN, Matching Network, and logistic regression for sentiment classification. The models are initialized with pre-trained Glove embeddings and compared with baseline methods. Parameters p1 and p2 are set to 0.5 for a balance between observed entries in Y. The proposed method involves training different models for each target task, including MTL-CNN, Matching Network, and logistic regression for sentiment classification. Parameters like window size, hidden layer size, learning rate, and initialization of embeddings are tuned based on average accuracy on dev sets. The CNN has a window size of 5 and 200 hidden units, with a learning rate of 0.001. MTL models use random initialized word embeddings for sentiment classification and Glove embeddings for intent classification. Hyper-parameter selection is challenging for FSL due to lack of validation data. Early stopping criterion is used for model training. During FSL setting, hyper-parameter selection is challenging without validation data. To address this, a subset of training tasks is preselected as validation tasks to tune learning rate and training epochs. Out-of-Vocabulary issues in transfer-performance evaluation are mitigated by using fixed pre-trained embeddings in single-task models. The MTL setting improves observed tasks, with results shown for 12 target tasks out of 69. In the MTL setting, the holistic MTL-CNN model does not show accuracy improvements due to conflicts among tasks, but the ROBUSTTC-MTL method based on task clustering achieves significant improvement over baselines. The ASAP-MTLR method also improves single-task linear models, but is limited by the representative strength of linear models. In the FSL setting, ROBUSTTC-FSL outperforms baselines by leveraging knowledge from previously observed tasks. Clustering-based MTL and FSL approaches also work for ASAP clusters, showing improved performance over baseline models. Our ROBUSTTC model performs better than baseline models in handling varying class labels and generating improved clusters for MTL/FSL of deep networks. Training CNNs on ASAP clusters yields better results compared to logistic regression models, emphasizing the importance of task clustering for deep models. MTL & FSL results on dialog intent classification show trends similar to sentiment classification tasks, with holistic MTL methods achieving superior results over single-task CNNs. ROBUSTTC-MTL shows significant improvement over single-task CNNs due to diverse tasks and task-clustering reducing conflicts. ROBUSTTC-FSL also improves over baselines, but with smaller margin due to task diversity causing some tasks to fail. Algorithm needs to automatically determine if new task belongs to existing clusters to benefit from them. The new method Adaptive ROBUSTTC-FSL improves performance by over 5% compared to the best ROBUSTTC-FSL result. Clustering-based Few-Shot Learning (FSL) approach maintains diverse metrics for tasks, unlike single metric FSL methods. Maintaining multiple metrics is crucial for diverse few-shot learning problems. The holistic MTL-CNN achieves a 6% improvement over single-task CNNs on the extra-large dialog intent classification dataset. The consistent development and test performance due to holistic multi-task training approach is the main reason for this improvement. The full transfer-performance matrix S was not evaluated due to time constraints, with only around 4.5% information used. In this paper, a robust task-clustering method is proposed with strong theoretical guarantees and empirical improvements when combined with MTL and FSL algorithms. Empirical studies confirm the effectiveness of the task clustering approach in diverse many-task learning settings, its ability to handle a large number of tasks efficiently, and the use of cross-task transfer performance as a task similarity measure. Our work introduces a task similarity measure using cross-task transfer performance. We demonstrate that the similarity matrix is of low-rank, supporting future research directions in many-task learning and enhancing MTL and FSL methods. The number of clusters is small, so the similarity matrix X should be of low rank. The proof of Theorem 4.1 involves defining notations and assumptions related to the singular value decomposition of matrix X. The orthogonal complement to the space T is denoted as T \u22a5, with P T being the orthogonal projection onto the subspace T. Proposition 1 states that if enough entries of a matrix Z \u2208 T are zero, then Z is a zero matrix with high probability. This is based on a theorem by Cand\u00e8s & Tao (2010) and involves a projection P \u2126 onto a subset \u2126 sampled uniformly at random. The theorem developed further discusses the conditions for Z to be zero. Theorem 1 guarantees the unique optimal solution to an optimization problem when certain conditions are met, including the existence of a dual certificate Q. The theorem states that with a probability of at least 1 - 3n^-\u03b2, the true matrices (X, E) are the unique optimizer if specific assumptions are satisfied. The unique optimal solution to an optimization problem is proven by contradiction. Assuming the existence of another optimal solution, it leads to a contradiction, proving the uniqueness of the solution. The proof of Theorem 3.1 involves constructing a matrix Q that satisfies specific conditions. By following the conditions outlined in Theorem 1, a dual certificate Q is defined. The conditions (a)-(e) are met by ensuring certain equations hold true. The proof of Theorem 3.1 involves constructing a matrix Q that satisfies specific conditions. It is shown that there exist solutions T and \u2206 that satisfy certain relationships. The existence of T that satisfies a specific relation is demonstrated, leading to a one-to-one mapping. The proof involves constructing a matrix Q satisfying specific conditions, leading to the existence of solutions T and \u2206. T is a one-to-one mapping, bounded by T F. The proof concludes with conditions for \u03bb to satisfy the inequalities."
}