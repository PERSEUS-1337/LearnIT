{
    "title": "BJlaYi05tm",
    "content": "We present a formal procedure for computing preimages of convolutional network outputs using a dual basis defined from hyperplanes associated with network layers. The special symmetry of hyperplane arrangements in convolutional networks forms regular multidimensional polyhedral cones. Efficiency is discussed in creating nested cones from incremental convolutions for data contraction and shaping preimage manifolds. A specific network is shown to flatten a nonlinear input manifold to an affine output manifold, relevant for understanding deep learning classification properties. Deep convolutional networks map input data to output domains corresponding to classes. Studies have explored the capacity of deep networks, but questions remain on why deeper networks are advantageous and the mechanisms behind their generalization properties. The effectiveness of deep learning over large datasets compared to earlier machine learning approaches is still not fully understood. More research on trained networks and their mappings is needed to address these questions. Trained deep networks efficiently produce mappings that are crucial for understanding their generalization properties. The complexity of these networks makes it challenging to compute how input domains are transformed into output classifiers. Research has focused on empirical studies of trained networks to uncover the relation between network parameters and the actual functions they perform. Understanding the geometry of deep networks and the data manifolds they process is key to their success. Convolutional networks with ReLU non-linearities can be characterized by hyperplanes associated with convolutional kernels. The arrangement of hyperplanes within and between layers is crucial for comprehension. The relative arrangement between layers in deep networks is crucial for understanding how data is mapped. This analysis focuses on the convolutional part of a single-channel deep network without subsampling or max pooling. The goal is to analyze how input data domains are mapped through the network, providing a detailed description of the network's workings. The structure of the input data manifold that can be mapped to a specified reduced dimensionality will be computed in detail. The input data manifold is flattened to a reduced dimensionality affine manifold in the final convolutional output layer. Understanding mappings between layers involves computing preimages for network activities. The arrangement of hyperplanes in convolutional networks forms multidimensional polyhedral cones nested within each other, leading to efficient contraction and shaping of the input domain. The paper discusses how extending the number of layers in a network with incremental updates of filters resolves the conflict between efficient contraction and shaping. It exploits nested cones to shape and contract non-linear manifolds for efficient preprocessing in object recognition. The convolutional part of the network is shown to flatten non-linear input manifolds as a crucial preprocessing step. Transformations between layers in a network with ReLU as nonlinear elements can be written as the ReLU function applied component wise to the input vector x, dividing the output vector y into two classes. The preimage of y can be empty, contain a unique element x, or consist of a whole domain of the input space, depending on the location of the input relative to the arrangement of the network. The preimage of the output y in a network with ReLU as nonlinear elements depends on the arrangement of hyperplanes defined by the mapping. These hyperplanes divide the input space into different cells, with the maximum number achieved when all hyperplanes cut through the input space. Understanding the arrangement of hyperplanes is crucial for grasping how input domains are affected by the network. Positive components of y are associated with hyperplanes, and the preimage problem can be approached geometrically using these hyperplanes and the constraint input domains. The preimage of the output y in a network with ReLU as nonlinear elements is defined by hyperplanes associated with positive components of y. These hyperplanes, along with half spaces defined by negative components, create constraints on the input domain. The preimage set is found by computing the intersection of an affine subspace with a polytope in d-dimensional space, a problem known to be exponential and intractable. When considering convolutional networks instead of fully connected ones, the nature of preimages changes substantially. Hyperplanes in general position intersect to form one-dimensional affine subspaces, creating a complete set of vectors that can be used as a basis in d-dimensional space. The mutual intersection of all planes \u03a0 i can be used as a basis spanning R d. The dual basis set can express the solution to the preimage problem, with the affine intersection subspace P * associated with positive components j 1 j 2 . . . j q of the output y. The positive direction of vector e i is defined by the negative side of plane \u03a0 i, which intersects with hyperplanes \u03a0 1 , \u03a0 2 . . . \u03a0 p constraining the preimage to negative sides' intersections. The preimage of the output y is a unique element x * \u2208 R d lying in the affine subspace of positive output components P * and on the intersection of boundary hyperplanes \u03a0 i. Specializing to convolutional networks with a single channel and no subsampling, the geometric properties play a fundamental role in the solution. In convolutional networks with a single channel and no subsampling, the convolutional matrices can be approximated as circulant matrices. This approximation is negligible when the convolution support is small relative to the dimension. The bias is assumed to be the same for all convolution kernels. The circulant matrices associated with hyperplanes intersect on the identity line and have a regular arrangement. The sum of elements along each row is constant, leading to a solution xj = -b/a for all rows. The set of hyperplanes formed by the circulant augmented convolutional matrix W will exhibit a highly regular pattern. The circulant matrices associated with hyperplanes intersect on the identity line and have a regular arrangement. The hyperplanes associated with weights form a regular multidimensional polyhedral cone in R d around the identity line, controlled by bias b and sum of filter weights a. The cone's geometry is determined by apex location, plane angles, and rotation in d-dimensional space. The convolution weights in CNNs restrict rotations of the cone, impacting the mapping between layers in a convolutional network. The transformation between layers can be viewed as a mapping between regular multidimensional polyhedral cones symmetric around the identity line in R d. This strong regularity imposes constraints on the geometric description of the mapping, broken down into transformations between intersection subspaces of the cones. The mapping of data from input to output space in a multi-layer network is illustrated in FIG1 for networks with 3 and 6 layers. The contraction flows show how data moves through the network, with most of the input domain mapped to output (0, 0). The second network demonstrates creating input domain manifolds with more structure. The importance of \"nested cones\" in creating manifolds with more structure is demonstrated. Red lines represent nested layer cones, while black lines show wider angles in succeeding cones. Hyperplanes intersect at the input-output cone boundary, resulting in linear transformation beyond this point. Data in figure 2 is remapped to input space, having no effect on shaping the input manifold. Layers beyond the intersection are considered \"wasted\" as they do not contribute to shaping or contraction of input data manifolds. The effect of nested vs. partially nested cones on input manifolds is seen as a less diverse variation of shape. In higher dimensions, this effect becomes more elaborate, with rotations of the cone adding complexity. Contraction of data from higher to lower dimensions becomes intricate as the number of subspace dimensionalities increases. The transformation properties of a network can be illustrated using a generic circulant matrix in 3-dimensional input and output spaces. The properties of the network are illustrated starting with the pure bias case. Elements in input space are mapped relative to hyperplanes, dividing the space into cells. The mapping for input elements remains the same in cells with the same relation to hyperplanes. In higher dimensions, elements are mapped to specific intersection subspaces in the output domain. The grey shaded boxes in FIG2 represent cells with different numbers of negative constraints, each mapped to specific hyperplanes or intersections of hyperplanes. The nesting of cones associated with the input and output layer divides the input space into cells that are irreversibly contracted to lower dimensions near the identity mapping. Changing the angle of the output cone affects the dual bases used for mapping, introducing a limit to the nesting beyond which the mapping properties change. In regions where this limit is surpassed, data no longer maps to a lower-dimensional manifold. The contraction property is lost in regions where nesting of cones ceases, defined as cones with restricted nesting. Complete nesting implies data contraction from one layer to the next, mapping elements to the same intersection subset with the same dimensionality. Elements from subsets of indexes are also mapped to this intersection subset. The contraction property in deep networks involves mapping data between layers from intersection subsets to subsets of equal or lower dimension. This leads to increased sparsity in the network nodes as data is concentrated on lower-dimensional subspaces. The convolutional part of a deep network functions as a component in a metric learning system, creating domains in input space associated with different classes for efficient separation in the final fully connected layers. The conflict between input manifold diversity and contraction in convolutional networks can be resolved by increasing the number of layers with incremental filters. This approach has been shown to improve deep network performance. The final fully connected layers are crucial for efficient class separation. The final fully connected layers are crucial for efficient class separation in convolutional networks. The network should map different classes to linearly separable domains for efficient classification. The final layer can be characterized by a set of hyperplanes, denoted as M and x (l). The output x (l) is associated with the intersection of the output manifold and corresponding hyperplanes. The dimensionality of M determines the degree of intersection q. Intersecting M with subsets generates pieces of intersections linked together, forming affine subsets with different dimensionality. The output manifold intersects with hyperplanes to generate pieces of intersections that form affine subsets with varying dimensionality. This process continues through the network, creating a piecewise planar input manifold that intersects with previous layers until reaching the input layer. The complexity of generating piecewise affine manifolds in arbitrary dimensions grows with increasing combinatorics. Each intersection of the manifold with possible hyperplane subsets generates seeding hyperplanes, acting as new manifolds at the next layer. The nested cone property reduces complexity compared to arbitrary hyperplanes. Figure 4 shows a piecewise planar manifold in 3D input space mapping to an affine manifold at the final convolutional layer in a network with circulant transformations. The axis Blue patches are mapped to 1d lines connecting points. Patches generated by selective components of the dual basis at each layer create linking patches for continuity of the input manifold. These manifolds do not necessarily correspond to actual class manifolds, but can be seen as specific building blocks. The text discusses the formal procedure for computing preimages of deep linear transformation networks with ReLU non-linearities using the dual basis extracted from hyperplanes. It highlights the reduced complexity and symmetry of hyperplane arrangements in convolutional networks, which can be modeled with multidimensional regular polyhedral cones. The crucial property of nested cones for efficient data contraction to lower dimensions is emphasized, suggesting its relevance in network design. Increasing the number of layers shapes input manifolds as preimages to retain key details. Increasing the number of layers shapes input manifolds to retain the nested cone property, limiting degrees of freedom for multidimensional rotation. This constraint acts as regularization, potentially explaining the efficiency of deep networks in generalization despite high complexity. Deep convolutional networks can compute non-linear input manifolds that map to affine output manifolds, allowing for the flattening of input data. This structure indicates a possible basic input manifold for classes, demonstrating that convolutional networks can be designed to achieve classification. Convolutional networks can be designed to separate covariant classes in input space, contributing to their success in classification. The presence of manifolds in the network structure is derived from formal grounds, even if their role in classification is not clear."
}