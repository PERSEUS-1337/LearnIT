{
    "title": "S1eYchEtwH",
    "content": "Learning control policies in robotic tasks often require a large number of interactions due to various constraints. Humans, on the other hand, can quickly adapt and find safe solutions after just one failure. To mimic this ability, a hierarchical Bayesian optimization algorithm was developed. This algorithm uses a Gaussian Process for modeling and sampling, allowing for rapid learning with large rates. Additionally, a mental replay phase helps inhibit policy regions that led to failures during the sampling process. The hierarchical Bayesian optimization method is evaluated in a humanoid postural balancing task, showing efficient learning compared to human subjects. It outperforms standard Bayesian Optimization in task-solving interactions, computational demands, and failure frequency. This approach provides a model for future motor control tasks in autonomous systems like robots and self-driving cars to ensure safe interactions with humans and the environment. State-of-the-art robot learning approaches are applied to simulations first, with final candidate policies selected by an expert for the real system. Fine-tuning on the real system is necessary to address unmodelled dynamics, motor noise, or uncertainties in hardware fabrication. Strategies for safe policy exploration have been proposed, including constrained operational space in tasks like robot arm manipulation. Null-space control approaches have been used in research labs, but may fail in changing environmental conditions. In research labs, robot learning approaches are tested in simulations before being fine-tuned on the real system to address uncertainties. Strategies for safe policy exploration include constrained operational space and Bayesian optimization techniques to avoid failures. An expert intervenes when potential dangerous situations are anticipated. Hierarchical Bayesian optimization (BO) algorithm proposed for safe policy exploration in robotics. Features are sampled to predict policies' potential, aiming to achieve human-like performance in high-dimensional problems. Existing strategies require expert knowledge or human intervention to prevent harm to the system or environment. In high-dimensional problems, methods like Hierarchical Diagonal Sampling, Bayesian Optimization, dropout algorithms, and Gaussian Processes are used to address the challenge of \"low effective dimensionality\" and optimize functions efficiently. The contribution of this paper is a computational model for psychological motor control experiments based on hierarchical acquisition functions in Bayesian Optimization (HiBO). The motor skill learning method uses features for optimization to reduce the number of required roll-outs significantly in postural control experiments. The focus is to develop a testable model for psychological motor control experiments using well-known postural control features. In future work, the model will be extended to autonomous feature learning for challenging robotic tasks. The methodology of the hierarchical BO approach is introduced, starting with the problem statement and summarizing the concept of BO. The goal is to find the best policy that maximizes return by executing motor commands in a given state. Samples are collected for learning the policy vector and context. The optimization problem involves finding the optimal parameter vector and context vector through a hierarchical optimization process. Bayesian Optimization is used as a benchmark, where a model is built based on observed data to solve global optimization problems efficiently. The model for a given system is built based on observed data D = {X, y}, describing a transformation from data point x \u2208 X to scalar value y \u2208 y. Gaussian Processes (GPs) are used for modeling the unknown system, representing a distribution over observed data points D = {X, y} and query points D* = {X*, y*}. GPs are state-of-the-art regression approaches successfully applied in robotic applications. The representation of the system is defined by the mean and covariance, with a Mat\u00e9rn kernel used for modeling the unknown system. The kernel has a smoothing parameter \u03bd = 5/2 and hyperparameters optimized in experiments. In experiments, hyperparameters for the squared-exponential kernel were optimized to maximize marginal likelihood. Predictions for query points are determined using expected improvement for choosing the next model evaluation point. Samples are evaluated and the best point is chosen based on acquisition function values. A joint distribution is learned for observed triples, used as a prior for the acquisition function. The hierarchical optimization process involves conditioning on features to estimate the best parameter values using a GP model. This iterative scheme aims to find the most promising policy vectors by evaluating mean and variance. Mental replays can be generated to ensure robustness in Bayesian Optimization. The new training data set is enlarged by generating perturbed copies of the policy parameter \u03b8 [k+1]. Different replay strategies are evaluated in the results section. Observations on human learning during perturbed movements are presented, comparing simulated humanoid learning to human participants. The hierarchical BO approach is evaluated against the standard BO, and the impact of mental replays on performance is assessed. In an experiment, 20 male participants underwent waist pull perturbation during squat-to-stand movements. They had to stand up without taking compensatory steps, with backward perturbation applied based on their mass and CoM velocity. Participants took an average of 6 trials to complete the motion successfully. Human learning was compared to simulated humanoid learning, showing faster and more reliable behavior. The humanoid learning behavior is slower and less reliable compared to human learning. Gravity constant was set to zero in the simulation to focus on motor adaptation. Evaluation was based on trajectory area (TA) measuring CoM deviation in sagittal plane. Positive sign indicates anterior direction, negative sign indicates posterior direction. The trajectory area mean and standard deviation for the training episodes of all participants are compared with simulation results of a humanoid, showing similar learning rates to real humans. The humanoid's trajectory area is adjusted for comparison, and a postural control task is simulated with external perturbation. The simulated humanoid learns to counterbalance external perturbations during standing up motion using a PD-controller with adjustable gains. The control policy is based on goal positions and velocities for the joints, with the option to learn control gains. The control policy for the simulated humanoid is based on goal positions and velocities for the joints, with the option to learn control gains. The policy is summarized in Table 2 with 9 or 17 parameters. Seven handcrafted features are used in simulations, including overall success and CoM deviation. Learning features simultaneously is left for future work. The humanoid is simulated for a maximum of 2 seconds with 1000 simulation steps, stopping if stand up fails or time limit is reached. The return of a roll-out J(\u03b8) is composed of balancing, time-dependent, and control costs. A comparison between our approach and standard Bayesian Optimization was conducted using specific features and seed points. Our approach showed improved rewards and success rates with fewer episodes. The impact of different features on learning behavior was also evaluated. Our proposed algorithm, HiBO, outperforms standard Bayesian Optimization in a complex humanoid postural control task. The average statistics show that all feature pairs generate better results than standard BO, with no significant difference in feature choice observed. The algorithm works best with 3 replay episodes, as more replays reduce the success rate. Additional experience replays were evaluated by adding noise to perturb policy parameters and features. Our proposed hierarchical BO algorithm, HiBO, outperforms standard BO in a complex humanoid postural control task. The choice of features and number of mental replay episodes were shown to impact performance. Comparison to real human learning behavior revealed similarities, with HiBO replicating rapid motor adaptation. Future work will focus on learning task relevant features in neural nets."
}