{
    "title": "Bkl7bREtDr",
    "content": "In many partially observable scenarios, Reinforcement Learning (RL) agents need long-term memory to learn optimal policies. Traditional memory methods fail in RL tasks due to environmental stochasticity and exploration. To address this, we propose AMRL models that use a memory module to summarize short-term context and aggregate prior states without order. This approach improves sample efficiency and resilience to noisy inputs, leading to a 19% increase in average return over a baseline in Minecraft and maze environments. In tasks requiring long-term memory, Deep RL has shown success in fully observable settings like Atari games. However, partial observability necessitates memory to recall past observations for the current state. Recent research has begun addressing this challenge, but effective learning in RL with long sequential dependencies remains a key issue. In Deep RL, effective learning with long sequential dependencies remains a challenge. Common approaches use memory components like LSTMs and GRUs originally developed for NLP tasks. New models are proposed to tackle the challenges of RL settings. In Deep RL, memory components like LSTMs and GRUs are commonly used to handle long sequential dependencies. However, in RL settings, the order of observations may not always be crucial, leading to decreased sample efficiency for LSTMs, stacked LSTMs, and DNCs. Solutions are proposed to address this issue. In Deep RL, memory components like LSTMs and GRUs are commonly used to handle long sequential dependencies. However, in RL settings, the order of observations may not always be crucial, leading to decreased sample efficiency for LSTMs, stacked LSTMs, and DNCs. Wayne et al. (2018) propose solutions to address this problem by introducing AMRL, which augments memory models like LSTMs with aggregators that are more robust to noise. LSTM fails in noisy settings, while AMRL-Max learns rapidly and maintains informative gradients over long horizons for sample-efficient learning in long-term memory tasks. In Section 5, the study evaluates how noise affects RL agents and the sample efficiency of AMRL and baseline methods through experiments in symbolic and 3D maze domains. Results show AMRL outperforms existing methods in solving long-term memory tasks faster. In Section 6, the characteristics of proposed and baseline models are analyzed to identify performance-affecting factors, confirming AMRL models are less prone to vanishing gradients. In the context of evaluating noise effects on RL agents and sample efficiency, AMRL models outperform existing methods in solving long-term memory tasks faster. The analysis of memory models shows that AMRL models can maintain signals over many timesteps, validating their effectiveness in solving long-term memory tasks. Previous work on external memory models in RL is discussed, highlighting the limitations of fixed-length memories in other models. The Neural Turing Machine (NTM) and Differentiable Neural Computer (DNC) have been used in RL settings, utilizing LSTM controllers and attention mechanisms to write memories into external memory. Unlike DNC, which stores the order of writes, we use order-invariant aggregation functions for benefits in noisy environments. DNC is chosen for baseline comparisons. Another approach in Deep RL is to learn a separate policy network to act as a memory unit. In Deep RL, learning a separate policy network to act as a memory unit is a common approach. This method involves training via policy gradient and is different from back-propagation through time (BPTT). The Low-Pass RNN uses a running average similar to our models but only propagates gradients through short BPTT truncation windows. LSTMs are used as a baseline in our work since we propagate gradients through the whole episode. Other studies propose the use of self-supervised auxiliary losses to force historical data to be utilized effectively. In Deep RL, memory units are often implemented using a separate policy network trained via policy gradient. This approach differs from back-propagation through time (BPTT) and utilizes self-supervised auxiliary losses to leverage historical data effectively. Model-based RL also incorporates memory modules for learning transition dynamics, while previous works focus on storing initial states in replay buffers. In supervised learning, memory techniques beyond LSTM and GRUs have been explored, such as using running averages over inputs. In Deep RL, memory units are often implemented using a separate policy network trained via policy gradient. This approach differs from back-propagation through time (BPTT) and utilizes self-supervised auxiliary losses to leverage historical data effectively. Model-based RL also incorporates memory modules for learning transition dynamics, while previous works focus on storing initial states in replay buffers. In supervised learning, memory techniques beyond LSTM and GRUs have been explored, such as using running averages over inputs. Al. (2014) use an RNN in conjunction with running averages. Mikolov et al. (2014) use the average to provide context to the RNN, all use an exponential decay instead of a non-decaying average. Various approaches have been proposed to extend the range of RNNs, which could be used in conjunction with our method as a drop-in replacement for the LSTM component. Other approaches for de-noising and attention are proposed but have prohibitive runtime requirements in RL settings with long horizons. Here, we limit ourselves to methods with O(1) runtime per step, considering a learning agent in a partially observable environment denoted as a Partially Observable Markov Decision Process. In a Partially Observable Markov Decision Process (POMDP), the agent operates in a partially observable environment with states, actions, rewards, observations, and a transition function. The agent's goal is to maximize the cumulative reward by selecting actions based on learned policies conditioned on observation trajectories. Our model AMRL utilizes a memory module to summarize context and enhance robustness to noise. It incorporates an LSTM base model for encoding observations and learning higher-level abstractions. The model AMRL uses a memory module to enhance robustness to noise by combining LSTM layers to learn higher-level abstractions. Noise can be introduced in various ways, such as observation noise and variance in the trajectory, affecting the output of the function. The model AMRL uses a memory module to enhance robustness to noise by combining LSTM layers to learn higher-level abstractions. To address noise sensitivity in the output, aggregators are introduced to cancel noise. Aggregators are commutative functions that combine previous encodings in a time-independent manner. Three aggregators (SUM, AVG, MAX) are considered, computed dynamically and in constant time, matching LSTM's computational complexity. The text discusses the design of aggregators in a memory module to enhance robustness to noise in the output of a model. The aggregators (SUM, AVG, MAX) are commutative functions that combine previous encodings in a time-independent manner. The signal-to-noise ratio (SNR) is proposed as a tool to assess the behavior of memory models in noisy settings. In this section, the SNR derived for the proposed aggregators is presented. The SNR decays linearly in time for SUM and AVG aggregators, while the MAX aggregator has a bound independent of time. These aggregators can also address vanishing gradients by acting as residual skip connections across time. The proposed aggregators show that the gradient does not decay over time and provide a path for the gradient to flow back to each time step. A novel memory model called SET is introduced, which has good properties in terms of SNR and gradient signal but lacks order-variant context maintenance. The proposed aggregators address the limitation of the SET model in maintaining order-variant context. They are combined with an LSTM model to learn order-dependent and order-independent information efficiently. Aggregators are applied to half of the memory, with the other half concatenated to preserve context. The final action is determined by feed-forward layers. The proposed aggregators, combined with an LSTM model, address the limitation of the SET model in maintaining order-variant context. They use straight-through connections to improve gradient flow by modifying the Jacobian of non-parametric aggregators. The AMRL-Avg model combines the AVG aggregator with an LSTM and straight-through connections, while the AMRL-Max model uses the MAX aggregator. In Section 5, the model components contribute to improved robustness to noise. Design choices are validated by analyzing gradients and SNR in Section 6. Experiments involve an RL agent interacting with maze-like environments, similar to previous studies by Bakker (2001) and Wierstra et al. (2009). Visual Minecraft mazes are used to evaluate memory in RL agents, as seen in Oh et al. (2016). Agents using AMRL models are compared in the experiments. In Oh et al. (2016), agents use AMRL models or baseline models as replacements for policy and value networks in maze tasks. The agent navigates a T-shaped maze, starting in a corridor and making binary decisions based on memory of the start indicator. The agent in TMaze tasks receives observations as vectors indicating the corridor's start or end and indicator color. Variants include TMaze Long (T-L) with deterministic decisions, TMaze Long Noise (T-LN) with added random variables, and TMaze Long-Short (T-LS). In T-LS, an additional short-term task is added to evaluate interference with a long-term memory task. An intermediate task is introduced where the agent must match a randomly sampled input to progress. The corridor length remains 100, and exploration is limited to a maximum episode length of 150. Multiple Minecraft environments are created using Project Malmo, with convolutional layers replacing fully-connected layers for efficient feature learning in the visual space. Discrete actions are used for movement in {North, East, South, West}. In Oh et al. (2016), discrete actions {North, East, South, West} are used in Minecraft environments for tasks like MC Long-Short (MC-LS) and MC Long-Short-Ordered (MC-LSO). The tasks involve navigating rooms with colored columns to reach a goal, with indicators to remember and order-dependencies to consider. The task involves navigating rooms with colored columns to reach a goal, with indicators to remember and order-dependencies to consider. Different approaches are compared, including AMRL-Max, AMRL-Avg, SET, LSTM, LSTM STACK, and DNC. The goal is to test robustness to noise while learning a short and long-term task in 10 rooms with a timeout of 200 steps. The study compares different approaches in navigating rooms with colored columns to reach a goal. Results show that LSTM and stacked LSTM learn slower than other methods in a noise-free task. Observation noise affects LSTM and DNC the most, while proposed models remain robust. Our proposed models, AMRL-Avg and ARML-Max, are robust to observation noise and maintain near-identical learning speed compared to the T-L task. In the T-LS task, our models achieve returns near the optimal 13.9, while baseline models only reach returns up to 10.4. The MC-LS task involves solving short-term tasks while retaining information about the initial indicator, with AMRLMax and AMRL-Avg learning the most rapidly. The DNC model learns more slowly but eventually reaches optimal performance. Our SET ablation does not learn the task, showing the importance of order-invariant features. In the experiment, the AMRLMax and DNC models outperform LSTM and aggregator models, with DNC achieving the best performance in a specific task. The optimal return is 10.2, and DNC benefits from shorter time dependency and lower observation noise. In contrast, LSTM and LSTM STACK baselines fail in this setting, highlighting the advantage of AMRL methods over LSTMs. In contrast to LSTM and LSTM STACK baselines, AMRL methods, particularly AMRL-Max, show strong performance under observation noise. The proposed methods using different aggregators with LSTMs do not suffer from vanishing gradients, as demonstrated in the analysis of AMRL characteristics and model choices. The final strength of the gradient for AMRL-Max and AMRL-AVG models is similar, outperforming LSTMs and DNC 2 in preserving gradient information. However, the drop in performance in noisy environments cannot be fully explained by gradients alone. Further analysis is suggested in the next subsection to understand the superior performance of MAX over AVG and SUM. In Section 3, SNR is quantified empirically for comparison across models. The SNR of a function over time is defined analytically for proposed aggregators, showing that AVG and SUM have the same linear decay, while Max has a lower bound independent of time. Empirical estimates of SNR are discussed in addition to analytical results. The empirical estimate of SNR for full AMRL models, including LSTM, compared to baselines, shows that AMRL-Max, Max, and baseline DNC have the highest SNR. LSTM and LSTM STACK have the lowest SNR with an exponential decay pattern. The decay for LSTM models is exponential, while other models show linear decay. SNR for LSTMs is time-dependent, unlike Max models and DNC. Models performing well on long-term memory tasks in noisy settings have informative gradients and high SNR over long time horizons. AMRL-Avg and AMRL-Max approaches outperform other methods in capturing key aspects of long-term memory tasks in noisy settings. The proposed AMRL-Avg and AMRL-Max approaches outperform all other methods, demonstrating the value of the ST connection. AMRL-Max improves over LSTM average return by 19% and outperforms DNC by 9% with fewer parameters. AMRL models are robust in noisy environments and can ignore irrelevant features in tasks. SNR and gradient strength are crucial for model success, with AMRL models achieving high SNR and strong gradients. The AMRL models achieve high SNR and strong gradients, leading to the highest empirical performance. The SUM model may face issues with growing activations, while the max aggregator is less susceptible to this issue. Analytical and empirical analysis validates the modeling choices in developing AMRL, providing insights into memory model learning performance. Our proposed AMRL approach demonstrates robustness to noise in RL settings, outperforming existing models significantly. Analyzing gradient strength and signal-to-noise ratio validates our model choices and explains the high empirical performance. This insight is valuable for future research on improving long-term memory in reinforcement learning. In future research, our models and analysis will enhance understanding and performance of memory models in RL. The issue of preventing interference between long-term and shorter-term memory tasks is highlighted. Integration of AMRL into models beyond LSTM could be explored. Our work emphasizes the need for approaches targeting long-term memory tasks in RL specifically. Additional details of tasks in the main paper are provided in this appendix. The experiments involve an agent in a corridor with a T-junction where the goal state is hidden. The agent in the experiment must choose between two directions at a T-junction, with the goal position randomized and indicated only at the start. The task involves minimizing noise, with the agent automatically moved to the next position in each step. The decision to move left or right at the T-junction is the focus, with the maze length fixed and the indicator moved to different locations. The agent receives a reward of 4 for successful navigation. The experiment involves the agent navigating a maze with a T-junction, receiving rewards for correct actions and penalties for incorrect ones. Observations are encoded as vectors with specific dimensions representing the agent's location. A noise feature is added to test robustness. This experiment is a variant of previous ones focusing on minimizing noise in navigation tasks. In a variant of experiments by Bakker (2001), discrete noise features are used to build up to a short-long decision task. The agent must recreate noise observations by taking actions matching the noise dimension. Rewards are given for correct actions. The agent starts on an elevated platform, falls into a maze, and enters a room on the north-south axis. The agent enters a room with columns blocking east and west movements. It must move onto a ledge and choose the correct direction to fall down for a reward. After navigating elevated steps, the agent must go right or left at the end for a final reward. In the Minecraft Maze environment, the agent receives rewards based on its actions, with a timeout of 200. The optimal score is 13.7, and the best possible for a memory-less policy is 10.2. There are 16 rooms in total, each requiring at least 6 steps to solve. The agent must navigate through different colored indicators to reach the end of the maze. The goal is to test if aggregating LSTM outputs provides an advantage over using an aggregator or LSTM alone in this environment with 10 rooms. In the Minecraft Maze environment, experiments were conducted to test the performance of DNC models compared to ST models. Gaussian noise was added to the RGB observation channels, and the experiment tested learning in a continuous observation space. The experiment used 10 rooms with optimal returns of 10.1 for the models. The experiments confirmed that the proposed models can learn order-dependent information through the LSTM path. The experiment modified the TMaze with indicators at opposite ends of the hallway, requiring the agent to remember both indicators in the correct order to solve the environment optimally. The experiments showed that while all approaches could learn order-dependent information and solve the task optimally, SET was unable to do so. By extending the environment with an additional indicator, policies conditioned on distant order-dependencies were explored. In the experiment, indicators were placed at positions 1 and 2 in the corridor to test sample efficiency. Results show AMRL outperformed baseline methods due to gradients decaying over long distances. AMRL-Avg and AMRL-Max achieved near-optimal rewards, while stacked LSTM performed worse. In a Minecraft environment, an agent must collect chickens for rewards in a 4 by 3 room. Chickens spawn in different locations and cannot move. The agent's goal is to collect as many chickens as possible to achieve near-optimal rewards. In a Minecraft environment, an agent collects chickens in a 4 by 3 room. Chickens spawn in different locations and cannot move. The agent's goal is to collect chickens for rewards. The agent must recall the initial floor color to make a decision. If correct, it receives a reward of 4, otherwise -3 and falls into lava. The agent has 5 actions: forward, backward, left, right, collect. All models plateau at a return near 4, although 8.8 is optimal. Models learned to remember room color but struggled to collect chickens. Training the best model, MAX, for 1.5 million steps showed perfect memory. Training the best performing model, MAX, for 1.5 million steps showed perfect memory and good chicken collection. Most mistakes were due to localization failure in a grey room. Chicken collection can be learned slightly faster without the memory-dependent long-term objective. The SNR was derived analytically for proposed aggregators in two different settings, including the weak signal setting where an initial impulse signal must be remembered. This setting is motivated by a POMDP scenario where an agent must recall a passcode to a door. In a POMDP scenario, an agent must recall a passcode to a door. The order of states between the passcode and the door is considered noise. Aggregators are commutative, allowing for the consideration of the set of states encountered by the agent. Analyzing the case where states are drawn i.i.d. from a stationary distribution \u03c1 is useful. The strong signal setting assumes a recurring signal without noise present simultaneously. Signal averaging is a common method to increase the signal of repeated measurements in scenarios like an agent recalling a passcode in a POMDP setting. The SNR of aggregators is derived for weak and strong signal settings, with linear improvement seen in the strong signal case. In signal averaging, linear improvement is observed in the strong signal setting. The SNR for sum aggregator is the same as that of the average, but inadequate for the max aggregator due to signal vanishing issues. In signal averaging, linear improvement is observed in the strong signal setting. The SNR for sum aggregator is the same as that of the average, but inadequate for the max aggregator due to signal vanishing issues. However, in the continuous setting, the gradient under observation noise remains similar to the non-noisy setting. The SNR of LSTM-based methods drops substantially when the signal is attenuated in the strong setting. In signal averaging, linear improvement is observed in the strong signal setting. The SNR of LSTM-based methods drops substantially due to signal attenuation, resulting in greater variance. Our models have a low variance SNR that is resilient to signal attenuation, with a stronger dependence on the number of prior signals than on the recency of the signal. The curr_chunk discusses the architecture and training details of a model with image input, including the use of convolution layers, a fully-connected layer, adam optimizer, PPO agent, and LSTM. Different versions of Python were used for experiments, and the LSTM maintains its hidden state. The curr_chunk does not overlap with the prev_chunk, which focuses on signal averaging and the SNR of LSTM-based methods."
}