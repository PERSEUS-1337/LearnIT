{
    "title": "B1gX8JrYPr",
    "content": "Sequence prediction models can be learned with various training algorithms, including maximum likelihood learning and reinforcement learning like policy gradient. Other algorithms such as data noising, RAML, and softmax policy gradient have also been developed. This paper introduces entropy regularized policy optimization, showing that different algorithms can be seen as special instances of this formulation. The differences lie in the reward function and two weight hyperparameters, allowing for a systematic comparison and new insights into algorithm design trade-offs. The new approach dynamically interpolates among algorithms for sequence prediction problems like machine translation, text summarization, and game imitation learning. It demonstrates superiority in experiments and learns the model in a scheduled way. The algorithm is based on supervised learning to maximize loglikelihood of example sequences. MLE training can suffer from compounding error, leading to states far from training data. RL techniques can overcome training/test discrepancy, but face challenges of poor sample efficiency and high variance. Various methods exist between MLE and RL paradigms, such as RAML adding reward-aware features. In this paper, a generalized entropy regularized policy optimization framework is presented, unifying various learning algorithms like MLE, RAML, data noising, and SPG. These algorithms can be seen as special cases of the framework, differing only in reward choice and weight hyperparameters. MLE is shown to use a Delta-function reward, returning 1 for exact matches and -\u221e otherwise. The paper presents a new interpolation algorithm that gradually expands the exploration space during training by annealing the reward function and weight hyperparameters. This approach leads to superior performance in tasks such as text generation, machine translation, text summarization, and game imitation learning compared to previous methods. The interpolation algorithm outperforms previous methods in sequence prediction models by maximizing loglikelihood of the next label. Reinforcement learning addresses training-test discrepancy by using models' predictions at training time. Various RL approaches like policy gradient and actor-critic have been applied for sequence generation. RAML is a hybrid algorithm between MLE and policy gradient, proposing the use of \u03b1-divergence as a combination of the two paradigms. The paper discusses different algorithms for sequence models, including learning-to-search paradigm, Scheduled Sampling, and policy optimization for reinforcement learning. Various researchers have introduced methods like relative entropy regularization and trust-region approach to improve learning efficiency. The paper introduces a trust-region approach for monotonic improvement in policy optimization algorithms. It discusses combining imitation learning with RL and presents an entropy-regularized policy optimization formulation as a generalization of previous methods. The framework is primarily formulated in the sequence generation context, with a reward function and weight hyperparameters defining the learning procedure. Varying these values results in a diverse set of algorithms. The paper presents a unifying view of popular algorithms in policy optimization, showing how they can all be seen as members in the same space. The framework is primarily focused on sequence generation, with the goal of learning a sequence from training examples. The approach can be extended to other settings like imitation learning in robotic and game environments. The curr_chunk discusses learning a sequence generation model p \u03b8 (y) and policy optimization in reinforcement learning. It aims to maximize the expected reward by evaluating the quality of generation y against the true y * using a reward function. Previous work has used entropy regularized approaches for stabilized training. The curr_chunk introduces a generalized variational formulation of ERPO for stabilized training in reinforcement learning. It involves maximizing the expected reward under a variational distribution q while minimizing the distance between q and the model p \u03b8, with maximum entropy regularization on q. This formulation is related to previous policy optimization approaches in RL literature. The curr_chunk discusses policy search and maximum entropy policy gradient methods in reinforcement learning, with a focus on the variational distribution q. It explains the EM procedure for optimizing q and \u03b8 iteratively, with insights on the closed-form solution for q in the E-step. The weight \u03b1 influences q to be close to p \u03b8, while the weight \u03b2 determines the temperature of the q softmax distribution. The curr_chunk discusses the ERPO formalism in reinforcement learning, focusing on the reward R and weight hyperparameters \u03b1 and \u03b2 > 0. Different algorithms in the ERPO family correspond to variations in these components, leading to different model update procedures. The ERPO family of algorithms in reinforcement learning corresponds to variations in the reward R and weight hyperparameters \u03b1 and \u03b2 > 0. Different algorithms are represented as points in a space spanned by these components. Softmax Policy Gradient (SPG) is one such algorithm that combines reward and policy to improve sampling quality. The ERPO family of algorithms in reinforcement learning involves variations in reward R and weight hyperparameters \u03b1 and \u03b2 > 0. SPG is an algorithm that combines reward and policy to enhance sampling quality. The MLE algorithm uses a special reward to simplify sampling, sacrificing exploration during training. Sequence Tutor also utilizes an MLE-trained policy as a prior to guide target policy learning in an RL framework. In this section, the MLE algorithm is connected to the ERPO formalism, allowing for analysis of learning behavior in terms of exploration efficiency. Variants of MLE like RAML and data augmentation are discussed. MLE is widely used in sequence generation for maximizing data log-likelihood. The algorithm recovers its objective from Eq.(2) using specialized reward and weight values, with a \u03b4-function reward giving a unit reward only for exact matches with true data. The MLE algorithm is a member of the ERPO family, with the conventional MLE objective equivalent to setting ERPO components to (R = R \u03b4 , \u03b1 \u2192 0, \u03b2 = 1). This configuration ensures that the M-step maximizes the log-likelihood of real data examples by giving a negative infinite reward to any sample that does not match the given data exactly. The MLE algorithm in the ERPO family compares with other RL algorithms by reformulating in the unifying ERPO form. The \u03b4-reward restricts exploration to training data, leading to a brittle model prone to errors. However, it simplifies sampling from a high-quality distribution over the sequence space, making MLE implementation efficient. In contrast, task-specific rewards like BLEU in standard policy optimization are more diffuse. Task-specific rewards like BLEU used in standard policy optimization are diffused, allowing exploration in a broader space. However, this can lead to inefficient exploration due to low-quality samples in the huge sequence space. Seeking a middle ground between exploration and computation efficiency is essential to combine the advantages of both approaches. Previous work has proposed variants of the vanilla MLE, which can be canonicalized in the ERPO framework to enhance learning behaviors. Data Noising is also discussed in this context. Data noising is a model regularizing technique that involves adding noise to training data. This approach, which can be expressed in the ERPO framework, expands exploration surrounding training examples to improve model robustness. Reward-Augmented Maximum Likelihood (RAML) introduces task-specific metrics into training by perturbing data based on a reward distribution. This allows for a larger exploration space surrounding training examples, improving model robustness compared to vanilla Maximum Likelihood Estimation (MLE). The approach of Reward-Augmented Maximum Likelihood (RAML) introduces task-specific metrics into training by perturbing data based on a reward distribution, allowing for a larger exploration space surrounding training examples. The classic policy gradient algorithm has also been used for sequence prediction, and can be connected to the unifying ERPO with moderate approximations. Ranzato et al. proposed a mixing training strategy that anneals from MLE training to policy optimization, which is a special case of a more general interpolation algorithm. The framework presented in the context of sequence generation can be extended to other settings. The formulation extends to game environments where y is a sequence of actions and states. GAIL uses adversarially induced reward R for training the policy. The new interpolation algorithm can improve vanilla GAIL. Previous work connects MLE and policy gradient as minimizing KL divergences. Our framework reformulates a comprehensive set of algorithms. Our framework reformulates a comprehensive set of algorithms for sequence prediction, providing new insights on exploration and efficiency. A new interpolation algorithm is proposed to start learning from a restricted yet efficient configuration and gradually expand. The framework reformulates algorithms for sequence prediction, emphasizing exploration and efficiency. A new interpolation algorithm is introduced to start learning from a restricted configuration and gradually expand, reducing training/test discrepancy. The interpolation strategy involves annealing from using a restricted \u03b4-reward to task reward and sampling by both reward and p \u03b8. The interpolation algorithm introduced in the framework reformulates algorithms for sequence prediction by gradually increasing \u03bb 1 and \u03bb 2 and decreasing \u03bb 3. This converts the energy-based model q(y) to a mixture of experts, making sampling easier. The algorithm allows tokens in a sequence to be sampled from different components in a mixed way, resembling the bang-bang rewarded SPG method. The interpolation algorithm introduced in the framework reformulates algorithms for sequence prediction by gradually increasing \u03bb 1 and \u03bb 2 and decreasing \u03bb 3. This converts the energy-based model q(y) to a mixture of experts, making sampling easier. The algorithm allows tokens in a sequence to be sampled from different components in a mixed way, resembling the bang-bang rewarded SPG method. In game imitation learning, the interpolation algorithm can be applied by plugging it into the GAIL framework to replace the standard RL routine for policy update. The annealing schedule is constrained due to agent interaction with the environment, gradually transitioning from data sampling to model/reward sampling to increase exploration until converging to standard RL. Experiments validate the superiority of easy-to-hard training over vanilla GAIL. Experiments validate the superiority of easy-to-hard training over vanilla GAIL. An annealing strategy similar to Ranzato et al. (2016) is applied in the GAIL learning setting, showing better performance than the restricted approach. The interpolation algorithm is evaluated in text generation and game imitation learning, with experiments conducted using 4 GTX 2080Ti GPUs and 32GB RAM. The code link is provided in the submission for future release upon acceptance. The study utilizes the Transformer model with specific training details and parameters. The dataset used is IWSLT2014 German-English, with a shared vocabulary size of 73,197. Various methods are compared using test-set BLEU scores, including MLE, RAML, MIXER, Scheduled Sampling, and Self-critic. The study compares different approaches like Scheduled Sampling and Self-critic to improve performance over vanilla MLE. The interpolation algorithm shows the best results, outperforming MLE by 1.36 BLEU points. Comparing with \"MIXER-alike Aneal\" validates the effectiveness of the proposed generalized annealing approach. The study compares different approaches like Scheduled Sampling and Self-critic to improve performance over vanilla MLE. The proposed more generalized annealing is superior to the restricted version. Other work explores various network architectures for machine translation, which is orthogonal and complementary to the learning algorithms. The model used is an attentional sequence-to-sequence model with specific configurations. The English Gigaword corpus is used for text summarization, following pre-processing guidelines. The study compares different approaches like Scheduled Sampling and Self-critic to improve performance over vanilla MLE. The proposed more generalized annealing is superior to the restricted version. The resulting dataset consists of 200K/8K/2K source-target pairs in train/dev/test sets. The proposed interpolation algorithm achieves the best performance on all three ROUGE metrics. Our method consistently provides the best results in text summarization. The interpolation algorithm is applied in GAIL with MuJoCo environments. Expert demonstrations are generated by running PPO under true reward functions. The study compares different approaches like Scheduled Sampling and Self-critic to improve performance over vanilla MLE. The proposed interpolation algorithm achieves better performance in imitation learning with varying numbers of demonstrations. The approach anneals from MLE mode to RL mode, making better use of data examples and steadily achieving better performance. The framework is based on a generalized entropy regularized policy optimization formulation. The study introduces a new interpolation algorithm that consistently improves machine translation, text summarization, and game imitation learning. It addresses exposure bias by utilizing the policy gradient algorithm and reveals the relation between the ERPO framework and policy gradient algorithm. The proposed algorithm shows improvement by specifying reward and weight hyperparameters, leading to systematic understanding and comparison across algorithms. The MIXER algorithm incorporates an annealing strategy that mixes between MLE and policy gradient training. It uses the first m tokens for evaluating MLE loss and then switches to policy gradient objective. The m value decreases as training progresses. The MIXER algorithm combines MLE and policy gradient training by using an annealing strategy. It switches from MLE to policy gradient objective after the first m tokens, with m decreasing during training. The interpolation algorithm follows a restricted annealing strategy for token-level hyperparameters (\u03bb1, \u03bb2, \u03bb3). Data pre-processing follows specific guidelines for machine translation and text summarization datasets. In the game imitation learning task, 50 state-action pairs are randomly sampled in each trajectory as demonstrations. At least 2,048 state-action pairs are collected every training iteration, with 1,000 iterations for every model in every environment."
}