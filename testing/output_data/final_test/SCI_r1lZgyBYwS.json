{
    "title": "r1lZgyBYwS",
    "content": "Fully convolutional VAE models trained on 32x32 ImageNet can generalize well to larger photographs without model changes. This property is applied to lossless compression, scaling the VAE-based 'Bits-Back with ANS' algorithm for compression of full-size ImageNet images. Craystack, an open-source library for lossless compression using probabilistic models, is released along with all compression results. Bits back coding is a method for lossless compression using a latent variable model, achieving an expected message length equal to the variational free energy. Bits back coding is a method for lossless compression using a latent variable model, achieving an expected message length equal to the variational free energy. HiLLoC, a new compression method, outperforms other codecs on color images from the ImageNet test set, demonstrating a significant speedup through vectorization. The text discusses the speedup achieved through vectorization in lossless compression using the BB-ANS algorithm. ANS is an entropy coder designed for compressing sequences of symbols, with a compressed message length equal to the negative log-probability of the sequence plus a constant overhead. The ANS coding algorithm guarantees near-optimal compression rates for long sequences by utilizing push and pop operations. Push adds a symbol to a message, while pop decodes a symbol and recovers the original message. ANS is known as a last-in-first-out coder due to the need to pop symbols in the reverse order they were pushed. These operations rely on a probabilistic model of symbols represented by a probability mass function. The ANS algorithm uses a probabilistic model of symbols with a probability mass function. Pushing a symbol increases message length, while popping reduces it. The pop operation can sample symbols from a different distribution, essential for bits back coding. The source of randomness for sampling comes from the message data. The BB-ANS algorithm specifies an indirect way to encode data x by pushing and popping it onto the stack, using prior, conditional, and posterior distributions from a latent variable model. The sender can perform three steps to encode x, without needing access to the marginal distribution p(x). The BB-ANS algorithm outlines a method to encode data x by pushing and popping it onto the stack using prior, conditional, and posterior distributions. The sender can encode x in three steps, and the expected message length for x is the negative evidence lower bound (ELBO), a common training objective. The BB-ANS algorithm uses the negative evidence lower bound (ELBO) as a training objective for latent variable models. Encoding data involves decoding a latent variable z from a buffer of random data, known as 'bits back'. Chaining can be used for compressing multiple samples. The techniques introduced scale up BB-ANS by using a fully convolutional VAE that can model images of any size. Empirically, a VAE trained on 32 \u00d7 32 images performs well as a model for larger images, leading to good compression rates. Lossless compression of arbitrary sized images is achieved using this VAE. The implementation of lossless compression for arbitrary sized images using a VAE in the Craystack library includes vectorization techniques to improve runtime performance. The vectorization process involves expanding the ANS stack head from a scalar to a vector and interleaving the input/output bit stream. This approach minimizes the compression rate overhead and allows for scalability to large ImageNet images. In the Craystack library, the BitKnit technique is used for lossless compression of images with a VAE. The vectorized encoding process optimizes compute time by utilizing neural net inference efficiently. The ANS coder is generalized using Numpy's n-dimensional array view interface, allowing for flexible shaping of the stack head to match data structures. This approach simplifies the implementation of VAE coding and enables concurrent operations during encoding. The vectorized encoding process in Figure 2 optimizes compute time by utilizing neural net inference efficiently. To use BB-ANS with latent variable models, a discretization scheme for hierarchical VAEs with multiple layers of latent variables is proposed. This scheme builds upon previous work on static discretization for VAEs with negligible impact on compression rate. Proposing a dynamic discretization approach for hierarchical VAEs with multiple layers of latent variables, where the discretization varies based on the context of the latents being coded. This method differs from static schemes by partitioning based on conditionals in the prior rather than marginals, allowing for more flexibility in encoding. Proposing a dynamic discretization approach for hierarchical VAEs with multiple layers of latent variables, based on conditionals in the prior. This method allows for more flexibility in encoding compared to static schemes. The implementation doesn't require calibrating to samples but restricts model structure to top-down posterior inference. This contrasts with the bottom-up, Markov restriction of Bit-Swap. Further details on dynamic discretization implementation are provided in the appendix. Graphical models of generative and inference models with HiLLoC and Bit-Swap are shown in Figure 3. The Bit-Swap technique involves a 3 layer latent hierarchy for encoding, requiring a top-down sampling order. Inference must be done bottom up, without skip connections, following a Markov chain on the latent variables z1 to zL. In our experiments, we do not use skip connections like in very deep models. Instead, we propose a simpler method to address the high cost of coding a small number of samples with BB-ANS by using a different codec for the first samples. We use the 'Free Lossless Image Format' (FLIF) to build up a buffer of compressed data before applying the BB-ANS algorithm. The codec chosen for compression outperformed others, with the amount of data needed to start compression proportional to image size. Using HiLLoC, partitioning images into smaller patches allows for quicker utilization of superior coding. Larger patches generally have better coding rates than smaller ones, leading to increased patch size as more images are compressed. Therefore, the size of image patches being compressed with HiLLoC increases as more images are compressed and the data buffer grows. The compression process starts with 32x32 patches, then moves to 64x64 and 128x128 before coding full-size images directly. This gradual improvement in coding rate from FLIF towards HiLLoC on full images is achieved by compressing edge patches of different shapes. Only 5 ImageNet images are compressed using FLIF before switching to compressing 32x32 patches with HiLLoC implemented using a ResNet VAE. The RVAE with 24 stochastic hidden layers utilizes skip connections for effective training. Trained on ImageNet 32, it was evaluated on various test sets including ImageNet64 and CIFAR10. HiLLoC achieves competitive compression rates on all benchmarks. HiLLoC achieves competitive compression rates on all benchmarks, including state-of-the-art performance on full-size ImageNet images. Despite changes in distribution, it maintains high compression quality, outperforming standard algorithms due to the simplicity and redundancy of datasets like CIFAR. The theory is supported by the performance of compression algorithms, which show better results on CIFAR images compared to ImageNet. Larger images exhibit a surprising improvement in compression rates, possibly due to the lower ratio of edge pixels to interior pixels, reducing uncertainty. Vectorization significantly speeds up the compression process for images of all sizes. Our experiments demonstrate HiLLoC as a bridge between large scale latent variable models and compression. Using simple variants of pre-existing VAE models, we show that bits back coding can compress well with large, complex models. Further research is needed in model structure search, optimizing compression rate, encode/decode time, and memory usage. Latent dimensionality is crucial for HiLLoC as compute time and memory usage scale with it. Weight compression is also important for storing/transmitting the model. Future work should focus on discovering the best training datasets for coding generic images, including the potential benefits of training on larger images or images of varying sizes. Batch compression of images using masking could be explored, similar to how recurrent neural nets process samples of different lengths. Additionally, applying state-of-the-art fully observed auto-regressive models like WaveNet for lossless audio compression shows promise. The performance of models like WaveNet for lossless audio compression and PixelCNN++ for images is hindered by slow sampling speed, especially in scenarios where decoding is done frequently. Dynamic programming and altering model architecture can help mitigate this issue, but sampling/decompression remains slower than with VAE models on parallel architectures. Fully observed models and flow-based models do not require bits back coding, avoiding the one-off cost of starting a chain. HiLLoC is an extension of BB-ANS to hierarchical latent variable models, showing good performance with large models. The implementation is open-sourced, including the Craystack package for lossless compression. Fully convolutional VAEs can generalize well to different datasets, achieving superior compression rates with HiLLoC for images of arbitrary size. After discretizing the latent space, the latent variable at layer l can be treated as an index into intervals created by the discretization. Notation is introduced for pushing and popping according to a discretized version of the posterior. The discretization splits the latent space into equal mass intervals under a distribution. The mass of an interval is the CDF at the upper bound minus the CDF at the lower end. The discrete values z l represent these intervals. The latent variable at layer l is represented by discrete values z l, which are reconstructed from interval indices i l. The prior distributions p(z k |z k+1:L ) are crucial for calculating z l+1:L and discretizing the latent space. The prior distribution over interval indices i l is uniform, allowing for parallel pushing/popping according to the prior. The encoding and decoding procedures with a hierarchical latent model and dynamic discretization are detailed in Table 3. The BB-ANS encoding and decoding operations for a hierarchical latent model with l layers are detailed in Table 3. A codec is described for compressing images of arbitrary size, where the encoder adds image dimensions to the compressed data stream for decoding. The resizing procedure for the vectorized ANS coder ensures the coder's top matches the image and latent sizes. The codec details are provided in Table 4 for efficiency. The resizing procedure for the vectorized ANS coder involves 'folding' the coder's top to halve/double the number of individual ANS coders, making resizing cost logarithmic. Compression rates on the ImageNet validation set are shown in Table 5. The PixelVAE model allows for parallel compression across pixels but decompression is not parallelizable. The time complexity of decompression in the vectorized ANS coder is linear in the number of pixels, making it slow for most image sizes. To reduce compression rate overhead, a technique from the BitKnit codec is used. Folding the stack head vector efficiently forms the final output message at the end of decoding. The final output message in decoding is formed by folding the stack head vector efficiently. The number of encode steps needed is logarithmic in the size of the stack head. Overhead from vectorization can be reduced by gradually growing the stack head vector length as more random data is added. The RVAE architecture is a hierarchical latent model trained by maximizing the evidence lower bound (ELBO) on the log-likelihood. It includes skip connections in both the generative and inference models, with a top-down inference model used for dynamic discretization. The KL terms are individually clamped using an optimization technique known as free bits. The RVAE architecture is a hierarchical latent model trained by maximizing the evidence lower bound (ELBO) on the log-likelihood. It includes skip connections in both the generative and inference models, with a top-down inference model used for dynamic discretization. The KL terms are individually clamped using an optimization technique known as free bits. The hierarchy consists of ResNet blocks with bottom-up and top-down activations, parameterized as diagonal Gaussian distributions for latent z and discretized logistic distribution for x. The activations in each ResNet block are a combination of stochastic and deterministic features from previous blocks. The ResNet blocks combine stochastic and deterministic features from previous layers through convolutions and skip connections. Each latent layer has 32 feature maps with spatial dimensions half of the input. This configuration follows the default hyperparameters from the original implementation."
}