{
    "title": "SkeJ6iR9Km",
    "content": "Variational auto-encoders Variational auto-encoders (VAEs) address issues of disperse and non-interpretable latent codes by incorporating sparsity through a Spike and Slab prior distribution. This new approach allows for truly sparse representations in the latent space of VAEs, improving performance on classification tasks. Variational auto-encoders (VAEs) with sparse representations outperform standard VAEs on classification tasks like MNIST and Fashion-MNIST. Sparse elements capture understandable sources of variation, addressing the challenge of making representations meaningful and efficient in representation learning. Sparse coding provides an elegant solution for linear mappings by inducing a sparse representation space, encouraging the use of minimal non-zero elements to describe observations. This efficient representation has been utilized in various learning and recognition systems for easier interpretation and increased efficiency in tasks like classification and clustering. The aim is to extend this capability to non-linear probabilistic generative models. The text discusses the extension of sparse coding capability from linear mappings to non-linear probabilistic generative models using a new variation of the classical VAE. By incorporating a sparsity inducing prior in the latent space and a discrete mixture recognition function, the model can efficiently recover sparse, informative, and interpretable representations. Experimental results on benchmark datasets demonstrate the model's ability to adjust to data complexity and recover meaningful latent representations. The text discusses the extension of sparse coding capability to non-linear probabilistic generative models using a new variation of VAE. It demonstrates the ability to automatically discover sources of variation in observations and outperform VAE in classification experiments. Additionally, sparse elements in retrieved codes control recognizable features in generated observations. The text discusses sparse coding as a generative model with latent variables and observation noise. It involves minimizing an objective function with data matrix X, basis matrix B, sparse codes matrix Z, and a sparsity inducing function. The model is described with prior and likelihood distributions. The text discusses variational EM inference for maximizing the marginal likelihood in a generative model with latent variables and observation noise. Previous work has shown limitations in scaling to more complex models with non-linear transformations and a large number of input vectors. The text discusses generalizing sparse coding to non-linear transformations on Riemannian manifolds, using variational auto-encoders (VAEs) for unsupervised efficient coding. VAEs aim to maximize the marginal likelihood by optimizing decoding and encoding parameters. Variational sparse coding models sparsity in the latent space with a Spike and Slab prior distribution. The text discusses using variational auto-encoders (VAEs) for efficient coding by maximizing marginal likelihood. VAEs introduce a recognition model to estimate an evidence lower bound for the true posterior distribution. The recognition function in variational auto-encoders is a parametric distribution estimated by a neural network. The ELBO consists of a prior term minimizing KL divergence and a reconstruction term maximizing data likelihood. The VLB is maximized with respect to model parameters through analytical and stochastic optimization. Sparse PDFs are closely related to sparsity, with models like BID21 and BID30 using Stick-Breaking Process and Indian Buffet Process priors to enable stochastic dimensionality in the latent space. However, these models do not result in truly sparse representations as the same elements are set to zero for every observation. Other models, like Rolfe (2016) and van den BID34, incorporate discrete variables in the latent space to capture both discrete and continuous sources of variation in observations. Van den BID34 utilized variational inference with a learned discrete prior and recognition function to create sparse latent spaces. The Epitomic VAE BID39 introduced a deterministic selection variable to prevent over-pruning and achieve sparse latent variables. However, this method is variational only in continuous variables and does not induce sparse statistics in the latent space. The proposed approach aims to induce sparsity in a continuous latent space through a sparse PDF using VAEs for approximate variational inference. The Spike and Slab probability density prior is used to model sparsity in the latent space by assigning point mass to null elements. This distribution has been utilized in various Bayesian sparse inference models. The Spike and Slab distribution is used in Bayesian sparse inference models to induce sparsity in a continuous latent space. It defines two variables, a binary spike variable and a continuous slab variable. The prior probability density over the latent variable is chosen to assume data is generated from sparse vectors. The recognition function is a discrete mixture model with distribution parameters obtained from a neural network. The recognition function neural network outputs \u00b5 z,i,j , \u03c3 2 z,i,j, and \u03b3 i,j based on input x i. It allows the posterior to match the prior and encode information in the latent space. The model uses Spike and Slab variables to control Gaussian moments and Spike probabilities independently. The goal is to perform approximate variational inference similar to a standard VAE setting. The ELBO for approximate variational inference in the VSC model involves maximizing a lower bound with Spike and Slab prior and a discrete mixture recognition function. The regularisation component of the lower bound corresponds to the negative KL divergence between the discrete mixture and the Spike and Slab PDF, split into four cross entropy components in each latent dimension. The ELBO for approximate variational inference in the VSC model involves maximizing a lower bound with Spike and Slab prior and a discrete mixture recognition function. The lower bound includes four cross entropy components in each latent dimension, involving Gaussian-discrete and Dirac Delta-discrete mixture components. The cross entropy terms reduce to weighted Gaussian-Gaussian entropy terms when point mass contributions vanish. The proof shows that the point mass contribution to the cross entropy between a Gaussian and a discrete mixture distribution is infinitesimal. The second and fourth terms involve the cross entropy between a Dirac Delta function and a discrete mixture distribution. The prior term of the VSC lower bound involves negative KL divergences between distributions of Slab and Spike variables, encouraging matching with prior probabilities. The reconstruction term also contributes to the lower bound. The reconstruction term of the VAE lower bound is maximized stochastically using a reparameterization trick to draw samples from the recognition function. A continuous relaxation of binary variables is used, along with auxiliary noise variables, to parametrize draws of the Spike variables through a non-linear binary selection function. The VSC model combines prior and reconstruction terms to estimate the VSC lower bound. The ELBO is maximized through gradient ascent to train the model on image datasets. The VSC model is tested on image datasets like MNIST and fashion-MNIST, as well as CelebA faces dataset. Results are shown in appendices and evaluated in different settings, including ELBO evaluation, classification using latent variables, and qualitative assessment of latent space. The VSC model is evaluated on image datasets like MNIST and fashion-MNIST, as well as CelebA faces dataset. Experimental conditions are detailed in appendix D, with ELBO evaluation at varying latent dimensions and prior sparsity levels. Results are presented in figure 2, showing high ELBO for the correct choice of latent space dimensions. The VSC model's performance is compared to the standard VAE in terms of ELBO values at varying latent space dimensions. The VSC remains stable with larger latent spaces, while the VAE drops in performance with larger latent spaces. The VSC reaches a lower maximum ELBO but is more stable with more latent dimensions compared to the VAE. The VSC model, compared to VAE, slightly under-performs but remains stable with larger latent spaces. VSC focuses on recovering information-rich latent codes, using a sparse prior to activate non-zero elements only when needed. Training VAEs and VSCs at varying latent dimensions for 20,000 iterations, with VSC showing stability and efficiency in representation. The VSC model outperforms VAE in recovering efficient codes for large latent spaces without needing to specify an optimal size. It marginally outperforms VAE in classification performance by activating only necessary variables for each observation. The VSC model discovers and encodes variations in data into few non-zero elements, outperforming VAE in larger latent spaces. Classification accuracy is evaluated at varying sparsity levels, and the interpretation of sparse codes is qualitatively examined using Fashion-MNIST and CelebA datasets. The VSC model encodes variations in data into few non-zero elements, outperforming VAE in larger latent spaces. Altering individual non-zero components controls interpretable features in the generated observations. Sparsity leads to higher interpretability expectations due to conditional activation of certain dimensions. The recognition function defines a low-dimensional sub-space by activating only a few non-zero elements that control necessary features for describing observations. The VSC model uses sparse encodings to define a sub-generative model for objects, activating different subsets of non-zero elements for various observations. Interpolation between objects in the latent space allows for examining individual non-zero elements. An example of interpolation between a shirt and a t-shirt in the Fashion-MNIST dataset is shown in FIG1. The study introduces a framework to induce sparsity in the latent space of VAEs, enabling approximate variational inference with complex sparse coding models. The resulting encoders produce efficient sparse codes that are beneficial for classification tasks and have interpretable non-zero components. Inducing sparsity in generative models shows promise for future research. Inducing sparsity in the latent space of generative models is a promising approach for obtaining useful codes and interpretable representations in VAEs. The VSC model aims to model diverse data by sparsely populating large latent spaces, isolating key features. The architecture includes neural networks for likelihood and recognition functions. The VSC model uses neural networks to compute the mean and log variance for likelihood and recognition functions. Different network architectures are used for training with different datasets. The recognition function outputs the mean, log variance, and log Spike probabilities vector. The elements of the Spike probabilities vector are constrained between 0 and 1. The VSC model utilizes neural networks to compute mean and log variance for likelihood and recognition functions. Different network architectures are used for training with various datasets. The recognition function outputs mean, log variance, and log Spike probabilities vector, with elements constrained between 0 and 1. Samples in the latent space are drawn according to a specific equation. The VSC model uses neural networks to compute mean and log variance for likelihood and recognition functions. Different network architectures are employed for training with various datasets. The recognition function outputs mean, log variance, and log Spike probabilities vector. Samples in the latent space are drawn according to a specific equation. The cross entropy components in each latent dimension reduce to Gaussian-Gaussian entropy terms as point mass contributions vanish. The point mass contribution to the cross entropy between finite density distributions is infinitesimal. The VSC model utilizes neural networks to compute mean and log variance for likelihood and recognition functions. Different network architectures are used for training with various datasets. The cross entropy components in each latent dimension reduce to Gaussian-Gaussian entropy terms as point mass contributions vanish. The KL divergence D KL N (z i,j ; \u00b5 z,i,j , \u03c3 2 z,i,j ) || N (z j ; 0, 1) has a simple analytic form. The VSC model uses neural networks to compute mean and log variance for likelihood and recognition functions. The KL divergence between distributions of Slab and Spike variables is computed analytically, revealing a linear combination of their divergences. The function T(y) is a scaled Sigmoid function used for differentiability in Spike variable reparametrisation. A warm-up strategy is employed to gradually increase the value of c during training. The aim is to find a function f(\u03b7, \u03b3) to express a binary variable w as a function of noise variable \u03b7 and \u03b3. The function T(y) is a scaled Sigmoid function used for differentiability in Spike variable reparametrisation. The function f(\u03b7, \u03b3) expresses a binary variable w as a function of noise variable \u03b7 and \u03b3, with the noise variable drawn from a distribution independent of \u03b3 i,j. The probabilities of w being 1 or 0 are linear in \u03b3 i,j, leading to a reparametrisation illustrated in figure 6. The function T(y) is approximated with a scaled Sigmoid for differentiability in Spike variable reparametrisation. In the implementation, the constant c is gradually increased from 50 to 200 during training. VAEs and VSCs used have a 400-dimensional hidden layer between observations and latent variables. The VAEs and VSCs used have a 400-dimensional hidden layer between observations and latent variables. MNIST and Fashion-MNIST datasets are encoded in 200-dimensional spaces with different values. CelebA dataset consists of 200,000 color images of celebrity faces. In high dimensional latent spaces, both MNIST and Fashion-MNIST datasets were encoded with varying prior Spike probability \u03b1 to measure aggregate posterior sparsity. Results showed that for larger \u03b1 values, codes retained expected sparsity, while lower \u03b1 values led to departure from induced sparsity due to recognition function activation. ELBO evaluation results were reported, along with behavior of lower bound at varying prior sparsity \u03b1. In high dimensional latent spaces, MNIST and Fashion-MNIST datasets were encoded with varying prior Spike probability \u03b1. Results showed that as \u03b1 decreased, ELBO increased due to smaller sub-spaces needed for representation. However, at very low \u03b1, ELBO decreased again as the number of dimensions activated by the recognition function was too high. Classification accuracy was measured for different values of \u03b1, and results were displayed in Figure 11. The results of classifying with a one layer network at varying prior Spike probability \u03b1 are displayed in Figure 11. VAEs can produce realistic samples from the prior through ancestral sampling, while VSC can generate good synthetic samples conditioned on specific features identified in observations. Samples directly from the sparse prior may not be as realistic, as not all combinations of sparse features are feasible. The recognition function defines a sub-space over active dimensions, allowing for variability in recognized features. Ancestral sampling with a VAE and VSC on Fashion-MNIST dataset can result in unnatural combinations of features. Conditional sampling in a VSC trained on Fashion-MNIST generates distinct types of objects based on activated dimensions."
}