{
    "title": "HJe4ipEKDB",
    "content": "A novel framework is proposed to generate clean video frames from a single motion-blurred image. In this work, the focus is on video restoration from a blurred image, a more challenging task than recovering a single image. The framework involves an encoder-decoder structure with spatial transformer network modules to restore a video sequence and its motion. Various models of the network are analyzed, and the effectiveness is demonstrated through experiments on different types of datasets. Capturing images involves exposure time to gather photons, leading to motion blur in high-speed videos. Motion blur is undesirable in vision applications like image editing, visual SLAM, and 3D reconstruction. Image deblurring aims to restore sharp images from blurred ones, a challenging task due to the unknown blur kernel. The blur kernel used for deconvolution is often assumed to be unknown, with previous studies simplifying the estimation by assuming a uniform blur. However, real-world scenarios often involve spatially variant blur patterns, such as those caused by out-of-plane camera rotation or dynamic motion blur. While previous literature focuses on recovering sharp images from blurred ones, we address the more challenging task of video restoration from a blurred image, requiring both content and motion prediction. In the context of video restoration from a blurred image, previous approaches focused on single frame restoration by estimating motion. However, recent studies have aimed at generating a clean sequence of images from a single motion-blurred input. The proposed framework aims to generate a sequence of deblurred images from a single motion-blurred input using a single encoder-decoder structure with STN and LW modules. Evaluation was done on rotation blur and dynamic blur datasets, showing promising results in restoring images with different motion patterns. Our method is robust and outperforms a competing approach in restoring clean video frames from motion-blurred images. We demonstrate transferability and propose a simpler model with negligible performance trade-off. Key contributions include a novel unified architecture, stable network training, and favorable performance compared to existing methods. Our model performs favorably against competitors, showing robustness to heavy blurs. Image deblurring focuses on motion blur, an ill-posed problem due to unknown blur kernels. Early deblurring studies assume a single blur kernel globally applied to images, often modeled as a probabilistic maximization problem. Blur kernel estimation in image deblurring is improved by utilizing natural image priors and various regularization techniques. Single blur kernel estimation methods are effective for shift-invariant blur but fail for non-uniform blur caused by factors like camera rotation or moving objects. Recent approaches incorporate geometric information of camera motion and deep learning networks for better deblurring results. Recent deep network-based methods have been proposed to handle general blur patterns without the uniform blur assumption. These methods include multi-scale deep networks with multi-scale loss and spatially variant neural networks. Jin et al. (2018) also introduced a framework using multiple deep networks to extract a video sequence from a single motion-blurred image. Despite the success of deep networks in generating image sequences from blurred images, there are still some limitations in their approach. Purohit et al. (2019) proposed a two-step strategy using three networks to generate a video from a motion-blurred image. They utilized a video autoencoder for pretraining to learn motion and frame generation, followed by a motion disentangle network to extract motion from the blurred image. Their approach requires a clean middle frame generated in advance, but error propagation occurs when frames are generated sequentially from the middle frame. Our approach differs from previous works by running in an end-to-end manner within a single training stage without error propagation across frames. Blurry image datasets are commonly generated by combining sharp images, and motion blur sources include camera shake and object motion. Rotation blurred image datasets are created using the SUN360 panorama dataset. The SUN360 panorama dataset is used to create blurred images by projecting panoramas onto a unit sphere and capturing synthetic images with a virtual camera. The camera is rotated using a random rotation matrix to generate initial images. To generate realistic blurred images, a camera is rotated using a random rotation matrix to capture initial and final images. A quaternion spherical linear interpolation technique is used for rotation. The number of intermediate images is adjusted based on the rotation magnitude, with a linear relationship between the number of frames and rotation magnitude. 26,000 training and 3,200 test images are generated from 1000 panoramic images. In order to create realistic blurred images, a GoPro high-speed video dataset is utilized to generate blurry images by averaging consecutive frames. The network structure includes spatial transformer networks in an encoder-decoder setup. The model outputs deblurred frames, with a focus on the middle frame. The network is trained using three loss terms for sharp image generation and stable training: multi-scale photometric loss, transformation consistency loss, and penalty term. The U-net like network reconstructs the middle frame I m with an encoder containing five convolutional blocks and a decoder with five convolutional blocks for upsampling features and predicting images at different scales. The network utilizes a U-net architecture with skip connections to reconstruct a full-scale middle frame. Features and images are successively upscaled and refined through convolutional layers to predict the final frame. The network uses STN modules and LW networks to reconstruct non-middle frames based on encoded features of the middle frame. Encoded middle frame features are transformed using FTN, and the decoded middle frame image is transformed using ITN. The transformed feature and image are concatenated and passed through a decoder to predict a non-middle frame, with the middle frame feature also guiding the decoder to learn spatial relations. The decoder network in this section uses multi-scale transformer networks to reconstruct non-middle frames based on encoded features of the middle frame. Unique transformer networks are applied at each feature and image scale to learn various types of transformations, making the model robust to different blur patterns, including large blurs. STNs in the model learn non-local transformations in motion-blurred images. The local warping network in the model learns non-local transformations in motion-blurred images by predicting pixel-wise displacement. It uses convolutional layers to output motion flow, which is then used to locally transform features for sharper video frame generation. The network is trained with a multi-scale photometric loss to ensure sharpness both locally and globally in the generated images. The model uses a bilinear downsampling technique to resize ground truth images for predicted frames at different scales. It employs individual transformer networks for each feature level to learn transformations, making it robust to various blur patterns. The goal is to align transformations at different scales for reconstructing temporally consistent non-middle frames. The model aims to reconstruct temporally consistent non-middle frames by aligning transformations at different scales. A transformation consistency loss is proposed to ensure the model understands the relationship between transformations across frame levels. Additionally, a penalty term is designed to enforce diversity among generated images and prevent the model from replicating the middle frame prediction as non-middle frames. The penalty term is designed to enforce diversity among generated images by maximizing the sum of absolute difference between predicted frames and their time-symmetric ground truth frames. This penalty is imposed symmetrically to make the model sensitive to transformations close to the middle frame as well as at the end frames. The loss function includes weight coefficients for transformation consistency and penalty terms. Temporal shuffling and reverse ordering are key ambiguities in the task. Different approaches like pair-wise order invariant loss and multi-scale photometric loss are used to train the network effectively. Our model enforces symmetric motion learning with transformer networks close to the middle frame decoding smaller motions and those further from the middle frame decoding larger motions. The joint optimization allows frames to be reconstructed in a motion-guided sequence, addressing issues like temporal shuffling and reverse ordering. The model is optimized with constraints and loss terms to improve performance. Our model is implemented and trained using pyTorch. Adam is chosen as an optimizer with fixed values for \u03b2 1 and \u03b2 2. Training is done on synthetic blur and high-speed video datasets with specific image sizes and batch sizes. The model is trained for 80 epochs with a set learning rate and decay schedule. Training and test images are cropped from original resolution images without resizing. In this section, the performance of the model is analyzed qualitatively and quantitatively on camera shake blurs from panoramic scenes and dynamic blurs from high-speed videos. Test results using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) metrics are reported. Comparison with Jin et al. (2018) on the high-speed video dataset shows favorable results in terms of PSNR and SSIM. Our model outperforms Jin et al. (2018) significantly in initial and final frame predictions on the high-speed video dataset. The performance gap between middle and non-middle frames is smaller in our method due to reduced error propagation. Jin et al. (2018) faces limitations with heavy blur affecting input images, making middle frame predictions less reliable. Our approach shows improved results in restoring rotation-blurred images and partially blurred dynamic motion examples. Figure 5 shows a partially blurred dynamic motion example where the man is blurred with dynamic motion while the background remains close to static. The proposed model is robust to heavy blur as it generates frames independently from multiple decoders, reducing error propagation. The model is also evaluated on camera rotational blurs, with the panorama scenario being more challenging due to a broader range of blurs. Despite lower quantitative performance, visually better results are observed for the panorama scenario compared to the high-speed video dataset, which contains blurry ground truth images. Our model shows sharp results in restoring multiple frames from blurred inputs in panoramic scenes and high-speed videos. The restored frames are sharper in the panorama scenario compared to dynamic blur cases, attributed to high-quality ground truth images. Comparisons with a previous method on heavily blurred images from the high-speed video dataset show that our approach consistently reconstructs contents across frames, producing visually sharper videos. Our model generates visually sharper videos by restoring frames from blurred inputs, especially in dynamic blur scenarios. The STN module infers blur motion by transforming middle frames and comparing them with ground truth non-middle frames. Failure cases occur with undersampled and heavily blurred inputs. Refer to Sec. 8 in the appendix for more qualitative results. The STN module transforms middle frame features to non-middle frame features to visualize motion information. A cross-dataset evaluation shows our model trained on panoramic scenes performs comparably to a competing approach trained on high-speed videos. The absence of dynamic motion in the panorama dataset may impact performance on high-speed videos. The study explores the impact of the panorama dataset on model performance in high-speed videos. To address performance loss, a lighter model with weight shared decoders and reduced transformer networks is tested. This architecture reduces model parameters by 48% with only a 0.4dB performance decrease on average. The study shows that the feature transformer network (FTN) and local warping (LW) layer are crucial for model performance. Combining FTN, LW, and ITN components leads to the best model performance. Multi-scale photometric loss (PML) ensures network convergence during training. Additionally, a model trained with transformation consistency loss (TCL) converges faster and performs better during testing. The penalty term (PT) marginally improves performance, especially in predicting fewer frames. The penalty term slightly improved model performance by around 0.25dB in a 7-frame prediction model. It enforces consideration of subtle differences, especially in small motion scenarios. A novel unified architecture was presented for restoring video frames from a single blurred image. The model was trained without motion supervision, using a loss function with regularizers for fast convergence. Evaluation on datasets with rotation and dynamic blurs showed favorable performance compared to other methods, even across different blur patterns. Additionally, a lighter version of the model was proposed. Our model proposes a lighter version with weight-sharing of decoders and STN modules, predicting frames in a single step without middle frame dependency. It is simple to use and robust to heavy blurs, outperforming competing methods consistently. The method shows promise for applications like deblurring, temporal super resolution, and flow estimation from motion-blurred images. Our method consistently outperforms the competing method in predicting frames, especially for middle frames. Qualitative results from various datasets, including high-speed videos and panorama images, demonstrate the effectiveness of our model in handling different types of blur patterns."
}