{
    "title": "H1goBoR9F7",
    "content": "We propose using deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and faster execution in both training and inference. Previous studies have focused on optimizing for inference, neglecting training complexities. DSG addresses issues by activating a small number of neurons with high selectivity and maintaining batch normalization compatibility. Experiments demonstrate significant memory savings. The curr_chunk discusses the use of BN compatibility through double-mask selection to achieve significant memory and operation reduction in Deep Neural Networks (DNNs). Various approaches such as matrix decomposition, data quantization, and network pruning have been explored to improve execution performance. Most previous work focuses on inference, but challenges remain in reducing training complexities. The curr_chunk discusses challenges in reducing representational and computational costs of training DNNs, focusing on single-node optimization. It highlights the increased memory consumption during training due to backpropagation and the need for larger mini-batches for higher throughput and accuracy. Memory capacity often limits training capabilities. The limitations of memory capacity in training deep neural networks are discussed, with a focus on the impact of Batch Normalization on sparsity and performance degradation. Existing sparsity techniques are challenging to apply during both training and inference phases due to the bottleneck of dynamic neuronal activations. Stashing a large batched activation space for backward gradient calculation further complicates training. In this paper, the focus is on sparsifying neuron activations to address memory bottlenecks in backward gradient calculation. Existing inference accelerations complicate training by adding extra optimization problems. Batch normalization is crucial for accuracy and robustness, but it can damage sparsity. In this work, the focus is on addressing memory bottlenecks in backward gradient calculation by sparsifying neuron activations. Batch normalization is important for accuracy and robustness but can harm sparsity. The proposal is to search for critical neurons to construct a sparse graph, activating only a small number with high selectivity to save memory and simplify computation. The neuron-aware dynamic and sparse graph (DSG) approach aims to save memory and simplify computation by activating only critical neurons in a variable sparse graph. This method maintains model expressive power and is designed for compressible activations with sparsity, accelerative vector-matrix multiplication (VMM), and compatibility with batch normalization. The approach utilizes dimension-reduction search and double-mask selection to achieve 1.7-4.5x acceleration. The DSG approach utilizes dimension-reduction search and double-mask selection to achieve memory compression and computation reduction without compromising accuracy. It forms accelerative and compressive DSGs for different inputs, reducing computational cost by involving critical neurons and compressing sparsified activations. Unlike previous methods, DSG does not prune neurons or weights but activates a sparse graph based on input samples, benefiting deep learning in cloud and edge environments. The DSG approach utilizes dimension-reduction search and double-mask selection to compress memory and reduce computations without compromising accuracy. It activates a sparse graph based on input samples, selecting critical neurons to save representational cost efficiently. The DSG approach utilizes dimension-reduction search and double-mask selection to compress memory and reduce computations without compromising accuracy. By estimating neuron importance and using a top-k search method, non-critical neurons with small activations can be removed. This process saves computational cost and allows for accurate activations of critical neurons in the original high-dimensional space. The dimension-reduction search in the DSG approach achieves vector-wise structured sparsity by identifying critical neurons and compressing sparse activations. This allows for significant savings in computational cost and memory access, while maintaining accuracy in a low-dimensional space compared to the original high-dimensional space. The CONV layer in FIG2 (a) can be converted to VMM operations, generating points at the same location across all output FMs. The computational complexity for VMM operations is O(m \u00d7 n P Q \u00d7 n CRS \u00d7 n K). For the FC layer, the complexity is O(m \u00d7 n C \u00d7 n K). Note the switch in the order of BN and ReLU layers for better activation value determination. The activation value of non-critical neurons is difficult to determine when the following layer is BN. Reorganization can improve accuracy. Each output activation requires an inner product operation. Preservation of activation is equivalent to preserving the inner product. Johnson-Lindenstrauss Lemma is used for dimension reduction in search. The Johnson-Lindenstrauss Lemma (JLL) BID18 is used for dimension reduction with inner product preservation. It states that points in high-dimensional space can be embedded into a low-dimensional space while preserving Euclidean distances. Norm and inner product preservation can be achieved with high probability. Random projection is widely used for this purpose. The detailed proof in Appendix A shows the use of random projection for constructing a linear map. A simplified version called sparse random projection with ternary values is used to reduce dimensionality with negligible overhead. This method allows for approximation of high-dimensional inner products in a low-dimensional space. In a low-dimensional space, important neurons can be selected for activation estimation using a reduced dimension. A double-mask selection method is proposed for dealing with the intractable BN layer after dimension-reduction search. In a low-dimensional space, important neurons are selected for activation estimation using a reduced dimension. A sparsifying mask is produced to remove unimportant neurons, maintained by the ReLU activation function. To address sparsity damage by the BN layer, the same selection mask is copied before the BN layer and directly used on the BN output. This achieves fully sparse activation dataflow, with back propagated gradients forcibly sparsified every time they pass a mask layer. The training algorithm involves sparsifying gradients passing through a mask layer. Projection matrices are fixed after random initialization, with weights updated in a low-dimensional space every 50 iterations. Evaluation is done using LeNet and MLP on the FASHION dataset. PyTorch framework and NVIDIA Titan Xp GPU are used, along with zero-value compression for memory and MKL compute library. The influence of sparsity on accuracy is analyzed using DSG on small and medium scale models. Results show that accuracy remains stable with sparsity <60% but drops sharply above 80%. ResNet models are more affected by sparsity than VGG models, with CNNs tolerating more sparsity than MLPs. Compared to MLP, CNN can tolerate more sparsity, as shown in experiments on large scale models like VGG16 and ResNet18. The WRN model with wider channels outperforms both. The influence of graph selection strategy on sparsity vs. accuracy is also examined, with dimension-reduction search and oracle methods showing better performance than random selection. The proposed random projection method can accurately estimate activations in low-dimensional space, with dimension-reduction search achieving similar accuracy to oracle top-k selection. Lower parameter values approach the original inner product more accurately but require more computation. BN compatibility is also addressed, showing different cases of graph sparsifying with and without BN operations. The study compares the effectiveness of using Batch Normalization (BN) and different mask selections in training neural networks. It is found that BN is crucial for training, and double-mask selection can improve accuracy by recovering sparsity damaged by BN. Additionally, the study explores the impact of network width and depth on robustness, showing that wider layers in a network can be more robust than deeper ones. In medium-sparse space, deeper networks show stronger representation ability due to their structure, while in ultra-high sparse space, deeper networks are more likely to collapse. The study also found that wider layers in a network can improve accuracy without reducing depth, and random projection for data dimension reduction does not slow down convergence speed. The selection patterns of samples converge during training but vary across samples. Storing all patterns is memory-intensive, so on-the-fly dimension-reduction is used during inference. DSG benefits representational cost by reducing memory consumption in training and inference phases. Memory optimization results using a zero-value compression algorithm are shown in FIG6. Stashing activations across layers is necessary for backward computation, with neuron activation dominating memory over weights. During inference, memory overhead is dominated by neuron activation rather than weight. The representational cost can be reduced by up to 7.1x for activations and 1.7x for overall memory under different levels of sparsity. The memory overhead for selection masks is minimal. While the benefits in inference are smaller compared to training, noticeable memory reduction can still be achieved. Reducing costs for training and inference is a major contribution, with noticeable acceleration achieved through compression. Results show significant reduction in computational cost for both training and inference, with operations reduced by 1.4x to 2.2x in training and 1.5x to 3.9x in inference under different levels of sparsity. The overhead of dimension-reduction search is relatively larger compared to memory cost, with training showing less improvement than inference. The training demonstrates less improvement than the inference due to partial acceleration of the backward pass. Weight gradient generation is not accelerated because of irregular sparsity. Evaluation on CPU using Intel MKL kernels shows our approach achieves 2.0x, 5.0x, and 8.5x average speedup under different levels of sparsity compared to VMM baselines. DSG generates dynamic vector-wise sparsity, not well supported by GEMM. To improve GEMM-based implementation, reordering executions at the granularity of vector inner-product and grouping non-redundant executions to the same tile can enhance local data reuse. Comparing with smaller dense models, our approach reduces training time with minimal accuracy loss. DNN Compression BID4 achieved up to 90% weight sparsity by randomly removing connections. DNN Compression methods achieve high weight sparsity through connection pruning, mainly on FC layers. However, these methods may not be effective for CONV layer-dominant networks like ResNet. Some approaches use reinforcement learning to optimize sparsity configuration across layers, but practical speedup is challenging due to irregular element-wise sparsity. DNN Acceleration focuses on sparse pattern optimization, with coarse-grain sparsity proposed for inference optimization. These methods typically require pre-trained models, iterative pruning, and fine-tune retraining. In contrast to fine-grain compression, coarse-grain sparsity aims to optimize execution speed by removing unimportant weight filters, training penalty coefficients, and solving optimization problems. Various methods such as L2-norm group-lasso optimization and neuron pruning have been proposed for achieving weight sparsity. Different approaches like predicting important neurons and leveraging randomized hashing have also been explored for inference acceleration. In this work, a DSG (dynamic and sparse graph) structure is proposed for efficient DNN training and inference. It involves dimension-reduction search for compressive memory, accelerative execution, and double-mask selection for BN compatibility. Previous methods focused on different aspects like back propagated errors pruning, distributed training acceleration, and fine-grain compression. DSG (dynamic and sparse graph) structure proposed for efficient DNN training and inference includes memory saving and computation reduction. It promises efficient deep learning in both cloud and edge environments. The inner product can be preserved with small epsilon values for X i and W j pairs. Previous work discussed random projection for big data applications, and this paper re-organizes supporting materials for a systematic proof. There is a trade-off between dimension reduction and recognition accuracy, with smaller k providing more accurate estimations but higher computational complexity. The effectiveness of the approach for training dynamic and sparse neural networks has been validated through experiments. The training algorithm for generating DSG is presented in Algorithm 1. The generation procedure of the critical neuron mask based on virtual activations estimated in low-dimensional space is shown in FIG9. The k value is determined by activation size and desired sparsity \u03b3. To reduce search cost, a top-k search is conducted over the virtual activation matrix. The overall activation mask is generated by setting elements to one if estimated activation is larger than top-k threshold, reducing search cost significantly. The generation procedure of the critical neuron mask based on virtual activations estimated in low-dimensional space is presented. A top-k search is conducted to reduce search cost, generating an overall activation mask for the mini-batch. The dimension-reduction search for importance estimation in VGG8 on CIFAR10 shows significant reduction in operations. The impact of DSG on training convergence is analyzed in FIG0. The impact of DSG on training convergence is analyzed in FIG0, showing that the convergence speed remains similar to the vanilla model training. The distribution of inner product differences between high-dimensional and low-dimensional data for the CONV5 layer of VGG8 on CIFAR10 is visualized in FIG0 (c), indicating an accurate dimension-reduction capability. This helps reduce training variance and prevent deceleration. Additionally, an experiment was conducted to study the convergence of selection masks during training. In a case study for data recording, the change in binary selection mask between adjacent training epochs is measured for each layer (CONV2-CONV6). The convergence of selection masks for each sample is shown in FIG0. The average L1-norm values of the difference mask tensors between adjacent training epochs and samples after training are calculated. Random projection matrix is inherited from training for implementation. In our implementation, we inherit the random projection matrix from training and perform on-the-fly dimension-reduction search in inference. Directly suspending selection masks is not efficient due to variations across samples. Storing trained masks for all samples consumes memory space, making on-the-fly search during inference more efficient. Smaller dense models with the same MACs save training time but may lead to accuracy degradation. DSG training includes warm-up training with a dense model for the first 10 epochs. Our work focuses on training and inference phases, unlike previous methods that mainly targeted inference compression. We compare our approach with prior methods on inference pruning using DSG for fine-tuning based on pre-trained models. Results are from the same network (VGG16) on ImageNet dataset to ensure fairness. Our DSG produces structured sparsity, different from DSG training from scratch. DSG produces structured sparsity for training and inference phases, compared with prior methods on inference pruning. Input sparsity at each layer is considered, achieving a balance between operation amount and model accuracy."
}