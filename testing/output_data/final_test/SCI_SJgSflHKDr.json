{
    "title": "SJgSflHKDr",
    "content": "Learning theory emphasizes the importance of data in minimizing generalization error, especially when training and test distributions differ. A novel perspective on function transfer learning uses Wasserstein distance to quantify the change in performance between training and test sets. There is a trade-off between function invariance to distribution shifts and the magnitude of the shift. Empirical evidence shows that test performance is strongly correlated with the distance in data distributions between training and test sets. This challenges the belief that more data is always better and highlights the importance of choosing training data wisely. Our results emphasize the importance of choosing a training data distribution close to the test data distribution for better generalization. An example with two students studying for an exam illustrates the trade-off between studying well and being able to answer different-looking questions that follow the same reasoning. This highlights the importance of considering the similarity between exercise and test questions in machine learning work. The importance of considering the similarity between training and test data distributions for better generalization in machine learning work. Invariance to distribution shifts is crucial for achieving robustness to irrelevant data variations. The optimal learned function should be invariant to data variations that do not affect the output. Matching training and test distributions is important for generalization in machine learning. Networks can fit random labels perfectly but fail to generalize to the correct label distribution. The learning problem involves training on one distribution and testing on another, treated as a transfer learning problem beyond standard generalization. The Wasserstein distance measures the closeness of these distributions and the network's ability to handle distribution shifts. Experimental results show a strong correlation between distribution distances and network performance, emphasizing the importance of aligning training and test distributions for optimal performance. Invariance is a crucial aspect of Deep Learning, with research focusing on increasing invariance by incorporating structure into models. The importance of aligning training and test distributions for optimal performance has been emphasized, with the Wasserstein distance measuring the closeness of these distributions. The trade-off between invariance to distribution shifts and the magnitude of such shifts has been an active research area, with the need for a more systematic approach to measuring distribution distances and invariances of learned functions. Several works have explored invariance in Deep Learning, with some using GANs for domain adaptation to align source and target distributions. Tzeng et al. (2015) utilized invariance for visual adaptation. Instead of focusing on beating benchmarks, this study aims to analyze the trade-offs between invariance and distribution shift quantitatively and qualitatively. Data augmentation methods, including automated ones using GANs, have been successful in achieving invariance. Invariance learning techniques, such as data augmentation and regularization, aim to increase robustness by removing unwanted nuisances. Distribution shifts in data set distribution can take various forms, affecting input distribution, target distribution, or both. Imbalanced datasets are also a concern in addressing distributional shifts. Distribution shifts can take various forms, affecting input distribution, target distribution, or both. Imbalanced datasets, methods of over and undersampling, and domain shifts can be described within this framework. Evaluating distribution shifts involves measuring probability distribution distances, such as the Maximum Mean Discrepancy (MMD) distance and the Wasserstein distance, which have applications in anomaly detection, two-sample testing, computational graphics modeling, and generating synthetic images using GANs. In this work, the Wasserstein distance is used to investigate the performance drop when transitioning from training to testing samples in neural networks. The study focuses on the impact of distribution shift and invariance of the learned function, as well as the type of function being approximated. Experiments show changes in input distribution by increasing input values, highlighting the quality of function approximation. The study uses the Wasserstein distance to analyze the performance drop in neural networks during the transition from training to testing samples. It focuses on the impact of distribution shift and invariance of the learned function. Normalizing the input data by subtracting the mean of the distribution makes the function approximation robust to distribution shifts. Experiments show that when the correct function is found, the approximated function works even in untrained domains. The study discusses the impact of distribution shift and invariance on neural network performance. It highlights that errors accumulate outside the training domain if the true function is not found exactly. An oversized training domain can negatively affect performance on the test domain. Finding the correct invariances is challenging, making it important to ensure that train and test distributions match to enforce a small change. The study emphasizes the importance of matching train and test distributions to enforce a small change in performance. Learning involves mapping a training dataset to a learned function, typically achieved through minimizing a loss function using stochastic gradient descent. The performance of the learned function on a testing dataset with potentially different distribution is of interest, as it relates changes in data distribution to changes in performance. Theorem 1 discusses the lower bound on the expected absolute difference in loss for two dataset distributions p1 and p2. It involves the learned function of a function family F continuous in the probability space P, minimizing the norm loss when data is distributed according to p1. The proof involves the line integral of the change of the function along the Wasserstein geodesic between p1 and p2. The lower bound on loss is determined by the difference in distribution from training to testing sets. If the function is invariant to these changes, the performance quality remains unaffected. The difference in distribution from training to test set becomes crucial when the class is not invariant. It is important to have good performance on the training distribution to avoid unnecessary invariance. Training sets should closely match the test set distribution to ensure accurate function approximation. If the function has too much invariance, it may overlook relevant variations in the data, while lack of invariance can lead to fitting the training set well but losing performance. The function's performance may vary between training and test distributions due to differences in distribution. To address this, the Fr\u00e9chet distance is used to measure distribution distances by computing the distance on an embedding of the raw data. This approach avoids the need to calculate high-dimensional distribution distances from raw data and train networks for both distributions separately. Comparing the distribution of a function trained on different-sized training and test sets can be challenging. The Fr\u00e9chet distance (FD) is a distance measure between two normal distributions, used in evaluating quality differences of real and synthetic images. It can be applied to various domains by replacing the inception network with a domain-relevant model. The FD is applied to measure the mismatch of two sets, showing the importance of distribution distance between training and test distributions in practical problems. The distance between training and test distributions is crucial in practical problems. Experiments in text classification, vision data augmentation, and speech separation highlight this perspective. The general experimental setup for each dataset is provided, with a focus on text classification using different subsets of a large dataset. The Amazon Review dataset is specifically mentioned for training a rating classifier on one product category and testing on another to measure distribution shift effects. The Amazon Review dataset is used for text classification experiments, focusing on 9 categories with binary ratings (good or bad). Ratings of 3 are excluded, and GloVe embeddings are utilized. Binarized ratings' classification accuracy decreases with distance between training and test sets, measured by the FD score. The relationship shows a strong negative correlation (Pearson -0.78). The study examines distribution shifts in vision classification tasks using data augmentation techniques on datasets like CIFAR10, SVHN, and Fashion MNIST. A convolutional neural network is used for classification, and the Inception-v3 network is used to compute the FD score. Various augmentation techniques are applied to create distribution shifts, resulting in reported classification accuracy. The study explores distribution shifts in vision classification tasks using data augmentation techniques on datasets like CIFAR10, SVHN, and Fashion MNIST. Results show that larger FD scores lead to worse performance, with nuances in the relationship between distribution distance and performance. The findings suggest selecting augmentation methods based on the FD score between the augmented training set and a validation set. Future work will validate this approach. Additionally, the study delves into speech separation, focusing on separating the speech of multiple speakers. Speech separation involves separating the speech of multiple speakers from audio data, creating large datasets through combinatorial possibilities. Data is collected from various sources like Libri Speech Corpus, Voxceleb2, Common Voice, and TED-LIUM speech corpus. The number of speakers and time per speaker are balanced across datasets. For the training set, 707 speakers are kept for each of the first four data sets, with 7.5 minutes per speaker totaling roughly 88 hours. The test set samples 10 hours from each data set with new speakers. Mixes are created from equal datasets for more consistent recording settings. The Conv-TasNet model with an SDRi score of 15.6 is used, processing 4-second samples at 8k Hz. SDR improvement scores are reported, calculated as the difference between processed and unprocessed samples. Speaker embedding network features are used to compute FD scores instead of the inception network. The speaker embedding network's encoded features are used to calculate FD scores, with a strong correlation found between performance loss and distribution distance between training and testing data. Larger datasets do not always result in better performance, as shown in experiments. It is important to ensure that the train and test data distributions are close for optimal network performance. The importance of train and test distribution closeness for optimal network performance is highlighted in experiments. Neural networks often struggle to find the true functional relationship between input and output, leading to distribution shifts impacting performance. The use of a representation network to obtain low-dimensional embeddings can help reduce the impact of noisy data, but careful selection of embeddings for each dataset and task is crucial. In order to optimize network performance, it is crucial to carefully select embeddings that can model data shifts effectively. Using word embeddings trained only on English texts may not yield meaningful results for other languages, leading to performance deterioration due to data distribution shifts. To address this issue, the Fr\u00e9chet distance can be applied to measure dataset distribution distances and guide the selection of training sets, data augmentation techniques, and network optimization strategies. This approach aims to mitigate the impact of mismatched invariances on predictive models. The appendix provides extended details of the work, not critical to the main text. Experimental details of small-scale insight experiments are specified, where a model approximates a function f(x) on a specific interval with added gaussian noise. Training involves shifting the test interval for model evaluation. During testing, the test interval is shifted by the scalar \u03b1 = [0, \u22120.5, 0.5]. A zero mean normalization is applied to the input data in the experiments. The neural network used has 8 hidden units in the first layer with a ReLU activation function and one output unit in the second layer. The network is optimized using mean squared error and Adam optimizer. In section 5.1, the experiment involves optimizing 25 parameters with biases using mean squared error and Adam optimizer. The preprocessing steps for the Amazon review dataset include converting text to numeric representations, utilizing overall score and review text, and limiting vocabulary size to 10,000 words. Reviews are split into positive and negative categories, with ratings of 3 removed. Sentence lengths are standardized to 256 words with padding, and less frequent words are replaced with < UNK > symbol. The classification network for the experiment includes an embedding, global average pooling, and three dense layers of varying sizes. ReLUs and Sigmoid activation functions are applied, with optimization using Adam and binary cross-entropy loss. Details on data augmentation and CIFAR10 dataset are provided. The CIFAR10 dataset contains 32 \u00d7 32 colored natural images divided into 10 classes, with 50,000 training images and 10,000 test images. The Street View House Numbers (SVHN) dataset has 10 categories for digits 1 to 10, with 73,257 training images and 26,032 test images. The Fashion MNIST dataset has 60,000 train and 10,000 test grayscale images of clothes in 10 classes. Data augmentation includes random cropping to 24 \u00d7 24 and salt and pepper noise for distorted pixels. The test set remains unchanged when rescaling. Salt and pepper noise distorts pixels equiprobably to black or white with a 33% probability. Random contrast adjustment has upper and lower limits of 1.8 and 0.2. The classification network is based on a CNN structure with specific layer configurations and activation functions. Optimization is done using cross-entropy loss and gradient descent with specific parameters. For speech separation experiments, a learning rate of 0.1 and exponential decay rate of 0.1 are used. Exponential moving average with a decay factor of 0.9999 is applied to weights. Inception-v3 network is used for computing FD scores, pretrained in Tensorflow library. Images are resized to 299x299 for input. Feature representations are obtained from pool-3 layer with 2048 dimensions. Speech embedding network employs GE2E loss with convolutional and LSTM layers trained on 3-second samples. The speaker embedding network utilizes convolutional and LSTM layers to predict a 100-dimensional speaker embedding based on spectrogram and MFCC inputs. The network architecture includes densely connected layers, fully-connected layers, and Statistical Pooling, with optimization done using the Adam optimizer. The speaker embedding network uses convolutional and LSTM layers to predict a 100-dimensional speaker embedding from spectrogram and MFCC inputs, with optimization done using the Adam optimizer and a learning rate of 0.0001."
}