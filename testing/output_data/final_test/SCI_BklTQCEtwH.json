{
    "title": "BklTQCEtwH",
    "content": "A novel curriculum learning algorithm based on clustering is proposed to improve training generative models like GAN for noisy data. The algorithm prioritizes data points with high centrality and uses an active set approach for scalability. Geometric analysis supports the necessity of cluster curriculum for generative models. Experiments on cat and human-face data show the algorithm can learn optimal generative models for noisy data. Deep generative models like ProGAN have shown significant progress in the past decade, including auto-encoder, VAE, GAN, normalizing flow, and autoregressive models. However, training these models to converge to a proper minimum is challenging due to issues like non-stability, mode collapse, and generative distortion. Various algorithms have been proposed to address these challenges. Various algorithms have been proposed to address challenges in training deep generative models like ProGAN, including feature engineering, discrimination metrics, gradient penalties, spectral normalization, and orthogonal regularization. A breakthrough for GANs has been made with progressively growing neural networks from low to high-resolution images, enabling StyleGAN to produce photo-realistic results and push the state of the art to new levels. The paper explores a new curriculum for training deep generative models, focusing on curriculum learning with clustering to handle noisy data. It introduces a cluster curriculum that treats data based on centrality, aiming to improve robust training. Key contributions include summarizing four curricula for generative models and proposing the cluster curriculum as a new approach. The paper introduces a cluster curriculum for training deep generative models, focusing on centrality of data points. It proposes an active set algorithm for large-scale learning and analyzes the hardness of noisy data using geometric principles. Curriculum learning is essential for algorithm performance in machine learning. Curriculum learning involves starting small, learning easier aspects of a task, and gradually increasing difficulty levels. It is a common learning rule for human, animal, and machine intelligence. The design of curricula is task-dependent and data-dependent. Representative curricula for generative models include architecture curriculum, focusing on deep neural architecture as a curriculum for learning concepts and disentangling representations. The different layers decompose distinctive features of objects for recognition and generation. Progressive growing of neural architectures is successfully exploited in GANs. Semantics and dimension are common curricula for learning tasks, with semantics determining the difficulty of knowledge acquisition and high dimensionality posing challenges in machine learning. The effectiveness of dimension curriculum is evident from recent progress on deep generative models, such as ProGANs, by gradually enlarging image resolution and language generation from short sequences to long sequences of more complexity. To train generative models robustly, it is plausible to raise cluster curriculum, meaning that generative algorithms first learn from data points close to cluster centers and then with more data progressively approaching cluster boundaries. The process of clustering data points for curriculum learning involves gradually approaching cluster boundaries. The importance of clusters in high-dimensional spaces is highlighted, with generative algorithms being beneficial in dense local spaces. Clusters are informative subsets within the dataset, containing data points that form clusters. Clusters are informative subsets within the dataset, containing common regular patterns of data points. Curriculum learning is effective in circumventing the negative influence of noisy data, allowing generative models to gradually learn from dense clusters to cluster boundaries and all data points. The centrality measure quantifies the compactness of clusters, aiding in avoiding the direct harm of noise or outliers. The centrality measure quantifies the compactness of clusters, aiding in avoiding the direct harm of noise or outliers. Centrality is determined by the eigenvector corresponding to the largest eigenvalue of the transition probability matrix drawn from the data, with data points arranged by cluster centrality. The cluster curriculum learning involves dividing data into subsets based on centrality order to efficiently train generative models. The optimal curriculum is determined using quality metrics like Fr\u00e9chet inception distance or sliced Wasserstein distance. The optimal curriculum for training generative models can be identified by the minimal value of a specified quality metric. The minimum score is metric-dependent and can be determined using reliable evaluation metrics. Two ways of using incremental subsets during training are re-randomizing model parameters or fine-tuning based on pre-training. To obtain the precise minimum score, the subset size needs to be much smaller than the total data size. The active set approach proposes training generative models with a small fixed subset size during each loop of cluster curriculum, reducing time-consuming training of large datasets. The active set consists of 200 data points, with additional random sampling to maintain density. This method aims to address the issue of large subset sizes in training loops. The active set approach involves training generative models with a small subset size during each loop of cluster curriculum, reducing training time for large datasets. This method utilizes a fixed set of 200 data points, with additional random sampling to maintain density. The active set allows for efficient training by adapting the dataset, leading to significant time savings in large-scale data processing. The text discusses the analysis of confidence ellipses and the relation between two ellipses based on centrality-ranked data points. It also touches on the difficulty of training generative models by comparing the number of data points in a geometric entity to lattice points. The enumeration of lattice points is computationally challenging in high dimensions. The text discusses encoding data of normal distributions using small spheres packed in ellipsoids instead of lattice points, simplifying the computation in high dimensions. The comparison between data points and lattice points in an annulus is illustrated, with the packing number denoted as N(E) in the context. Theorem 1 provides the exact form of N(E) for a set C. Theorem 1 defines the ellipsoid E\u03b1 for a set of data points drawn from a normal distribution. N(E\u03b1) represents the number of spheres packed in the ellipsoid. The annulus A is formed by removing one ellipsoid from another, and the relationship between the number of spheres in the annulus and the ellipsoid is investigated. Corollary 1 provides additional insights related to the packing of spheres in the annulus. The number of spheres packed in an annulus formed by removing one ellipsoid from another is explored in relation to the ellipsoid sizes. As the ellipsoid cluster grows, the number of packed spheres decreases exponentially with dimensionality. The functional relationship between the number of spheres in the annulus and the ellipsoid is demonstrated, with considerations for the minimal Mahalanobis distance ellipsoid and suitable sphere radius. The oracle ellipse is used to determine the sphere radius for comparability of scales between the number of spheres in the ellipsoid and annulus. The demonstration involves using an oracle ellipse to compute the free parameter \u03b5, based on the isotropic normal distribution of data points. The number of data points in the annulus decreases significantly as the ellipsoid size increases near the critical point, exhibiting percolation phenomena. The number of lattice points remains large and stable until E \u03b12 approaches the boundary, making data fitting in the annulus challenging. The study uses the ProGAN algorithm for experiments, with FID as the quality metric, and samples 200,000 cat images from the LSUN dataset. The LSUN dataset contains cat images captured in the wild with varying styles. Curriculum parameters are set for training with 20,000 images initially and then 10,000 more based on centrality. The CelebA dataset is used for face attribute generation with 70,000 randomly sampled faces for cluster-curriculum learning. The curriculum parameters for cluster-curriculum learning with 70,000 faces include setting | \u2212 \u2192 X 0 | = 10,000 and | \u2212 \u2192 X i | = 5,000. ResNet34 is used to extract 512-dimensional features for faces and cat images, with directed graphs built from these features. The parameter \u03c3 for edge weights is determined to be 0.8, and TensorFlow is used for coding. The FID curves of cluster-curriculum learning for ProGAN on the cat dataset and CelebA face dataset show evidence of global minima amid the training process. Noisy data and outliers deteriorate the quality of generative models during training. The optimal curricula found by two algorithms indicate that the curriculum of the active set is crucial. The curriculum of the active set in cluster-curriculum learning differs from normal training by one-step data increment, indicating reliability for fast learning. Despite worse FID performance with noisy data, the V-shape accuracy curve remains unchanged. The centrality-FID curve on cat data shows low centrality noisy data lacks effective information, while CelebA face dataset's low centrality images still convey face features. ProGAN continues optimization despite challenges. ProGAN continues to be optimized by the majority of the data until the curriculum size reaches 55,000. A comparison with the FFHQ face dataset, containing 70,000 high-quality face images, shows the difficulty of noisy data for generative algorithms. The geometric method is used to analyze cat and face data, with percolation processes conducted using 512-dimensional features from ResNet34. The curve of n(A) in Figure 7 is the variable of interest in this scenario. The critical point in the percolation process occurs for both cases, with optimal curricula falling into feasible domains after the critical points. Noisy data plays a role in tuning generative model parameters, leading to a fast learning strategy derived from the percolation process. Training can start from the curriculum specified by the critical point, accelerating cluster-curriculum learning. The pink strips represent intervals of optimal curricula derived by generative models. The optimal interval in the percolation process is determined by searching for maxima of the absolute discrete difference of associated curves. The y-axes scales are normalized by 10,000. Noisy data leads to the optimal interval being closer to the critical point. The optimal curriculum for heavily noisy data coincides with the critical point of the percolation process in the annulus. Cluster curriculum is proposed for robust training of generative models, with the active set devised to facilitate scalable learning. Experimental results show that generative models trained with cluster curriculum can learn optimal parameters based on quality metrics like Fr\u00e9chet inception distance and sliced Wasserstein distance. Geometric analysis reveals a close relationship between optimal curricula and critical points of associated percolation processes. This geometric phenomenon warrants further exploration. The centrality-FID curve is a visual tool for monitoring model training, helping to understand the learning process and select suitable models based on data noise levels. Optimal model parameters are achieved with cluster curriculum, balancing robustness against noise and data fitting capacity. The meaning of model optimality is tied to the global minimum of the centrality-FID curve, which is metric-dependent. More informative data leads to a more powerful model covering diverse data, but a trade-off exists between noise robustness and data capacity. Selecting a trained model close to the optimal curriculum is beneficial for heavily noisy data. In this paper, the focus is on cluster-curriculum learning for datasets with varying levels of noise. The study does not cover multi-class cases like the ImageNet dataset with BigGAN. The centrality or clustering coefficient in machine learning and complex systems is well-studied. The graph-theoretic centrality is introduced for cluster curriculum by constructing a directed graph with K nearest neighbors. The study focuses on cluster-curriculum learning for datasets with varying levels of noise. It introduces graph-theoretic centrality for cluster curriculum by constructing a directed graph with K nearest neighbors. The weighted adjacency matrix of the digraph is formed based on distances between data points. The density of data points is quantified using the stationary probability distribution of a Markov chain. The stationary probability u is obtained by solving an eigenvalue problem, which coincides with the centrality for density-based cluster curriculum. The study introduces graph-theoretic centrality for cluster curriculum by constructing a directed graph with K nearest neighbors. The density of data points is quantified using the stationary probability distribution of a Markov chain. The final formula for volume of a d-dimensional ellipsoid is derived, leading to the volume of the packing sphere. The proof of the theorem is concluded."
}