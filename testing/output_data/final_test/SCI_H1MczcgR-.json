{
    "title": "H1MczcgR-",
    "content": "Careful tuning of the learning rate is crucial for effective neural net training. Recent interest lies in gradient-based meta-optimization to minimize expected loss by tuning hyperparameters or learning an optimizer. Short-horizon meta-objectives can lead to bias towards small step sizes, termed short-horizon bias. Experiments on benchmark datasets show that meta-optimization often selects learning rates that are too small. Meta-optimization tends to choose learning rates that are too small, leading to slow progress or divergence in neural net training. Tuning the learning rate is crucial for good performance, especially on complex benchmarks like ImageNet BID23. Various decay schedules, such as polynomial or exponential, are used to achieve convergence guarantees for stochastic gradient methods. Learning rate heuristics and momentum are crucial for training efficiency and performance gains in optimization algorithms like Adam. Different approaches, including online adaptation and learning rate schedules, have been attempted to improve convergence guarantees for stochastic gradient methods. Some have even tried to learn an optimizer. Meta-optimization involves tuning hyperparameters to minimize a meta-objective, but it is much more expensive than base-level optimization. Short-horizon bias leads to a tradeoff between short-term and long-term performance in learned optimizers. In this work, the short-horizon bias is investigated both mathematically and empirically, analyzing a quadratic cost function with noisy gradients. The dynamics of SGD with momentum are analyzed to derive optimal learning rates and momentum for short-term performance. The differences between short-horizon and long-horizon schedules are also examined. When the problem is stochastic and badly conditioned, greedy learning rate schedules decay too quickly, hindering progress along low curvature directions. This leads to slow convergence towards the optimum, as illustrated in a comparison of fixed and exponentially decaying learning rate schedules in a noisy quadratic problem. The schedule initially has higher loss but makes more progress towards the optimum, achieving a smaller loss once the learning rate is decayed. Figure 2 quantitatively shows this effect for a noisy quadratic problem in 1000 dimensions. Solid lines represent loss after various steps with a fixed learning rate, favoring small rates. Dashed curves show loss after steps with exponentially decayed rates, favoring higher rates. This highlights the challenge of selecting learning rates based on short-horizon information. The paper investigates gradient-based meta-optimization for neural net training, comparing offline and online algorithms. Short-horizon meta-optimizers underperform fixed learning rates, slowing down optimization progress. The paper explores gradient-based meta-optimization for neural net training, comparing offline and online algorithms. Short-horizon bias is a fundamental obstacle that needs to be addressed for meta-optimization to be practically useful. The paper discusses the noisy quadratic model of BID25, where the true function optimized is a quadratic with noisy observations of the gradient. It analyzes the dynamics of SGD with momentum and compares different learning rate schedules. Approximating the cost surface of neural networks with a quadratic function has led to powerful insights and algorithms, including second-order optimization methods like Newton-Raphson and natural gradient. Hessianfree optimization is an approximate natural gradient method that can fit deep neural networks with fewer updates. In the context of neural network optimization, quadratic approximations have provided insights into learning rate adaptation and momentum. Second-order optimization algorithms like Hessian-free can eliminate the need to tune hyperparameters. In neural network optimization, quadratic approximations offer insights into adapting learning rates and momentum. While deterministic settings show that greedily choosing these hyperparameters is optimal, this is not the case in stochastic settings. The focus is on adapting scalar learning rate and momentum hyperparameters shared across all dimensions, as opposed to dimension-specific rates. In neural network optimization, quadratic approximations provide insights into adapting learning rates and momentum. Methods like K-FAC BID19 still require tuning scalar learning rate and momentum hyperparameters. The analysis applies to all methods as they can be seen as performing SGD in a preconditioned space. The focus is on the SGD with momentum algorithm, where the update is defined by the loss function, training step, learning rate, velocity, and momentum. A noisy quadratic model is defined for each iteration, providing the optimizer with noisy gradients. The optimizer in neural network optimization is provided with noisy gradients in a noisy quadratic model, where the stochastic cost function is defined with a diagonal Hessian and noise covariance. The expected loss and stochastic gradient are calculated accordingly. The stochastic gradient in neural network optimization is a noisy Gaussian observation of the deterministic gradient with variance. The iterate is treated as a random variable, and the expected loss in each iteration is calculated. A recursive formula is derived for the mean and variance of the iterates, and a greedy-optimal schedule for the global learning rate and momentum decay parameter is analyzed. The dynamics of SGD with momentum on the noisy quadratic model are compactly modeled, with observations on the independence of coordinate evolution and the means and variances of the parameters. The dynamics of SGD with momentum are modeled as a deterministic recurrence relation with sufficient statistics. The expectations and variances of the parameters and velocity are updated recursively to obtain an optimized schedule for learning rate and momentum. This allows for fitting a locally optimal schedule for gradient-based optimization. The optimized schedule for learning rate and momentum in SGD with momentum can be solved using a closed-form solution for one-step lookahead. The greedy-optimal schedule is derived from the statistics at a particular time T. The greedy-optimal learning rate and momentum schedule is found to be optimal for SGD without momentum in the case of univariate noisy quadratics, extending previous findings on deterministic quadratic objectives. The sequence of learning rates for SGD without momentum on a univariate noisy quadratic follows the greedy-optimal schedule derived by BID25. If the Hessian and gradient covariance are spherical, each dimension evolves independently. Approximate second-order optimizers like K-FAC can be seen as preconditioned SGD. Comparisons of optimized learning rates and momenta in noisy and deterministic quadratic settings are shown in Figure 3. In the noisy and deterministic quadratic settings, the optimized schedule matched the greedy one as predicted by theory. The Hessian and gradient covariance being better conditioned in a space where a good enough preconditioner could make them close to spherical. Comparisons between optimized and greedy-optimal schedules were made on a 1000-dimensional noisy quadratic problem. The Fisher information matrix approximates the Hessian matrix and reflects gradient noise covariance. Greedy-optimal schedules were computed using Theorem 3 to minimize expected loss at T = 250 with Adam optimizer. An upper bound on learning rate prevented loss from growing too large. Fixed learning rate and momentum were also considered. Optimized schedule achieved a much lower final expected loss value of 4.25. The optimized schedule achieved a significantly lower final expected loss value (4.25) compared to the greedy-optimal schedule (63.86) or fixed schedule (42.19). It maintained a high learning rate initially, leading to a decrease in losses along low curvature directions. After 50 iterations, it reduced the learning rate, resulting in a drop in both high-curvature and total losses. In contrast, the greedy-optimal schedule reduced losses on high curvature directions early on but struggled to make progress along low curvature directions due to small learning rates. The learning rates are too small to make substantial progress along low curvature directions, leading to a higher total loss in the end. Short-horizon bias in meta-optimization encourages quick decay of learning rate and momentum for short-term gains at the expense of long-term performance. Stochasticity is necessary for short-horizon bias to manifest, as seen in the comparison with the deterministic case where learning rate and momentum schedules are nearly flat. The optimized schedules for stochastic hyperparameter adaptation are more complex compared to the deterministic case. Gradient-based hyperparameter optimization involves tuning hyperparameters through gradient descent on a meta-objective. An empirical analysis of a gradient-based meta-optimization algorithm called stochastic meta-descent (SMD) is conducted, focusing on adapting learning rate and momentum hyperparameters. The SMD version used in the analysis is idealized by allowing more memory and computation for the meta-optimizer and limiting the representational power of the meta-model. In this study, the focus is on adapting learning rate and momentum hyperparameters, simplifying the problem to optimize the meta-objective effectively. The experiments conducted are relevant to practical meta-optimization algorithms, despite the simplifications made. Poor meta-optimization could lead to hyperparameters getting stuck in suboptimal regions, highlighting the importance of effective meta-optimization methods. Improved meta-optimization methods may result in decreased base-level performance, as tuning the meta-optimizer could indirectly tune learning rates and momenta. The experiments are applicable to meta-optimization methods aiming to learn algorithms, where implicit learning rate schedules could be encoded. Stochastic meta-descent (SMD) involves performing gradient descent on hyperparameters like learning rates. Meta-optimization methods can be computationally demanding, but forward mode autodiff can be used cheaply when optimizing only two hyperparameters. This method involves computing directional derivatives alongside the forward computation. The forward differentiation equations for obtaining the gradient of vanilla SGD learning rate can be computed efficiently using reverse-on-reverse or forward-on-reverse automatic differentiation. This allows for gradient-based meta-optimization of hyperparameters, applicable to various optimization algorithms beyond vanilla SGD. The SMD algorithm allows for meta-optimization of hyperparameters using gradient-based optimizers like Adam. It involves optimizing regular parameters and hyperparameters using gradient-based methods with lookahead window size T and number of meta updates M. The algorithm simplifies by fitting coordinate-wise adaptive learning rates. The original SMD algorithm BID26 used approximate updates and accumulated intermediate gradients during training, introducing bias into meta-gradients. Changes were made to tune a global learning rate parameter, use exact forward mode accumulation, compute meta-updates on separate SGD trajectories, and ensure unbiased meta-gradients through multiple meta-updates. The approach of ensuring unbiased meta-gradients and optimizing the meta-objective comes with high computational overhead. It is not recommended for practical SMD implementation but rather for understanding biases in the meta-objective. An offline experiment was conducted on a multi-layered perceptron (MLP) to study the sensitivity of optimized hyperparameters to the horizon. The experiment involved fitting learning rate decay schedules offline and optimizing parameters like the initial learning rate and learning rate decay exponent. The network had two layers of 100 hidden units with ReLU activations. Weights were initialized with a zero-mean Gaussian. SMD optimization trajectories on meta-objective surfaces converged to the global optimum, with longer horizons favoring a smaller learning rate decay exponent \u03b2. Meta-objective surfaces varied based on the time horizon. The meta-objective surfaces varied based on the time horizon, with significant differences in the final \u03b2 value between 100 and 20k step horizons. Short horizons initially performed better but were surpassed by longer horizons. Online adaptation was studied for short-horizon bias using Algorithm 1 to adjust hyperparameters while training the network. For MNIST experiments, an MLP network with two hidden layers of 100 units and ReLU activations was used. For CIFAR-10 experiments, a CNN network with 3 convolutional layers and 2 max pooling layers was utilized. Meta-optimization involved adapting the learning rate and momentum over 100 steps of Adam for every 10 steps of regular training. Learning rate was exponentially decayed to 1e-4 after 25k steps. The learning rate and momentum were re-parameterized for optimization in the log space. Training curves with online SMD were compared to fixed hyperparameters. SMD runs showed efficient optimization of the meta-objective. Online SMD with a single mini-batch lookahead trajectory was considered to address conservative learning rate choices. The deterministic lookahead scheme in meta-optimization led to large adapted learning rates causing instability. Stochasticity of mini-batch training cannot be ignored. A noisy quadratic toy problem highlighted the difference between greedy and optimal learning rate schedules. Greedy schedules decay learning rates drastically, while optimal schedules maintain high rates for steady progress on low curvature directions, resulting in lower loss. This bias arises from the combination of stochasticity and training dynamics. The bias in learning rate schedules arises from the combination of stochasticity and ill-conditioning in neural net training. Empirical verification using gradient-based meta-optimization showed fast learning rate decay and poor long-run performance. Removing ill-conditioning or stochasticity may improve performance in the short-horizon regime. The text discusses the potential for short-horizon meta-optimization to work well with variance reduction techniques in optimization algorithms. It calculates the mean and variance of parameters and velocity over time, as well as the covariance between them. The dynamics of the system are described, leading to the derivation of optimal learning rates at different time steps. The optimal learning rate at time step T - k is calculated by rewriting L min in terms of A T - k min. Theorem 4 states the formula for the optimal learning rate \u03b1 (t) at timestep t. The proof involves induction on k and setting a specific identity to zero."
}