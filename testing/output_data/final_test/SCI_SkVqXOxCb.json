{
    "title": "SkVqXOxCb",
    "content": "Generative adversarial networks (GANs) have become successful in generating realistic images, but often face issues like mode collapse. Coulomb GANs approach the learning problem as a potential field, attracting generated samples to training set samples while repelling each other. The model learns to generate samples according to the whole target distribution, avoiding mode coverage issues. Coulomb GANs have only one optimal Nash equilibrium where the model distribution matches the target. Coulomb GANs aim to address mode collapse in GANs by utilizing a potential field approach to generate samples that match the target distribution. They have one optimal Nash equilibrium where the model distribution equals the target distribution. GANs excel at creating realistic images and text through a game between a generator and discriminator network. Despite their successes, GANs face challenges such as local Nash equilibria where neither the generator nor discriminator can improve their objectives. Despite recent successes, GANs have issues such as mode collapsing, where samples are generated in limited regions, and difficulty in accurately modeling the density of training samples. Batch normalization can help with mode collapsing but introduces harmful fluctuations. Various methods have been proposed to address mode collapsing without batch normalization. GANs struggle to accurately model the density of training samples, as the discriminator can only distinguish between the support of the model distribution and the target distribution. While GANs can locally align model and target densities through proper objectives, they fail to equalize them globally. This leads to the generator neglecting large parts of the target distribution, as the discriminator does not inform it where probability mass is missing. The discriminator of GANs may forget previous errors of the generator, leading to oscillatory behavior instead of convergence. Recent advancements have shown that GAN learning can converge to a local Nash equilibrium, but this does not guarantee optimal generative performance and can result in mode collapse. The Coulomb GAN model addresses the issue of GANs not capturing global sample density effectively. It introduces a potential field created by point charges to avoid shortcomings and achieve optimal distribution matching. The Coulomb GAN model introduces the concept of learning in a potential field, proving its optimal solution. It discusses how the discriminator and generator work in a Coulomb GAN, highlighting its effectiveness in capturing the original distribution well. Other GAN approaches like Geometric GANs and Energy-Based GANs are also mentioned for distribution alignment. The curr_chunk discusses various approaches like McGANs, Generative Moment Matching Networks, and MMD nets that optimize energy landscapes to approximate target distributions. These approaches use loss functions based on the maximum mean discrepancy criterion. Unlike other methods, the Plummer kernel is employed in this work for optimal solution finding. The Plummer kernel used in this work leads to optimal solutions, ensuring gradient descent convergence. MMD GAN approaches face sampling issues in high-dimensional spaces, while the Coulomb GAN learns a discriminator network to improve its potential field approximation. The red samples repel each other, with a vector field showing forces to equalize potential differences. The approach is the first to prove optimality in learning a target distribution using Coulomb potential. This method is based on previous works like Coulomb Potential Learning and Potential Support Vector Machine. The approach utilizes a potential function based on Plummer kernels for optimal unsupervised learning in Coulomb GANs. GAN learning aims to align model and target densities by minimizing the difference of densities \u03c1(a). The discriminator D(a) distinguishes between generated and target samples, while the generator G(z) maps random variable z to target samples. The GAN uses the gradient of the discriminator to improve the generator by adjusting the generated samples towards the direction where target examples are more likely. This adjustment is based on the local neighborhood of the samples. Theoretical analysis of GAN learning can be done at three different levels: in the space of distributions p x and p y, in the space of functions G and D, or in the space of the parameters of G and D. Level (1) is used to prove convergence of GAN learning in a scenario where small, local moves are made in p x space. In practical implementations, changes to the generator function G can only be made indirectly through small adjustments to its parameters. The mapping from a distribution p x to the generator G is not continuous, as moving a distribution p1 x to p2 x by shifting density to an isolated region does not result in a continuous change in the inducing generator functions G1 and G2. In GAN learning, small steps in parameter space lead to small steps in function space and distribution space. Local Nash equilibria can occur in function space due to the difficulty of making small steps in distribution space. Coulomb GANs do not exhibit local Nash equilibria in the functions G and D, making it a unique formulation in GAN learning. Coulomb GANs are parametrized neural networks that are not immune to issues like overfitting and underfitting. Gradients vanish when the density approaches a Dirac delta-distribution, similar to how electric point charges are represented. The electric potential created by a point charge influences the space globally, not just locally. This concept is applied to GAN learning to introduce a similar approach. In GAN learning, a potential function \u03a6(a) is introduced to minimize the difference of densities \u03c1(a). The potential \u03a6(a) allows each point to influence every other point in space if the kernel k is chosen properly. Gradient optimization of \u03a6(a) may not always lead to \u03a6(a) = 0 for all points, depending on the kernel used. The Coulomb GANs utilize a kernel leading to the Coulomb potential, where samples influence each other over long distances. Real and generated samples generate a potential field, with samples of the same class repelling each other but attracting samples of the opposite class. The generator can move its samples along the forces generated by the potential field. The discriminator learns to predict the potential function. The discriminator learns to predict the potential function to approximate the current potential landscape of all samples. The generator distributes its samples across the field to minimize energy and avoid mode collapse. In an electrostatic field, particles organize themselves to equalize forces and eliminate potential differences. In an electrostatic field, particles organize themselves to equalize forces and eliminate potential differences. The Coulomb GAN models this behavior by using real and generated samples to minimize energy and prevent mode collapsing. The generated samples are attracted to real samples and repel each other, similar to charges in an electrostatic field. The potential function gives rise to a field that applies forces on the samples, pushing them towards lower energy configurations. The Coulomb GAN aims to make the potential \u03a6 zero everywhere via the field E(a), which is the negative gradient of \u03a6. Proper kernels can push \u03a6 to zero, leading to \u03c1(a) = 0 for all a. During learning, the location a = G(z) changes, ensuring conservation of generator samples. The continuity equation in the context of the Coulomb GAN describes how the difference in distributions of samples changes as particles move along the field. It connects moving samples to changes in generator density, indicating whether positive or negative charges are present and whether samples move towards or away from a given field. The equation states that if the generator density increases, samples flow into the region, and if it decreases, they flow outwards. The density difference \u03c1(a) indicates how many samples are locally available for being moved. At each local minimum and local maximum a of \u03c1 we obtain \u2207 a \u03c1(a) = 0, which ensures the uniform decrease of the maximal absolute density differences. The choice of kernel is crucial for Coulomb GANs, with the m-dimensional Coulomb and Plummer kernels leading to optimal solutions but numerical instabilities for large m. The Coulomb potential for the Coulomb GAN is constructed using a low-dimensional Plummer kernel. The resulting field and potential energy converge to zero for freely moving generated samples. Theorem 1 states that for freely moving generated samples, the densities p x (.) and p y (.) equalize over time when minimizing energy F with the low-dimensional Plummer kernel by gradient descent. The Coulomb GAN minimizes the electric potential energy using a stochastic gradient descent approach with mini-batches. The Coulomb GAN minimizes the electric potential energy using a stochastic gradient descent approach with mini-batches. To address the high variance in estimating the potential between generated and training set points, a network that generalizes over mini-batch specific potentials is proposed. This approach aims to counteract the need for extremely low learning rates, which hinder practical learning. The Coulomb GAN uses generator and real world samples to create a batch-specific potential. The discriminator's goal is to learn the potential averaged over mini-batches, discriminating between real and generated data. The generator aims to move its samples to areas where they are missing, minimizing the approximated energy. The Coulomb GAN utilizes generator and real world samples to create a batch-specific potential. The generator minimizes the approximated energy as predicted by the discriminator, with loss functions pushing negative potential values towards zero. The Coulomb GAN consists of a generator to generate model samples and a discriminator for learning signal. The discriminator in the Coulomb GAN uses each sample in the mini-batch twice for performance reasons. The learning algorithm is detailed in Algorithm 1 in the appendix, and convergence of the GAN learning process was proved for a two time-scales update rule. The Coulomb GAN achieves a local Nash equilibrium with a unique global Nash equilibrium, where the energy minimization leads to a single optimum point. The local Nash equilibrium is also the global Nash equilibrium, with no other local equilibria existing. The output distribution of G is equal to p x = p y. Learning Coulomb GANs involves training neural networks for the generator (G) and discriminator (D) with differentiable parameters. Recent research suggests that local minima in deep learning become less restrictive with increasing network depth. The main challenge lies in approximating the complex potential function \u03a6 in a high-dimensional space. Adequate data sampling and averaging are crucial when training the discriminator. The Coulomb GAN learning process involves training neural networks for the generator and discriminator with differentiable parameters. It is important to ensure enough data sampling and averaging, and to address approximation problems for optimal learning. The formulation of GAN learning as a potential field helps solve mode collapsing issues, where a normal GAN may get stuck in local Nash equilibria. The Coulomb GAN utilizes a low-dimensional Plummer Kernel for optimal performance and to avoid numerical issues. The FID metric is used to evaluate the quality of the GAN, comparing it to other GAN models like BEGAN, DCGAN, and WGAN-GP. Additionally, MMD-GAN is compared to the Coulomb GAN, which uses a Gaussian Kernel instead. The Coulomb GAN, similar to the Coulomb GAN, uses a Gaussian Kernel instead of the Plummer Kernel. The highest FID score during training is reported, with images generated using a random seed. The implementation for these experiments is available online. The Coulomb GAN is shown to not suffer from mode collapse when fitting a Gaussian Mixture of 25 components. It is also able to learn distributions in high dimensional spaces, demonstrated by training on popular image datasets such as CelebA and LSUN bedrooms. The DCGAN architecture was used with modifications for experiments on 64x64 pixel images from LSUN BID40 and CIFAR-10 datasets. Changes included smaller random seed dimensions, increased feature channels in the discriminator, and specific optimizer settings. Learning was monitored using the FID metric and the Plummer kernel was set to 1. The learning rate was scaled down by a factor of 10 until the FID plateaus. Results are reported in Table 1b, with Coulomb GANs outperforming BEGAN and DCGAN but being outperformed by Improved Wasserstein GAN. The low FID of Coulomb GANs is attributed to the wide variety of samples in the generated images, such as different faces, backgrounds, eye colors, and orientations. Nearest neighbor analysis was conducted on batches of 1024 generated faces from Coulomb GAN. The Coulomb GAN was able to spread out its samples over the whole target distribution, as indicated by generating duplicates with a probability of around 50% using samples of size 1024. Nearest neighbor analysis confirmed that the Coulomb GAN does not just memorize training images. The Coulomb GAN, a generative adversarial network with strong theoretical guarantees, can approximate the real distribution perfectly with sufficient capacity and training. Its potential field outperforms MMD based approaches due to the low-dimensional Plummer kernel, effectively eliminating mode collapse in GANs. Coulomb GANs address mode collapse in GANs by ensuring generated samples cover different distribution regions. However, it may produce nonsensical interpolations more frequently. Improving discriminator learning and capacity can enhance generative performance. The key challenge lies in accurately approximating the potential field \u03a6. Coulomb GANs are based on a potential field created by point charges, avoiding mode collapsing and accurately modeling the target distribution. If learning converges, it does so to the optimal solution, showing that samples can move freely. In Coulomb GANs, the generator may collapse onto specific regions of support, leading to the discriminator classifying all points from other regions as \"real.\" This results in the discriminator seeing more generated data points than real ones, affecting its prediction error for a proper objective. The discriminator in Coulomb GANs aims to minimize prediction error by outputting 1/3 for every point from C1, regardless of the real distribution shape. The generator matches the target distribution shape locally, improving if not matched. When local improvements are no longer possible, the discriminator output becomes constant, leading to stable conditions with zero gradients for both discriminator and generator. The GAN learning process has reached a local Nash equilibrium, with the discriminator detecting deviations from the real distribution and pushing the generator back to its original shape. Fluctuations in parameters depend on learning rate and sample size, with decaying learning rates used to anneal random fluctuations for true mathematical convergence. Theorem 1 states that densities equalize over time when minimizing energy with the low-dimensional Plummer kernel by gradient descent. Convergence is faster for larger dimensions. The expression sign(\u2207 \u00b7 E(a)) = sign(\u03c1(a)) holds for local maxima or minima of \u03c1. The integral \u2207 \u00b7 E(a) is dominated by large negative values of \u2207 2 a k around a, which can be decreased by adjusting . This ensures that at each local minimum and maximum of \u03c1, sign(\u03c1(a)) = -sign(\u03c1(a)), causing points to move towards zero. The movement of \u03c1 towards zero is analyzed, showing that new maxima or minima cannot appear and the convergence to zero stops at zero. The movement is lower bounded by \u03c1(a) = \u2212sign(\u03c1(a))\u03bb\u03c1 2 (a), slowing down at \u03c1(a) = 0. The differential equation solution leads to \u03c1 converging to zero over time. The Laplace operator in spherical coordinates is defined for the low-dimensional Plummer kernel, showing negative values with a minimum at r = 0 and increasing with r. The operator is restricted for d = m - 3 to ensure an increase in its value. The Laplace operator in spherical coordinates is defined for the low-dimensional Plummer kernel, showing negative values with a minimum at r = 0 and increasing with r. It is restricted for d = m - 3 to ensure an increase in its value. In the following sphere S \u03c4 (a), a sphere with radius \u03c4 around a is defined where sign(\u03c1(b)) = sign(\u03c1(a)) for each b \u2208 S \u03c4 (a). The gradient \u2207 2 k(a, b) is continuous and differentiable. At each local minimum and maximum of \u03c1, sign(\u03c1(a)) = -sign(\u03c1(a)), causing the maximal and minimal points of \u03c1 to move towards zero. Points not in the vicinity of these points cannot become maximal points in an infinitesimal time step. The factor \u03bb, dependent on k and initial \u03c1, is proportional to d. Larger d leads to larger |\u2207 \u00b7 E(a)|. Initial conditions \u03c1 may lead to \u03bb \u2192 0, but impossible in applications. Therefore, maximal or minimal points approach zero faster or equal than given by \u03c1 converges to zero function over time. The global Nash equilibrium in Coulomb GANs implies no other local equilibria exist. The Coulomb potential energy is minimized by the generator, ensuring only one minimum exists. The Coulomb GANs reach a global Nash equilibrium with only one minimum, ensuring the generator minimizes the energy to reach p y = p x. The optimal pair (D * , G * ) leads to the global minimum, with no other local equilibria possible according to Theorem 1. Local Nash equilibria are not possible in GANs, which are sample-based models drawing samples from the model for learning. Mini-batches consist of target samples Y and model samples X. Unbiased estimates of the model distribution p x (.) and target distribution p y (.) are obtained using delta distributions for finite samples. The paper simplifies notation by using the hat sign for estimates and provides unbiased estimates for the potential energy. Unbiased estimates for potential, energy, and field are obtained using fixed samples X and Y. The sample-based formulation in GANs involves point charges with local energy minima or maxima at sample locations. Field lines guide model samples towards real-world samples, with factors N y and N x representing the charge of each sample. The Coulomb GAN uses gradient descent to follow the force induced on samples by the field. It utilizes a synthetic data set with a discriminator network and Plummer kernel in 3 dimensions. The generated samples closely approximate the target distribution with correct ratios for each component. The Coulomb GAN outperforms other methods by capturing the within-mode spread of a Gaussian distribution and avoiding within-cluster collapse. It accurately captures the underlying distribution, does not miss any modes, and places most of the probability mass on the modes. Pseudo code for training GANs is provided, emphasizing the importance of deriving with respect to 'a' only. The Coulomb GAN is trained using minibatch stochastic gradient descent to update discriminator and generator weights. Gradient calculations and weight updates are performed while ensuring back-propagation is only done with respect to 'a'. Training results on LSUN bedroom and CIFAR 10 datasets are shown."
}