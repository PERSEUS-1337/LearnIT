{
    "title": "S1ln1TNKvH",
    "content": "In this paper, a harmonic acoustic model for pitch detection is designed, combining conventional convolution and sparse convolution to capture global harmonic patterns from local patterns. The model outperforms existing systems when trained on the MAPS dataset, and even surpasses a more complex system trained on the MAESTRO dataset with data augmentation. The harmonic model shows potential for advanced automatic music transcription systems, highlighting the effectiveness of deep learning in pitch detection. This paper focuses on pitch detection for solo piano music using an acoustic model called the Kelz model, based on previous work by Schl\u00fcter & B\u00f6ck. The Kelz model predicts active pitches in a frame and is a key component in automatic music transcription systems. The Kelz model, inspired by LeNet-5, uses convolution layers for pitch detection but lacks capturing harmonic patterns, impacting generalization. Hawthorne et al. (2018) developed an AMT system with onset and frame detectors, incorporating bi-directional LSTM and skip connections. They later used a similar system for piano sound generation in a larger system. The AMT system in Hawthorne et al. (2019) uses more features and a separate detector for offset detection. Kelz et al. (2019) designed an AMT system with three separate detectors for pitch, onset, and offset detection. Bittner et al. (2017) developed a fully convolutional acoustic model called the harmonic constant-q transform (HCQT) model using HCQTs as input representation. The HCQT model uses HCQTs as input representation, which includes a harmonic dimension capturing fundamental frequency/pitch and harmonics. Elowsson (2018) designed an AMT system with cascaded networks and skip connections for training. The system consists of two networks, N1 and N2, with N1 detecting pitches using variable-q transforms and N2 providing more accurate pitch estimation. N1 and N2 capture harmonic patterns from more frequency bins compared to the HCQT model, which focuses on local patterns. Four additional networks estimate onsets, offsets, and notes probabilities. The fundamental frequency, or pitch, is the lowest frequency in a music note waveform, with other frequencies being integer multiples of the pitch. The fundamental frequency, or pitch, is the lowest frequency in a music note waveform, with other frequencies being integer multiples of the pitch. In polyphonic music, multiple notes can be active simultaneously, leading to challenges in pitch detection. A VQT spectrogram shows frequency stripes due to non-perfect pitches and windowing effects in the DFT calculation. Harmonic patterns are used to determine if a note is active based on the energy of its pitch and harmonics. Harmonic patterns are essential for pitch detection in polyphonic music. Sparse convolution is needed to capture the sparsely distributed frequencies of harmonic patterns. This type of convolution is critical for detecting both pitch and harmonic frequency patterns accurately. The paper introduces a harmonic acoustic model for pitch detection, utilizing VQTs as inputs. It captures local frequency patterns with conventional convolution and global harmonic patterns with sparse convolution. The model outperforms other systems by 3.5% when trained on MAPS, even surpassing a more complex system trained on the MAESTRO dataset with data augmentation. The MAESTRO dataset undergoes complex data augmentation and has a training split 15 times larger than MAPS. MAPS is a piano dataset created from MIDI files using a Yamaha Disklavier piano with different sound settings. The MAESTRO dataset consists of 270 recordings, with 160 musical compositions and 9 settings overlapping. The dataset is partitioned into training, validation, and test splits, with no instrument and composition overlap between training and test splits. The test split includes 60 recordings from Disklavier, while the training split has 139 recordings and the validation split has 71 recordings. MAESTRO is a piano dataset generated by Yamaha Disklavier grand pianos from an international piano competition, with 1184 recordings totaling about 172 hours and 103 GB in size. The MAESTRO dataset has a total duration of 172 hours and a size of 103 GB, while MAPS only has 18 hours and 11 GB. MAESTRO is partitioned into training, validation, and test splits following recommendations. The training split has 954 recordings, validation has 105, and test has 125 recordings. The training split of MAESTRO is 15 times larger than MAPS. The harmonic model uses VQT as input representation with specific settings for computation. The maximum frequency is determined by MIDI note 132 multiplied by a factor of 2 1/B, resulting in 336 frequency bins. The hop size is set to 64 samples, down-sampled by a factor of 22 for pitch detection. Frame-wise pitch detection is formulated as a multi-label classification problem using frame labels from the MIDI file. Onset and offset times for notes are expressed in samples, and a harmonic acoustic model is proposed for frame-wise pitch detection. The proposed harmonic acoustic model for frame-wise pitch detection is structured in Table 1, with layers defined for convolution, dropout, sparse convolution, max-pooling, and fully connected layers. The model is divided into three parts, with the input shape denoted as (none \u00d7 none \u00d7 336 \u00d7 1) and dynamic dimensions represented by \"none\". The proposed harmonic acoustic model for frame-wise pitch detection is structured with layers for convolution, dropout, sparse convolution, max-pooling, and fully connected layers. It is divided into three parts, with the first part using four consecutive convolution layers to capture local frequency patterns. The second part includes a sparse convolution layer that selects 50 frequency bins relative to a pitch f0 for detecting its presence, including harmonics and non-harmonics. The harmonic acoustic model for frame-wise pitch detection includes 50 frequency bins per octave, which are converted to 36 bins per octave for the harmonic model. Each f0 has 79 input features, with out-of-range features assumed to be zeros. Only the first 264 of the 336 frequency bins are used for pitch detection. The output after the sparse convolution layer is (none \u00d7 none \u00d7 264 \u00d7 256), with 256 features for each pitch. Max-pooling with a receptive field of 1 \u00d7 3 is used to down-sample the output for note-level labels. The harmonic model uses max-pooling to down-sample output to note level, with ReLU activation function and batch normalization. Dropout is used to control overfitting. It captures harmonic frequency patterns explicitly with sparse convolution. The Kelz model learns harmonic patterns implicitly, leading to overfitting on training data with different timbres and composition styles. The harmonic model differs from network N1 of Elowsson (2018) by capturing more local frequency patterns and using sparse convolution. It avoids overfitting by using a small overall receptive field. The binary cross entropy loss is used for training. The binary cross entropy loss is used for training in the harmonic model. Performance in pitch detection is measured by the f-measure, which includes precision and recall metrics. Metrics can be calculated for individual recordings and averaged, or treated as an ensemble for direct calculation. The harmonic model is compared to the Kelz model and other models. The harmonic model is compared to the Kelz model and other complex pitch detection systems. Various techniques like data augmentation, RNN, HMM, joint-task training, and larger training splits were used to enhance performance. To ensure a fair comparison, two tricks - data augmentation and RNN - are applied to the harmonic model. Data augmentation includes pitch shifting and changing frequency powers. The harmonic model is enhanced by topping it with an LSTM layer after training. The removed layers are replaced with an LSTM of 64 hidden units followed by an FC(1) layer. Tensorflow 1.13.1 is used as the neural network framework with the Adam optimizer. The batch size and frame count vary between the harmonic model with and without LSTM. The performance of different pitch detection systems is compared, including systems trained on self-made datasets, MAESTRO, and MAPS. The Kelz model results are cited from a study that tested it on MAPS. MAESTRO cannot be used for both training and testing due to instrument overlap, so only test performance is compared for generalization assessment. The test performance of different pitch detection systems on MAPS is compared, with some using ensemble results and others using average results. The system by Hawthorne et al. (2019) without data augmentation outperforms all existing systems except their own, attributed to MAESTRO's larger training split and complex network structure. The system with data augmentation surpasses the harmonic model with data augmentation. The harmonic model outperformed existing pitch detection systems, reaching a record high of 85.82 with data augmentation and LSTM. The model effectively captures complex frequency interactions in polyphonic music through convolution and sparse convolution. The harmonic model with an LSTM layer outperformed a complex system trained on MAESTRO, showing potential for advanced AMT systems. Future directions include exploring complex spectrograms and utilizing deep complex networks. There is no standard procedure for computing VQT, but a possible sketch can be provided. The advantages of CQT for characterizing harmonic frequency patterns include log-linear frequency bins and proportional bandwidth. However, CQT has low time resolution for lower frequencies, leading to the development of VQT. VQT (Holighaus et al., 2013) expands on CQT by allowing for larger bandwidths, improving time resolution by shrinking frame length. Filters in VQT have finite bandwidth and infinite length in the time domain. Frame length is defined as the zone containing a major proportion of energy, with a frame length of 2.88/\u2126 k seconds or 2.88 \u00d7 sr/\u2126 k samples for over 99% energy. In VQT, the frame length is determined by the bandwidth and sampling rate. To achieve a maximum frame length of 20000 samples at a sampling rate of 44.1 kHz, the minimum bandwidth should be set to 6.35 Hz. Zero padding can be used to avoid undesirable effects when computing spectral coefficients. The hop size in VQT should satisfy sr/h \u2265 \u2126 max to ensure proper processing. After determining the frame length in VQT based on bandwidth and sampling rate, the hop size should satisfy sr/h \u2265 \u2126 max. The spectrogram is converted to dB scale for amplitude calculation, with padding values set to -200 for zero power level. Setting the number of frequency bins per octave to 84, the VQT is calculated. The pitch of the VQT is then shifted by 0, \u00b11, and \u00b12 frequency bins following a specific procedure. Step 1 Output: Shift pitch of VQT by 0, \u00b11, and \u00b12 frequency bins. Recover DFT for each pitch-shifted VQT. Calculate VQT for B = 36.\n\nStep 2 Output: Determine frame length in VQT based on bandwidth and sampling rate. Convert spectrogram to dB scale. Set padding values to -200. Calculate VQT with 84 frequency bins per octave. Shift VQT pitch by 0, \u00b11, and \u00b12 frequency bins.\n\nSummarized Text Chunk: Shift the pitch of VQT by 0, \u00b11, and \u00b12 frequency bins, recover DFT for each pitch-shifted VQT, and calculate VQT for B = 36."
}