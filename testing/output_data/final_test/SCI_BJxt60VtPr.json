{
    "title": "BJxt60VtPr",
    "content": "Predictive coding theories suggest that the brain learns by predicting observations at different levels of abstraction, such as view prediction. Humans excel at imagining scenes from alternative viewpoints, which is crucial for 3D visual recognition. This paper explores view-predictive representation learning and proposes inverse graphics networks to map 2.5D video streams to stable 3D feature maps of a scene. The model can project these maps to new viewpoints for prediction and matching against target views. The proposed model utilizes contrastive prediction losses to handle visual input stochasticity and scale view-predictive learning to photorealistic scenes. It learns 3D visual representations for semi-supervised and unsupervised 3D object detection by estimating motion in dynamic scenes. This work empirically demonstrates the usefulness of view prediction as a self-supervised task for 3D object detection, aligning with predictive coding theories on learning through prediction. Theories with empirical support show that stimuli are processed faster when predictable, prediction error increases neural activity, and disproven expectations aid learning. Humans excel at view prediction tasks, effortlessly imagining missing information for predicting alternative views, tightly linked with visual perception. The ability for 3D perception emerges early in infancy despite 2D visual input. Computational models are trained to predict views of static scenes in 3D using 2.5D video streams as input. The models map 2.5D input streams into 3D feature volumes and account for camera motion. The model estimates camera motion, projects 3D feature maps to new viewpoints, and uses contrastive losses for error measurement. It inpaints occlusions and infers object 3D extents from RGB-D input, improving 3D object detection with few annotations. The model uses view contrastive pretraining to detect objects in 3D with few annotations. It can also detect moving objects in 3D without human annotations by estimating motion fields and clustering objects. View prediction is utilized to aid object detection, rather than for generating images for graphics applications. The work of Eslami et al. (2018) focused on view prediction in full scenes using toy data with colored 3D shapes. Their model lacked generalization across scenes with varying numbers of objects. In comparison, another study demonstrated strong generalization in view prediction across scenes, objects, and arrangements, but only in simulated scenes. The scalability and usefulness of view prediction for self-supervised visual representation learning is questioned, which is the focus of our work. Our work introduces novel view-contrastive prediction objectives for self-supervised visual representation learning. We demonstrate superior performance in semi-supervised 3D object detection compared to existing methods. Additionally, we propose an unsupervised 3D moving object detection method that outperforms previous approaches. Our work introduces novel view-contrastive prediction objectives for self-supervised visual representation learning, showing superior performance in semi-supervised 3D object detection. We also propose an unsupervised 3D moving object detection method that outperforms previous approaches. The code and data will be publicly available upon publication. The researchers introduced novel view-contrastive prediction objectives for self-supervised visual representation learning, focusing on the video domain. Their work aimed to preserve mutual information between future bottom-up extracted features and predicted contextual latent features. The researchers focused on the video domain, using different architectures for contextual and bottom-up representations with a 3D bottleneck. They trained a mobile agent with an RGB camera and depth sensor to imagine 3D scenes and estimate egomotion. The agent chooses viewpoints randomly during testing and estimates egomotion on-the-fly from RGB-D inputs. The researchers developed a recurrent neural network (RNN) model with a memory state tensor to capture 3D deep features of the world space. The model's imagination aims to create a geometrically-consistent representation of the environment using groundtruth depth data. Our model utilizes differentiable modules for 3D feature imagination and 2D image space transition. It improves upon existing geometry-aware recurrent neural networks by handling general camera motion and decoding 3D maps into 2D feature maps for view-contrastive prediction. Our model utilizes view-contrastive prediction with 3D feature imagination and 2D image space transition. It converts point clouds to 3D occupancy grids and then to 3D feature tensors using a convolutional encoder-decoder network. Egomotion estimation module computes relative 3D rotation and translation. Further implementation details are provided in the appendix. The egomotion estimation module computes the relative 3D rotation and translation between the current camera pose and the reference pose, allowing for feature tensor registration. The 3D feature tensors are suitable for a 3D coarse-to-fine alignment search, similar to the optical flow model PWC-Net. The 3D-bottlenecked RNNs learn to map 2.5D video streams to egomotion-stabilized 3D feature maps by optimizing for view-contrastive prediction. The neural architecture is trained to optimize a view prediction objective, with an egomotion module trained supervised using annotated egomotion pairs. The latent map update module aggregates egomotion-stabilized feature tensors into a memory tensor. The neural architecture updates a memory tensor with feature tensors and projects 3D features into a 2D map. It also includes a 3D object detector and view-contrastive rendering for predicting unseen input features. The neural architecture updates a memory tensor with feature tensors and projects 3D features into a 2D map. It includes a 3D object detector and view-contrastive rendering for predicting unseen input features. Two types of representations for the target view are considered: top-down and bottom-up. 2D and 3D versions of these representations are constructed using architecture modules. Contrastive losses pull corresponding features close together and push non-corresponding ones beyond a margin of distance. The neural architecture updates a memory tensor with feature tensors and projects 3D features into a 2D map. It includes a 3D object detector and view-contrastive rendering for predicting unseen input features. The losses ask tensors depicting the same scene to contain the same features, with a focus on metric learning strategies. The model learns to map RGB-D input to a complete 3D imagination and trains a motion estimation module to predict 3D motion fields between consecutive 3D maps. Our 3D FlowNet model focuses on estimating motion for independently moving objects in a scene. It utilizes a 3D adaptation of the PWC-Net optical flow model and iterates across scales to align 3D displacement vectors. The model is trained using tasks such as synthetic transformation of feature maps and unsupervised 3D temporal feature matching. The proposed 3D imagination flow model aligns 3D displacement vectors by utilizing a differentiable 3D spatial transformer layer. It extends self-supervised 2D optical flow to 3D feature constancy and includes 3D voxel occupancy estimation for accurate 3D flow field estimates. This approach offers advantages over 2D optical flow and 3D scene flow by avoiding occlusions and projection artifacts. The 3D imagination flow model computes non-rigid 2D flow fields between visual features, generating 3D object segmentation proposals based on motion saliency scores. Training is done in CARLA for accurate 3D segmentation proposals in video scenes. Training for accurate 3D segmentation proposals in video scenes is conducted in CARLA, an open-source simulator of urban driving scenes. Data is generated by capturing frames from 1170 autopilot episodes with various weather conditions and locations. 36 viewpoints are defined along a hemisphere in front of the ego-car, with 6 random viewpoints sampled per episode. Train/test examples are created by combining different viewpoints, filtering out frames with zero objects in the \"in bounds\" region. We use Town1 and Town2 data for training and testing, respectively, with no overlap. Additionally, we evaluate our 3D feature representations in tasks such as object detection and motion estimation. The proposed view-contrastive prediction is used for pretraining in 3D object detection. The proposed view-contrastive prediction is utilized for pretraining in 3D object detection. The inverse graphics network weights are pretrained, followed by training a 3D object detector module supervised to map a 3D feature volume to 3D object boxes. The model's performance is evaluated across different levels of label supervision using subsets of the CARLA train set, showing mean average precision for car detection based on the number of annotated 3D bounding box examples. The model optimizes for RGB regression with 6 DoF camera motion, trained from random weight initialization. Pre-training improves results in low-data scenarios, especially for view-contrastive learning. The supervised model outperforms with more labeled data. Comparisons against alternative view prediction models were not possible. In this study, the researchers evaluate the usefulness of 3D predictive feature representations learned in the CARLA simulator for real-world 3D object detection on the KITTI dataset. They use view prediction pre-training in CARLA and box supervision from KITTI to assess 3D object detection performance. Existing real-world datasets like KITTI lack diverse camera viewpoints necessary for view-predictive learning. The study evaluates the transferability of features learned in the CARLA simulator for real-world 3D object detection on the KITTI dataset. They compare view contrastive prediction pre-training, view regression pre-training, and random weight initialization. Results show that view-contrastive pretraining outperforms the other methods, even across distribution shifts in data density and artifacts. In this section, the model's ability to detect moving objects in 3D without annotations is tested using 3D motion vectors. The validation set is split into scenes with stationary and moving cameras, as moving object detection is more challenging with a moving camera. Precision-recall curves for 3D moving object detection under a stationary camera are shown, comparing the model with RGB view regression and a 2.5D baseline. Our model proposes object masks by thresholding and clustering 2D flow magnitudes, which are then mapped to 3D boxes by segmenting the input point cloud. It outperforms baselines and avoids underestimating object extents. Precision-recall curves for 3D moving object detection under a moving camera are compared with different models in Figure 5. The 2.5D baseline uses optical flow estimated from PWC-Net and stabilizes the flow by subtracting ground-truth scene flow. The egomotion module effectively disentangles camera motion from 3D feature maps. Performance drops after stabilizing 2D flows due to imperfect scene flow estimation. Comparisons with unsupervised object segmentation methods have been attempted. The 3D FlowNet module is evaluated for accuracy in predicting object bounding boxes and frame-to-frame displacements in video sequences. The success of previous models may rely on specific priors for object location and size parameters, which are not assumed in the evaluation of the CARLA dataset. The 3D FlowNet module is evaluated for accuracy in predicting object motion in dynamic scenes using two-frame video sequences from the CARLA test set. Results show lower error compared to a baseline RGB regression method, indicating successful learning of correspondent features for moving objects. The proposed model has limitations due to the need for an embodied agent and high GPU memory consumption. Future work could involve sparsifying the feature grid or using points instead of voxels. The model learns 3D feature abstractions from 2.5D input by minimizing 3D and 2D view contrastive prediction objectives. The curr_chunk discusses the benefits of view-contrastive prediction for 3D object detection and the ability to estimate dense 3D motion fields without human supervision. It highlights the model's capability to drive 3D object detection through visual imagination in 3D without human annotations. The implementation details of the 3D-bottlenecked architecture, egomotion module, and 3D imagination FlowNet are provided, along with additional experiments and visualizations of the output. The input images are 128 \u00d7 384 pixels, and input point clouds are trimmed to a maximum of 100,000 points. The input images are 128 \u00d7 384 pixels, and input point clouds are trimmed to a maximum of 100,000 points. The 2D-to-3D unprojection module converts the input 2D image and pointcloud into a 4D tensor by filling the 3D imagination grid with samples from the 2D image grid using perspective projection. The 3D encoder-decoder network processes the unprojected RGB and binary occupancy grid data. It uses a specific architecture for feature encoding and decoding, followed by egomotion estimation to calculate camera viewpoint changes relative to a reference coordinate system. The egomotion module in the 3D encoder-decoder network allows for full 6-DoF camera motion estimation using a coarse-to-fine architecture. It incorporates spatial pyramids, incremental warping, and cost volume estimation for better performance. The egomotion module in the 3D encoder-decoder network enables full 6-DoF camera motion estimation through a coarse-to-fine approach. It involves downsampling feature tensors, generating candidate rotations, calculating cost volumes, averaging alignment scores, and applying a fully-connected network to convert scores into a 6D vector for warping the second tensor into alignment with the first. The egomotion module in the 3D encoder-decoder network enables full 6-DoF camera motion estimation through a coarse-to-fine approach. It involves downsampling feature tensors, generating candidate rotations, calculating cost volumes, averaging alignment scores, and applying a fully-connected network to convert scores into a 6D vector for warping the second tensor into alignment with the first. The module transforms the 6D vector to align with a specified transformation, accumulating corrections at each scale without repeatedly warping input tensors to avoid interpolation errors. The 3D-to-2D projection module renders 2D feature maps based on a desired viewpoint by projecting the 3D feature state. The perspective-transformed tensor M (t) is warped to align viewing rays, then passed through a CNN to convert it to a 2D feature map M with specific architecture. The curr_chunk describes the process of max-pooling, 3D convolution, reshaping, and 2D convolutions with specific kernel sizes and strides. It also mentions the use of supervised labels for training a 3D FlowNet model with synthetic transformations and an unsupervised loss based on variational loss. The curr_chunk discusses the importance of sampling small and zero motions for accurate learning in a 3D FlowNet model. It also introduces a variational loss to address overly-smooth flow issues and a smoothness loss to penalize local flow changes. Additionally, it mentions the goal of estimating occupied and free voxels in a 3D occupancy estimation step. The curr_chunk explains the process of obtaining labels for \"free\" and \"occupied\" voxels in a 3D occupancy estimation step. It involves using depth data to label voxels, tracing source-camera rays, and applying a 3D convolution layer with a sigmoid nonlinearity to produce a probability tensor for voxel occupancy. The network is trained with logistic loss and balanced across classes. The process involves balancing the loss across classes within each minibatch. A 2D CNN is used to convert the target view into an embedding image using a residual network with two residual blocks. The network has specific channel dimensions and a final convolution layer with an embedding dimension of 32. Contrastive loss is applied for both 2D and 3D, where positive and negative correspondences are sampled for supervision. On each iteration, negative samples are obtained using distance-weighted sampling. An L2 loss is applied to penalize distance at all positive correspondences. The model is implemented in Python/Tensorflow with custom CUDA kernels for efficient training. CUDA operations in this study use less memory than native-tensorflow equivalents, enabling training with large imagination tensors. Training to convergence takes 48 hours on a single GPU with specific learning rates and optimizer settings. The code, data, and pre-trained models will be publicly available. Additional experiments involve testing the method on scenes from the CARLA simulator, which allows for moving the camera to any desired viewpoint, essential for the view-based learning strategy. The Shepard-Metzler dataset includes colored cubes in random arrangements, while the Rooms-Ring-Camera dataset features various shapes in rooms with different colors and textures. The ShapeNet arrangements dataset consists of table arrangements of ShapeNet synthetic models. The CARLA simulation environments provide photorealistic rendering with diverse weather conditions, shadows, and objects, making them closer to real-world conditions. In the CARLA dataset, occlusion statistics show that a single camera view reveals only 0.23 of all voxels, leaving 0.77 totally occluded. Adding multiple views increases the revealed volume, with all 6 views revealing 0.42 of the scene. The CARLA dataset shows that only a small portion of the scene is revealed in a single camera view, with the majority needing to be \"imagined\" by the model for view prediction. Different methods for view prediction are compared, highlighting the challenges of achieving photorealistic results. The model uses view prediction to learn visual representations for 3D object tasks, not as the end goal. Performance is evaluated in estimating visual correspondences in 2D and 3D through a nearest-neighbor retrieval task. In 2D, patches are ranked based on L2 distance from a query, while in 3D, feature cubes are used for retrieval from different viewpoints and scenes. The proposed model generates 1000 samples from 100 random test examples, with 10 samples from each. Results are compared against RGB prediction baseline, Generative Query Networks (GQN), and a VAE alternative. The model shows better performance with view contrastive losses, especially in 3D correspondence. The RGB-based models show high accuracy. Training 3D bottlenecked RNNs as a variational autoencoder improves precision at lower ranks thresholds. Contrastive learning outperforms baselines, with a large boost from adding 3D contrastive loss. 2D-bottlenecked architectures cannot perform 3D patch retrieval. The method proposes 3D object segmentations converted into boxes for evaluation. Precision-recall curves are computed with an IOU threshold of 0.5. Sample visualizations of 3D box proposals are shown. The model's ability to estimate occupied and free space is tested by outputting an occupancy probability for each voxel in the scene. Classification accuracy is evaluated independently for free and occupied voxels, with observed and estimated heightmaps computed from 3D occupancy grids. The estimated heightmaps are dense and viewpoint-invariant, showing high accuracy overall. The model's accuracy in estimating occupied and free space is high, with dense and viewpoint-invariant heightmaps. The occupancy module fills in the \"holes\" of partial views, imagining the complete 3D scene. Egomotion error comparison shows that our model is on par with baselines. Table 5: Egomotion error comparison between our model, ORB-SLAM2, and SfM-Net. Results show that our egomotion method performs similarly to the baselines, with ORB-SLAM2 performing best at larger image sizes. The egomotion module assumes fixed camera motion with 2 degrees of freedom, while the task involves a free camera with 6 degrees of freedom. The debate in Computer Vision revolves around the value of 3D models as output for visual recognition, with different perspectives on the importance of replicating the 3D world for decision-making."
}