{
    "title": "r1Ddp1-Rb",
    "content": "Large deep neural networks have enabled breakthroughs in fields such as computer vision, speech recognition, and reinforcement learning. In this work, mixup is proposed as a simple learning principle to address issues like memorization and sensitivity to adversarial examples in neural networks. By training on convex combinations of examples and labels, mixup improves generalization, reduces memorization of corrupt labels, increases robustness to adversarial examples, and stabilizes training of generative adversarial networks. Large deep neural networks have been instrumental in advancements in computer vision, speech recognition, and reinforcement learning. These networks are typically trained to minimize errors using the Empirical Risk Minimization (ERM) principle. However, a contradiction arises as the size of these networks scales with the number of training examples, challenging the effectiveness of ERM in training current neural networks. The contradiction challenges the suitability of ERM to train current neural network models. ERM allows large neural networks to memorize training data instead of generalizing, leading to issues with adversarial examples. An alternative method is data augmentation through Vicinal Risk Minimization (VRM) principle. The Vicinal Risk Minimization (VRM) principle involves using human knowledge to describe a vicinity around each training example, allowing for the generation of additional virtual examples to expand the training distribution. Data augmentation, such as mixup, is introduced as a data-agnostic approach to address issues with dataset-dependent procedures and assumptions about class similarities in the vicinity. Mixup is a simple data augmentation technique that constructs virtual training examples by linearly interpolating raw input vectors and one-hot label encodings. It extends the training distribution by incorporating the knowledge that linear interpolations of feature vectors should lead to linear interpolations of targets. Despite its simplicity, mixup achieves state-of-the-art performance in image classification datasets like CIFAR-10, CIFAR-100, and ImageNet-2012. It also improves the robustness of neural networks when learning from corrupt labels or facing adversarial examples. Mixup improves generalization on speech and tabular data, stabilizes GAN training, and outperforms related methods in previous work. Ablation study experiments show that design choices contribute to its performance. Source code for replicating CIFAR-10 experiments is available. The text discusses minimizing the differences between predictions and actual targets using training data to approximate the data distribution. This process is known as Empirical Risk Minimization (ERM) principle. The Empirical Risk Minimization (ERM) principle involves monitoring the behavior of a function at a finite set of examples. Memorizing training data can lead to undesirable behavior outside the training data. Vicinal Risk Minimization (VRM) approximates the true distribution by using a vicinity distribution. The paper introduces mixup, a generic vicinal distribution for Vicinal Risk Minimization (VRM). Mixup involves sampling virtual feature-target vectors by interpolating between pairs of feature-target vectors from the training data. The mixup hyper-parameter \u03b1 controls the interpolation strength, with ERM principle recovered as \u03b1 approaches 0. Implementation of mixup training in PyTorch is straightforward with minimal computation overhead. Mixup training in PyTorch involves using a single data loader to obtain one minibatch, applying mixup after random shuffling. Interpolating between inputs with equal labels did not lead to performance gains. The mixup vicinal distribution encourages linear behavior in training examples, reducing the amount of computation needed. Mixup training encourages linear behavior in training examples, reducing oscillations and providing a smoother estimate of uncertainty. Models trained with mixup show more stability in predictions and gradient norms between training samples. This approach is evaluated on the CIFAR-10 and ImageNet-2012 datasets. This dataset contains 1.3 million training images and 50,000 validation images across 1,000 classes. Standard data augmentation practices are followed during training, including scale distortions, random crops, and horizontal flips. Mixup and ERM are used to train ImageNet-2012 classification models, with a minibatch size of 1,024 and a specific learning rate schedule. A range of \u03b1 values for mixup leads to improved performance over ERM. Mixup with \u03b1 values in the range of [0.1, 0.4] improves performance over ERM, but can lead to underfitting with larger \u03b1 values. Models with higher capacities and longer training runs benefit the most from mixup, with ResNet-101 and ResNeXt-101 showing a 0.5% to 0.6% improvement over ERM. Additional experiments on CIFAR-10 and CIFAR-100 datasets further evaluate mixup's generalization performance. In experiments using various models like WideResNet-28-10 and DenseNet BID20 with mixup, \u03b1=1, models are trained on Nvidia Tesla P100 GPU for 200 epochs with 128 examples per minibatch. Learning rates are adjusted during training, and weight decay is set to 10. Results show that models trained with mixup outperform those trained with ERM in CIFAR-10 and CIFAR-100 classification tasks. The DenseNet models in BID20 were trained for 300 epochs with learning rate decays at 150 and 225 epochs. Spectrograms were extracted from waveforms at 16 kHz and zero-padded to 160x101. Mixup was applied at the spectrogram level before feeding data to the network. LeNet BID23 and VGG-11 BID30 architectures were compared, each trained for 30 epochs with minibatches of 100 examples using Adam as the optimizer. Mixup was used during training with a warm-up period to improve convergence. Mixup outperformed ERM, especially with VGG-11. The robustness of ERM and mixup models against corrupted labels was evaluated. Increasing mixup interpolation strength was hypothesized to make memorization more difficult. Three CIFAR-10 training sets were generated for evaluation. In this study, three CIFAR-10 training sets were created with varying levels of label corruption. Different methods such as mixup, dropout, and their combination were compared using the PreAct ResNet-18 model. Various parameters were tested for mixup and dropout, with the results summarized in TAB3. The study compared mixup, dropout, and their combination using the PreAct ResNet-18 model on three CIFAR-10 training sets with varying label corruption levels. Mixup with a large \u03b1 outperformed dropout in reducing overfitting and achieving lower training errors on real labels. Mixup + dropout combination showed the best performance, indicating compatibility between the two methods. Models trained using Empirical Risk Minimization (ERM) were found to be fragile to adversarial examples. Adversarial noise is generated to deteriorate model performance, with methods like penalizing the norm of the Jacobian or training on adversarial examples. Mixup improves neural network robustness without adding computational overhead by penalizing the norm of the gradient of the loss. Mixup improves model robustness by reducing loss and gradient norm compared to vanilla ERM. Robustness of mixup models against white and black box attacks is evaluated using ResNet-101 models trained on ImageNet-2012. White box attacks involve generating adversarial examples using FGSM or I-FGSM methods, while black box attacks use adversarial examples produced by the ERM model. The mixup model shows increased robustness compared to the ERM model against adversarial attacks in both white box and black box settings. It outperforms the ERM model by 2.7 times in Top-1 error for FGSM white box attacks and 1.25 times for FGSM black box attacks. Additionally, mixup is about 40% more robust than ERM in black box I-FGSM attacks. Overall, mixup produces neural networks that are significantly more robust without additional overhead. Mixup improves test error on most datasets and never underperforms ERM. Generative Adversarial Networks (GANs) involve a generator and discriminator competing to model a distribution P by transforming noise vectors into fake samples that resemble real samples. Mixup is proposed as a stabilizer for GAN training by acting as a regularizer on the discriminator's gradients, ensuring a stable source of gradient information for the generator. The mixup formulation of GANs is illustrated in Figure 5, showing its stabilizing effect on training GANs. The neural networks in the experiments have three hidden layers of 512 ReLU units and are trained using the Adam optimizer for 20,000 mini-batches of size 128. Mixup GANs show robustness to hyper-parameter and architectural choices, utilizing a data augmentation method involving random convex combinations of inputs and label encodings. Design choices include interpolating latent representations and choosing between nearest neighbors or inputs of the same class. In an ablation study experiment, mixup is compared with interpolating raw inputs and mixing random pairs of inputs using PreAct ResNet-18 on CIFAR-10 dataset. Different weight decay settings are tested, and comparisons are made between interpolating latent representations at different layers. In an ablation study experiment, mixup is compared with mixing random pairs of inputs using PreAct ResNet-18 on CIFAR-10 dataset. The comparison involves mixing nearest neighbors from the same class or all classes during training, as well as mixing within the same class or all classes in a minibatch. Additionally, mixing inputs and labels is compared with mixing inputs only, with variations in target selection and label smoothing techniques. Results from the ablation study experiments show that mixup is the most effective data augmentation method tested, outperforming other methods like mix input + label smoothing. Regularization effects are evident, with different weight decay values impacting the test error for ERM and mixup. The advantage of large weight decay decreases in higher layers of latent representations. Mixing random pairs from all classes is highlighted as a beneficial input interpolation method. Data augmentation methods like mixing random pairs from all classes (AC + RP) show strong regularization effects, while label smoothing and adding Gaussian noise have a smaller impact. The SMOTE algorithm BID4 does not significantly improve performance. Data augmentation is crucial for successful deep learning applications, utilizing domain knowledge to enhance generalization. Techniques like rotation, translation, cropping, resizing, flipping, and noise injection are commonly used to enforce invariances in models for image classification and speech recognition. Recent approaches have proposed methods to improve model robustness and accuracy, such as augmenting rare classes in imbalanced datasets and regularizing output distributions through label smoothing or penalizing high-confidence softmax distributions. These techniques, similar to mixup, rely on multiple smooth labels for supervision rather than single hard labels. However, they do not consider changes in corresponding labels and operate at the input/feature level. Mixup is a data augmentation technique that establishes a linear relationship between data augmentation and the supervision signal, improving generalization. It does not require significant domain knowledge and ensures that every example's supervision is not overly dominated by the ground-truth label. This approach leads to a strong regularizer that enhances generalization, with similarities to other methods like Sobolev training of neural networks. Mixup is a data augmentation principle that involves training on virtual examples created by linearly interpolating two random examples and their labels. It improves generalization error on various datasets and helps combat issues like memorization of corrupt labels and sensitivity to adversarial examples. Increasing the mixup parameter \u03b1 controls model complexity. Our hypothesis is that mixup implicitly controls model complexity, but we lack a theory for the 'sweet spot' of the bias-variance trade-off. In CIFAR-10, low training error is achieved even with high \u03b1 values, while in ImageNet, training error increases significantly with high \u03b1 values. Increasing model capacity may make training error less sensitive to large \u03b1 values, giving mixup an advantage. Mixup also opens up possibilities for exploring its application in regression and structured prediction problems. Mixup can potentially be extended to regression and structured prediction problems, offering new possibilities in unsupervised, semi-supervised, and reinforcement learning. The interpolation principle could serve as an inductive bias for robust model behavior beyond supervised learning. While speculative, these directions show promise for future development."
}