{
    "title": "rJe4EcSohE",
    "content": "The cost of annotating training data has been a bottleneck for supervised learning. An active multitask learning algorithm is proposed to transfer knowledge between tasks, forming committees for joint decision-making and data sharing. This approach reduces queries during training while maintaining high accuracy on test data, showing significant improvements on benchmark datasets. In supervised learning, the bottleneck is annotating data to obtain labeled training examples, especially in personalized systems like music recommenders and spam filters. Multitask learning can help by transferring knowledge between tasks, reducing the need for a large amount of labeled data. In an online multitask learning setting, the learner receives sequential data with task identifiers, predicts labels, and updates models. The assumption of readily available true labels for querying is impractical, leading to inefficiency in costly annotation. Active learning reduces annotator work by selectively requesting labels based on learner uncertainty. In an online multitask learning setting, the learner combines active learning with online multitask learning using peers or related tasks to adopt a measure of uncertainty. The learner queries similar tasks before requesting a true label from the oracle when the classifier is not confident, incurring a lower cost. The proposed active multitask learning framework considers both the current task and its peers' predictions simultaneously using a weighted sum, with a committee making joint decisions for each task. Training samples with obtained true labels are shared directly with similar tasks. In an online multitask learning setting, training samples with true labels are shared directly with similar tasks to improve efficiency. The problem setup is similar to BID11 BID10, with K tasks and N k training samples per task. Each task is treated as a linear binary classification problem, with extensions to multiclass or non-linear cases possible. The perceptron-based update rule is used, updating the model only when predictions are in error. The data for task k includes instances {x (i) , yk} where x is the feature dimension and y is the label. The online setting involves training examples coming at round t, with weights learned for the K tasks. The weight matrix w is learned for K binary classifiers at round t. The hinge loss of task k on the sample at round t is calculated. Losses of peer tasks are also considered to learn similarities among tasks and committee weights. The goal is to achieve high accuracy on test data with minimal queries to the oracle. The algorithm Active Multitask Learning with Committees (AMLC) aims to efficiently share knowledge among tasks and minimize queries to the oracle during training. It utilizes a relationship matrix to represent the closeness between tasks and update committee weights. Comparisons with state-of-the-art online multitask learning algorithms are made in Section 3.3. The algorithm Active Multitask Learning with Committees (AMLC) utilizes a relationship matrix to represent task closeness and committee weight updates. Task confidence is jointly decided by the committee, with prediction based on a common confidence measure. A Bernoulli distribution is used to decide whether to query true labels, with confidence controlled by hyperparameter b. The hyperparameter b controls the confidence level for not requesting true labels. A binary variable M (t) is set to 1 if a task makes a mistake, updating its weight vector using the perceptron scheme. The relationship matrix is updated following a similar policy as in BID11 BID10 Table 1. Performance is evaluated on test set accuracy and total number of queries during training. The best performance is highlighted, with AMLC showing lower mean and variance on Spam Detection. Hyperparameter C determines weight decrease given non-zero loss, with \u03bb = K m=1 l (t) km. The learner updates weights based on non-zero loss and normalizes them. Data is shared with similar tasks to encourage information transfer. The method differs from active learning from peers by jointly deciding task confidence using all tasks' confidences weighted by the committee weight vector. Our approach updates the committee weight vector based on task confidence, ensuring equal influence among tasks for final predictions. This prevents blind confidence and promotes information transfer among tasks. Our algorithm allows sharing of training data across similar tasks directly, after acquiring the true label. While PEER indirectly accesses others' data through querying peer tasks, this sharing mechanism can be insufficient when tasks are highly similar. Our algorithm converges to a relationship matrix with identical elements when all tasks are identical, eventually training on every queried example. Our proposed algorithm is evaluated on three benchmark datasets for multitask learning, compared with baseline models. Parameters are tuned via cross-validation. Landmine Detection 1 has 19 binary classification tasks with 9 features each. Spam Detection 2 includes labeled spam/non-spam data from 15 users. Sentiment Analysis 3 involves product reviews from 22 domains. Training and test set details are in the Appendix. Our proposed algorithm, AMLC, is compared with baseline models on three benchmark datasets for multitask learning. The performance of 5 different models, including Random, Independent, PEER, and PEER+Share, is evaluated based on accuracy on the test set and the total number of queries to oracles during training. The 95% confidence level is also shown for each model. Our re-implementation of PEER achieves similar performance on the Landmine and Spam datasets but performs worse on Sentiment due to using a different representation of training examples. The highlighted values show the best performance across all models, with PEER+Share and AMLC outperforming others. Simply adding data sharing can improve accuracy and reduce the number of queries used during training. Adding data sharing can improve accuracy and query efficiency during training. In active multitask learning with joint decisions in AMLC, there is a significant decrease in the number of queries while maintaining high accuracy. A fixed query budget is given to each model, and AMLC outperforms all models on three datasets (Landmine, Spam, Sentiment) with limited query budgets. The proposed active multitask learning algorithm, AMLC, outperforms all models on three datasets by encouraging knowledge transfer among tasks. It achieves high accuracy with limited queries, unlike PEER+Share which requests true labels frequently. The algorithm uses joint decision/prediction and data sharing among similar tasks to improve accuracy and reduce query numbers. Future work includes theoretical analysis and comparison with baseline models. The proposed active multitask learning algorithm, AMLC, outperforms baseline models by encouraging knowledge transfer among tasks and reducing query numbers. Future work includes analyzing error bounds and handling unbalanced task data."
}