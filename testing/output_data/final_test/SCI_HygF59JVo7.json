{
    "title": "HygF59JVo7",
    "content": "Classification systems typically work in isolation, requiring memorization of all candidate classes for classification. A proposed model uses reference images for verification during classification, reducing memory usage and improving efficiency. The model utilizes non-differentiable queries for image classification, demonstrating feasibility and improved parameter efficiency. Balancing image recognition and verification is crucial for optimal model behavior, suggesting a recognition-verification pipeline for designing more powerful networks with simpler architectures. Our hybrid model, RVNN, utilizes a verification network to compare input images with reference images from each class for classification. It employs a recognition-verification pipeline and episodic-training regime to enhance one-shot learning scenarios. Matching networks extend this approach by predicting based on weighted similarity scores across classes. Prototypical Networks and Retrieval-Augmented Convolutional Neural Networks (RaCNN) are few-shot learning approaches that use verification with support images for classification decisions. While Prototypical Networks use Euclidean Distance for verification, RaCNN combines CNN recognition with a retrieval engine for adversarial robustness. Our hybrid model, RVNN, combines verification and recognition for classification, similar to RaCNN. Our model, RVNN, combines verification and recognition for classification, introducing a non-differentiable component not present in previous work. The recurrent querying model f rnn was implemented using a Gated Recurrent Unit (GRU) and additional information can be passed into it. The recurrent querying model f rnn implemented in RVNN incorporates additional information by sampling a class based on the softmax of the logits. Two approaches were explored during training: the Gumbel-Softmax trick with a temperature parameter and the Straight-Through estimator. The model uses the Straight-Through estimator BID0 for discrete query choices, with the Gumbel-Max trick in the forward pass. The derivative of the query with respect to softmax probabilities is set to maintain gradient flow. Performance is evaluated based on reduced parameter usage and sample efficiency compared to a CNN baseline model. Various architectural considerations and hyper-parameters unique to the model are also experimented with. The model's architectural considerations include Query Memory (QM) where the RNN adapts its policy for better results. Performance metrics show that the model is more parameter efficient with varying RNN sizes. The model's learned query policy outperforms random but falls short of optimal performance. The model's learned query policy improves with a higher RNN size, suggesting it can track previously unsuccessful queries. Informed queries outperformed random and no query models. A recognition operation followed by verification is recommended over simultaneous execution. The model may not be well-suited for one-shot learning tasks without a well-trained recognition module."
}