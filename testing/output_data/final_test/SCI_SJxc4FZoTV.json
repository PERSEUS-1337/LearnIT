{
    "title": "SJxc4FZoTV",
    "content": "Recent developments in autonomous vehicle technology show progress, but testing remains a challenge. Real-world testing is risky and requires billions of miles to validate performance claims. A simulation framework is implemented to test modern autonomous driving systems, including deep-learning algorithms. Adaptive sampling methods are used to estimate accident probabilities. The importance of testing AV perception and control systems is highlighted by fatal accidents involving autonomous vehicles. Testing autonomous vehicles in real environments is time-consuming due to the rare occurrence of serious accidents. A recent study suggests that AVs need to drive hundreds of millions to billions of miles to demonstrate their safety. Formal verification of AV algorithms is challenging as driving policies are prone to crashes caused by other drivers, making it difficult to assign fault accurately. The risk-based framework BID14 aims to evaluate the probability of accidents in autonomous vehicles by assigning probabilities to environmental states and agent behaviors. It considers the performance of AV policies under a data-driven model of the world, regardless of the complexity of the ego-policy. The goal is to assess the probability of dangerous events based on a threshold value. Our risk-based framework treats the ego-policy as a black-box module, allowing for deep-learning perception systems. Estimating rare event probabilities (p \u03b3) is a challenge, with the naive Monte Carlo method being inefficient. Adaptive sampling techniques are used to address these challenges and make the evaluation of p \u03b3 tractable. The multilevel splitting method BID7 BID3 BID5 BID24 decomposes the rare-event probability p \u03b3 into conditional probabilities with interim threshold levels, making it easier to estimate. Markov chain Monte Carlo (MCMC) is used to estimate each term accurately, guiding samples to the rare set through a series of supersets. The method balances the trade-off between easy estimation and large conditional probabilities. The AMS approach complements other procedures like adaptive importance sampling methods, offering convergence guarantees for bias, variance, and runtime. Unlike model-based AIS methods, AMS does not require computation of likelihood ratios or postulate models for optimal importance sampling distribution. The \"modes\" of failure discovered by AMS are limited by the number of samples and mixing properties of the MCMC sampler used. To implement a risk-based framework, a base distribution of nominal traffic behavior is learned using videos of highway traffic. Policies of human drivers are trained through imitation learning techniques, with reinforcement learning methods like generative adversarial imitation learning improving generalization performance. The model-based variant of GAIL (MGAIL) allows for end-to-end differentiation and has been validated to mimic human-like driving behavior across various metrics. The scenario involves six agents, with five considered part of the environment, following policies learned via GAIL. All vehicles aim to reach the end of a 2 km stretch of road from set initial configurations. The study involved a photorealistic simulator of a portion of I-80 in Emeryville, CA where traffic data was collected. The AMS algorithm outperformed naive Monte Carlo for events with low probability, reducing the variance of failure probability by up to 56 times. AMS can be combined with importance sampling for more efficient sampling. Our risk-based framework offers a more efficient approach compared to real-world testing. Verified subsystems are limited by computational complexity and require complete specifications. Adaptive sampling techniques efficiently identify dangerous scenarios, independent of blame assignments. Building and validating the model of the environment represent open research questions for AV safety evaluation. The framework for evaluating AV safety requires benchmarks based on adaptive adversarial conditions. It only needs black-box access to the driving policy and simulation environment, offering speedups over real-world testing. The simulator is a distributed, modular framework supporting new AV systems and policy updates. The architecture allows for parallel simulation rollouts, distributing components on GPU clusters while enabling local analysis on desktops. Implementation using ZeroMQ is fully-distributed among CPUs and GPUs, achieving rollouts up to 30P times faster than real time. In our implementation, the safety measure is minimum time-to-collision (TTC) between the ego-vehicle and other vehicles. TTC is calculated based on the distance and relative velocity between the vehicles. Vehicles are represented as oriented rectangles in a 2D plane. To calculate the time-to-collision (TTC) between the ego-vehicle and other vehicles, a finite set of range and range-rate measurements are used. Angles are computed with respect to the ego vehicle's orientation, rays are cast outward, and distances are measured to other vehicles. The minimum TTC for a simulation rollout is approximated using these measurements. Normalizing flows are utilized to describe distributions using neural networks, which are more expressive than traditional parameterizations. A base distribution is chosen, followed by the use of multi-layer neural networks to model the distribution. This method allows for better approximation of the true time-to-collision (TTC) by adjusting parameters and discretizing time steps. Additionally, crashes are defined as the intersection of vehicle boundaries, leading to non-zero TTC values even during collisions. Normalizing flows use neural networks to model distributions by applying invertible transformations to samples from a chosen base distribution. These transformations, parameterized by trainable weights, allow for learning a density by maximizing log-probability of observed data in the transformed distribution. Rezende and Mohamed introduced an efficient method where the Jacobian is upper triangular, making determinant computation easier. The architecture for fitting distributions involves maximizing log-probability of observed data in the transformed distribution p(y), with further enhancements discussed in BID9 BID15."
}