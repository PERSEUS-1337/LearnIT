{
    "title": "rJeQE8LYdV",
    "content": "The dataset introduced for sequential procedural text generation from images in the cooking domain consists of 16,441 recipes with 160,479 associated photos. Two models, SSiD and SSiL, incorporating high-level structure learned by a Finite State Machine, show improvement in empirical and human evaluation. The best performing model, SSiL, achieves a METEOR score of 0.31, an improvement of 0.6 over the baseline model. Human evaluation of the generated recipes revealed that 61% found them satisfactory. In human evaluation, 61% preferred the SSiL model over the baseline for overall recipes, and 72.5% favored it for coherence and structure. The output analysis also addresses key NLP issues for future directions. The context of real-world interactions in vision and language is crucial for interpretation. The task involves generating textual descriptions for a sequence of images, creating a storyboard that narrates procedural steps. This differs from image captioning and visual storytelling, emphasizing the sequential context. The dataset used is similar to others in the field. The task involves generating sequential textual descriptions for a sequence of images, focusing on procedural steps. This task complements existing work on visual storytelling by adding richer context of goal-oriented procedures. It lays the foundation for storyboarding videos by selecting key clips to describe significant events. The main focus is on generating text from images. The main focus of this work is generating text from images, specifically in the domain of cooking recipes. The approach discussed in the paper focuses on generating coherent recipes by modeling state transitions between cooking stages. The framework introduced incorporates traditional FSMs to enhance structure in text generation, with plans to explore backpropable variants in the future. The paper introduces a dataset of 16k recipes for sequential multimodal procedural text generation and two models (SSiD and SSiL) that incorporate high-level structure learned by an FSM to improve coherence in neural text generation models. It also discusses related work, data collection, model descriptions, results, and future directions. The focus is on generating coherent recipes by modeling state transitions between cooking stages. The paper discusses the importance of domain constraints in improving the predictive ability of seq2seq models, particularly in specialized domains like recipes. It also addresses the challenge of maintaining structure in neural text generation for long sequences, highlighting the need for pre-selecting content and planning accordingly. In contrast to previous approaches, a hierarchical multi-task approach is proposed for structure-aware generation in the food domain. Recent advancements in food datasets and challenges have led to innovations in food recognition and generating cooking instructions. This work focuses on generating procedural text by incorporating ingredients as checklists or treating recipes as flow graphs. Our work focuses on generating procedural text for recipes from images, incorporating ingredients as checklists or treating recipes as flow graphs. We improve understanding and generation by tracking entities dynamically and modeling actions using state transformers. The study focuses on generating procedural text for recipes from images, incorporating ingredients as checklists or treating recipes as flow graphs. Two techniques are proposed to impose structure on procedural text. The term \"foodstagram\" refers to food being one of the most photographed subjects on Instagram. Two how-to blogs were identified, with stepwise instructions for various activities. A dataset of 16,441 samples with 160,479 photos for food, dessert, and recipe topics was used for training, validation, and testing models. The potential for research in selecting the most representative image is highlighted. The distribution of topics is visualized. A baseline model for storyboarding cooking recipes is described, along with two models for incremental improvements. The task is to generate step-wise textual descriptions of the recipe from a sequence of images. The architecture of scaffolding structure is presented in FIG1. The baseline model for generating step-wise textual descriptions of a recipe involves resizing images, extracting features using ResNet-152, passing them through an affinity layer, and processing them with a Bi-LSTM. Dropout and batch normalization are applied for feature representation. The encoder part of the sequence to sequence architecture involves batch normalization with a momentum of 0.01. Global vectors are used for decoding each step, passed through a fully connected layer for a 1024-dimensional representation, followed by a ReLU transformation. A decoder LSTM is used for each step in the recipe, trained with teacher forcing. The model is optimized using Adam with a learning rate of 0.001 and weight decay of 1e-5. The model does not explicitly consider the structure of recipe narration in the generation process. The text discusses two models, SSiD and SSiL, that impose a high-level narrative structure on a cooking recipe by incorporating phases. The SSiD model focuses on incorporating the sequence of phases in the decoder to impose structure during text generation. Two sources of supervision drive the model: a multimodal dataset and unimodal data. The text discusses using clustering and FSM to learn phases in a cooking recipe model. K-Means clustering is used on sentence embeddings to identify categories like desserts, drinks, and main course foods. Phases are defined as content words with high tf-idf values, and are latent states learned through an FSM in the actual model. The text explores using k-means clustering to categorize phases in recipes, representing recipes as sequences of phases. An FSM is used to model hard and soft representations of phase sequences, with an algorithm originally developed for language models. The algorithm utilizes entropy to maximize prediction by finding the best state split in an ergodic state for all phase types. Recipes are represented as state sequences, with a transition to a softer representation of states to smooth irregularities in learned phases. State transition probabilities are obtained from the FSM output, allowing each state to be represented as a soft state transition probability to other states. The SSiD model incorporates state transition probabilities from FSM output to decode phase sequences in recipes. The complexity of phases and states in FSM are considered, with plans to explore hidden markov models in the future. Comprehensive results are presented in Table 2. The study explores using hidden Markov models instead of FSM in the future. The deviation of the structure in the generated output from the original structure is reflected in the loss. Phase sequences are decoded using a clustering model, and state transition probabilities are decoded from FSM for the generated output. Investigating the divergence between the phases of generated and original steps is also considered. This can be seen as hierarchical multi-task learning. The study explores hierarchical multi-task learning in recipe generation. It involves decoding recipe steps using cross entropy and penalizing the model with KL divergence. The KL divergence is measured between distributions of phases in original and generated recipes. The combined loss is used to penalize the model, with the weight of the KL term gradually increasing during training. The clustering model and FSM are used to learn the number of phases and states in an unsupervised manner. The study explores hierarchical multi-task learning in recipe generation, using a clustering model and FSM to learn phases and states unsupervised. Results show that the model with soft phases is more stable than hard phases, with SSiL model generating recipes conditioned on each example. Human evaluation was also conducted through a user preference study. Human Evaluation: A user preference study compared the baseline with the SSiL model for recipe generation. 10 users evaluated 20 randomly sampled recipes based on overall preference and structural coherence. The SSiL model was preferred 61% and 72.5% for overall and structural preferences, respectively. While there is room to improve recipe structure, enhancing overall preference by focusing on generating edible recipes is crucial. This is a simple recipe for making delicious baked chicken wings. Ingredients include 5 pounds of chicken wings, flour, salt, paprika, melted butter, and a baking pan. Preheat oven to 450 F, mix dry ingredients in a bag, spread butter on a baking pan, and bake the chicken wings. Enjoy the evening with this easy dish! To make delicious baked chicken wings, preheat oven to 450 F. Mix dry ingredients in a bag, spread butter on a baking pan, then bake chicken wings for 30 minutes until crispy. Serve and enjoy! The text discusses the importance of maintaining global context in baking, using referring expressions for coherence, and explicitly modeling procedural text structure in recipe generation. The SSiD and SSiL models generate recipe conclusions with specific words like 'serve' and 'enjoy', while the Glocal model mentions setting aside at the end. SSiD model shows a co-occurrence issue with ingredients like 'sugar' and 'salt'. SSiL model misses 'tongs' in the first step. There is a balance between detail and conciseness in recipe generation. A storyboard format implicitly addresses the complexity of steps through pictorial representation and text detailing. This relates to multimodal summarization techniques. In this paper, the focus is on incorporating structure from FSMs into neural models for generating sequential procedural text with multimodal data, specifically cooking recipes presented as graphic novels. A dataset of 16k recipes with text and images is gathered, emphasizing a how-to approach. A baseline model is established based on ViST, highlighting a high-level structure of recipes as phases and representations of states. The study proposes two techniques, SSiD and SSiL, to incorporate structure learned from finite state machines into neural models for generating sequential procedural text. The SSiL model improves upon the baseline, achieving a METEOR score of 0.31. Future plans include exploring backpropable variants as a scaffold for structure and extending the models to other domains. Evaluation strategies for the high-level structure learned in this task are also to be explored."
}