{
    "title": "HkxlcnVFwB",
    "content": "An important problem in reinforcement learning and Monte Carlo methods is estimating quantities from the stationary distribution of a Markov chain with limited access to the transition operator. Consistent estimation is still possible in this scenario using a ratio correction method called GenDICE, which corrects for distribution discrepancies and minimizes variational divergence. The algorithm is effective, consistent under general conditions, and has a detailed error analysis. Estimation of quantities defined by the stationary distribution of a Markov chain is crucial in various scientific and engineering applications. Markov chain Monte Carlo (MCMC) methods are commonly used in machine learning for approximate Bayesian inference. Queueing theory also relies on studying queue lengths and waiting times under the limiting distribution. The GenDICE method corrects for distribution differences and minimizes variational divergence to estimate quantities effectively and consistently. Estimation of stationary distribution quantities in reinforcement learning is essential, but traditional methods require direct access to the environment, which is often not available. In practical scenarios where sampled trajectories are collected offline, there is a need to estimate stationary quantities. Off-policy policy evaluation in RL is one such example where estimating stationary quantities is crucial. In off-policy policy evaluation in RL and off-line PageRank (OPR), estimating values or importance without direct access to the environment is crucial. A new approach using a stationary distribution corrector is proposed for off-line estimation of stationary values from a fixed sample of state transitions. This setting differs from traditional methods that assume continuous sampling from the environment. The off-line setting presents challenges in inferring asymptotic quantities from finite data. Techniques have been developed to allow consistent estimation under general conditions and provide effective estimates in practice. The main contributions include formalizing the problem of off-line estimation of stationary quantities, proposing a novel stationary distribution estimator called GenDICE, and demonstrating its significant improvements in various applications. The paper introduces GenDICE, a new method that improves off-policy evaluation benchmarks and offline PageRank estimation. It extends previous work by introducing a more effective estimation method for estimating stationary quantities from sampled transitions. PageRank algorithm considers a random walk on a directed graph representing the World Wide Web. The algorithm iterates to estimate the rank of web pages based on sampled transitions from the graph. The PageRank algorithm involves a random walk on a directed graph of the World Wide Web to estimate web page ranks. In the context of policy evaluation in RL, a Markov Decision Process is considered with state space, action space, transition dynamics, reward function, discounted factor, and initial state distribution. A trajectory is generated based on a policy that selects actions according to a probability distribution. Behavior-agnostic Off-Policy Evaluation (OPE) involves estimating a policy's value without relying on a known behavior policy. This scenario requires a different approach than standard importance sampling estimators. The behavior-agnostic OPE problem involves estimating the policy's value without a known behavior policy. The stationary distribution can be defined using a correction ratio function \u03c4, reducing the problem to estimating \u03c4. Previous works by Liu et al. and Nachum et al. also utilize this approach but with different assumptions and methods. The GenDICE estimator is a behavior-agnostic OPE estimator applicable for \u03b3 = 1 and \u03b3 \u2208 (0, 1). It is designed for estimating a stationary distribution correction ratio in the MDP setting, with an efficient optimization algorithm. The algorithm for optimization in the MDP setting involves a distribution operator that operates in the reverse temporal direction. Equation 6 always has a fixed-point solution when \u03b3 \u2264 1, with more complex conditions for the continuous case. The development is based on a divergence D. Based on a divergence D, the development in this paper assumes a unique stationary distribution \u00b5 for a given target policy \u03c0. In a behavior-agnostic setting, samples from P are used to form a mixture Tp\u03b3,\u00b50 of \u00b50\u03c0 and Tp. Sampling from this mixture yields a sample (s, a, s', a') \u223c Tp\u03b3,\u00b50. The stationary condition for the ratio from Equation 6 can be re-expressed in terms of Tp\u03b3,\u00b50. Estimating \u03c4* involves solving an optimization problem to match the LHS and RHS of Equation 8 with respect to a divergence D. However, there are issues with this naive formulation that need to be addressed in a principled manner. To avoid degenerate solutions, the solution must be a proper density ratio. To avoid degenerate solutions, the optimization formulation includes the constraint that the solution must be a proper density ratio. Solving this optimization with expectation constraints is complex, so the penalty method offers a simpler alternative by solving regularized problems with increasing \u03bb. However, ensuring strict feasibility often requires \u03bb to approach infinity, which can be impractical, especially in stochastic scenarios. The optimization problem requires a proper density ratio solution, which can be challenging to achieve. Increasing \u03bb in regularized problems may lead to unbounded variance and optimization divergence. However, a special structure in the solution sets shows that increasing \u03bb is unnecessary. The solution to the equation is \u03c4 * (s, a) = u(s,a) p(s,a), and the correction ratio function \u03c4 * can be estimated with just one optimization. By using a dual embedding technique, the difficulties of solving the optimization involving integrals and obtaining unbiased gradients can be bypassed. The optimization problem can be solved using a dual embedding technique, bypassing difficulties in solving integrals and obtaining unbiased gradients. The objective function is convex with a suitable convex function representation. By applying the interchangeability principle, the inner max can be replaced to maximize over a function, simplifying the optimization process. The main optimization formulation simplifies the optimization process by maximizing over a function f. The proposed estimator is compatible with various divergences, such as MMD, Dudley metric, and Wasserstein distance, each with specific requirements on the dual function. The dual function requirements for distance estimation may add complexity in practice. For example, using Wasserstein distance and the Dudley metric may necessitate an extra gradient penalty. MMD offers a closed-form solution for the dual function but requires two independent samples in each outer optimization update. Additionally, MMD relies on RKHS conditions, introducing kernel parameters that may not be as flexible as neural networks. The practical instantiation of GenDICE introduces a consistent stationary distribution correction estimator using a min-max saddle point optimization. The \u03c72-divergence is chosen due to its numerical stability and squared regularization properties, making it suitable for optimization. This choice is motivated by the need for a reliable ratio correction function in estimating expectations, ensuring bounded errors between estimates and ground-truth values. The \u03c72-divergence is utilized for its numerical stability and squared regularization properties in the GenDICE framework. Neural networks are used to parameterize the correction ratio \u03c4 and dual function f. Positive neurons are added to ensure non-negativity of \u03c4, and different positive neurons are empirically compared. An unbiased gradient estimator is obtained for optimization, allowing for scalable stochastic gradient descent. The GenDICE algorithm is analyzed theoretically, showing that the suboptimality of its solution can be decomposed into three terms: approximation error, estimation error due to sample randomness, and optimization error. The GenDICE algorithm's solution suboptimality can be broken down into three terms: approximation error, estimation error from sample randomness, and optimization error. The flexibility of function classes affects these errors, with more flexible classes reducing approximation error but potentially increasing estimation and optimization errors. Conversely, linearly parameterized classes tend to have smaller estimation and optimization errors but may lead to higher approximation error. Off-policy evaluation with importance sampling has been explored in contextual bandits and episodic RL settings, achieving empirical successes. However, IS-based methods suffer from exponential variance in long-horizon problems, known as the \"curse of horizon.\" Various variance reduction techniques have been introduced but still cannot eliminate this issue. By rewriting the accumulated reward as an expectation w.r.t. a stationary distribution, OPE can be recast as estimating a correction ratio function, which significantly reduces variance. GenDICE is a new algorithm that addresses the challenge of undiscounted reward estimation in the behavior-agnostic setting, unlike DualDICE which is designed for discounted problems. Model-based methods are also applicable in this setting but typically rely on learned models for reward estimation. GenDICE is evaluated on OPE and OPR problems, using behavior policies to collect trajectories for estimating average and discounted rewards. Comparison is made with model-based approaches and step-wise weighted IS. Additionally, comparison is made with Liu et al. (2018) in the Taxi domain with a learned behavior policy. GenDICE is evaluated on offline PageRank (OPR) in the Taxi domain with a learned behavior policy. Comparison is made with DualDICE in the discounted reward setting and model-based methods for OPR. Results are validated in tabular and continuous cases with 20 random seeds. The code will be publicly available for reproduction. GenDICE is also tested on Barabasi-Albert (BA) graph, Cora, and Citeseer graphs for offline PageRank. GenDICE outperforms model-based methods in sample efficiency, especially with limited data. Even with 20k samples for a BA graph with 100 nodes, GenDICE shows better performance in estimating the stationary distribution vector. In comparison, model-based methods need to learn an entire transition matrix with many more parameters. The taxi domain used has a grid size of 5 \u00d7 5, yielding 2000 states in total. In the taxi domain with a grid size of 5 \u00d7 5, various policies were tested using tabular Q-learning and behavior cloning methods. Results show that GenDICE consistently outperforms IS, IPS, and model-based methods, especially with longer horizons. DualDICE is noted to only work effectively with \u03b3 < 1, while GenDICE remains stable even with larger values of \u03b3. GenDICE is more stable than DualDICE for larger discount factors (\u03b3 < 1). It performs well in OPE tasks for Cartpole, Reacher, and HalfCheetah. Neural networks with two hidden layers are used for function approximation. Comparison is mainly with DualDICE for discounted rewards. Additional details are in Appendix D. GenDICE demonstrates good performance in challenging continuous control tasks with a continuous action space, maintaining stability even with function approximation and stochastic optimization. Empirical validation shows improved performance with longer trajectory length and closer behavior policies to the target policy. GenDICE shows strong performance in continuous control tasks with stable results. It outperforms DualDICE in discounted reward cases and HalfCheetah domain. An ablation study on GenDICE explores robustness and implementation details. Results on HalfCheetah show GenDICE's performance in continuous control tasks, outperforming DualDICE in discounted reward cases. An ablation study investigates the effects of learning rate and activation function on GenDICE's robustness and implementation details. The activation function on the last layer ensures non-negative outputs for the ratio. Empirical comparison shows that using (\u00b7) 2 achieves low variance and better performance. GenDICE is stable with varying \u03b3 values, outperforming DualDICE in stability and generality. The importance of the ratio constraint is highlighted, investigating the issue without the constraint. In this paper, a novel algorithm GenDICE is proposed for stationary distribution correction estimation, showing superiority over existing methods in off-policy evaluation and offline PageRank. The algorithm addresses the issue of a trivial solution without the constraint penalty, emphasizing the importance of the constraint for accurate correction. The discussion is based on the assumption of a unique stationary distribution under the target policy. The total variation divergence is selected, with the transition operator needing to be ergodic. The optimal solution to the optimization equation is a density ratio, unaffected by an extra penalty. The density ratio leads to zero for both terms, making it the solution to J(\u03c4). The optimal \u03c4(x) is the target density ratio, with \u00b5 being the stationary distribution of T. The proof involves the convexity of the function \u03c6 and its Fenchel dual representation. The algorithm requires inputs such as convex function \u03c6, initial state s0 \u223c \u00b50, target policy \u03c0, and learning rates. The unbiased gradient estimator for \u2207w\u03c4,u,wf J(\u03c4,u,f) is provided, along with the psuedo code for applying SGD. The objective function of GenDICE is reformulated as a saddle-point problem, with a bounded version of the divergence used to avoid numerical infinity. The optimal solution \u03c4* is the stationary density ratio p(x). The algorithm optimizes J(\u03c4,u,f) with inputs like convex function \u03c6, initial state s0 \u223c \u00b50, and target policy \u03c0. The dual u = Ep(\u03c4) \u2212 1, where |u| \u2264 (C + 1). The error between \u03c4 and \u03c4* is considered using standard arguments. In special cases, the suboptimality implies a distance between \u03c4 and \u03c4*. If P\u03c0 = Q\u039bQ\u22121, with Q as eigenfunctions and \u039b as eigenvalues, the largest being 1, then d(\u03c4, \u03c4*) is bounded by a metric between them. The error between \u03c4 and \u03c4* is analyzed using standard arguments. By introducing a dual approximation, the terms in the error decomposition are considered one-by-one. The final bound for the error is given as d(\u03c4, \u03c4*) \u2264 4est + \u02c6opt + 2C. The statistical error in batch RL setting is analyzed, focusing on the error bound between \u03c4 and \u03c4*. The error is bounded by 4est + \u02c6opt + 2C, with the use of Pollard's tail inequality and covering number. The generalization to \u03b2-mixing samples is omitted for simplicity. Pollard's tail inequality bounds the maximum deviation using the covering number of a function class with i.i.d. samples. The covering number is bounded by the function class's pseudo-dimension. The statistical error can be bounded under certain assumptions, including the Lipschitz continuity and finite pseudo-dimension of the function classes. The proof involves verifying conditions and computing the covering number. Based on Assumption 2, we consider bounded \u03c4 \u2208 H and u \u2208 U. We rectify f \u221e \u2264 C and bound h \u221e. The covering number of G is checked, and the distance in G is bounded. The statistical error is bounded under certain assumptions. The optimization error for the general neural network remains an open problem. The convergence rate with tabular, linear, and kernel parametrization for (\u03c4, f) is discussed, focusing on the linear parametrization. The optimization steps with vanilla SGD are bounded by the primal-dual gap. The main theorem states that under certain assumptions, the stationary distribution \u00b5 exists, and the error between the GenDICE estimate and \u03c4 is finite. The error between the GenDICE estimate and \u03c4 is finite, with optimization error and approximation induced by (F, H) for parametrization of (\u03c4, f). The total error is bounded, and results for SGD can be applied. In the Taxi domain, behavior and target policies are followed as in Liu et al. (2018), with a grid size of 5\u00d75 and 2000 states in total. Target policy is set as the final policy \u03c0 * after convergence. In the Taxi domain, the target policy \u03c0 * is set after running Q-learning for 1000 iterations. Another policy \u03c0 + is set after 950 iterations as the base policy. The behavior policy is a mixture policy controlled by \u03b1, where \u03c0 = (1 \u2212 \u03b1)\u03c0 * + \u03b1\u03c0 +. The optimal stationary ratio \u03c4 is solved exactly using matrix operations. Importance sampling requires knowledge of the importance weights \u00b5(a|s)/p(a|s). Offline PageRank graph statistics are illustrated in Table 1, and degree statistics and graph visualization are shown in Figure 7. The BarabasiAlbert (BA) Graph starts with an initial connected network of m 0 nodes, where new nodes are connected to existing nodes with a preference for heavily linked nodes. Real-world citation networks are used to build the graph, with weights randomly drawn from a Gaussian distribution. Offline data is collected by a random walker on the graph for experiments. In experiments, the effectiveness of GenDICE with limited offline samples is validated using Cartpole, Reacher, and HalfCheetah tasks from OpenAI Gym. Importance sampling involves learning a neural network policy through behavior cloning and using its probabilities for computing importance weights. The Cartpole task is modified to be infinite horizon with a reward of -1 for termination and 1 otherwise. The target policy \u03c0* is a weighted combination of a pre-trained policy and a uniformly random policy. The behavior policy \u03c0 is a combination of a pre-trained policy and a random policy. Different optimization methods are used for training, and the target policy is defined as a Gaussian with specific parameters. For the HalfCheetah task, a deterministic policy is trained until convergence via DDPG. The target policy \u03c0 is defined as a Gaussian with mean from the pre-trained policy and standard deviation of 0.1. The behavior policy \u03c0 b is a Gaussian with mean from the pre-trained policy and standard deviation of 0.2 \u2212 0.1\u03b1. Training for stationary distribution correction estimation methods is done using the Adam optimizer with batch size 2048 and optimal learning rate of 0.003. On the discrete control task, the modified Cartpole task with infinite horizon and adjusted reward function showed competitive results with IS and Model-Based in average reward, but outperformed them in log MSE loss. GenDICE excelled in continuous control tasks like HalfCheetah and Reacher, beating other baselines in log MSE and training steps. Figure 10 and 11 demonstrate how GenDICE outperforms other baselines in accurately estimating rewards for the target policy. The figures show performance with off-policy datasets collected by single and multiple behavior policies. Figure 12 presents ablation study results on estimated rewards, highlighting the impact of different learning rates on GenDICE's performance. The performance of GenDICE in accurately estimating rewards for the target policy is demonstrated in Figures 10 and 11. Different activation functions are compared, with the square function showing low variance and better performance in most cases. The results on HalfCheetah show that larger learning rates lead to better performance, and the square function is preferred for its consistency. The estimator is consistent, leading to the stationary distribution correction ratio asymptotically. Different choices of divergences and \u03bb may pose challenges in numerical optimization. Empirical effects of f-divergence, IPM, and weight of constraint regularization \u03bb are investigated. The study focuses on offline PageRank task on BA graph with 100 nodes and 10k samples, evaluating performances with 20 random trials. GenDICE is tested with alternative divergences like Wasserstein-1 distance. The GenDICE estimator is tested with various divergences such as Wasserstein-1 distance, Jensen-Shannon divergence, KL-divergence, Hellinger divergence, and MMD. The addition of a gradient penalty ensures the dual function is 1-Lipchitz. Results show that GenDICE outperforms model-based estimators with different divergences, except for KL-divergence which performs notably worse due to issues with its conjugate function. The GenDICE estimator performs well with various divergences, except for issues with the KL-divergence conjugate function. The penalty weight \u03bb affects the performance, with \u03bb = 1 being a reasonable choice. Markov Chain Monte Carlo requires heavy computational cost for updating particles. GenDICE leverages stationary density ratio estimation for approximating stationary quantities, distinct from classical methods like moment matching and ratio matching. GenDICE uses stationary density ratio estimation for approximating stationary quantities, unlike classical methods. Yao & Schuurmans (2013) developed a reverse-time RL framework for PageRank, which adapts faster with graph changes but still considers an online approach."
}