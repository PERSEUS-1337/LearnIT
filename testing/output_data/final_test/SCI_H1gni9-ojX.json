{
    "title": "H1gni9-ojX",
    "content": "Transferring representations from large-scale supervised tasks to downstream tasks in Machine Learning has shown remarkable results in Computer Vision and NLP. Multilingual NMT systems, once trained, can perform zero-shot translation between multiple languages. This paper explores extending this capability to cross-lingual NLP tasks like sentiment classification and natural language inference. By reusing the encoder from a multilingual NMT system, a multilingual Encoder-Classifier achieves impressive zero-shot cross-lingual classification performance. The study explores the zero-shot cross-lingual classification performance of multilingual NMT systems on various downstream tasks like Amazon Reviews, Stanford sentiment treebank (SST), and Stanford natural language inference (SNLI). The research analyzes the impact of shared vocabulary, training data type, classifier complexity, encoder representation power, and model generalization on zero-shot performance. Results indicate that representations learned from multilingual NMT systems are widely applicable across languages and tasks, with high classification performance linked to generalization capability. The study details the model and training specifics of the base multilingual NMT model and task-specific classifiers. The study focuses on zero-shot cross-lingual classification using multilingual NMT systems for various tasks. Details are provided on task-specific classifiers, model training specifics, and hyper-parameter tuning. The multilingual NMT model includes a shared encoder and separate decoders for English and French. The study explores zero-shot cross-lingual classification using a multilingual NMT system with separate encoders for English and French. The model utilizes a max-pooling operator for pooling activation over time and extends the Encoder-Classifier model to a multi-source model for the SNLI task. The encoder outputs are processed using separate network blocks for each source. The study analyzes the effectiveness of a multilingual Encoder-Classifier system for zero-shot classification. Experiments are conducted to understand the impact of shared sub-word vocabulary and the use of a language agnostic representation. The best zero-shot performance is achieved by using a complex classifier and up to layer 3 of the encoder. The study examines the impact of training phases on zero-shot classification using a multilingual Encoder-Classifier system. Overfitting on the English task can hinder generalization to new languages, prompting the use of averaged parameter estimates for improved generalization and reduced overfitting effects during inference. The study explores the impact of training phases on zero-shot classification using a multilingual Encoder-Classifier system. By combining bilingual lexical data to learn contextualized representations, the system demonstrates enhanced generalization for new tasks. Freezing the encoder after initialization significantly improves performance, especially for document-level classification tasks with long input sequences. Freezing the encoder after initialization improves performance for document-level classification tasks with long input sequences. This effect is supported by results from SNLI and SST, showing no significant difference between freezing and not freezing the encoder for sentence-level input sequences."
}