{
    "title": "SJlRWC4FDB",
    "content": "Many machine learning models, including industrial copyright detection tools, are vulnerable to adversarial attacks where small changes to inputs can evade classifiers. Neural network based copyright detection systems are particularly susceptible to these attacks. A music identification method implemented as a neural net was successfully fooled using simple gradient methods, highlighting the importance of strengthening copyright detection systems against adversarial examples. Adversarial attacks can manipulate machine learning systems, including copyright detection systems. These attacks involve small changes to input data that result in significant alterations to the model's output. Despite the critical importance of securing copyright detection systems, they have not been extensively studied by the ML security community. Copyright detection systems extract features from audio or video samples to match content. Copyright detection systems use fingerprints from video or audio to match with a library of known fingerprints. Tools like YouTube's Content ID and Google Jigsaw help flag copyrighted material and remove videos promoting terrorism. The EU Copyright Directive mandates copyright filters for platforms allowing user-generated content. Various proprietary copyright detection systems exist, with a regulatory push for their implementation. The paper discusses the vulnerability of copyright detectors to adversarial attacks, demonstrating an attack on real-world music copyright detection systems using a modified \"Shazam\" algorithm implemented as a neural network in TensorFlow. The attack creates adversarial music that evades machine detection while remaining recognizable to humans, successfully fooling industrial systems. Our adversarial music successfully fools industrial systems like AudioTag music recognition service and YouTube's Content ID system. Adversarial examples have been studied for non-vision applications such as speech recognition. Attacks on copyright detection systems have increased potential for vulnerability due to the direct upload of digital media to servers. Adversarial attacks on copyright detection systems can bypass physical-world challenges by directly uploading manipulated audio files to servers, avoiding the need for perturbations to survive various measurement processes. This poses a significant threat as copyright detection is an open-set problem, making it vulnerable to attacks that do not correspond to any known class. Copyright detection algorithms must address the open-set problem, where most uploaded content is not copyright protected and should be labeled accordingly. These systems need to be tuned conservatively to avoid flagging non-protected material and must operate in an environment where the majority of content is not flagged. Copyright detection systems face challenges in distinguishing between protected and non-protected content, similar to the vulnerability of ImageNet classifiers to untargeted attacks due to feature overlap between classes. Fingerprinting algorithms extract feature vectors from source content to match with a library of known vectors associated with copyrighted material. Adversarial methods can exploit neural network-based feature extractors, making systems susceptible to attacks. Video fingerprinting involves using object detectors to identify objects in video frames and creating a hash based on the features and temporal relationships between these objects. However, recent research has shown that object detectors can be manipulated to adversarially alter video frames, making fingerprinting vulnerable to attacks. Current methods focus on training networks to handle common distortions but do not consider adversarial perturbations, making them susceptible to white-box or black-box attacks. Plagiarism detection systems also rely on neural networks but may not be robust against adversarial attacks. Recent plagiarism detection systems like Yasaswi et al. (2017) use neural networks to create document fingerprints, which can be vulnerable to adversarial attacks. Audio fingerprinting, on the other hand, relies on hand-crafted features, but even these can be susceptible to attacks. An acoustic fingerprint is a feature vector used to quickly locate or find similar samples in an audio database. Audio fingerprinting is crucial for detection algorithms like Content ID. A generic model, such as Shazam's, is used for identifying music with properties like temporal localization, translation invariance, and robustness. Audio fingerprinting is essential for detection algorithms like Content ID, with models like Shazam's used for identifying music based on properties like temporal localization and robustness. Spectrogram peaks are utilized for generating hashes in a reproducible manner, with a shallow neural network incorporating additional layers for creating transferable adversarial examples. The generic neural network model used for generating audio fingerprints involves convolutional neural networks to extract features that uniquely identify signals, independent of start or end times. The network layers apply transformations to the input, with the first layer convolving with a normalized Hann function to smooth adversarially perturbed audio waveforms. The neural network model uses a normalized Hann window to smooth adversarially perturbed audio waveforms before fingerprinting. The next layer computes the spectrogram of the waveform, converting it to the frequency domain using an ensemble of Fourier kernels. After applying convolutional layers to compute the STFT magnitude of the audio signal, a feature representation \u03c6(x) is obtained. This representation is dense and susceptible to noise, making it difficult to store and search. Wang et al. (2003) propose using local maxima of the spectrogram as features to address these issues. By applying max pooling over \u03c6(x), we can identify local maxima locations, creating a binary map that serves as the signal's fingerprint. The binary fingerprint \u03c8(x) is generated by identifying local maxima locations in the signal using a 2-layer convolutional network. An adversarial perturbation \u03b4 is crafted by finding a surrogate loss that measures the similarity between extracted fingerprints. The perturbation \u03b4 is bounded to ensure similarity between perturbed and clean audio signals. In experiments, the perturbation budget \u03b4 p is enforced to bound the perturbation size. The Hamming distance is used as a similarity measure between binary fingerprints. Attacks using a differentiable loss function are effective in the white box case but not transfer well to black-box systems due to brittleness. To enhance the transferability of adversarial examples to black-box industrial systems, a robust loss function is proposed. This loss function encourages significant shifts in local maxima of the spectrogram by adjusting their positions outside the neighborhood of other maxima. Two max pooling layers with different widths are used to implement this constraint efficiently. A specific loss function penalizes spectrogram peaks that are close to each other, ensuring the outputs of the max pooling layers are distinct. The loss function enforces differences in local maxima of the spectrogram by a margin c. It is made differentiable using a smoothed max function with a smoothing hyperparameter \u03b1. The optimization problem is solved using projected gradient descent with Adam updates and perturbation clipping to satisfy constraints. The optimization problem aims to create an adversarial example with a fingerprint different from the original signal's fingerprint. To achieve a more natural sounding perturbation, the perturbed signal's fingerprint is enforced to be similar to a different audio signal. This is done by introducing a loss function that makes two signals look similar, utilizing the linear superposability characteristic of spectrogram peaks. The optimization problem introduces a loss function to create adversarial examples with fingerprints different from the original signal. The perturbed signal's fingerprint is enforced to be similar to another audio signal, resulting in more natural sounding perturbations. These attacks are tested on real-world audio search/copyright detection systems. Black-box attacks were conducted on two proprietary audio search/copyright detection systems, using transfer attacks due to the systems' inner workings being undisclosed. The systems claim to be robust against noise and distortions. Adversarial examples were crafted for top billboard songs, with default examples removing identifiable frequencies and remix examples introducing new frequencies to confuse the systems. The effectiveness of white-box attacks was also evaluated before testing black-box transfer attacks on real-world systems. To evaluate the effectiveness of white-box attacks, a model was tested using a defined loss function to remove fingerprints with imperceptible perturbations. Norms of perturbations needed to eliminate fingerprint hashes were shown in Table 1. AudioTag 3, a music recognition service, claims robustness against distortions and noise. The AudioTag music recognition system claims to be robust against distortions and noise, but it can be easily fooled by small perturbations. The system fails to detect adversarial examples and is sensitive to proposed attacks. The perturbations required to fool AudioTag are small and not easily noticeable by humans. This suggests that the fingerprinting model used in AudioTag may have similarities to the surrogate model discussed earlier. Table 2 displays the norms of perturbations needed to deceive AudioTag on 90% of songs. Random noise must be 4 times larger than crafted noise to fool AudioTag. YouTube Content ID can identify copyrighted material in user-uploaded videos. Default and remix attacks evade Content ID undetected. Our attacks successfully evade Content ID on YouTube, but AudioTag is easier to fool. Larger perturbations are needed to deceive Content ID, making them noticeable to humans. Experiments with different hyper-parameters were conducted on songs from the dataset, showing the norms of perturbations required to fool YouTube. The recall of YouTube's copyright detection tool on the dataset for different perturbation magnitudes is also shown. Copyright detection systems are important in machine learning, but their robustness to adversarial attacks is crucial. The text discusses the vulnerability of copyright detection systems to adversarial attacks, demonstrating this with a simple song identification method using neural networks. The authors acknowledge their lack of expertise in audio processing and highlight the potential for stronger attacks using less perceptible perturbation types. Transfer attacks using basic surrogate models are shown to be effective, even on online systems. The text highlights the importance of strengthening copyright detection systems against adversarial attacks, emphasizing the use of adversarial training and advanced defenses. Commercial systems likely use trainable neural nets, while the current method relies on surrogate models with hand-crafted features. The goal is to raise awareness of threats in this space, not to promote copyright evasion."
}