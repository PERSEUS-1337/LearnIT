{
    "title": "SyqShMZRb",
    "content": "Deep generative models have been successful in modeling continuous data, but capturing representations for discrete structures like computer programs and molecular structures remains a challenge. A novel syntax-directed variational autoencoder (SD-VAE) is proposed, inspired by the theory of compiler, to generate syntactically and semantically correct data by introducing stochastic lazy attributes. This approach enforces constraints on the output space to ensure both syntactic validity and semantic reasonableness. The model is evaluated in programming language and molecule applications for reconstruction. The text discusses the challenges of generating discrete structured data like graphs, molecules, and computer programs using generative models. It highlights the importance of incorporating syntactic and semantic constraints to improve model effectiveness. The text discusses the challenges of generating structured data using generative models, focusing on sequences and the seq2seq framework. It emphasizes the importance of incorporating constraints to improve model effectiveness. The text discusses challenges in generating structured data with generative models, focusing on incorporating constraints for effectiveness. BID11 (CVAE) and SD-VAE models are highlighted for molecule generation using SMILES notation. SD-VAE reshapes the output space more effectively than existing models by utilizing attribute grammar. The importance of formalizing syntax and semantics to avoid invalid outputs is emphasized. To address the challenge of generating valid combinations of structures efficiently, incorporating structure restrictions into generative models is essential. Context-free grammars (CFG) are used in decoder parametrization to balance computational cost and model generality. For example, the grammar variational autoencoder (GVAE) in molecule generation tasks incorporates the CFG of SMILES notation into the decoder to generate parse trees in a top-down manner. However, while CFG helps generate syntactically valid objects, it does not fully regulate the model for generating semantically valid objects. The challenge in generating valid structures efficiently is addressed by incorporating structure restrictions into generative models. While context-free grammars (CFG) help generate syntactically valid objects, they do not fully regulate the model for generating semantically valid objects. Additional constraints beyond CFG are needed to ensure semantic validity in tasks such as molecule generation and program generation. One approach is to use attribute grammars to attach semantics to parse trees, allowing for offline semantic checking after generating syntactically valid candidates. The proposed syntax-direct variational autoencoder (SD-VAE) addresses the challenge of generating semantically valid structures efficiently by incorporating syntax and semantics enforcement. This new formalization converts offline semantic checks into online guidance, ensuring both syntactic and semantic validation in the tree generation process. The model brings the theory of formal language into stochastic generative models, offering a significant contribution to the field. The SD-VAE efficiently addresses syntax and semantic constraints in stochastic generation by converting offline checks into online guidance. It has a computational cost of O(n) and outperforms existing methods like CVAE and GVAE. The model shows strong empirical performance in Python programs and molecules, consistently improving results in generation, reconstruction, and optimization. The variational autoencoder BID16 BID23 is a framework for learning probabilistic generative models and their posterior. It involves a decoder modeling generative processes and an encoder approximating the posterior. The decoder and encoder are learned simultaneously by maximizing the evidence lower bound (ELBO) of the marginal likelihood. A context free grammar (CFG) is defined as G = V, \u03a3, R, s, where symbols are divided into V, the set of non-terminal symbols, \u03a3, the set of terminal symbols, and s \u2208 V, the start symbol. Attribute grammar enriches the CFG with \"semantic meaning\" by introducing attributes and rules. Attributes can be inherited or synthesized, with inherited attributes depending on parent and sibling attributes. The attribute grammar enriches context-free grammars by introducing inherited and synthesized attributes. These attributes are used to compute non-context-free semantics, as exemplified in a toy grammar subset of SMILES. The attribute grammar is leveraged to check if ringbonds come in pairs, ensuring semantic constraints are met. The semantic constraint in SMILES, known as cross-serial dependencies (CSD) BID4, is non-context-free BID24. Syntax and semantics checks in compilers are illustrated in FIG2. Attributes are calculated bottom-up in the semantic correctness checking procedure. The attribute grammar guides top-down generation but faces challenges due to the need for information from children nodes. The Syntax-Directed Variational Autoencoder (SD-VAE) addresses both syntax and semantics constraints by incorporating attribute grammar online into VAE. This method aims to guide top-down generation of tree-structured data efficiently. The proposed method Syntax-Directed Variational Autoencoder (SD-VAE) addresses challenges in incorporating attribute grammar into tree generation by introducing stochastic lazy attributes for on-the-fly computation of synthesized attributes. This enables semantic validation during tree generation. The decoder with stochastic lazy attributes generates valid output by sampling from the decoder p \u03b8 (x|z) in a tree generation procedure. This involves sampling production rules, updating context vectors, creating nodes with attributes, recursive generation of children nodes, and updating synthetic attributes with lazy linking. Stochastic predetermination is illustrated in FIG3, determining the index and bond type of the ringbond matched at a node. The decoder with stochastic lazy attributes generates valid output by sampling from the decoder p \u03b8 (x|z) in a tree generation procedure. Stochastic predetermination is used to guess a value for the absence of synthesized attribute s.matched, passing constraints to children nodes, and sampling rules under constraints. The decoder with stochastic lazy attributes generates valid output by sampling from the decoder p \u03b8 (x|z) in a tree generation procedure. The sampling distribution is carefully designed to satisfy inherited constraints. When expanding atom 1, the sampling distribution only has positive mass on specific rules. Lazy linking is used to pass down attributes to regulate the generation of atom 2. The generation of atom 2 is demonstrated in a syntax tree construction process within grammar G. Starting with the root symbol, nonterminal nodes are expanded step by step until all nodes in the tree are terminals. Lazy linking passes attributes to regulate the generation process. The algorithm for sampling syntactic and semantic valid structures is obtained by generating trees in the model training phase. The likelihood computation procedure involves fixed tree structures, while the sampling procedure involves sampled structures. The generative likelihood is computed using an encoder approximating the posterior of the latent variable. The encoder parametrization in the observation x is crucial, utilizing deep learning models like BID9 and BID19. The proposed syntax-directed decoder incorporates attribute grammar for semantic restrictions, using the same encoder as in BID18 for comparison. The encoder model in Kusner et al. FORMULA0 converts parse trees into production sequences for deep convolutional neural networks. The text discusses using a deep convolutional neural network as an encoder to map sequences of one-hot vectors to a continuous vector. The goal is to maximize the evidence lower bound and map the input structure to a latent space. The variational posterior is parameterized with a Gaussian distribution, and the prior is a Gaussian distribution as well. The decoding stage aims to maximize the conditional likelihood, and syntax and semantics constraints are precomputed during training. There is no significant time penalty observed during training compared to previous works. Generative models for discrete structured data, like the BID25 sequence to sequence model, have gained interest. Validity issues persist, even with techniques like data augmentation and reinforcement learning. Overfitting to simple structures is common, neglecting diversity. Parse tree generation for structured data with formal grammars is beneficial. Generative models for structured data benefit from parse tree generation using formal grammars. The Grammar VAE BID18 and BID21 use CFG constraints and R3NN to capture context information. Our work introduces stochastic lazy attributes to capture semantics when generating structured outputs, improving efficiency and reshaping deep networks. Our proposed SD-VAE improves generative models for structured data by addressing common semantics, reshaping decoder output, and providing better reconstruction accuracy and diversity in generated structures. It outperforms previous methods like CVAE and GVAE in program and molecule regression tasks. Training details and code are available at https://github.com/Hanjun-Dai/sdvae. The SD-VAE outperforms previous methods like CVAE and GVAE in program and molecule regression tasks, finding better solutions and demonstrating a smoother and more discriminative latent space. The datasets consist of programs represented as atomic arithmetic operations on variables and immediate numbers, with semantic constraints to be respected. The SD-VAE outperforms previous methods in program and molecule regression tasks by addressing key semantics such as variable definition before use and returning a variable in programs, and ensuring ringbonds satisfy constraints in SMILES strings. The SMILES semantics addressed include ringbond and valence constraints. Reconstruction accuracy of VAEs is measured using a held-out dataset. Encoding and decoding in VAEs are stochastic, so a Monte Carlo method is used for estimation. This involves multiple encoding and decoding iterations to assess reconstruction and validity of prior distributions. Our model achieves near-perfect reconstruction rates and perfect valid decoding programs from prior by utilizing full semantics. The reconstruction success rate remains high even as the size of the program grows. Comparison results with CVAE and GVAE from BID18 are also included. Our model achieves high rates of successful reconstruction and valid prior decoding, outperforming CVAE and GVAE from BID18. Results show that using an alternative kekulized form of SMILES can further improve the valid prior portion. Additionally, our SD-VAE aligns better with ground truth compared to CVAE, while GVAE struggles to find meaningful programs. In this application, models are tasked with finding the program most similar to the ground truth program by measuring the distance with log(1 + MSE). The MSE calculates the discrepancy of program outputs using 1000 different inputs sampled evenly in the range of [-5, 5]. Our method outperforms CVAE and GVAE in finding the best program compared to the ground truth. Additionally, our method optimizes drug properties of molecules by optimizing for octanol-water partition coefficients, showing better results than previous works. The SD-VAE method found molecules with better scores than previous works, with richer structures compared to baselines. Predictive performance using encoded mean latent vectors is reported in Table 2. VAEs also aid in unsupervised feature representation learning. The latent space predicts properties of programs and molecules, with sparse Gaussian Process training on latent vectors. The SD-VAE method outperforms CVAE and GVAE baselines in producing more discriminative latent spaces. Results in Table 2 show Log Likelihood and Regression Mean Square Error performance. Table 3 displays diversity statistics, indicating better diversity for SD-VAE compared to GVAE. The diversity of generated molecules is measured using a molecular similarity metric in the SMILES dataset. Both GVAE and SD-VAE methods show no mode collapse issue and produce similar diversity scores. This experiment aims to assess the methods' ability to generate diverse data and avoid mode collapse in the learned space. Our method avoids mode collapse and maintains diversity in generating molecules. We visualize the latent space by interpolating between two programs encoded in the space, showing our model's ability to produce coherent structures. Comparing our results with previous works, our SD-VAE can decode valid programs efficiently. SD-VAE can decode valid programs efficiently and produce visually smooth interpolations in the latent space, while CVAE and GVAE make errors in generating programs. The program contains multiple mathematical operations involving trigonometric functions and exponentials. The operations are performed on variables like v0, v1, v2, v3, v4, v5, v6, v7, v8, and v9. The program returns different values after each set of operations. The program involves mathematical operations on variables v0, v6, v8, and v4, returning different values. The proposed SD-VAE enforces syntactic and semantic constraints for smooth interpolation in latent space. Visualization of molecules in 2 dimensions involves interpolating a 2-D grid and projecting back to latent space. The proposed SD-VAE introduces a new method to address syntax and semantic constraints in generative models for structured data. It utilizes a stochastic lazy attribute for online guidance in stochastic generation, showing consistent improvement over previous models. Future work aims to refine formalization theoretically and apply it to a wider range of data modalities. The SD-VAE distinguishes itself from previous works like CVAE and GVAE in its formalization of syntax and semantics. SD-VAE differentiates itself from previous works like CVAE and GVAE by formalizing syntax and semantics. It uses a deep neural network model with specific architecture, including CNNs in the encoder and RNNs in the decoder. The latent space has 56 dimensions, and the model is implemented in PyTorch framework. Hyperparameters are tuned on a 10% validation set for fair comparison. The study uses ReconstructLoss + \u03b1KLDivergence as the loss function for training, exploring different values for \u03b1. Bayesian optimization is employed to search for latent vectors with specific properties, such as fitting input-output pairs in symbolic program regression or finding drug-like molecules in drug discovery. The variational autoencoder is trained unsupervisedly, structures are encoded into latent space, and comparisons with baseline algorithms follow the settings used in BID18. The study uses Bayesian optimization to search for latent vectors with specific properties, such as fitting input-output pairs in symbolic program regression or finding drug-like molecules in drug discovery. Latent vectors are proposed using batch Bayesian optimization with the expected improvement heuristic, and reconstruction results of SMILES are visualized in FIG6. The decoder successfully recovers the exact origin in most cases. The decoder successfully recovers the exact origin input in most cases, with some small variations due to its stochastic nature."
}