{
    "title": "SJl7DsR5YQ",
    "content": "Reinforcement learning (RL) is a powerful framework for problem-solving by learning from mistakes, but in autonomous vehicle (AV) control, RL can be dangerous in the real world. AV RL is generally only viable in simulation due to imperfect representations. A new learning framework is proposed that learns from restricted exploration with a human demonstrator, focusing on safe behavior for continuous control. The proposed alternative framework, ReNeg, learns continuous control from safe behavior using human-labeled driving data. It focuses on rating atomic actions rather than entire action sequences, outperforming behavioral cloning in lane-following tasks with limited data. The goal is to enable autonomous vehicle control using only RGB input from a front-facing camera. The text discusses learning autonomous vehicle control through \"Learning from Demonstration\" (LfD) where a human explorer takes actions to train the agent to behave optimally in dangerous situations. The goal is to map states to actions for lane-following while ensuring safety. The text discusses learning autonomous vehicle control through \"Learning from Demonstration\" (LfD) where a human explorer takes actions to train the agent to behave optimally in dangerous situations. In order to improve performance, the focus is on letting humans assign feedback that evaluates sub-optimal actions, without representing the optimal action. The goal is to learn a deterministic continuous control policy from demonstration, including both good and bad actions and continuous scores representing this valence. The ReNeg framework focuses on mapping sensor input to continuous policy output using feedback for predetermined actions. It combines supervised learning with evaluative scalar feedback, resembling regression with positive and negative weights attached to data points. A human demonstrator drives to provide state-action pairs for the agent to learn from. The ReNeg framework uses supervised learning and evaluative scalar feedback from a human demonstrator to teach the agent good and bad actions. To create an effective policy, a loss function is defined, driving data is collected to expose the agent to various states, and human feedback is carefully chosen for the agent to learn from. The Loss Function, Driving Data, and Feedback sections discuss the choices made for the problem. Learning from demonstration is mainly studied in supervised learning and Markov Decision Process frameworks. In supervised learning, imitation learning is used, while in the MDP framework, it is known as apprenticeship learning. Relevant RL algorithms for LfD are lacking in this problem setting. Supervised learning focuses on least squares regression mapping input state to action, with the expert providing optimal demonstrations for training data. The main issue in learning from demonstration is the Covariate Shift, where the trained agent struggles when encountering unseen states. DAgger is a common solution, involving exploration and expert labeling of new data. Improved versions address scalar feedback and deep neural networks. However, DAgger-based policies require exploration and mistakes in the Markov Decision Process research. Inverse reinforcement learning (IRL) methods like distance minimization for reward learning from scored trajectories allow for gradation in scores but are limited in handling arbitrary reward functions on continuous inputs or labels for atomic actions. These methods do not address the issue of Covariate Shift in learning from demonstration. Off-policy RL algorithms like Off-Policy Actor-Critic and Q-Learning require agents to have a non-zero probability of choosing any action in any state for convergence. This is different from Learning from Demonstration (LfD), where data is collected before training and the policy remains constant. The Normalized Actor Critic (NAC) algorithm attempts to bridge the gap between off-policy and Learning from Demonstration (LfD) frameworks. However, NAC does not support restricted exploration and is limited to discrete action control, not continuous. The COACH algorithm, based on stochastic policy gradient, uses human feedback for on-policy exploration. It differs from LfD methods like behavioral cloning and IRL in the AV context. No research has focused on using evaluative scalar feedback in the context of AV control with LfD. This oversight could be addressed by incorporating expert feedback to improve AV performance without the need for extensive data collection. The rate of loss minimization should be determined by the magnitude of the feedback provided. The scalar loss function is a generalization of mean squared error, commonly used in behavioral cloning. It assumes Gaussian noise in training data and aims to produce a maximum likelihood estimate for model parameters. The function helps the network avoid negative examples while seeking the best ones. The scalar loss function, parameterized by p or \u03b8 p, is used in behavioral cloning to estimate model parameters with Gaussian noise in training data. It aims to minimize negative examples and maximize likelihood. The feedback magnitude |f| can represent certainty, with adjustments for positive and negative examples to minimize negative data probability. This aids in recovering behavioral cloning easily. The introduction of two hyperparameters in behavioral cloning helps in recovering the model easily. Thresholding feedback values and scaling down negative examples with parameter \u03b1 trade off between behavioral cloning and avoiding negative data. The scalar loss function resembles a loss inducing a stochastic policy gradient in standard RL policy networks. The loss function in continuous control involves predicting a mean for a normal distribution and sampling actions from it. By substituting the probability density function for a normal distribution, the loss becomes MSE scaled by the return. This scalar loss function is derived from the stochastic policy gradient and Gaussian policy, resembling a loss inducing a stochastic policy gradient in standard RL policy networks. Additionally, online feedback can be viewed as a sample from the advantage function, empirically verified to work for online RL with discrete feedback. The text discusses issues with using a pre-determined policy in off-policy training for online RL with discrete feedback. The stochastic policy gradient requires exploration to be stochastic, but data is drawn from a deterministic policy, leading to challenges in justifying the loss function. When using a deterministic policy in off-policy training for online RL with discrete feedback, the network may struggle to converge to the optimal action. This is because the network's predictions are fixed, leading to a worsening policy. In contrast, a stochastic RL policy allows for exploration and gradual convergence to the optimal action. When using a deterministic policy in off-policy training for online RL with discrete feedback, the network may struggle to converge to the optimal action. The network's predictions are fixed, leading to a worsening policy. In contrast, a stochastic RL policy allows for exploration and gradual convergence to the optimal action. The neural network's inability to influence the probability of seeing an example again can lead to learning policy issues. The network may encounter bad examples in training, pushing it further away from them with each epoch, which may not be helpful if the network did not favor taking those actions before. The use of importance sampling in off-policy exploration may not be effective due to the network's struggle to converge to optimal actions with deterministic exploration. The gradient tends towards 0 as the difference between examples increases, impacting learning performance. Importance sampling has shown to reduce performance in learning from demonstration. The feedback in learning from demonstration may not accurately represent the optimal policy Q\u03c0, leading to potential risks in decision-making. Comparing to stochastic policy gradients is useful but not entirely convincing. Applying reinforcement learning losses to supervised learning lacks mathematical justification. In learning from demonstration, feedback may not accurately represent the optimal policy Q\u03c0, posing risks in decision-making. Extending and generalizing MSE is a focus, with the importance of feedback sign emphasized. The stochastic policy gradient no longer computes the gradient of the current policy, making the sign crucial in the absence of sampling actions dependent on the policy. The introduction of the \u03b1 parameter scales down the importance of negative examples in case of an abundance. The text discusses the challenges of using negative examples in learning from demonstration. It highlights how an abundance of negative examples can overwhelm the regression process, leading to an \"exponential\" loss. The introduction of a new loss function aims to address this issue by assigning infinite loss to negative examples at distance 0, which then decreases exponentially with distance. The new loss function aims to prevent regressions diverging to positive or negative infinity by assigning infinite loss to negative examples at distance 0, which decreases exponentially with distance. The concern is that for positive fractional differences and negative non-fractional differences, the desired property of loss functions may no longer hold. The new loss function aims to prevent regressions diverging to positive or negative infinity by assigning infinite loss to negative examples at distance 0, which decreases exponentially with distance. To address concerns about positive fractional differences and negative non-fractional differences, a potential solution involving another neural network (FNet) is discussed in the appendix. This FNet could be used as a loss function for regression with the PNet, maximizing feedback for better regressions. The FNet can be used as a loss function for training a policy network or for efficient inference by selecting the best action based on its predictions. It is noted that adding more negative points to the FNet does not significantly affect regression, but rather increases confidence in negative feedback. However, the FNet cannot handle purely positive points without gradation, making it unsuitable for behavioral cloning. The FNet cannot handle purely positive points without gradation, making it unsuitable for behavioral cloning. Data on \"safe\" driving was collected, along with bad driving types like \"swerving\" and \"lane changing\". Swerving involved driving in a \"sine wave\" pattern on both sides of the road, while lane changing data was collected by driving to the right side of the road. The team collected data on \"safe\" driving and bad driving behaviors like \"swerving\" and \"lane changing\". They practiced lane changing by driving to the right side of the road, straightening out, staying there, and then returning to the middle for 10 minutes on each side. They use the Backseat Driver framework to collect feedback in the AV context, which includes more information than just a reward in reinforcement learning. They label data with feedback to measure how \"good\" an action is relative to other possible actions, using a method that is more intuitive and contains more signal than using a slider from -1 to 1. To improve the labeling process for feedback in autonomous vehicles, the team decided to collect feedback using the steering wheel. They found it easier for people to focus on the steering angle, making it more intuitive. Instead of turning the steering wheel to the correct angle, they labeled the differential by turning the wheel to the left if the car should steer more left, providing a clearer signal for error detection. The team used the steering wheel to collect feedback for autonomous vehicles, labeling the error by turning the wheel left if the car should steer more left. This method, called \"Backseat Driver,\" converts angle labels into feedback values in the range of [-1, 1] using a specific equation. The feedback is proportional to the steering error, with a greater error resulting in a less positive signal. The feedback for autonomous vehicles is based on steering error, with greater errors resulting in more negative feedback. Positive feedback is scaled based on the direction of the steering, rewarding quick actions back to the center of the road. Limited data was used to test the effectiveness of the feedback system. We collected feedback for autonomous vehicles based on steering error, using limited data. Sampling states at a rate of two frames per second, we augmented our data for training. End-to-end optimization of a neural network was chosen to learn our policy, aiming to optimize all aspects of the pipeline for the task at hand. We used transfer learning with a pretrained Inception v3 model for computer vision tasks. The network was split at a layer called mixed 2, with added fully connected layers. For the FNet, the angle input was concatenated onto the first fully-connected layer. The FNet architecture includes the angle input concatenated onto the first fully-connected layer. Trained models were tested in a Unity simulator to record crash times. The PNet with scalar loss, PNet with exponential loss, and FNet were compared. The FNet primarily predicted feedback based on state, not angle. The scalar loss model performed best and was trained correctly, leading to more time spent tuning hyperparameters. Figure 4 shows that negative data is useful, and thresholding feedback to -1 and 1 increased performance to about 30 seconds compared to 6 seconds with default settings. Increasing the learning rate instead of thresholding yields better performance, indicating that gradations in the data help training. The performance is achieved by tuning the learning rate to work well on thresholded data, leading to smaller steps and effectively decreasing the learning rate. Note that \u03b1 can be in [0, \u221e], but focusing on \u03b1 = 0 and \u03b1 = 1 corresponds to no negative examples and an equal weighting between positive and negative examples. Adding negative examples with some relative importance to positive examples can be beneficial for learning lane following for autonomous vehicles from demonstration. The best learning rate for the models was 1e-5, and after training each model 3 times, the mean performance was calculated. The results showed improved model performance with the addition of negative examples. Our scalar loss model outperformed the behavioral cloning baseline by 1.5 times, confirming the effectiveness of adding negative examples for autonomous vehicles. The regression method used allows for learning deterministic continuous control problems from a range of behaviors without the need for an additional neural network. The introduced Backseat Driver method enables efficient collection of continuous human feedback for autonomous vehicles, potentially improving performance in the industry. The model architecture includes Inception v3 with added fully connected layers and Tanh activation. Training details include a batch size of 100, 5 epochs, \u03b1 value of 1.0, and learning rate of 1e-6. Input preprocessing involves bilinear sampling, mean subtraction, and standard deviation division. During training, two validation metrics were tracked: loss and cloning error, which is the average angle the model is off by on positive data. The error started to increase on models with increased learning rates, while the loss was still decreasing. This led to experimenting with varying learning rates to determine the importance of loss. After experimenting with varying learning rates on different models, it was found that behavioral cloning models performed better on the \"cloning\" error when thresholded with \u03b1 = 0.0. Despite potential increases in cloning error, it was considered a useful metric for training and comparison. Comparing performance, a learning rate of 1e-5 worked best for both behavioral cloning and ReNeg models. The scalar loss performed best with a learning rate of 1e-5. Future research should focus on the loss function and enforcing continuity. Fine-tuning the policy once it is safe is a separate but interesting problem. Supervised approaches with a safety driver and active learning methods could be explored. The loss function in an AV application of DAgger should focus on alterations to ensure negative examples have less impact as they get farther away, and positive examples have more impact. Two proposed loss functions meet these criteria by incorporating exponential decay and modifying the scalar function. The loss function in an AV application of DAgger should focus on alterations to ensure negative examples have less impact as they get farther away, and positive examples have more impact. A proposed loss function avoids exponential decay and increase with distance, allowing outliers more easily but ensuring minimized loss with more positive examples than negative examples. The loss function in AV application of DAgger should prioritize minimizing loss with more positive examples than negative examples. To prevent regression towards positive or negative infinity, adding a regularization term to penalize discontinuity could be beneficial. Adding a regularization term to penalize discontinuity in the loss function for AV application of DAgger could be beneficial by penalizing dissimilar answers for nearby states."
}