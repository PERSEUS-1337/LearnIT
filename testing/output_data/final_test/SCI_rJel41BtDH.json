{
    "title": "rJel41BtDH",
    "content": "Semi-supervised learning in image classification focuses on learning from unlabeled samples using soft pseudo-labels generated by network predictions. Overcoming confirmation bias, mixup augmentation, and setting a minimum number of labeled samples per mini-batch are effective regularization techniques. The proposed approach achieves state-of-the-art results in CIFAR-10/100 and Mini-ImageNet, showing the effectiveness of pseudo-labeling. Convolutional neural networks (CNNs) are widely used in computer vision, requiring large amounts of labeled data. Pseudo-labeling has shown to outperform consistency regularization methods, providing an alternative to obtaining labeled data. Knowledge transfer through deep domain adaptation is another popular approach in learning transferable representations. The code for the results will be made available. Semi-supervised learning (SSL) focuses on learning useful representations in a target domain with sparsely labeled data and extensive unlabeled data. It includes learning with label noise and self-supervised learning. SSL is a key task in various domains like images, audio, time series, and text. Recent image classification approaches emphasize consistency in predictions. Recent image classification approaches focus on exploiting consistency in predictions for the same sample under different perturbations or generating labels for unlabeled data. This is achieved through strategies like warm-up phases, uncertainty weighting, adversarial attacks, and graph-consistency to address confirmation bias or noise accumulation. This paper explores pseudo-labeling for semi-supervised deep learning from network predictions, showing that simple modifications can prevent confirmation bias and achieve state-of-the-art performance without consistency regularization strategies. The approach adapts a method proposed by Tanaka et al. (2018) to address label noise exclusively on unlabeled samples. Experiments reveal limitations due to confirmation bias, which are mitigated by using mixup augmentation effectively. The paper proposes using mixup augmentation as a regularization technique to address confirmation bias in deep neural networks. It also highlights the effectiveness of dropout regularization and data augmentation in mitigating confirmation bias issues. The purely pseudo-labeling approach achieves state-of-the-art results without the need for multiple networks or extensive training epochs. The proposed SSL approach simplifies the process by not requiring graph construction or consistency regularization methods, yet achieves state-of-the-art results. Previous deep SSL methods differ in their use of consistency regularization or pseudo-labeling for learning from unlabeled data, all utilizing cross-entropy loss on labeled data. Consistency regularization enforces consistent outputs for the same sample under different perturbations. In (et al., 2016), randomized data augmentation, dropout, and random max-pooling are applied to enforce similar softmax predictions. (Laine & Aila, 2017) extends perturbations to different epochs, requiring current predictions to match past ensemble predictions. (Tarvainen & Valpola, 2017) interprets temporal ensembling as a teacher-student problem, separating the teacher and student networks to address confirmation bias. In (Li et al., 2019) and (Miyato et al., 2018), methods are proposed to improve learning from unlabeled samples by introducing uncertainty weights and virtual adversarial training. Luo et al. (2018) suggest using contrastive loss for regularization, extending consistency regularization to different sample comparisons. In (Tarvainen & Valpola, 2017) and (Miyato et al., 2018), methods are combined to enhance performance by encouraging consistent predictions at interpolated unlabeled samples. Verma et al. (2019) and Berthelot et al. (2019) also utilize consistency regularization techniques to improve learning from labeled and unlabeled examples. Co-training (Qiao et al., 2018) involves training multiple networks to agree on predictions and disagree on errors. Consistency regularization methods encourage networks to make consistent predictions and disagree on errors, benefiting from stochastic weight averaging to improve generalization. Pseudo-labeling is also utilized to enhance learning from labeled and unlabeled examples. Pseudo-labeling is a method that generates labels for unlabeled samples to guide learning. Recent approaches use network predictions as labels and introduce uncertainty weights based on sample distances in feature space. They also include terms to encourage intra-class compactness and inter-class separation, along with consistency between perturbed samples. Improved results are reported when combined with other methods. Pseudo-labeling is a method that generates labels for unlabeled samples to guide learning. A recent work implements pseudo-labeling through graph-based label propagation, alternating between training from labeled and pseudo-labeled data. Uncertainty scores are added for samples and classes to address confidence in predictions and class-imbalance. SSL is formulated as learning a model from a set of training samples split into labeled and unlabeled sets. The model parameters are optimized using categorical cross-entropy with softmax probabilities. Pseudo-labels for unlabeled samples are generated using soft labels, outperforming hard labels. This approach confirms previous observations on relabeling with label noise. In the context of relabeling with label noise, softmax predictions are stored and used to modify soft pseudo-labels for unlabeled samples at the end of each epoch. Regularizations are applied to improve convergence, addressing issues with early training stages where the network's predictions are mostly incorrect. The prior probability distribution for class c is assumed to be uniform, and two regularizations are applied to improve convergence in semi-supervised learning. The first regularization focuses on the mean softmax probability of the model for each class, while the second regularization concentrates the probability distribution of soft pseudo-labels on a single class to avoid local optima. The total semi-supervised loss is controlled by \u03bb A and \u03bb H. The total semi-supervised loss is controlled by \u03bb A and \u03bb H, where regularization terms \u03bb A and \u03bb H are used. Pseudolabeling approach is not state-of-the-art for SSL, and mechanisms proposed in Subsection 3.1 make it a suitable alternative. Incorrect predictions used as labels in pseudo-labeling can lead to overfitting and confirmation bias. Mixup data augmentation combines data augmentation with label smoothing to address this bias. Mixup training with label smoothing reduces overconfidence in deep neural networks by randomly combining labels during training, improving model calibration and reducing prediction confidence. This approach is especially beneficial in semi-supervised learning with pseudo-labeling to prevent overfitting to model predictions. Mixup training with label smoothing reduces overfitting by combining labels, improving model calibration. However, mixup alone may not effectively address confirmation bias with few labeled examples. Setting a minimum number of labeled samples per mini-batch helps reduce confirmation bias and improve model training. Mixup training, along with techniques like a minimum number of samples per mini-batch, dropout, and data augmentation, reduces confirmation bias and enhances pseudo-labeling effectiveness in image classification datasets like CIFAR-10/100, SVHN, and Mini-ImageNet. Validation sets are used for studying hyperparameters, and test results are reported for comparisons. In experiments with CIFAR-10, CIFAR-100, and SVHN datasets, various architectures like \"13-CNN,\" Wide ResNet-28-2, and PreAct ResNet-18 were used with different numbers of labeled images. The study also included semi-supervised learning with Mini-ImageNet, focusing on generalization to different architectures. The dataset used in the study consists of 100 classes with 600 color images per class from ImageNet. Train and test sets contain 50k and 10k images, with 500 images per class for training and testing. A ResNet-18 architecture is utilized, along with image normalization, data augmentation, and color jitter for robustness. Training is conducted using SGD with momentum of 0.9, weight decay of 10^-4, and batch size of 100. The learning rate starts high and is reduced twice during training. For CIFAR and Mini-ImageNet, training lasts 400 epochs with learning rate reductions at epochs 250 and 350, while for SVHN, training lasts 150 epochs with reductions at epochs 50 and 100. Regularization weights \u03bb A and \u03bb H are set to 0.8 and 0.4. Dropout is used between consecutive convolutional layers in certain ResNet blocks. Regularized pseudo-labeling is shown to be a viable option for semi-supervised learning, utilizing weight normalization in all networks. Figure 1 illustrates the impact of naive pseudo-labeling, mixup, and a combination of mixup with a minimum number of labeled samples per mini-batch on decision boundaries. Naive pseudo-labeling can lead to overfitting, while mixup helps model the structure better and results in a smoother boundary. Regularized pseudo-labeling is a viable option for semi-supervised learning, utilizing weight normalization in all networks. Naive pseudo-labeling can lead to overfitting, while mixup helps model the structure better and results in a smoother decision boundary. When training with cross-entropy loss for 4000 labels in CIFAR-10/100, naive pseudo-labeling leads to an error of 11.40/48.54, which can be reduced to 7.16/41.80 with mixup. However, with only 500 labels in CIFAR-10, mixup alone is insufficient to ensure low error (32.10). Setting a minimum number of samples per mini-batch, such as 16, can help tackle this issue. Setting a minimum number of samples per mini-batch, like 16, proves effective in reducing error rates in CIFAR-10 and CIFAR-100 datasets when combined with mixup. This approach dramatically decreases errors, such as reducing error from 32.10 to 13.68 in CIFAR-10 with 500 labels. Additionally, using mixup and minimum k as regularizers helps reduce confirmation bias during training, as shown by the decrease in certainty of incorrect predictions. In a study comparing different network architectures for image classification, skip-connections in ResNet were found to be crucial for representation quality. The importance of exploring various architectures was highlighted, leading to testing with Wide ResNet-28-2 and PreAct ResNet-18. Pseudo-labeling with mixup and k = 16 showed good results across architectures, except for WR-28 with 500 labels. The study compared network architectures for image classification, finding that skip-connections in ResNet are crucial. Pseudo-labeling with mixup and k = 16 worked well across architectures, except for WR-28 with 500 labels due to confirmation bias. PR-18 was more robust to bias than WR-28, and the 13-layer network outperformed PR-18 with fewer parameters. Moving from cross-entropy to mixup reduced certainty of incorrect predictions during training. Table 2 shows that dropout and data augmentation are essential for good performance across all architectures in image classification. Dropout p = 0.1, 0.3 helps achieve better convergence in CIFAR-10, and adding color jitter as additional data augmentation further reduces errors. Quality of pseudo-labels is crucial, requiring disabling dropout and data augmentation in the second forward pass for improved performance. This configuration is used for comparison with state-of-the-art approaches. The proposed pseudo-labeling approach outperforms related work using various methods in CIFAR-10/100 and SVHN datasets. The approach shows superior performance compared to consistency regularization methods and other pseudo-labeling approaches combined with consistency regularization methods. The proposed approach demonstrates superior generalization compared to other methods in CIFAR-10/100 and SVHN datasets. It also scales successfully to higher resolution images, outperforming related work in Mini-ImageNet. Additionally, it achieves better performance than ICT and is competitive with MM for 500 and 4000 labels using WR-28. The approach converges to reasonable performance for 4000 and 500 labels in PR-18, but not for 250 labels. The 13-CNN model robustly converges even with 250 labels. The 13-CNN model converges robustly even with 250 labels, suggesting further exploration of the relationship between label numbers, dataset complexity, and architecture type. Data augmentation and dropout help with 500 labels/class but are insufficient for 250 labels. Better data augmentation or self-supervised pre-training may overcome this challenge. Hyperparameters like regularization weights and mixup parameters require further study. Pseudo-labeling modification offers a competitive semi-supervised learning approach without consistency regularization. The paper presents a semi-supervised learning approach for image classification using pseudolabeling without consistency regularization. The approach utilizes network predictions as soft pseudo-labels for unlabeled data along with mixup augmentation, a minimum number of labeled samples per mini-batch, dropout, and data augmentation to improve performance. This method outperforms related work in four datasets, showing that pseudo-labeling is a viable alternative to consistency regularization. Future work should explore SSL in class-unbalanced and large-scale datasets, synergies of pseudo-labeling and consistency regularization, and hyperparameter tuning. The study explores the impact of hyperparameters in a pseudo-labeling approach for image classification. Results suggest that adjusting \u03b1 values can improve performance, with \u03b1 = 4 and \u03b1 = 8 showing potential enhancements. Validation error in CIFAR-10 using different label quantities is reported, indicating the importance of hyperparameter tuning for robust convergence. In CIFAR-10 experiments with 500 labels, the final configuration with dropout and data augmentation showed marginal differences. Stronger mixup regularization may not add to performance. Hyperparameter tuning could slightly improve results, but the current configuration is already effective. SVHN experiments with the 13-CNN network compared state-of-the-art algorithms, showing the use of labeled samples for initial pseudo-label computation. In CIFAR-10 experiments with 500 labels, dropout and data augmentation had marginal differences. Stronger mixup regularization did not improve performance significantly. Hyperparameter tuning could slightly enhance results, but the current configuration is already effective. SVHN experiments with the 13-CNN network utilized labeled samples for initial pseudo-label computation, with a long warm-up to enhance robustness to lower levels of labeled samples."
}