{
    "title": "HklZOfW0W",
    "content": "In this work, a novel approach for learning graph representation using backpropagation gradients is proposed. A neural network architecture compatible with the optimization approach is built, demonstrating richer structure in the learned graph compared to nearest neighbors graphs. Experiments show improved prediction quality for convolution on graphs architectures, with some models being insensitive to the input graph. The rise of deep learning models that can handle non-linearities is noted, with the Multilayer Perceptron (MLP) being a powerful predictor but facing challenges of over-fitting. Convolutional Neural Networks (CNNs) have fewer parameters compared to Multilayer Perceptrons (MLPs) and show groundbreaking results in object recognition in images. CNNs use a sliding window operation on images, reducing the number of parameters proportional to the window size. This approach is effective for images but not applicable outside the image domain. Recent works have focused on generalizing the convolution operation. Multiple works have focused on generalizing the convolution operation to a general domain where the graph is not a lattice. The quality of the graph is crucial for classification performance, but learning the graph for prediction has not been addressed. Two main challenges arise when estimating the graph within a neural network architecture: the architecture itself and the difficulty in obtaining the graph's gradient. A novel neural network architecture is proposed in Section 3, which is differentiable with respect to the graph adjacency matrix and utilizes graph filtering in the vertex domain. The text discusses graph optimization based on backpropagation, focusing on learning the adjacency matrix of a graph with various properties. It highlights the use of gradient obtained through backpropagation to enforce common graph properties and utilize modern deep learning libraries for graph estimation. Other graph-based neural networks are evaluated, and the analysis includes graph estimation for text categorization and time series forecasting. The text discusses optimizing graphs using backpropagation to learn the adjacency matrix. It aims to estimate a function using weight parameters that minimize loss, with the ability to evaluate partial derivatives. The update process involves adjusting the adjacency matrix based on the gradient and desired graph properties. The Undirected Positive Sparse UPS optimizer aims to deliver key properties for graph applications, such as undirected structure, positive edge weights, and sparsity. This optimizer can be easily implemented in modern deep learning libraries and is beneficial for reducing computational complexity, preventing overfitting, and enhancing interpretability of learned graphs. When backprop is used for gradient computation in undirected graphs with positive edge weights, the adjacency matrix should be symmetric. Gradient correction can be applied to ensure the correctness of the procedure, especially for stochastic optimization methods like Adam. Adding an inequality constraint for positive edge weights in the optimization task is necessary for the existence of the graph Laplacian and interpretability of the resulting graph. Various techniques for constrained optimization are available, with the most natural solution being built on top of backpropagation. The projected gradient descent method is effective for optimization on non-convex setups. It enforces positive weights constraint through a projection operator. Sparsity is crucial in high dimensional problems, reducing computational complexity in neural networks relying on graphs. Sparse graphs are more common and beneficial in graph optimization tasks. Sparse graphs are crucial for graph optimization tasks. Variable selection often involves using the L1 penalty to promote sparsity. However, the L1 norm is not differentiable at 0, leading to subgradient descent optimization. To address this, the proximal gradient method is proposed, aligning well with backpropagation-based optimization. The final step involves soft thresholding to obtain a sparse solution. The final step in the optimization process involves soft thresholding by scaling the threshold with the step size. Self connections in the graph can be controlled by setting the diagonal of the adjacency matrix to 0. The UPS optimizer relies on evaluating the partial gradient of the adjacency matrix and proposes a neural network architecture based on Graph Signal Processing to meet this requirement. Processing data as a signal on a graph can enhance feature quality. The text discusses processing data as a signal on a graph using Graph Signal Processing (GSP) for filtering in the vertex domain. It involves modifying signals at features reachable in k steps and combining them into a filtered graph signal for prediction. Linear polynomial graph filters are proposed, and the filtered signal is obtained through matrix multiplication. This filtering method can be used for graph optimization with UPS but has limitations. The text introduces a Graph Polynomial Signal (GPS) neural network to address limitations of linear graph filters in UPS optimization. GPS utilizes non-linear feature mappings based on vertex neighborhoods, with the option to build a more complex neural network on top of GPS features. The architecture is easily implemented using deep learning libraries, allowing for backpropagation to obtain partial derivatives required by the UPS optimizer. The text introduces the role of weights in scaling the graph adjacency for proximal optimization in the UPS optimizer. While penalizing the number of nonzero elements in the adjacency A is ideal for inducing sparsity, the L1 norm is used instead of the NP-hard L0 norm. The drawback of using L1 is that it penalizes nonzero edge weights by their magnitude, which can affect prediction quality. To address this, re-scaling with weights helps to avoid disagreement between the penalty term and prediction quality. Additionally, the weights also play a role in weight sharing similar to classical CNN on images, although in this case, there is no assumption of location invariance for the general data type considered in the work. In our work, we assume weights should be shared among neighboring graph regions without making a location invariance assumption. The Graph Processing System (GPS) enforces weight sharing inside connected components, allowing the UPS optimizer to learn the adjacency. Graph Neural Network (GNN) is a general framework for utilizing graph neighborhood information, with Graph Convolutional Network (GCN) being a modern variation that directly utilizes adjacency for graphs among observations. The Graph Convolutional Network (GCN) BID10 architecture can be modified for graph among features by stacking layers with trainable parameters. This architecture performs 1-hop filtering inside each layer, but becomes complex with multiple layers due to non-linearities. Using UPS with GCN for graph learning is possible, but the connection to graph filtering would be lost. Deep learning on graphs is an active area of research. Deep learning on graphs is an active area of research. Prior work assumes the graph be given or estimated before model fitting. Different approaches involve spatial and spectral architecture design using graph adjacency. Graph optimization methods in spatial and spectral approaches for graph learning include creating receptive fields with graph labeling and 1D convolution, building CNN architectures with fixed degree neighborhoods using transition matrix powers, and utilizing Diffusion-Convolutional Neural Networks (DCNN) for graph, node, and edge classifications. DCNN pre-computes transition matrix powers for optimization but may face challenges with stochasticity and weight restrictions. Spectral architectures utilize eigenvector basis of the graph Laplacian for filtering in the Fourier domain. The key challenge is optimizing nonparametric filters for eigenvectors, which are not sparse and need to be orthonormal. Hierarchical graph clustering for pooling requires redoing on every iteration of graph optimization. BID3 utilized a filtering function for vertex domain filtering with Chebyshev polynomial approximation to bypass eigen decomposition. Graph coarsening and pooling were used based on non-differentiable operations. Optimizing for graph Laplacian with Chebyshev polynomials complicates optimization. The goal is to show insights from graph learned using UPS graph optimizer in the experimental section. The GPS architecture was fitted using UPS optimizer for varying degrees of neighborhood in the graph. Different graph convolutional architectures were trained using various graph initializations and optimization methods. The experiment evaluated the performance on the 20news groups dataset, showing that GPS with optimized graph can achieve good results. Results from the experiment on graph convolutional architectures show that GPS performs well with optimized graphs, while ChebNet and ConvNet are not sensitive to the type of graph used. GCN performs poorly overall but improves with estimated graphs. A comparison is made between learned graphs and kNN graphs, with GPS 4 architecture utilizing nested stochastic block model for visualization. The experiment on graph convolutional architectures shows that GPS performs well with optimized graphs, while ChebNet and ConvNet are not sensitive to the type of graph used. GCN performs poorly but improves with estimated graphs. The dataset consists of time series of visits to a popular website across 100 US cities, with the task of predicting average visits for tomorrow. GPS demonstrated good performance, with noticeable improvement of GCN 3 when the graph was learned. In this work, a novel architecture for graph estimation was developed to account for neighborhoods of varying degrees in a graph. The resulting graph showed more structure than a commonly used kNN graph and performed well for text categorization and time series forecasting. Some modern deep convolution networks on graphs were insensitive to the type of graph used, with only GPS and GCN showing noticeable performance improvement with better graphs. Pooling in GPS architectures remains a challenge as it is unclear how to incorporate it while maintaining gradient extraction ability. This limitation is of interest for further investigation, especially considering the noticeable performance improvement seen with better graphs in deep networks like GPS and GCN."
}