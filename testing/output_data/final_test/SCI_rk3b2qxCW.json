{
    "title": "rk3b2qxCW",
    "content": "In recent years, deep reinforcement learning has shown success in solving sequential decision processes with high-dimensional state spaces like in Atari games. This paper introduces a novel policy gradient methodology for large multidimensional discrete action spaces using LSTM and Modified MDP parameterization. These approaches allow for backpropagation during training. The inclusion of an entropy bonus in the reward function is also discussed for enhancing exploration in high-dimensional action spaces. The paper introduces novel unbiased estimators for the entropy bonus and its gradient to address computational challenges in enumerating all actions in high-dimensional action spaces. The algorithms are tested on various environments, including a multi-hunter multi-rabbit grid game and a multi-agent multi-arm bandit problem. This is in contrast to previous successes in deep reinforcement learning with relatively small action spaces, highlighting the need for solutions in high-dimensional action spaces like in StarCraft and coordinating self-driving cars. In this paper, a novel policy gradient methodology is developed for large multidimensional action spaces. The challenges include designing expressive parameterized policies for efficient sampling and approximating entropy and its gradient to maintain exploration in high-dimensional action spaces. In this paper, two approaches are proposed for parameterizing the policy: a LSTM model and a Modified MDP (MMDP) leading to a Feed-Forward Network (FFN) model. Novel unbiased estimators for the entropy bonus and its gradient are developed, which can be combined with stochastic gradient descent for a new class of policy gradient algorithms with improved exploration. The algorithms are tested on a multi-agent multi-arm bandit problem and a multi-agent hunter-rabbit grid game. In the standard RL setting, an agent interacts with an environment by sampling actions from a policy distribution \u03c0(\u00b7|s) at each timestep. The return is the discounted accumulated reward from the current timestep until the end of the episode. Parameterized policies \u03c0 \u03b8 (\u00b7|s) are generated by a neural network, with \u03b8 representing the network's weights and biases. The parameters are updated using stochastic gradient descent. The parameters in the network are updated using stochastic gradient ascent on the expected reward. The REINFORCE algorithm updates the parameters in each episode. To prevent convergence to suboptimal policies, entropy regularization is added to the update rule. This approach, known as adding entropy bonus, is widely used in various applications of neural networks.\u03b2 is the entropy weight used in policy gradient for MDP with large action spaces. When applying policy gradient to MDPs with large multidimensional action spaces, challenges arise in designing an expressive parameterized policy and efficiently sampling it. Calculating entropy and its gradient for such spaces may be infeasible due to the need to enumerate all actions. Strategies involve sequentially generating sample components and enhancing exploration in a principled manner. When applying policy gradient to MDPs with large multidimensional action spaces, challenges arise in designing an expressive parameterized policy and efficiently sampling it. Two novel methods are proposed for creating parameterized distributions in multidimensional action space literature. An LSTM is used to generate a parameterized multidimensional distribution and sample actions from it. The LSTM is utilized to create a parameterized multidimensional distribution for sampling actions in large multidimensional action spaces. The process involves generating actions sequentially based on conditional probabilities, which are calculated using the LSTM output and a linear layer. The final action is determined by sampling from the one-dimensional distribution. During training, backpropagation is used to efficiently calculate the conditional probabilities. An alternative approach involves modifying the MDP to create an equivalent MMDP with a one-dimensional action space at each time step. The state in MMDP includes the original state and selected action dimensions, while the action space is {0, 1, ..., K} and the state space is S \u00d7 {0, 1, ..., K}d-1. The MMDP has a one-dimensional action space and uses a feed-forward network to generate actions. An entropy bonus is included for exploration, but calculating it for large action spaces is computationally intensive. Efficient estimates for entropy and its gradient are developed in this section. In this section, computationally efficient unbiased estimates for entropy and its gradient are developed for the RNN and MMDP architectures. The RNN uses a linear layer and softmax to generate a distribution for sampling actions, while the MMDP uses a feed-forward network with a softmax layer. During training, the policy generates actions for each state using LSTM or MMDP. The text discusses generating action samples using LSTM or MMDP, and introduces an alternative unbiased estimator for entropy that only requires one episodic sample. This estimator accounts for entropy along each dimension of the action space. The text introduces a computationally efficient approximation of entropy bonus called smoothed entropy, which has properties that mimic the exact entropy for a multi-variate normal parameterization of the policy. Theorem 2 and Theorem 3 provide insights on the relationship between the smoothed entropy and the exact entropy under certain conditions. The smoothed estimator H \u03b8 (a) serves as a good proxy for the exact entropy, with lower variance compared to the crude estimator H crude \u03b8 (a). However, the gradient of estimators H crude \u03b8 (a) and H \u03b8 (a) are not unbiased estimates of the gradient of the entropy. In this subsection, unbiased estimators for the gradient of the entropy are provided. A crude unbiased estimator for the gradient of the entropy is derived, along with a smoothed estimator. The smoothed estimator for the gradient of the entropy is shown to be an unbiased estimator. The unbiased gradient estimate of the entropy is compared between LSTM and MMDP models in experiments. Different entropy approximations are tested with tuned entropy weights and hidden layers. In the environment, hunters and rabbits are placed on a grid, with hunters having nine possible actions. In each episode, hunters aim to capture rabbits quickly. Capturing a rabbit yields a reward of 1, discounted by time steps with a factor of 0.8. After capture, both hunter and rabbit become inactive. Active entities are represented as (1, y position, x position), while inactive ones are (0, -1, -1). Performance of LSTM and MMDP models with different entropy estimates is shown in TAB0. Smoothed mode entropy is detailed in Appendix D. The evaluation compared LSTM and MMDP models with different entropy estimates on a grid with 5 hunters and 5 rabbits. LSTM outperformed MMDP, and policies with entropy approximations performed better than those without. Smoothed entropy was the best performing approximation, reducing episode length by 45% and increasing reward by 10%. There was no significant difference between smoothed entropy, smoothed mode estimate, and unbiased gradient estimate. In comparing LSTM and MMDP models with different entropy estimates, it was found that policies trained with entropy approximations performed better than those without. Smoothed entropy was the top-performing approximation, showing robustness to initial seed variations. The best entropy approximations performed slightly worse than exact entropy for both models. The LSTM model consistently outperformed the MMDP model in the evaluation. The MMDP model involves a multi-agent version of the multi-armed bandit problem, where d agents choose from K arms to maximize total rewards. The optimal policy is for agents to collectively choose the arms with the highest rewards. Experiments were conducted with 4 agents and 10 arms, with a bonus reward added for a specific agent-to-arms assignment. The study involved a multi-agent bandit problem with agents choosing from arms to maximize rewards. Results showed policies with entropy approximations outperformed others, especially in pulling optimal arms. Limited attention in RL literature for large discrete action spaces was noted. The study focused on multi-agent bandit problems with agents selecting arms to maximize rewards. Different approaches were discussed, including approximate linear programming and embedding discrete actions in a continuous space. Each approach had its limitations, such as the need for tuning hyper-parameters and homogenous agents. The decentralized approach in BID17 showed promise for systems with agents entering and leaving but may lead to suboptimal policies. The study proposes using LSTMs and a modified MDP for RL problems with large multidimensional action spaces, introducing novel estimators for entropy regularization. Prior work has focused on encouraging exploration in policy gradient, with BID10 modifying the entropy term to optimize under-appreciated reward exploration. The study introduces a new optimization objective called under-appreciated reward exploration by applying entropy regularization to Q-learning methods. They develop a novel policy gradient methodology for large multidimensional discrete action spaces using LSTM and Modified MDP parameterization. Novel unbiased estimators for entropy bonus and its gradient are also proposed, with experimental work conducted in two environments with large multidimensional action space. The study introduces a new optimization objective called under-appreciated reward exploration by applying entropy regularization to Q-learning methods. Experimental work in two environments with large multidimensional action space showed that both LSTM and MMDP approaches were successful, with LSTM generally performing better. Smoothed entropy estimates and unbiased gradient estimates helped reduce computational cost without sacrificing performance. Hyperparameters for the policies included 128 hidden nodes for LSTM and varying numbers of hidden layers for MMDP. The network was trained using first visit Monte Carlo return to minimize the L1 loss between actual and predicted values of states visited during the episode. Experimental results are averaged over five seeds and run with 4 agents and 10 arms. LSTM policy has 32 hidden nodes and the baseline is a truncated average of the reward of the last 100 rounds. The learning rates for different entropy methods are 0.006, 0.008, 0.002, and 0.005. Experimental results are averaged over ten seeds. The entropy of a multivariate normal random variable depends only on the variance and not on the mean. The distribution of a variable given previous values has a normal distribution with a variance that does not depend on the previous values. The smoothed entropy H \u03b8 (a) may not depend on previous actions a i\u22121, leading to unbiased estimators. Conditional expectations are used to calculate terms in a double sum, with considerations for episodic actions impacting policy gradient performance. Alternative approximations are explored to potentially enhance performance with minimal computational cost. A computationally efficient approach for calculating entropy in reinforcement learning involves choosing actions greedily or using beam search to approximate the mode of the distribution. This method, known as smoothed mode entropy, can provide a better approximation for entropy than traditional methods. Calculating this entropy and its gradient, however, comes with additional computational costs. Calculating entropy H \u03b8 and its gradient in reinforcement learning comes with computational costs. The biased estimator H \u03b8 has no variance, leading to a bias-variance tradeoff. Theorem 2 and 3 in subsection 4.2 also apply to H \u03b8."
}