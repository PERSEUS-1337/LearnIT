{
    "title": "H1xJjlbAZ",
    "content": "In order for machine learning to be trusted in various applications, it is important to explain why the algorithm makes certain predictions. Interpreting black-box predictors is a key research area, focusing on the trustworthiness of the interpretations. This paper demonstrates the fragility of interpretation in deep learning predictions, showing that inputs with the same predicted label can have very different interpretations. The study evaluates the fragility of interpretations from popular feature-importance methods on ImageNet and CIFAR-10 datasets. Interpretation methods like saliency maps, integrated gradient, and DeepLIFT were tested on ImageNet and CIFAR-10 datasets. Small random perturbations can alter feature importance, while systematic perturbations can lead to different interpretations without changing the label. Explanations for machine learning predictions are crucial in various fields like technology, medicine, and law. Fragility in interpretations poses a challenge to current approaches, as demonstrated by the study's analysis of the Hessian matrix geometry. Interpretations for machine learning predictions are crucial for establishing trust and transparency between users and algorithms. The explanation must be robust to ensure human trust, as demonstrated by the importance of interpretation methods like saliency maps in the medical field. The fragility of prediction in deep neural networks against adversarial attacks is an active area of research. Fragility is exhibited when two perceptively indistinguishable images are assigned different labels by the neural network. In this paper, the definition of fragility is extended to neural network interpretation, where the interpretation is considered fragile if it varies significantly between visually similar images. The interpretation of neural networks is considered fragile when visually similar images with the same prediction label have substantially different interpretations. Three popular interpretation methods were used to generate feature-importance scores, showing that perturbed images can have meaningless saliency maps despite the prediction label remaining unchanged. The interpretation of neural networks is fragile, as perturbed images can lead to dramatically different interpretations. This highlights the limitations in trusting and learning from interpretations, posing a new security concern, especially in critical applications like medicine or economics. The fragility of neural network interpretation is a significant concern, as small input manipulations can lead to different interpretations. This poses a security risk, especially in critical fields like medicine or economics. The complexity of decision boundaries in deep nets can result in drastic changes in importance scores, making it challenging to detect adversarial attacks. Feature-importance methods aim to explain predictions by identifying the most influential input features on a neural network's output. These methods assign saliency scores to each feature, which are then normalized to ensure any perturbations do not alter the absolute feature importance. This is crucial in fields like medicine and economics where small changes can have significant consequences. The training point has a significant impact on the loss in Figure 2, showing why interpretation is fragile. A small perturbation to the input can change the direction of the gradient of the loss, affecting feature-importance analyses. Points near decision boundaries in neural networks are especially sensitive to interpretability-based analysis. The text discusses different methods for calculating the normalized saliency score in feature-importance analyses, including the simple gradient method and integrated gradients. These methods aim to detect the sensitivity of the model to perturbations in input dimensions. The text discusses DeepLIFT as an improved version of layer-wise relevance propagation (LRP) method for calculating feature importance in neural networks. DeepLIFT defines a reference point in the input space and propagates relevance scores proportionally to the changes in input dimensions. DeepLIFT with the Rescale rule defines a reference point in the input space and propagates relevance scores based on changes in neuronal activations. It uses training examples to explain network predictions, with the highest influence examples clarifying why a network made a specific prediction. Similarity metrics are used to evaluate interpretation effectiveness for different images. The effectiveness of targeted attacks on interpretability can be evaluated using Spearman's rank order correlation and top-k intersection metrics. These metrics compare the similarity between interpretations and focus on the most important features. Feature importance and influence function methods provide interpretations for neural networks and input data points, producing vectors of feature scores or training example scores. The goal is to devise efficient methods for interpretation. In this paper, the goal is to devise efficient perturbations to change the interpretability of a test image while ensuring the perturbations are visually imperceptible and do not alter the prediction label. Three types of input perturbations are explored, including random sign perturbation and iterative attacks against feature-importance methods. These perturbations aim to measure robustness against untargeted perturbations and challenge feature-importance methods. The paper explores efficient perturbations to alter the interpretability of a test image without changing the prediction label. Two methods, top-k attack and mass-center attack, aim to decrease the importance of key features and displace the center of mass of saliency scores. These attacks can be applied to feature-importance methods, providing effective adversarial images without iterative procedures. Linearizing around current inputs and parameters, an optimal single-step perturbation can be obtained by constraining the L \u221e norm of the perturbation. The paper discusses efficient perturbations to alter the interpretability of a test image without changing the prediction label. It introduces an optimal single-step perturbation algorithm for iterative feature-importance attacks, aiming to decrease the importance of key features and displace the center of mass of saliency scores. The perturbation is constrained by the L \u221e norm to ensure effectiveness. The center of mass for an image is defined, and perturbed images are generated using a smooth approximation to attack interpretability in networks with ReLUs. The attack involves applying a negative perturbation to decrease the influence of key training images on the original test image. The influence function is computed following a setup by Koh & Liang (2017). To evaluate the robustness of feature-importance methods, two image classification datasets were used: ILSVRC2012 (ImageNet) and CIFAR-10. Different models were employed for each dataset, including a pre-trained SqueezeNet 2 model for ImageNet and a custom convolutional network for CIFAR-10. Gradients were calculated with respect to parameters in the final layer of the network, and standard back-propagation was used to compute the corresponding gradient for the input test image. The perturbation of the input was obtained by taking the sign of this gradient and clipping the image to produce the adversarial test image. For CIFAR-10 and ImageNet datasets, various feature-importance methods were tested using different reference points. Iterative attack algorithms were run for 300 iterations with a step size of 0.5. An InceptionNet v3 model was trained with all layers frozen except the last one, which was pre-trained on ImageNet. The last layer of an InceptionNet v3 model was trained on a binary flower classification task with a dataset of 1,000 images, achieving a validation accuracy of 97.5%. Evaluation of feature-importance methods was done on 512 correctly-classified images from the ImageNet test set using a mass-center attack. Visualizations showed the gradual change in saliency maps with iterations of the attack. The study evaluated feature-importance methods on 512 correctly-classified images from the ImageNet test set using a mass-center attack. Results showed significant changes in saliency maps with different attack methods, indicating fragility in interpreting individual or small groups of pixels. Targeted perturbations led to even more dramatic changes in interpretations. When perturbing L \u221e = 2, interpretations of saliency maps change significantly. The displacement of the center of mass between original and perturbed maps is a strong metric for similarity. Center attack method is more effective than top-k attack. Integrated gradients method shows fragility in neural network interpretation. Top-k and center attacks perform similarly in measures, more effective than random attacks. The integrated gradients method, along with other interpretability methods, demonstrate the fragility of neural network interpretation in top-1000 intersection and rank correlation measures. Influence functions are evaluated for robustness on a test dataset, showing a change in influential training examples under gradient sign attacks. In Fig. 6, gradient sign attacks are more effective at distorting influential training images compared to random perturbations. Even visually imperceptible perturbations can significantly alter the top-5 influential images. Random attacks also impact influence functions, reducing rank correlation to 0.8 on average. This section aims to understand the source of these effects. In this section, the fragility of gradient-based interpretations is explored by analyzing the impact of imperceptible perturbations on a test image's interpretability. The score function S(x; W) is defined, where x is the input vector and W represents the neural network weights. The influence functions are affected by gradient sign attacks, leading to significant changes in exemplar-based interpretability. The influence functions are analyzed by perturbing test images with gradient sign attacks, revealing different influential training images. The impact of perturbations on the influence of training images is shown in a comparison of random and targeted attacks on flower classification. The influence of training images in flower classification is analyzed by perturbing test images with gradient sign attacks. Targeted attacks can significantly affect the rank correlation and change the most influential images. Even with maximal noise, changes to perturbed images are visually imperceptible, and prediction confidence remains stable. The feature-importance vector is robust and independent of input, showing the impact of non-linearity. The feature-importance vector is robust and independent of input, showing the impact of non-linearity in a simple network susceptible to adversarial attacks. The change in saliency map due to perturbation in x can be calculated, with the relative change determined by the first and second derivatives of the non-linearity function g(\u00b7). The saliency of x and w is affected by data preprocessing and weight decay regularization. The perturbation in x is proportional to the 1-norm of w, especially in high-dimensional inputs like images. The perturbation aligns with the first right singular vector of the Hessian \u22072xS. Adversarial attacks on interpretability and prediction are aligned in simple networks, but not in more complex ones. In a study on neural network interpretation, it was found that perturbations can change the interpretation without affecting the prediction. Adversarial attacks on interpretability and prediction are aligned in simple networks but not in more complex ones. This concept has not been previously explored in the context of neural network interpretation. Research on adversarial attacks to manipulate neural network predictions has been actively studied. Szegedy et al. (2013) showed how easy it is to fool neural networks into making different predictions for visually similar test images. The Fast Gradient Sign Method (FGSM) was introduced as a one-step attack, followed by more effective iterative attacks (Kurakin et al., 2016). Interpretation of neural network predictions is also a focus, with post-hoc interpretability methods seeking to explain predictions without revealing the model's hidden mechanisms. Various tools have been developed to explain predictions based on test example features and the contribution of training examples to predictions at test time. Interpretation of neural networks can be fragile, with two similar inputs having different interpretations despite the same predicted label. New perturbations illustrate this fragility and propose evaluation metrics. Fragility of interpretation is separate from prediction fragility, as perturbations can change interpretation without affecting the predicted label. Factors contributing to fragility are discussed, focusing on the interpretation method rather than the original network. Our main message is that the robustness of interpreting predictions is a challenging problem, especially in applications where users value interpretation as much as the prediction itself. Concerns are raised about how interpretations of neural networks can be sensitive to noise and manipulation, particularly in settings where individual feature importance is crucial. Targeted perturbations can dramatically alter interpretations, raising security concerns. Interpretation methods for neural networks need to be used cautiously as they can be manipulated by targeted perturbations, raising security concerns. Saliency maps are vulnerable to perturbations, but this does not mean the methods are broken by them. The saliency correctly captures the sensitivity of the neural network at different inputs, even after perturbations. The saliency captures sensitivity at inputs, showing network fragility. Fragility extends beyond image data to other applications. Training used ADAM optimizer with default parameters, achieving 73% accuracy. Replacing ReLU with Softplus in experiment. The experiment involved replacing ReLU with Softplus and retraining the network, resulting in 73% accuracy. Different convolutional layers and hidden sizes were used in the network. Three examples from ImageNet were provided, showing attacks on feature importance methods. The center-shift measure was found to be most correlated with subjective perception of change in saliency maps. Random sign perturbations did not significantly alter the results. The integrated gradients method is the most robust against adversarial attacks in the center-shift measure for CIFAR10 feature importance methods. Mass-center attack and top-k attack with k=100 show similar results for rank correlation and top-100 intersection measurements. Images in the CIFAR10 dataset are more resistant to adversarial attacks compared to ImageNet images. Increasing the dimension of the input of a simple neural network increases its fragility with respect to influence functions. The influence of a training image on a test image is calculated, focusing on the term dependent on x. The infinitesimal effect of each parameter on the loss function is evaluated, and the change in this term due to a small perturbation in x is calculated. The first-order approximation for the change in the loss function due to a small perturbation in x is calculated for a simple neural network. The derivatives are simplified by defining the loss function as L = |y \u2212 g(w x)|. The saliency relative change grows with the dimensionality of x when \u03b4 = sign(w). Consider a two-layer neural network with activation function g(\u00b7), input x \u2208 R^d, hidden vector u \u2208 R^h, and score function S. The analysis and experiments in this paper show that small perturbations in input layers of deep neural networks can significantly impact interpretations, similar to adversarial examples. The perturbation direction for maximizing the 2 norm of saliency difference is different from the direction of feature importance, indicating non-parallel directions unless certain conditions are met for activation functions like Softplus and Sigmoid. In the context of deep neural networks, small input perturbations can greatly affect interpretations, akin to adversarial examples. To address this, constraining the Lipschitz constant during training has shown some success. A proposed method aims to limit interpretability changes by bounding the impact of input perturbations on a neural network's output. This involves designing a network with gradient insensitivity to input perturbations. The Lipschitz constant of a neural network's gradient can help make its feature importances robust. In a fully-connected network, the Lipschitz constant is related to the weights and non-linearities, suggesting a conservative upper bound for regularization. This approach aims to limit interpretability changes caused by input perturbations. Regularization based on operator norms of weights can help train networks robust to attacks on feature importance. Lipschitz bounds in practice are rarely tight."
}