{
    "title": "ryx0N3IaIV",
    "content": "This paper provides a guide to rigorous comparisons of reinforcement learning algorithms, focusing on statistical significance. It reviews relevant tests, compares them based on false positive rate and power, and explores their robustness. Empirical distributions from Soft-Actor Critic and Twin-Delayed Deep Deterministic Policy Gradient are compared on Half-Cheetah. Guidelines and code for performing these comparisons are also provided. Reproducibility in Machine Learning, especially in Reinforcement Learning (RL), has become a significant issue. Henderson et al. highlighted challenges in reproducing RL results due to differences in codebases, hyperparameters, and random seeds. They emphasized the importance of statistical tests to support claims of algorithm superiority. This paper offers a guide for statistical comparisons of RL algorithms, focusing on their unique characteristics and the need for rigorous evidence. This paper reviews statistical tests for comparing RL algorithms, emphasizing the need for robust difference testing. It compares false positive rates and statistical power, using simulations and empirical distributions from SAC and TD3 on Half-Cheetah. Guidelines for conducting meaningful comparisons are provided, with a repository for raw results and code available. The paper discusses comparing RL algorithms by focusing on central tendencies of their performances, such as means or medians of associated random variables. It highlights the challenge of accurately comparing these central performances using empirical estimates, illustrated with examples of normal distributions and sample collections. The text discusses statistical difference testing to compare central performances of two algorithms based on mean or median values. It introduces null and alternative hypotheses, p-values, and the process of rejecting the null hypothesis when the p-value is low. The p-value indicates the likelihood of a true underlying difference. A low p-value leads to rejecting the null hypothesis (H0), while a high p-value lacks evidence to conclude. The significance level \u03b1 (usually \u2264 0.05) determines rejection of H0. False positives (\u03b1*) and false negatives (\u03b2*) impact the trade-off between statistical power and error rates. The size of the underlying difference affects the risk of false negatives. The statistical power of a test is impacted by the effect size and sample size. Larger effect sizes and sample sizes increase the statistical power, making it easier to detect differences. The sample size is typically chosen to achieve a theoretical statistical power of 0.8. Different tests have varying statistical powers based on their assumptions and how the p-value is derived. Parametric vs. non-parametric tests compare means and medians, respectively. Non-parametric tests are recommended for skewed data or small sample sizes. Test statistics, like t \u03b1 in a t-test, summarize data and follow specific distributions. The p-value in statistical tests follows the Student's distribution with density function. Relative effect size is calculated as the absolute effect size divided by the pooled standard deviation. Assumptions for statistical tests in RL include random and independent performance measurements. Non-parametric tests are recommended for skewed data or small sample sizes. The text discusses assumptions and statistical tests for comparing RL performances, focusing on underlying assumptions and providing implementation from the Python Scipy library. It highlights the limitations of common assumptions such as normal distributions and known standard deviations in RL. The Kolmogorov-Smirnov test is not recommended for comparing RL performances. Statistical tests for comparing RL performances include t-test, Z-test, Welch's t-test, and Wilcoxon Mann-Whitney rank sum test. These tests compare means or medians of two distributions, with assumptions about variances and distribution shapes. Implementation is available in Python Scipy library. The implementation of statistical tests in Python Scipy library includes the Mann-Whitney U test, ranked t-test, and bootstrap confidence interval test. The Mann-Whitney U test compares medians by ranking all realizations together. The bootstrap test approximates the original distribution by sampling with replacement and computing the difference in empirical means. This process is repeated a large number of times to obtain confidence intervals. The 100(1\u2212\u03b1)% confidence interval around the true mean difference \u2206\u00b5 is calculated using \u03b1\u00d7100 2 and 100(1\u2212 \u03b1 2 ). The permutation test compares the empirical mean difference (\u2206x) by performing permutations of realization labels. If the proportion of |\u2206x| that falls below the original difference |\u2206x| is higher than 1\u2212\u03b1, the null hypothesis H0 is rejected. The implementation of these statistical tests can be found at https://github.com/facebookincubator/bootstrapped. The section compares statistical tests in terms of false positive rates and statistical powers. False positive rate impacts research reproducibility, while statistical power refers to finding evidence for an effect. Experiments are conducted using RL distributions and candidate distributions are selected to model RL performance distributions. The study compares statistical tests for false positive rates and statistical powers using various distributions tuned with specific parameters. It includes empirical distributions from SAC and TD3, investigates unequal standard deviations, and measures false positive rates by aligning central performances of distributions. The study compares statistical tests for false positive rates and statistical powers using various distributions tuned with specific parameters. It includes empirical distributions from SAC and TD3, investigates unequal standard deviations, and measures false positive rates by aligning central performances of distributions. The procedure involves sampling x1 and x2 from distributions X1, X2, comparing them using a test with \u03b1 = 0.05, and repeating the process N r = 10^3 times to estimate \u03b1*. The standard error of this estimate is calculated as se(\u03b1*) = (\u03b1*(1\u2212\u03b1*)/N r. The procedure is repeated for every test, combination of distributions, and sample sizes to measure true positive rates. The study compares statistical tests for false positive rates and statistical powers using various distributions tuned with specific parameters. Tables in the supplementary results report statistical powers for different effect sizes, sample sizes, tests, and assumptions. Results show that using bootstrap tests with large sample sizes (>40) leads to very large \u03b1*. Permutation and ranked t-tests also show large \u03b1* with small sample sizes (<5). Results using two log-normal distributions exhibit similar behaviors. Comparing two normal distributions with different standard deviations shows differences in behaviors. Comparing different distributions with equal standard deviations does not significantly impact false positive rates. Mann-Whitney and ranked t-tests consistently overestimate \u03b1* regardless of sample size. For log-normal distributions, false positive rates respect the confidence level with sample sizes over 10. Other tests show large \u03b1* even with large sample sizes. Skewed distributions also affect false positive rates. When comparing different distributions with unequal standard deviations, skewed distributions like log-normal ones lead to high false positive rates for Mann-Whitney and ranked t-tests, especially with larger sample sizes. More than 50 samples are needed to detect a relative effect size of 0.5 with 80% probability. When comparing two RL algorithms (SAC, TD3) on Half-Cheetah, a small increase in false positive rates was observed with the ranked t-test. The relative effect size favored SAC with values of 0.80 (median) and 0.93 (mean). Sample sizes of 10-15 are needed for mean comparisons and 15-20 for median comparisons to achieve a statistical power of 0.8. The bootstrap test should not be used for sample sizes below 50, and the permutation test should not be used for sample sizes below 10. The ranked t-test shows high false positive rates and no statistical power when N = 2. Comparing two samples of size N = 2 can result in unreliable p-values due to quantization issues. Non-parametric tests can also exhibit high false positive rates when comparing different distributions. When comparing a skewed distribution to a symmetric one, the sample size affects the bias in statistical tests. This violates Mann-Whitney assumptions and leads to high false positive rates in non-parametric tests. Comparing two log-normal distributions does not pose the same issues. When comparing distributions, non-parametric tests are more robust than parametric tests when assumptions are violated. T-tests and Welch's t-test are found to be robust, but may have slightly higher error rates with skewed or bimodal distributions. Statistical power remains stable regardless of distributions or tests used. Performance measures should be obtained before conducting statistical tests for RL algorithms. Before using statistical tests, performance measures must be obtained for RL algorithms. Evaluation should be done offline, measuring performance after t steps as the average returns over E evaluation episodes. A learning curve is formed by collecting performance measures, which can be represented on a plot after obtaining curves for N runs. The empirical mean or median can be used at each evaluation to represent the center of the sample. The empirical mean assumes a Gaussian distribution, while the median directly represents the sample center. The standard deviation (SD) represents variability in performances, but is only accurate for normally distributed values. For non-normal distributions, interpercentile ranges are preferred. When sample size is small, showing all learning curves along with mean or median is informative. For normally distributed performances, standard error of the mean (SE) or confidence intervals can be used. Welch's t-test is recommended for robust comparisons due to lower false positive rates and similar statistical power compared to other tests. When comparing algorithms, using lower confidence levels (e.g. \u03b1 = 0.01) is recommended to reduce false positives. The number of random seeds needed for a statistical power of 0.8 depends on the effect size. A real case analysis required a sample size between 10 and 15 for a strong effect. Multiple comparisons increase the probability of false positives linearly. The Family-Wise Error Rate (FWER) increases with the number of comparisons. Corrections like the Bonferroni correction adjust the confidence level to control FWER. Comparing learning curves instead of just final performances can reveal differences in convergence speed. This paper advocates for these corrections to improve statistical comparisons. This paper recommends using Welch's t-test with a low confidence level (\u03b1 < 0.05) to maintain a false positive rate below \u03b1 * < 0.05. It cautions against unreliable tests like the bootstrap test (for N < 50), Mann-Whitney and ranked t-test (without careful assumptions), or permutation test (for N < 10). Small sample sizes (<5) with t-tests or Welch's t-test can lead to high false positive rates, requiring large effect sizes (over = 2) for good statistical power. Sample sizes above N = 20 generally meet the 0.8 statistical power requirement for a relative effect size = 1. Algorithm 1 provides the pseudo-code for the experiment. The full code is available at https://github.com/ccolas/rl_stats. The code at https://github.com/ccolas/rl_stats involves comparing distributions in different settings with various parameters like sample sizes and effect sizes. The rejection of the null hypothesis is determined based on test results, with Welch's t-test recommended for maintaining a low false positive rate. The experiment correction depends on the number of comparisons and other factors. The criteria for comparing learning curves depends on the number of comparisons and the criteria used to determine algorithm superiority. A specific example is provided where Algorithm 1 is considered better than Algorithm 2 if more than 50 out of 100 comparisons show a significant difference. The probability of meeting this criterion by chance is constrained to a confidence level of \u03b1=0.05. To correct for this, \u03b1 is adjusted to \u03b1 corrected = \u03b1/2. The bimodal distribution has two peaks with different standard deviations. One peak is centered at 0, while the other is shifted by the effect size. Both peaks have the same standard deviation. Results show the percentage of true positives over 10,000 repetitions, with bold results indicating a true positive rate above 0.8."
}