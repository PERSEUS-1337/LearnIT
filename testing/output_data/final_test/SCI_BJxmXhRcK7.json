{
    "title": "BJxmXhRcK7",
    "content": "Recent deep multi-task learning has shown success in leveraging domain-specific knowledge from related tasks to address data scarcity. However, key issues such as sharing mechanisms, model complexity, and network flexibility remain unresolved. A novel approach called tensor ring multi-task learning (TRMTL) is proposed to address these challenges. TRMTL offers a compact representation, effective transfer of task-invariant knowledge, and flexibility in learning task-specific features. It can handle tasks with varying input data dimensions and feature sizes at different layers. Our model improves single task performance, especially when tasks have limited data. Multi-task learning boosts overall performance by learning related tasks simultaneously. Deep MTL is popular in various applications like computer vision, natural language processing, and speech recognition. One major challenge in deep multi-task learning is designing effective information sharing mechanisms across tasks to improve parameter sharing patterns in deep networks. Previous work has used hard parameter sharing, but this can be harmful for learning high-level task-specific features as it focuses only on common low-level features. This approach may lead to negative transfer among tasks. The lack of efficiency in model complexity poses a challenge for deep multi-task learning. Soft parameter sharing strategies have been used to link individual DNNs, but they often result in a large number of trainable parameters, making deployment on resource-constrained devices impractical. The MRN, DMTRL, and TRMTL sharing mechanisms for multi-task learning involve different ways of sharing weights between tasks. MRN shares weights at lower layers and models relatedness at top layers with tensor normal priors. DMTRL equalizes layer-wise weights for factor decomposition, with most factors shared except the last 1D vector. TRMTL encodes layer-wise weights separately for tasks, tying a subset of latent cores. Sharing patterns vary across layers. BID28 integrates tensor factorization with deep MTL to address model complexity inefficiency. BID28 proposed deep multi-task representation learning (DMTRL) to address model complexity inefficiency by integrating tensor factorization with deep MTL. However, DMTRL's sharing of layer-wise weights restricts effective knowledge sharing, leading to dominance of common factors and suppression of task-specific variations. In this work, a generalized latent-subspace based solution is proposed to address the challenges of deep multi-task learning (MTL). The approach involves sharing different portions of weights as common knowledge at distinct layers to allow individual tasks to convey their private knowledge effectively. Knowledge is shared in the latent subspace using a tensor ring (TR) representation for efficiency. The approach in this work involves sharing weights in a latent subspace using a tensor ring (TR) representation for efficiency in deep multi-task learning (MTL). TR generalizes other tensor networks like tensor train (TT) in terms of model expressivity power and can approximate tensors using lower ranks, resulting in a more compact model with fewer parameters. Adopting TR with lower ranks can benefit deep MTL by decomposing layer-wise weights into higher-order weight tensors, facilitating sharing of cores and enhancing effectiveness. The TRMTL framework utilizes tensor ring (TR) representation to share weights in a latent subspace for efficient deep multi-task learning. By parameterizing one DNN per task and encoding layer-wise weights using distinct cores, TRMTL maximizes flexibility while considering discrepancies between tasks' features. This approach achieves state-of-the-art performance on various datasets, demonstrating the benefits of the proposed architecture. The proposed architecture for multi-task learning utilizes matrix and tensor factorization to share weights efficiently across tasks. The approach extends from matrix to tensor representations, enhancing the model's capacity to learn task-specific features in deep learning frameworks like CNN. TRMTL extends the DMTRL approach by tensorizing the weight into a higher-order weight tensor before factorizing it, allowing for more latent cores and finer granularity sharing. This results in a more compact model with task-specific information embedded in the weight tensor. TRMTL extends DMTRL by allowing for any sharing pattern at different layers, individual input dimensionality for each task, and generalizing TT to TR-format. It subsumes DMTRL-Tucker in terms of generalizations. Our method incorporates a more general tensor network into the deep MTL context, aiming to alleviate under-transfer and negative-transfer of knowledge among tasks. Unlike MRN, which follows a shared lower layer architecture, our approach allows for more flexibility in task correlation. Constantly updating covariance matrices becomes computationally prohibitive for large scale networks in non-latent-subspace methods. TRMTL is a compact method with fewer parameters compared to non-latent-subspace methods, making it advantageous for tasks with small sample sizes. High-order tensors are multi-way arrays of real numbers, with tensor decompositions successfully applied in imaging analysis and computer vision. The tensor ring decomposition decomposes a tensor into 3rd-order latent cores that are multiplied circularly, with adjacent cores linked by a common dimension. The authors compare Tensor Train (TT) format with Tensor Ring (TR) format, concluding that TR is more flexible for low-rank approximation. TR-ranks are equally distributed on cores, unlike TT where middle cores have much larger ranks. TR has smaller overall ranks and is more compact than TT. TR is invariant under circular dimensional permutation and is used in Tensor Ring Multi-Task Learning (TRMTL) to learn one DNN per task. The text discusses the use of Tensor Ring (TR) format in deep multi-task learning. TR is compared to Tensor Train (TT) format and found to be more flexible for low-rank approximation. TR-ranks are evenly distributed on cores, making it more compact and powerful for representing matrices. In deep multi-task learning, Tensor Ring (TRL) is shown to be more powerful than Tensor Train (TT) in terms of modeling expressivity. TRL offers benefits such as a sparser tensor network format and finer sharing granularity across tasks. Training with TRL involves applying stochastic gradient descent on the cores. TRL is similar to TR-based weight compression for neural networks but uses 4th-order latent cores in TRmatrix. In a CNN setting, TR can be extended to a convolutional kernel. The sharing strategy involves partitioning parameters into task-independent and task-specific TR-cores for each hidden layer of an individual task. Tensor Ring (TRL) is utilized in deep multi-task learning to reformulate layer weights into TR-cores for individual tasks. The input tensor is transformed into an output tensor with a subset of common TR-cores, leaving task-specific TR-cores. The sharing strategy allows for flexibility in choosing shared cores and does not require them to be in a consecutive order. The output tensor is reshaped back into a vector, with the level of sharing, measured by c, varying across layers. TRMTL represents weight elements as a function of shared and private cores. The sharing strategy in Tensor Ring (TRL) for deep multi-task learning involves using a function of sequence product of slice matrices from shared and private cores. This distributed sharing approach is more efficient than conventional methods, as weight elements are influenced by both common and private latent factors. Different cores control image resolution at various scales, with the first core affecting small-scale patches and the last core influencing large-scale partitions. In the current work, a sharing strategy is implemented in Tensor Ring (TRL) for deep multi-task learning. Cores are selected in a left-to-right order based on location, with the option to automatically choose core pairs with high similarity. The TRMTL method is compared with other approaches using the same network architecture, with experiments repeated five times for accuracy evaluation. The sharing involves tensorization for efficiency in weight element influence. In Tensor Ring (TRL) for deep multi-task learning, a sharing strategy is implemented by tensorizing layer-wise weights into a Dth-order tensor. The number of cores needed for sharing, denoted as c, can be tuned via cross validation. To reduce the search space for this hyper-parameter, a greedy search on c layer by layer can be applied. Experiments are conducted on datasets like MNIST LeCun et al. (1998) for classification tasks involving odd and even digits. In CIFAR-10, task A classifies odd digits and task B classifies even digits from 10 object classes. Omniglot contains 1623 characters from 50 alphabets divided into 5 tasks. In the Omniglot-MNIST setting, task A classifies alphabets and task B recognizes digits. Refer to the appendix for more details. In the study, different sharing styles were analyzed to see their impact on performance. Patterns like 'bottom-heavy' and 'top-heavy' were examined using MLP with tensorized hidden layers. Transferability between tasks with unbalanced training samples was evaluated, showing that 'bottom-heavy' patterns achieved better results. The pattern '420' outperformed '044' in terms of accuracy. TRMTL outperforms pattern '044' in Figure 4 by sharing weights at the top layers but not at the bottom layer. TRMTL is robust to small perturbations for pattern selection. '410' and '420' patterns in Table 1 show similar performance. Compared to other models, TRMTL significantly reduces the number of parameters to 13K by tensorization, resulting in a sparser TRL with lower ranks. The study aims to verify the effectiveness of different models in transferring knowledge from data-abundant to data-scarcity tasks, tested on the CIFAR dataset using CNN. TRMTL demonstrates superior performance on CIFAR dataset with limited training samples, outperforming competitors by a large margin. The advantage of TRMTL remains significant in terms of averaged accuracies across tasks. A second test on Omniglot with CNN architecture shows TRMTL's effectiveness in a challenging scenario with data scarcity. Our TRMTL method effectively utilizes knowledge from task C to improve accuracy, especially with limited data. It also shows advantages in handling tasks with different input types. To do this, heterogeneous inputs are converted into equal-sized features using unshared weights in a hidden layer. This allows for better performance compared to competitors using MLP with 4 hidden layers. Our TRMTL method outperforms DMTRL based methods significantly, especially in scenarios where tasks are loosely related. By utilizing three hidden layers with 5 cores each, TRMTL shows its effectiveness in tasks like recognizing character symbols and handwritten digits simultaneously. Task A, which involves character symbols, is particularly challenging due to fewer training samples, but TRMTL still manages to partially share information between tasks at the first layer. TRMTL, a novel knowledge sharing mechanism in deep MTL, outperforms other methods in multi-dataset tasks. It significantly improves task A by 4.2%, 4.9%, and 4.7% compared to STL. TRMTL is compact and flexible, learning task-specific and task-invariant features by sharing common knowledge among tasks. Empirical verification on various datasets shows state-of-the-art results. TRMTL achieves state-of-the-art results in multi-dataset tasks by sharing common knowledge among tasks. It outperforms other methods in improving individual task performance. Additional experiments on CIFAR-10 dataset show the effectiveness of transferring knowledge from data-abundant tasks to data-scarcity tasks. Table 4 shows the results of TRMTL patterns '4431' and '4421' outperforming other methods in various scenarios. TRMTL-4431's precision increases by 1.7% with full task C data and up to 5.5% with full task B and C data. Visualization in FIG3 displays task-specific features of TRMTL using t-SNE for dimensionality reduction. The TRMTL model shows clear clustered features separated for different classes, beneficial for downstream classification tasks. The architecture includes TRL with specific input/output feature modes. The '432' sharing pattern is selected as the best by CV, with TRMTL achieving the best overall performance in both data-rich and data-scarcity situations."
}