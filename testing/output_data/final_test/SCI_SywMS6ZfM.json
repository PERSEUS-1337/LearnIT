{
    "title": "SywMS6ZfM",
    "content": "Modeling hypernymy is crucial for various NLP tasks like entailment and relation extraction. Existing unsupervised methods for hypernym discovery have limitations in scalability and accuracy. A new method called DIVE is introduced in this paper, which uses per-word non-negative vector embeddings to preserve word contexts. Experimental evaluations show that DIVE provides higher precision compared to previous unsupervised methods. Our method, DIVE, offers double the precision of previous unsupervised methods and the highest average performance using compact word representations. The interpretability of each dimension in DIVE allows for a novel approach to word sense disambiguation. Distributional semantics assigns meaning to objects by compactly representing context distributions, benefiting various applications. Word embedding models like word2vec and matrix factorization techniques have successfully compressed bag of words into lower dimensional spaces, improving scalability and preserving geometric embedding similarities. The distributional inclusion hypothesis suggests that a word's context set is a subset of its hypernyms' contexts, leading to more semantic features. For example, adjectives that apply to poodle also apply to dog, as dog is a hypernym of poodle. This broader context understanding enhances the effectiveness of word embeddings. Several methods have been developed for automatic hypernymy detection based on distributional inclusion hypothesis (DIH). Hypernymy detection is crucial for various NLP tasks like textual entailment, coreference, relation extraction, and question answering. Leveraging context distributions can improve the discovery of taxonomic structure among words. Some unsupervised learning approaches like word2vec lack the ability to preserve these features, limiting their semantic representation power. New methods aim to encode hypernym relations in dense embeddings. Recent studies have highlighted the challenges of generalizing supervised hypernymy annotations to unseen pairs, leading to the development of more accurate and scalable unsupervised embeddings for hypernym detection. A novel unsupervised low-dimensional embedding method using non-negative matrix factorization is proposed to model inclusion relations among word contexts. The study proposes a method using non-negative matrix factorization to model relations among word contexts efficiently. It introduces new comparison functions for evaluating unsupervised embeddings and achieves state-of-the-art performance on hypernym detection with less memory and compute. The method, called DIVE, can also be used for word sense disambiguation by efficiently modeling word senses at multiple granularities. The study introduces a method called DIVE using non-negative matrix factorization to efficiently model relations among word contexts. It aims to produce lower-dimensional embeddings that preserve the inclusion property where the embedding of a hypernym is larger than or equal to its hyponym in every dimension. Additional non-negativity constraints are added to increase interpretability of the embeddings. The study introduces DIVE, a method using non-negative matrix factorization to model word context relations efficiently. Popular unsupervised word embedding methods are based on matrix factorization, computing co-occurrence statistics between words and context words. Most unsupervised hypernymy detection studies use SBOW due to its accuracy, while Gaussian embeddings have shown degraded accuracy. The matrix M is factorized to approximate w T c, where w and c are word embeddings. The statistic in M is typically based on pointwise mutual information. Different variations like PPMI or shifted PMI can be used. Larger embedding values of w imply larger co-occurrence counts, but there are flaws in the derivation. The novel word embedding method, distributional inclusion vector embedding (DIVE), addresses flaws in the derivation by performing non-negative factorization on the matrix M. It encourages the inclusion property to be satisfied, ensuring that the context vector c is non-negative and the embedding of hypernym y is greater than or equal to the embedding of its hyponym x in every dimension. During training, DIVE optimizes embeddings using skip-gram with negative sampling (SGNS). SGNS is equivalent to factorizing a shifted PMI matrix and applying non-negativity constraints to embeddings. The objective function is optimized using ADAM, with non-negativity enforced through projection. The optimization process in DIVE involves preserving DIH by adjusting word embeddings through gradient ascent and negative sampling. The inclusion property in DIVE is approximately equivalent to DIH, with frequent target words having many embeddings. In DIVE, the optimization process involves adjusting word embeddings to preserve DIH by filtering out context words with low PMI co-occurrences. This helps in reducing noise in word and context vectors, leading to a more intuitive interpretation of word embeddings as representing topics. The word embeddings in DIVE can be viewed as unnormalized probability distributions over topics. By removing normalization of target word frequency in the shifted PMI matrix, specific words have values in few dimensions, while general words appear in more topics. Comparing the concreteness level of two words can be done by looking at the magnitude of their embeddings. Each dimension of the learned embeddings roughly corresponds to a topic, with more general words having higher values in the corresponding dimension. The embedding in DIVE captures common contexts where words appear in different topics. Four experiments are described, comparing DIVE with other unsupervised embeddings and SBOW using hypernymy scoring functions. The last experiment focuses on word sense disambiguation, testing on 11 datasets from recent reviews. The performance of various word embeddings is evaluated on 11 datasets, including BLESS, EVALution, Lenci/Benotto, Weeds, Medical, LEDS, TM14, Kotlerman, HyperNet, WordNet, and HyperLex. Average precision AP@all is used as the main evaluation metric, with Spearman rank coefficient \u03c1 adopted for the HyperLex dataset. Out-of-vocabulary words in testing data are handled accordingly. The study evaluates word embeddings on various datasets using WaCkypedia corpus for training. Out-of-vocabulary words are handled by pushing them to the bottom of the prediction list. Stop words and rare words are removed during preprocessing. The number of embedding dimensions is set to 100, with other hyper-parameters listed in the supplementary materials. The hyper-parameters of DIVE were determined based on HyperNet training set performance. To train embeddings more efficiently, the corpus was chunked into subsets of 100 tokens. Training SBOW and DIVE on the first 512,000 lines yielded better performance than training on the whole WaCkypedia or randomly sampled lines. This may be due to categorical words occurring more frequently in the first 51.2 million tokens. The scoring function for hypernym candidates is based on cosine similarity and difference of summation. Gaussian embedding is the only unsupervised method capturing asymmetric relations between hypernyms and hyponyms. Gaussian embedding was trained on 51.2 million tokens and tested on 11 datasets. Hyper-parameters were determined to maximize AP on HyperNet. DIVE is compared with GE 2 in Experiment 1, showing better performance. A scoring function (C\u00b7\u2206S) accurately detects hypernymy using DIVE's embedding space. Various scoring functions are tested to select representative ones for evaluating DIVE on hypernymy detection tasks. Embedding/context vectors for hypernym and hyponym candidates are denoted as w p and w q. The SBOW model represents words by neighboring word frequency (SBOW Freq) or PPMI features (SBOW PPMI). Cosine similarity is used to measure similarity between hypernym and hyponym word vectors. Different scoring functions like Cosine and Word2vec are compared. The distributional informativeness hypothesis suggests using entropy of context distributions to capture context diversity. The approach proposed by BID38 uses entropy of context distributions to capture context diversity. Two variations of the approach are SLQS Row (\u2206E) and SLQS Sub. The generality of a word is measured by its L1 and L2 norms. Two scoring functions are proposed: difference of vector summation and difference of vector 2-norm. The combination of similarity functions (Cosine and Word2vec) and generality functions (difference of entropy, summation, and 2-norm of vectors) leads to six scoring functions. These scoring functions measure inclusion properties of SBOW based on DIH. Weeds Precision BID49 and CDE BID7 measure the intersection between feature vectors. The scoring functions BID7 measure the intersection between feature vectors by calculating the elementwise minimum in CDE. Three functions, including invCL BID17, are chosen for their ability to detect hypernymy well. It is difficult to confirm if their good performances are solely due to the inclusion property between context distributions. The AL 1 distance with different constant values is evaluated for hypernymy detection. A constant of 5 is found to produce the best results. Combination functions like C\u00b7\u2206S and W\u00b7\u2206S perform well overall. Among inclusion-based scoring functions, CDE is the most effective. AL 1 shows good performance compared to other functions like Word2vec and Cosine. DIVE with C\u00b7\u2206S and W\u00b7\u2206S scoring functions is compared in TAB4. In TAB4, DIVE is compared with previous approaches using different datasets. Performance gaps may be due to various factors, including the effectiveness of DIVE and proposed scoring functions. Different training corpora used in each paper also impact performance. A more comprehensive comparison is conducted to isolate factors. The experiment examines if DIVE preserves signals for hypernymy detection tasks using the same scoring functions designed for SBOW. Summation difference (\u2206S) and CDE perform best among generality. In TAB4, DIVE is compared with previous approaches using different datasets, with a focus on hypernymy detection tasks. The best performing scoring functions are Summation difference (\u2206S) and CDE. Additionally, comparisons are made with other baselines such as Cosine similarity, balAPinc, SLQS, and Freq ratio. The results are presented using these scoring functions, along with W\u00b7\u2206S and C\u00b7\u2206S, in addition to classic representations like SBOW Freq and SBOW PPMI. The method involves DIVE without the PMI filter, NMF on shifted PMI without frequency weighting, and K-means clustering of words in skip-gram embedding space into topics. This approach can be seen as a form of NMF approximation. The orthonormal G is an approximated solution of NMF. Hashing context vectors into topic vectors can be written as a constant log(k) shifting to SBOW PPMI. Performance degrades as k increases. Different hypernymy scoring functions do not always outperform others due to variations in datasets. The evaluation of different scoring functions on various datasets shows that negative samples from random word pairs favor symmetric similarity measures, while negative samples from related words benefit from computing generality difference. DIVE consistently outperforms SBOW across all datasets, with superior results on combination scoring functions and AL 1 compared to SBOW PPMI. The combination scoring functions outperform generality functions, with W\u00b7\u2206S producing the best average performance on 4 and 10 datasets in DIVE. SBOW PPMI improves combination functions but sacrifices AP on inclusion functions. Changing frequency sampling or computing SBOW PPMI on the whole WaCkypedia can hurt performance. AL 1 fails in HyperLex dataset using SBOW PPMI, suggesting PPMI may not preserve distributional inclusion property. K-means (Freq NMF) has similar AP to SBOW Freq but worse AL 1 scores. DIVE outperforms other methods in AP scores on different datasets. BID0 shows Gaussian mixtures can discover multiple word senses. DIVE achieves similar results without fixing the number of senses beforehand. Each dimension in DIVE corresponds to a topic, with higher values indicating higher likelihood of word occurrence in that topic context. In word sense disambiguation tasks, determining the number of senses for each word is challenging. Existing approaches fix the number of senses before training the embedding, but this may not capture the diversity of contexts. Spectral clustering is used to group topics based on embedding values, allowing for the discovery of senses in polysemy. The training process may not capture different granularity of senses, such as in the example of \"race in the car.\" The training process may not capture different granularity of senses, leading to the need for re-training to capture such nuances. Clustering dimensions in DIVE are adjusted after training, allowing for efficient changes in cluster numbers. Hierarchical clustering is also an option. Similar to BID31, word senses are discovered through graph-based clustering, with the main difference being the clustering of top n words related to the query word instead of topics. The hyper-parameter n in graph clustering algorithms is challenging to choose, as a large n can make the algorithm inefficient while a small n may hinder the discovery of less frequent senses. Previous unsupervised approaches have focused on improving hypernymy scoring functions for sparse bag of word (SBOW) features, as summarized in BID38. BID38 also explores different contexts and their influence but overlooks scalability issues. The Gaussian embedding model BID45 encodes the context distribution of each word as a multivariate Gaussian distribution, with hypernyms having higher variance and overlapping with hyponyms. However, retaining frequency information during the embedding process is difficult due to the normalization of the Gaussian distribution. Order embedding (BID44) is a supervised approach to encode hypernym pairs into a compact embedding space, where the hypernym embedding should be smaller than the hyponym embedding in every dimension. The method learns embeddings from raw text, with hypernym embeddings larger than hyponym embeddings. Experiments show that word frequency alone can achieve good results in hypernym detection tasks. Images from search engines can help determine the generality of lexicons, but may not be available for all corpora like scientific literature. DIVE is an unsupervised order embedding method where hypernym embeddings are larger than hyponym embeddings in every dimension. HyperScore BID27 has a similar goal but relies on annotated hypernym pairs for training. Previous work leverages training data to find text patterns indicating is-a relations, but increasing recall of hypernym detection remains challenging. In NLP, Non-negative matrix factorization (NMF) has a history in constructing topic models. Non-negative sparse embedding (NNSE) suggests non-negativity improves word similarity evaluations. A new type of NMF is proposed for unsupervised hypernymy detection, showing state-of-the-art performance. Simple baselines like K-mean clusters and non-negative skip-grams do not perform well in this task. In NLP, non-negative matrix factorization (NMF) has a history in constructing topic models. A new method called distributional inclusion vector embedding (DIVE) using NMF on a weighted PMI matrix shows improved performance in hypernymy detection compared to traditional methods. Scoring functions measuring inclusion and generality properties in SBOW can also be applied to DIVE, which outperforms SBOW with fewer dimensions. Unsupervised scoring functions combining similarity and generality measurements work best overall, with a combination of unsupervised DIVE and these scoring functions achieving state-of-the-art performance on various datasets. DIVE, an unsupervised method, shows that clusters of topics correspond to word senses, aiding in word sense disambiguation without prior knowledge of senses. Comparing DIVE with semi-supervised approaches, it is noted that semi-supervised methods perform better with sufficient training data. However, in domains like scientific literature with limited annotated data, unsupervised methods like DIVE are valuable. In TAB10, DIVE's performance is comparable to previous semi-supervised approaches on limited training data, showcasing its robustness. TAB11 displays the most general words in DIVE under different queries and the accuracy of predicting hypernyms. Summation difference method performs better than SQLS Sub, and DIVE predicts directionality accurately. In HyperNet and WordNet, hypernym relations are determined between phrases instead of words. Phrase embeddings are composed by averaging word embeddings or SBOW features. The window size of SBOW, DIVE, and GE is set as 20. DIVE's hyper-parameters were decided based on the performance of HyperNet. The hyper-parameters of DIVE and Gaussian embedding (GE) were determined based on the performance of HyperNet training set. Skip-gram parameters include a window size of 10 and 5 negative samples. Gaussian embedding has 1 mixture, 100 dimensions, and specific variance and mean values. Phrase scores are calculated using the average score of token pairs. Testing pairs and OOV word pairs are specified in TAB1. Spectral clustering library in Scikit-learn 0 is used with default hyper-parameters. When using the spectral clustering library in Scikit-learn 0, default hyper-parameters are typically used. However, clustering based on global features may group topics together regardless of query words. To address this, a query-dependent similarity measurement is created by considering only the embedding of words related to the query word. This approach allows for focusing on specific meanings of words based on the query, such as geographical or economic interpretations. The feature vector of dimensions is defined based on the query word, considering top words in each dimension. The features are weighted by the likelihood of the query word in that dimension and concatenated. After normalization, pairwise similarity is computed for spectral clustering. To achieve norm 1, calculate pairwise similarity and apply spectral clustering for clustering results. An efficient method to compute asymmetric L1 involves converting the problem into a linear programming problem by introducing slack variables \u03b6 and \u03be."
}