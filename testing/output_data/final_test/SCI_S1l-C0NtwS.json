{
    "title": "S1l-C0NtwS",
    "content": "Learning multilingual representations of text has proven successful for cross-lingual transfer learning tasks. Two main paradigms for learning such representations are alignment, which maps independently trained monolingual representations into a shared space, and joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives. Comparisons of representations learned using both methods across diverse cross-lingual tasks reveal pros and cons for each, with performance being task-dependent. A novel framework combining alignment and joint training approaches is proposed, showing improved results in various tasks. The proposed framework combines alignment and joint training approaches for learning multilingual representations, showing improved results in various tasks like MUSE bilingual lexicon induction and CoNLL cross-lingual NER benchmarks. This framework outperforms existing methods and achieves state-of-the-art results in tasks like text classification, dependency parsing, named entity recognition, and machine translation using continuous word representations. Machine translation methods aim to learn embeddings in a shared vector space for multiple languages through cross-lingual alignment and joint training paradigms. The cross-lingual embedding alignment method has been successful by training embeddings independently in different languages and then aligning them to a shared vector space using supervised or unsupervised methods. Recent research in Cross-Lingual Word Embeddings (CLWE) explores alignment methods and joint training approaches for learning embeddings in a shared vector space across multiple languages. These methods can be supervised or unsupervised, utilizing bilingual dictionaries, parallel corpora, or contextualized word representations for effective cross-lingual learning. Naive unsupervised joint training approach has gained attention for its simplicity and effectiveness in training embeddings on concatenated monolingual corpora of related languages using a shared vocabulary. This method has shown higher accuracy in unsupervised machine translation tasks compared to more complex alignment methods. Unsupervised multilingual language model pretraining with a shared vocabulary has also achieved state-of-the-art results for contextualized representations. Previous research has not systematically compared alignment and joint training methods or analyzed their advantages and disadvantages. It is important to understand when one method may be preferred over the other. In this work, the authors aim to evaluate alignment and joint training methods across various cross-lingual tasks such as BLI, cross-lingual NER, and unsupervised MT. They propose a new framework that combines unsupervised joint training as initialization and alignment as refinement. The experiments conducted demonstrate the effectiveness of this approach. The framework proposed combines unsupervised joint training as initialization and alignment as refinement, improving over alignment and joint training baselines. It outperforms existing methods on the MUSE BLI benchmark and produces state-of-the-art results on the CoNLL cross-lingual NER benchmark. This is the first framework to combine previously mutually-exclusive alignment and joint training methods. The alignment methods involve training embedding sets for different languages independently, obtaining a seed dictionary, and learning a projection matrix to improve cross-lingual transfer. Researchers have proposed optimizing the projection matrix to find the optimal solution, with further enhancements achieved by restricting the matrix to be orthogonal. Researchers have proposed different methods for training embedding sets for different languages independently and learning a projection matrix to improve cross-lingual transfer. Most methods involve solving the Procrustes problem with a closed form solution and using joint training with monolingual objectives and a cross-lingual regularization term. Recent studies have shown that unsupervised joint training can be effective for cross-lingual tasks without parallel resources. The shared words between languages can serve as implicit translations by sharing embeddings, ensuring representations lie in a shared space. This approach does not require direct cross-lingual supervision and can achieve competitive results. Some alignment methods assume isomorphism of monolingual embedding spaces, but recent studies challenge this assumption. Unsupervised joint training is simpler and avoids these limitations but assumes shared words serve as translations implicitly. The unsupervised joint training method assumes shared words between languages serve as translations, leading to misalignment issues like oversharing. This problem can be mitigated through vocabulary reallocation and alignment refinement steps. The unsupervised joint training method has issues with oversharing and lacks a seed dictionary, affecting alignment accuracy. A proposed unified framework combines unsupervised training for initialization and alignment methods for refinement to address these limitations. The proposed framework combines unsupervised joint training for initialization and vocabulary reallocation to address oversharing issues in alignment methods. This approach aims to improve the quality of embeddings by maximizing sharing across two languages. The proposed framework combines unsupervised joint training for initialization and vocabulary reallocation to address oversharing issues in alignment methods. This involves reallocating words from the shared vocabulary to language-specific vocabularies based on their frequency in each language. The goal is to better align embeddings across languages without the need for dictionaries. The unsupervised joint training method does not explicitly use dictionaries or alignment. The resulting embedding set is coarse and ill-aligned in the shared vector space. A final refinement step involves using an off-the-shelf alignment method to refine alignments across non-sharing embedding sets. This step can be conducted by supervised or unsupervised methods. The framework is generic and can be applied to contextualized word representations by aligning the fixed outputs of a multilingual encoder like M-BERT. Vocab reallocation is not necessary for contextualized representations, but alignment can still be applied. Alignment refinement on contextualized representations can be achieved by using word alignment pairs from parallel corpora as a dictionary to learn an alignment matrix W. This method is applicable to fixed representations but not finetuning. The proposed approach is evaluated and compared with alignment and joint training methods on three NLP benchmarks to systematically compare their pros and cons. The proposed framework aims to alleviate limitations of alignment and joint training paradigms in cross-lingual tasks. It is evaluated on Bilingual Lexicon Induction (BLI) and Name Entity Recognition (NER) tasks, showcasing effectiveness in both non-contextualized and contextualized settings. The study evaluates word representations for NER tasks on CoNLL benchmarks with 4 European languages. Zero-shot cross-lingual classification is used to measure CLWE quality. Unsupervised Machine Translation is tested, emphasizing the importance of CLWE initialization. The study evaluates word representations for NER tasks on CoNLL benchmarks with 4 European languages. Zero-shot cross-lingual classification is used to measure CLWE quality. The initialization of CLWE plays a crucial role in comparing with similar studies. English-French and English-German language pairs are considered, evaluating on WMT'14 en-fr and WMT'16 en-de benchmarks. The framework is compared to recent state-of-the-art methods for the BLI task. Different tools like MUSE, GeoMM, and RCSLS are used to obtain missing results. Various methods, including supervised joint training and unsupervised joint training with fastText word vectors, are explored. For alignment refinement, RCSLS and GeoMM are used in the proposed framework, along with MUSE for unsupervised methods. The top 200k most frequent words are considered, and CSLS is used as the retrieval criteria. A new retrieval method based on MT systems has high computational costs and is not within the scope of this work. For the NER task, non-contextualized representations are trained similarly to the BLI task, using a vanilla Bi-LSTM-CRF model. The supervised Procrustes method is applied for all alignment steps using dictionaries from the MUSE library for simplicity. The framework utilizes supervised Procrustes method with MUSE dictionaries for alignment. Contextualized representations are tested on M-BERT and XLM models. Alignment refinement is applied to M-BERT. Comparison is made with fine-tuning and feature extraction methods. Task-specific model includes 2 Bi-LSTM layers and a CRF layer. Alignment of contextualized embeddings is done using parallel sentences from Europarl corpus. Word alignments are used instead of BPE for learning alignment matrix. Sum of last 4 layer outputs is used as extracted features. The alignment matrix is utilized with the sum of the last 4 layer outputs as extracted features. Different embeddings are used for the UMT task. Alignment methods outperform joint training significantly in all language pairs for BLI and NER tasks. Unsupervised joint training is superior to alignment for unsupervised MT task. The unsupervised MT task shows different performance compared to its alignment counterpart. Further analysis reveals three limitations, including the lack of fine-grained seed dictionary. Unsupervised joint training fails to generate high-quality alignments, with significantly lower scores compared to alignment methods on BLI and NER tasks. The PCA visualization also indicates poorly aligned embeddings for non-sharing parts. The alignment method outperforms unsupervised joint training in the first few epochs on the MT task, but is surpassed by joint training in later epochs. Parameter sharing is crucial for better performance, as shared words can serve as cross-lingual constraints. Oversharing, however, can be sub-optimal. Supervised joint training method performs poorly on the MT task even with supervision. Our proposed framework improves performance by reallocating vocabulary in unsupervised joint training, overcoming limitations of alignment and joint training methods. It outperforms existing methods on BLI and NER tasks for all language pairs, achieving state-of-the-art results on 2 out of 3 language pairs for the NER task. The proposed framework improves alignment and joint training methods, achieving comparable results in unsupervised cases and outperforming previous methods in supervised settings. It generates well-aligned embeddings and improves accuracy on words not shared between languages. The effectiveness of the vocabulary reallocation technique is demonstrated in the ablation study. The vocabulary reallocation technique significantly boosts performance in joint training, addressing the issue of oversharing. It is crucial to detect what to share for better cross-lingual transfer. The proposed framework outperforms previous methods in machine translation tasks, achieving a maximum gain of 2.97 BLEU over baselines. The proposed framework, Joint Align, outperforms baselines in crosslingual NER tasks, achieving state-of-the-art results on 2 out of 3 languages. It shows effectiveness in generalizing to contextualized representations and surpasses M-BERT feature extraction and finetuning. XLM performs worse than M-BERT due to the uncased vocabulary used, where casing information is crucial for NER tasks. Casing information is crucial for NER tasks. Word embeddings are essential for success in monolingual NLP tasks, but using language-specific embeddings may hinder cross-lingual transfer. Various methods rely on cross-lingual supervisions like bilingual dictionaries, sentence-aligned corpora, and document-aligned corpora to capture cross-lingual mapping. The focus is on observing improvements in embedding initialization rather than outperforming state-of-the-art methods. Unsupervised alignment methods aim to eliminate the need for cross-lingual supervision. Different approaches, such as matching mean and standard deviation of embedding spaces, using generative adversarial networks (GAN), and introducing Sinkhorn distance or latent variables, have been developed to improve alignment methods. These methods train word embeddings independently on different languages or jointly for better stability and robustness. Unsupervised alignment methods train word embeddings independently or jointly on different languages. Various approaches include bilingual dictionary-based regularization, cross-lingual regularization with parallel corpus, and pseudo-bilingual corpus creation. Recent methods involve unsupervised joint training of contextualized word embeddings through multilingual language model pretraining. The use of multilingual language model pretraining for word embeddings has shown state-of-the-art results on benchmarks. A comparison between alignment and joint methods in bilingual lexicon induction tasks has been conducted, with a proposal for a novel training framework that has been empirically verified for effectiveness. The paper systematically compares alignment and joint training methods for Cross-Lingual Word Embeddings (CLWE). The paper systematically compares alignment and joint training methods for Cross-Lingual Word Embeddings (CLWE). A simple hybrid framework is proposed to combine the strengths of both methods, leading to significantly better performance in BLI, MT, and NER tasks. This work opens a new direction for research by combining previously exclusive lines of research. Future work could focus on finding a more optimal word sharing strategy."
}