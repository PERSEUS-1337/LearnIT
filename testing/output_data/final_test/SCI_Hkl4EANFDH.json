{
    "title": "Hkl4EANFDH",
    "content": "Regularization-based continual learning prevents catastrophic forgetting by adding an auxiliary objective to the training loss. However, in practical optimization scenarios with noisy data and gradients, stochastic gradient descent may unintentionally alter critical parameters. In this paper, a new co-natural gradient update rule is proposed for continual learning to reduce forgetting and combat overfitting. Endowing machine learning models with the capability to learn tasks sequentially is crucial for versatility and persistence. Techniques to mitigate catastrophic forgetting are essential in continual learning. Techniques to mitigate catastrophic forgetting in continual learning can be categorized into regularization-based, dynamic architectures, and memory-based approaches. Regularization-based methods are appealing as they do not increase model size or require access to past data, making them practical for real-world scenarios. Regularization-based methods in continual learning aim to prevent catastrophic forgetting by ensuring gradient descent converges to parameters that perform well on new tasks while maintaining performance on previous tasks. Optimization of the regularized objective is crucial, especially in non-convex scenarios with noisy data or gradients. This biological-inspired approach of synaptic consolidation is of independent intellectual interest due to its practicality and privacy considerations. The paper demonstrates the issue of unintended catastrophic forgetting in optimization paths, particularly in non-deterministic scenarios. It compares standard finetuning, EWC, and co-natural finetuning using a two-parameter model trained on tasks T1 and T2. The co-natural optimization trajectory consistently converges towards a better solution compared to plain finetuning and EWC. The co-natural optimization trajectory converges towards the optimum with lowest loss for T1 by preconditioning gradient descent with the empirical Fisher information of previously learned tasks. This approach, known as co-natural gradient, changes the loss on T1 more slowly and leads to better convergence compared to standard finetuning and EWC. The co-natural gradient approach reduces forgetting in continual learning scenarios and improves performance without modifying the training objective. It also shows benefits in transfer learning for two-task, low-resource scenarios by optimizing the trajectory for better trade-offs between source and target domain performance. In continual learning, tasks are defined as triplets with input and output spaces, and a distribution. A model is trained to approximate the conditional distribution induced by the distribution. The goal is to learn a second target task in a multitask setting, where input/output spaces may differ. Various methods can address the discrepancy between input/output spaces. In continual learning, tasks involve input/output spaces and distributions. Various methods can address the discrepancy between these spaces, such as adding task-specific parameters. The objective during training is to minimize the loss function by updating parameters incrementally. This paper focuses on models with a fixed architecture for continual learning, split into regularization-based approaches and fixed capacity models. Regularization techniques in continual learning involve using penalties in the loss function to push weights back towards a target, with a regularization strength hyper-parameter \u03bb. Different methods for encoding parameter importance have been proposed, such as the diagonal empirical Fisher information matrix or pathintegral based measures. More complex regularizers include Bayesian formulations or distillation terms. These approaches do not require access to previous task data, unlike memory-based methods that store and reuse data from past tasks. Regularization techniques in continual learning involve using penalties in the loss function to push weights back towards a target. Various methods have been proposed for sample selection and retrieval in memory-based methods for replay. Stochastic gradient descent is commonly used for optimization, except for GEM, which projects gradients onto the orthogonal complement of previous task's gradients. However, GEM has shown poor performance compared to simple replay. In continual learning, traditional gradient descent methods like GEM perform poorly compared to simple replay. A new update method is proposed to better preserve the model's distribution over previous tasks by minimizing the Lagrangian with a Lagrangian multiplier. The focus is on preserving the probability distribution in a continual learning setting. In continual learning, a new update method is proposed to preserve the model's distribution over previous tasks by minimizing the Lagrangian with a Lagrangian multiplier. The focus is on preserving the probability distribution in a learning setting by incorporating the Kullback-Leibler divergence term. In continual learning, a new update method is proposed to preserve the model's distribution over previous tasks by minimizing the Lagrangian with a Lagrangian multiplier. The focus is on incorporating the Kullback-Leibler divergence term to maintain the probability distribution. A second order Taylor approximation of the function is used to compute the Fisher information matrix, which helps in optimizing the update with hyper-parameters like learning rate and damping coefficient. To prevent large updates, the delta is re-normalized to match the original gradient's norm. The Fisher information matrix is approximated and maintained at \u03b8 S to reduce computational burden. The diagonal approximation simplifies storage and inversion operations. The expected log-likelihood of the true distribution is used in place of the model's distribution expectation. The diagonal of the empirical Fisher can be computed using Monte Carlo sampling, with similarities to the natural gradient but with a crucial difference in execution and purpose. The new update is referred to as the co-natural gradient, leveraging the curvature of the KL divergence on D S to slow down divergence from p S \u03b8 S. In a continual learning scenario, the new update, known as the co-natural gradient, adjusts the Lagrangian to incorporate constraints for all previous tasks. Setting equal importance to all tasks is suboptimal due to model capacity limitations and changing approximations. To address this, a rolling exponential average of Fisher matrices is maintained. In a continual learning scenario, the co-natural gradient adjusts the Lagrangian to incorporate constraints for previous tasks. A rolling exponential average of Fisher matrices is used to control optimization trajectory and reduce catastrophic forgetting. Experiments are conducted on Split CIFAR and Omniglot datasets to validate the hypothesis. In a continual learning scenario, the co-natural gradient adjusts the Lagrangian to incorporate constraints for previous tasks. Experiments are conducted on Split CIFAR and Omniglot datasets to validate the hypothesis. Datasets are split into separate tasks, each with its own architecture and classification setup. Validation sets are created for hyper-parameter selection, with the remaining tasks used for evaluation. Separate softmax layers are trained for each task, with continual learning applied to the \"feature-extraction\" part. The study applies continual learning to the \"feature-extraction\" part of the model for each task. Results are reported using average accuracy and forgetting metrics. Three baselines are compared: Finetuning, EWC, and the co-natural update rule. The study compares three baselines for continual learning: Finetuning, EWC, and ER. ER utilizes experience replay with episodic memory and has shown strong performance on various benchmarks. Training involves using a memory size of 1,000 with reservoir sampling. Training involves using a memory size of 1,000 with reservoir sampling. Hyper-parameter combinations are exhaustively searched and evaluated based on accuracy. Best hyper-parameters are selected for continual training on evaluation tasks. Results are reported over 5 random restarts, showing that co-natural gradient consistently outperforms finetuning, EWC, and ER. The co-natural finetuning method outperforms EWC and ER, requiring fewer resources and reducing forgetting in evaluation tasks. Visualization of accuracy curves shows improved stability over time compared to previous approaches. The co-natural finetuning method shows improved stability over time compared to previous approaches, reducing forgetting in evaluation tasks. In models of fixed capacity, there is a trade-off between intransigence and forgetting. The co-natural gradient is positioned as a low-forgetting/moderate intransigence basis for future work, especially in adapting a model to a new task with minimal data. Using the co-natural gradient during adaptation helps preserve source task performance and improve overall target task performance. Experiments were conducted on image classification with MiniImagenet as the source task and CUB as the target task, as well as machine translation adaptation. The method shows improved stability over time and reduces forgetting in evaluation tasks. Machine translation adaptation from English to French is conducted using a Transformer model pretrained on WMT15 and adapted to MTNT. Different methods such as Finetuning, Co-natural finetuning, EWC, and Co-natural EWC are compared for domain adaptation. Scores are reported in terms of BLEU score, with a low-resource setting simulated by using a sub-sample of 1000 sentence pairs for training. No access to data from the source task is allowed during training on the target task. The adaptation results for different methods in machine translation show that Co-natural gradient helps preserve source task performance and even improves performance on the target task. This supports the hypothesis that introducing a regularizer is beneficial. The co-natural gradient technique helps counteract overfitting and catastrophic forgetting in models trained in a continual setting. It efficiently complements existing techniques and can serve as a foundation for agents that learn without forgetting. In a continual setting, the dataset is split into 20 sub-tasks with 5 classes each. Training is done for one epoch on each task with a batch size of 10. Each alphabet is treated as a separate task, with characters distributed in training, validation, and test sets. Training continues for 2500 steps with a batch size of 32. Validation data is not considered. Full details can be found in the provided code. For each method, grid-search is performed over parameter values including learning rate, regularization strength, and Fisher damping coefficient. The diagonal Fisher is re-normalized to make hyper-parameter choice less dependent on model size. This allows for grid-search over smaller regularization parameters for EWC."
}