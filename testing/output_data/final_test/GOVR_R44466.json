{
    "title": "R44466",
    "content": "Many analysts and officials believe that the development of lethal autonomous weapon systems (LAWS) is crucial at this time. Autonomous weaponry may be key to the Department of Defense's plans for U.S. combat advantage, but it also raises concerns about risks, legality, and morality. Congress has a significant role in shaping the future of these capabilities and policies. Questions surrounding the research, development, and deployment of lethal autonomous weapon systems (LAWS) have been controversial for years. Congress may need to address these issues due to the DOD's \"third offset\" strategy, which aims to maintain U.S. combat advantage through future technologies like robotics and autonomous systems. This nuanced understanding can help Congress regulate the manning, funding, and equipping of U.S. military forces. The DOD highlights autonomous weapon systems as part of future military efforts. Congress sets legal standards for U.S. forces during armed conflict. The use of LAWS raises moral, ethical, and strategic concerns. The DOD regulates LAWS development through DOD Directive 3000.09. The DOD regulates the development and deployment of autonomous weapon systems through DOD Directive 3000.09. Some analysts view this directive as the de facto policy of the United States on this controversial topic. Congress plays a crucial role in U.S. participation in international bodies and agreements related to LAWS, such as the Convention on Certain Conventional Weapons. Numerous international NGOs advocate for multinational regulation or prohibition of LAWS through the CCW. Senate approval would be needed for any potential international treaty on the subject. A proposal to ban research, development, and deployment of fully autonomous weapon systems is often discussed as a way to restrict their use and development. Opponents argue against a ban on fully autonomous weapon systems, stating it would be undesirable and ineffective due to the potential for compliant systems and rapid civilian technology development. Peers may not agree to a ban, enforcement would be challenging, and ambiguity in defining \"fully autonomous\" could hinder its effectiveness. Proponents and opponents debate the effectiveness of a ban on fully autonomous weapon systems. Some suggest regulating the technology's development and deployment through international bodies and binding agreements. In the US, the Department of Defense Directive 3000.09 is the only current regulatory document for autonomous weapon systems. The Department of Defense Directive 3000.09 is praised for providing a framework for testing and guiding principles for autonomous weapon systems. International regulation should focus on transparency in LOAC compliance-testing methods to ensure thorough testing and minimize unexpected decisions. Both opponents and supporters agree that regulation is necessary, with Congress likely playing a central role. Senate approval would be required for any international treaty on banning or regulating lethal autonomous weapon systems. Congress could set legal bounds for researching, developing, and deploying autonomous weapons within the DOD, even without international action. While the DOD Directive provides standards for review, Congress could establish additional regulations. The current weapons review process may not be sufficient for the complexity of autonomous weapons, creating opportunities for congressional oversight. The complexity of autonomous weapons requires a more nuanced and transparent review process. Legal analysis must consider the nature and reliability of an autonomous system's behavior, which may necessitate technical insight or simulation capabilities. Standardization, centralization, and additional resources may be needed for weapons review in emerging technology areas. Congressional reporting and oversight at intermediate stages of acquisition and legal review could be necessary for these high-profile weapon systems. The development and use of autonomous weapons may present regulatory issues for Congress, such as assisting in development or preventing proliferation. Congressional budgetary actions will also influence military development. Definitions and taxonomy of autonomous weapon systems vary among authors. The development and use of autonomous weapons may present regulatory issues for Congress, such as assisting in development or preventing proliferation. Definitions and taxonomy of autonomous weapon systems vary among authors, acknowledging a continuum of autonomy based on target specificity and execution flexibility. Autonomy of a weapon system varies by conflict, mission, or individual objective, occupying a range within the continuum determined by its potential uses. The discussion on the autonomy of weapon systems often focuses on how they operate autonomously. Lawmakers and regulators should define acceptable conditions of use across all devices, rather than evaluating autonomy on a per platform basis. Military systems vary in their level of automation and autonomy, including technologies like automated drone flight and selection of high-interest imagery. This report does not cover force-multiplication technologies beyond lethal autonomy. The discussion on autonomy of weapon systems focuses on various capabilities, including automated selection of high-interest imagery and self-driving vehicles. Autonomous systems like the Joint Light Tactical Vehicle and Phalanx defense system demonstrate technological advancements with varying levels of autonomy and lethality. These systems lay the groundwork for more controversial lethal autonomous weapons. The discussion on autonomy of weapon systems includes ship and land-based defense systems capable of initiating lethal responses without human involvement. These systems operate too quickly for human operators to be directly involved in targeting and firing decisions. Despite concerns about lack of human control, these systems are often exempt from restrictive regulations. Some weapon systems are granted exemptions from regulations or considered non-autonomous precursors due to their high target specificity and lack of execution flexibility. Critics may refer to them as \"automated\" rather than autonomous. Other weapon systems incorporate some autonomy with high target specificity. Some weapon systems, like encapsulated torpedoes, have specific targeting and may raise controversy. These systems target a designated individual or group at the time of weapon initiation. The Israeli Harpy system is an autonomous aerial drone that targets and strikes hostile radar sources without specific human designation. It exhibits behavior that could be considered autonomous under prevailing standards. The use of lethal autonomous weapon systems is a topic of debate in policy and academic literature, with discussions on risks, legal issues, and moral/ethical concerns. South Korea has deployed gun towers with autonomous lethal capacity at the DMZ, requiring human consent before initiating lethal response. The potential value of autonomous lethality in armed conflict is widely acknowledged. Traditional automation in non-lethal military systems already serves as a force-multiplier by handling repetitive tasks. However, autonomous systems with learning algorithms and contextual awareness can automate more complex tasks, such as flight requirements for drones, leading to significant savings and increased efficiency. Autonomous systems in armed conflict can provide significant advantages by automating tasks like flight control, allowing for faster reactions than humans. The \"OODA loop\" concept emphasizes the importance of quick decision-making to control the initiative in conflicts, forcing opponents to react rather than initiate. This cycle of reaction delay can drive opponents out of sync. Autonomous systems in warfare offer advantages by automating tasks for faster reactions than humans. The delay in opponents' reactions can drive them out of sync. Concerns with non-autonomous remote controlled systems include unreliable connections and enemy interference, which autonomous systems can minimize. The risk of enemy hacking autonomous control systems is not significantly greater than with modern non-autonomous systems. The development of autonomous weapon systems poses greater risks compared to modern non-autonomous systems due to the potential for programming errors, novel situations, and adversary activity leading to loss of control. These risks include the possibility of simultaneous failures across the fleet of autonomous systems, resulting in disproportionately high damage compared to human-controlled or partially autonomous systems. The United States focuses on lethal autonomous weapon systems to capitalize on advances in civilian autonomous technology, aiming to maximize asymmetric advantages difficult for opponents to replicate. Investment in synergistic technologies like stealth, reconnaissance, and precision weapons drives industrial and commercial development. The development of autonomous capability in military and civilian sectors could lead to a symbiotic relationship, driving industrial capacity and commercial development. However, the translation of civilian autonomy into weapon systems may trigger an \"arms race\" dynamic, forcing competitor states to invest in lethal autonomous weapon systems (LAWS) for military competitiveness. LAWS are seen as beneficial for ethical warfare as they can facilitate compliance with the law of armed conflict and are not susceptible to emotional effects that may lead to abuses by human soldiers. The presence of LAWS in mixed teams with human soldiers may restrict inappropriate conduct. Introducing autonomous weapon systems in environments with vetted targets could provide humanitarian benefits. Autonomous decision-making at the moment of lethal action may improve precision in weapon systems. The potential benefits of autonomous decision-making at the moment of lethal action are questioned by many, including concerns about ethical decision-making technology not existing. There are also worries that potential opponents may not employ ethical decision-making in an arms race, leading to legal and ethical critiques of autonomous weapon systems. The development of LAWS may encourage inappropriate aggression. The concern with LAWS is that they may create a moral hazard for national leadership, potentially increasing the likelihood of unlawful aggression by minimizing military casualties. Some argue that this objection is too generic and could apply to any weapon system. The development of LAWS may trigger wider arms races as competitors seek to match the tactical advantage, potentially leading to less control over the behavior of autonomous weapon systems. The development of LAWS may lead to arms races as competitors try to match the tactical advantage, potentially resulting in autonomous weapon systems that do not comply with the laws of war. Some argue that an arms race is already underway, with nations developing autonomous weapons regardless of U.S. actions. Additionally, asymmetric competitors could exploit civilian sector advancements for military purposes. The development of LAWS may lead to arms races as competitors try to match the tactical advantage, potentially resulting in autonomous weapon systems that do not comply with the laws of war. There is a risk of increased attacks on civilian targets, particularly in the United States, as enemies may target civilians if U.S. soldiers are not present in the war zone. This could incentivize attacks on civilian rather than military targets. The use of autonomous weapon systems poses risks such as increased attacks on civilians, as enemies may target civilians if U.S. soldiers are not present in the war zone. There is also a concern that reliance on autonomous systems could make the military more vulnerable to hacking or subversion of software and hardware. The complexity and interdependence of autonomous weapon systems could magnify harmful impacts if security vulnerabilities are exploited. Autonomous capabilities may counter hacking, but all weapon systems are susceptible to subversion. Autonomy may make a system more resilient than non-autonomous systems. The literature discusses the risk of \"run-away\" escalation with large-scale adoption of autonomous weapon systems, leading to warfare that may not have occurred otherwise. The complexity and flexibility of these systems can result in unpredictable lethality, and the speed of decision-making creates a time delay for corrective action in case of failure. Multiple autonomous systems in a tense armed confrontation could increase the danger of uncontrolled escalation. The risk of inadvertent escalation with autonomous weapon systems is a concern, as initial errors could trigger a full-scale response in a vicious cycle. Some argue that autonomous systems are not inherently more destructive than conventional weaponry directed by humans. Questions arise about the plausibility of free-ranging autonomous systems engaging in lethality without human authorization. The main perceived risk lies in the reliability and predictability of these systems. Proponents of a ban on autonomous weapon systems argue that decision-making by these systems is inherently unpredictable due to the technology's reliance on learning algorithms, making future reliability uncertain. Some experts, however, believe that autonomous systems could reach a level of predictability comparable to human soldiers. The proponents of autonomous weapon systems argue that they may reach a level of reliability comparable to human soldiers, despite the unpredictability of decision-making due to learning algorithms. Legal disputes focus on compliance with international humanitarian law and rules of engagement. The operational concern regarding autonomous weapon systems is whether they will comply with appropriate requirements. Another concern is accountability, as the use of LAWS may make it harder to hold parties responsible for misconduct in armed conflict. Various authors point to legal norms related to jus ad bellum, classification of weapons, and laws governing conduct during war as key areas affecting consideration of LAWS. The debate on autonomous weapons focuses on moral hazard rather than legal justification for their use. International law limits the use of weapons causing unnecessary suffering or environmental damage. States must conduct legal reviews of new weapons systems to ensure compliance with international law. The United States conducts weapons evaluations to ensure compliance with international legal obligations, considering if a weapon is prohibited under the laws of armed conflict. Weapons that cause unnecessary suffering or injury beyond military necessity are prohibited. Weapons must be capable of distinguishing between military and civilian targets, and their evaluation considers normal or expected use rather than any conceivable use. Some argue that autonomous weapons are inherently illegal because they cannot adequately distinguish lawful and unlawful targets, but opponents point out that even \"dumb\" bombs are not per se illegal. Autonomous weapons without the ability to distinguish between combatants and civilians may be used in limited combat scenarios. The evaluation of weapons also considers their proposed uses and compliance with the law of war in specific circumstances. The evaluation of autonomous weapons focuses on their compliance with the law of war, particularly the principles of distinction and proportionality. Distinction requires distinguishing between military and civilian targets, while proportionality ensures that the use of force is not excessive. Feasible precautions are also important but have received less attention in debates. The Geneva Conventions emphasize the need to distinguish between civilians and combatants in conflict situations. Concerns arise with autonomous weapons' ability to make this distinction, especially in irregular warfare where combatants are mixed with civilians. This complexity can lead to critical decision-making challenges, as illustrated by the scenario of a robot encountering a person in a house-to-house search. Some argue that autonomous weapons may struggle to distinguish between combatants and noncombatants due to their lack of empathy and human emotion. This could lead to difficulties in complex situations involving surrendering or wounded fighters. Defenders of the technology believe future autonomous weapon systems may improve in distinguishing between different targets. Future autonomous weapon systems may be more capable of distinguishing between combatants and civilians than human soldiers, as they are not affected by stress and emotional intensity. Governments could use shared testing standards and ethicists' evaluations to improve accuracy. Some argue that even if they never operate in zones with noncombatants, LAWS will still be useful in high-intensity conflicts. LAWS in combat zones with noncombatants may have lower requirements for decision-making. Concerns arise about their potential use outside these zones. U.S. military leaders may restrict their use based on the criticality of the interest being protected. U.S. political and military leaders may impose restrictions on military operations in certain situations but could be less inclined to do so if facing an existential threat. Analysts foresee the misuse of LAWS by near-peer or non-state actors. Proportionality in military action aims to limit civilian casualties and damage to civilian property relative to the military advantage gained. Some argue that making proportionality judgments in warfare is a complex task. The proportionality judgment required in military action is deemed too complex for autonomous systems to simulate. It involves understanding the immediate battlefield circumstances and overall strategic goals. The determination of whether collateral impact is excessive relies on inherently human judgment and reasonableness, appealing to common sense and shared human values. Some argue that autonomous weapon systems lack access to human common sense and values, while others suggest a system where the commander makes initial judgments on mission goals and collateral impact. Critics mention the time-sensitive nature of these judgments, but proponents emphasize the reliability of preprogrammed evaluations. Some argue that autonomous weapon systems lack human common sense and values, while others suggest commanders make initial judgments on mission goals and collateral impact. Opponents of a ban on LAWS point out that collateral damage estimates are regularly made using objective data and scientific algorithms with current weapon systems. The commander who sets the LAWS in motion plays a critical role in the legal responsibility for its resulting action, raising questions about accountability. Questions have been raised about accountability for \"war crimes\" committed by autonomous weapon systems. Proponents of a ban argue that machines cannot be held responsible for decision-making, making it difficult to hold someone accountable for illegal actions. Opponents argue for command responsibility and point out that intentional design for war crimes could lead to accountability issues. The use of LAWS in committing war crimes could lead to criminal liability for designers, manufacturers, and commanders. Proponents argue that unintended actions by autonomous systems are more likely to result in war crimes, creating challenges in holding individuals accountable. This lack of accountability may result in a fundamental lack of justice and responsibility associated with autonomous weapons, leading some to advocate for a ban. Some argue for banning LAWS due to concerns about responsibility and war crimes. Opponents suggest that individual criminal liability is not the only way to address misconduct, pointing to laws managing various circumstances. They argue that state responsibility for using LAWS should take precedence, reducing the urgency of establishing individual culpability. The question of justice for noncombatant victims of LAWS-related violence remains unresolved. The moral implications of allowing autonomous weapon systems to make decisions about taking human life have sparked debate. Some argue that it goes against human dignity and treats humans as objects, while others believe it is necessary for state responsibility in using LAWS. The question of justice for victims of LAWS-related violence remains unresolved. Opponents of a ban on autonomous weapon systems argue that assigning moral status to these systems is based on anthropomorphism and unlikely to reflect military technology accurately. They believe that the decision to kill is ultimately made by the commander, not the autonomous weapon. Some authors suggest that autonomous weapons could make better ethical decisions than human soldiers. From various sources, it is highlighted that fully autonomous weapons can identify and engage targets without human intervention, raising concerns about the weapon's adaptive decision-making capabilities and the level of human supervision required. The concept of autonomous action by robots involves unsupervised activities, allowing them to select and engage targets independently once activated. This autonomous 'choice' regarding the use of lethal force is a key element in understanding autonomy in weapon systems. Autonomy in weapon systems can refer to different concepts, including the human-machine relationship, machine complexity, and type of decision being automated. Autonomous weapon systems are defined as those that can independently select and attack targets. These systems can engage targets without human intervention, with some allowing human operators to override their actions. Autonomous weapon systems can engage targets without human intervention, with some systems allowing human operators to override their actions."
}