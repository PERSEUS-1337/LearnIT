{
    "title": "r154_g-Rb",
    "content": "The model proposed learns a policy for transitioning between sets of attributes in an environment augmented with user-defined attributes. It maintains a graph of possible transitions and infers attributes of the current state to plan and execute tasks without specific training. The model shows effectiveness in grid-world games and 3D block stacking. Our model can generalize to longer, more complex tasks in grid-world games and 3D block stacking, even with only exposure to short, simple tasks during training. Deep reinforcement learning has shown success in solving difficult tasks, but challenges arise when tasks are unknown or too complex for uninformed exploration. Recent papers have addressed this issue by structuring the task space to enable agents to solve various tasks effectively. In this paper, attribute sets are used to structure the task space, allowing for the representation of end goals and intermediate sub-tasks. By trading extra supervision for generalization, agents can plan by searching over attribute sets instead of sequences of actions. This approach enables agents to generalize to novel, complex tasks after training on simple tasks. Our agents can generalize to complex tasks by training on simple tasks in a Markov environment with attributes mapped to states. The attributes are human-defined and form a space, guiding the semi-parametric model's graph structure for transitioning between states. The model consists of a neural-net based attribute detector, a policy network, and a tabular transition function. The attributes are inferred from observations through a neural network, and the graph structure and policy are learned via random exploration. The goal attribute set is used to find the shortest path in attribute space, and the policy network executes actions at each stage. The model includes a neural-net attribute detector, a policy network, and a tabular transition function. The transition table tracks training transitions to create a high-level attribute graph for planning. The model aims for \"ignorability,\" where transitions depend only on attributes, not the exact state. This ensures planned transitions are achievable by the policy network. The model includes a neural-net attribute detector, a policy network, and a tabular transition function. The transition table tracks training transitions to create a high-level attribute graph for planning. The model aims for \"ignorability,\" where transitions depend only on attributes, not the exact state. This ensures planned transitions are achievable by the policy network. In the experiments, the agent samples transitions from the environment to find the sets of attributes that occur, using labeled examples to fit attribute detectors. The model includes a neural-net attribute detector, a policy network, and a tabular transition function. The transition table tracks training transitions to create a high-level attribute graph for planning. The model aims for \"ignorability,\" where transitions depend only on attributes, not the exact state. This ensures planned transitions are achievable by the policy network. In the experiments, the agent samples transitions from the environment to find the sets of attributes that occur, using labeled examples to fit attribute detectors. The agent then takes random actions to build statistics on possible transitions within attribute space, resulting in a transition graph G with vertices representing attributes seen. A second phase of training can be performed to learn the probability of transitioning between attributes based on a low-level policy \u03c0. The model includes a neural-net attribute detector, a policy network, and a tabular transition function. The transition table tracks training transitions to create a high-level attribute graph for planning. The model aims for \"ignorability,\" where transitions depend only on attributes, not the exact state. This ensures planned transitions are achievable by the policy network. In the experiments, the agent samples transitions from the environment to find the sets of attributes that occur, using labeled examples to fit attribute detectors. The agent then takes random actions to build statistics on possible transitions within attribute space, resulting in a transition graph G with vertices representing attributes seen. A second phase of training can be performed to learn the probability of transitioning between attributes based on a low-level policy \u03c0. At a random state s with attributes \u03c1 i, we pick \u03c1 j from the set of goals for which c(\u03c1 i, \u03c1 j) > 0 and see if \u03c0 is able to achieve this goal. We keep track of the fraction of successful attempts and store this probability in c \u03c0 (\u03c1 i, \u03c1 j). Finally, we need to train a policy network \u03c0 = \u03c0(s, \u03c1 g) to solve simple tasks that require a few actions to move between nearby attribute sets. One way of training \u03c0 is as an \"inverse model\" in the style of BID1; BID3. In the first phase of training the graph, we sample each initial state s 0 and an action sequence [a 0, a 1, ..., a k] from the exploration policy, causing the agent to arrive at a new state with attributes \u03c1 1. We treat a 0 as the \"correct\" action for \u03c0(s 0, \u03c1 1) and update its parameters for this target. Alternatively, \u03c0 can be trained using reinforcement learning. After an initial graph c is constructed, tasks can be. The model can be trained using reinforcement learning to plan paths on a graph G with a distance metric. Dijkstra's algorithm is used to find the optimal path maximizing DISPLAYFORM0. The policy moves along the path between attribute sets by taking actions according to a set policy. If attributes don't match, a new path is computed. Many researchers have explored Hierarchical RL. The options framework in Hierarchical RL divides a MDP into subprocesses, dealing with multistep \"macro-actions\" using deep learning. This approach controls an agent in a Markovian environment without explicit rewards, showing how an agent can learn to decompose transitions into short sequences. Our work focuses on using macro actions to sequence tasks in a Markovian environment without explicit rewards. We aim to show that adding human supervision to parameterize task space allows for compositionality through planning, similar to the options framework in Hierarchical RL. Our work aims to generalize to new tasks by using attributes for planning, rather than just as tools for building a reactive policy. It is closely related to factored MDPs, where the environment is represented by discrete attributes and transitions between attributes are modeled as a Bayesian network. However, our approach differs in that the attributes do not determine the state, and the focus is on organizing the space of tasks through attributes. Our approach aims to generalize to new, complex tasks by using attributes for planning. It is related to Relational MDP and Object Oriented MDP, where states are described as objects with attributes. The model learns from examples to map attributes to the state. Our model uses explicit search to reason over attributes, unlike the end-to-end neural architecture in BID7. While BID7 trains and tests on similar tasks with novel object properties, our model is trained on simple tasks but generalizes to complex ones. Our approach shares ideas with BID15 in quickly adapting to new learning problems by augmenting tasks with descriptors. In this work, tasks are augmented with descriptors and featurized, similar to BID15. The low-level actor learns to transit between sets of features and plan in that space. Recent deep reinforcement learning works have used modular architectures and hierarchy for generalization to new tasks. Our assignments of attributes serve a similar purpose to their analogical methods. In this work, a \"meta-controller\" searches over attributes for generalization, using high-level annotation to enable task completion. The model maintains an explicit memory of attribute sets seen, different from non-parametric memory methods used in previous works. Future work aims to combine approaches from other studies like BID2 and BID22. In this work, the \"meta-controller\" utilizes user-specified attributes to guide task completion. The approach involves memorizing transitions between attributes during training and using this knowledge at test time in dynamic agent/environment settings. The model, called Attribute Planner (AP), is evaluated in various environments including randomly generated grid-worlds. The model, Attribute Planner (AP), is tested in three different environments: randomly generated grid-worlds and a block stacking simulation. It aims to generalize to complex tasks after training on simpler ones. Comparison is made with baseline policies trained using reinforcement learning with different training variants. The Attribute Planner (AP) model is trained on full sequences, giving it an advantage over models that only see short sequences during training. An inverse model is also trained to predict the next action based on state and goal in 2-D environments with randomly generated worlds. Tasks involve changing switches into specified configurations. The Attribute Planner (AP) model is trained on full sequences, giving it an advantage over models that only see short sequences during training. In 2-D environments with randomly generated worlds, tasks involve changing switches into specified configurations. In another environment, an agent collects resources to craft items by combining them. The agent can grab resources and craft items if it has the necessary items in its inventory and is standing on a special square. The goal is to add a specified item to the inventory. The environment contains three types of resources and products. Episodes start with random changes in resources and inventory items. Crafting key: Switch color game. Observation given as a bag of words with features and locations. High-level transitions are built by running a random agent until attributes change, recorded as edges in a graph. Low-level policy trained concurrently with edge estimates. The low-level policy is trained with the Reinforce algorithm to reach a target set of attributes from the current state. Training episodes end when the task completes or after 80 steps, with a reward of -0.1 given at each step to encourage quick task completion. The policy network has two hidden layers of 100 units. Experiments are run three times with different random seeds, and the mean success rate is reported. In the switches environment, multi-step tasks are generated by setting a random attribute as the target, requiring up to 12 attribute transitions. In the crafting environment, multi-step tasks are generated by randomly selecting an item as the target state. During training, tasks are gradually increased in difficulty by adding more toggles in the switches environment and requiring multiple actions in the craft environment. The transition function is computed using success rates of the low-level policy. During training, tasks are gradually increased in difficulty by adding more toggles in the switches environment and requiring multiple actions in the craft environment. The transition function is computed using success rates of the low-level policy. \u03c0 is the number of task (\u03c1 i , \u03c1 j ) during epoch t, and M t \u03c0 is the number of successful episodes among them. A decay rate of \u03b3 = 0.9 is used. We consider a 3D block stacking task in the Mujoco environment BID35. The input to the model is the observed image, and there are a total of 36 binary properties corresponding to the relative x and y positions of the blocks and whether blocks are stacked on one another. Each training episode is initiated from a random initial state and lasts only one step, i.e. dropping a single block in a new location. The policy network takes a 128 \u00d7 128 image, which is featurized by a CNN with five convolutional layers and one fully connected (fc) layer. The model uses a CNN with five convolutional layers and one fully connected layer to produce a 128d vector for input. Goal properties are transformed into a 128d vector and concatenated with the input vector. The combined vectors go through two fully connected layers with softmax for action output. Performance is compared on block stacking tasks with different models. Tasks include one-step and multi-step goals, with the model outperforming reactive policy baselines. The model uses a CNN with five convolutional layers and one fully connected layer to produce a 128d vector for input. Goal properties are transformed into a 128d vector and concatenated with the input vector. The combined vectors go through two fully connected layers with softmax for action output. Performance is compared on block stacking tasks with different models, including single-step and multi-step goals. The AP model performs well on single-step tasks but struggles on multi-step tasks without proper training, highlighting the importance of normalized transition probabilities. The model also handles underspecified goals by finding the shortest path to a satisfactory attribute set. The AP model performs well on block stacking tasks with underspecified goals, even when trained with a less accurate attribute detector. However, the \"ignorability\" assumption made in Section 2 is violated in the block stacking task due to property aliasing, where states with possible and impossible transitions are aliased with the same properties. The dominant error source in the multi-step task is property aliasing, leading to stuck plans. Training on large sample sizes exacerbates this issue. The second training step computes transition probabilities to mitigate aliasing effects in block stacking tasks. In the AP model trained on one million samples, the second training step improves multi-step performance from 29.7% to 66.7%. Structuring tasks with high-level attributes allows an agent to compose policies for simple tasks into solutions for complex tasks. Planning at the attribute level and executing steps with a reactive policy leads to generalization from simple to complex tasks. Further work is needed on the sample complexity of the planning module and the explicit non-parametric form for c. The parametric lower level policy has a reasonable success rate, but struggles with unseen edges in the graph. With 100K samples, the model can make nontrivial plans due to the non-parametric nature of the graph. Future work aims to combine parametric models with search for more efficient planning. In one example, the policy directs placing a block in front of others, but this is impossible. Progress is hoped to be made on dynamic abstraction to reduce the effective number of edges. The text discusses the challenges of reinforcement learning in building composable models, focusing on exploration and composability. The method described allows for sampling low-level transitions to cover the state space, sidestepping the exploration problem. The attributes \u03c1 and c are highlighted for their usefulness in this context. The attributes \u03c1 and c are useful for incentivizing exploration in reinforcement learning. Discovering these attributes automatically could reduce the need for human supervision. Exploration can be promoted by learning a policy to discover undiscovered edges in the planning graph. Negative rewards can be given for encountering edges multiple times. In reinforcement learning, attributes like \u03c1 and c incentivize exploration. Discovering these attributes automatically could reduce the need for human supervision. To promote exploration, a policy can be learned to discover undiscovered edges in the planning graph, with negative rewards for encountering edges multiple times. In a Mazebase environment designed for exploration testing, a random agent discovered 18.6 edges on average, while an agent with an exploration reward found all 25 edges. The AP model outperforms other baselines in success rates on multi-step tasks. Training data shows that the AP model has a success rate of 99.8% on one-step tasks, compared to 51.5% for Reinforce multi-step + curriculum and 26.0% for Reinforce one-step. The Attribute Planner model also shows improved performance on both one-step and multi-step tasks with increased training examples."
}