{
    "title": "rJeU_1SFvr",
    "content": "In this work, a new form of latent optimization is introduced for improving adversarial dynamics in training generative adversarial networks. The approach, inspired by CS-GAN, enhances interactions between the discriminator and generator, leading to significant performance improvements on the ImageNet dataset. The model achieves an Inception Score of 148 and a Frechet Inception Distance of 3.4, outperforming the baseline BigGAN-deep model by 17% and 32% in IS and FID respectively. Generative Adversarial Nets (GANs) are powerful models for matching data distributions, particularly in image generation. Recent advancements focus on architectural improvements and regularization techniques. A new approach inspired by CS-GAN utilizes latent optimization with natural gradient descent, enhancing adversarial dynamics in training GANs. This method shows promising results on the ImageNet dataset, outperforming baseline models in Inception Score and Frechet Inception Distance. Latent optimised GANs (LOGAN) improve the dynamics of adversarial training by utilizing efficient second-order updates. This algorithm enhances the state-of-the-art BigGAN-deep model without architectural changes, resulting in higher quality images and more diverse samples. The text discusses the use of \u03b8 D and \u03b8 G to represent generator and discriminator parameters in GANs. It explains the process of generating images from a latent source and training GANs through an adversarial game. The min-max game involves the generator trying to fool the discriminator. The exact form of h(\u00b7) depends on the choice of loss function. The text discusses the use of Wasserstein loss and hinge loss in GANs, with a focus on balancing updates to the generator and discriminator. It mentions the significance of BigGAN and BigGAN-deep in high fidelity image generation. BigGANs utilize residual blocks and self-attention to scale up GANs. The focus is on improving adversarial dynamics during training, specifically addressing the gradient updates in GANs that can exhibit cycling behavior. This cycling behavior, referred to as the simultaneous gradient, can slow down or prevent convergence in GAN models. Latent-optimised GANs (LOGAN) improve training stability and final performance by using latent optimization to guide updates of z, leading to better samples compared to randomly sampled z. This approach has been shown to benefit medium-sized models like DCGANs and Spectral Normalised GANs. The algorithm for Latent-optimised GANs (LOGAN) involves using optimized latent z for training, updating z with natural gradient descent, clipping z values between -1 and 1, computing generator loss, and updating parameters until reaching maximum training steps. The analysis of LOGAN as a 2-player differentiable game is provided, along with a comparison to unrolled GANs and stochastic approximation methods. The problem with gradient-based optimization in GANs is that the vector-field generated by the losses of the discriminator and generator is not a gradient vector field. Gradient descent may not find a local optimum and can lead to issues like mode collapse. Symplectic Gradient Adjustment (SGA) by Balduzzi et al. and Gemp & Mahadevan proposed a method to improve gradient dynamics in adversarial games. SGA adjusts the gradient by adding a term reflecting interactions between D and G. The adjusted gradient in SGA improves GAN training by incorporating second-order terms reflecting interactions between D and G. SGA allows faster convergence to stable fixed points but is expensive to scale due to computing second-order derivatives. LOGAN computes extra terms to include second-order effects in the gradient. In LOGAN, extra terms are computed through automatic differentiation during latent optimization, reducing cycling effects by introducing a symplectic gradient adjustment. Latent optimization approximates SGA using second-order derivatives with respect to latent z and discriminator/generator parameters separately, omitting expensive cross-parameter computations. Latent optimisation efficiently couples the gradients of the discriminator and generator using lower-dimensional latent source z, allowing for larger step sizes during training. This accelerates the speed of updating D relative to G, facilitating convergence in GAN training dynamics. The use of natural gradient descent (NGD) for latent optimization in GAN training dynamics allows for larger step sizes compared to basic gradient descent (GD), which may be more detrimental for larger models with complex loss surfaces. NGD is considered an approximate second-order optimization method. Natural Gradient Descent (NGD) is an approximate second-order optimization method that uses the positive semidefinite Gauss-Newton matrix to approximate the Hessian. It has been successfully applied in various domains and is efficient for latent optimization, even in large models. The update in NGD is computed using the Fisher information matrix, and it is commonly used with error functions like cross entropy loss. The Poisson log-likelihood interpretation for the hinge loss in GANs is discussed in Appendix C. Latent optimization in NGD differs from common scenarios as it lacks expectation over the condition (z). Per-sample Fisher is computed using empirical Fisher with Tikhonov damping, making it cheaper than full Fisher. The damping factor \u03b2 regulates step size in NGD to handle poor Hessian approximation or significant changes. NGD update in large scale GANs adapts step size based on curvature estimate. It automatically smooths updates by down-scaling gradients as their norm grows. Results from applying the algorithm on Spectral Normalised GANs trained with CIFAR dataset are presented in Appendix E. The study modified the BigGAN-deep architecture by increasing the latent size, using a uniform distribution, and leaky ReLU. Results showed only slightly better scores compared to the baseline model. FID and IS values were computed as in previous studies, with means and standard deviations calculated from 5 models with different random seeds. The study introduced a new model called LOGAN (NGD) which achieved better FID and IS scores compared to the BigGAN-deep baseline. Various hyper-parameters were adjusted, including a damping factor, step size, and regularization of weight-change. Training with LOGAN also eventually collapsed, possibly due to higher-order dynamics. During training, LOGAN was 2-3 times slower per step compared to BigGAN-deep. Optimizing z during evaluation did not improve sample scores. Movement in Euclidean space \u2206z grew until training collapsed, but movement in D's output space remained unchanged. Optimizing z improves training dynamics, allowing LOGAN to work well without latent optimization. After training, LOGAN was slower than BigGAN-deep. Optimizing z during evaluation did not improve sample scores. Removing certain terms in the gradient stabilized training, as predicted by the analysis. The study analyzed the impact of removing certain terms on training stability. Truncation technique was used to balance FID and IS in model training, resulting in higher visual quality but reduced diversity in samples. The LOGAN model significantly enhances large scale GAN training for image generation. The study analyzed the impact of LOGAN on large scale GAN training for image generation, showing improvements in quality and diversity. Additional samples and results are provided, along with analyses of the algorithm's effectiveness in various tasks. The algorithm LOGAN combines symplectic gradient adjustment, unrolled GANs, and stochastic approximation with two time scales. It treats the latent step \u2206z as a third player in the game played by the discriminator and generator, optimizing \u2206z online for each sample z \u223c p(z). The per-sample loss L G (z) is emphasized over batch losses for practical implications. When optimizing latent variables in GANs, the per-sample loss L G (z) should guide the process, rather than batch losses. The Fisher information matrix should be computed using the current sample only. Symplectic gradient adjustment helps counteract rotational dynamics in the optimization process. The focus is on optimizing \u03b8 D and \u03b8 G for efficient training. Latent optimisation improves GAN training by adjusting gradients efficiently through automatic differentiation, unrolling GANs in the latent space. This method has been shown to significantly enhance training, as demonstrated in experiments and previous research. LOGAN is a scalable approach for GAN training in the latent space, avoiding second-order derivatives over a large number of parameters. Latent optimization affects both the discriminator and generator, approximating Unrolled GANs. The generator loss is based on a Bernoulli distribution, with options beyond this choice. Loss in LOGAN is based on a Bernoulli distribution, but other valid options exist for the discriminator's output distribution. Multiple independent discriminators can reject a fake input, described by a Poisson distribution with parameter \u03bb. The generator loss is the negative log-likelihood when \u03bb = \u2212D(G(z)), with the caveat that the Poisson distribution is not well defined when D(G(z)) > 0. The discriminator's hinge loss aims to push D(G(z)) < 0 during training. To normalize the variance of z during training, a moving average and standard deviation over a window of size N are computed. The experiments with different window sizes showed robust results. Latent optimization was also applied to SN-GANs, with similar setup as CS-GAN but without class conditioning. NGD used a smaller damping factor and z regularizer weight. For SN-GANs, a smaller damping factor \u03b2 = 0.1 and a z regularizer weight of 3.0 were found to work best. Optimizing 70% of the latent source and running extra latent optimization steps improved evaluation results. Table 2 shows significant improvement over the baseline SN-GAN model in terms of FID and IS. Further investigation into the impact of model size on latent optimization is planned for future studies. The LOGAN (NGD) model showed a significant improvement over the baseline SN-GAN model, with a 16.8% increase in IS and a 39.6% decrease in FID. Samples from LOGAN (NGD) displayed higher contrasts and sharper contours compared to SN-GAN."
}