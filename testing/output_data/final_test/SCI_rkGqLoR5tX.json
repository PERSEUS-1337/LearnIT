{
    "title": "rkGqLoR5tX",
    "content": "Adoption of deep learning in safety-critical systems raises the need for understanding what deep neural networks do not comprehend. Outlier Detection In Neural networks (ODIN) is a method for detecting outlier observations during prediction without altering the neural network's architecture or training. ODIN efficiently detects outliers on various datasets like Fashion-MNIST, ImageNet-synsets, and speech command recognition. Deep neural networks are being used in various applications, but they can fail when input data differs from training data. Detecting outlier observations post-training can help improve autonomous decision making based on deep learning. In manufacturing process control, models like Partial Least Squares regression have been used for decades to predict outcomes and detect outlier input. ODIN is a method for detecting outliers in deep neural networks during prediction, inspired by manufacturing process control principles. Unlike PLS regression, neural networks learn a non-linear mapping, making it challenging to determine the model's knowledge limits. ODIN uses a linear approximation to identify outliers in the neural network's predictions. Using principles from manufacturing process control, ODIN proposes a linear approximation of intermediate activations to detect outliers in deep neural network predictions. This method allows for reliable and safe autonomous decision making based on deep learning, without imposing constraints on architecture or training. Various methods have been developed to describe uncertainty in predictions, such as predicting probability distributions instead of point inferences. BID10 introduced MC-dropout, which uses prediction time dropout to estimate predictive uncertainty. Several alternatives to MC-dropout for perturbing Monte-Carlo samples have been proposed, including sampling based on batch-normalization parameters, model ensembles, multiple prediction heads in a shared base network, variational inference of weight distribution, and Laplace approximation of distributions from existing weights. However, these methods either constrain network construction or training, limiting their use in production systems. Many of these methods also require multiple inferences per prediction. Alternative approaches for estimating uncertainty in classification problems include training linear classifiers on intermediate layers of a base model and using a meta-model to determine the model's correctness. Another method involves leveraging Generative Adversarial Networks (GANs) to augment datasets with outliers for training a deep neural classifier. However, training GANs can be challenging. Anomaly detection is closely related to outlier detection in prediction times. Anomaly detection methods include support vector machines, observation density, distances, isolation forests, and deep neural networks like autoencoders. Outlier detection systems train a separate model to detect deviant observations, while prediction time outlier detection describes the limits of a predictive model's knowledge. Partial Least Squares regression model uses latent variables to detect outliers after training. The Partial Least Squares regression model uses latent variables for outlier detection after training. In neural networks, Outlier Detection In Neural networks (ODIN) applies similar principles by using a linear approximation of the hidden layer manifold. The Partial Least Squares regression model uses latent variables to maximize covariance between X and Y. The model finds orthogonal subspace T in data-matrix X, similar to PCA but maximizing covariance. Columns in T are calculated sequentially using basis-vectors w and residuals F. The Partial Least Squares regression model uses latent variables to detect outliers during prediction by measuring distance to the sub-space and training observations. Distance to sub-space is typically measured using residual sum of squares (RSS) and Mahalanobis distance is used to estimate distance to training observations within the sub-space. The Mahalanobis distance for new data projections in a fitted PLS model can be approximated as a multivariate normal distribution. To address non-linear manifold modeling, a density-based metric like Local Outlier Factor (LOF) in the latent variable space can be used. In deep neural networks, data undergo nested non-linear transformations instead of linear mapping to a single sub-space. According to the manifold hypothesis, deep neural networks may disentangle complicated manifolds, allowing for a transformation to a linear manifold. This can be used for outlier detection by modeling the data representation within the neural network model using PCA. ODIN is a method for outlier detection in neural networks that utilizes the data representations within the network itself. It involves modeling the data representation as a linear manifold using PCA and measuring distances to the activation manifold for outlier detection. ODIN is a method for outlier detection in neural networks that utilizes data representations within the network. It measures the distance from new observations to the training data to detect outliers during prediction time on classification tasks. ODIN is compared to MC-Dropout for outlier detection using Fashion-MNIST dataset. The Fashion-MNIST dataset consists of 70,000 greyscale 28x28 pixel images, with 10,000 test set images across ten categories of fashion products. Five classes were excluded as outliers, including all shoes and two clothing classes. A small CNN was trained on five classes using rmsprop optimization, achieving a test set accuracy of 97%. Features were extracted from max-pooling layers for outlier detection using ODIN. ODIN was evaluated for outlier detection using different metrics (RSS, Mahalanobis distance, LOF), PCA variance levels (50-99%), and feature extraction layers. Results showed clear separation of shoe outliers using RSS (ROC-AUC 0.97), benefiting from increased PCA R2. In contrast, MC-dropout failed to detect shoe outliers effectively (ROC-AUC 0.495). The Fashion-MNIST experiment showed that ODIN successfully detects outliers in image classification. Strong outliers are best detected by measuring distance to manifold, while subtle outliers are better detected in low-density regions using LOF. Prediction time outlier detection was demonstrated using a pre-trained CNN on ImageNet synsets, with clear separation of outliers in a cat vs. dog classifier. We used a pre-trained Inception-v3 network with a hidden layer and achieved 93% test set accuracy. Outlier detection was performed on features extracted from each module, successfully detecting cars and horses as outliers. In this experiment, ODIN was used to detect outliers in images of cats and dogs. Results showed that ODIN performed slightly better than MC-dropout without relying on dropout or any training constraints. Higher PCA R2 values produced more reliable results, with all metrics peaking at inception module 8. ODIN reliably detected outliers using a pretrained CNN on real-world images. In a speech command recognition experiment, a LSTM-based model was used with the Speech Commands dataset containing 105,000 short utterances of various words. The dataset included digits zero to nine, command words, and arbitrary words used as outliers. Utterances were transformed into Mel-Frequency Cepstral Coefficients and a bi-directional LSTM model was trained for classification. The bi-directional LSTM model with dropout was trained for 30 epochs using the Adam optimizer and batch size 512, achieving a test-set accuracy of 78% for 25 classes. Outlier detection was performed by extracting features from the third LSTM layer, fitting a PCA model, and calculating RSS and Mahalanobis distances. Precision, recall, and F1-score were used to evaluate outlier detection at the 9th deciles. Outlier detection was performed using RSS and Mahalanobis distances, with ODIN consistently outperforming MC-dropout in detecting outliers in the speech-commands dataset. Different classification combinations were evaluated to improve precision or recall based on the two metrics used. The experiment showed varying F1-scores for different words, with ODIN achieving better results overall. Deep neural networks, like ODIN, show success in various applications such as speech recognition and image classification. It is crucial to understand when new data does not align with training data for safety-critical applications. Linear approximation of hidden layer manifolds is used to measure distances within the manifold. Comparisons to MC-dropout are made in this context. ODIN, a deep neural network, is compared to MC-dropout for outlier detection post-training in image classification and speech recognition tasks. ODIN helps define the limits of neural networks' knowledge for safer deep learning use."
}