{
    "title": "SkgKzh0cY7",
    "content": "Unsupervised video-to-video translation is a new task that involves translating videos to different styles or domains without paired examples. This task presents challenges in capturing realistic motion and transitions between frames. The performance of per-frame video translation using existing image-to-image networks is evaluated, and a spatio-temporal 3D translator is proposed as an alternative solution. The 3D method is tested on various datasets, showing that frame-wise translation produces realistic results on a single frame level but underperforms significantly. The curr_chunk discusses the limitations of per-frame video translation compared to a three-dimensional approach, which better captures the complex structure of videos. It introduces the new task of unsupervised video-to-video translation, using unpaired videos to preserve high-level semantic information. This method has shown promising results in tasks like style transfer and semantic segmentation. Computer vision tasks involve video-to-video translation for tasks like semantic segmentation, video colorization, and quality enhancement. Motion-centered tasks such as action recognition can benefit from robust unsupervised video-to-video translation methods. However, traditional per-frame methods like CycleGAN may struggle to maintain continuity and consistency in videos. In this paper, a video-to-video translation model is proposed to preserve cross-frame consistency and spatio-temporal structure by translating an entire video as a three-dimensional tensor. The model is evaluated using multiple datasets, including synthetic videos of moving digits and volumetric images imitating medical scans. Realistic segmentation-to-RGB and colorization experiments are also conducted on the GTA dataset BID14. Additionally, a new MRI-to-CT dataset is introduced for medical volumetric image translation. The task of unsupervised video-to-video translation is emphasized, with results shown for MR-to-CT translation and moving MNIST digits colorization. The proposed 3D convolutional model for video-to-video translation provides more accurate and stable results compared to framewise translation. The model preserves cross-frame consistency and spatio-temporal structure, showing improved color preservation in videos. More results and videos can be found in the supplementary video. The proposed 3D convolutional model for video-to-video translation outperforms framewise methods by preserving cross-frame consistency and spatio-temporal structure. It includes two generator networks (F and G) and two discriminator networks (D A and D B) to translate input volumetric images between domains. The model also ensures cycle consistency in translation tasks. Recent advances in unsupervised image-to-image translation have been driven by the adversarial formulation used in many recent translation models. The main goal is to learn a cross-domain mapping that produces fake target samples indistinguishable from actual ones. This approach enables domain adaptation and unsupervised learning of per-pixel labels. Recent advances in domain translation involve the introduction of cycle-consistency, where models aim to learn mappings F(x) and G(y) that are inverses of each other. This idea has been explored in video-to-video translation and combined with variational autoencoders for better results. Adversarial and non-adversarial supervised image-to-image translation models achieve higher visual fidelity by pairing samples in source and target datasets. Adversarial video generation has gained traction in recent years, utilizing frame-level models based on long short-term memory and spatiotemporal convolutions to improve visual fidelity. However, existing works do not address cross-domain video translation from unpaired data. A proposed solution involves a neural approach for video-to-video translation using a conditional GAN that treats inputs and outputs as three-dimensional tensors. The generator module in the video-to-video translation model aims to produce realistic volumes in domain B from images in domain A. Two generator-discriminator pairs are used, with a cycle consistency loss ensuring that mapped samples are close to the originals. The generators are implemented as 3D convolutional networks with specific architecture, including convolutional layers, residual blocks, and additional layers. The overall objective includes an adversarial loss and a cycle consistency loss to generate realistic videos and maintain consistency in the translation process. The 3D CycleGAN model utilizes generator networks to create realistic videos in domain B from domain A images, with discriminators distinguishing between real and fake samples. The cycle consistency loss ensures that translated samples match the originals. The total objective includes adversarial and cycle consistency losses for video-to-video translation within a 3D convolutional framework. The study compares different training strategies for video-to-video translation using CycleGAN, including random CycleGAN, sequential CycleGAN, and const-loss CycleGAN. The 3D CycleGAN approach operates on three-dimensional inputs for improved performance. The study explores various training strategies for video-to-video translation with CycleGAN, including random, sequential, and const-loss approaches. Contrary to conventional wisdom, sequential CycleGAN outperformed random CycleGAN in terms of image continuity and translation quality. Const-loss CycleGAN aims to improve image sequence consistency by adding a total variation penalty term to the loss function. This helps address motion artifacts like rapid object shape changes, color shifts, and disappearing objects. The study used the GTA segmentation dataset for unsupervised segmentation-to-video translation, highlighting the challenges in preserving continuity in shape, color, and texture. The MRCT dataset was created to evaluate the performance of a method on MR to CT volumetric image translation. It includes 225 MR images from LGG-1p19qDeletion dataset BID0 and 234 CT images from Head-Neck-PET-CT dataset BID16. The Volumetric MNIST dataset was also created using MNIST handwritten digits database BID9. The task involves creating volumetric images from MNIST handwritten digits using erosion transformation to simulate 3D scans. Two types of domains, \"spherical\" and \"sandglass\", were used. The goal is to transform digits between these domains while preserving the global intensity pattern. Additionally, a dataset with moving digits of different colors was generated to train models to translate white digits to colored ones. Another dataset, the Playing for Benchmarks dataset BID15, consists of GTA gameplay recordings for segmentation tasks. The BID15 dataset is a large collection of GTA gameplay recordings used for image-to-image translation evaluation. 1216 short clips were generated from the daylight driving and walking subsets, with corresponding ground truth segmentation videos. Frame-level methods diverged more frequently than 3D models in unsupervised video-to-segmentation translation. Frame-wise translation produces plausible images but lacks temporal consistency, while forming batches from consecutive frames improves convergence. Our proposed 3D convolutional model improves convergence by reducing motion artifacts with a penalty term on consecutive frames. The model produces coherent outputs in time but with fewer details due to approximating a higher dimensional mapping. Experiments show that frame-level approaches struggle with spatio-temporal patterns, while our 3D method performs almost perfectly in capturing them. In experiments, different datasets were used with various approaches for training models with a set number of parameters. The visual fidelity of larger models did not improve, but segmentation quality increased. The performance of a large random 2D model was evaluated for MRCT and GTA segmentation-to-RGB tasks through human evaluation on Amazon Mechanical Turk. Participants were asked to choose the more realistic output between two different models for the same input. The probability of choosing a video generated by each model over a real one was estimated, with significance levels reported for the MRCT domain pair. Definitive ground truth answers were available for some domain pairs. In the evaluation of different domain pairs, definitive ground truth answers were available for some. For the rgb-to-segmentation translation, metrics such as segmentation pixel accuracy and L2 distance between stochastic matrices were used. The colorization task focused on average standard deviation of non-background colors and L2 error between original and translated shapes. The experiments on volumetric MNIST showed that standard CycleGAN struggled to capture global motion patterns in image sequences. The 3D CycleGAN model successfully learned global motion patterns in image sequences, while sequential models struggled to generate correct shapes. In a colorization experiment on MNIST videos, models that preserved color throughout the sequence collapsed to using a single color, despite not having access to previous frames. The 3D CycleGAN model produced smoother videos compared to framewise methods, but struggled with rapid changes in input. The additional constraint on total variation improved visual fidelity but reduced variability in outputs. The spatio-temporal model in GTA video colorization produces consistent results. The 3D method outperformed others in MRI-to-CT translation, providing more realistic anatomy. The CT-to-MRI task is challenging, but the 3D model showed significant improvement. Sequential batch approach yielded more realistic results compared to random methods. The sequential batch approach in unsupervised video-to-video translation produced more realistic results compared to random batch selection. The similarity between images within a batch is crucial for faster convergence. Multiple baselines were proposed for framewise translation using CycleGAN and 3D CycleGAN, showing that per-frame approaches struggle to capture global motion patterns and object consistency. Sequential batch selection reduces motion artifacts in unsupervised video-to-video translation by improving motion patterns and shape and texture consistency of translated objects."
}