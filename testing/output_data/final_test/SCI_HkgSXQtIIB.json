{
    "title": "HkgSXQtIIB",
    "content": "Neural networks trained with feedback alignment, without weight transport, are more robust against adversarial attacks compared to networks trained with backpropagation. On MNIST, networks without weight transport have an adversarial accuracy of 98% versus 0.03% for backpropagation. This gap decreases on CIFAR-10 but remains significant for small perturbations. Neural networks trained with feedback alignment, without weight transport, are more robust against adversarial attacks by using random fixed synaptic weights in the feedback path to approximate gradients computed by backpropagation. This mechanism, called feedback alignment (FA), shows higher adversarial accuracy compared to networks trained with backpropagation. Attacks on neural networks rely on accurate gradients to deceive the system. Comparing networks trained with backpropagation (BP) and feedback alignment (FA) on various attacks reveals FA's higher robustness. Neural networks without weight transport have not been tested for adversarial attacks before. The backpropagation algorithm computes error signals and weight updates in the feedback path. The weight update in neural networks involves the derivative of the activation function and a learning-rate factor. A biologically implausible computation arises in the backward error calculation due to the need for transposed synapses in the feedback path. To address this, feedback alignment fixes the feedback-path synapses to different matrices that are randomly fixed during training, enabling error signal computation without weight transport issues. In this paper, the authors introduce the terms \"bp\" and \"fa\" to differentiate between backpropagation and feedback alignment in neural networks. They found that feedback alignment provides inaccurate gradients that are not aligned with true gradients, which may result in less effective adversarial attacks. Gradient-based attacks aim to find minimal perturbations by utilizing true gradients for maximizing network loss function. The Fast Gradient Sign Method generates adversarial examples by perturbing the input with one step gradient update along the direction of the sign of the gradient. This perturbation can be computed through transposed forward-path synaptic weights like in backpropagation or through random synaptic weights like in feedback alignment. Kurakin et al. extended this method to the Basic Iterative Method, running multiple gradient updates with small step size and clipping pixel values. The Projected Gradient Descent Method extends the Fast Gradient Sign Method by introducing momentum to generate adversarial examples iteratively. It runs multiple gradient updates with small step size and clips pixel values to ensure each pixel is within a specified interval. Experiments were conducted on neural networks using the LeNet architecture with cross-entropy loss function, varying perturbation magnitude from 0 to 1. Adversarial examples were generated using 10 iterations for BIM and MI-FGSM attacks. When conducting adversarial attacks on a neural network using FGSM, BIM, and MI-FGSM methods with perturbation magnitude on MNIST, the accuracy remains around 97%. Adversarial examples generated by FA gradients cannot deceive FA neural networks, unlike BP neural networks whose accuracy drops to 0% with increasing perturbation. Transferability of adversarial examples between BP and FA networks was also investigated. The study compared the robustness of BP and FA networks to adversarial attacks on CIFAR-10. Adversarial examples from FA networks did not fool BP networks, but examples from BP networks could deceive FA networks. The accuracy of FA networks decreased with perturbation magnitude, but at a lower rate than BP networks. Transferability of adversarial examples showed that BP examples could fool FA networks, unlike the non-transferability observed in MNIST. The study evaluated the robustness of deep neural networks without weight transport to adversarial attacks on MNIST and CIFAR-10 datasets. Results show that FA networks are robust to adversarial examples generated with FA on MNIST, but not on CIFAR-10. Adversarial examples generated by FA are not transferable to BP networks. More complex datasets require further analysis to understand the impact of different attacks. Performing in-depth analysis on complex datasets to assess the effect of approximated gradients from feedback alignment on the adversarial accuracy of biologically plausible neural networks under gradient-based attacks."
}