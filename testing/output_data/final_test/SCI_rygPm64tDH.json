{
    "title": "rygPm64tDH",
    "content": "In deep learning, model priors transfer information from humans to a model, while model attributions explain the model's behavior to humans. Existing methods using attributions to align a model's behavior with human intuition are ineffective. A new feature attribution method, expected gradients, and a framework called attribution priors enforce prior expectations about a model's behavior during training. Experiments show that models trained with attribution priors are more effective across various types of data. Recent work on interpreting machine learning models has focused on feature attribution methods, which assign importance to input features for making predictions. These methods can reveal problems in a model or dataset, providing insight into model behavior. Models trained with attribution priors are more intuitive and achieve better generalization performance compared to existing methods. Recent work on interpreting machine learning models has focused on feature attribution methods, which assign importance to input features for making predictions. Ross et al. (2017b) introduced the idea of regularizing explanations to train models that better align with domain knowledge. Their method penalizes the gradients of unimportant features based on a binary variable indicating feature importance. However, this approach has limitations compared to modern feature attribution methods in terms of theoretical guarantees and practical applicability. Incorporating intuitive human priors is crucial for developing interpretable models. A new framework called attribution priors is proposed to encode abstract priors by regularizing feature attributions during training. This method enhances the encoding of domain knowledge more effectively than traditional gradient-based regularization. In 2018, Roth et al. introduced a novel feature attribution method called expected gradients, which improves deep models for various prediction tasks. This method is regularized under an attribution prior, leading to better model interpretability and generalization. It also reduces prediction error and captures biological signals effectively by encouraging similarity among gene expression features using a graph prior. Additionally, it helps develop sparser models and improves performance in learning from limited data for patient mortality prediction. In this section, an attribution prior is formally defined to encourage a skewed distribution of feature attributions in deep learning models. The prior is a penalty function representing a log-transformed probability distribution over possible attributions, aimed at improving model performance with limited training data. The attribution prior is a penalty function representing a log-transformed probability distribution over possible attributions in deep learning models. It aims to improve model performance with limited training data by encouraging a skewed distribution of feature attributions. Different attribution methods can be used, and attribution priors can be tailored to specific tasks based on domain knowledge and data. In the following sections, three different priors for three data types are defined and applied to capture human intuition in various domains. Previous work on interpreting image models has focused on creating attribution maps, which can be noisy and difficult to understand, potentially indicating vulnerability to adversarial attacks. Existing methods only post-process attribution maps without changing model behavior, which may not faithfully represent the original model. In this section, a framework is described to train image models with smoother attributions by applying a total variation loss on pixel-level attributions. The penalty encourages smoothness over adjacent pixels, similar to placing a Laplace prior on the differences between pixel attributions. This approach aims to address the limitations of existing methods in interpreting image models and improving model behavior. The curr_chunk discusses how prior information about relationships between features in a graph can improve performance on biological prediction tasks. It mentions using a weighted adjacency matrix to represent interactions between genes or proteins in a biological network, and penalizing the squared Euclidean distance between feature attributions to encourage similarity along graph edges. The curr_chunk discusses penalizing global feature attributions using the graph Laplacian and placing a Normal(0, \u03bb \u22121 ) prior on differences between adjacent features in a graph. Feature selection and sparsity help alleviate the curse of dimensionality and improve model generalization by using a small number of input features. Applying an L1 penalty to the first layer can build a sparse deep model. The curr_chunk discusses different approaches to building sparse deep models, such as applying L1 penalty to the first layer or using sparse group lasso (SGL) to penalize weights connected to specific features. However, these methods may not always effectively create sparse models. The Gini coefficient, known for its effectiveness in sparse linear regression, is now applied to deep models through an attribution penalty based on global feature attributions. This method, called expected gradients, maximizes when attributions are focused on one feature and minimized when spread evenly. Expected gradients is an extension of integrated gradients with fewer hyperparameter choices, aiming to explain the difference between a model's current prediction and the prediction with a baseline input. Choosing a baseline input is necessary for feature attribution methods, but the choice is often arbitrary. Our method avoids this issue by not relying on a specific baseline input. Expected gradients is a method that avoids arbitrary baseline choices by integrating over a dataset to represent a lack of information. It offers a natural sampling-based approximation approach by drawing samples from the training dataset and computing the value inside the expectation for each sample. Training with expected gradients involves using a batch training procedure to approximate expected gradients over the training process. Even with as few as k = 1 samples drawn for each mini-batch, the explanations can be regularized due to the averaging effect of the expectation. Expected gradients with k = 1 samples suffice to regularize explanations due to the averaging effect. This choice results in more reference samples than needed for reliable individual attributions. Expected gradients outperforms other feature attribution methods on 18 benchmarks, showing its effectiveness in finding important features for a given dataset and model. The method involves applying an attribution prior to a VGG16 network trained on the CIFAR-10 dataset. The choice of \u03bb is optimized to minimize the attribution prior penalty while maintaining test accuracy. Expected gradients attribution maps show smoother attributions, highlighting the target object structure in images. Recent work has shown that image classifiers are sensitive to small domain shifts, affecting test accuracy. To simulate this shift, Gaussian noise is added to test set images, comparing the performance of regularized, baseline, and gradient-based models. The regularized model proves more robust to noise, as shown in Figure 1. The gradient-based model sacrifices test set accuracy for robustness and intuitive saliency maps. Despite the trade-off, stricter hyperparameter cutoffs still result in modest but significant robustness compared to the baseline. Incorporating the \u2126 graph attribution prior enhances model performance. For more details and experiments, refer to the Appendix. Incorporating the \u2126 graph attribution prior improves model performance by allowing the incorporation of prior biological knowledge. Gene expression and drug response data for patients with acute myeloid leukemia were used to predict drug response. The tissue-specific gene interaction graph for AML was downloaded from the HumanBase database. The two-layer neural network trained with the graph attribution prior outperforms other methods in test performance. Replacing the biological graph with a randomized graph results in no improvement. Applying the novel graph prior as a penalty on the model's gradients does not significantly enhance performance. Using prior graph information improves test performance in a linear LASSO model. Our graph attribution prior neural network outperforms graph convolutional neural networks in utilizing graph information. Gene Set Enrichment Analysis shows that our model captures more biologically relevant pathways compared to a model without attribution priors. The \u2126 sparse attribution prior improves model performance in limited data settings using a healthcare mortality prediction dataset. A neural network trained with this prior achieves the best test performance compared to other models. A neural network trained with a graph attribution prior significantly captures biological pathways, including AML-relevant pathways, in medical data for predicting patient survival after 10 years. Sparse models enable accurate predictions with few labeled patient samples, using L1, sparse group lasso, and sparse attribution prior penalties in a 3-layer binary classifier neural network. The regularization strength was tuned for all methods, with the sparse attribution prior enabling more accurate test predictions and sparser models. The sparse attribution prior is much more effective at concentrating importance in the top few features compared to other methods. The sparse attribution prior outperforms other models in terms of performance and sparsity. Various attribution methods have been proposed for deep learning models, with integrated gradients chosen for its ease of differentiation and theoretical guarantees. Gradient penalties during training have also been discussed in existing literature. Gradient regularization has been utilized as an adversarial defense mechanism to enhance generalization performance in digit classification. Recent studies have linked gradient-based training for adversarial purposes with network interpretability, emphasizing the importance of incorporating human intuition into the training process. Previous work has focused on the problem of unexpected classifier behavior on inputs not seen in the training distribution, highlighting the need to incorporate feature attribution methods into training. The curr_chunk discusses an active learning algorithm that updates a model based on counter-factual examples generated from feature attributions. It compares their approach to previous work that focuses on training models with correct explanations. The work presented is more general, introducing three penalty functions and a novel feature attribution method that can be efficiently regularized. The curr_chunk introduces a novel feature attribution method that can be efficiently regularized using a sampling procedure, leading to better generalization performance. It discusses the use of attribution priors to encode prior knowledge into deep learning models, emphasizing the integration of expected gradients with attribution priors. The combination of expected gradients with attribution priors improves model performance by encoding prior knowledge across various domains. This approach leads to smoother and more interpretable image models, biological predictive models incorporating graph-based knowledge, and sparser healthcare models for data-scarce scenarios. Attribution priors offer a versatile framework for encoding domain knowledge, with potential applications across different fields. The attribution method, expected gradients, requires background reference samples from the training data. By re-using the same batch of input data as a reference batch, we can avoid additional data reading. The explicit form of the attribution priors used in the paper is elaborated on in this section. Minimizing the error of a model corresponds to maximizing the likelihood of the data under a generative model. The curr_chunk discusses the use of an additive regularization term and an image prior in the context of maximum likelihood estimation. The image prior utilizes a total variation penalty, which is equivalent to placing Laplace-distributed priors on adjacent pixel differences. This penalty is widely used and directly implemented in Tensorflow. The curr_chunk introduces a graph prior and a sparsity prior in the context of maximum likelihood estimation. The graph prior extends the image prior to arbitrary graphs by placing a Gaussian prior on adjacent features in an arbitrary graph. The sparsity prior uses the Gini coefficient as a penalty, with minimizing it equivalent to maximizing likelihood under a specific prior distribution. The curr_chunk discusses the importance of drawing an adequate number of background samples for convergence of attributions when benchmarking a method. The benchmarking was done on the Correlated Groups 60 synthetic dataset, explaining all 1000 samples with background samples. Convergence was assessed by measuring the mean absolute difference between attribution matrices from different numbers of background samples, showing convergence around 100-200 background samples. During benchmarking experiments, it was found that 200 background samples were sufficient for well-converged attributions. The performance of expected gradients was compared with other methods using benchmark metrics proposed by Lundberg et al. (2019), including the Keep Positive Mask metric (KPM) to evaluate feature importance estimates. The Keep Positive Mask (KPM) metric measures the area under a curve that shows the impact of progressively removing features on model output. Expected Gradients outperformed all other attribution methods on various benchmark metrics. The Independent Linear 60 dataset consists of 60 features, with certain groups of 3 features having high correlation. Expected Gradients was found to be the best method across all metrics for this dataset. The Independent Linear 60 dataset has 60 features with high correlation in groups of 3. Expected Gradients was the most effective method for this dataset. Integrated gradients, on the other hand, fails to highlight important features when using a constant black image as the baseline input. The figure shows that integrated gradients fails to highlight black pixels in a VGG16 model trained from scratch on the CIFAR-10 dataset. Training details include using 200 sample points, stochastic gradient descent with a learning rate of 0.1 and exponential decay of 0.5 every 20 epochs, momentum of 0.9, image augmentation with shifts and rotations, batch size of 128, normalization of datasets, and using 1 background reference sample for attribution prior. In the study, per-pixel attribution maps are normalized before computing total variation to prevent arbitrary small variations. The robustness of the image attribution model was demonstrated by choosing \u03bb to minimize total variation while maintaining test accuracy within 10% of the baseline model. \u03bb values of 0.001 and 0.0001 for both gradients and expected gradients were found to provide modest robustness to noise. The study demonstrated the robustness of the image attribution model by selecting \u03bb values of 0.0001 for both gradients and expected gradients. A trade-off between accuracy and minimizing total variation was shown in Figure 7, with a clear elbow point where test accuracy degrades. The total variation of attributions is based on the attribution being penalized, either expected gradients or gradients. Test accuracy breaks down when using the immediate two values before the gradient and image attribution models. The study tested the image attribution model on MNIST using a CNN with specific layers and parameters. Training details include using the ADAM optimizer, dropout, and batch size of 50 images. A background reference sample was used per attribution during training. During training, a batch size of 50 images was used with k = 1 background reference sample per attribution. The optimal \u03bb value of 0.01 was chosen to minimize total variation of attributions while maintaining test error within 1% of the baseline model. Penalizing gradients and using an image attribution prior improved model robustness and accuracy on MNIST. The model with an image attribution prior highlighted digits more smoothly in attribution maps compared to the baseline model. In experiments on applying \u2126 pixel to classifiers trained on ImageNet 2012 challenge, a model with an image attribution prior highlighted digits more smoothly. The VGG16 architecture was used for fine-tuning with pre-trained weights from Tensorflow Slim package. Fine-tuning was done on ImageNet 2012 training set using cross entropy loss function and \u2126 pixel with asynchronous gradient updates. The training procedure outlined by Silberman and Guadarrama (2016) involves cropping and flipping images, normalizing them, and using gradient descent for optimization. Weight decay and fine-tuning with different penalty values are also utilized. Attribution maps are plotted using expected gradients on images from the validation set. Fine-tuning with the penalty results in sharper attributions. Fine-tuning with a penalty results in sharper and more interpretable image maps compared to the baseline network. Attribution maps generated by Expected Gradients show clearer results than other methods, indicating smoother pixel viewing by the network. Trade-offs between test accuracy and interpretability/robustness are observed, similar to findings by Ilyas et al. (2019). Validation performance of the VGG16 network before and after fine-tuning is shown in Table 3. The validation accuracy of the VGG16 network decreases after fine-tuning, but no hyperparameter search was conducted due to computational constraints. With more resources, a better balance between interpretable attribution maps and test accuracy could be achieved. RNA-seq gene expression data was carefully preprocessed for quality signal extraction. The RNA-seq gene expression data was preprocessed by calculating FPKM values, removing non-protein-coding transcripts, filtering out transcripts with low observations, log 2 transforming the data, and standardizing the expression levels across all samples. The gene expression data was preprocessed and standardized. Batch effects were corrected using the ComBat tool. Drug identity was used as a feature to increase sample size. Samples were carefully stratified for training, validation, and testing sets to focus on learning trends from gene expression data. We focused on learning trends from gene expression data by stratifying samples at a patient-level. The test set included 20% of total patients, while the validation set for hyperparameter selection included 20% of the training data. LASSO was implemented with optimal \u03b1 value of 10^-2, and Graph LASSO utilized the Adam optimizer in TensorFlow with a learning rate of 10^-5. The loss function involved weights vector w and graph laplacian L G from the HumanBase network. The hematopoietic stem cell network was used with a threshold for pairwise interactions. The optimal \u03bb value was selected from the regular LASSO model, and a range of \u03bd values were tested. A value of 10 was found to be optimal based on MSE on the validation set. Various feed-forward network architectures were tested for neural networks, with different hidden layer sizes. The optimal neural network architecture consisted of two hidden layers with sizes 512 and 256, an L1 penalty of 10^-3 on the weights, and a learning rate of 10^-5. Training was stopped after 120 epochs with no improvement on validation error for 20 rounds. The attribution prior was applied after tuning the network to optimal conditions. After tuning the neural networks to optimal conditions, extra epochs of fine-tuning were added using an alternating minimization approach. The attribution prior was tested using gradients as the feature attribution method. Graph convolutional networks were implemented following Kipf and Welling (2016), with architectures consisting of two hidden layers and specific parameter settings. The architectures searched for in the neural networks included a single graph convolutional layer followed by two fully connected layers of specific sizes. Hyperparameters were tuned, with the optimal settings being two hidden layers of size 512 and 256, an L2 penalty on the weights of 10^-5, a learning rate of 10^-5, and a dropout rate of 0.6. The predictive performance was improved using the graph prior. The use of graph prior in a neural network model significantly improves predictive performance compared to L1-regularization alone. A neural network without the graph information also performs well, but the best model includes the graph attribution prior. A t-test confirms the higher predictive performance of the model with the graph attribution prior. The addition of graph-regularization through fine-tuning ensures the improved performance is not solely due to additional training epochs. The study compared neural networks with and without a graph attribution prior. No significant difference was found in test error between models with and without the graph prior. Additionally, replacing the gene-interaction graph with a randomized graph did not significantly affect test error. The study compared neural networks with and without a graph attribution prior. Test error was not significantly different between the models. The network with a graph attribution prior outperformed graph convolutional neural networks. The explanation graph penalty was much lower in regularized models. The top attributed genes were enriched for AML-relevant pathways. The data for sparsity experiments used NHANES I survey data with 36 variables from 13,000 patients. The prediction task was binary classification of patient survival. Data was mean-imputed and standardized. A fixed train/validation/test split was used. In 100 experimental replicates, neural networks were trained to predict survival in NHANES data using various architectures and regularizers. ReLU activations and 2-class softmax output were used, with models running for 20 epochs on a size-100 training data. Different architectures and regularizers were tested, including single-hidden-layer networks and two-layer networks. Regularizers were tested in neural network experiments to predict survival in NHANES data. Optimal regularization strength was found for each regularizer. Italicized entries were evaluated in small-data experiments, while non-italicized entries were evaluated using full data. The best models had an average regularization strength of \u03bb = 1.60 \u00d7 10 \u22121. A Mixed L1/Sparse Attribution Prior was also tested but did not yield significant results. In neural network experiments for predicting survival in NHANES data, various regularization techniques were tested. The Sparse Group Lasso method was applied to encourage entire columns of the matrix to shrink together. The average optimal regularization strength was \u03bb = 1.62 \u00d7 10 \u22122. This penalty was similar to previous studies but only penalized the first-layer weight matrix. In neural network experiments for predicting survival in NHANES data, various regularization techniques were tested. The Sparse Group Lasso method penalized only the first-layer weight matrix. The model outperformed the SGL implementation but did not surpass other penalties like the Gini penalty or unregularized models. The average optimal regularization strength was \u03bb = 2.16 \u00d7 10 \u22123. Different penalties were applied, including L1 on the input layer, L1 on all layers, L1 on expected gradients, and L2 on the input layer. In neural network experiments for predicting survival in NHANES data, various regularization techniques were tested, including penalties on input layers, all layers, expected gradients, and global gradients. Different penalties like L2 on all layers, L1 on global gradients, and dropout were applied to achieve global sparsity. The study focused on achieving global sparsity in neural networks by using a Gini coefficient-based sparsity metric as a penalty. The average optimal regularization strength was \u03bb = 1.33 \u00d7 10 \u22121. Comparisons were made with methods such as L1 penalty on all layers, sparse group lasso methods, and L1 gradients penalty. In the study, various methods were evaluated for achieving global sparsity in neural networks, including L1 penalty, sparse group lasso methods, and L1 gradients penalty. The Gini gradients penalty was also assessed. Hyperparameters were selected based on validation performance, with one free parameter for most methods. Penalties were searched with 131 points sampled on a log scale over a specified range. The study evaluated various methods for achieving global sparsity in neural networks, including L1 penalty, sparse group lasso methods, and L1 gradients penalty. Some penalties produced NaN outputs for certain regularization settings, which were skipped after multiple restarts. Performance bar graph was generated by plotting mean test ROC-AUC of the best model of each type averaged over subsampled datasets. The study analyzed methods for achieving global sparsity in neural networks using various penalties. The sparsity bar graph was constructed based on Gini coefficients, while the Feature Importance Distribution Plot showed the distribution of feature importances using Lorenz curves. The curves were plotted using averaged mean absolute sorted feature importances from 100 replicates on subsampled datasets. The study analyzed methods for achieving global sparsity in neural networks using various penalties. For a given model, different q points represented the mean absolute feature importance of the least important features. Performance vs Sparsity Plot showed the tradeoff between model sparsity and validation ROC-AUC, averaged over 131 regularization strengths and 100 replicates. The scatterplot displayed the range of model sparsities and ROC-AUC performances, highlighting the sparse attribution prior as the only model achieving a smooth tradeoff between sparsity and performance. The sparse attribution prior model achieved a smooth tradeoff between sparsity and performance, outperforming other methods by a wide margin. Statistical significance was assessed using ROC-AUC comparisons and Wilcoxon signed-rank tests. The Gini coefficient was used to measure model sparsity. Additional penalties were studied, with confidence intervals from 100 experimental replicates. The sparse attribution prior model outperforms other methods significantly. The combination of EG and Gini coefficient penalties leads to better performance. The first-layer SGL slightly increases sparsity but does not outperform an unregularized model in ROC-AUC. The plain gradients Gini penalty also results in a small increase in sparsity. No method competes with the sparse attribution prior in sparsity or performance. The study compared different penalty methods for feature importance in machine learning models. Results showed that the sparse attribution prior model outperformed others in sparsity and performance. A range of penalties was explored to find the best tradeoff between sparsity and AUC-ROC performance. Fine parameter sweeps were conducted to optimize model performance. The study compared penalty methods for feature importance in machine learning models. The sparse attribution prior was the best performing and sparsest. Gini penalty and gradients L1 penalty followed. First-layer SGL did not improve models. Sparse attribution prior achieved highest performance and sparsity. Gini gradients penalty had slightly higher sparsity. Scatterplot showed sparse attribution prior excelled in most parameters, but first-layer SGL outperformed in one setting. The study compared penalty methods for feature importance in machine learning models. Sparse attribution prior was the best performing and sparsest, followed by Gini penalty and gradients L1 penalty. First-layer SGL did not improve models. Gini-based gradient penalty also built sparse models while maintaining performance, although less sparse than the sparse attribution prior. In one parameter setting, first-layer SGL outperformed other models in validation loss, but did not win in final test performance."
}