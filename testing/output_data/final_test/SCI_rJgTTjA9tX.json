{
    "title": "rJgTTjA9tX",
    "content": "Recent research has primarily focused on comparing different universal function approximators like neural networks, polynomials, and rational functions in a worst-case scenario. Classical tools from approximation theory are commonly used in this setting. Recent research has focused on comparing universal function approximators in worst-case scenarios using classical tools from approximation theory. However, in typical high-dimensional data applications, it is important to approximate the desired function well on the relevant part of its domain, such as a small manifold where real input data lies. The quality of approximation may not be uniform, especially in classification problems where accuracy near the decision boundary is crucial. These issues have remained unexplored until now. In a natural regression setting with sparse latent structure, neural networks and polynomial kernels are analyzed for performance. Theoretical analysis and simulations confirm the results, revealing qualitative differences from worst-case scenarios. The concept of representational power in machine learning is of interest due to various classes of universal approximators, but not all perform well empirically. In recent years, neural networks have become the class of choice for various tasks, inspiring theoretical study on their power, depth, architecture, generalization properties, and optimization procedures. Research compares the relative power of polynomial kernels and ReLU networks, with a focus on real-life settings. Neural networks have become popular for various tasks, leading to theoretical studies on their power, depth, and generalization properties. Research compares polynomial kernels and ReLU networks in real-life settings. Approximating a predictor function well on a relevant domain is crucial, rather than using a \"worst-case\" approach for measuring approximation. In real-life settings, neural networks are used for various tasks, with research focusing on their power and generalization properties. The importance lies in approximating a predictor function well on a relevant domain, especially when dealing with data distributions that have sparse latent structure. This sparsity assumption is natural in high-dimensional data and is key to methods like lossy image compression and compressed sensing. In the context of neural networks and data distributions with sparse latent structure, a regression task is considered where pairs of observables and labels are generated by a latent-variable process. The data distribution X is standard in setups like sparse linear regression, compressive sensing, and sparse coding, with an additional regression task attached to it. The labels Y are linearly generated by a predictor from the latent vector Z, with a focus on a slightly different interest than traditional setups. Our interest is different from traditional setups as we focus on how well different subsets of universal approximator families can fit data points (X, Y). We aim to understand if these classes can accurately reconstruct Y given X, compared to the Bayes-optimal estimator E[Y |X]. This helps determine the predictor's performance regardless of the training method used. The performance of predictors is measured in a distributional sense, with small two-layer ReLU networks achieving close to optimal rates. Polynomial predictors of degree lower than log m perform substantially worse, while those of degree O((log n)2) achieve optimal rates. Fitting a polynomial to data points requires searching through a high-dimensional space. Practical aspects of using polynomial kernels with lower degree have been a key concern in empirical research. The analysis shows that a degree greater than polylog(m) is not necessary for good statistical performance. Neural networks' ability to approximate polynomials and well-behaved functions has been extensively studied in recent work. Recent work has focused on approximating ReLU networks by polynomials, showing that good approximations can be achieved with polynomials of degree O(log^2(1/\u03b5)), without needing high degrees as classical results suggest. This contrasts with worst-case scenarios where higher degrees are required for approximation. BID0 studied approximating neural networks by rational functions, showing that polylog(1/\u03b5) degree rational functions can approximate bounded depth ReLU networks in L\u221e-norm. BID8 related quadratic activation networks to sigmoidal networks in the depth-2 case. Zhang et al. (2016) also studied neural network approximations. The work of Zhang et al. (2016) focused on the power of kernel regression methods to simulate certain neural networks with bounded depth and \"nice\" activation functions. However, their results do not apply to networks with ReLU activations, leading to exponential bounds in n due to the large 2 norm of the network's input vector. The curr_chunk discusses the analysis of kernel methods and neural networks' representation power, focusing on sparse recovery with specific assumptions on parameters. The upper bound via soft thresholding is highlighted, with formal results and techniques explained. The assumptions on the generative model's parameters are detailed, including sparsity of Z, incoherence of matrix A, and normalization of w. References to relevant literature on sparse recovery are provided. The curr_chunk discusses the importance of assumptions like the RIP property for guaranteeing the effectiveness of algorithms like LASSO in sparse recovery. The matrix being \"random-like\" is crucial, with results applicable even in high-dimensional settings. The notation A \u221e = max i,j |A i,j | is used for convenience. The reader can assume \u00b5 = 0 and n = m for simplicity, as the results remain interesting. The upper bounds are presented for a more general setting with \u00b5 \u2265 0, showcasing relevance in scenarios where m >> n. The lower bound is proven as well. The curr_chunk discusses the statistical rate achievable by small 2-layer ReLU networks in high-dimensional settings. The estimator for y in a 2-layer neural network is presented, with the size of the ReLU net comparable to the input. The estimator satisfies a certain condition with high probability. The curr_chunk discusses the statistical rate achievable by small 2-layer ReLU networks in high-dimensional settings. The estimator for y in a 2-layer neural network is presented, with the size of the ReLU net comparable to the input. The estimator satisfies a certain condition with high probability. The error of the estimator is essentially O(\u03c3^2k^2log(m)), with an additional factor of k that can be removed with technical effort. The analysis of this soft thresholding estimator is implicit in literature on sparse linear regression. A complete proof of Theorem 4.1 is included in Section A, showing that polynomials of degree smaller than O(log m) cannot achieve a \"nontrivial\" statistical rate. The curr_chunk discusses the statistical rate achievable by small 2-layer ReLU networks in high-dimensional settings. In the case where A is an orthogonal matrix, with specific noise and signal distributions, a theorem is presented. It states that for a multivariate degree d polynomial, the error of the estimator is related to the signal-to-noise ratio and the size of d with respect to m. The statement is given in terms of expectation, with a similar one possible. The curr_chunk discusses the need for polynomials of high degree to approximate ReLU functions closely. This is crucial for accurate reconstruction in neural network construction, especially in high-dimensional settings. The statement is given in terms of expectation, with a similar one possible with high probability. The need for high-degree polynomials to approximate ReLU functions closely in neural network construction is discussed. Surprisingly, it is shown that using only a polylog(m) degree polynomial can yield similar statistical performance. This is summarized by a theorem stating that the estimator satisfies certain conditions with high probability. The construction of the estimator involves novel methods that may be of independent interest. The proofs of the results are outlined in this section, with full details provided in the appendices. The estimator utilizes non-linearity to zero out small coordinates in the estimate, improving reliability in sparse regression. The estimator improves reliability in sparse regression by utilizing non-linearity to zero out small coordinates in the estimate. The proof of Theorem 4.2 highlights a structural lemma and an analysis of the decoupled estimator's bias-variance calculation in an appropriate basis. The optimal low-degree polynomial estimator decouples along the coordinates of the latent variable, leading to a simple structure for the optimal estimator for Y. The optimal estimator for Y reconstructs Z coordinate-wise and takes an inner product with w. The coordinates of Z are independent, leading to the optimal polynomial estimator having no mixed monomials. This is shown in Lemma 5.1, where a unique minimizer f * d exists for all degree d polynomials with no mixed monomials. In the proof overview, Fourier analytic methods are used to analyze the performance of low degree polynomials in a decoupled structure. Hermite polynomials are chosen due to their orthogonality with respect to the Gaussian distribution, leading to a bias-variance tradeoff analysis using Plancherel's Theorem. Theorem and Lemma 5.1 are used to analyze bias-variance tradeoff in predictors. Large Fourier coefficients lead to high sensitivity to noise, while small coefficients and low-degree functions result in high bias. Efficient use of Plancherel's theorem is crucial for proving these results. High-degree functions can balance bias and variance effectively by leveraging the noise and signal scales. In the context of bias-variance tradeoff in predictors, a polynomial approximation to ReLU of degree O(log 2 n) is designed to be close to 0 in the negative region while sacrificing accuracy near the point of non-smoothness. This approach leverages ReLU's \"denoising\" ability and is crucial for achieving a good regression rate. In the context of bias-variance tradeoff in predictors, a polynomial approximation to ReLU of degree O(log 2 n) is designed to be close to 0 in the negative region while sacrificing accuracy near the point of non-smoothness. Theorem 5.2 states that for R > 0, 0 < \u03c4 < 1/2, and d \u2265 7, there exists a polynomial for x \u2208 [0, R]. The proof involves a \"soft-max\" mollification of ReLU and synthetic experiments are conducted to verify the predictions. In an experiment, random orthogonal matrices and Gaussian noise are used to sample data for polynomial regression. The regression problem is reduced by observing structural properties, allowing for estimation of Y from X to be simplified. Results are shown in FIG2 on a log scale, with experiments conducted using k = 5 and \u03c3. The experiment results in FIG2 show that the log-error decays linearly with polynomial degree for low degrees. The baseline 2-Layer ReLU network was also evaluated, with the ReLU network performing slightly better despite the high degree. The study aims to provide representation bounds for different universal approximators in a statistical setup with sparse latent structure. The study explores sparse latent structure and challenges researchers to consider representational power beyond worst-case scenarios. Techniques involve designing polynomials that are accurate in specific regions, potentially useful in classification setups. An open problem is posed regarding sparse recovery guarantees for LASSO even when sparsity is close to the dimensionality. Sparse recovery guarantees for LASSO are explored even when sparsity is close to the dimensionality. The soft-thresholding estimator plays a key role in achieving near-optimal solutions using deep neural networks. The study aims to determine if shallower networks or polynomials of degree polylog(n) can also provide similar guarantees. The study explores sparse recovery guarantees for LASSO even with sparsity close to dimensionality. The soft-thresholding estimator is crucial for optimal solutions in deep neural networks. The analysis involves estimating the bias of A x without noise and analyzing the error in thresholding for \u00b5-incoherent matrices. The study focuses on sparse recovery guarantees for LASSO with sparsity close to dimensionality. The soft-thresholding estimator is essential for optimal solutions in deep neural networks. The analysis includes estimating the bias of A x without noise and examining the error in thresholding for \u00b5-incoherent matrices. In the main theorem, an upper bound is derived using Holder's inequality, and further analysis is done for orthogonal matrices with independent nonzero coordinates. The error estimate is improved to eliminate the extra factor of k without changing the algorithm. The study focuses on sparse recovery guarantees for LASSO with sparsity close to dimensionality. The soft-thresholding estimator is essential for optimal solutions in deep neural networks. An upper bound is derived using Holder's inequality for orthogonal matrices with independent nonzero coordinates, improving the error estimate by eliminating the extra factor of k without changing the algorithm. The construction of a k-sparse vector Z with independent coordinates is discussed, along with the choice of an orthogonal matrix A and a sign vector w. Linear predictors and low degree polynomials are shown to not achieve the information-theoretic rate, even as the degree of polynomials grows with n. The use of bias-variance trade-off is highlighted in the context of linear predictors. The linear predictor used in the context of bias-variance trade-off shows that it either has high variance or high bias. The theorem states that a linear estimator must make a square loss of \u2126(\u03b3 2 k) when the signal to noise ratio is not too high. This is particularly true when the signal is not significantly larger than the noise. The bias-variance trade-off in linear predictors shows that high variance or high bias is inevitable. A linear estimator incurs a square loss of \u2126(\u03b3 2 k) when the signal to noise ratio is low, indicating that the signal is not much larger than the noise. The optimal estimator structure is proven through lemma 5.1, demonstrating the minimization of square loss and risk. The optimal degree d polynomial f * d is the projection of w i E[Z i |X i ] onto the space of degree d polynomials, with no mixed monomials. Minimizing the squared loss for predicting Y is equivalent to minimizing the squared loss for the sparse regression problem of recovering Z, with an information theoretic rate of \u0398(\u03c3 2 k). The information-theoretic rate for predicting Y is \u0398(\u03c3 2 k), matched by Theorem A.1. The lower bound for polynomials uses Fourier analysis on Hermite polynomials, forming an orthogonal basis with respect to the Gaussian distribution. Plancherel's theorem is applied in this context. Using Plancherel's theorem, lower bounds on noise sensitivity of degree d polynomials can be obtained, analogous to variance. By expanding f Z in terms of the Fourier expansion of f, it is shown that if the bias is small for a low-degree polynomial, accurate prediction of y is not possible. The text discusses expanding a multivariate polynomial f of degree d in terms of Hermite polynomials. It then proves a theorem using Lemmas 5.1, B.1, and B.2, providing a variance-type lower bound. The text also mentions bounding the sum of coefficients and moments of a Gaussian, utilizing Holder's inequality and the reverse triangle inequality. In this section, it is shown that polynomial estimators with degrees lower than log n/ log log n have rates no better than the trivial zero-estimator. By bounding rare tail events, it is possible to derive a similar statement to Theorem 4.2 with high probability for low-degree polynomials. The growth rate estimates and Gaussian tails of the noise are used to demonstrate the behavior of these polynomials. In this section, polynomials achieving close to the optimal rate of degree O(log 2 m) are constructed. The key technical result is Theorem 5.2, which gives a new polynomial approximation to ReLU. By substituting this polynomial into the neural network, we derive the analogous version of Lemma A.2. The construction leads to important guarantees for the polynomial approximation. Theorem 5.2 provides guarantees for polynomial approximation of ReLU. By choosing appropriate parameters, statistical performance similar to ReLU network can be achieved. Theorem 5.2 guarantees polynomial approximation of ReLU with appropriate parameters for achieving statistical performance similar to ReLU network. The proof involves approximating ReLU with an \"annealed\" version and showing it can be well-approximated by low-degree polynomials based on its complex-analytic properties. Theorem C.2 states that the optimal rate of approximation for a function is determined by its complex-analytic properties. It involves bounding the coefficients of a Fourier series by using Chebyshev polynomials and contour shifting. This theorem will be applied to function g \u03b2. The theorem C.2 determines the optimal rate of approximation for a function based on its complex-analytic properties. It involves bounding Fourier series coefficients using Chebyshev polynomials and contour shifting. This theorem is now applied to function g \u03b2, showing its analytic properties on D \u03c11."
}