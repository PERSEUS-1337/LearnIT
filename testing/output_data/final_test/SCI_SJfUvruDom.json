{
    "title": "SJfUvruDom",
    "content": "Operating deep neural networks on devices with limited resources requires reducing memory footprints and computational requirements. A training method called look-up table quantization (LUT-Q) assigns weights to a dictionary's values, allowing for flexibility in network optimization. LUT-Q can generate networks with pruned weight matrices or powers-of-two dictionaries to eliminate multiplications. A multiplier-less version of batch normalization is introduced for fully multiplier-less networks. Experimental results demonstrate that LUT-Q outperforms other methods in image recognition and object detection tasks with the same quantization bitwidth. The proposed training method, Look-Up Table Quantization (LUT-Q), reduces the size and operations of deep neural networks by assigning weights to a dictionary's values. LUT-Q is flexible and can implement various weight compression schemes, such as generating networks with pruned weight matrices. The algorithm iteratively updates the assignment matrix and dictionary after each minibatch, offering advantages in network optimization. The LUT-Q method reduces the size of deep neural networks by assigning weights to a dictionary's values, allowing for various weight compression schemes like generating networks with pruned weight matrices. This approach enables the learning of networks with weights that are powers-of-two, reducing memory usage in affine/convolution layers. The LUT-Q method reduces deep neural network size by compressing weights and minimizing computations. Quasi multiplier-less networks avoid multiplications in most layers, while fully multiplier-less networks eliminate all multiplications. Extensive experiments were conducted on CIFAR-10 dataset. Extensive experiments were conducted with LUT-Q and multiplier-less networks on CIFAR-10, ImageNet, and Pascal VOC tasks using the Sony Neural Network Library. LUT-Q achieved error rates of 7.6% and 8.0% for 4-bit and 2-bit quantization on CIFAR-10. Pruning and quantization with LUT-Q showed a 70% network reduction with 2-bit quantization on CIFAR-10. For ImageNet, ResNet-18, ResNet-34, and ResNet-50 were used as reference networks. In experiments with LUT-Q on ImageNet, ResNet-18, ResNet-34, and ResNet-50 were used as reference networks. LUT-Q consistently outperformed other methods in terms of performance, except for ResNet-18 with 2-bit weight and 8-bit activation quantization. Notably, ResNet-50 with 2-bit weights and 8-bit activations achieved a 26.9% error rate, only 1.0% worse than the baseline, with a memory footprint of 7.4MB. The memory footprint for parameters and activations of the network is significantly reduced with LUT-Q, achieving a 20x reduction while maintaining a mAP above 70%. This was achieved by replacing the feature extraction network, using factorized convolutions, and quantizing weights to 8-bit with LUT-Q. Further reduction to 1.72MB was possible with 4-bit quantization. Different compression methods have been proposed in the past to reduce the memory footprint and computational requirements of DNNs. These methods include pruning, quantization, and teacher-student network training. Quantization methods can be classified into soft weight sharing, fixed quantization, and dictionary-based quantization. Our LUT-Q approach combines the best of fixed and trained quantization methods by updating both dictionary and weight assignments during training. This approach is similar to Deep Compression, where a dictionary is learned and weights are assigned using the k-means algorithm, but we iteratively update both assignments and dictionary at each mini-batch iteration. Look-up table quantization is a novel compression approach for DNNs. Look-up table quantization is a novel approach for reducing the size and computations of deep neural networks. The LUT-Q method updates quantization values and assignments after each minibatch, allowing for efficient pruning of weight matrices and training of multiplier-less networks. A new form of batch normalization is introduced to avoid multiplications during inference. Experiments are conducted with activations quantized uniformly to 8-bit, with potential for further optimization through non-uniform activation quantization. The text discusses the benefits of training quantized networks using a distillation strategy and the use of Look-up table quantization for efficient pruning of weight matrices. It introduces a new form of batch normalization to avoid multiplications during inference by quantizing parameters to powers-of-two. The text introduces a novel multiplier-less batch normalization approach that quantizes parameters to powers-of-two during inference, avoiding multiplications. This differs from BID2's shift-based batch normalization, which focuses on speeding up training by avoiding multiplications during training time."
}