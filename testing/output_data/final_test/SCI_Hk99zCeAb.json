{
    "title": "Hk99zCeAb",
    "content": "The new training methodology for generative adversarial networks involves progressively growing both the generator and discriminator to model fine details, speeding up training and stabilizing it. This approach has led to producing high-quality images like CelebA images at 1024^2 and achieving a record inception score of 8.80 in unsupervised CIFAR10. Implementation details are also discussed to prevent unhealthy competition between the generator and discriminator, along with a new metric for evaluating GAN results in terms of image quality and variation. Additionally, a higher-quality version of the CelebA dataset has been constructed. Generative methods like autoregressive models, variational autoencoders (VAE), and generative adversarial networks (GAN) are widely used for producing novel samples from high-dimensional data distributions, such as images. Autoregressive models like PixelCNN generate sharp images but are slow and lack a latent representation. VAEs are easy to train but produce blurry results, while GANs have strengths and weaknesses. Generative Adversarial Networks (GANs) are known for producing sharp images, although limited in resolution and variation. Despite recent progress, training GANs remains unstable. Hybrid methods combining strengths of different approaches still lag behind GANs in image quality. GANs consist of a generator and discriminator network, with the generator creating samples from a latent code that should match the training distribution. The discriminator assesses the generated samples, providing gradients to improve both networks. The discriminator in Generative Adversarial Networks (GANs) is an adaptive loss function that is discarded once the generator is trained. Different distance metrics have been proposed to measure the gap between training and generated distributions, such as Jensen-Shannon divergence and Wasserstein distance. Our contributions are independent of these discussions and focus on other aspects of GANs. Our key insight is to progressively grow both the generator and discriminator from low-resolution images, adding new layers for higher-resolution details as training progresses. This approach speeds up training and improves stability in high resolutions, addressing the challenges of generating high-resolution images. The text discusses methods for measuring variation in generative models and proposes a new metric for evaluating quality and variation. It also mentions a modification to network initialization to balance learning speed across layers and addresses mode collapses in GANs. The text discusses mode collapses in GANs, proposing a mechanism to prevent generator escalation. It evaluates contributions using various datasets and improves the inception score for CIFAR10. Additionally, a higher quality version of the CELEBA dataset is created for experimentation with resolutions up to 1024 \u00d7 1024 pixels. The dataset and implementation are available at the provided link. The primary contribution of the methodology for GANs is the incremental increase in resolution by adding layers to the networks, allowing for the discovery of large-scale structure before focusing on finer details. The generator and discriminator networks grow in synchrony, with all existing layers remaining trainable throughout the process. New layers are added to the networks to further enhance the training. Trained networks, result images, and a supplementary video can be found at the provided links. Progressive training involves smoothly fading in new layers to avoid shocks to well-trained smaller-resolution layers. This approach stabilizes training, making it possible to synthesize megapixel-scale images using WGAN-GP and LSGAN loss functions. Progressive training involves incrementally adding layers to the generator and discriminator, increasing the spatial resolution of generated images. This allows for stable synthesis in high resolutions and speeds up training. With progressively growing GANs, most iterations are done at lower resolutions, resulting in faster training times. The idea of growing GANs progressively involves using a single GAN instead of a hierarchy of them, unlike previous methods. This approach simplifies the learning process of complex mappings from latents to high-resolution images by deferring the introduction of pre-configured layers. Our approach simplifies GAN training by implementing minibatch discrimination to encourage similar statistics between generated and training images. This involves adding a minibatch layer to the discriminator without learnable parameters, improving variation in the generated images. Our simplified solution for GAN training involves adding a minibatch layer to the discriminator without learnable parameters, improving variation in generated images by computing standard deviations and replicating values across spatial locations. This layer can be inserted towards the end of the discriminator. Additionally, when doubling the resolution of the generator and discriminator, new layers are smoothly faded in. During the transition from 16 \u00d7 16 images to 32 \u00d7 32 images, new layers are smoothly faded in. The weight \u03b1 increases linearly from 0 to 1 for layers operating on higher resolution, treating them like a residual block. When training the discriminator, real images are downscaled to match the network's current resolution, interpolating between two resolutions. Alternative solutions to the variation problem include unrolling the discriminator to regularize updates. The problem of variation in GANs is addressed by different solutions such as unrolling the discriminator and adding a \"repelling regularizer\" to the generator. These methods aim to prevent signal magnitudes from escalating due to unhealthy competition between networks. Previous solutions involve using batch normalization, but the need for it in GANs is questioned as it may not address the variation issue effectively. In GANs, the issue of signal magnitudes and competition is addressed by a different approach that involves dynamically scaling weights at runtime instead of using careful weight initialization. This method aims to maintain scale-invariance in adaptive stochastic gradient descent methods like RMSProp and Adam. Our approach ensures that all weights have the same dynamic range and learning speed, independent of parameter scale. To prevent spiraling magnitudes in the generator and discriminator due to competition, we normalize feature vectors in the generator after each convolutional layer. Our approach involves normalizing feature vectors in the generator to prevent spiraling magnitudes. Existing methods like MS-SSIM reliably detect large-scale mode collapses but struggle with smaller effects like loss of variation in colors or textures. Automated methods are desirable for comparing GAN results, as manual comparison can be tedious and subjective. Our approach involves studying the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid BID6 representations of generated and target images. Starting at a low-pass resolution of 16 \u00d7 16 pixels, the pyramid progressively doubles until full resolution is reached, with each level encoding the difference to an up-sampled version of the previous level. Randomly sampling 16384 images, we extract 128 descriptors from each level in the Laplacian pyramid, resulting in 2.1M descriptors per level. Each descriptor represents a 7 \u00d7 7 pixel neighborhood with 3 color channels. We evaluate the similarity between patches from different levels of the training and generated sets using Wasserstein distance. The distance between patches from low-resolution images indicates similarity in large-scale structures, while finer-level patches encode pixel-level attributes. Experiments were conducted to assess the results, with detailed network structures and training configurations provided in the appendix. Additional result images and interpolations can be found in the accompanying video. In this section, we evaluate the importance of network structure, training configuration, and training loss using SWD and MS-SSIM metrics. We build upon WGAN-GP and BID16 in an unsupervised setting with CELEBA and LSUN BEDROOM datasets. SWD measures distance between generated and training images, while MS-SSIM assesses similarity among generated images at 128 \u00d7 128 resolution. Figure 3 displays CELEBA examples from Table 1, showcasing intentionally non-converged images. The dataset contains artifacts like aliasing and blur, challenging the generator to replicate faithfully. The training setups amplify differences with a low-capacity network structure and termination after 10M real images shown to the discriminator, resulting in non-fully converged results. Table 1 provides numerical values for comparison. The results in Table 1 show numerical values for SWD and MS-SSIM in various training configurations, with images generated from these setups displayed in Figure 3. MS-SSIM averages were calculated from 10000 image pairs, while SWD was determined as described in Section 5. Evaluation metrics like MS-SSIM may not capture the quality of images accurately, as seen in the comparison between different configurations. The SWD metric shows a clear improvement in training configurations, with batch normalization in the generator and layer normalization in the discriminator. Progressive growing of networks results in sharper images. Decreasing minibatch size from 64 to 16 leads to unnatural images, impacting both SWD and MS-SSIM metrics. Stabilizing training by adjusting hyperparameters is crucial for high output resolutions. In the study, adjustments were made to stabilize the training process by modifying hyperparameters and removing batch normalization and layer normalization. Minibatch discrimination did not improve metrics, but minibatch standard deviation showed enhancements in SWD scores and image quality. Further contributions led to an overall improvement in SWD and visual quality. The use of a non-crippled network and longer training resulted in generated images comparable to the best published results. The effect of progressive growing on SWD metric and image throughput was illustrated in FIG4. Progressive growing in the training configuration of BID16 offers benefits of converging to a better optimum and reducing training time by half. This is due to an implicit form of curriculum learning imposed by gradually increasing network capacity, allowing for quicker convergence to optimal values. Progressive growing in training with BID16 leads to quick convergence to optimal values, reducing training time by half. The largest-scale similarity curve reaches optimal value quickly and remains consistent, while smaller-scale curves level off as resolution increases. Non-progressive training results in all scales converging roughly together. Progressive growing gains a head start in training progress, with image throughput equalizing once full resolution is reached. The progressive variant of training with BID16 reaches 6.4 million images in 96 hours, while the non-progressive variant would take about 520 hours to reach the same point, offering a 5.4\u00d7 speedup. A high-quality version of the CELEBA dataset with 30000 images at 1024 \u00d7 1024 resolution was created to demonstrate results at high output resolutions efficiently. Selected 1024 \u00d7 1024 images produced by the network are shown in Figure 5. Our network produced high-quality 1024 \u00d7 1024 images, surpassing previous results in variety and perceptual quality. Appendix F contains more result images and nearest neighbors from the training data. A video showcases latent space interpolations and training progression. The network was trained on 8 Tesla V100 GPUs for 4 days, with adaptive minibatch sizes based on output resolution. The network was trained on 8 Tesla V100 GPUs for 4 days with adaptive minibatch sizes based on output resolution. Results include high-quality 1024 \u00d7 1024 images, surpassing previous variety and quality. Additional images and nearest neighbors are in Appendix F, with a video showcasing interpolations and training progression. In Appendix B, six examples of 1024 \u00d7 1024 images using LSGAN are shown, while Figure 7 displays selected examples from seven LSUN categories at 256 \u00d7 256. More results from all 30 LSUN categories are in Appendix G, with high overall quality noted. Inception scores for CIFAR10 are 7.90 for unsupervised and 8.87 for label conditioned setups. In the unsupervised setting, the Inception score is 7.90, while for label conditioned setups it is 8.87. The difference is due to \"ghosts\" between classes in unsupervised settings. With all contributions enabled, the score improves to 8.80. The network setup was similar to CELEBA, with a focus on minimizing ghosts using a customization to the WGAN-GP's regularization term. The results show high quality compared to previous work on GANs, with stable training in large resolutions. However, there is still a long way to achieve true photorealism, especially in terms of semantic sensibility and understanding dataset-dependent constraints. Improvements are needed in the micro-structure of images, but convincing realism may be achievable, particularly with the CELEBA-HQ dataset. Network architectures for the generator and discriminator are detailed, starting with 4x4 resolution and gradually increasing during training. The network architecture for the generator and discriminator starts with 4x4 resolution and gradually increases during training. The process involves alternating between fading in 3-layer blocks and stabilizing the networks for a total of 800k real images shown to the discriminator. The architecture includes convolutional layers with upsampling and downsampling steps to achieve different resolutions. The network architecture for the generator and discriminator gradually increases in resolution during training. Latent vectors are random points on a 512-dimensional hypersphere, and images are represented in [-1,1]. Leaky ReLU with leakiness 0.2 is used in all layers except the last layer, which uses linear activation. Batch normalization, layer normalization, and weight normalization are not employed, but pixelwise normalization is done after each Conv 3 \u00d7 3 layer in the generator. The generator and discriminator architecture increases in resolution during training. Bias parameters are initialized to zero, weights are initialized with normal distribution, and scaled at runtime. Across-minibatch standard deviation is injected as an additional feature map in the discriminator. Upsampling and downsampling operations correspond to element replication and average pooling. Adam is used for training with specific parameters. Exponential running average is used for visualizing generator output. Minibatch size is 16 for resolutions 4^2 - 128^2, gradually decreasing in size. The text describes the use of a minibatch size of 16 for resolutions ranging from 4^2 to 128^2, gradually decreasing to avoid memory issues. The WGAN-GP loss is used with n critic set to 1. A fourth term is added to the discriminator loss with a small weight to prevent output drift. Spatial resolution lower than 1024x1024 is achieved by omitting layers. Additionally, a lower-capacity version is used with reduced feature maps in Conv 3x3 layers. In Table 1 and FIG4, the training configuration for the \"BID16\" case closely follows the original setup with specific parameters. Modifications include using ReLU in the generator, batch normalization in place of pixelwise normalization, and layer normalization in the discriminator. Latent vectors consist of 128 components sampled independently from a normal distribution. LSGAN is less stable than WGAN-GP and tends to lose variation towards the end of long runs. WGAN-GP is preferred, but high-resolution images have been produced using LSGAN with additional techniques. One hack with LSGAN involves increasing Gaussian noise in the discriminator to prevent training from spiraling out of control. The process used to create the high-quality version of the CELEBA dataset involved taking in-the-wild images from the original dataset with varied resolutions and visual quality. Noise was added to the discriminator to improve image quality, with WGAN-GP BID16 showing stable results. To create the high-quality CELEBA-HQ dataset, various image processing steps were applied to focus on the facial region of individuals in the images. Preprocessing involved using neural networks to remove JPEG artifacts and enhance image quality. Padding and filtering were used to handle cases where the facial region extended outside the image. An oriented crop rectangle was selected based on facial landmark annotations to center the images on the face. The CELEBA-HQ dataset was created by enhancing the visual quality of images from the original CELEBA dataset through various image processing steps, including artifact removal, super-resolution, padding, filtering, and high-quality resampling. Facial landmark locations were used to select an appropriate crop region for the final image at 1024 \u00d7 1024 resolution. After enhancing images in the CELEBA-HQ dataset, a crop rectangle is calculated and transformed to 4096 \u00d7 4096 pixels, then scaled to 1024 \u00d7 1024 resolution. This process is applied to all 202599 images, with resulting images analyzed for quality. Images are sorted based on a frequency-based quality metric, favoring those with a broad range of frequencies and radial symmetry. The top 30000 images are selected for best results. The unsupervised setting yielded the best results in generating non-curated images, compared against prior art in terms of inception scores. The scores were reported in two ways: the highest score observed during training runs and the mean and standard deviation computed from the highest scores seen during training. No augmentation was used with the dataset. Metz et al. (2016) described a setup where a generator synthesized MNIST digits simultaneously to 3 color channels, achieving a 0.4% error rate. They generated a total of 25,600 images. The unsupervised setting achieved a record 8.80 inception score with 25,600 images generated. Results for MNIST discrete mode test using tiny discriminators were also reported. Additional results for CELEBA-HQ were presented, including nearest neighbors and generated examples. Mirror augmentation was enabled for all tests. The recently introduced Fr\u00e9chet Inception Distance (FID) BID18 was computed from 50K images across 30 LSUN categories. Separate networks were trained for each category using identical parameters, with mirror augmentation enabled for most categories except BEDROOM and DOG. Figure 18 shows larger collections of images corresponding to non-converged setups in Table 1, with training intentionally limited to highlight differences between methods. Nearest neighbors were found from the training data for CELEBA-HQ results using activations from five VGG layers. The text discusses the generation of images using the CELEBA-HQ dataset and LSUN categories, focusing on Sliced Wasserstein Distance (SWD) and Fr\u00e9chet Inception Distance (FID) metrics for evaluation. The comparison is made at different levels of image resolution, with specific values provided for SWD and FID. The text provides specific values for SWD and FID at different image resolution levels, with a larger set of generated images shown for non-converged setups."
}