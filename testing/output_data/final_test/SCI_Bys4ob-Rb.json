{
    "title": "Bys4ob-Rb",
    "content": "Neural networks struggle with adversarial perturbations, leading to low accuracy despite high performance on standard image classification tasks. Various defenses have been proposed, but new attacks often overcome them. This study focuses on one hidden layer neural networks and introduces a method using semidefinite relaxation to provide a certificate ensuring robustness against attacks. By optimizing this certificate with network parameters, the approach achieves a network that can withstand perturbations up to 0.1, resulting in no more than 35% test error on MNIST. Despite the impressive accuracies of machine learning on various tasks, classifiers still fail in the presence of small adversarial perturbations, exposing a vulnerability in current ML systems. The existence of \"adversarial examples\" highlights the challenge of constructing robust image classifiers that can withstand attacks. Proposed defenses may be successful against known attacks but new, stronger attacks often emerge, undermining their effectiveness. Proposed defenses against adversarial attacks may be effective initially but are often rendered useless by new, stronger attacks. To address this arms race between attackers and defenders, defenses need to be resilient against all attacks within a certain class. However, computing the worst-case error for a network against all adversarial perturbations is computationally challenging. One common approach is to approximate the worst-case loss using heuristic attack strategies like the Fast Gradient Sign Method or iterative methods. Adversarial training aims to minimize the loss with respect to these heuristics, but it essentially minimizes a lower bound on the worst-case loss. One approach to defending against adversarial attacks is to compute the worst-case perturbation exactly using discrete optimization methods. This method aims to bound the maximum of the gradient of the margin function over the neighborhood, providing a more accurate defense strategy. In this paper, an approach is introduced to compute an upper bound on the worst-case loss for neural networks with one hidden layer, based on a semidefinite relaxation. This method aims to provide a certificate of robustness against all attacks for a given network and input, offering a safer alternative to minimizing a lower bound. The paper introduces a method to compute an upper bound on worst-case loss for neural networks with one hidden layer using a semidefinite relaxation. This method provides a certificate of robustness against attacks, offering a safer alternative to minimizing a lower bound. The approach is trainable and acts as a regularizer to encourage robustness against attacks. The network trained on MNIST has a test error of 4.2% and a certificate that no attack can misclassify more than 35% of test examples using perturbations of size 0.1. The paper discusses score-based classifiers for learning mappings from input space X to class labels Y, driven by scoring functions. It focuses on evaluating classifiers using the 0-1 loss and defines the pairwise margin for class distinctions. The paper focuses on linear classifiers and neural networks with one hidden layer, evaluating them using the 0-1 loss. It discusses attackers perturbing features within a specified range. The paper discusses adversarial attacks on neural networks, focusing on the optimal attack strategy in a white-box setting. It introduces the adversarial loss function and different attack methods like FGSM and Carlini-Wagner attack. The discussion starts with binary classification and extends to multiclass scenarios. The text discusses the optimal attack strategy in a white-box setting for neural networks. It introduces the concept of the adversarial loss function and different attack methods. The optimal attack is successful if the margin of the incorrect class over the correct class is greater than 0. The paper reviews a classic result in linear networks and extends it to general classifiers, bounding the maximum 1-norm of the gradient at any point. For two-layer networks, this quantity is upper bounded by the optimal value of a non-convex quadratic program, which is further bounded by the optimal value of a semidefinite program. The text discusses the upper bounds of the adversarial loss function in the context of neural networks. It highlights the importance of computing the exact value of a semidefinite program for obtaining certificates. The analysis covers different types of classifiers, including linear classifiers, and introduces a linear approximation method using gradients for more general classifiers. The text discusses the importance of using gradients to obtain a linear approximation for generating A(x) in neural networks. However, the linear approximation g(x) becomes meaningless when defenses minimize the gradient locally at training points, resulting in sharp curvature in loss surfaces. Some attacks evade these defenses, leading to large adversarial loss values. The proposed alternative approach suggests using integration to derive an exact expression for f(x) in terms of gradients. The text discusses using integration to obtain an exact expression for f(x) in neural networks, considering gradients over the entire ball B(x). For two-layer neural networks, optimization has additional structure that can be exploited. The upper bound for two-layer neural networks is unpacked, focusing on bounding the norm of the gradient \u2207f(x) within B(x) by leveraging activation derivatives. The text discusses optimizing activation derivatives to obtain an upper bound on the adversarial loss in neural networks. By reparametrizing variables and following a similar approach to the semidefinite programming relaxation for MAXCUT, an upper bound on the adversarial loss is obtained. By reparametrizing variables and using semidefinite programming relaxation, a convex semidefinite relaxation of the problem is obtained. The optimization of the semidefinite program depends only on weights v and W, not on inputs x, and can be solved with off-the-shelf optimizers. A fast stochastic method for training is proposed in Section 4, which only requires computing the top eigenvalue of a matrix. The arguments generalize to pairwise margins f ij for multiple classes. In the previous section, an upper bound on the loss of any attack A was proposed based on classification loss. Training with hinge loss or cross-entropy encourages large pairwise margins but may not reduce the second term involving M ij. Regularized objective with hyperparameters \u03bb ij is suggested to address this. Duality theory helps in computing gradients efficiently. The theory of duality provides an equivalence between primal maximization and dual minimization problems, allowing for efficient training using stochastic gradient methods. An upper bound on adversarial loss is obtained directly from regularization loss, eliminating the need to optimize an SDP each time. Convex relaxations yield an efficient upper bound on adversarial loss, offering a simpler alternative to optimize the loss. One could use spectral and Frobenius norms to upper bound the loss. The spectral norm is the maximum singular value of W, while the Frobenius norm is often regularized for ease in training. Empirical comparisons were made on the MNIST dataset, showing tighter certificates of robustness than other bounds. In Section 6.1, certificates of robustness are shown to be tighter than those based on simpler methods like Frobenius and spectral bounds. However, the bounds are still too high for general networks. Training on the certificates in Section 6.2 results in networks with better bounds and meaningful robustness. Implementation details, design choices, and empirical observations are presented in Section 6.3 for two-layer networks with 500 hidden units using TensorFlow's Adam optimizer. In Section 6.3, different training objectives were considered for networks with more hidden units, but they did not substantially improve accuracy. Hyperparameters were tuned based on the error of the Projected Gradient Descent attack. Various training objectives were explored, including cross-entropy loss with no explicit regularization, hinge loss with regularizers, and cross-entropy with adversarial loss as a regularizer. The regularized loss was found to work better than optimizing only the adversarial loss. The PGD adversary was set with a step size of 0.1, 40 iterations, and perturbation size of 0.3. Various upper bounds on the adversarial loss were evaluated, comparing them to a lower bound computed by running PGD attack against cross-entropy loss. The attack against hinge loss was not effective, so cross-entropy was used instead. In experiments, upper bounds on adversarial loss were computed using different methods. An SDP bound was calculated for networks trained with a specific method, requiring solving an SDP at the end of training. YALMIP BID27 with Sedumi BID49 was used to solve the SDPs, taking around 10 minutes per SDP. Results showed the SDP bound to be tighter than the Frobenius bound. Our bound is tighter than the Frobenius and spectral bounds for all networks considered, but its tightness relative to the PGD lower bound varies. The only network robust and with tight upper bounds is SDP-NN, explicitly trained to be both robust and certifiable. The SDP bound, although tighter than simpler bounds, can still be loose on arbitrary networks. Optimizing against the SDP certificate can make it tighter. Different optimization objectives were explored, showing that the SDP certificate is a useful training objective for encouraging robustness compared to other regularizers. The SDP-NN network is robust and has tight upper bounds, explicitly trained for robustness and certifiability. The SDP-NN network is robust against various attacks, including PGD, Carlini-Wagner, and FGSM. It outperforms other bounds in the literature, showing substantial robustness without being explicitly trained against these attacks. The SDP-NN network is robust against attacks like PGD, Carlini-Wagner, and FGSM, outperforming other bounds in the literature. It has an error rate of 16% against the PGD attack and an upper bound of 35% against any attack, showing substantial robustness without explicit training against these attacks. The network misclassifies 6 out of 10 examples at = 0.05, with an error rate of 16% against the PGD attack. The AT-NN network, trained similarly to Madry et al., has an error rate of 11% against the PGD attack. LP-NN has a certificate that no attack can misclassify more than 26% of the examples, showing promise for deeper networks. Comparison with BID24's approach using linear programs to minimize adversarial loss is also discussed. LP-NN and SDP-NN are comparable in robustness against PGD attacks. SDP and LP approaches provide vacuous bounds for networks not trained to minimize the respective upper bounds. BID24's approach extends to deeper networks with a provable upper bound on adversarial error. The implementation of the Lanczos algorithm for fast top eigenvector computation is used, with a back off to full SVD when Lanczos fails to converge. Hinge loss is used as the classification loss, with a decayed learning rate. Gradient steps involve computing top eigenvectors for different matrices for each pair of classes. Regularization parameters are set equally for all pairs, leading to unweighted regularization. Tuning \u03bb to 0.05 resulted in good bounds, with certain class pairs having larger margins than others. The study observed that certain class pairs had larger margins, leading to a weighted regularization scheme. The weights were updated every 20 epochs, resulting in improved performance compared to unweighted networks. Dual variables provided a quick certificate of robustness, making it easy to track across training epochs. In this work, a method for producing certificates of robustness for neural networks is proposed, along with training against these certificates to obtain provably robust networks against adversaries. The approach uses a single semidefinite program to compute an upper bound on adversarial loss, while related work by BID24 provides provably robust networks against perturbations using convex relaxations. Experiments suggest that combining different approaches could be a promising future direction. BID21 and BID10 offer certificates of robustness for neural networks against perturbations using SMT solvers. However, these methods are slow due to exponential-time scaling and require various approximations. BID2 provide tractable certificates but need to be small to ensure robustness. Recently, Hein & Andriushchenko (2017) proposed a bound for guaranteeing robustness to p-norm perturbations based on the maximum p p\u22121-norm of the gradient. BID31 perform adversarial training against PGD on MNIST and CIFAR-10 datasets, claiming networks are \"secure against first-order adversaries\" based on empirical observations. The notion of a certificate in convex optimization differs from its use in neural network robustness. In the context of neural network robustness, certificates provide upper bounds on non-convex functions. Approaches from control theory literature can be used to verify robustness of neural networks by treating activations as a dynamical system and proving stability around a trajectory using sum-of-squares verification methods. Certificates provide upper bounds on non-convex functions for neural network robustness. Sum-of-squares verification methods like BID38 and BID43 are used for low-dimensional dynamical systems. Constructing provably robust network families could eliminate the need for model verification. Recent work on security in ML systems includes adversarial examples for neural networks, attacks, and defenses like gradient-based methods and adversarial training. Other attacks use saliency maps, KL divergence, and elastic net optimization. Recent work on security in ML systems includes attacks using saliency maps, KL divergence, and elastic net optimization. Some research focuses on detecting adversarial examples rather than making networks robust. However, it has been shown that most detection methods can be subverted by strong attacks. Different attack models exist beyond test-time attacks, such as data poisoning attacks. Certificates of performance for machine learning systems are also desirable in various settings. Certificates of performance are essential for deploying machine learning systems in critical infrastructure like air traffic control and self-driving cars. In robotics, certificates of stability are used for safety verification and controller synthesis. While verifying robustness for neural networks is challenging, there are possibilities to learn networks amenable to verification. Rice's theorem highlights the difficulty of verifying most properties of programs, but this work suggests that it is feasible to create verifiable neural networks. The paper discusses the possibility of training vector representations of natural images with strong robustness properties to address adversarial vulnerabilities in the visual domain. All code, data, and experiments for this paper are available on the Codalab platform."
}