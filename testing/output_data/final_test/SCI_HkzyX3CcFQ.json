{
    "title": "HkzyX3CcFQ",
    "content": "In this work, a Contextual Recurrent Convolutional Network was designed to incorporate feedback connections in a standard CNN structure. These connections allow lower layers to reconsider their representations with top-down contextual information, leading to improved performance in tasks like noise image classification and object recognition. This work aims to bridge the gap between computer vision models and the biological visual system. The primate's ventral visual system has a hierarchical structure, with deep convolutional neural networks imitating this structure. There is a correspondence between the internal feature representations of CNN layers and neural representations of different visual areas. Deep CNNs explain neuron responses in the ventral visual system. In this paper, the focus is on exploring different models with recurrent architectures and contextual modules to understand the computational advantages of feedback circuits in the visual system. Recurrent connections between neurons in different areas are crucial for object recognition under noise, clutter, and occlusion. In this study, the focus is on exploring the computational advantages of feedback circuits in visual tasks. The researchers investigated VGG16 BID18, a CNN approximating the ventral visual stream, and added feedback connections with contextual modules to improve performance. Various networks with contextual modules were tested against VGG16 in image classification tasks and object recognition under feedback guidance. The study focused on the benefits of feedback circuits in visual tasks, enhancing object recognition under degraded conditions and fine-grained recognition. The network outperformed baseline feedforward networks in occlusion tasks and internal feature representations illustrated its effectiveness. While recurrent network modules have been used in temporal prediction and video classification, few studies have explored augmenting feedforward CNNs with recurrent connections in image-based computer vision tasks. The inclusion of feedback connections in deep CNNs for image classification helps refine feature representations at lower layers using information from higher layers. This refinement process suppresses input signals from distracting objects, improving the final feature representation. Previous methods used output posterior possibilities for refinement, but this approach requires supervision. Another study utilized global and semantic features at higher convolutional layers to enhance local features. Our experimentation suggests that using global and semantic features at higher convolutional layers to sharpen local feature maps at lower layers for image classification may only be effective when there is a small semantic gap between the layers. Other studies have explored models with horizontal recurrent connections for contour detection and evaluated their performance on digit recognition tasks. However, the applicability of these findings to real-world computer vision problems remains uncertain. The study focuses on feedback and recurrent connections in the visual hierarchy, with a proposed model called Contextual Recurrent Convolutional Network (CRCN). The model aims to mimic recurrent connections in the brain, specifically focusing on feedback recurrent connections. This work complements a recent study that explores the computational benefits of local recurrent connections. The Contextual Recurrent Convolutional Network (CRCN) model includes feedback connections attached to some layers, with a contextual module that fuses top-down and bottom-up information to refine input. The model generates intermediate feature representations and output responses in multiple time steps, with each contextual module fusing output representations of lower and higher layers to generate refined input at each time step. The CRCN model incorporates feedback connections and contextual modules in its layers. The contextual module refines input by combining top-down and bottom-up information. The model generates intermediate feature representations and output responses in multiple time steps, with each contextual module fusing output representations of lower and higher layers to refine input. The CRCN model combines top-down and bottom-up information through a gate map generated by a tanh function. This controls the flow of contextual information by point-wise multiplication. To stabilize information flow, the new feature representation is added with the bottom-up feature map. Recurrent connections across too many layers can harm performance, so three sets of connections are derived to address this issue. The Contextual Recurrent Convolutional model combines top-down and bottom-up information through gate maps, enhancing contextual information flow. Connections between layers are optimized to prevent performance degradation. The model was tested on various image classification tasks, showing increased robustness against attacks. Top-1 accuracy on CUB-200 dataset was 74.90 for VGG-CRCN-2 model. Our model, based on VGG16 with 2 recurrent connections, showed improved performance on noise image classification, adversarial attacks, and occluded images. Despite limitations in CIFAR-10 datasets, our model outperformed the baseline VGG16 network by a large margin. Our model, based on VGG16 with 2 recurrent connections, outperformed other recent models on noise image classification. It also showed better results on the CIFAR-100 dataset compared to feedforward models. Additionally, our model demonstrated robustness by performing well on ImageNet with added Gaussian noise. Our model, utilizing two recurrent connections, outperformed feedforward VGG model on noise image classification. The performance gap increased with higher noise levels, as shown in Table 7 and FIG3. Adversarial attack tests revealed our model's robustness with lower fooling rates compared to other models. The contextual module helped preserve fine-grained details in feature representations. Our model, tested on CUB-200 and VehicleOcclusion datasets, outperformed the feed-forward VGG model and similar models. The model contains two recurrent connections and preserves fine-grained details in feature representations. The robustness of our model was further demonstrated on the VehicleOcclusion dataset, showing improved performance even with occlusions. Our model achieved significant improvement by adding more recurrent connections, leading to better classification accuracy and robustness in noise classification experiments. The additional loops in the model showed similarities to the reciprocal loops in the hierarchical visual cortex, enhancing its performance. The study implemented three additional structures to further explore the impact of top-down information and contextual modulation in the hierarchical visual cortex. These structures included Module 1 for top-down feature map gating contextual map, Module 2 for contextual map gating itself, and Module 3 for a combination of top-down and contextual map gating. The final output of these modules is the gating output added by bottom-up feature map. The study implemented three structures to explore the impact of top-down information and contextual modulation in the visual cortex. The best model was obtained by generating the gate map from the contextual map and using it to gate top-down information. Comparing it with Module 1, using only top-down information to control data flow was found to be inadequate. The study compared the best model with Module 2, finding that gating top-down information is essential. They used t-SNE visualization to analyze feature representations in different layers. By utilizing ImageNet bounding box annotations, they captured changes in intermediate convolutional layers. The Contextual Recurrent Network showed the ability to form a comprehensive understanding of visual data. The study compared the best model with Module 2, finding that gating top-down information is essential. They used t-SNE visualization to analyze feature representations in different layers. By utilizing ImageNet bounding box annotations, they captured changes in intermediate convolutional layers. The Contextual Recurrent Network showed the ability to form a more distinct clustering than VGG16 network, even with high noise added to the images. Results in both intermediate representations and final classification tasks showed consistent improvement over VGG16. The contextual module dynamics in recurrent connections not only refine low-level feature representation but also feedforward weights, resulting in better performance in computer vision tasks. The proposed Contextual Recurrent Convolutional Network utilizes recurrent connections between layers in a feedforward deep convolutional neural network. The new network with recurrent connections in a deep convolutional neural network shows robust properties in computer vision tasks and shares similarities with the biological visual system. The study aims to bridge the gap between computer vision models and real biological visual systems. Additional contextual modules were tested, with detailed formulations provided. The study introduced a new network with recurrent connections in a deep convolutional neural network to improve computer vision tasks. Contextual modules were added to control information flow and enhance performance. Examples of image occlusion and adversarial noise tasks were demonstrated. The study demonstrated examples of image occlusion and adversarial noise tasks, showing the impact of noise on neural networks. Results of Imagenet Top1 accuracy were presented, comparing different models and levels of noise. The study compared the standard VGG16 model with BID11's \"VGG-LR-2\" model."
}