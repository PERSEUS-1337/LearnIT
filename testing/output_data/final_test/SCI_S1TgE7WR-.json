{
    "title": "S1TgE7WR-",
    "content": "Most existing neural networks for learning graphs use a message passing scheme where nodes sum feature vectors from neighbors. A new architecture called Covariant Compositional Networks (CCNs) is proposed to address limitations in representation power. CCNs ensure covariance by transforming neuron activations based on a tensor representation of the permutation group. Experiments show CCNs outperform other methods on graph learning benchmarks. Learning on graphs has a long history in the kernels literature, with various approaches such as random walks, counting subgraphs, spectral ideas, label propagation schemes, and algebraic ideas. Recently, neural network based approaches have emerged to address graph representation problems, integrating classification or regression tasks in an end-to-end system. This has led to a surge in research activity in this area, with proposed graph learning architectures seeking inspiration from different sources. Some proposed graph learning architectures draw inspiration from classical CNNs used in image recognition. These methods involve fixing a vertex ordering, moving a filter across vertices, and computing local neighborhood information to build a deep graph representation. Other works on graph neural networks have also been influential, with recent research showing that many approaches can be seen as instances of message passing neural networks (MPNNs). MPNNs have been successful in applications and are actively researched, differing from classical CNNs in their internal feature representations. MPNNs differ from classical CNNs by being fully invariant to transformations like translation and rotations. However, this limitation restricts their representation power. To address this, a new class of neural network architectures called compositional networks (comp-nets) is introduced to study covariance behavior and improve information propagation. The paper emphasizes the connection to convolutional networks and introduces the concept of steerability in comp-nets. It discusses the transformation of activations at neurons based on the symmetry group of the receptive field, leading to tensor activations of different orders. Each channel in the network corresponds to a specific way of contracting higher order tensors to lower order ones. The covariant comp-nets architecture utilizes multidimensional arrays of activations that transform under permutations. The mixing matrix parameters dictate channel communication at each node, outperforming scalar message passing neural networks on standard datasets. Graph learning involves problems with graph inputs and class label outputs in supervised learning settings. Graph learning involves predicting labels for graphs not in the training set by learning a function h : G \u2192 y. Each graph G is represented as a pair (V, E) where V is the vertex set and E is the edge set. Graphs have no self-loops and are symmetric. Edges can have weights and vertices can have feature vectors. This is important in scientific applications where features encode information like atom types in molecules or protein identities in biochemical networks. Graph learning involves representing graphs as adjacency matrices and labeled graphs require additional information to fully specify them. Graphs contain structure at multiple scales, making graph learning challenging. An ideal algorithm would capture structure at the level of individual atoms, functional groups, interactions between groups, subunits, and overall shape of proteins. Storing and presenting graphs to learning algorithms is critical for successful graph learning. The usual ways to store and present graphs to learning algorithms have a critical spurious symmetry. Permuting the vertices of a graph changes the adjacency matrix and vertex labels, but the topological representation remains the same. This symmetry poses a challenge for feature mapping in graph learning. The feature map \u03c6 (and consequently the algorithm A) is permutation invariant if, given any n \u2208 N, any n vertex labeled graph G = (A, l 1 , . . . , l n ), and any permutation \u03c3 \u2208 S n, letting G = (A , l 1 , . . . , l n). In graph learning literature, capturing multiscale structure and respecting permutation invariance are key constraints. Many papers employ a compositional approach, building up graph representations from subgraphs. A general architecture called compositional networks (comp-nets) is introduced for this purpose. Compositional networks (comp-nets) are introduced as a general architecture for representing complex objects as a combination of their parts. Existing graph neural networks can be viewed as special cases of this framework, where each node in a directed acyclic graph is associated with subsets of elementary parts. Each node also carries a feature vector to represent the corresponding part. Figure 2 shows a composition scheme for an object G, where a DAG represents atoms and sets of atoms. Each node in the compositional network carries a feature vector computed from its children's feature vectors. Figure 3 highlights the requirement for composition schemes to be invariant to permutation. The compositional network N is a DAG with nodes representing atoms and feature vectors computed from children's feature vectors. The representation \u03c6(G) is given by the feature vector f r of the root node. The compositional network N is a DAG with nodes representing atoms and feature vectors computed from children's feature vectors. The feature vectors can be treated as tensors to describe their transformation with respect to permutations. Comp-nets combine information from graphs at different scales, with atoms usually being vertices and parts corresponding to clusters of nodes or neighborhoods. This formalism also satisfies the permutation invariance criterion. The composition scheme of object M is permutation invariant, with an algorithm generating M that permutes atoms based on a bijection. If the aggregation function used is invariant to permutations, the overall representation \u03c6(G) is also invariant. This leads to a permutation invariant graph representation in graph learning. Graph learning, like convolutional neural networks (CNNs) in image tasks, emphasizes invariance and multiscale structure. CNNs aggregate information hierarchically, similar to a compositional network, with pixels as atoms. This connection has led to framing graph learning as a generalization of convolutional nets to the graph domain. The CNN analogy suggests defining aggregation functions linearly followed by a nonlinearity for graph comp-nets. The composition scheme M is defined in layers, connecting nodes based on graph neighbors. In layer L+1, a single node represents the entire graph and collects information from all nodes at level L. The resulting composition scheme is permutation invariant and can be interpreted as a label propagation algorithm. Some authors describe graph neural networks as message passing neural networks, where different graph learning architectures are special cases. The classic Weisfeiler-Lehman test of isomorphism and the Weisfeiler-Lehman kernel are successful approaches to graph learning. Label propagation and message passing algorithms define the source domain of vertices, corresponding to the receptive field of neurons in comp-nets. Message passing neural networks produce permutation invariant representations of graphs. Invariant message passing networks are limited in representation power, but a generalization via comp-nets can overcome these limitations. They are not the most general compositional models for producing permutation invariant representations of graphs. Analogous to image recognition, CNNs behave quasi-invariantly to translations. CNNs exhibit equivariance with respect to translations, where the activation of a neuron is transferred to another neuron based on the translation amount. However, rotations complicate the situation as both the position and orientation of receptive fields change. This results in features being picked up by different filters. It is challenging to construct a CNN that is quasi-invariant to both translations and rotations. CNNs exhibit equivariance with respect to translations, where activations transform predictably. Rotations complicate the situation by changing both position and orientation of receptive fields, leading to features being picked up by different filters. Steerability in CNNs allows for reversible transformations of activations. In steerable CNNs, activations are rotated by 90 degrees using horizontal and vertical filters. Quasi-invariance in comp-nets ensures that activations at any node remain the same after permutations. The concept of quasi-invariance in comp-nets states that node activations should only depend on the set of atoms in the receptive field, not their internal ordering. This can lead to loss of information about the contribution of each vertex in the receptive field, affecting the orientation of the field and aggregation of feature vectors in the network. The solution involves upgrading the receptive fields to ordered sets and establishing how they co-vary with internal ordering. The concept of covariance to permutations is introduced, ensuring that node activations are dependent on the set of atoms in the receptive field, not their internal ordering. The text discusses the assumption of linear maps in steerable representations, leading to the formation of matrices that represent a group in a group theoretic sense. Additionally, it illustrates how information is aggregated and shared among nodes in a graph structure. The text delves into the representation theory of symmetric groups, specifically focusing on the defining representation of S m using permutation matrices. This representation dictates the transformation rules of activations in a comp-net, where each activation must be a |P i | dimensional vector. The curr_chunk discusses first order permutation covariance in comp-nets, where activations transform under permutations of the receptive field. It also introduces P \u03c0 \u2297 P \u03c0 -covariant comp-nets, where feature vectors become |P i | 2 dimensional and transform under internal ordering permutations. The curr_chunk discusses higher order covariance in comp-nets, where activations transform under permutations of the receptive field. It introduces second order covariant nodes and mentions the possibility of considering third, fourth, and general k'th order nodes in compnets. The curr_chunk introduces the concept of k'th order Ptensor in comp-nets, emphasizing the transformation properties of quantities. It unifies notation by referring to feature tensors and activations as F i instead of f i. The curr_chunk discusses the transformation properties of activations in comp-nets, emphasizing the use of k'th order P-tensors. It explains how tensor arithmetic can be used to derive aggregation functions in comp-nets. The curr_chunk explains tensor operations such as projection, contraction, and Einstein notation in the context of k'th order tensors. The curr_chunk discusses tensor operations like projection, contraction, and Einstein notation for k'th order tensors, showing how these operations preserve the behavior of P-tensors under permutations. The curr_chunk discusses the aggregation scheme for comp-nets, focusing on how to relate P-tensors at different nodes in a comp-net. Propositions are provided to explain the relationship between nodes and how P-tensors are aggregated. The curr_chunk explains the process of promoting P-tensors in comp-nets for covariance. Propositions are given to show how children nodes' activations are promoted to P-tensors of the parent node. This ensures topological information is added to the adjacency matrix. The process of promoting P-tensors in comp-nets for covariance involves using Propositions to show how children nodes' activations are promoted to P-tensors of the parent node, adding topological information to the adjacency matrix. The general scheme involves mixing lower order tensors with a matrix and applying a nonlinearity to get the final activation of the neuron. Contractions are used to counteract the tendency for tensor order to increase at every node, pulling it back to a small number. The algorithm involves mixing channels by W to stabilize the number of channels. The only learnable parameters are the entries of the W matrix and bias terms, learned through stochastic gradient descent. Our scheme involves learning in the network through stochastic gradient descent. The framework allows for various design choices, but the overall structure is determined by the covariance constraint. The final output of the network must be permutation invariant, with the root node producing a tuple of zeroth order tensors. Graph representation algorithms compute \u03c6(G) by summing activations at level L or creating histogram features. Special cases show how tensor aggregation relates to conventional message passing rules. When input and output tensors are constrained to be scalars, the aggregation algorithm simplifies to a basic formula. For neural networks, this simplification is too basic, but the Weisfeiler-Lehmann isomorphism test builds on a similar formula with a specific choice of parameters. If more channels are allowed in inputs and outputs, a matrix is formed, leading to a more complex aggregation process. In neural message passing algorithms, when more channels are allowed in inputs and outputs, a matrix is formed, leading to a more complex aggregation process. This aggregation function transforms a single input channel to two output channels. In neural message passing algorithms, the aggregation process involves doubling the number of channels in each subsequent layer. The second order tensor aggregation results in three different matrices and the weight matrix is 3x3. The first nontrivial tensor contraction case occurs when multiplying with A\u2193 Pt, resulting in 50 different ways to contract a 5th order tensor down to second order. In neural message passing algorithms, tensor contractions involve various ways to contract tensors along different dimensions. The tensor T can be contracted in 10 ways for the \"1+2\" case, 30 ways for the \"1+1+1\" case, and 10 ways for the \"3\" case. Including symmetries, there are a total of 18 contractions. The activations of vertices in the receptive field are stacked into a 3rd order tensor, undergo a tensor product operation with the adjacency matrix, and are contracted in different ways. In neural message passing algorithms, tensor contractions involve various ways to contract tensors along different dimensions. The tensor T can be contracted in different ways. In this figure, we only consider single channel, each channel is represented by a 5th order tensor. We compared the second order variant (CCN 2D) of our CCNs framework to several standard graph learning algorithms on datasets involving learning the properties of molecules from their structure. The dataset contains molecules with 13 target properties to predict, using chemical graph and atom node labels. Target variables were normalized for experiments. MAE and RMSE were reported for all learning targets. Graph kernel datasets include MUTAG, PTC, NCI1, and NCI109. Comparison was made between CCN and other algorithms like lasso, ridge regression, random forests, and gradient boosted trees on the HCEP dataset. Baseline methods such as random forests, gradient boosted trees, and neural graph fingerprints were used for molecule feature extraction. Different parameters were tuned for each method, including the number of trees, depth, and learning rate. The \"patchy-SAN\" convolutional algorithm was also employed with specific configurations. The study compared their second order CCN method with graph kernel results from BID22, neural graph fingerprints, and PSCN on various datasets. The CCN method initialized vertex features with histogram alignment features and concatenated them to create base labels for each vertex. The study compared their second order CCN method with graph kernel results from BID22, neural graph fingerprints, and PSCN on various datasets. The CCN method utilized unique contractions to create additional channels and applied learnable weight matrices to compress channels. Training was done on 80% of the dataset, with validation on 10% and testing on the remaining 10%. Adam optimization with a learning rate of 0.001 was used, decaying linearly towards 10^-6. A custom Deep Learning framework was developed for the experiments. Our custom Deep Learning framework, GraphFlow, supports symbolic/automatic differentiation, dynamic computation graphs, tensor operations, and GPU acceleration. Covariant Compositional Networks and other graph neural networks are implemented using GraphFlow. High-order tensors are handled using a virtual indexing system to avoid storing them explicitly in memory. Source code available at https://github.com/HyTruongSon/GraphFlow. CCN outperforms other methods on the subsampled HCEP dataset and wins on MUTAG and PTC for graph kernels datasets. Neural network approaches struggle on small datasets like NCI1 and NCI109. In QM9 experiments, CCN beats other algorithms in mean absolute error and root mean squared error. CCNs, a general framework for constructing covariant graph neural networks, outperformed other algorithms on benchmark datasets. The experiments showed superior results compared to state-of-the-art methods, especially on QM9. The proposition states that for every node in G, the function remains the same. This is proven by showing that nodes with equivalent receptive fields have equivalent functions. The proposition states that under the action of permutations, certain tensors transform in a specific way, ensuring that the function remains the same for nodes with equivalent receptive fields. The proposition explains how tensors transform under permutations, ensuring consistency for nodes with equivalent receptive fields. \u03c0 permutes slices according to DISPLAYFORM4, making F a k + 1'th order P-tensor. A\u2193Pi transforms to A\u2193Pi under any permutation \u03c0 \u2208 Sm of Pi, resulting in a second order P-tensor. Consequently, F \u2297 A\u2193Pi becomes a k + 2'th order P-tensor."
}