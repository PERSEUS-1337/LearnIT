{
    "title": "HJxhUpVKDr",
    "content": "In the context of multi-task learning, neural networks with branched architectures are used to jointly tackle tasks. These networks start with shared layers before branching out into task-specific layers. Prior methods for determining layer sharing have been ad hoc or expensive with neural architecture search. This paper proposes a principled approach to automatically construct branched multi-task networks based on task affinities within a specific parameter budget. The proposed approach for multi-task learning generates architectures with task-agnostic shallow layers and task-specific deeper layers. Experimental analysis shows that this method consistently produces high-performing networks with fewer learnable parameters compared to traditional deep neural networks. This approach is inspired by the multi-tasking strategy observed in biological data processing, where different tasks share early processing layers in the brain. Multi-task networks with branched architectures aim to improve generalization and processing efficiency by jointly learning related tasks. They have advantages such as lower memory footprint, faster inference speed, and potential outperformance of single-task networks. Multi-task networks aim to improve generalization and processing efficiency by jointly learning related tasks. Designing these networks poses challenges in deciding on shared layers among tasks. Researchers have explored alternatives like routing, stochastic filter grouping, and feature partitioning to address this issue. In this paper, a novel approach is proposed to determine the degree of layer sharing between tasks in multi-task networks. The approach is based on measurable levels of task affinity or relatedness, where tasks sharing similar features are considered strongly related. This method aims to eliminate the need for manual exploration and improve performance by avoiding negative transfer between unrelated tasks. Our approach uses representation similarity analysis (RSA) to measure task affinity in a neural network, allowing for the construction of a branched multitask network. The task clustering algorithm groups similar tasks together and separates dissimilar tasks, reducing negative transfer between unrelated tasks. Our method uses representation similarity analysis to construct a branched multitask network, grouping similar tasks together and reducing negative transfer between unrelated tasks. Extensive empirical evaluation shows the superiority of our approach in terms of multi-task performance versus computational resources. In the deep learning era, MTL models can be classified as utilizing soft or hard parameter sharing. Soft parameter sharing assigns each task its own set of parameters and uses a feature sharing mechanism to handle cross-task communication. Cross-stitch networks share features among tasks through a linear combination of activations. Sluice networks extend this concept by allowing selective sharing of layers, subspaces, and skip connections. Multi-task attention networks use an attention mechanism to share a general feature pool among task-specific networks. MTL networks can use soft or hard parameter sharing. Hard parameter sharing divides parameters into shared and task-specific ones. Various MTL models use hard parameter sharing with a shared encoder and task-specific decoders. Some models extend this with tensor normal priors or hierarchical structures. Branching points in these approaches are often ad hoc, leading to potential negative transfer. In contrast to ad hoc task groupings in traditional MTL networks, our branched multi-task networks determine layer sharing based on task affinities. This method clusters tasks using feature affinity scores and establishes a tree structure offline, promoting optimal task groupings. Neural architecture search (NAS) aims to automate network architecture construction, with different algorithms characterized by search space, strategy, and performance estimation. Existing works on NAS are often limited to task-specific models due to the complexity of jointly optimizing layer sharing with layer types and connectivity in MTL. Transfer learning is used to measure levels of task affinity in the context of neural architecture search. Unlike traditional NAS methods, recent works start from a predefined backbone network and automatically determine a layer sharing scheme. This approach is more efficient than building the architecture from scratch and allows for exploring alternatives like routing, stochastic filter grouping, and feature partitioning. The approach proposed by Dwivedi & Roig (2019) uses RSA to measure task affinity by correlating models pretrained on different tasks. Loss weighting in multi-task learning involves balancing loss functions, gradient normalization adjusts gradient magnitudes, and dynamic task prioritization focuses on difficult tasks. Zhao et al. (2018) noted that competing tasks can be detrimental. In this paper, the authors aim to jointly solve N different tasks given a computational budget, using a backbone architecture with shared layers and task-specific layers. The approach proposed by Dwivedi & Roig (2019) uses RSA to measure task affinity, while Zhao et al. (2018) proposed a modulation module to alleviate destructive interference in gradient caused by competing tasks. The proposed method aims to group related tasks together in the same branches of the tree by finding effective task grouping for the sharable layers of the encoder. Task affinity scores are derived at various locations in the encoder to construct a branched multi-task network within the computational budget. Representation dissimilarity matrices are calculated using RSA to measure task affinity at predefined locations in the encoder. The proposed method groups related tasks in the same branches of the tree by finding effective task grouping for the sharable layers of the encoder. Representation dissimilarity matrices are calculated using RSA to measure task affinity at predefined locations in the encoder, resulting in a branched multi-task network within a computational budget. The proposed method uses RSA to measure task affinity scores and group related tasks in the same branches of a branched multi-task network within a computational budget. Single-task models are trained for each task using a shared encoder and task-specific decoder. The proposed method uses RSA to measure task affinity scores and group related tasks in the same branches of a branched multi-task network within a computational budget. Only task-specific operations are performed in the decoder, which is smaller in size compared to the encoder. Different single-task networks are trained under the same conditions. Task affinities are calculated by comparing representation dissimilarity matrices in selected locations of the shared encoder. To compare task affinities, representation dissimilarity matrices (RDM) of single-task networks trained in the previous step are compared at specified locations. A subset of K images is used to compare feature representations, filling a tensor of size D \u00d7 K \u00d7 K with dissimilarity scores 1 \u2212 \u03c1. RDMs are symmetrical with a diagonal of zeros, measuring similarity between upper or lower triangular parts. The task affinity is measured by comparing the similarity between RDMs of single-task networks at specific locations using Spearman's correlation coefficient. The method focuses on features used to solve tasks rather than task difficulty. The sharable encoder's layers are shared among tasks based on a computational budget. Each layer is represented as a node in a tree structure. The tree structure represents layers of the sharable encoder, with nodes at different depths corresponding to different layers. Each node splits into branches, with task-specific decoders at the leaves. The branched multi-task network aims to separate tasks by dissimilarity scores, minimizing the sum of these scores. The branched multi-task network minimizes task dissimilarity scores by calculating task affinity scores a priori and selecting the tree that minimizes the dissimilarity score. The task dissimilarity score of a tree is defined as the average maximum distance between dissimilarity scores of elements in each cluster, encouraging separation of dissimilar tasks. This approach considers clustering cost at all depths to find an optimal task grouping globally. The proposed method for task grouping in a branched multi-task network involves deriving a tree in a top-down manner, using spectral clustering at each step to minimize task dissimilarity globally. A beam search approach is employed to select top-n task groupings with minimal cost at each layer, ensuring efficient clustering of tasks. This method is evaluated quantitatively and qualitatively in CelebA experiments. The proposed method is evaluated on diverse multi-tasking datasets like Cityscapes, focusing on urban scene understanding with tasks like semantic segmentation, instance segmentation, and monocular depth estimation. A ResNet-50 encoder with dilated convolutions and a Pyramid Spatial Pooling decoder are used, with images rescaled to 512 x 256 pixels. Instance segmentation follows the approach from Kendall et al. (2018). The study evaluates a method for multi-tasking on datasets like Cityscapes, focusing on tasks such as semantic segmentation, instance segmentation, and monocular depth estimation. Results are obtained after a grid search on hyperparameters for fair comparison. Task affinity decreases in deeper layers of the model, with features becoming more task-specific. Performance of task groupings is compared with other approaches, and training is done on possible task groupings derived from branching the model in the last three ResNet blocks. Visualizations show performance vs number of parameters. The study evaluates a method for multi-tasking on datasets like Cityscapes, focusing on tasks such as semantic segmentation, instance segmentation, and monocular depth estimation. Results are obtained after a grid search on hyperparameters for fair comparison. Task affinity decreases in deeper layers of the model, with features becoming more task-specific. Performance of task groupings is compared with other approaches, and training is done on possible task groupings derived from branching the model in the last three ResNet blocks. Visualizations show performance vs number of parameters. Our method generates specific task groupings based on computational budget, achieving higher performance compared to other methods. Branched multi-task networks are compared with cross-stitch networks and NDDR-CNNs, showing competitive performance. Our branched multi-task networks strike a better trade-off between performance and number of parameters compared to other models like cross-stitch or NDDR-CNN. The Taskonomy dataset contains images annotated for various tasks, and we focus on scene categorization, semantic segmentation, edge detection, monocular depth estimation, and keypoint detection. The selected tasks are diverse yet manageable for computations. The study focuses on utilizing a diverse set of tasks from the Taskonomy dataset, using a dataset split of 275k train, 52k validation, and 54k test images. The architecture includes a ResNet-50 encoder and a 15-layer fully convolutional decoder for pixel-to-pixel prediction tasks. Results are compared against previous methods, with task affinity measured after each ResNet block. The study limits itself to three architectures due to the increased number of tasks, and the numerical results are presented in Table 2. The study compares the performance of different models on a diverse set of tasks from the Taskonomy dataset. Results show that the models consistently outperform previous methods, with multi-task performance being influenced by task groupings. Branched multi-task networks handle diverse tasks positively, unlike cross-stitch networks and NDDR-CNNS. The study demonstrates the ability to solve multiple heterogeneous tasks. In a study by Maninis et al. (2019), it is shown that solving multiple heterogeneous tasks simultaneously is possible with limited negative transfer by separating dissimilar tasks. Their approach demonstrates consistent performance across various multi-tasking scenarios and datasets, unlike existing approaches tailored for specific cases. The Ours-Thin-32 and Ours-Thin-64 architectures are optimized for different parameter budgets, showing stable performance in different experimental setups. The CelebA dataset (Liu et al., 2015) contains over 200k real images of celebrities, labeled with 40 facial attribute categories. The training, validation, and test set contain 160k, 20k, and 20k images respectively. Various models have been tested on this dataset, with accuracies ranging from 87% to 91.63%. The study focuses on facial attribute prediction using a binary classification task approach. The experiment utilizes a thin-\u03c9 model with a set parameter budget for fair comparison. Beam search adaptation is employed due to the large number of tasks, with n set to 10 for groupings during optimization. Results on the CelebA test set show that branched multi-task networks outperform prior works with similar parameter amounts. The proposed method devises more effective task groupings for attribute classification tasks on CelebA. The Thin-32 model performs similarly to the VGG-16 baseline but with 64 times fewer parameters. The Thin-64 model outperforms the ResNet-18 model by 1.35% with a uniform loss weighing scheme and matches the state-of-the-art ResNet-18 model while using 31% fewer parameters. The study introduces a principled approach to construct branched multi-task networks within a given computational budget. The proposed approach constructs branched multi-task networks by leveraging task affinities for layer sharing. It optimizes layer sharing without the need to jointly optimize layer types and connectivity, outperforming existing methods in multi-tasking performance vs number of parameters. Extensive experiments show consistent results across various scenarios and datasets. The authors conducted extensive hyperparameter tuning but failed to achieve meaningful results on the Cityscapes dataset for all three tasks jointly. Results were only shown for semantic segmentation and monocular depth estimation. They used a ResNet-50 encoder, modified the last stride 2 convolution to stride 1, and employed a 15-layer fully-convolutional decoder for pixel-to-pixel prediction tasks. ReLU was used as non-linearity, with batch normalization in all layers except the output layer. Kaiming He's initialization was used for both encoder and decoder, with L1 loss for depth, edge detection, and keypoint tasks. The scene categorization task is learned with a KL-divergence loss. Performance is reported by measuring the overlap in top-5 classes between predictions and ground truth. Multi-task models were optimized with task weights and heatmaps were rescaled. Single-task models use an Adam optimizer with a decayed learning rate. Training lasts for 120000 iterations with a batch size of 32 and no data augmentation. Baseline multi-task model follows the same optimization procedure. Branched multi-task models are also discussed. The branched multi-task models were optimized using the same procedure as single-task models. Architectures generated by the method are shown in Fig. 4, while Fig. 5 displays architectures from a different method. Predictions from the third branched multi-task network are shown in Fig. 6 for qualitative evaluation. Cross-stitch networks/NDDR-CNN utilized hyperparameter settings optimized on Cityscapes and a CNN architecture based on VGG-16. The branched multi-task network is trained using stochastic gradient descent with momentum 0.9 and initial learning rate 0.05. It contains convolutional and fully connected layers with specific numbers of features. The model is trained for 120000 iterations with a sigmoid cross-entropy loss function."
}