{
    "title": "rJGgFjA9FQ",
    "content": "This paper introduces methods to interpret contextual effects in a pre-trained deep neural network by analyzing how input units collaborate to influence the network output. The research focuses on understanding local contextual effects of certain input units, with applications in explaining the gaming strategy of the alphaGo Zero model. The goal is to uncover the decision-making logic within neural networks, a growing area of research in recent years. The research focuses on interpreting contextual effects in a pre-trained deep neural network by analyzing how input units collaborate to influence the network output. Previous studies have visualized neural networks and input-output correlations, but this study aims to understand how local input units contribute to the network output. The study aims to interpret contextual effects in a pre-trained deep neural network by analyzing how input units collaborate to influence the network output. Two methods are proposed to disentangle contextual collaborations at different scales, independent of the CNN structure. The study proposes methods to interpret contextual collaborations in a pre-trained deep neural network, distilling knowledge into local models to analyze contributions from different input regions. The approach aims to extract fine-grained contextual effects from a student net, explaining the rationale behind moves in the alphaGo model. The study analyzes contextual collaborations within different regions of the Go board using a pre-trained deep neural network. It focuses on fine-grained collaborations within specific regions to explain moves in the alphaGo model. The model evaluates the game state based on contextual shapes on the board, with high output scores indicating a high probability of winning. The study focuses on explaining the logic of moves in the alphaGo model by analyzing contextual collaborations on the Go board. The alphaGo Zero model is pre-trained via self-play without human supervision, showcasing automatically learned intelligence in decision-making. The paper focuses on visualizing local contextual effects in decision-making of a pre-trained neural network, specifically the alphaGo Zero model. Two new methods are proposed to extract contextual effects, and experimental results show their effectiveness in explaining the model's decisions. Understanding feature representations in neural networks is a growing research area, with related studies including visualization and diagnosis of network features. The paper discusses visualizing network features, disentangling feature representations, and learning neural networks with interpretable features. Studies have focused on visualizing specific units in CNNs related to semantics and scenes, as well as retrieving objects from feature maps. The paper discusses visualizing network features, disentangling feature representations, and learning neural networks with interpretable features. Previous studies have focused on extracting objects from feature maps and using model diagnosis and distillation methods to interpret network outputs. A new trend is learning networks with meaningful feature representations in intermediate layers in a weakly-supervised or unsupervised manner. Capsule nets and interpretable models are examples of this approach. Our work focuses on analyzing quantitative contextual effects of specific input units during the inference process in CNNs. We introduce methods to extract contextual collaborations at different scales and discuss their application in understanding how input units contribute to network outputs. Our work focuses on analyzing contextual collaborations in CNNs at different scales. We propose a method using knowledge distillation to determine the region of contextual collaborations with respect to the target input unit. The input feature in most CNNs can be represented as a tensor, where H and W indicate the height and width of the input, and D represents the dimensions. The input tensor in CNNs is represented as I \u2208 R H\u00d7W \u00d7D, where H and W are the height and width, and D is the channel number. Different lattices are clipped from the input tensor, with units within each lattice used by student nets to make predictions. We generate weights for 2x2 and 3x3 lattices for different student nets, distilling knowledge from the value net into a mixture of student nets to approximate decision-making logic. The student nets approximate decision-making logic of the value net by assigning scalar weights \u03b1 i to input units within different lattices based on their importance. The method estimates specific weights for each input, with the head appearance being a dominating feature in certain classifications. The significance of contextual collaborations within each lattice \u039b i w.r.t. an input unit can be measured as impacts from the first lattice \u039b1. Two knowledge-distillation processes are conducted to learn student nets and determine {\u03b1 i }. The first process distills knowledge from the teacher net to each student net f i with parameters \u03b8 i based on the distillation loss. The second distillation process involves estimating weights for each specific input to infer the weight for another neural network. This process focuses on distilling knowledge of contextual collaborations into student nets, identifying input units that collaborate to compute the network output. The student net is represented as a cascade of functions of N layers, denoted as DISPLAYFORM1. The technique of BID21 is extended to estimate the contribution of each neural activation in a feature map to the final prediction. The contribution distribution of neural activations on each layer is denoted as C x. In the context of a cascade of functions in a student net with N layers, the method of back-propagating contributions to feature maps in low layers is discussed. This contribution propagation technique aims to distribute numerical contributions objectively over {x i}, rather than biasing towards the most important activations. The process involves representing convolution operations for computing elementary activation scores in a vectorized form. The contribution propagation technique in a cascade of functions in a neural network involves decomposing contributions of different layers and computing the overall contribution of each feature. This process allows for the absorption of normalization parameters into convolution layers and handles skip connections efficiently. Each neural activation in a middle-layer feature represents the detection of a mid-level inference pattern. In this research, a method is developed to determine mid-level patterns and measure collaboration between input units to activate middle-layer feature units in a neural network. The contribution of a neural unit depends on its activation score, where the absolute effect of a target unit on a feature map is represented by the difference between the feature map with and without the target unit activated. The method developed in this research focuses on determining mid-level patterns and measuring collaboration between input units to activate middle-layer feature units in a neural network. The contribution of a neural unit is influenced by its activation score, with the activation strength affecting the proportion of contribution to the target unit. This method highlights specific neural activations related to the target unit and utilizes fine-grained contextual collaborations to propagate this information back to input units. The proposed method involves using the relative activation change to evaluate the correlation between input units and inference patterns in a neural network. It combines different methods to explain the logic behind each move in the alphaGo Zero model during a game. The model's performance relies on the enumeration power of its value net, policy nets, and Monte-Carlo Tree Search module. The value net in the alphaGo model is crucial for evaluating the current state of the game. It is a residual network with 20 blocks, providing information on contextual collaborations for each move on the Go board. Knowledge from the value net is distilled to student networks to approximate collaborations within different regions. The value net in the alphaGo model evaluates the game state with contextual collaborations. Student networks approximate collaborations within regions using lattices of different scales. Four student nets share parameters and input two channels for white and black stones on the Go board. The neural network in the alphaGo model evaluates the game state by rotating the board state within lattices to align with the corner. It uses a concatenation of lattices as input and outputs scalar weights for local student networks. The board is divided into overlapping lattices, with nine student nets encoding local knowledge. Another neural network weights the lattices for collaboration. The most relevant lattices are selected for explanation based on relevance. Estimating unit-level contextual collaborations involves applying a method to explain student nets and the value net. Maps of contextual collaborations are computed for each neural network and normalized. The final map is obtained by summing up the maps of the three networks. Knowledge from the value network is distilled to student nets to disentangle fine-grained contextual collaborations. Comparisons are made between extracted contextual collaborations and human explanations. In this study, two metrics were proposed to evaluate the accuracy of extracted contextual collaborations in relation to new moves in the Go game. The complexity of the game makes it challenging to determine an exact ground-truth explanation for contextual collaborations. Comparisons were made between the extracted contextual collaborations and human analysis, with the understanding that the knowledge encoded in the alphaGo sometimes surpasses human understanding of the game. The extracted contextual collaborations provided rough explanations from the alphaGo's perspective. The study involved Go players with four-dan grading rank labeling collaboration strength between stones and target moves. A set of existing stones, excluding the target stone, was denoted as \u2126. Collaboration strength was labeled by players and estimated by the method. The study involved Go players labeling collaboration strength between stones and target moves. Collaboration values were normalized and Jaccard similarity was computed. Go players also rated contextual collaborations. The significance of contextual collaborations was shown in FIG0 using absolute collaboration strength q v. The study involved Go players labeling collaboration strength between stones and target moves. Collaboration values were normalized and Jaccard similarity was computed. The winning probability of black was analyzed based on the collaboration score. Stones could have positive or negative values, affecting the game outcome. Ineffective placement of stones could waste winning opportunities in a zero-sum game. Jaccard similarity was used to compare extracted collaborations with manually-annotated data. The Jaccard similarity between extracted collaborations and manually-annotated collaborations was 0.3633. Stones collaborate to escape or capture opponents, affecting game outcome. Stones with high value collaborate strategically to gain advantage. Black stones collaborate to escape white's influence, while white stones aim to escape black's control. The extracted collaborations in the game of Go involve strategic placement of stones to gain advantage. Black stones collaborate to escape white's influence, while white stones aim to escape black's control. The average rating score given by Go players for these collaborations was 3.7. In this paper, two methods for analyzing contextual collaborations in neural networks are proposed. The methods are applied to the alphaGo Zero model to uncover hidden logic learned through self-play. Experimental results show the effectiveness of the proposed methods, although evaluating the quality of contextual collaborations remains a challenge. The study proposes methods to analyze contextual collaborations in neural networks, specifically in the alphaGo Zero model. The goal is to visualize these collaborations without bias towards human-interpretable concepts. Future work includes refining the algorithm with professional Go players to uncover more accurate knowledge within the model. The convolutional operation of a conv-layer is represented as o = \u03c9 \u2297 x + \u03b2, with each element corresponding to an element in \u03c9 for fully-connected layers. The conv-layer involves a fully-connected layer where each element corresponds to an element in \u03c9. The sparse matrix W is used when o i and x j are too far apart for the convolutional filter. The contribution of o i to its compositional elements x j is propagated based on their numerical scores. When o i > 0, the activation score is o i \u2212 b. If o i \u2264 0, no information is passed through the ReLU layer. When a batch-normalization layer follows a conv-layer, the two cascaded layers function together. When a batch-normalization layer follows a conv-layer, parameters can be absorbed into the conv-layer. For ReLU and Pooling layers, contribution propagation is similar to gradient back-propagation. In the complex game of Go, there are no ground-truth annotations for contextual collaborations, leading to different interpretations by players. Results reflect the logic of the alphaGo Zero model, not human logic. The significance of contextual collaborations in Go is evaluated by Go players based on the alphaGo Zero model, not human logic. The score for each lattice is reported as si j sj."
}