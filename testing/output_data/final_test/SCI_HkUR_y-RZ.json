{
    "title": "HkUR_y-RZ",
    "content": "SEARNN is a novel training algorithm for recurrent neural networks inspired by the \"learning to search\" approach to structured prediction. It addresses the limitations of maximum likelihood estimation in training RNNs by leveraging test-alike search space exploration to introduce global-local losses. SEARNN has shown improved performance over MLE on OCR and spelling correction tasks, and a subsampling strategy enables scalability to large datasets. SEARNN is a novel training algorithm for recurrent neural networks that addresses the limitations of maximum likelihood estimation. It introduces a subsampling strategy to scale to large vocabulary sizes and has shown improved performance on tasks like OCR and spelling correction. The approach is validated on a machine translation task, showcasing the benefits of leveraging test-alike search space exploration. Improving RNN training is a relevant endeavor that has received much attention recently. Train/test discrepancies, such as exposure bias, can impact model performance. Ideas from reinforcement learning, like the REINFORCE and ACTOR-CRITIC algorithms, have been explored to address these issues. In response to the limitations of MLE training for RNNs, a novel training algorithm called SEARNN is proposed. This algorithm utilizes ideas from the \"learning to search\" approach to derive a global-local loss that is more closely related to the test error. The SEARNN approach shows significant improvements on structured prediction tasks. The SEARNN algorithm, a novel training approach for RNNs, leads to significant improvements on structured prediction tasks. It explores scaling solutions to reduce training times while maintaining performance. The algorithm is applied to machine translation, showing significant enhancements compared to traditional methods. Additionally, the approach is contrasted with other related methods in Section 7. The design choices focus on modeling the joint probability of a target sequence given an input in structured prediction tasks. This involves feeding hidden states through a projection layer to obtain scores over possible tokens, which are then normalized to obtain a distribution. The RNN model calculates the conditional distribution for each token, forming the joint probability of the sequence. The model does not assume conditional independence, making prediction challenging. Beam search or greedy prediction is used for decoding. \"Teacher forcing\" regimen involves using ground truth tokens for training. The RNN model calculates the joint probability of the sequence for training using MLE. However, MLE training suffers from issues like exposure bias, where the model's predictions at test time differ from the ground truth due to conditioning on its own previous predictions. The RNN model faces challenges in making accurate predictions on unseen paths during training, leading to a compounding of errors. There is a discrepancy between training loss and test errors, with MLE ignoring information from structured losses, which affects performance. The RNN model struggles with accurate predictions on unseen paths during training, leading to compounding errors. Recent papers highlight the local nature of RNN loss compared to sequence-level losses. The standard RNN analysis assumes a complete graph model, but if RNN outputs are only conditioned on the last few predictions, the maximum likelihood training loss can be considered local. The RNN model faces challenges in making accurate predictions on new sequences during training due to compounding errors. Recent research emphasizes the localized nature of RNN loss compared to sequence-level losses. When RNN outputs are only influenced by the most recent predictions, the maximum likelihood training loss can be viewed as local. This limitation has sparked interest in exploring new training methods for RNNs, with a focus on connections to structured prediction, particularly the L2S approach introduced by BID8. BID8 proposes a learning reduction approach in their SEARN algorithm to simplify structured prediction problems into multiclass classification. The algorithm trains a shared local classifier to predict tokens sequentially, searching greedily in the combinatorial space of structured outputs. The training procedure is iterative, using the current model to build an intermediate dataset with cost vectors for each token in the output vocabulary. The SEARN algorithm simplifies structured prediction problems by training a shared local classifier to predict tokens sequentially. Cost vectors are computed for each token in the output vocabulary, and features are extracted from the context at each time step to create an intermediary dataset. The SEARN algorithm simplifies structured prediction by training a shared classifier on an intermediary dataset. The classifier is then used for multi-class cost-sensitive classification, and the policy is updated for the next round. The roll-in/roll-out mechanism in SEARNN is illustrated to define a cost-sensitive loss for training the network. The SEARN algorithm simplifies structured prediction by training a shared classifier on an intermediary dataset. In the roll-out phase, every token is fed to the model to predict the full sequence, resulting in a predicted sequence \u0177a. The roll-in and roll-out policies control the exploration of the search space and the computation of token costs, with the reference policy aiming to pick the optimal token based on the ground truth. The learned policy in structured prediction uses the current model, while the mixed policy combines both the ground truth and the model. Similarities between RNNs and L2S include recursive token output and shared local classifiers. Teacher forcing in RNNs is equivalent to the roll-in policy in structured prediction. The roll-in policy in RNNs is akin to teacher forcing, where outputs are conditioned on ground truth. However, conditioning on previous model predictions results in a roll-in learned policy. Unlike standard RNN training, no roll-outs are involved. Leveraging structured loss information from L2S could enhance RNN training, as L2S fully utilizes structured losses information. The roll-out policy is a key tool in constructing a cost-sensitive dataset for structured prediction. The SEARNN algorithm integrates roll-outs in RNN models by using a global loss for each local cell. It computes costs for each possible token at each step of the trajectory, making it computationally tractable. The SEARNN algorithm integrates roll-outs in RNN models by computing costs for each possible token at each step of the trajectory. This information is used to derive cost-sensitive training losses for each cell, allowing for parameter updates. The process involves performing roll-outs for all actions to collect cost vectors, which are then used to calculate the final loss. Pseudo-code for SEARNN is provided in Algorithm 1. SEARNN integrates roll-outs in RNN models by computing costs for each token at each step. The RNN cell serves as a multi-class classifier, learning features directly from the context. The algorithm involves collecting cost vectors for all actions and updating network parameters. During the roll-out phase in SEARNN, token selection is done by emulating the teacher forcing technique. A token 'a' is chosen for the next cell and in the output sequence before computing the cost. The training loss function is derived from cost vectors, with a focus on multi-class classification rather than binary classification to simplify the process. Successful loss functions are introduced, with details of other experiments in the Appendix. The global loss in L2S is the sum of all T losses, with a focus on learning target tokens for the model. The first loss is a log-loss with the minimal cost token as the target, similar to MLE but maximizing the probability of the best performing action. This approach, known as target learning, is advantageous for optimization in RNN training. The log-loss approach in RNN training focuses on learning target tokens by maximizing the probability of the best performing action. To further optimize this process, converting cost vectors into probability distributions and minimizing the divergence between the model distribution and the target distribution derived from the costs can be beneficial. This approach leverages structured information more effectively than simply using the minimal cost value. To optimize RNN training, converting cost vectors into probability distributions and minimizing the KL divergence between the model distribution and the target distribution derived from the costs is crucial. This approach leverages structured information effectively and mitigates the 0/1 nature of MLE better than log-loss. The scaling parameter \u03b1 controls the peakiness of target distributions and can be chosen using a validation set. The optimization process of LOLS, an online variant, is adapted to facilitate training. To facilitate training, the optimization process of LOLS, an online variant of SEARN, is adapted. At each round, a random mini-batch of samples is selected, and a single gradient step is taken on the parameters with the associated loss. The costs do not need to be differentiable, as they are fixed when minimizing the training loss. This approach defines a different loss at each round, similar to L2S, resulting in an unbiased gradient. However, the possibility of not fixing the costs and backpropagating through the roll-in and roll-out remains an open problem. SEARNN can potentially improve performance due to these adaptations. SEARNN can potentially improve performance by leveraging test error and structured information in computed costs. It can mitigate exploration bias with a \"learned\" roll-in policy and address the vanishing gradients problem in RNN training. The introduction of specialized cells like LSTMs and GRUs addressed the vanishing gradients problem in RNN training. The ACTOR-CRITIC algorithm was used on data splits for a spelling task, with results reported in the AC column. SEARNN was tested on two datasets and compared against MLE, using the same optimization routine for fairness. Greedy decoding was used for cost computation and evaluation in all experiments. The first dataset used an encoder-decoder model with GRU cells of size 128 for OCR, with SGD optimization and Hamming error cost. The second dataset involved recovering text from a corrupted version, generated from a text corpus with a fixed probability of character replacement. Total tokens were 43. The study involved two sub-datasets with replacement probabilities of 0.3 and 0.5. They used an attention-based encoder-decoder model with GRU cells of size 100 and Adam optimizer with a learning rate of 0.001. Results showed that SEARNN outperformed MLE significantly on different tasks and datasets. The study found that structured information improved performance, with losses close to MLE yielding the best results. A combination of learned roll-in and mixed roll-out strategy was most effective. SEARNN showed significant improvements on tested tasks, even when outputting ground truth. SEARNN provides significant improvements on tested tasks but requires a large number of roll-outs for cost computation. This limits its applicability to tasks with large output sequences or vocabulary sizes. Forward passes can be parallelized but still have an asymptotic cost proportional to the number of model parameters. Experiments used a learned roll-in and mixed roll-out strategy. Ways to mitigate this issue are discussed in the paper. In this paper, the focus is on subsampling cells and tokens to compute costs efficiently in SEARNN. Sampling strategies include uniform sampling for steps and exploring different options for token sampling, such as stochastic current policy sampling. This approach leads to significant speedups in roll-out computations. In this paper, the focus is on subsampling cells and tokens to compute costs efficiently in SEARNN. Sampling strategies include stochastic current policy sampling, biased current policy sampling, and top-k strategy. These strategies aim to improve performance and scalability in L2S methods. To address issues with tokens not sampled in SEARNN, a new approach replaces the full softmax with a layer applied only on sampled tokens. This helps avoid the need for a \"default\" score for unsampled tokens in KL. The new losses are referred to as sLL and sKL. Experiments were conducted to assess the viability of combining subsampling with SEARNN on two datasets, focusing on subsampling tokens due to vocabulary size limitations. Different sampling strategies were evaluated. The experiment evaluated different sampling strategies and training losses in combination with SEARNN. Subsampling proved to be a viable strategy to achieve improvements while controlling computational costs. The best token sampling strategy depended on the chosen loss, with top-k performing well for sLL and biased current policy for sKL. Task-dependent performance was also observed in the experiment. The SEARNN method was introduced as a cheaper alternative for large-scale structured prediction tasks, showing a 5\u00d7 speedup in running time. It was applied to investigate improvements over Maximum Likelihood Estimation in challenging real-life settings, using neural machine translation. The algorithm improves upon Maximum Likelihood Estimation in real-life settings, using neural machine translation. It utilizes Adam optimizer with a decreasing learning rate, batch size of 64, and dropout of 0.3. SEARNN method employs reference roll-in, mixed roll-out, and a mixed sampling strategy. The evaluation metric used is the BLEU score. The BLEU score is used as an evaluation metric for tasks, with a smoothed BLEU score adopted for sentence-level costs. SEARNN shows significant improvements over MLE, comparable to related methods like MIXER. Performance is stronger than BSO, with similar improvements to ACTOR-CRITIC but slightly lower absolute performance. SEARNN presents similar improvements to ACTOR-CRITIC but with slightly lower absolute performance due to requiring fewer parameters. The learned roll-in policy performed poorly, so a reference roll-in was used instead. Contrary to previous analysis, scheduled sampling did not improve results, possibly due to the reference policy not being strong enough or the optimization problem becoming harder with a learned roll-in. SEARNN is compared to traditional L2S approaches and RNN training methods inspired by L2S and RL. It is closer to LOLS BID5, with customizable roll-in/roll-out strategies. The recommended strategy is a learned roll-in and mixed roll-out, except for very difficult problems where a reference roll-in is suggested. SEARNN is a full integration of L2S ideas into RNN training, with modifications in dataset construction, classifier choice, and surrogate loss functions. It differs from traditional L2S algorithms by using a stochastic method and optimizing each intermediate dataset for a single gradient step. Several papers have explored L2S-inspired approaches for improving RNN training, such as Wiseman & Rush (2016) adapting the \"Learning A Search Optimization\" approach. This approach introduces \"scheduled sampling\" to address exposure bias by gradually using model predictions during training. However, SEARNN stands out as a full integration of L2S ideas into RNN training, with modifications in dataset construction, classifier choice, and surrogate loss functions. BSO's training loss is defined by violations in the beam-search procedure, making it different from SEARNN. BSO does not handle general structured losses and requires a meaningful loss on partial sequences. Its ad hoc surrogate objective provides sparse sequence-level training signal, requiring warm-start. BID1 uses a loss similar to LL for parsing, where cost-to-go is essentially free. SEARNN can be used on tasks without a free cost-to-go oracle, unlike RL-inspired approaches. In structured prediction tasks, ground truth trajectories provide more information than traditional RL. One major direction of research in structured prediction tasks is to adapt RL techniques to optimize the expectation of the test error directly. This involves leveraging additional information provided by ground truth trajectories. Different approaches such as subsampling trajectories, using the REINFORCE algorithm, and adapting the ACTOR-CRITIC algorithm have been explored to handle non-differentiable test errors efficiently. SEARNN utilizes \"global-local\" losses with a local loss attached to each step, containing global information computed on full sequences. This requires sampling more trajectories through roll-in/roll-outs, unlike other approaches like REINFORCE and ACTOR-CRITIC which rely on good pre-trained models for significant improvement on tasks. SEARNN does not require warm-starting and does not involve optimizing additional models like baselines or critics. Unlike RL approaches, SEARNN allows for exploration starting from scratch, potentially leading to larger gains. Additionally, at test time, SEARNN selects the \"best\" sequence using a search algorithm instead of sampling from a stochastic policy. SEARNN avoids the adverse effect of selecting the \"best\" sequence by using deterministic roll-outs for cost computation. RAML introduces noise in target outputs to mitigate the 0/1 aspect of MLE training. SEARNN is a novel algorithm that addresses limitations of MLE training for RNNs by using structured cost information to define global-local losses. This allows for training RNNs from scratch and outperforming MLE on challenging structured prediction tasks. Additionally, efficient scaling techniques have been proposed for application. SEARNN is a novel algorithm that improves RNN training by using structured cost information to define global-local losses, outperforming MLE on challenging prediction tasks. Efficient scaling techniques have been proposed for applications like neural machine translation. Further research directions include adapting \"bandit\" L2S alternatives, focused costing, and targeted sampling for more efficient exploration in large-scale tasks. SEARNN requires a hidden state for roll-outs from the t th cell, obtained using a reference policy roll-in with teacher forcing. Predictions up to the t th cell are obtained by copying ground truth. Choosing a classifier in L2S involves distinguishing between the feature extractor and classifier, with convergence properties being crucial. In SEARNN, the RNN itself is chosen as the classifier with convergence properties carrying over to the initial problem. The fixed feature extractor is minimized to one-hot encoding, and the classifier performs feature learning afterwards. The intermediate dataset consists of the initial state, previous decisions, and cost vector. Another perspective is to view the RNN cell as a shared classifier, with the beginning of the RNN acting as a feature extractor. This allows for selecting the RNN cell as the SEARNN classifier, with the intermediate dataset being the previous state, decision, and cost vector. The RNN cell can be seen as a shared classifier in the SEARNN framework, with feature extraction learned instead of fixed. The choice of classifier impacts the optimization routine, where backpropagation differs based on whether the RNN itself or the cell is selected as the classifier. The reference policy in BID8 aims to minimize future costs by selecting actions optimally. The SEARNN framework utilizes a shared classifier in the RNN cell, with learned feature extraction. The reference policy in BID8 focuses on minimizing future costs through optimal action selection. In the roll-in phase, choosing the ground truth leads to predicting the best possible loss. However, in the roll-out phase, finding an optimal policy is often not feasible, requiring the use of heuristics. The choice of heuristic, such as using the ground truth, is crucial for performance, but may not always be optimal, especially when the model skips tokens. In the roll-out phase, it may be beneficial to skip a in the roll-out phase rather than repeat it. Tailored alternatives can yield better results for tasks like machine translation. Additional experimental details on losses tried but not performing as well as presented in the main text are described. The LLCAS is a smooth version of the structured hinge loss used for structured SVMs. It enabled RNNs to learn but resulted in slightly worse performance than MLE. RNNs may have difficulty optimizing this objective compared to traditional MLE. The curr_chunk discusses optimization issues encountered when using a loss function inspired by traditional structured prediction. Despite efforts, significant improvements over the MLE baseline could not be achieved. The use of KL and label smoothing techniques in the context of the Hamming loss is also explored, showing equivalence to existing methods. The curr_chunk discusses custom sampling and reference policies for improving NMT performance, focusing on token selection and BLEU score optimization. The curr_chunk discusses the switch from a learned to a reference roll-in for NMT performance improvement, highlighting potential gaps in reduction theory from the L2S framework. This change may have led to a harder optimization problem and ultimately worse overall performance. Switching from a learned to a reference roll-in for NMT performance improvement may have resulted in a harder optimization problem and ultimately worse overall performance."
}