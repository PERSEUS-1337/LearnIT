{
    "title": "B1GHb2RqYX",
    "content": "The proposed Polynomial Convolutional Neural Network (PolyCNN) is a weight-learning efficient variant of traditional CNNs. It uses only one convolutional filter, called the seed filter, at each layer, with all other filters being polynomial transformations of the seed filter. This approach, known as early and late fan-out, reduces model complexity by saving 10x to 50x parameters during learning without sacrificing performance. PolyCNN enhances performance with non-linear polynomial expansion in convolutional layers, offering a flexible balance between efficiency and model complexity. It matches standard CNN performance on various visual datasets like MNIST, CIFAR-10, SVHN, and ImageNet. CNNs have excelled in perception tasks, with evolving architectures like AlexNet, VGG, Inception, and ResNet. Training these networks with fully learnable convolutional filters remains computationally expensive. In this paper, an alternative approach called PolyCNN is introduced to reduce the computational complexity of CNNs while maintaining high performance. PolyCNN uses a single seed filter at each convolutional layer, with other filters being polynomial transformations of the seed filter. This method, known as early fan-out, helps in achieving efficient CNNs without sacrificing performance. The PolyCNN method uses a single seed filter at each layer, with other filters being polynomial transformations of the seed filter. This approach reduces model complexity and can lead to significant parameter savings during the learning stage. The PolyCNN maintains performance on visual datasets like MNIST, CIFAR-10, SVHN, and ImageNet. The PolyCNN method utilizes different types of modules for convolutional neural networks, including Single-Seed, Early Fan-Out, and Multi-Seed modules. The objective is to find filters that maximize correlation peak and minimize performance criteria. The method involves point-wise nonlinear transformations of input channels to enhance performance on visual datasets like MNIST, CIFAR-10, SVHN, and ImageNet. The PolyCNN method introduces the polynomial convolutional neural network as a weight-learning efficient variant of traditional CNNs. It involves learning a seed filter at each convolutional layer and augmenting other filters with point-wise polynomials of the seed filter. This approach results in (k + 1) response maps without the need to update the weights of augmented filters during network training. The PolyCNN method introduces the concept of early and late fan-out procedures, creating (k + 1) response maps from seed filters. The weights are generated through a non-linear transformation, resulting in convolutional outputs computed based on the input image channels and filter weights. During the forward pass, weights are generated from the seed convolutional kernel and convolved with the inputs to produce normalized response maps. Non-linear functions operate on the filter elements, and backpropagation involves computing gradients with respect to the weights and inputs. New feature maps are computed via non-linear transformations, ensuring response map normalization to prevent vanishing or exploding responses. The PolyCNN approach involves normalizing response maps to prevent vanishing or exploding responses. It restricts the network to learn only one convolutional filter at each layer, augmenting it through polynomial transformations. The augmented filters do not need to be updated during back-propagation. The PolyCNN approach involves using polynomial transformations of seed filters to create response maps, which are then passed through non-linear activation gates to generate feature maps. These feature maps can be further combined using learnable weights. Compared to standard CNNs, PolyCNN has significantly fewer learnable parameters. The PolyCNN approach utilizes polynomial transformations of seed filters to produce response maps, which are then processed through non-linear activation gates to generate feature maps. This method results in fewer learnable parameters compared to standard CNNs. The parameter saving ratio for PolyCNN is approximately 10\u00d7, 26\u00d7, and 50\u00d7 for 3 \u00d7 3, 5 \u00d7 5, and 7 \u00d7 7 convolutional filters, respectively. The PolyCNN utilizes polynomial transformations of seed filters to generate feature maps with fewer learnable parameters compared to standard CNNs. The training process involves back-propagation through polynomial augmented filters, similar to layers without learnable parameters like ReLU or Max Pooling. Non-learnable filter banks in the PolyCNN are generated by taking polynomial transformations from the seed filter, using integer or randomly sampled fractional exponents. The PolyCNN layer allows for exponents to be negative or fractional, extending beyond traditional polynomial restrictions. It analyzes how early and late fan-out can approximate a standard convolutional layer and extends to a generalized convolutional layer representation. The layer involves vectorized patches and convolution filters from input maps and filter tensors, respectively. The PolyCNN layer extends traditional polynomial restrictions by allowing negative or fractional exponents. It involves vectorized patches and convolution filters from input maps and filter tensors. The filter tensor W contains m fan-out convolutional filters, resulting in output feature map values through convolution. The PolyCNN layer extends traditional polynomial restrictions by allowing negative or fractional exponents. It involves vectorized patches and convolution filters from input maps and filter tensors. The output feature map value d early \u03c0 for early fan-out PolyCNN layer is obtained by linearly combining multiple elements from intermediate response maps via 1 \u00d7 1 convolution with parameters \u03b1 1 , \u03b1 2 , . . . , \u03b1 m. The PolyCNN layer extends traditional polynomial restrictions by allowing negative or fractional exponents. In the late fan-out PolyCNN layer, a single response map is obtained by convolving the input map with a convolutional filter. Multiple response maps are generated by expanding the response map with Hadamard power coefficients. The output feature map value is a linear combination of elements from the response maps via 1 \u00d7 1 convolution. The approximation d late \u03c0\u2248 d \u03c0 holds under the assumption that c relu is not an all-zero vector. The PolyCNN layer allows for negative or fractional exponents, extending traditional polynomial restrictions. It involves convolving input maps with convolutional filters to generate multiple response maps. The output feature map is a linear combination of these response maps via 1 \u00d7 1 convolution. By setting parameters to fixed sparse binary values, a general version of local binary CNN can be achieved. Special cases arise when kernel functions are linear, leading to simplified expressions. The PolyCNN layer allows for negative or fractional exponents, extending traditional polynomial restrictions. It involves convolving input maps with convolutional filters to generate multiple response maps. The output feature map is a linear combination of these response maps via 1 \u00d7 1 convolution. The expression can be simplified when kernel functions are linear. The MNIST dataset contains 60K training and 10K testing gray-scale images of hand-written digits. SVHN dataset has 604K training images of house number digits from street view. The curr_chunk discusses different image classification datasets, including CIFAR-10 and ImageNet ILSVRC-2012, with varying numbers of training and testing images. The PolyCNN layer is mentioned as being easily implementable in any existing system. PolyCNN can be easily implemented in any deep learning framework, with fixed convolutional weights that do not require gradient computation or weight updates. A custom backpropagation implementation for PolyCNN is 3x-5x more efficient than autograd-based backpropagation in PyTorch. Model architectures in the evaluation are based on ResNet BID7 with a default 3x3 filter size. Different numbers of PolyCNN layers are experimented with, ranging from 10 to 75, equivalent to 20 to 150 convolutional layers. The convolutional weights for PolyCNN are generated following a specific procedure. The PolyCNN approach involves using randomly sampled fractional exponents to create polynomial filter weights for convolutional filters. Spatial average pooling is applied to reduce image dimensions, followed by ReLU activation and batch normalization. In experiments with ImageNet-1k, standard CNN layers are replaced with PolyCNN layers. Different CNN architectures like AlexNet and ResNet are tested. The performance is compared with other models like BNN, ResNet, Maxout, and NIN. PolyCNN approach offers significant parameter savings compared to traditional CNNs by using randomly sampled fractional exponents for polynomial filter weights. Best performing PolyCNN models for MNIST, SVHN, and CIFAR-10 datasets are highlighted. The experiments show improved image classification accuracies with PolyCNN compared to other models like BNN, ResNet, Maxout, and NIN. The best performing PolyCNN models for image classification are compared to baselines and state-of-the-art methods like BinaryConnect, Binarized Neural Networks, ResNet, Maxout Network, and Network in Network. Results show that early fan-out PolyCNN achieves similar performance to standard CNN while saving 10\u00d7 parameters. Higher output channels lead to better performance on CIFAR-10 dataset. The PolyCNN model achieves comparable performance to standard CNN with early fan-out, outperforming its late fan-out counterpart. Varying the number of seed filters from 1 to 512 increases model complexity and performance monotonically. The top-1 accuracy on the 100-Class subset of ImageNet 2012 dataset is reported, showing insights into the trade-off between performance and model complexity. The PolyCNN model, with 20 layers and 512 convolutional filters, is compared to the baseline model on the ImageNet dataset. An AlexNet BID13 architecture is used, with five convolutional layers and two fully connected layers. The experiment focuses on the early fan-out version of PolyCNN for classification tasks. For this experiment, a single-seed PolyCNN (early fan-out) counterpart based on the AlexNet architecture is created. By setting the fan-out channel m = 256, PolyCNN saves about 6.4873\u00d7 learnable parameters in the convolutional layers compared to AlexNet. The performance of PolyCNN is not compromised, as shown in accuracy and loss curves after 55 epochs. Additionally, the experiment includes the native ResNet family architecture. The experiment includes a single-seed PolyCNN counterpart based on the AlexNet architecture, with significant parameter savings. The native ResNet family architecture is also experimented with, showing competitive performance with 10x parameter savings. The proposed PolyCNN achieves on-par performance with state-of-the-art models and will have a publicly available PyTorch implementation. In the context of neural networks, there is a focus on improving efficiency by compressing models through approaches like weight pruning, parameter sharing, and knowledge distillation. These methods aim to reduce computational and memory requirements while maintaining performance. PolyCNN is proposed as an alternative to standard convolutional neural networks, inspired by the polynomial correlation filter. It offers significant parameter savings during training, with lower model complexity compared to traditional CNNs. Despite this, PolyCNN demonstrates performance on par with state-of-the-art architectures on various image recognition datasets."
}