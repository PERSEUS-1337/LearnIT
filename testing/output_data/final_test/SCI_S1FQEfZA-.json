{
    "title": "S1FQEfZA-",
    "content": "In this paper, new techniques are proposed to evaluate the ability of Generative Adversarial Networks (GANs) to capture key dataset characteristics. These techniques require minimal human supervision and can be scaled to assess various GANs on popular datasets. Results show that GANs struggle to reproduce the distributional properties of training data, with synthetic data diversity significantly smaller. Generative Adversarial Networks (GANs) have gained attention for their ability to learn generative models of natural image datasets. A key question is how well GANs truly learn the underlying data distribution, which is important for both scientific understanding and practical applications. Researchers have explored this issue in various ways to evaluate the quality and limitations of GANs. When evaluating the quality of Generative Adversarial Networks (GANs), it is important to ensure that the generated samples resemble the true distribution. Visual inspection of generated images is a common method for assessing GAN performance. While there is concern about GANs memorizing the training data, evidence suggests they model the distribution to some extent. Interpolations in the latent space of the generator can produce novel results. Previous studies show that interpolations in the latent space of the generator produce novel and meaningful image variations, indicating a disparity between generated samples and their nearest neighbors in the true dataset. This suggests that GANs could be successful distribution learning algorithms, motivating further study of their distributions. However, comparing the probability density assigned by the generator with estimates of the true distribution is complicated due to GANs not naturally providing probability estimates for their samples and the challenging nature of estimating the true distribution's probability density. Recent work has focused on studying the behavior of GANs in low-dimensional problems like two-dimensional Gaussian mixtures. Mode collapse, where the generator assigns a large mass to a subset of modes, is a common issue. Concerns about lack of diversity in synthetic GAN distributions have been raised, with some studies showing low support size for certain datasets. However, existing methods heavily rely on human annotators and do not easily scale to comparing multiple GAN variants or asking more detailed questions. The focus of this work is to develop quantitative approaches to study synthetic GAN distributions. Two new evaluation techniques are proposed, inspired by comparing moments of distributions. Image classifiers are trained to construct test functions for semantically meaningful properties of the distributions. Our approach requires minimal human supervision and can be scaled to evaluate multiple GANs and large datasets. We analyze five state-of-the-art GANs on CelebA and LSUN datasets, revealing significant distortions in basic image attributes. Experiments show a lack of diversity in GAN data, leading to lower accuracy in image classifiers compared to classifiers trained on true data. Our analysis of GAN data reveals a lack of diversity, with synthetic distributions only comparable to a small subset of true data. Low-order moments like mean and variance are commonly used to compare distributions, but can be misleading for complex, high-dimensional data. For example, in generative models like MNIST, even if digits are shifted significantly, the distribution may still be considered a good approximation of the true data. The analysis of GAN data shows a lack of diversity compared to true data. Low-order moments may not accurately represent high-dimensional data. Using convolutional neural networks can capture meaningful properties of image distributions. Automating image annotation can help study synthetic GAN data. Mode collapse is a common issue in GANs. Mode collapse in GANs refers to the generator concentrating probability mass on a few modes of the true distribution. While evidence exists for mode collapse, visualizations are limited to toy problems. To address this, a classification-based tool is proposed to assess GAN performance in assigning mass across concepts. This tool uses a trained classifier to label important features in synthetic data for analysis. Our goal is to investigate if a GAN trained on a well-balanced dataset can reproduce this balanced structure. The procedure involves training an annotator and an unconditional GAN on the dataset, then creating a synthetic dataset by sampling images from the GAN and labeling them. This annotated data provides insight into the GAN's class distribution. In Section 3.3, mode collapse is visualized in state-of-the-art GANs on CelebA and LSUN datasets by choosing challenging classes. The dominant mode varies during training, and comparing GANs on different datasets based on mode collapse is possible. The method provides a coarse look at distribution statistics, capturing simple diversity notions. The second classification-based approach for evaluating GAN distributions aims to assess if GANs can capture key aspects of real data for training a good classifier. This measure of sample diversity is important for generating diverse training datasets for high-dimensional image classification tasks. If GANs can accurately represent the quality and diversity of real data, classifiers trained on synthetic data should perform similarly to those trained on real data. To evaluate the classification performance of synthetic data from GANs, separate GANs are trained for each class in the dataset. Samples from these class-wise GANs are combined to create a labeled synthetic dataset. The labels are determined based on the class modeled by the GAN. The classification performance is compared by training a classifier on true data as a benchmark and generating a balanced synthetic dataset by aggregating samples from per-class GANs. Both true and synthetic datasets have N samples, with N/C examples per class. By design, both true and synthetic datasets have N samples, with N/C examples per class. Use synthetic labeled data to train a classifier with the same architecture as the true data. Comparing the classifiers can shed light on the disparity between the two distributions. Previous experiments on MNIST dataset showed comparable performance between true and synthetic data using GANs. Similar good results were obtained on MNIST, possibly due to GANs' efficacy in learning the distribution. To further investigate, analysis is restricted to more complex datasets like CelebA and LSUN. The evaluation focuses on how well GANs recover nuances of the decision boundary in the classification task. The studied GANs show low diversity in synthetic data compared to true data. Classifier accuracy on GAN data is similar to a heavily subsampled version of true data. Increasing sample size from GANs does not improve performance. Visual quality of samples does not always correlate with classification accuracy. In GAN literature, metrics involving human supervision are commonly used to assess performance. Annotator-based metrics, such as manual classification of GAN generated images, help identify failure modes. Visual inspection remains popular for evaluating GAN samples, but there are attempts to develop automated metrics. Parzen window estimation for log-likelihood approximation is one method, although it may not work well for high-dimensional data. In GAN literature, metrics involving human supervision are commonly used to assess performance. Annotator-based metrics help identify failure modes. Parzen window estimation for log-likelihood approximation may not work well for high-dimensional data. BID16 developed a method for a better log-likelihood estimate using annealed importance sampling. BID13 introduced Inception Score to assess diversity in GAN samples. GANs have shown promise in generating realistic samples, with efforts to apply them to various datasets like CelebA and LSUN. Using our framework, we compare popular GAN variants: Deep Convolutional GAN (DCGAN), Wasserstein GAN (WGAN), Adversarially Learned Inference (ALI), and Boundary Equilibrium GAN (BEGAN). Visualizations show mode collapse in GAN-generated data from CelebA and LSUN datasets. The study compares popular GAN variants like DCGAN, WGAN, ALI, and BEGAN using synthetic datasets. ImGAN, a GAN with semi-supervised learning, is also analyzed. BEGAN is excluded due to convergence issues. The analysis is based on 64x64 samples with standard hyper-parameter settings. In the study, GAN variants like DCGAN, WGAN, ALI, and ImGAN are compared using synthetic datasets. BEGAN is excluded due to convergence issues. The analysis is based on 64x64 samples with standard hyper-parameter settings. Two types of classification models are used: ResNet and Linear Model. ResNet is chosen for its high accuracy on various datasets, while the Linear Model has one fully connected layer with softmax non-linearity. Linear models implement a function for classification tasks with a D \u00d7 C matrix and a C \u00d7 1 vector. The model serves as a baseline in experiments with classifiers trained to convergence and no data augmentation. Experimental results on mode collapse are presented, with details on datasets used in the analysis. Class distribution in synthetic data is compared to true data in Figure 1. The left panel compares mode distribution in true data with GAN-generated datasets, showing significant mode collapse. This collapse persists throughout training, with dominant modes fluctuating. The study analyzes mode collapse in various GAN models, highlighting differences in performance on different datasets. GANs often struggle with generalization to new datasets, with some models showing significant mode collapse. Temporal analysis reveals fluctuation in dominant modes for certain GANs. The study examines mode collapse in different GAN models, showing varied dominant modes for WGAN and Improved GAN compared to BEGAN. A quantitative assessment of sample diversity in GANs on CelebA and LSUN datasets is conducted through binary classification. Results are presented in tables and figures, with a focus on labeled GAN dataset quality assessment using high-accuracy annotators. Confidence scores and label correctness are measured for predicted labels on GAN generated data. The study evaluates mode collapse in various GAN models and assesses sample diversity through binary classification on CelebA and LSUN datasets. Results show labeled GAN dataset quality using high-accuracy annotators, with a focus on confidence scores and label correctness. Empirical results for label agreement and annotator confidence are presented in tables and figures. Table 1 presents results from a comparative study on classification performance of true data vs. GANs on CelebA and LSUN datasets. It includes label correctness measures, inception scores, training and test accuracies for linear models, as well as ResNet test accuracy. Deep networks face challenges when trained on synthetic datasets. Classifiers trained on true and labeled GAN-generated datasets are also examined. The study compares classification performance of true data and GAN-generated data on CelebA and LSUN datasets. While deep networks like ResNets perform well on true data, they overfit on synthetic data, leading to poor test accuracy. Even simple networks face overfitting issues on synthetic data, prompting the use of a basic linear model for classification experiments. Tables 1 and 3 display results from binary classification experiments using linear models. Experiments compared classifier accuracies on various datasets using linear models. Down-sampled true data was used for comparison with synthetic data. Results showed oversampling GANs can improve performance. Major findings are summarized in Tables 1 and 3. The experiments showed strong agreement between annotator labels and true labels for synthetic data, indicating high-quality GAN images. However, scores were lower for LSUN compared to CelebA. Simple classification-based benchmarks highlighted relevant properties of synthetic datasets. The inception score was not very informative, as it was similar for true and synthetic datasets due to the simple nature of the binary classification task. The performance gap between true and synthetic data on classification tasks is significant. Linear models fit synthetic datasets well but underfit true data. The diversity of GAN-generated data is comparable to true data with only a few hundred samples, indicating a lack of diversity in the synthetic data. The diversity of GAN-generated data is lacking, even with 10-fold oversampling. WGAN and ALI perform better on CelebA, while BEGAN has good visual quality but performs poorly on classification tasks. LSUN shows a larger gap between true and synthetic data. Consider other metrics besides visual inspection when studying GANs. Our key experimental finding is that simple classification-based tests can provide insight into the learned distribution in GANs, revealing potential issues with the quality and diversity of LSUN samples. This approach offers a quantitative platform to compare different GANs and could potentially be applied to assess other generative models like VAEs. In this paper, techniques are presented to evaluate GANs' ability to capture key characteristics of training data through classification. The tools are scalable, quantitative, and automatic, allowing for a nuanced comparison of GANs on large-scale image datasets. Empirical studies on popular GANs show that mode collapse is a prevalent issue, and synthetic GAN-generated datasets have reduced diversity when examined from a classification perspective. The diversity of synthetic data generated by GANs is significantly lower than that of true data, even when oversampling. Good perceptual quality of samples does not always correlate with distribution diversity. More quantitative tools are needed to assess GAN quality beyond visual inspection. Classification tasks on CelebA and LSUN datasets are used to evaluate GAN performance. The CelebA dataset has images labeled with 40 binary attributes, allowing for multiple associated attribute labels per image. Classification tasks are constructed by considering binary combinations of attributes. Details on dataset sizes, number of classes, and annotator accuracy are provided in Table 2. In Section 3.3, LSUN subsets were used for studies. A classifier trained on true data is used as an annotator to infer label distribution for GAN-generated data. Annotator's accuracy is based on the classifier's performance on a test set of true data. For CelebA, attribute-wise binary classifiers are used as annotators for higher accuracy. Benchmarks were conducted using standard implementations to ensure visual quality of GAN samples. Random samples from multi-class datasets are shown in figures for CelebA and LSUN datasets. Studies were conducted to observe mode collapse in GANs. In studies on GANs, a pretrained classifier is used to analyze class distribution in generated datasets. Annotator confidence for synthetic data is comparable to true data, indicating good quality. A comparative study on classification performance of true and GAN data is presented in Table 3 and visualized in Figure 5. Based on visualizations in Figure 5, GANs show comparable classification performance to a subset of training data that is over 100 times smaller. This indicates that GANs have diversity similar to a few hundred true data samples. The classification performance of true data compared to GAN-generated synthetic datasets is illustrated in Figure 5, showing accuracy on a hold-out set of true data. The comparison between true data and GAN-generated datasets shows that GAN samples have diversity comparable to a small subset of true data. The classification performance of both types of data is detailed in Table 3, including label correctness and inception scores. Training and test accuracies for a linear model classifier on true and synthetic data are also presented. Test accuracies for linear model classifier on true and synthetic datasets, along with ResNets trained on these datasets, are reported. Deep networks face challenges when trained on synthetic data."
}