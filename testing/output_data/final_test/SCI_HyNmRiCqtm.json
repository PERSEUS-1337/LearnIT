{
    "title": "HyNmRiCqtm",
    "content": "The proposed method aims to visually explain the classification decisions made by deep neural networks (DNNs). Various methods in machine learning and computer vision have been developed to clarify the decisions of DNNs, focusing on why a certain class was chosen as the answer. These methods aim to address questions such as why a specific answer was chosen and why other answers were not selected efficiently. The proposed method efficiently explains classification decisions made by deep neural networks (DNNs) using images. Results demonstrate its superiority in gaining insight into machine learning models' inner representations, particularly in computer vision tasks like image classification, segmentation, and denoising. DNNs have shown exceptional performance, surpassing handcrafted methods and approaching human expertise in various tasks. DNNs are approaching human expertise in various tasks, but interpretability is crucial. Accuracy alone does not provide insights into the inner workings of the network. Explanations are important for securing human trust, especially in settings like medical treatments and system verification. Asking direct questions or seeking contrasting explanations can help understand the network's decisions. In this work, a framework is presented to understand why a Deep Neural Network (DNN) makes certain decisions. The framework involves learning a model over the input space to generate synthetic samples and then altering these samples to change the classification outcome. This method does not require changing the network itself, is applicable as long as the model can handle backpropagation, and is faster than input perturbation methods. The only requirement is the availability of a latent model over the input, which can be learned using generative adversarial methods or variational autoencoders. The framework presented involves learning a latent space model using generative adversarial methods or variational autoencoders. Interpretability methods are categorized into three groups, with the first group focusing on understanding individual or sets of units in the network to gain insight into what the network has learned. These methods are example-based explanations but may not be practical for networks with thousands of units. The methods proposed by BID5, BID18, BID31, and BID30 aim to explain the decisions made by neural networks by analyzing image contributions and making changes to the image to understand the network's decision-making process. These methods provide insights into how the network learns concepts and determine the importance of different parts of the image in the decision-making process. The methods proposed by BID30 and BID20 aim to explain neural network decisions by analyzing image activations. However, these methods have limitations in explaining fine-grained datasets and are restricted to convolutional networks. BID22 used gradients to generate heat maps, closely related to DeconvNets BID28. The weakness in backpropagation of ReLU units is addressed by using heuristics to make the results more visually specific. Methods like xGEMs use a GAN to regularize explanations and focus on producing a \"morph\" from one class to another. Their approach differs from formulating a constrained optimization problem as shown in the current work. Existing approaches in literature have downsides such as using another black box to explain a current black box, requiring specific layers or architecture, and using heuristics during backpropagation. Our model addresses the limitations of existing approaches by not requiring specific layers or architecture, using heuristics during backpropagation, or needing a set of probe images. It does, however, require a generative model of the input space. This model can be estimated independently from the network to be explained. In Section 3.2, the method learns a latent space to generate natural images similar to the input space. Section 3.3 explains how to generate explanations from the latent representation. The framework is summarized in Procedure 1 and FIG0. The discriminator network DISPLAYFORM0 is used to generate explanations. DISPLAYFORM1 generates images similar to the dataset. The adversarial network G is trained on the input dataset used for D. The question is why D produced label y true and not label y probe for input I. The query image is about which the question is asked. The question \"why not class y probe?\" refers to a query image, and to answer it effectively, we need to understand the manifold of natural-looking images similar to the input. Learning a compact manifold allows us to navigate this space efficiently. Different methods like variational autoencoders and generative adversarial networks (GANs) can be used to find this mapping. In this study, GANs were employed to map latent space to input space following a specific training method. The structure of the networks is similar to previous proposals. The process involves finding an initial point in latent space that represents the input image. The loss function used includes a misclassification cost, with a residual error defined as \u2206 z0. The goal is to find a point in latent space where a different class would be equally likely as the true class. This is achieved through a constrained optimization problem involving log likelihood and classifier output. The explanation focuses on the constraints imposed on the classifier output to ensure that the true and probe classes remain the most likely. The method, CDeepEx, is compared with other methods on datasets like MNIST and fashion-MNIST, highlighting the challenges of contrasting two outputs. The complexity of the MNIST dataset is discussed, with examples of subtleties that are difficult for humans to grasp. The explanation compares the method CDeepEx with Lime, GradCam, PDA, LRP, and xGEMs on a network architecture similar to the original MNIST papers. GradCam and Lime were not designed for this type of question, so explanations are generated by extracting true and probe class explanations and subtracting them. Weighted explanations for the probe class are used to adjust network probabilities. The final explanation is the average of all maps that meet the condition. The method produced the best results compared to GradCam and Lime. Two sets of experiments were conducted, with a 99% success rate in the first set. Explanations were generated using CDeepEx, PDA, GradCam, Lime, and LRP. The method identified why a digit was classified as a 4 instead of a 9 based on the gap on top of the digit. Our method provides the best explanation for why certain digits are classified differently, highlighting specific features such as gaps and thickness of lines. GradCam and Lime were not as effective in providing accurate explanations in this context. The absence of red regions and presence of blue regions explain the true label over the predicted label. Comparison with xGEMs shows the importance of constraints in achieving correct explanations. Optimization path with and without constraints is illustrated to highlight the significance of constraints in finding the explanation. In the second experiment, a network was trained on modified data with a 6x6 gray square added to images of class \"8.\" The network only recognized 5% of the 8s correctly without the modification, but recognized 77% of other classes as 8 with the square added. The method used was able to identify the bias in the network, unlike other methods. The results were compared on a network trained on biased data, showing the effectiveness of the method in detecting biases. The testing accuracy for Vgg16 BID23 and ResNet101 BID7 on Fashion MNIST dataset is 92%. Using CDeepEx, it is evident that Vgg16 learned more general concepts than ResNet101, as shown in the generated explanations with and without constraints in FIG6. Our contrastive explanation method (CDeepEx) effectively queries a learned network to discover its representations and biases. By asking contrastive questions, we can gain insights into how the network makes decisions. This method sheds light on the robustness of the network and outperforms other current methods. The method does not require knowledge of the network's architecture or modality. Explanations can reveal unintended correlations in the input data. Qualitative results using GAN and VAE show the method's robustness. Comparison against xGEMs is shown in the results. The optimization procedure can find points with equal likelihood for probe and true class but higher likelihood for another class. The explanation comes from the network, not the generative model. Discriminator networks have varying accuracy in classifying class 8. The discriminator networks have different accuracy for classifying class 8. Explanations generated using a GAN show improvements as network accuracy increases. The generated explanations vary based on latent codes, with some inconsistencies noted."
}