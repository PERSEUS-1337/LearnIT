{
    "title": "ryxjH3R5KQ",
    "content": "Recently, Neural Architecture Search (NAS) has gained interest in academia and industry due to its challenges in a vast search space. This paper introduces a Direct Sparse Optimization NAS (DSO-NAS) method, using model pruning to address the issue. By starting with a fully connected block and introducing scaling factors to adjust information flow between operations, DSO-NAS applies sparse regularizations to remove unnecessary connections in the architecture. An efficient optimization method is then used. DSO-NAS is both differentiable and efficient, making it suitable for large datasets like ImageNet. On CIFAR-10, DSO-NAS achieves an average test error of 2.84%, while on ImageNet it achieves 25.4%. Deep Neural Networks (DNN) have revolutionized AI applications by enabling end-to-end learning, eliminating the need for manual feature engineering. However, the tedious task of \"network engineering\" still persists, requiring extensive hyperparameter tuning. Despite the challenges, DSO-NAS achieves impressive test errors on CIFAR-10 and ImageNet datasets, showcasing its efficiency in neural network architecture design. Neural Architecture Search (NAS) and AutoML aim to democratize the process of designing neural network architectures. NAS can be done through reinforcement learning or evolutionary algorithms, but these methods are resource-intensive. DARTS introduced a gradient-based approach for selecting connections in neural networks. In this work, the Direct Sparse Optimization NAS (DSO-NAS) method is introduced as a simpler approach to Neural Architecture Search (NAS). DSO-NAS prunes useless connections from a large network, optimizing the network structure directly during training. This method unifies neural network weight learning and architecture search into one optimization problem, using a modified accelerated proximal gradient method for efficient optimization. DSO-NAS combines neural network weight learning and architecture search into one optimization problem without the need for a controller or performance predictor. It achieves 2.84% average test error on CIFAR-10 and 25.4% top-1 error on ImageNet with FLOPs under 600M. The contributions include a novel model pruning formulation, a theoretically sound optimization method, and successful results. The proposed method simplifies and accelerates the search process for neural architecture search (NAS). Network pruning is a common technique for model acceleration, with works focusing on connection and neuron level pruning. Structure pruning has been proposed to address issues with irregular weights on modern GPUs. In neural architecture search, BID13 introduced a method using scaling factors for structure pruning, which proved more effective than previous methods like BID41. This approach extends to evolutionary algorithms for automatically generating network architectures. Neural architecture search methods involve changing filter sizes or adding identity mapping as mutations in evolution. Reinforcement learning with an RNN agent is used to design network architecture, but it is computationally intensive. BID45 uses an RNN network as a controller to decide layer types and parameters, achieving remarkable results but requiring 800 GPUs. Methods to accelerate the search process include limiting search space, early stopping with performance prediction, progressive search, and weight sharing. Recent methods in neural architecture search treat the search of network architecture as a black-box optimization problem with limited search spaces. BID0 and BID9 integrated reinforcement learning into model compression, constraining the action space for efficient search. BID40 also constrained the search space by predefining it as a super-network with a set of connected layers. While similar to other methods, our approach is more flexible, with a focus on ResNet and convolutional neural fabrics. Our related work includes the gradient-based method DARTS. Our proposed method, DSO-NAS, aims to optimize neural network architecture by representing it as a completely connected Directed Acyclic Graph (DAG). This allows for flexibility in the search space, unlike previous methods like DARTS. The architecture space can be represented by a sub-graph, enabling the exploration of various network structures. The architecture in this space is represented by a sub-graph, allowing for the selection of specific edges and nodes. Previous works focused on searching the architecture of convolution and reduction blocks. The complete graph is used to represent the search space of an individual block, with the final network architecture being a stacking of blocks with residual connections. The whole search space is illustrated by a completely connected DAG, with nodes representing local computation and edges representing information flow. The search for network architecture involves removing unnecessary connections and nodes in a DAG. Scaling factors are applied to edges to adjust node outputs, and sparse regularization is used to prune edges with zero scaling factors. This process helps identify the most important structures in the network. DSO-NAS can search the structure of each building block in DNN and share it for all blocks, or directly search the whole network structure. Each block consists of sequential levels with different operations connected to each other and the input/output of the block. Scaling factors and sparse regularization are applied to connections for optimization of the final architecture. The final architecture is generated by pruning connections with zero \u03bb and isolated operations after imposing sparse regularization. The network structure consists of stages with convolution blocks, with a reduction block at the end of each stage except the last one. Two search spaces are explored: shared search space where \u03bb is shared among blocks, and full search space. The search spaces explored include the shared search space where \u03bb is shared among blocks, and the full search space where \u03bb in different blocks are updated independently. The convolution block operations consist of separable convolutions with kernel sizes 3\u00d73 and 5\u00d75, as well as average pooling and max pooling with kernel size 3\u00d73. Reduction blocks use convolutions with kernel sizes 1\u00d71 and 3\u00d73 with a stride of 2 to reduce feature map size and double the number of filters. The task of searching blocks involves learning \u03bb on every edge. The sparse regularization of \u03bb poses challenges in optimization, especially in stochastic settings in DNN. A method called Sparse Structure Selection (SSS) BID13 addresses this issue by modifying the Accelerated Proximal Gradient (APG) method. This reformulation avoids redundant calculations and instability in optimization. The method, named APG-NAG, updates weights W and \u03bb jointly using NAG and APG-NAG. The authors introduced APG-NAG for updating weights W and \u03bb jointly. However, DNN overfitting poses a challenge for directly applying APG-NAG. To address this, training data is divided into two parts to update W and \u03bb separately. This ensures that the network structure is learned on a different subset of data, leading to better generalization on the test set. Hand-crafted networks incorporate domain knowledge, such as memory access. The authors address DNN overfitting by dividing training data into two parts to update weights W and \u03bb separately. Hand-crafted networks incorporate domain knowledge, such as memory access and balancing FLOPs among different blocks by adjusting regularization weight \u03b3 adaptively. The authors introduce Adaptive FLOPs and Adaptive MAC methods to improve DNN performance by adjusting regularization weights based on FLOPs and Memory Access Cost. They implement these methods and evaluate them on CIFAR-10 and ImageNet datasets, analyzing each design component in detail. The pipeline of the method involves training a fully connected network, searching for network architecture, and re-training the final architecture from scratch. Scaling parameters in batch normalization layers are fixed to one in the first two stages. The model is denoted as DSO-NAS-share or DSO-NAS-full based on block structure sharing. In experiments on CIFAR and ImageNet datasets, the network architecture is optimized using DSO-NAS-share and DSO-NAS-full structures with specific hyper-parameters. The experiments are conducted using MXNet on NVIDIA GTX 1080Ti GPUs. The CIFAR-10 dataset is divided into training and testing sets, with data pre-processing and augmentation techniques applied. The network is pre-trained for 120 epochs before architecture search. The network architecture is optimized using DSO-NAS-share and DSO-NAS-full structures with specific hyper-parameters. The network consists of three stages with convolution and reduction blocks. Adaptive FLOPs are applied during the search, followed by training the final model from scratch. Various improvements are made including dropout, cutout, drop path, and auxiliary towers. The searched models are trained for 630 epochs with NAG and additional enhancements. The performance of our searched models, DSO-NAS-full and DSO-NAS-share, is shown with mean and standard deviation of five runs. DSO-NAS-share's block structure is displayed in FIG3. Compared to a random structure baseline, both our models perform better with fewer parameters. Our method competes well with state-of-the-art methods, achieving similar results with fewer parameters in just one GPU day. In ILSVRC 2012 experiments, data augmentation is based on 'fb.resnet' implementation. The training dataset is split into 4/5 for weights and 1/5 for structure training. Pre-training involves training the entire network for 30 epochs with a constant learning rate. In the pre-training stage, the network is trained for 30 epochs with a learning rate of 0.1 and weight decay of 4 \u00d7 10 \u22125. The mini-batch size is 256 on 8 GPUs. The search stage also uses the same setting and takes about 0.75 days with 8 GPUs. After the search, the final model is trained from scratch using NAG for 240 epochs, with a batch size of 1024 on 8 GPUs. Label smoothing and auxiliary loss are used during training. The network architecture is transferred from CIFAR-10 and directly searched on ImageNet. The final structure generated by DSO-NAS-share is shown in FIG3. Results for ImageNet are shown in TAB1, with results obtained by transferring CIFAR-10 blocks to ImageNet. DSO-NAS achieves competitive performance with less search cost compared to other methods, except for MnasNet. The transferred block structure from CIFAR-10 demonstrates impressive performance, showcasing the generalization capability of the architecture. Searching directly on ImageNet leads to additional improvements. In this section, ablation analyses are conducted on the method to demonstrate the effectiveness of each component. The adaptive FLOPs technique adjusts sparse regularization weights for each block. Distribution of FLOPs among blocks is shown in FIG5, preventing complete pruning of some blocks. Error rates for different settings are also displayed in FIG5 and FIG5. Experiments on various settings of the architecture search method are performed to validate the necessity of each component, with results presented in TAB4. The Ratio of W&S means the ratio of training sample for weight learning and structure learning. Weight is updated x times and \u03bb is updated y times for every x + y iterations. Pre-training the model on the weight learning set helps prevent overfitting and improves performance by 0.2%. Good weight initialization is crucial to avoid a drop in accuracy. Neural Architecture Search is essential for AutoML, and a Direct Sparse Optimization method for NAS is proposed in this paper, beneficial for both academic research and industrial practice. Our unified weight and structure learning method is fully differentiable, providing a novel model pruning view to the NAS problem. The induced optimization method is efficient and effective, leading to state-of-the-art performance on image classification datasets. Future work aims to incorporate hardware features for network co-design, opening a new direction for AutoML."
}