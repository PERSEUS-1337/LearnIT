{
    "title": "ByxtHCVKwB",
    "content": "The traveling salesman problem (TSP) is a combinatorial optimization problem with real-life applications. Machine learning and variable neighborhood search are used to tackle TSP, treating the search process as a Markov decision process. A 2-opt local search and Monte Carlo tree search method are employed to sample targeted actions. This approach outperforms existing ML-based TSP algorithms in experiments with public datasets. Our approach outperforms existing learning-based TSP algorithms, showing high potential for solving the TSP and other combinatorial optimization problems. The TSP involves finding the cheapest tour that visits each city exactly once, a challenging problem in computer science. Many algorithms have been developed for the TSP, but our approach stands out for its performance and flexibility. Many algorithms have been developed for the Traveling Salesman Problem (TSP), with the best exact solver Concorde demonstrating optimality for a large instance. Leading heuristics can obtain near-optimal solutions for millions of cities but are complex and rely heavily on expert knowledge. To address these limitations, ML-based algorithms have been proposed for the TSP, automating the search process without the need for expert knowledge. Existing ML-based algorithms for the Traveling Salesman Problem (TSP) can be categorized into two paradigms: end-to-end ML and ML followed by OR. These methods are still in their early stages and struggle with instances over 100 cities. A novel framework combining Monte Carlo tree search (MCTS) with a basic OR method is proposed to address this challenge. The curr_chunk introduces a new method using variable neighborhood strategy to solve the TSP, combining OR and ML. It starts with a 2-opt based local search in a small neighborhood, then switches to a reinforcement learning based method in an enlarged neighborhood. This framework is flexible, targeted, and can be applied to various combinatorial optimization problems. The curr_chunk introduces a Monte Carlo Tree Search (MCTS) method for solving the Traveling Salesman Problem (TSP) in an enlarged neighborhood. Unlike existing approaches, this method focuses on converting complete TSP tours rather than constructing partial tours. The MCTS iterates through simulation, selection, and back-propagation steps to guide the sampling process efficiently. The curr_chunk discusses the methodology and results of a new Monte Carlo Tree Search (MCTS) algorithm for solving the Traveling Salesman Problem (TSP). Experimental results show that the MCTS algorithm outperforms existing learning-based algorithms on TSP instances. The paper is organized into sections reviewing existing methods, detailing the new paradigm and MCTS method, presenting experimental results, and concluding. The idea of applying machine learning (ML) to solve the Traveling Salesman Problem (TSP) dates back several decades. Early attempts using neural networks achieved limited success due to hardware and data limitations. However, with advancements in hardware and data availability, ML, especially deep learning, has achieved great success in artificial intelligence. For more information on leading TSP algorithms, refer to (Applegate et al., 2009), (Rego et al., 2011), (Helsgaun, 2017), and (Taillard & Helsgaun, 2019). ML, particularly deep learning, has been successful in artificial intelligence. This success has reignited interest in using ML for combinatorial optimization, such as the Traveling Salesman Problem (TSP). Various ML-based algorithms have been developed for the TSP, including the end-to-end ML paradigm introduced by Vinyals et al. (2015) using a pointer network with an encoder and decoder. This method utilizes recurrent neural networks to predict the probability distribution over unvisited cities, allowing for the creation of a complete TSP tour. The pointer network's advantage lies in its ability to process graphs of different sizes efficiently. Several successors have chosen reinforcement learning (RL) over supervised learning (SL) for the Traveling Salesman Problem (TSP) to avoid the need for pre-computed solutions. Different approaches include using actor-critic RL architecture, maintaining a partial tour and using RL to select relevant cities, implementing actor-critic neural networks, and utilizing graph attention networks (GAN). Deudon et al. (2018) and Kool et al. (2019) proposed a graph attention network (GAN) with RL to improve solution quality. Shimomura & Takashima (2016) introduced a MCTS algorithm for TSP. ML combined with OR is recommended for better performance, as seen in Nowak et al. (2017) and Joshi et al. (2019) using GNN and beam search. Several algorithms, including deep graph convolutional networks and OR based methods, have been used to improve solution quality for TSP. ML is often combined with OR techniques, such as post-optimization and local search, resulting in statistically better performance. Additionally, ML methods have been proposed for related problems like decision TSP, multiple TSP, and vehicle routing. The proposed new paradigm for combining OR and ML to solve the TSP involves treating the search process as a Markov Decision Process (MDP) and using 2-opt based local search within small neighborhoods. When no further improvement is possible, the MDP explores an enlarged neighborhood using Monte Carlo Tree Search (MCTS) to sample promising actions efficiently. In the proposed paradigm for solving the TSP, the MDP uses 2-opt local search and MCTS to iteratively sample and choose improving actions until a termination condition is met. Each state represents a complete TSP solution, with actions transforming one state to another by deleting and adding edges. Each action involves a series of sub-decisions to optimize the tour. The proposed paradigm for solving the TSP involves using MDP with 2-opt local search and MCTS to iteratively sample and choose improving actions. Each action consists of a series of sub-decisions to optimize the tour, represented by deleting and adding edges. To address redundancy, a compact method is developed to represent an action with only k sub-decisions, simplifying the process. The proposed paradigm for solving the TSP involves using MDP with 2-opt local search and MCTS to iteratively sample and choose improving actions. Each action consists of a series of sub-decisions to optimize the tour by deleting and adding edges. A compact method is developed to represent an action with only k sub-decisions, simplifying the process. This process involves breaking inner cycles and reducing the degree of cities to reach a new state, ultimately leading to a feasible TSP solution. The proposed method for solving the TSP involves using MDP with 2-opt local search and MCTS to iteratively sample and choose improving actions. Each action corresponds to a k-opt transformation, simplifying the process by making only k sub-decisions. Unpromising edges and actions are eliminated to ensure feasible solutions. The proposed method for solving the TSP involves using MDP with 2-opt local search and MCTS to iteratively sample and choose improving actions. Unpromising actions are eliminated directly to reduce the search space. State initialization starts from an arbitrarily chosen city and iteratively selects unvisited cities to form a complete tour. Each possible state is chosen as the starting state with a certain probability. The method for solving the TSP involves using MDP with 2-opt local search and MCTS to sample improving actions. Complex techniques like \u03b1-nearness criterion and partition and merge are avoided. The method searches within a small neighborhood by examining promising actions with k = 2 iteratively until no improvement is found. This is equivalent to a 2-opt based local search procedure. The method involves using MDP with 2-opt local search and MCTS to sample improving actions. Inspired by previous works, the MCTS procedure consists of four steps. The MCTS procedure outlined by Silver et al. (2017) involves four steps: Initialization, Simulation, Selection, and Back-propagation. In the Initialization step, weight and access matrices are defined, along with a variable to track simulated actions. Simulation involves probabilistically generating actions based on a given state. The MCTS procedure involves Initialization, Simulation, Selection, and Back-propagation. Sub-decisions are represented as a series of a i and b i values, with Z bij estimated using a formula balancing intensification and diversification. The parameter \u03b1 helps achieve this balance. The MCTS procedure involves sub-decisions represented by a series of a i and b i values. To make decisions sequentially, a i+1 is determined based on certain conditions. Selection is made during the simulation process, where an improving action is applied to the current state. During the MCTS procedure, if an improving action is found, it is applied to the current state to generate a new state. If no such action is available, the MDP transitions to a random state. The values of M, W, and Q are updated through back propagation when actions are examined, with specific adjustments made for better states. The MCTS algorithm updates W and Q through backpropagation to enhance the selection of good edges, iterating through simulation, selection, and back-propagation steps until no improving action is found. This process is repeated until the allowed time has elapsed, returning the best found state as the final solution. To evaluate the performance of our MCTS algorithm, we program it in C language and conduct experiments on various TSP instances. We compare solution quality and list run-times for efficiency evaluation. Two widely used data sets for learning-based TSP algorithms are utilized as benchmarks: Set 1 with subsets of 2D-Euclidean instances and Set 2 with instances from the TSPLIB library. The MCTS algorithm is evaluated using four hyperparameters (\u03b1, \u03b2, H, T) with default settings. Parameter T is set differently for two data sets to control termination conditions. Results on data set 1 show comparisons with non-learned and learning-based algorithms, including exact solvers and optimization tools. The best heuristic solver is LKH3, and OR tools are optimization tools by Google. The table shows end-to-end ML models and hybrid algorithms using OR methods for post-optimization, with details on tour length, optimality gap, and clock time for different instances. Results are compared with those from a previous study. In the latest learning-based algorithms, experiments were conducted on either a single GPU or 32 instances in parallel on a virtual CPU system. The run-time was recorded as the wall clock time used to solve 10,000 test instances. Exact solvers and LKH3 performed well on all test instances, while non-learned algorithms performed poorly. Hybrid algorithms combining ML and OR showed much better solution quality. Our MCTS algorithm performs well, matching or improving the best known solutions on most instances from the TSPLIB dataset. The computation times are reasonable, similar to the latest learning-based algorithm. Our MCTS algorithm outperforms S2V-DQN in solving instances with n cities, achieving optimal solutions on 28 out of 38 instances with a small average gap of 0.21%. S2V-DQN only reaches optimality on one instance (berlin52) with a larger average gap of 4.75%. This demonstrates the superiority of MCTS over S2V-DQN. The experimental results show that MCTS outperforms S2V-DQN in solving TSP instances, with an average gap of 0.21% compared to 4.75%. The novel paradigm combines OR and ML in a variable neighborhood search strategy, achieving competitive performance in learning-based TSP algorithms. The integration of ML and OR remains an open question for future investigations and extending to other optimization problems."
}