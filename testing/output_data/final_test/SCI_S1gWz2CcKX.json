{
    "title": "S1gWz2CcKX",
    "content": "The artificial intelligence research platform is inspired by MMORPGs and aims to study behavior and learning in neural agents. It supports persistent environments with variable agent numbers and open-ended tasks. Through experiments, it was found that large-scale multiagent competition leads to the development of skillful behavior, with larger populations resulting in more complex behaviors and out-competing smaller populations. The success of biological evolution has inspired attempts to emulate it in silico through genetic algorithms and artificial life simulations. The field of deep reinforcement learning now trains algorithms by having them compete in simulated games, providing easy metrics for evaluation. Our aim is to develop a simulation platform that captures important properties of life on Earth by simulating a \"Neural MMO\" where each agent is a neural net learning to survive using RL, inspired by Massively Multiplayer Online Role-Playing Games (MMORPGs). The platform simulates a \"Neural MMO\" where agents are neural nets learning to survive using RL. Experiments show that large populations encourage exploration and skillful behavior. Organized into species, agents naturally diverge to occupy different behavioral niches. The platform will be opensourced upon publication, with experience collected across 100 worlds and agents' parameters updated via policy gradients. Test time visualization offers insight into learned policies through value function estimates, map tile visitation distribution, and agent-agent dependencies. Multiagent reinforcement learning has gained attention recently, focusing on large populations of agents in complex environments. Unlike single-agent environments, multiagent interactions introduce nonstationarity due to continual learning and co-adaptation. The goal is to maintain agents' health through food and water for survival, rather than direct rewards for foraging or fighting. The curr_chunk discusses the simulation of artificial life in a setting of coevolution where agents coadapt alongside others. The platform can simulate complex behaviors and morphologies, grounded in the MMO game genre. Unlike past work, the focus is on maintaining agents' health through food and water for survival in a dynamic environment. The platform simulates artificial life in a coevolution setting within the MMO game genre. Agents are trained using deep reinforcement learning and compete for resources while balancing food and water levels. Combat agents engage with others using melee, range, and mage attack styles. Graphics are essential for visualizing properties specific to the environment. Game platforms like ALE and Gym Retro provide arcade games for testing research ideas. Performance across games is a metric of quality, motivating further research. Recent research has shown strong performance in multiplayer games like Go, DOTA2, and Quake 3 Capture the Flag, advancing our understanding of algorithms. However, these games are limited in complexity compared to the artificial life simulation platform discussed earlier. MMORPGs are role-playing games where many human players participate, offering a complex environment with problems that have multiple solutions and long-term consequences. MMOs are multiplayer versions of RPGs, run on multiple servers with hundreds to millions of players, featuring challenges that require increasing skill levels. In MMORPGs, late game content requires specific skills that may be difficult for new players to grasp. Players must gather resources and level up to progress further in the game. The platform aims to explore game mechanics that support complex behaviors and agent populations. Developers continuously balance mechanics while players improve their skills in utilizing them. The systems are iteratively developed and not fixed, with every numeric parameter subject to change. The environment for the MMORPG game allows agents to move on discrete tiles, compete for food, refill water, and engage in combat with different attack options. Agents make decisions based on local game state, with the possibility of using neural networks or hardcoded algorithms. The code base includes modules for trade, gathering, crafting, and more. Up to 100M agent trajectories have been tested on 100 cores in 1 week, with ongoing balancing and integration efforts. The neural API integrates equipment, communication, and trade into the MMORPG game environment. Input Agents observe local game state within a fixed distance, while Output Agents provide action choices for the next game tick. Movement options include North, South, East, West, and Pass, while attack options are Melee, Range, and Mage. Invalid actions are ignored, and the policy architecture is detailed. The policy architecture for the MMORPG game environment includes a preprocessor that embeds stimuli into fixed-length vectors and entity embeddings. Linear layers are applied to the embeddings for movement, attacks, and value prediction. Additional action choices can be added by including more output heads. Training is done with policy gradients and a value baseline, with rewards postprocessed using a discounting factor. Good performance was achieved without discounting, but stability was lower. Initial experiments using this platform are presented. In initial experiments using the platform, agent competence scales with population size. Increasing the maximum number of concurrent players magnifies exploration, while increasing the maximum number of populations with unshared weights magnifies niche formation. Agents are sampled uniformly from populations with unshared weights for efficiency. Each experiment is run using 100 worlds with a spawn cap defined for agent limitations. Training with larger and more populations in MMOs can impact foraging performance. Parameters are shared across groups of up to 16 agents for efficiency. Four experiments were conducted with varying population sizes and spawn caps. Evaluating the influence of these variables is complex due to differences in task difficulty and player competence among MMO servers. In MMO servers, player bases can merge, leading to tournament-style evaluations to compare policies learned in different settings. Results show that agents trained in larger settings consistently outperform those trained in smaller settings. Introducing combat as a learnable mode of variation alongside foraging results in more interesting policies, with agents' actions becoming strongly linked to other agents' states. Populations trained with combat outperform those trained with only foraging. In MMO servers, player bases can merge, leading to tournament-style evaluations to compare policies learned in different settings. Results show that agents trained in larger settings consistently outperform those trained in smaller settings. Introducing combat as a learnable mode of variation alongside foraging results in more interesting policies, with agents' actions becoming strongly linked to other agents' states. Populations trained with combat outperform those trained with only foraging. The analysis is decoupled into two modes of variability: maximum number of concurrent players (N ent) and maximum number of populations with unshared weights (N pop). Isolating the effects of environment randomization on a fixed map reveals that exploration scales with the number of concurrent agents. In a large and resource-rich environment, different populations of agents tend to separate to avoid competition. Specialization to particular regions of the map increases with the number of populations, forcing agents to discover advantageous skills. Agents seek areas with enough resources to sustain their population, leading to diversification to separable regions of the map. Regions that are difficult to access or unoccupied are especially desirable. In valuable environments, agents separate to avoid competition and specialize in different regions. Jungle climates foster more biodiversity than deserts, which in turn have more biodiversity than mountains. Earth is the only known planet with life, emphasizing the importance of initial conditions for intelligent life. Successful MMO games attract dedicated players who understand game systems better than developers. Feedback drives game development, but creating the initial seed requires significant effort. Agents in simulations learn to rely on each other for survival. In a multiagent setting, learning is influenced by competitive and collaborative pressures from other agents. The environment must support these pressures for diverse and interesting behavior to emerge. Multiagent competition acts as a curriculum magnifier, not a curriculum itself. Agents in simulations learn to fixate on the presence of other agents within combat range, leading to responsive learning behaviors. The environment must support pressures from other learning agents for multiagent interaction to drive complexity. Modeling after MMOs is chosen for feasibility and supporting desired interactions, unlike other environment classes. The developers cherrypick game design decisions to support complexity commensurate with engaging play at the level of general human intelligence. The trend of increasing exploration with increasing entity number is clear when training on a single map. Smaller populations learn brittle policies that do not generalize to scenarios with more competitive pressure. Agents learn policies dependent on each other in foraging and combat environments. Attack patterns are visualized, with melee intentionally overpowered as a sanity check. Keeping distance is crucial as the first to strike wins. Targeting automation leads to learned behavior. Our experiments show that automated targeting leads to learned behavior in agents. Jointly learning attack style selection and targeting requires an attentional mechanism. Larger batch sizes result in more stable performance during training. The importance of niche formation in training agents is highlighted, emphasizing the need for an environment where interaction with other agents is limited. Procedurally generated maps and configurable parameters are discussed in detail in the appendix. The game state involves agents moving on different terrain tiles with varying resources such as food, water, and health. Agents can gather food from forest tiles, but must avoid lava tiles which are deadly. The resources have a carrying capacity, leading to an exploration arms race. The combat system in the game involves three attack styles: Melee inflicts 10 damage at 1 range, Ranged inflicts 2 damage at 1-2 range, and Mage inflicts 1 damage at 3 range and freezes the target. Each style has trade-offs, with Melee being high risk high return, Ranged less risky but prolonged conflicts, and Mage preventing movement for two ticks. The combat system involves three attack styles: Melee is high risk high return, Ranged is less risky but prolonged conflicts, and Mage allows agents to freeze opponents in place. Attack range is defined by distance. Agents receive food/water resources equal to the damage they inflict. Spawn Killing Agents are immune for the first 15 game ticks to prevent exploit. The Gym framework provides two APIs for MMOs: Gym Wrapper and Native. Gym Wrapper extends the Gym VecEnv API to support variable numbers of agents per world, while Native interface simplifies usage by pinning environment and agents on the same CPU core, reducing communication overhead. The Gym framework offers two APIs for MMOs: Gym Wrapper and Native. Gym Wrapper supports variable numbers of agents per world, while Native interface reduces communication overhead by pinning environment and agents on the same CPU core. Interprocess communication is used infrequently to synchronize gradients across all environments on a master core. The backwards pass is currently done on CPU cores but will be separated into an asynchronous experience queue for GPU workers. A front end visualizer with research tools is provided for analyzing agent policies and statistics. Detailed documentation is included in the source release, featuring a 2D game renderer, a beta 3D game renderer, value map \"ghosting,\" exploration maps, interagent dependence, and attack maps. The preprocessor embeds indices for each tile into a 7D vector, projects visible attributes of nearby entities to 32D, flattens the tile embeddings, max pools over entity embeddings, and concatenates the two. The entity embeddings are max pooled to handle variable observations and concatenated to produce an embedding for each head. The attack network is present but actions are ignored for foraging experiments. A linear layer is applied to visible entity features like health, food, water, and position. Max pooling is shown to be more efficient than attention BID0. The pooled entity vector is concatenated with tile and self embeddings, followed by a final linear layer."
}