{
    "title": "Bkl1uWb0Z",
    "content": "In this work, the authors aim to enhance neural machine translation by incorporating source side dependency syntax without explicit annotation. They propose models that induce dependency trees on the source side and utilize this information on the target side. The dependency trees capture important syntactic features of language and improve translation quality for language pairs En-De and En-Ru. The basic architecture extends the machine learning paradigm to map natural language strings. Input is summarized by a neural network into a summary vector and decoded into observations. Models have been enhanced with attention mechanisms, variational dropout, LSTM cells, and advanced optimizers. Despite these advances, the community struggles to explain the models' success in linguistic tasks. Recent research shows LSTM captures syntax, but evaluation is based on downstream tasks, not representation. Recent research in neural machine translation has highlighted the importance of explicitly modeling syntax using parse trees instead of relying on the model to automatically encode it. This work aims to enhance seq2seq models by incorporating a gate that allows the model to prioritize between syntactic and semantic objectives. By utilizing syntactic structured attention, dependency trees can be extracted to provide a more interpretable interface for testing the model's internal representation and attention. The model aims to incorporate syntactic and semantic information without explicit annotation, using structured attention to extract dependency trees. The syntactic objective ensures translation fluency, while the semantic objective focuses on word translation prediction. The quality of induced grammar depends on the target language choice. The importance of structure in translation is highlighted by the dependency tree of a sentence, showing how syntactic agreement requires long-distance information transfer. Translating from English to German, knowledge of the subject is crucial for accurate verb translation. The importance of structure in translation is highlighted by the dependency tree of a sentence, showing how syntactic agreement requires long-distance information transfer. Translating the word \"next\" can be done in isolation without knowledge of its dependencies. LSTMs struggle with long-distance dependencies, and using attention reduces the decoder's capacity to learn target side syntax. A model has been designed to induce syntactic relations in source sentences and decide when to use syntactic information for generating target words. Syntactic dependencies on the source side can be modeled via a self-attention layer, allowing direct interactions amongst source words. The text discusses the use of syntactic attention and a gating mechanism to improve translation quality by imposing non-projective dependency structures and learning when to use source side syntax. It also mentions the analysis of non-projective dependency trees and the benefits of incorporating hierarchical structures for better task performance. The models aim to learn latent trees with a joint syntactic-semantic objective. In this work, the authors present the first results on learning latent trees with a joint syntactic-semantic objective for machine translation. They propose a new NMT model that improves translation quality significantly over a strong baseline. The paper also includes an in-depth analysis of the learned structures and how syntax is utilized during decoding. The rest of the paper is organized into sections detailing the NMT baseline, proposed models, experimental setups, and translation results. The experimental setups and translation results are presented in section \u00a74, followed by an analysis of models' behavior through visualization in section \u00a75. The latent trees induced by the model are discussed in section \u00a76, and the work is concluded in the last section. The NMT system used is a seq2seq model with a bidirectional LSTM encoder, an LSTM decoder, and an attention mechanism. The decoder in the NMT system is composed of stacked LSTMs with input-feeding. It takes the previous hidden state, the embedding of the previous generated word, and a context vector as inputs. A feed-forward network computes a context vector using an attention mechanism. Previous work focused on modifying the encoder to incorporate source-side syntax, returning annotation vectors compressing semantic and syntactic relations. In the NMT system, the encoder returns annotation vectors compressing semantic and syntactic relations. The proposed alternative approach involves the encoder outputting two sets of vectors: content annotations and syntactic annotations. These are used to compute attention weights and context vectors for generating the target words. The NMT system encoder outputs content and syntactic annotations. A syntactic attention layer is added to convey dependency information to the decoder, allowing it to control the amount of syntax needed for predictions. The gating mechanism helps inspect the decoder state and determine the relevance of source side syntax. The NMT system encoder includes a syntactic attention layer for head word selection and an attention with gating mechanism to control syntax for target word generation. The head word selection layer uses structured attention to select soft head words for each source word. This layer transforms the input into a matrix encoding implicit dependency structure. The structured attention probabilities are computed based on a function inspired by previous work but with modifications for non-projective dependency trees. The NMT system encoder utilizes structured attention for head word selection and an attention with gating mechanism for target word generation. The model differs by using Kirchhoff's Matrix-Tree Theorem for fast evaluation of attention probabilities on non-projective dependency trees. The scoring matrix \u03c6 scores the likelihood of one word being the head of another. The probability of a dependency tree is calculated based on the scoring matrix. The head selection model uses Kirchhoff's Matrix-Tree Theorem to compute marginals for non-projective dependency structures. The marginals are fully differentiable, allowing for end-to-end training of the model by maximizing conditional likelihood. The decoder is encouraged to use syntactic annotations through attention mechanisms. The model implements a shared attention layer from decoder's state to encoder's annotations, allowing it to focus on the head word of a source word when generating the next target word. This approach aims to enhance the model's ability to translate by incorporating syntactic information when necessary. The model introduces a gating mechanism to control the amount of source side syntactic information. It uses separate attention layers to incorporate syntactic annotation into the decoder, improving translation by reallocating mass to both subject and object. The decoder uses shared attention to translate the English word \"ordered\" by considering the subject \"boy\". An experiment with hard structured attention is conducted to improve grammar induction by forcing the model to make hard decisions during training. The performance on grammar induction is improved by converting soft weights to hard ones. Training the model involves using the straight-through estimator. To enforce a tree structure, the best tree is decoded using the maximum spanning tree algorithm. However, due to speed constraints, a greedy approach is used to assign parent words to each word in the sentence. The Chu-Liu-Edmonds algorithm assigns a parent for each word, followed by discussing experimental setup and results for En\u2194De and Ru\u2194En translation models using WMT17 data. Different corpora are used for each language pair, with specific datasets for development and testing. BPE BID26 with 32,000 merge operations is utilized, and modifications are made to the NMT model architecture. In our proposed models, we validate modifications to the basic architecture empirically. We compare the structured attention module to a self-attention module, referred to as flat-attention (FA). We also validate the benefit of using two sets of annotations in the encoder, combining hidden states with syntactic context to obtain a single set of annotation. This baseline model has only one attention layer from target to source. Target word embeddings and output layer weights are shared across all models. In experiments, models were trained with specific parameters and evaluated on development data. BLEU scores were reported using the multi-bleu.perl script. Statistical significance was tested using bootstrap resampling, with marked significance levels compared to baselines and FA-NMT models. The SA-NMT (shared) model outperforms FA-NMT models with two separate attention layers, showing significant gains in BLEU scores for various language translation directions. Structured attention proves beneficial for languages with long-distance dependencies and complex morphological agreements. Sharing attention in FA-NMT models is helpful, while having two separate sets of annotations in the encoder improves syntax modeling. The hard structured attention model performs comparably to the baseline. The hard attention model shows better quality induced trees compared to soft attention. Hard attention tends to concentrate on fewer tokens and may struggle with equally probable heads, potentially affecting translation performance. Structured attention models capture similar structures in source sentences, suggesting syntax plays a role in inducing trees. The induced trees from the hard attention model are of better quality compared to soft attention. Syntax plays a role in inducing trees, as shown by structured attention models. The model attention focuses on the verb as the head of the pronoun, even though long distance dependency phenomena are less common in English. The study investigates when the target LSTM needs to access source side syntax by analyzing gate activations of the best model, SA-NMT (shared). The study analyzes gate activations of the best model, SA-NMT (shared), to understand how syntactic information flows into the decoder during translation. Activation norms are highest when predicting verbs and decrease towards function words. This shows that translating verbs requires knowledge of syntax, with nouns having high activation norms. The study investigates the highest activation norm in a sentence applied to a part-of-speech tag on newstest2016, comparing it to a frequency baseline. The research explores the interpretability of the model's representations through quantitative attachment accuracy and qualitative grammar comparison, aiming to identify similarities and differences with linguistics. The results both support and challenge previous findings. The study examines syntactic information discovery through latent structured attention and challenges conventional syntax definitions. Non-projective dependency trees are extracted using the Chu-Liu-Edmonds algorithm. Unlabeled attachment accuracies are computed on gold annotations from the Universal Dependencies dataset. Target language impacts the results. The study shows that the target language affects the source encoder's performance in determining headedness. Models with hard attention outperform baselines for undirected dependency metrics, with unclear benefits for German and no benefits for Russian. The use of hard attention helps in extracting linguistic structures, indicating the ability of models to capture complex structures. This approach outperforms previous methods despite lacking additional resources like part-of-speech tags. The study found that the target language influences the source encoder's performance in determining headedness. Models with hard attention outperform baselines for undirected dependency metrics, with unclear benefits for German and no benefits for Russian. The lack of correlation between syntactic performance and NMT suggests that useful structures learned for a task may not correspond to known linguistic formalisms. The target language has a significant impact on grammar induction, with a boost in scores for morphologically rich languages like Russian. The study shows that the target language influences the encoder's ability to capture syntax, with a stronger impact than previously thought. Qualitative analysis reveals that the model's strength lies in directed attention, raising questions about structural elements ignored by the grammar. The analysis compares learned grammars to gold production frequencies, highlighting the influence of target language on grammar induction. The study proposes a structured attention encoder for NMT, showing significant performance gains over a strong baseline on standard WMT benchmarks. The models do not rely on external information like parse-trees or part-of-speech tags, inducing dependency trees over source sentences that outperform previous work. The quality of induced trees is not correlated with translation quality."
}