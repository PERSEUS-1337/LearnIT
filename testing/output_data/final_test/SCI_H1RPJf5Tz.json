{
    "title": "H1RPJf5Tz",
    "content": "CBF is an exploration method that operates without rewards or end of episode signals. It uses intrinsic reward based on a dynamics model in feature space. Inspired by previous work, it has achieved success in various tasks like Super Mario Bros and SpaceInvaders. Combining CBF with auxiliary tasks did not consistently improve results. Modern reinforcement learning methods struggle with environments that have sparse, costly, or misspecified reward functions. BID19 introduced an exploration strategy that uses intrinsic rewards based on a dynamics model in feature space, leading to sophisticated behavior in games without external rewards. Inspired by this, researchers aim to enhance the method further. The researchers aimed to improve a method for learning features in reinforcement learning by using different tasks. Surprisingly, the choice of feature-learning task did not significantly impact the results, and skipping this step sometimes led to better outcomes. The method showed purposeful behavior in various games, including passing four levels of Super Mario Bros without external rewards. Additionally, the method was tested on VizDoom maze environment and Atari games, showcasing its effectiveness. The Curiosity by Bootstrapping Features (CBF) method utilizes a forward dynamics model trained with the agent's policy to drive exploration in unpredictable regions of the environment. This adversarial dynamics can lead to complex behaviors, and novelty-based intrinsic rewards can be generated using smoothed state visitation counts. The forward dynamics model in exploration methods like CBF uses input as a constant, with no gradients propagated through it. Different approaches to intrinsic motivation, such as uncertainty about value function estimates or policy perturbations, can be combined with maximizing a reward function. The dynamics model can output a point estimate or distribution over the next state based on the current state and action. It can also operate in a lower dimensional feature space for computational efficiency. In exploration methods like CBF, the forward dynamics model uses a constant input with no gradients. Different intrinsic motivation approaches can be combined with maximizing a reward function. The model can output a point estimate or distribution over the next state based on the current state and action, operating in a lower dimensional feature space for efficiency. The method involves an embedding network, policy network, forward dynamics model, and optional auxiliary prediction task. The intrinsic reward is based on state embeddings and chosen actions, replacing extrinsic rewards. The policy network aims to maximize rewards by identifying unexpected transitions, while the dynamics model minimizes prediction errors. The dynamics loss is optimized only for the dynamics model parameters to prevent feature collapse. The Proximal Policy Optimization (PPO) algorithm is used for training, with the policy loss optimized with respect to both the embedding and policy network parameters in joint training. The embedding function \u03c6 is optimized with auxiliary losses and policy network loss. When training jointly, \u03c6 is optimized with a combination of auxiliary loss and policy network loss. If no auxiliary task is present, \u03c6 has randomly initialized parameters \u03b8 \u03c6. The forward dynamics model prefers small norm features to reduce error, while the policy network prefers large norm features to increase reward. The forward model's loss does not optimize the embedding network parameters, and the policy network can only optimize actions, not directly manipulate. The policy network optimizes actions but cannot directly manipulate the reward function by increasing feature norms. In stochastic environments, errors from a deterministic forward dynamics model may lead to exploration of irrelevant aspects due to \"TV static\". Predicting dynamics in feature space can help alleviate this issue. In stochastic environments, errors from a deterministic forward dynamics model may lead to exploration of irrelevant aspects due to a \"lottery\" effect. This can result in high rewards for participating in unpredictable outcomes, such as in games with random obstacles and enemies. A stochastic dynamics model may be more appropriate in such situations for measuring exploration progress. In stochastic environments, errors from a deterministic forward dynamics model may lead to exploration of irrelevant aspects due to a \"lottery\" effect. A stochastic dynamics model may be more appropriate for measuring exploration progress, with possible goals including learning about the environment dynamics, obtaining control policies, fine-tuning policies for optimizing rewards, and discovering unexpected aspects of the environment. Progress can be measured by achieving high returns, counts of visited states, and other proxies. In reinforcement learning, progress can be measured by high returns, counts of visited states, and time staying alive. Clues in the environment can aid learning but can be challenging to specify. Some games require specific actions to start, but these are avoided in the agent's training. Death is viewed as just another transition to be avoided only if it is uninteresting. End of episode signals can provide significant information in some games. In reinforcement learning, agents tend to avoid dying after exploration, as it brings them back to a mastered state space. Some games have stochastic transitions, posing a challenge for deterministic models. Using a stochastic dynamics model is left for future work. The HER method is used for learning a policy to achieve any goal by training an embedding network \u03c6. It is based on the Bellman loss for goal-conditioned state-action value function with a discount factor \u03b3. The algorithm learns state-action values with respect to future goals. The HER method involves training an embedding network \u03c6 to learn state-action values for future goals. The model samples transitions from a replay buffer and uses a goal-conditioned state-action value function q(s, a; g) to represent aspects of the environment useful for control. Features are learned using a neural network to embed previous and next states for predicting actions. The HER method trains an embedding network to learn state-action values for future goals. Features are learned using a neural network to embed states for predicting actions. The exploration method Fixed Random Features (FRF) uses a fixed embedding network during training, while Curiosity by Bootstrapping Features (CBF) learns features only from policy optimization. Curiosity by Bootstrapping Features works through a bootstrapping effect where features encode relevant aspects of the environment, and the forward model predicts transitions of those features. Initially, features contain little information but as the policy seeks unpredictable transitions, additional aspects of the environment are encoded in the feature space. This process continues as more aspects become relevant, leading to an unstable fixed point where features are constant and the dynamics model predicts constant transitions. The fixed point in the feature space is unstable, with features encoding more aspects of the environment over time. The models were implemented using Tensorflow and OpenAI Gym, with extrinsic return used as a proxy for exploration. There is no guarantee that extrinsic return aligns with intrinsic interestingness based on dynamics. The text chunk describes the process of optimization steps in a reinforcement learning algorithm, including adding data to a replay buffer, optimizing parameters, and using different auxiliary tasks. The hyperparameters for the experiments are also mentioned. The text chunk discusses joint training with random agent RAND, hyperparameter selection based on PPO and DQN implementations, adjustments made for PPO on Pong and Breakout, testing on various games including Super Mario Bros and VizDoom, and the results of Pong experiments showing the effectiveness of joint training with IDF. After testing various methods, joint training proves more effective than non-joint approaches. An unexpected bug in the Atari emulator caused the ball to disappear during gameplay. Breakout experiments show that joint training with HER performs exceptionally well. Super Mario Bros was tested but encountered speed issues. After encountering speed issues with Super Mario Bros, the researchers switched to using an internal version with an action wrapper replicating the action space used in BID19. Results showed that their best method was able to pass the first 4 levels, defeat the boss Bowser, and move onto the second world in the game. The agent also found the secret warp room on level 2 and visited all of the pipes. The agent in the 'DoomMyWayHomeVerySparse' environment with 9 connected rooms receives an extrinsic reward per timestep, a timelimit of 2100 steps, and a reward for getting the vest. Different methods were tested, with HER, joint HER, and CBF methods reliably reaching the vest. The agents' progress was tracked by recording their (x, y) coordinates. In the 'DoomMyWayHomeVerySparse' environment, agents' progress was tracked by recording their (x, y) coordinates and visualizing their locations on a grid. Most methods achieved good maze coverage, with even random agents performing well. Training on fixed random features resulted in poor exploration. The number of unique bins visited increased with training for each method. IDF performed the best among all methods tested. The best run passed almost 4 levels, with IDF performing the best among all methods tested. Agents used a unique strategy of hovering below the water's surface to survive indefinitely. Joint training methods showed good results for exploration, with CBF performing surprisingly well. Future research directions include testing CBF on environments with continuous action spaces and exploring feature-bootstrapping for count-based exploration. Experimental details for Atari games preprocessing involved downscaled observations to 84x84 pixels, grayscale conversion, stacking sequences of four frames, and using a frame skip of four. Implementation of HER and PPO was based on code from BID10, with the embedding network consisting of three convolutional layers and a dense layer similar to DQN. The network architecture included convolutional and dense layers for the policy and auxiliary heads, with specific configurations for the forward dynamics, HER, and IDF tasks. Hyperparameters were consistent across experiments, except for adjustments to the entropy bonus and optimization steps. A discount factor of 0.99 was used for the HER task. For the HER task, a discount factor of 0.99 was used. Stabilization technique for the target value in the Bellman loss involved a Polyak-averaged version of the value function with a decay rate of 0.999. The learning rate for the HER task was 10^-4. Experience replay buffer contained 1000 timesteps per environment. Learning rate for the forward dynamics model was 10^-5. Training runs consisted of 50e6 steps, equivalent to 200e6 frames with a standard frame-skip of 4. Salient differences with the work BID19 were noted."
}