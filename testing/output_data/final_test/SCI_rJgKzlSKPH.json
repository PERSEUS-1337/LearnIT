{
    "title": "rJgKzlSKPH",
    "content": "Deep neural networks (DNNs) are prominent in machine learning research, with large models requiring high computational effort. To be effective on embedded devices, DNNs need to reduce model size and inference time significantly. We propose a soft quantization approach to train DNNs using fixed-point arithmetic, reducing computational effort and memory costs. Our method achieves state-of-the-art performance with 4-bit fixed-point models on CIFAR-10 dataset. Modern DNNs require large models and training data for optimal results, leading to high memory usage and computation costs. To address this, fixed-point quantization reduces memory, inference time, and energy consumption by converting floating-point to fixed-point arithmetic. Ternary or binary-valued weights can further reduce computational complexity by replacing multiplications with additions. Quantization approaches focus on the Linear component in modern DNNs, preserving floating-point batch normalization layers. This is crucial as BN layers are folded into the preceding layer after training, destroying its fixed-point representation. Channel-wise floating-point multiplications are also a concern. In this paper, a soft quantization approach is proposed to learn pure fixed-point representations of state-of-the-art DNN architectures. The approach follows a block structure and transfers individual components into fixed-point representations before combining them. Bit-size dependent fixed-point constraints are formulated for each component and transferred into regularization terms. This is the first soft quantization approach to learn pure fixed-point representations of DNNs, validated on benchmark datasets and architectures. The approach in this paper focuses on soft quantization to learn fixed-point representations of DNN architectures. Two special cases are tested: a pure fixed-point model with 4-bit weights and activations, and a model with ternary-valued weights and 4-bit activations. Different quantization strategies can be applied to components like convolution, batch normalization, and ReLU activation. Recent approaches and their respective bit-sizes during test time are summarized in Table 1. Recent approaches in DNN architectures include using binarized weights with high-precision counterparts, combining ternary-valued weights with a real-valued step-size, and discretizing weights and activations to \u00b11 during the forward pass. These methods aim to optimize model capacity and convergence during optimization processes. During the backward pass, the straight-through estimator (STE) is used to estimate the local gradient of the rounding function for passing on the upstream gradient during backpropagation. Recent approaches involve using fixed-point quantization functions with learnable parameters, such as signed quantization functions with adjustable step sizes and dynamic ranges. Optimization constraints are proposed to limit memory costs, and a soft quantization approach is suggested for training DNNs with multimodal fixed-point weights. During training, various approaches have been proposed to optimize posterior distributions for post-quantization. Different regularization terms and Bayesian methods have been used to train DNNs with quantizing priors, resulting in multimodal weight distributions. High-precision BN layers are crucial for success, as they increase model capacity but limit the use of pure fixed-point arithmetic on dedicated hardware. A hard quantization framework has been introduced to train DNNs in low-bit fixed-point arithmetic, including fixed-point BN layers, focusing solely on computations and variables within the training procedure. In this work, the authors focus on completing the soft quantization approach by learning pure fixed-point representations on DNN architectures. They introduce fixed-point constraints for each component during training, including a novel fixed-point BN constraint. Additionally, they propose an exponentially increasing regularization parameter to control model capacity and improve numerical stability. The authors introduce fixed-point constraints for DNN architectures during training, improving numerical stability and accelerating computations. Fixed-point numbers consist of N-bit integers with a global scaling factor, allowing for faster operations on fixed-point hardware. Converting all layers into fixed-point representations and meeting specific conditions is necessary for evaluating DNNs using pure fixed-point arithmetic. The authors introduce fixed-point constraints for DNN architectures during training, improving numerical stability and accelerating computations. The training objective involves regularization terms and a quantization function that maps input signals to discrete values using basic operations like scaling, rounding, and clipping. Three types of quantization functions are used based on different layer types and value ranges, with uniform step-sizes and a fixed-point representation fulfilled when the step-size is a power of two. The authors introduce fixed-point constraints for DNN architectures during training, improving numerical stability and accelerating computations. The scaling operation is replaced by a bit shift when the step-size is a power of two. An additional logarithmic quantizer rounds input values to the closest power of two, enabling fixed-point constraints for important layers. Convolution and fully-connected layers are summarized as Linear components performing weighted sums. Weight multiplications are the focus, with individual Gaussian priors recommended to change the weight distribution. The authors recommend using individual Gaussian priors to change the weight distribution in DNN architectures, inducing symmetric and multimodal distributions. The priors are updated with every forward pass, allowing weights to switch between neighboring modes continuously. The actual step-sizes are learned within the batch normalization component. The weights in DNN architectures can switch between neighboring modes continuously using individual Gaussian priors. Batch normalization layers can be folded into preceding layers after training to increase efficiency. Batch normalization is performed channel-wise, with each channel normalized and transformed linearly. Batch normalization is performed channel-wise, with each channel normalized using mean and variance of the current mini batch during training. At test time, normalization is done using layer statistics that have been continuously updated during training. Each channel is shifted and multiplied with a learnable parameter \u03b3, which can be regularized using the L2-norm. After training, Batch Normalization parameters are quantized by folding BN layers into the preceding Linear component, simplifying the BN calculation. The fixed-point constraint is still fulfilled post-training. After training and quantization, the BN fixed-point constraint enables individual step-sizes that fulfill the constraint. ReLU is used as the non-linear activation function, and a uniform unsigned quantization function is applied during forward pass. The STE is utilized for defining partial derivatives during the backward pass. The gradient with respect to the input and step-size is determined based on saturation bounds and quantization steps. The step size must be a power of two to meet the fixed-point constraint. A regularization term is proposed to adjust the step-size gradient during training to approximate the fixed-point constraint. The regularization parameter in training controls the model capacity by balancing learning task and quantization. It helps prevent overfitting and ensures optimal convergence. A training-time dependent parameter can adjust model capacity dynamically, with a recommended linear increase approach by Enderich et al. (2019). The regularization parameter in training adjusts model capacity dynamically, with exponential growth recommended for better capacity changes. The calculation involves an initial value \u03bb(0), current training epoch e, and growth factor \u03b1 E. Weight clipping in soft quantization aims for well-qualified posterior distributions. In soft quantization, weight clipping is used to limit parameter space and promote reasonable weight adaptation. Quantization values are constrained to discrete intervals to prevent optimization issues. Step-size clipping ensures positive quantization steps and avoids numerical problems. In soft quantization, weight clipping limits parameter space for reasonable weight adaptation. Quantizing gradients are scaled by regularization parameters before update steps, with gradients clipped for numerical stability. Evaluation of fixed-point model on MNIST, CIFAR-10, and CIFAR-100 datasets using stochastic gradient descent with nesterov momentum. Comparison with approaches from Table 1 using 4-bit weights and activations, showing comparable performance to floating-point baseline in Table 2. The Add-Net model utilizes symmetric 2-bit weights, 4-bit activations, and bit-shift batch normalization. It achieves a test error of 0.65% on the MNIST dataset after 40 epochs of training. The model's performance is comparable to other quantization methods like SGM and TWN. Our Fix-Net model utilizes 2-bit weights and activations, achieving a test error of 0.59% on the CIFAR-10 dataset. The data consists of 32\u00d732 RGB images divided into training and test samples. We preprocess the images as recommended in previous studies. Three different model architectures are used for testing: VGG7, DenseNet (L=76, k=12), and ResNet20. DenseNet and ResNet20 are considered difficult to quantize due to their efficient architecture with fewer parameters. Our Add-Net model performs best among all models with binary or ternary-valued weights, with an error rate of 6.22%. The Fix-Net model achieves the best accuracy with an error rate of 4.98%, outperforming the floating-point baseline. The Fix-Net configuration is comparable to previous studies with error rates of 6.21% and 8.30%. In the DenseNet architecture, Add-Net achieves a test error of 6.54% and outperforms other approaches. The Fix-Net configuration achieves a test error of 5.63% in the ResNet architecture. Our Add-Net model achieves a test error rate of 10.13%, while DQ performs slightly better at 9.62%. The Fix-Net reduces the test error to 8.68%, but still falls short of the floating-point baseline. CIFAR-100 presents a challenging classification task with limited training samples. The Add-Net model achieves a test error rate of 33.16%, while the Fix-Net configuration performs best with an error rate of 30.25%, outperforming the floating-point baseline. Soft quantization reduces DNN complexity at test time by learning fixed-point representations. Our approach involves increasing fixed-point priors and weight clipping for self-reliant weight adaptation, leading to new state-of-the-art quantization results. The combination of 4-bit weights, 4-bit activations, and fixed-point batch normalization layers shows promise. The training process of the VGG11 Add-Net on Cifar-100 is illustrated in Figure 3, depicting the weight distribution evolution of different layers over epochs. Initial weights exhibit a unimodal distribution with peaks at zero, which then rearrange to meet fixed-point constraints and learning objectives. After increasing fixed-point priors and weight clipping, our approach rearranges weights into a ternary-valued distribution, reducing variance with regularization. By epoch 100, weights are close to fixed-point centers, minimizing post quantization errors. Note: y-axis scaled individually for convenience, x-axis wider for epoch 0 to capture entire distribution."
}