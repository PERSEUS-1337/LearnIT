{
    "title": "r1eJssCqY7",
    "content": "Neural Network (NN) has excelled in image, speech, and text tasks due to specialized structures like CNN and RNN. However, designing specific structures for diverse tabular data is challenging. To address this, a universal neural network solution called TabNN is proposed to automatically derive effective architectures for tabular data tasks. TabNN is designed to automatically derive effective NN architectures for tabular data tasks by leveraging expressive feature combinations and reducing model complexity. It uses GBDT to power its implementation and has shown better performance than baseline solutions in experimental analysis on various tabular datasets. TabNN is designed to automatically derive effective NN architectures for tabular data tasks by leveraging expressive feature combinations and reducing model complexity. It uses GBDT to power its implementation and has shown better performance than baseline solutions in experimental analysis on various tabular datasets. The design of structures is based on prior knowledge in specific domains, such as CNN for image classification and RNN for speech recognition and language modeling. Real-world applications like click through rate prediction, time series forecasting, and web search ranking often involve structured input with multi-dimensional features. TabNN is designed to automatically derive effective NN architectures for tabular data tasks by leveraging expressive feature combinations and reducing model complexity. Despite the success of CNN and RNN in other domains, adopting NN over tabular data remains challenging due to unsatisfactory performance with Fully Connected Neural Networks. Recognizing effective feature combinations is crucial for designing effective NN models on tabular data. To design effective NN models on tabular data, two principles are identified: leveraging expressive feature combinations and reducing model complexity. A universal neural network solution, TabNN, is proposed to achieve this goal in various tasks. TabNN is a solution for deriving effective neural network architectures for tabular data tasks by leveraging knowledge from GBDT models. It consists of four steps: Automatic Feature Grouping, Feature Group Reduction, Recursive Encoder with Shared Embedding, and Transfer Structured Knowledge. The TabNN solution leverages knowledge from GBDT models to create effective neural network architectures for tabular data tasks. Extensive experiments demonstrate its superior performance compared to other solutions, making it a versatile off-the-shelf model for various datasets. The proposed method is an off-the-shelf model for tabular data that achieves state-of-the-art performance by efficiently utilizing tree-based methods like GBDT and Random Forest. These models excel at selecting features with the most statistical information gain to fit targets well, but struggle to integrate into complex frameworks. The proposed method efficiently utilizes tree-based models like GBDT and Random Forest for tabular data, achieving state-of-the-art performance. However, these models face challenges in integrating into complex frameworks and learning from streaming data. Soft decision trees and neural decision trees have been proposed to address these issues. Target propagation is suggested for passing back errors in non-differentiable functions, but it is less efficient than back-propagation. Learning tree-based models like XGBoost and LightGBM over streaming data is challenging due to the need for global statistical information. While some models have been designed for learning trees from streaming data, their performance falls short compared to using all data at once. Efforts are being made to address these shortcomings and improve the efficiency of tree-based methods. Recent studies have focused on using neural networks (NN) for tabular data applications like click-through rate prediction and recommendation systems. While most studies concentrate on preprocessing categorical features for NN, the utilization of numerical features remains limited. This paper aims to provide a universal NN solution for all types of tabular data. Efforts have also been made to combine NNs with tree-based methods to leverage their respective advantages. Efforts have been made to combine neural networks (NN) with tree-based methods for tabular data applications. Two main approaches include tree-like NNs with decision ability and converting trained decision trees to NNs. Efforts have been made to combine neural networks (NN) with tree-based methods for tabular data applications. Two main approaches include tree-like NNs with decision ability and converting trained decision trees to NNs. However, converting tree-based models to NNs can be inefficient and challenging for realistic scenarios. Additionally, current efforts in network architecture search for NNs mainly focus on non-tabular data applications like computer vision and speech recognition, making it difficult to apply to tabular data learning. These search methods are time-consuming due to enumerating combinations in a large search space. In this paper, an efficient and strategic way to automatically derive effective neural network (NN) architecture for tabular data is proposed. The design of TabNN follows two key principles: explicitly leveraging expressive feature combinations and increasing information gain for the learning task. Compared to traditional methods, TabNN's explicit feature combinations are more robust and can significantly improve performance. In this paper, a GBDT-powered TabNN is proposed to reduce model complexity and improve generalization ability. TabNN follows two key principles: leveraging expressive feature combinations and reducing unnecessary parameters. The TabNN architecture includes Automatic Feature Grouping to discover effective feature groups from tabular data using GBDT. The designed NN model leverages feature combinations from feature groups derived from a tabular dataset using GBDT. Feature Group Reduction clusters feature groups into sets to reduce parameters and encourage parameter sharing. Recursive Encoder with Shared Embedding effectively utilizes common features to reduce parameters in the architecture. The Recursive Encoder with Shared Embedding (RESE) is an efficient NN architecture designed over clustered tabular feature groups based on the results of Feature Group Reduction (FGR) and feature group importance from GBDT. The Transfer Structural Knowledge from GBDT (TSKG) step aims to transfer rich structural knowledge from GBDT trees to the NN architecture. The AFG component determines expressive feature combinations in the dataset. The TabNN model is designed to identify important feature combinations for tabular data dynamically and automatically. While traditional methods like correlation tests and feature clustering can obtain feature groups dynamically, they may not capture complex combinations among features. Tree-based models, on the other hand, are effective in discovering rich non-linear dependencies among features, making them a valuable resource for identifying expressive feature combinations. The GBDT model is chosen for its ability to create diverse feature groups by learning many trees. Each tree in the model represents a feature group, allowing for the merging of overlapping features into compact sets. This approach aims to reduce redundancy and improve performance. The FGR method aims to merge feature groups into compact sets for parameter sharing, addressing computational complexity as an NP-hard problem. The FGR problem aims to minimize the maximum load of machines by merging feature groups into compact sets for parameter sharing. It is proven to be NP-hard by reducing the P m||C max problem to an instance of the FGR problem. The FGR problem, proven to be NP-hard, faces challenges with increasing feature groups and shrinking intersections. To address this, soft intersections are adopted, with \u03b1-soft intersection defined as features covered by \u03b1 fraction of all groups. The soft intersection version of FGR remains NP-hard, leading to the use of a heuristic algorithm for efficiency. The algorithm in Alg. 2 enumerates feature groups randomly and adds them greedily based on gain. This process is repeated n times to select the sets with the largest minimum \u03b1-soft intersection. Despite not guaranteeing an optimal solution, it efficiently provides a sub-optimal solution. The Recursive Encoder with Shared Embedding organizes feature groups generated by FGR into efficient NN architectures based on their diverse importance within sets. The Recursive Encoder with Shared Embedding (RESE) approach proposes a recursive NN architecture based on feature group sets generated by FGR. The architecture utilizes a recursive structure to prioritize important feature groups and share parameters for efficient learning. The RESE architecture is illustrated in FIG1, with circles representing neurons and arrows indicating connections. Smaller indices denote layers closer to the output layer. The RESE approach utilizes a recursive NN architecture to prioritize important feature groups by arranging them based on descending importance. It exponentially increases the number of feature groups in each layer to reduce model complexity and encourage parameter sharing within feature group sets. The RESE approach utilizes a recursive NN architecture to prioritize important feature groups by arranging them based on descending importance. It encourages parameter sharing within feature group sets by extracting common features and using them as shared inputs to all layers. The architecture efficiently arranges feature groups and reuses the embedding of common features. The RESE approach utilizes a recursive NN architecture to prioritize important feature groups by arranging them based on descending importance. It encourages parameter sharing within feature group sets by extracting common features and using them as shared inputs to all layers. Additionally, the model employs shared embedding to reduce parameters and improve back-propagation efficiency. Key details include shrinking feature representation, using non-linear activation functions, and incorporating multiple fully connected layers before the output. The RESE approach utilizes a recursive NN architecture to prioritize important feature groups by arranging them based on descending importance. It encourages parameter sharing within feature group sets by extracting common features and using them as shared inputs to all layers. Additionally, the model employs shared embedding to reduce parameters and improve back-propagation efficiency. Final combination involves concatenating outputs of k RESE modules for the final fully connected layer. Transfer Structural Knowledge from GBDT involves using knowledge distillation technology to transfer GBDT's structural knowledge for better model initialization in TabNN. The RESE approach utilizes a recursive NN architecture to prioritize important feature groups. It uses shared embedding to reduce parameters and improve back-propagation efficiency. To transfer structural knowledge from GBDT, knowledge distillation technology is employed for better model initialization in TabNN. In training data, leaf indices from trees in G j are extended with one-hot representation to represent structural knowledge efficiently. Embedding technology BID48 is used to reduce dimensionality while retaining important information. To speed up embedding learning, a FCNN with one hidden layer is used to learn the embedding supervised, based on bijection relations between leaf indices and values. The one-hot coding of leaf index is taken as input, with the corresponding leaf value as the training label. The output of the hidden layer is the embedding, denoted as H j,d. After preparing H j, the data (D, H j) is used to pre-train the parameter \u03b8 j of the RESE module for G j. Once all RESE modules are initialized, they are concatenated and the whole architecture is trained from ground truths. Thorough evaluations on TabNN 1 are conducted by comparing its performance with several methods. Thorough evaluations were conducted on TabNN 1 by comparing its performance with various baseline methods over public tabular datasets. The datasets cover diverse real-world applications, and to ensure efficient learning, numerical features were normalized and categorical features were converted to numerical vectors. TabNN was compared with GBDT as one of the baselines. TabNN was evaluated against baselines such as GBDT and FCNN, with hyper-parameter settings optimized using NNI BID47. NRF (GBDT) BID5 converts regression trees to NN for fair comparison. NNRF is a recent NN solution for tabular data designed for classification tasks. Two simplified variants of TabNN were added for comparison, including TabNN (R) randomly clustering features. TabNN outperforms baselines on tabular datasets, including well-tuned FCNN. Training curves show TabNN converges faster than FCNN. TabNN utilizes structural tree knowledge and random feature combinations for improved performance. TabNN outperforms baselines and achieves superior performance in tabular data learning. It converges faster than FCNN and can further improve performance by leveraging knowledge learned by GBDT. NNRF, converted from GBDT, tends to overfit and performs worse than FCNN on large datasets. TabNN's performance surpasses all baselines, demonstrating its effectiveness in tabular data learning. The importance of tree knowledge from GBDT in TabNN is highlighted by the gaps in performance and faster convergence shown in the results. The RESE module significantly reduces the number of parameters compared to the basic Concat approach, which linearly increases with the number of trees. The RESE module in TabNN reduces model complexity significantly compared to the basic Concat approach, which increases linearly with the number of trees. This demonstrates the importance of key components in TabNN for enhancing effectiveness and efficiency in tabular data learning. Additionally, TabNN shows an advantage over GBDT in learning from streaming data, as demonstrated in a simulation experiment using FLIGHT data. TabNN is a universal neural network solution for tabular data learning that leverages GBDT to power its implementation. It excels in streaming data learning and demonstrates significant performance improvement with more data. The design of TabNN focuses on leveraging expressive feature combinations and reducing model complexity. TabNN utilizes GBDT to automatically identify feature groups and cluster them for parameter sharing. It then uses tree importance knowledge to construct recursive NN architectures, enhancing training efficiency. Experimental results show TabNN's advantages in modeling tabular data. The model setting details for the experiments include using LightGBM with specific parameters, batch normalization, ReLU activation, and AdamW optimizer for FCNN. Hyper-parameter tuning was done using NNI toolkit to select the best model among various settings. The hyper-parameter searching in NNI includes learning rate, batch size, and FCNN structure. The best models on each dataset outperform the human setting. NRF (GBDT) replaces Random Forest with GBDT for fair comparison. NNRF's depth is set to log2(#class) + 1, with 150 NNs for ensemble. Learning rate is 0.001, batch size is 128, and three hidden layers are added before the output of RESE module."
}