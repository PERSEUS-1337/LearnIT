{
    "title": "HylijQ35IS",
    "content": "The Deep Image Prior (DIP) is a recent approach for image recovery that is not fully understood. This study investigates the early outputs of DIP, showing invariance to adversarial perturbations and potential as a defense against adversaries. The early DIP outputs may remove non-robust image features, as evidenced by classification confidence values. The Deep Image Prior (DIP) is a powerful approach for image restoration tasks like denoising, super-resolution, and in-painting. It demonstrates comparable performance to state-of-the-art methods by using an untrained network trained to guide output towards a specific target image. The network architecture acts as a strong prior and has shown excellent performance as a natural image prior. This is significant in detecting natural images, especially with the rise of security concerns related to adversarial examples. The Deep Image Prior can be used as a recovery method against non-natural perturbations. The Deep Image Prior is utilized as a recovery method against perturbations and to develop an adversarial defense. Saliency maps are produced to highlight relevant pixels for target classification, aiming to show clearer and more robust image features. Adversarial examples are attributed to nonrobust features in images, with the Deep Image Prior successfully recovering original classes from such examples. The Deep Image Prior is tested using a dataset of robust and non-robust image features to see if it can select robust images. Different methods for generating adversarial examples are considered, including FGSM, BI, and LLCI. The study aims to demonstrate the ability of the Deep Image Prior to recover from adversarial perturbations. The Deep Image Prior is tested with various adversarial methods to generate examples. Results show the classifier can recover true class confidence values, with differences in peak confidence iterations among adversaries and strengths. The Deep Image Prior is effective in recovering from adversarial perturbations, with differences in peak confidence iterations among adversaries and strengths. A novel saliency map approach, MIG-SG, is introduced to analyze the transformation of input into DIP outputs. The saliency map shows key features of the knife identified after 300-400 iterations, with a confidence increase in DIP output. However, at 2000 and 4000 iterations, focus shifts to the blade, losing clarity in salient features. The saliency maps lose features as DIP converges towards an adversarial example, indicating early DIP outputs are resistant to adversarial effects. The Deep Image Prior shows resistance to adversarial effects, aiming to make adversarial perturbations undetectable. Results are compared to a defense using randomization. Using the Deep Image Prior decreases accuracy on clean images, with a noticeable decrease in top-1 accuracy, especially with fewer DIP iterations. Top-1 accuracy increases as the number of iterations increases. The Deep Image Prior demonstrates competitive performance against adversarial attacks, showing higher accuracy compared to a reference defense method. Increasing the number of iterations improves top-1 accuracy, but also slows down computational speed. However, testing for larger iteration numbers was not done due to high computational costs. Overall, there is a trade-off between correctly classifying images and defending against adversaries, similar to findings by Ilyas et al. (2019). The Deep Image Prior uses a robust image dataset to highlight its ability to select robust features. The architecture was adjusted for CIFAR-10 images, and results show that robust images contain more information about the true class compared to non-robust images. Non-robust datasets show a trough in classification confidence before converging, while robust datasets show a peak. The study evaluated the sensitivity of the Deep Image Prior (DIP) to changes in network architecture. While some changes had little impact on performance, the network failed without skip connections or with a very shallow network. No evidence supported the idea of sensitivity as a \"resonance\" as mentioned in the original paper. The study observed the network's ability to recover from adversarial perturbations and create a promising adversarial defense. Evidence was provided for the Deep Image Prior's selection of robust image features over non-robust features in its early iterations. The method integrates a baseline and the image numerically by calculating the forward derivative of the network using SmoothGrad. Combining two methods improves saliency maps by averaging derivatives over multiple samples. A modification to the algorithm, taking the absolute value of the final result, produced promising results. Using absolute values of gradients for colored images allows negative gradients to contribute to salient features. The saliency method involves integration steps and samples for computation. The MIG-SG saliency map focuses on the eyes and mouth of a panda image, providing useful visual interpretation. The Deep Image Prior was tested with iteration numbers of 500, 750, and 1000, showing less sensitivity to adversarial perturbations at earlier iterations. The Deep Image Prior was tested with iteration numbers of 500, 750, and 1000 on a dataset of images from 200 randomly selected classes from the ImageNet database. The defence performance was evaluated on 500 images correctly classified using the ResNet18 classifier. Two architectures were considered, with changes in encoder depth and feature maps compared to the original paper. Architecture details can be found in Tables 3 and 4."
}