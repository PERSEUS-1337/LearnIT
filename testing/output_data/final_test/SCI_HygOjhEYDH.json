{
    "title": "HygOjhEYDH",
    "content": "Temporal point processes are commonly used to model sequences of events occurring at irregular intervals. Instead of estimating the conditional intensity function, this study proposes directly modeling the conditional distribution of inter-event times using normalizing flows. A simple mixture model is also introduced for increased flexibility and efficiency, achieving high performance in prediction tasks and being applicable to various scenarios such as learning sequence embeddings and imputing missing data. Visits to hospitals, purchases in e-commerce systems, financial transactions, and social media posts can be represented as discrete events using temporal point processes. By combining these models with deep learning, algorithms can learn complex behavior from real-world data. Existing methods fall short in terms of flexibility, efficiency, and ease of use. Instead of modeling the intensity function, this study proposes directly modeling the conditional distribution of inter-event times using normalizing flows for improved performance in prediction tasks. The study proposes treating learning in temporal point processes as conditional density estimation using neural density estimation tools. Contributions include connecting temporal point processes with neural density estimation, introducing a simple mixture model for prediction, and demonstrating the model's versatility in various tasks through experiments. The paper discusses generation, sequence embedding, and training with missing data in temporal point processes. It explains the use of conditional intensity functions to specify point processes with predefined behaviors like self-exciting and self-correcting processes. The study connects temporal point processes with neural density estimation for conditional density estimation using neural tools. Intensity parametrization is commonly used in learning models from data, with the main challenge being choosing a good parametric form. Several approaches are developed for modeling inter-event times, including neural density estimation methods and a simple mixture model. The core idea of normalizing flows is to define a flexible probability distribution by transforming a simple one using invertible transformations. By stacking multiple transformations, an expressive probability distribution can be obtained. Sampling from this distribution involves drawing samples and computing forward transformations. The density of an arbitrary point can be obtained by evaluating inverse transformations. In normalizing flows, flexible functions like neural networks are used to parametrize transformations. While the inverse function exists, it often requires numerical approximation. In this context, the goal is to model the distribution of inter-event times for maximum likelihood estimation. In normalizing flows, neural networks parametrize transformations to model inter-event time distribution for maximum likelihood estimation. The inverse transformation g \u22121 converts positive \u03c4 into z M, using parametric functions f \u03b8 like deep sigmoidal flow (DSF) and sum-of-squares (SOS) polynomial flow. The models DSFlow and SOSFlow stack multiple layers of f \u03b8 to approximate any transformation, with g \u22121 m = f \u03b8m and a final sigmoid transformation g \u22121 1 = \u03c3 to convert z 2 into z 1 \u2208 (0, 1). The DSFlow and SOSFlow models use inverse transformations for efficient training via maximum likelihood. These models are flexible in approximating any distribution but may not be optimal for computing expected time until the next event. Designing a model for p(\u03c4) that is as expressive as flow-based models but allows for easy sampling and computing moments in closed form is a challenge. The log-normal mixture model is used for density estimation in low dimensions, specifically for modeling the distribution of one-dimensional inter-event times \u03c4. The model architecture includes mixture weights, means, and standard deviations to generate parameters based on conditional information. The simplicity of the log-normal mixture model makes it attractive for its ease of sampling and computing moments in closed form. The log-normal mixture model is attractive due to its simplicity and properties. Moments can be computed easily, and sampling can be done in closed form. Gradients of samples can be computed w.r.t. model parameters, making it useful for applications like reinforcement learning. The log-normal mixture model is useful for applications like reinforcement learning as gradients can be computed w.r.t. model parameters. Reparametrization gradients have lower variance and are easier to implement than score function estimators. Reparametrization sampling can also be used for training with missing data and capturing dependencies in temporal point processes. Conditioning on additional features allows for the distribution of the time until the next event to depend on factors other than the history. Side information, denoted as a vector y i, influences the distribution p*(\u03c4 i |y i) without affecting the objective function. Learnable sequence embedding vectors can help distinguish between sequences from different distributions when learning from multiple event sequences. The model learns to distinguish between sequences from different distributions using embeddings for visualization and downstream tasks. Parameters for the distribution are obtained by concatenating history, metadata, and sequence embeddings into a context vector. The parameters are then calculated as an affine function of the context vector, resembling a mixture density network architecture. The process is illustrated in Figure 1, and parameters for flow-based models are obtained in a similar manner. The SOSFlow and DSFlow models, along with mixture models, can approximate any probability density on R well. The mixture distribution is as expressive as flow-based models, assuming the RNN can encode all relevant information into the history embedding. The universal approximation theorems for RNNs do not guarantee the quality of the approximation or provide guidance on learning model parameters. However, models with the UA property tend to outperform less flexible ones. Theorem 1 does not restrict the base density q(x) to a log-normal distribution, allowing for other distributions like mixtures. Intensity function is important in modeling traditional models like self-exciting processes, as it is intuitive, easy to specify, and reusable. However, modern RNN-based intensity functions may offer advantages over modeling the conditional density. The proposed models are as intuitive and interpretable as existing intensity-based neural network models. The intensity function \u03bb * (t) is easy to specify, while p * (\u03c4 ) must integrate to one. By using normalizing flows or a mixture distribution, the PDF integrates to one without sacrificing model flexibility. Merging two independent point processes with intensities \u03bb * 1 (t) and \u03bb * 2 (t) results in a merged process with intensity \u03bb * (t) = \u03bb * 1 (t) + \u03bb * 2 (t). The CDF of the merged process is obtained as 2 (\u03c4). Modeling p * (\u03c4) instead of \u03bb * (t) does not impose any restrictions. Modeling p * (\u03c4) instead of \u03bb * (t) allows for flexibility and does not restrict the approach. A mixture distribution is favored for its flexibility, ease of sampling, and well-defined moments compared to other intensity-based deep learning models. Recent works propose neural-network-based point process models to address model misspecification issues with simple TPP models. These neural models define intensity functions and offer flexibility at the cost of requiring Monte Carlo integration for likelihood evaluation. Recent works propose using RNNs to encode event history into a vector h i, which is then used to define conditional intensity models. Constant intensity models correspond to exponential distributions, while exponential intensity models correspond to Gompertz distributions. A flexible FullyNN intensity model is introduced by Omi et al. (2019) to model the cumulative intensity function with a neural net. This approach offers more flexibility compared to traditional models. The FullyNN model, while flexible, has a suboptimal network architecture leading to non-zero probability for negative inter-event times. In contrast, SOSFlow and DSFlow ensure a valid PDF on R+. Previous works used kernel mixtures for conditional intensity functions but could only capture self-exciting influences and lacked closed-form expectation computation and sampling capabilities. Neural models proposed for learning marked TPPs focus on event type prediction and share limitations of other intensity-based approaches. Recent works explore alternatives to maximum likelihood training, such as noise-contrastive estimation, Wasserstein distance, and reinforcement learning. These models can be combined with the proposed models for TPPs. In mixture density networks and normalizing flows, distribution parameters are directly produced or obtained by transforming a simple distribution. These approaches have been used for modeling sequential data, but have not been applied to Temporal Point Processes (TPPs) before. The proposed models are evaluated on event time prediction tasks and show potential for incorporating extra conditional information, training with missing data, and learning sequence embeddings using real-world datasets. The curr_chunk discusses event data from various domains and the generation of synthetic datasets. It also introduces different models such as SOSFlow, DSFlow, LogNormMix, RMTPP, FullyNN, and LogNormal for analyzing the datasets. An RNN is used to encode the history for all models. The curr_chunk discusses the benefits of the mixture model in predicting event times using RNN encoding. Different models are compared, excluding NeuralHawkes. Datasets are split for training, validation, and testing, with models trained using negative log-likelihood. Hyperparameters are tuned for fair comparison, and results are reported based on NLL loss averaged over 10 runs. The NLL loss of each model on the test set is reported, averaged over 10 train/validation/test splits. Differences in NLL loss compared to the LogNormMix model are highlighted for real-world datasets. Simple unimodal distributions are outperformed by more flexible models like LogNormMix and DSFlow. LogNormal shows better fit than RMTPP/Gompertz. Real-world data often exhibit heavy-tailed inter-event time distributions. The LogNormMix and DSFlow models consistently achieve the best loss values in learning marked temporal point processes. Marks are known to enhance model performance, and the RNN now takes a tuple (\u03c4 i , m i ) as input at each time step. Results show LogNormMix's dominant performance in the marked case, with improved NLL loss compared to other models on real-world datasets. In the study, the LogNormMix model is analyzed to determine if additional conditional information can enhance model performance in predicting the time until the next check-in for a restaurant. Different variants of the model are considered, with results showing that the inclusion of conditional information improves performance. The study also addresses dealing with missing data in practical scenarios. TPPs are a generative model used to handle missing data through imputation in sequences generated by a Hawkes process. Three strategies are considered for learning from partially observed sequences: ignoring the gaps, filling them with average \u03c4, or filling them with samples generated by the model. Sampling with reparametrization is needed to optimize the expected log-likelihood in case (c). More details on the setup can be found in Appendix F.4. The LogNormMix model is crucial for training to optimize loss. Different sequences in the dataset may have varying inter-event times, and assigning trainable embedding vectors to each sequence helps the model distinguish between them. The learned sequence embeddings capture similarities between sequences. The LogNormMix model is used to learn sequence embeddings for different sequences in the dataset, allowing the model to differentiate between sequences from different distributions in an unsupervised manner. The model also generates sequences using the learned embeddings and maps them to different distributions. Additionally, tools from neural density estimation are used to design new models. The LogNormMix model differentiates between sequences from different distributions in an unsupervised manner and generates sequences using learned embeddings. It uses tools from neural density estimation to design new models for learning in TPPs, showing competitive performance with state-of-the-art methods and addressing shortcomings of existing approaches. The proposed models provide CDF and conditional intensity function, offering a new perspective on learning in TPPs. The LogNormMix model utilizes neural density estimation for unsupervised differentiation between sequences from different distributions in TPPs. It provides CDF and conditional intensity functions, enabling the computation of cumulative intensity for merged processes. The LogNormMix model uses neural density estimation for distinguishing sequences from different distributions in TPPs. It provides CDF and conditional intensity functions for computing cumulative intensity in merged processes. The PDF of the merged process is obtained by differentiating the CDF with respect to \u03c4, utilizing normalizing flows or mixture distributions without losing benefits of intensity parametrization. The constant intensity model is represented by an exponential distribution, with the conditional intensity function defined as \u03bb H being the history embedding from an RNN. The LogNormMix model utilizes neural density estimation to distinguish sequences from different distributions in TPPs. It models the integrated conditional intensity function using a feedforward neural network with non-negative weight matrices, providing flexibility in approximating any density compared to exponential and Gompertz distributions. The FullyNN model utilizes neural networks with non-negative weight matrices to approximate density functions in TPPs. It involves transforming variables using the change of variables formula and the inverse method to draw samples from the resulting distribution. Sampling from the FullyNN model requires numerical approximation due to the PDF not integrating to 1. The FullyNN model, used in TPPs, does not integrate to 1 due to saturation of tanh activations. It assigns probability mass to negative inter-event times, violating the assumption of strictly positive inter-event times. Various models, including SOSFlow and LogNormMix, are compared with baselines in terms of data preprocessing, parameter tuning, and model selection. Our implementation uses Pytorch to calculate inter-event times and generate parameters for the distribution p*(\u03c4). RNN processes log-transformed inter-event times and mark embeddings. Additional conditional information like metadata and sequence embeddings are used. Parameters are transformed to enforce constraints, and decoders rely on normalizing flows. The implementation relies on normalizing flows to generate parameters for the log-normal mixture distribution. Affine transformations are used to initialize the distribution parameters for faster convergence. The model samples from a Gaussian mixture in the forward direction and applies transformations before estimating parameters using the entire dataset. The FullyNN model implements a feed-forward neural network with non-negative weights to compute the density under a Gaussian mixture. It also implements RMTPP/Gompertz and exponential distribution models to define the distribution of inter-event times. The FullyNN model implements various distribution models for inter-event times, including linear scaling for initialization and batch normalization flow layers for faster convergence in DSFlow and SOSFlow models. SOSFlow model has no constraints on polynomial coefficients and uses Pyro 6 for implementation. When implementing SOSFlow, Pyro 6 was used as a reference. A log-normal mixture model allows for sampling with reparametrization, useful for tasks like imputing missing data. The reparametrization trick can provide a lower variance estimator compared to the score function estimator. Sampling from the mixture model is done using the Straight-Through Gumbel Estimator. The Straight-Through Gumbel Estimator is used for sampling in a log-normal mixture model, providing a lower variance estimator. Gradients flow through a differentiable sample during the backward pass. Alternative methods exist for unbiased gradients but are more computationally expensive. Synthetic data is generated using point processes with Poisson conditional intensity functions. The curr_chunk discusses different types of point processes, including Poisson, Renewal, Self-correcting, and Hawkes processes, each with specific characteristics and conditional intensity functions. Real-world datasets are also mentioned in the context of these processes. The curr_chunk discusses using marked temporal point processes to predict event types in real-world datasets. One dataset involves sequences of songs listened to by users, another dataset includes posts submitted to subreddits, and a third dataset involves users receiving rewards over time on a question-answering website. The dataset contains interactions of users with online systems, such as watching videos or solving quizzes. It also includes sequences of edits on Wikipedia pages and reviews for the most visited restaurants in Toronto. Training sequences are optimized using Adam with a learning rate of 10^-3 and split into mini-batches for training. We train models using Adam with a learning rate of 10^-3 and mini-batches of 64 sequences for up to 2000 epochs. Validation loss is computed at every epoch, and optimization stops if there is no improvement for 100 epochs. Hyperparameters are selected based on the lowest average loss on the validation set. Different values of L2 regularization strength and the number of transformation layers are considered for each model. The values of K are chosen to match the parameter count of specific models. In the experiment, models were trained with different hyperparameters and optimization settings. The choice of K was found to be robust for LogNormMix, DSFlow, and FullyNN models. For SOSFlow, using higher values of R led to unstable training. The NLL loss values reported can be shifted by rescaling the data, affecting the absolute loss values. The absolute values of the (negative) log-likelihood for different models are of little interest; what matters are the differences between them. Loss values are dependent on the train/val/test split. Aggregating scores and reporting averages may not show significant differences between models, but computing differences first reveals a clearer picture. This latter strategy is used in the analysis. In Table 4, results from the first strategy and constant intensity model are reported. Results for the constant intensity model are excluded from Figure 3 due to poor performance. Experiments were conducted on synthetic datasets, with LogNormMix and DSFlow achieving the best results. Conditional distribution p(\u03c4 |H) with models trained on Yelp dataset is plotted in Figure 9, showing inter-event times for check-ins at restaurants. The inter-event time prediction test NLL on real-world data is conducted using the same setup as in Section F.1, with differences in the architecture for learning in a marked temporal point process. The RNN takes a tuple (\u03c4 i , m i ) as input at each time step, where m i is the mark, and the loss function includes a term for predicting the next mark using a categorical distribution. In a feedforward neural network, the parameters are softmax V \u03c0. Differences in time NLL between models are reported in Figure 3. Table 6 shows total NLL averaged over multiple splits. Using marks as input improves time prediction quality. All models have similar mark prediction accuracy. In the Yelp dataset, the task is to predict the time until the next customer check-in based on the history of check-ins. The distribution p*(\u03c4 i) depends on the current time. The distribution of p*(\u03c4 i) until the next event depends on the embedding vector of the time stamp t i\u22121 of the most recent event. The dataset for the experiment is generated as a two step process: 1) We generate a sequence of 100 events from the model. The dataset is generated in two steps: first, a sequence of 100 events is generated, and then random events are removed within a specific time interval. Three strategies for learning with missing data are considered: no imputation, mean imputation, and sampling. The RNN encodes history up to time t i to define distribution p * (\u03c4 |h i ) for sampling. Multiple sequences are sampled to approximate expected log-likelihood of observed inter-event times. Reparametrization sampling is used to obtain gradients w.r.t. distribution parameters. Sequence embeddings are pre-trained before enabling history embedding for model training. The RNN encodes history up to time t i to define distribution for sampling. Samples are generated using different embeddings, including a self-correcting sequence and a renewal sequence."
}