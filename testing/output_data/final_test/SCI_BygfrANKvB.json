{
    "title": "BygfrANKvB",
    "content": "The proposed model aims to improve retrosynthetic reaction predictions by using a Transformer architecture with two novel pre-training methods. It incorporates a discrete latent variable model to generate diverse predictions. The model outperforms the baseline on the USPTO-50k dataset and offers a new approach for one-step retrosynthesis in material and drug research. The proposed approach for one-step retrosynthesis aims to predict reactants needed to generate a target molecule. Traditionally, template-based methods have been used, but they have limitations in coverage. Template-free approaches have been developed to better generalize to newer chemical spaces. Template-free approaches, such as Transformer models, are used for sequence-to-sequence prediction tasks in chemical synthesis. However, these models struggle to generalize to rare reactions and lack diversity in predicting reactants. Additional criteria like green chemistry are important in manufacturing processes. Green chemistry is important in chemical synthesis to minimize environmental impact. Predicted reactions often lack diversity and may not cover multiple reaction classes. Molecular Transformers are extended to address these challenges by providing alternative reactant predictions and improving generalization to rare reactions. We propose a novel pre-training approach for molecular Transformers to improve alternative reactant predictions. This approach involves chemically-relevant auxiliary tasks, such as randomly removing bond types or transforming the target based on templates. These tasks help in constructing valid chemical reactions and provide useful initializations for the model. The pre-training approach for molecular Transformers involves incorporating latent variables to improve the diversity of predicted reactions. The model learns to associate reactions with latent classes, covering multiple reaction classes in the training set. At test time, a diverse collection of reactions is obtained by conditioning on each latent class. Analogous mixture models have shown promise in generating diverse predictions in natural language translation tasks. In the chemical context, the model shows improved performance on the USPTO-50k dataset compared to template-free baselines using the Transformer model. Top-10 accuracy is the focus of evaluation, with a significant increase of over 13% for the best model. The model also performs well when data is split based on different reaction templates, showcasing diversity in outputs through quantitative and human evaluations. Template-based methods are traditionally used for retrosynthetic reaction prediction. Template-based models are traditionally used for retrosynthetic reaction prediction, utilizing templates or rules to denote atom and bond changes. Various approaches have been explored, including neural networks to learn associations between molecules and templates. In contrast, template-free generative models offer flexibility beyond extracted rules. Molecule generation can be approached through graph or SMILES representations, with different algorithms for each. Li et al. (2018) explored a node-by-node generation algorithm for graph representation, but validation of output validity remains a challenge. In contrast to template-based models for retrosynthetic reaction prediction, recent work focuses on generative models for molecule generation. Pre-training methods, inspired by NLP tasks, have shown significant improvements in Transformer models. Different approaches, such as node-by-node generation algorithms and SMILES string representations, have been explored to enhance the validity of output chemical graphs. Our work focuses on generative models for molecule generation, using pre-training tasks relevant to retrosynthesis prediction. The task involves predicting likely reactants from a target molecule represented as a SMILES string. Each SMILES string encodes a 2-D molecular graph, and the output predictions are multiple molecules concatenated by separators. Validity of the predicted SMILES is crucial for reconstructing a valid molecular graph. The model architecture for molecule generation involves using a Transformer model for the seq2seq task, with an encoder-decoder structure. The encoder maps input SMILES tokens to continuous representations, which are then decoded to generate output tokens. Multiple SMILES representations of the same molecule are used to ensure robustness in the model. The Transformer model architecture for molecule generation utilizes global self-attention layers to predict the latent graph structure of SMILES strings. The model can leverage long-range dependencies in SMILES due to its global connectivity. Two techniques are proposed to enhance the base molecular Transformer model for better generalization and diverse output generation. Two techniques are proposed to enhance the base molecular Transformer model for better generalization and diverse output generation. The first technique involves constructing chemically meaningful prediction examples for each input target molecule without requiring additional data or supervision. The second technique includes automatically generating pre-training targets by breaking specific bonds or adding functional groups to the molecule. Random pre-training involves generating new examples by breaking a random acyclic single bond in the input molecule, resulting in two output molecules. This method covers a diverse range of transformations, although not all chemical reactions are represented. Template-based pre-training uses templates extracted from training data to match specific patterns in the input molecule and transform them accordingly. Template-based pre-training involves matching specific patterns in the input molecule and transforming them according to template specifications. This method generates outputs with additional functional groups compared to random pre-training, resulting in more realistic reaction contexts. On average, there are over 200 different possible examples that can be extracted using the template-based pre-training method. The template-based pre-training method generates chemically valid reactions with additional functional groups compared to random pre-training. The random pre-training method can break bonds not represented in templates, potentially improving generalization. The model is pre-trained on auxiliary tasks before fine-tuning on retrosynthesis data to generate diverse predictions for one-to-many mappings. The model aims to produce diverse predictions for retrosynthesis reactions using a mixture seq2seq model, which introduces a latent variable to capture different reaction types. The prior and likelihood functions are learned to generate varied predictions effectively. The model uses a uniform prior for latent variable selection and shares the encoder-decoder network among mixture components. Training is done with the online hard-EM algorithm, selecting the value of z that minimizes loss for each example. Dropout is turned off during latent variable selection and turned back on for gradient regularization. The model uses a uniform prior for latent variable selection and shares the encoder-decoder network among mixture components. Training is done with the online hard-EM algorithm, selecting the value of z that minimizes loss for each example. Dropout is turned off during latent variable selection and turned back on for gradient regularization. The benchmark dataset used is a subset of the open source patent database of chemical reactions. Each example reaction is labeled with one of ten reaction classes, but this information is not used in the experiments. The mixture model can represent different reaction types in the training data and show improved diversity over the baseline. In experiments, reagent molecules are removed from examples for retrosynthesis prediction. Data is split based on reaction templates to prevent overlap between train and test sets. Template extraction code from Coley et al. (2017) is used. Retrosynthesis evaluation is challenging due to multiple valid syntheses for each input target. Retrosynthesis evaluation is challenging as each input target has multiple valid syntheses. The model output may not exactly match the single solution in the data, but can provide plausible alternatives. Top-10 accuracy is focused on for evaluation, with results from all experiments presented. Accuracies are computed by matching canonical SMILES strings of molecule sets. Diversity is measured through quantitative and human evaluations, using a model to predict reaction class based on input target molecule and predicted output. In 2017, a predictor was developed to compute reaction embeddings using weight-sharing. Trained on the USPTO-50k dataset, it achieved 99% accuracy on the test set. A baseline model, SMILES transformer (Base), was used for comparison. Ablation experiments were conducted for pre-training and mixture models to analyze their impact. Random and template-based pre-training methods were explored, along with constructing up to 10 new auxiliary examples for training. In addition to constructing new auxiliary examples and pre-training the model, data augmentation with variations of input SMILES strings is used. The accuracy results show improvement over the baseline, especially when combined with data augmentation. Pre-training tasks help the model learn the chemical reaction representational space effectively. Pre-training methods for chemical reaction prediction involve constructing additional decompositions of input targets, with marginal differences between template-based and SMILES-based pre-training. While template-based pre-training adds extra functional groups, the generated examples may not always be chemically valid. However, variations of input SMILES strings as pre-training output targets did not yield the same results. Our original motivation for using a mixture model was to improve diversity and performance. Increasing the number of latent classes generally leads to higher accuracies, with a slight decrease in top-1 accuracy but a significant increase in top-10 accuracy. Combining outputs from different latent classes is not perfect, affecting top-1 accuracy, but top-10 accuracies are more meaningful. In Section 5.2, top-10 accuracies are more significant for our problem. Our template-free methods can generalize to the test set, even on a challenging data split. Template-free models have advantages over template-based models in generalization. Evaluations of diversity for our model are also discussed. The diversity of predictions for the model is evaluated using the reaction class model. The mixture model shows a more diverse range of outputs compared to the base model, with 3.32 unique reaction classes. Each latent class in the mixture model predicts a different distribution of reaction classes, as shown in Figure 6. The study evaluates the diversity of predictions for a model by presenting 100 reactions to a human chemist for evaluation. The human chemist chose the mixture model as more diverse than the base model in generating outputs. This demonstrates that the model produces a wider range of predictions compared to the baseline. The study demonstrates that their model surpasses state-of-the-art methods in accuracy and diversity of predictions, even on challenging tasks. By focusing on rare reactions, they show significant improvement over baseline models in terms of generalizability. The new models show improved generalizability over baseline methods, with SMILES representing different graph traversals of molecules. Decoding SMILES requires awareness of long-range dependencies to ensure validity. In the molecular graph, the carbon atom and oxygen atom are neighbors, but in the SMILES string, they are far apart."
}