{
    "title": "Byht0GbRZ",
    "content": "The model introduced focuses on comparing two sentences by matching their latent structures using a structured attention mechanism. This approach allows for the discovery of the tree structure of each sentence and enables a fully differentiable comparison. The model is trained solely on the comparison objective and is evaluated on two sentence comparison tasks. The model focuses on comparing two sentences using a structured attention mechanism, which results in superior performance compared to word-level comparisons. It is evaluated on tasks such as natural language inference and paraphrase detection, highlighting the importance of considering the inherent structure of sentences during comparison. The structured alignment networks proposed in this paper aim to compare substructures in two sentences without the need for an external parser. By using a structured attention mechanism, the model learns a latent tree structure for each sentence and aligns spans between them, improving performance in tasks like natural language inference and paraphrase detection. Our method involves jointly learning a latent tree structure for each sentence and aligning spans between them using a fully differentiable CKY chart. This allows for comparisons between sentences leveraging their internal structure in an end-to-end, fully differentiable model. The model is evaluated on various sentence comparison datasets, showcasing its effectiveness. In experiments on sentence comparison datasets like SNLI and TREC-QA, comparing sentences at the span level outperforms word-level comparison. Learning sentence structure on the comparison objective results in well-formed trees that mimic syntax. Results motivate incorporating latent structure into models comparing two sentences. A common word-level comparison model, decomposable attention, is compared to a latent tree matching model. The latent tree matching model replaces word-level comparisons in the decomposable attention model. The decomposable attention model consists of three steps: attend, compare, and aggregate. It computes attention scores for each pair of tokens across two input sentences and normalizes them as a soft alignment. In the compare step, input vectors are concatenated with their corresponding attended vectors and fed into a neural network for comparison. The decomposable attention model in the latent tree matching model replaces word-level comparisons. It ignores the order and context of words in the sequence, focusing on span alignments in sentence pairs. The model considers all possible span comparisons, weighted by the spans' marginal likelihood. The model presented in this section operates on the latent tree structure of sentences, comparing spans determined by subtrees. It compares all possible spans in one sentence with all possible spans in the second sentence, weighted by likelihood. The model compares all possible spans in one sentence with all possible spans in the second sentence, weighted by likelihood, using a graphical model with latent variables. The graph-based CRF constituency parser uses unary and binary potentials to calculate the marginal probability of constituency nodes. The inside-outside algorithm is used to generate a binary tree, with most structures easily binarized. The score parameterization follows grammar production rules, with normalized categorical distributions for non-terminals. The unary and binary potentials in the graph-based CRF constituency parser calculate marginal probabilities for constituency nodes. The score parameterization involves multi-layer perceptrons operating on subtree representations, allowing for efficient computation of constituent-like spans in the inside-outside algorithm. The binary tree constraint ensures syntactic completeness of tree nodes. The binary tree constraint in the graph-based CRF constituency parser introduces artificial tags T 0 and T 1 to distinguish between comparable constituents and intermediate nodes. The inside pass algorithm calculates \u03b1 values for spans in the sentence, which are used in the outside algorithm to obtain normalized marginal probabilities for each span. When computing unary and binary potentials, Long Short-Term Memory Neural Networks (LSTMs) are used to represent spans in sentences. A bidirectional LSTM is run over the sentence to obtain output vectors, and span vectors are represented as concatenations of vector differences. Potentials are computed using multilayer perceptions with ReLU activation function. The structured alignment model uses latent constituency trees to compare sentences at the span level, instead of word-level comparisons. Marginal probabilities from the inside-outside algorithm are used for alignment, with LSTM span features representing potential spans. Attention scores are computed between spans across sentences for alignment. The structured alignment model compares sentences at the span level using latent constituency trees. Marginal probabilities from the inside-outside algorithm are used for alignment, with span vectors concatenated with attended vectors and fed into a neural network. Weighted summation with marginal probabilities is applied for vector aggregation, and the final output is obtained through a softmax function. The model is evaluated on two natural language matching tasks. Our structured alignment model compares sentences at the span level using latent constituency trees for question answering and natural language inference tasks. The experiments aim to test the effectiveness of the model in an end-to-end fashion and its ability to improve over standard token-level alignment models. The model is evaluated on answer sentence selection tasks using the TREC-QA dataset. The structured alignment model is evaluated on answer sentence selection tasks using the TREC-QA dataset BID40. Experimental results of state-of-the-art models and the structured alignment model are compared using MAP and MRR metrics. The baseline model includes token-level decomposable attention with a bidirectional LSTM for contextualized token representation. Answer sentence selection is treated as a binary classification problem, with rankings based on predicted positive possibilities. GloVe word embeddings and neural networks with ReLU activation functions are used in the model. In the experiment, feed-forward neural networks F and G are used with ReLU activation function and 300 hidden size. Dropout is applied to the output of BiLSTM and two-layer perceptrons with a ratio of 0.2. Parameters are updated with Adagrad BID10, and the learning rate is set to 0.05. Two variants of the structured alignment model are tested, one sharing parameters for computing structures and the other using separate parameters. Various models are evaluated based on MAP and MRR metrics. Results from the experiment show that structured alignment models outperform the decomposable attention model on both MAP and MRR metrics for matching questions to answers. Separated parameters in the model also lead to higher scores. In addition, the study includes natural language inference tasks using the Stanford NLI dataset, with specific numbers of pairs for training, development, and testing. The experiment results show that the structured alignment model outperforms the decomposable attention model on MAP and MRR metrics for matching questions to answers. The model uses shared parameters for computing latent representations, with a hidden size of 300 and a dropout ratio of 0.2. Various models were tested on the SNLI dataset, with the Structured Alignment model achieving a test accuracy of 86.6. The structured alignment model in the experiment shows improved accuracy over the baseline word-level comparison model by introducing a structural bias in sentence alignment. The model outperforms state-of-the-art models on the answer selection task, with a qualitative analysis of automatically learned tree structures presented in CKY charts. The model learned a model of sentence structure that corresponds well to known syntactic structures, identifying likely constituents and resolving attachment ambiguities correctly. It can recover tree structures closely mimicking syntax. Our model can recover tree structures closely mimicking syntax without syntactic supervision, unlike prior work. By comparing spans directly, the model induces trees with comparable constituents, providing a strong signal. The Stanford natural language inference dataset and the expanded multi-genre natural language inference dataset are popular sentence comparison tasks. The recent shared task on Multi-NLI provides a survey of sentence-level comparison models, including those using sentence structure. Some models squash structure into a single vector, losing the ability to compare substructures easily. Word-level comparison models like the decomposable attention model are prevalent in literature. Sentence comparison models that compare subtrees directly between two sentences have also been proposed. The first model to compare latent tree structures end-to-end on the comparison objective, using structured attention to induce tree structures efficiently. Our model is the first to include latent structured alignments between two sentences, moving beyond word and sentence-level comparison to comparing spans without the need for an external parser. Training on more complex objectives may aid in grammar induction, as shown in our experiments. Our model introduces latent structured alignments for comparing spans between sentences, outperforming word-level comparisons without additional supervision. It also uncovers syntax-like tree structures without syntactic guidance, suggesting potential for more complex models and grammar induction research. The model introduces latent structured alignments for comparing spans between sentences, outperforming word-level comparisons. The speed of the model remains a problem, being 15-20 times slower than the decomposable attention model due to the insideoutside algorithm."
}