{
    "title": "BklhoQ258B",
    "content": "Current practice in machine learning involves using deep nets with a high number of parameters, similar to compressed sensing or sparse regression with $l_1$ penalty terms. This approach helps in understanding phenomena in deep nets, such as their success in generalization with zero training error. Recent research suggests that data interpolation procedures can provide good generalization performance without overfitting. The distinction between \"classical\" and \"modern\" regimes in machine learning is marked by a peak in generalization error. The distinction between \"classical\" and \"modern\" regimes in machine learning is marked by a peak in generalization error, known as \"double descent\". This peak is not necessarily indicative of good generalization under overparametrization. We introduce the MiSpaR model and compute GE curves for $l_2$ and $l_1$ penalties. Overparametrization can increase sparsity but may lead to poor generalization. However, $l_1$ penalized regression can still perform well in data interpolation scenarios. Theoretical avenue for studying inverse problems in the interpolating regime using overparametrized fitting functions like deep nets. Modern machine learning characterized by large numbers of measurements and non-linear parametric models with many fitting parameters. Models with large parameters successful in dealing with real-life complexity, raising questions about generalization ability in the overparametrized regime. Classical statistical procedures balance training and generalization error by controlling model complexity. The overparametrization of deep nets can drive training error to zero while maintaining good generalization error. Theoretical studies have focused on the implications of overparametrization in deep learning models. Recent theoretical activity has focused on regression and classification algorithms that interpolate data and generalize optimally. An interesting phenomenon observed is the presence of a peak in generalization error with increasing model complexity, separating classical and modern regimes. Understanding the significance of these peaks is an open question, motivating further research. The text introduces the MiSpaR model for sparse regression, analyzing the GE and TE curves for l2 and l1 penalized regression in the high-dimensional regime. It shows that the overfitting peak occurs at the data interpolation point but does not indicate the onset of \"good generalization.\" The text discusses the generalization properties of penalized interpolation, showing that for small values of sparsity and noise, l1 regularization generalizes well while l2 penalization does not. This highlights the importance of inductive bias in determining generalization outcomes. The text discusses the implications of inductive bias on deep nets for solving inverse problems. It compares the under-parametrized and over-parametrized cases in MiSpaR, where the number of parameters in the inference model can vary freely. In the over-parametrized case, the design matrix is augmented with extra rows. Parameters in the generative model are assumed to be sparse, and the effect of sparsity-inducing regularization in the interpolation limit is considered. This study combines misparametrization with sparsity to analyze data interpolation and generalization error under different regularization procedures. In the context of sparse parameter inference, the choice of variance is crucial for normalization. Different choices have been used in literature, such as x ij \u21e0 N (0, 1/m). Undersampling and sparsity are key factors, with \u00b5 = p/m denoting overparametrization and \u21b5 = m/n denoting undersampling. The inference model (\"Student\") is affected by mis-parametrization, leading to under-specified or over-specified cases. Parameter inference involves minimizing a penalized mean. Parameter inference involves minimizing a penalized mean squared error with l2 and l1 penalties. Analytical expressions for the risk in the limit of n, p, and m tending to infinity are obtained. This approach is similar to limiting procedures used in statistical physics and modern statistics. Analytical formulae for training and generalization errors with l2 or ridge regularization are derived, along with a pair of simultaneous nonlinear equations for the l1 case. These equations can be solved numerically to obtain the generalization error without hidden parameters or integration. The analytical formulae for training and generalization errors with l2 or ridge regularization are derived using cavity mean field theory approach. The formulae agree for the l2 and l1 cases in the underparametrized regime, but diverge in the overparametrized regime. \"Good generalization\" begins when \u00b5\u21b5 > 1, not at the overfitting peak."
}