{
    "title": "Byg_vREtvB",
    "content": "In this paper, a general framework for distilling expectations from the Bayesian posterior distribution of a deep neural network is presented. The method extends prior work on \"Bayesian Dark Knowledge\" and applies to classification models. The distillation process involves compressing the posterior expectation using Monte Carlo samples from the teacher model. Experimental results explore various data sets, teacher model architectures, and student model optimization approaches. Deep learning models have shown promising results in various fields. Existing training methods may lead to poorly calibrated predictive uncertainties and confident errors. Bayesian inference can offer more robust posterior predictive distributions compared to point estimation-based training. Distilling into a student model with a matching architecture as the teacher can result in sub-optimal performance. Student architecture search methods can identify models with improved performance. Bayesian Dark Knowledge is a model training method that compresses the Bayesian posterior predictive distribution of a \"teacher\" network into a \"student\" network in the classification setting. This method aims to overcome the intractability of integrals required for Bayesian inference in neural network models. In this paper, a Bayesian posterior distillation framework is presented that generalizes the Bayesian Dark Knowledge approach by approximating posterior predictive distributions as Monte Carlo averages over a teacher ensemble. This approach reduces computational complexity at test time and is better suited for learning models in resource-constrained settings. The framework generalizes the target of distillation to general posterior expectations in the classification case. The study evaluates distillation of posterior predictive distribution and expected posterior entropy across various models and datasets, including those with increased uncertainty. It also explores the impact of student model architecture on distillation performance, highlighting the sub-optimality of matching teacher architecture and the benefits of student architecture optimization methods. Optimization methods can improve student models by generalizing distillation to capture a wider range of useful statistics of the posterior. This includes distilling the expected posterior entropy to distinguish between model uncertainty and fundamental uncertainty. This distinction is crucial for understanding why predictions are uncertain, and forms the basis for the Bayesian active learning by disagreement (BALD) score used in active learning. The paper discusses the Bayesian active learning by disagreement (BALD) score used in active learning to minimize model uncertainty. It presents background material on Bayesian inference for neural networks, approximate inference, and model compression. The proposed framework and Generalized Posterior Expectation Distillation (GPED) algorithm are detailed in Section 3, with experiments and results in Section 4. Additional information on data sets and experiments can be found in the appendices. When labeled data is limited, a full Bayesian treatment of the model can offer advantages over maximum likelihood estimation. Bayesian inference defines a posterior distribution over parameters using Bayes rule, with a prior distribution and prior parameters. The focus in machine learning prediction is on the posterior predictive distribution rather than the parameter posterior. Bayesian inference in neural network models requires approximations due to unavailable closed-form distributions. Variational inference (VI) and Markov Chain Monte Carlo (MCMC) methods are commonly used for approximations. VI involves defining an auxiliary distribution to approximate the true parameter posterior, with variational parameters selected to minimize the KL divergence. Various methods like stochastic VI and expectation propagation have been developed for neural networks. In the family of expectation propagation (EP) methods, Soudry et al. (2014) introduce an online EP algorithm for neural networks that can handle continuous and discrete weights. Hern\u00e1ndez-Lobato & Adams (2015) present the probabilistic backpropagation (PBP) algorithm for approximate Bayesian learning in neural networks. While VI, EP, and ADF algorithms often lead to biased posterior estimates for complex distributions, MCMC methods offer unbiased but computationally expensive alternatives for sampling-based posterior approximations. Bayesian Dark Knowledge aims to reduce test-time computational complexity in neural networks by using SGLD to approximate the posterior distribution. This method addresses the scalability issue of needing to compute over a large set of samples during predictions. Bayesian Dark Knowledge uses SGLD to approximate the posterior distribution for neural networks. It creates a teacher ensemble of neural network models with different parameter values, which is then used to train a single student model. This approach reduces test time computational complexity compared to using Monte Carlo averages. Additionally, there are generative models like Generative Adversarial Networks for approximating posterior sampling. In Henning et al. (2018), methods for generating samples similar to SGLD are proposed, aiming to speed up computations. However, these samples still require a Monte Carlo average for posterior predictive distribution in Bayesian neural networks. Bayesian Dark Knowledge addresses the costly computation by reducing test-time complexity. The focus is on enabling trade-offs between speed, storage, and accuracy, with background in network compression and pruning. Overparameterized deep learning models have shown better learnability in previous studies. Overparameterized deep learning models have better learnability and can be pruned substantially without losing generality. Various regularization techniques like Group LASSO and hierarchical priors are used for pruning neurons. These methods directly result in smaller networks, making computational savings easier to achieve. In this section, a framework is proposed for distilling Bayesian posterior expectations for neural network classification models, enabling test-time speed-storage-accuracy trade-offs. Various inferences are considered, with examples including posterior predictive distribution and entropy calculations. The method proposed aims to approximate posterior expectations under a teacher-student model architecture using labeled and unlabeled data, entropy, and variance for uncertainty quantification. It involves a student model, online expectation estimator, and loss function to measure approximation error. The proposed method involves an online distillation approach using the SGLD sampler to approximate posterior expectations in a teacher-student model architecture. Key components include a spherical Gaussian prior distribution, minibatch sampling, Langevin noise, and a distillation procedure for uncertainty quantification. The distillation learning procedure involves using a secondary unlabeled data set to update estimates of the posterior expectation in a teacher-student model architecture. This includes sampling a minibatch from the data set and updating the student model using gradient descent. The distillation learning procedure updates the student model using gradient descent in the teacher-student model architecture. The estimation of expectation targets involves an online update function to handle storage cost issues. The distillation learning procedure updates the student model using gradient descent in the teacher-student model architecture. An online update function is used to estimate expectation targets, with two different versions available. The online update function results in lower variance but requires maintaining expectation estimates, increasing storage costs. The fully stochastic update is memoryless and does not require past expectation estimates. The distillation learning procedure updates the student model using gradient descent in the teacher-student model architecture. The update is memoryless, saving space by not retaining past expectation estimates. The algorithm description includes input parameters and functions for the method. The distillation learning procedure updates the student model using gradient descent in the teacher-student model architecture. The framework involves choices related to the loss function and model architecture, with the addition of noise to instances from the distillation data set on each iteration. The algorithm scales efficiently with the data set size, allowing for trade-offs in reducing variance in estimates at the cost of additional storage. The distillation learning procedure updates the student model using gradient descent in the teacher-student model architecture. It involves choices related to the loss function and model architecture, with the addition of noise to instances from the distillation data set on each iteration. The framework provides computational and storage savings at test time but may result in a student network with insufficient capacity to represent a specific posterior expectation accurately. In exploring the problem of selecting a student model architecture, a general approach is needed due to constraints in low-resource compute environments. One approach involves a search over student models based on the teacher model, adjusting layer widths and kernel numbers. This method aims to balance test time inference speed and storage with accuracy. The regularization function R(\u03c6) in the GPED framework is used to prune a large initial network by grouping parameters and causing unnecessary parameters to go to zero simultaneously. This approach offers better computational complexity compared to adjusting layer widths and kernel numbers in student models based on the teacher model. In the GPED framework, weights in a convolution layer can be grouped together and pruned if below a threshold, leading to a more compact architecture. Various weight compressing, pruning, and architecture search methods can be combined with GPED. Experiments evaluating the approach using different data sets, teacher and student model architectures, and basic search methods are presented. In experiments, MNIST and CIFAR10 data sets are used with modifications to explore uncertainty impact on distillation performance. Modifications include subsampling and introducing occlusions with masking rates from 0% to 86.2%. Three teacher models are evaluated: a three-layer FCNN for MNIST, a four-layer convolutional network for MNIST, and a five-layer convolutional network. In experiments, teacher models with different architectures are evaluated for MNIST and CIFAR10 datasets. The student models are searched over a space of layer width multipliers based on the teacher models. Distillation procedures involve distilling posterior predictive distribution and entropy using specific estimators. Burn-in iterations and total training iterations are specified, with varying hyper-parameters and learning rate schedules for different datasets. For this experiment, the MNIST and CIFAR10 datasets are used without subsampling or masking. Distillation involves distilling posterior predictive distribution and entropy, with results showing low error in entropy. The FCNN NLL results on MNIST closely replicate previous findings. In Experiment 2, the study explores increasing uncertainty on models applied to MNIST and CIFAR10 datasets. Different sub-sampling and masking rates are considered to improve student model performance. In Experiment 2, the study explores increasing uncertainty on models applied to MNIST and CIFAR10 datasets by considering different sub-sampling rates. The posterior predictive distribution and posterior entropy distillation targets are evaluated using negative log likelihood (NLL) and mean absolute error metrics on held-out test data. The experiment focuses on matching student and teacher architectures, showing results for convolutional models on MNIST and CIFAR10. Additionally, a performance comparison between Uo and Us estimators is provided in Appendix B. The NLL of the teacher decreases as the data set size decreases, affecting both CIFAR10 and MNIST similarly. The NLL difference between student and teacher decreases with increasing training data, but the teacher NLL increases more rapidly with higher masking rates for MNIST. The gap between teacher and student peaks for moderate masking rates due to varying posterior uncertainty levels. The quality of student model approximations varies based on data set properties. Performance gaps can arise when restricting student architecture to match the teacher. Experiment focuses on searching for improved student model architectures. Comparing distillation performance using Uo estimator is generally better than using Us estimator. In an experiment comparing different approaches to finding improved student model architectures, the group lasso regularizer combined with pruning is compared to exhaustive search. Results are presented in terms of performance versus computation time and storage cost, using accuracy, negative log likelihood, and mean absolute error as performance measures. Results for the negative log likelihood of a convolutional model on MNIST are shown in Figure 2. The convolutional model on MNIST with a masking rate of 29% and 60,000 training samples is analyzed for posterior predictive distribution distillation. NLL vs FLOPS and NLL vs storage are plotted, showing significant improvements over the baseline student model. The group 1/2 method achieves better NLL at the same computation and storage cost compared to exhaustive search on MNIST. The group 1/2 method achieves better performance on MNIST with less computational cost compared to the baseline model. Results for other models and distillation targets show similar trends. The capacity of the student model significantly impacts the distillation procedure's performance, requiring optimization for a speed-storage-accuracy trade-off. The framework for distilling expectations from a deep neural network's Bayesian posterior distribution is presented, showing sensitivity to the student model's architecture. The study explores how the architecture of the student model affects posterior distillation, with basic architecture search methods helping to find improved trade-offs. Future work includes distilling a broader class of posterior statistics, developing advanced student model search methods, and applying the framework to larger models. The original investigation focused on the MNIST dataset, but it is considered a poor benchmark due to low posterior uncertainty. Two modifications to the standard MNIST dataset are investigated in this section. In this section, the study investigates modifications to the standard MNIST dataset to increase uncertainty by reducing the training set size and masking regions of input images. The goal is to create benchmark problems with varying posterior predictive uncertainty. The CIFAR10 dataset is also used in the experiments with the same subsampling technique. The full MNIST dataset consists of 60,000 training images and 10,000 test images distributed among 10 classes. Different sizes of labeled training data are considered for posterior sampling, while all 60,000 unlabeled training cases are used in the distillation data set. This allows for the decoupling of the impact of reduced labeled training data on posterior predictive distributions. The study investigates modifications to the MNIST dataset by reducing the training set size and masking regions of input images to increase uncertainty. Different sizes of labeled training data are considered for posterior sampling, while all unlabeled training cases are used in the distillation dataset. The CIFAR10 dataset is also used with the same subsampling technique. The masking rate for generating occluded images ranges from 0% to 86.2%. The full CIFAR10 dataset consists of 50,000 training images and 10,000 test images. In the study, sub-sampling is limited to training the teacher model with all 50,000 unlabeled training images in the distillation dataset. Experiments are conducted with fully-connected and convolutional neural networks to assess the impact of uncertainty levels and student model architecture. Teacher models include a 3-layer fully connected neural network and a CNN with 2D convolution and maxpooling layers. The architecture used includes input, convolutional layers, max-pooling layers, and fully-connected layers. The output size varies depending on the task, with ReLU non-linearities and softmax used for classification, and exponential activation for entropy. Student models in the experiments are based on this architecture. The distillation procedure involves using a set of width multipliers to search for student models based on the teacher architecture. Hyperparameters include fixed teacher learning rates, teacher prior precision, student learning rate, dropout rate, burn-in iterations, thinning intervals, and total training iterations. The Adam optimizer is used for training the student model. For training the student model, the Adam algorithm is used with a learning schedule that halves the learning rate every 200 epochs for MNIST models and every 400 epochs for CIFAR10 models. Regularization function R(\u03c6 s) is only applied during Group 1/2 pruning, with dropout used otherwise. Hyperparameters for Group 1/2 pruning experiments involve regularization strength values \u03bb chosen from a log-scale ranging from 10^-8 to 10^-3. Fine-tuning epochs for MNIST and CIFAR100 models are 600 and 800 respectively, with the student learning rate reinitialized at the start of fine-tuning. The student learning rate is reinitialized for fully-connected and convolutional models. Results of Experiment 2 on fully-connected networks for MNIST are shown. Accuracy-Storage-Computation tradeoff using CNNs on CIFAR10 with subsampling training data is demonstrated. The optimal student model is obtained with group 1/2 pruning. The optimal student model for this configuration is achieved with group 1/2 pruning, having 5.4\u00d7 the parameters and 5.6\u00d7 the FLOPS of the base student model."
}