{
    "title": "H1e0-30qKm",
    "content": "In this paper, two novel disentangling methods are introduced for interpreting and generating generalizable representations of data. The first method, Unlabeled Disentangling GAN (UD-GAN, unsupervised), decomposes latent noise by generating similar/dissimilar image pairs and learning a distance metric with siamese networks and contrastive loss. The second method (UD-GAN-G, weakly supervised) modifies UD-GAN with user-defined guidance functions to focus on desired semantic variations in the data. Both methods outperform existing unsupervised approaches in measuring semantic accuracy of learned representations. Generative Adversarial Networks (GANs) are used to generate realistic images without the need for likelihood-based modeling. Conditional GANs can provide control over the generated data by conditioning the generator with supervised labels. In this paper, two GAN-based methods are proposed to learn disentangled representations without using labeled data. The first method, Unlabeled Disentangling GAN (UD-GAN), generates image pairs and learns a distance metric on a disentangled representation space. The second method, UD-GAN-G, uses guidance functions to capture desired semantic variations in siamese networks. In BID models, disentangled representations are achieved through various methods such as reconstructing objects from different viewpoints, separating style and category via autoencoders, combining autoencoders and GANs to decompose identity and attributes, and clamping hidden units for images with the same identity but different pose or expression. In BID models, disentangled representations are achieved through various methods such as reconstructing objects from different viewpoints, separating style and category via autoencoders, combining autoencoders and GANs to decompose identity and attributes, and clamping hidden units for images with the same identity but different pose or expression. Techniques like InfoGAN, \u03b2-VAE, and DIP-VAE use unsupervised methods to discover categorical and continuous factors in image generation. Our method builds on existing approaches with two important modifications: (i) operating on pairs of similar/dissimilar image pairs and (ii) computing image embeddings using separate networks to guide the disentangling process with information restriction. In GANs, the generator maps a latent variable to a more complex distribution, while the discriminator distinguishes real images from generated ones. The training of GAN involves distinguishing real images from generated ones using a minimax game. The generator loss is modified for stability by maximizing log(D(G(z))). The latent variable is sliced into multiple vectors to control different semantic variations. The network architecture is illustrated in Figure 1. In training GAN, the generator loss is stabilized by maximizing log(D(G(z))). The latent variable is divided into vectors to control semantic variations. Image pairs are generated with specific attributes using Siamese Networks and Contrastive Loss for learning a distance metric on embedding vectors. Optional guidance function restricts information for desired representation space. The unsupervised UD-GAN approach uses a Contrastive Loss function to pull similar image pairs together and push dissimilar pairs apart, with an adaptive margin for improved training stability. The discriminator network remains unchanged and is trained to separate real and generated image distributions. Another approach uses latent variable slicing to capture illumination and pose variations while maintaining a fixed identity. Our method utilizes latent variable slicing to capture illumination and pose variations of a face with a fixed identity without the need for labeled real images. By creating similar and dissimilar image pairs through concatenating latent variables, we generate image batches. The final loss function includes a GAN loss, an embedding loss, and a guidance function to restrict input variations in the embedding space. The method utilizes latent variable slicing to capture variations in hair color by cropping images. Different guidance functions lead to capturing various variations in the CelebA dataset. A probabilistic interpretation is gained through a toy example of generating images of colored polygons with independent factors of shape and color. The method uses latent variable slicing to capture variations in hair color by cropping images. It involves generating colored polygons with independent factors of shape and color, represented by mixture distributions for each attribute. The method involves generating colored polygons with independent factors of shape and color, represented by mixture distributions for each attribute. The contrastive loss aims to minimize the spread of each mixture component to achieve disentanglement in the representation. The second term separates all components using an adaptive margin. The method involves generating colored polygons with independent factors of shape and color, represented by mixture distributions for each attribute. The contrastive loss aims to minimize the spread of each mixture component to achieve disentanglement in the representation. The second term separates all components using an adaptive margin \u03b3 (1,2) i based on the difference between input latent pairs. This ensures a smooth embedding space where shapes like rectangles, circles, and ovals are separated, but circles are closer to ovals than squares due to their relative similarity. The method involves using the Contrastive Loss to minimize the spread of mixture components for shape and color disentanglement. An adaptive margin \u03b3 (1,2) i separates input latent pairs, maximizing the lower bound for energy distance D E. Experiments were conducted on a server with specific hardware, and training utilized ADAM BID9 optimizer with set parameters. The method involves using the Contrastive Loss to minimize the spread of mixture components for shape and color disentanglement. Experiments were conducted on two image datasets: CelebA and 2D Shapes. Different weight values for the contrastive loss were used for each dataset. Two versions of the algorithm, UD-GAN and UD-GAN-G, were compared in terms of results obtained with and without guidance at the input of siamese networks. Our method compares against \u03b2-VAE, DIP-VAE, and InfoGAN for both autoencoder and GAN-based approaches. The CelebA dataset uses unguided and guided siamese networks, while the 2D shapes dataset involves guiding networks based on estimated center of mass. Our method utilizes siamese networks to estimate the center of mass and size of generated objects, which are then fed to our networks for evaluation. The disentanglement metric scores show that our methods outperform baseline approaches on the CelebA dataset. Our guided network (UD-GAN-G) outperforms the unguided approach, especially on the CelebA dataset, by relating similarities of latent variable pairs to image pairs. This guidance prevents cluttering with irrelevant attributes, leading to better disentanglement and improved scores. The disentanglement scores for the 2D shapes dataset are high due to simple synthetic images used. The guided method outperforms the unguided approach on the CelebA dataset by isolating attributes like hair and mouth for better classification performance. This is achieved through top and bottom crops that detach the effects of other variations. The DIP-VAE method generates blurrier images compared to adversarial methods like InfoGAN and UD-GAN-G due to the data likelihood term in VAE-based approaches. This term is usually implemented as a pixel-wise image reconstruction loss, while in GAN-based approaches, it is handled via a learnable discriminator in an adversarial setting. In UD-GAN-G, the guided approach allows for better control over captured attributes like smile, azimuth, and hair color. The guidance provided in FIG1 enables direct manipulation of these attributes using knobs q bot, q mil, and q top. This approach also shows promising results for the 2D Shapes dataset. The guided approach in UD-GAN-G allows for better control over captured attributes like smile, azimuth, and hair color using knobs q bot, q mil, and q top. This weak supervision helps capture some variation in the data without training the model multiple times with different hyperparameters or initializations. In the aligned CelebA dataset, faces are centered around the nose, simplifying guidance design. A pre-trained object detection method like YOLO can be used for complex scenarios. Backpropagating gradients into an image may cause adversarial samples, but a discriminator can help. The guidance function used must be differentiable for backpropagation from siamese networks to the generator. In this paper, UD-GAN and UD-GAN-G are introduced as novel GAN formulations using Siamese networks with contrastive losses to disentangle latent noise space slices. The method employs differentiable relaxations for network guidance and measures image similarity using L2-distance between image embeddings. Challenges with modeling latent dimensions as categorical variables were encountered, leading to training stability issues that the authors plan to address in future work. Our experiments with guided and unguided approaches for embedding networks showed how our method can be used for semantically meaningful image manipulation. Results indicate that our method can adjust well to data variations and outperform current state-of-the-art methods on CelebA and 2D Shapes datasets. Future work will explore more powerful embedders, such as extracting information from pre-trained networks for advanced image manipulation techniques. TAB2 displays the neural network layers used in our generator for different datasets. The Siamese Networks aim to map images into embedding spaces for grouping in a semantic context. Disentangling shape and color may require small assumptions and domain knowledge, without labeled data. An example extends the MNIST dataset with random colors to capture shape and color independently. The generator, trained with networks \u03c61 and \u03c62, captures variations in data with knobs q1 and q2. By modifying the network architecture, shape and color attributes can be separated. The modified \u03c62 focuses only on color, allowing the first network to capture shape and the second to capture color variations. The generator, trained with networks \u03c61 and \u03c62, captures variations in data with knobs q1 and q2. The second knob focuses on color variations, as shown in experiments on the MS-Celeb dataset. The guided siamese networks demonstrate how the first knob controls the overall outline and identity of the generated face, while the unguided second knob modifies the image with minimal changes to image edges, corresponding to lighting adjustments. The second knob in TAB3 adjusts the average color of the generated image. In the third experiment with cropped guidance networks, the knobs modify specific parts of the face. The siamese networks for the 2D shapes dataset use the center of mass and object size for guidance. The generated object's size is determined by various factors such as pixel intensity, center of mass, and size estimate. The 2D shapes dataset's guidance is closely related to ground truth attributes. Additional semantic properties captured by UD-GAN-G are illustrated in FIG5. Comparison of classification performance between our method and InfoGAN on CelebA dataset attributes is shown in TAB5. Correlations between different embedding dimensions and CelebA attributes are compared in TAB6. DIP-VAE encodes a less correlated representation, but this doesn't necessarily lead to disentanglement. The correlation between CelebA attributes does not always result in a disentangled semantic representation, as seen in the quantitative results in TAB0 and 2."
}