{
    "title": "B1G6uM0WG",
    "content": "The paper discusses autonomous lane changing for self-driving vehicles in a multi-lane, multi-agent setting. A framework using deep reinforcement learning is presented to create a high-level policy for decision making while integrating with low-level controllers. Q-masking is utilized to incorporate prior knowledge and constraints, simplifying the learning process and improving efficiency. Preliminary results in a simulator demonstrate the effectiveness of the approach. In recent years, there has been a growing interest in self-driving vehicles for their potential to improve road safety and efficiency. One crucial skill for autonomous vehicles is performing lane change maneuvers on multi-lane highways, which requires efficient decision-making to avoid congestion and accidents. Prior work on lane changing has explored various approaches, but the complexity lies in reasoning about interactions with other agents while maintaining safety. Prior work on lane changing has explored different approaches, focusing on local perspectives and interactions with neighboring vehicles. However, there is a lack of consideration for long-term strategic decisions, such as reaching an exit on a multi-lane highway. Formulating a control or optimization problem for this scenario is challenging and may not be feasible due to the complexity involved. Reinforcement learning offers a way to learn policies with specific goals, such as reaching the exit safely and efficiently. Deep reinforcement learning has been successful in learning policies from raw sensor information. This study explores the use of deep reinforcement learning in solving the autonomous lane changing problem. Our framework utilizes deep Q-learning to develop a high-level tactical decision-making policy, incorporating Q-masking to focus on a subspace of Q-values guided by a low-level module. This integration balances learning high-level policies with established optimization methods for low-level control in solving the autonomous lane changing problem. The framework integrates deep Q-learning for high-level policy and low-level control, simplifying the reward function for faster learning. It eliminates collisions during training/testing, enabling direct training on real systems. The framework outperforms a greedy baseline in efficiency and safety, generalizing to unseen scenarios without extra training. The autonomous lane changing problem is formally defined in a multi-lane-multi-agent setting using the SUMO BID10 traffic simulator. The simulation environment includes a highway with speed limits that cars must follow. Traffic density is generated using SUMO, with cars controlled to avoid collisions. The ego car is set up to reach the exit while being governed by the approach. The ego car aims to reach the exit efficiently by making lane changes while avoiding collisions and respecting speed limits. Q-masking is used to facilitate collision-free lane changes between adjacent lanes. The performance is evaluated based on success rate and average speed compared to a greedy baseline and human drivers in a simulator. The goal is to develop a policy for the ego car to plan long-term lane change maneuvers to reach the exit in minimal time while ensuring safe adjacent lane changes. In this work, deep reinforcement learning is used to achieve collision-free lane changes for the ego car. High-level decisions are broken down into 5 actions: N (no-op), A (accelerate), D (decelerate), L (make a left lane change). A separate low-level module is integrated for low-level actions. The ego car's state includes internal and external information represented by scalar inputs like velocity, lane, and distance to goal scaled between 0 and 1. This scaling improves performance and robustness to environmental changes. Zero-shot transfer allows the approach to be applied to new problems. The ego car uses a binary occupancy grid for external information, with a visibility region of 50m in front and back. It also considers relative motion of traffic by including a history of occupancy grids from previous time steps. The ego car learns from its actions through rewards and estimates the optimal Q-value function. The ego car uses a binary occupancy grid for external information, with a visibility region of 50m in front and back, considering relative motion of traffic. It learns from actions through rewards and estimates the optimal Q-value function. State transitions are executed randomly, with neural network weights adjusted based on minimizing a loss function during training. The target value in Q-learning is calculated by maximizing Q-values for the next state-action pair. The ego car uses a binary occupancy grid for external information, with a visibility region of 50m in front and back, considering relative motion of traffic. It learns from actions through rewards and estimates the optimal Q-value function. State transitions are executed randomly, with neural network weights adjusted based on minimizing a loss function during training. The target value in Q-learning is calculated by maximizing Q-values for the next state-action pair. Q-values for the next state action pair, y t = r t + \u03b3 max at+1 Q(s t+1 , a t+1 ; \u03b8), but since we simulate the full trajectory we can back calculate the target exactly from the terminal reward r T, where \u03b3 is the discount factor. We maintain two experience buffers: good G and bad B, saving transitions from successful and failed trajectories respectively. The network is optimized using a mini batch of transitions sampled from both buffers. This process helps avoid the network getting stuck in local minima. The network architecture used includes a sparse reward function to guide the ego car towards the exit efficiently. It utilizes a convolution layer, fully connected layers, and Q-values for tactical actions. The discount factor encourages faster exit reaching. In this work, Q-masking is introduced to incorporate prior information for faster and more efficient learning in deep Q-learning. This technique simplifies the reward function and allows for the generation of trajectories to achieve specific goals while avoiding collisions. The focus is on learning a policy for tactical decision-making without the need to maintain speed limits or avoid collisions. In this work, a technique called Q-masking is proposed for deep Q-learning to incorporate prior information for faster and more efficient learning in making tactical decisions. Q-masking involves applying a mask on the output Q-values before selecting the best action, allowing for the restriction of actions that the agent does not need to explore or learn from in a given state. Q-masking in deep Q-learning involves masking certain actions based on prior knowledge, such as not selecting the left action to avoid getting off the highway. This approach simplifies the reward function, speeds up learning, and focuses on necessary Q-values. Constraints can also be incorporated, like masking acceleration at maximum speed. This method optimizes learning by avoiding unnecessary exploration of certain states. Incorporating Q-masking in deep Q-learning simplifies the reward function, speeds up learning, and focuses on necessary Q-values. This approach allows for exclusive learning of high-level decision making policies while leveraging deep learning for long term decision making in a multi-lane multi-agent setting. The rule-based method in the low-level module masks off high-level actions to focus on learning high-level strategy. This approach simplifies the reward function, speeds up learning, and allows for exclusive learning of high-level decision making policies. It can also be used for imitation learning and has potential relations to reward shaping and options. In the implementation, highway shoulders information and speed limits are incorporated in the lower-level module along with a rule-based time to collision method. The framework's performance is evaluated by comparing it against a greedy baseline policy and human drivers in a simulator under specific problem parameters. These parameters include traffic density settings for different lanes to simulate varying traffic conditions for non-trivial lane changes. The study evaluates the performance of a network trained for highway driving in dense traffic conditions, focusing on lane change maneuvers. The network is trained for 10k episodes with specific parameters and explores the effects of visibility on the ego car. The study evaluates a network trained for highway driving in dense traffic conditions, focusing on lane change maneuvers. The network uses specific parameters and explores the effects of visibility on the ego car, with a baseline tactical policy prioritizing right lane changes and maintaining speed limits. Data from 10 human subjects driving the ego car in a simulator was aggregated for analysis. The study evaluated a network trained for highway driving in dense traffic conditions, focusing on lane change maneuvers. A GUI indicated the ego car's location, speed, and available actions. Subjects were asked to drive naturally with the objective to reach the exit as fast as possible. Initially, subjects felt uncomfortable with the low-level module on, specifically the TTC component for collision avoidance. Turning off the TTC component resulted in subjects feeling more in control. The study evaluated a network trained for highway driving in dense traffic conditions, focusing on lane change maneuvers. Subjects felt more in control after turning off the low-level module, resulting in more collisions. The benchmark results showed that the baseline policy was successful in reaching the exit but inefficient. The humans inclined to drive faster were less successful due to collisions. The network's approach achieved higher average speed, success rate, and no collisions. Improved success rate was seen with increased visibility. The network learned to make human-like lane change decisions, resulting in emergent behaviors like merging and overtaking. Zero-shot transfer was achieved by designing inputs in a scaled manner, allowing the network to perform well on unseen scenarios without prior training. Tests were conducted with variations in starting positions, exit locations, and lane numbers on the highway. Aggregate results of 100 trials showed improved performance with increased visibility. Our approach shows promising results in generalizing well to new scenarios with a slight decrease in success rate and average speed. The lower speed in scenario 1 is due to less highway left to reach the exit, while the higher speed in scenario 3 indicates decreased problem difficulty. These preliminary findings demonstrate the potential of deep reinforcement learning in addressing tactical decision-making problems, suggesting further exploration with different network architectures and real-world applications. Future work can involve making the lane changing problem more realistic by considering occlusions and introducing uncertainty with a probabilistic occupancy grid. A framework leveraging deep reinforcement learning for high-level decision making and traditional methods for low-level control, using Q-masking to incorporate prior knowledge and constraints, simplifying the reward function, and eliminating collisions during training or testing. The framework applied to autonomous lane changing for self-driving cars uses a neural network for high-level decision making. Results show it outperforms a greedy baseline and human drivers in a simulator, with a more efficient and safer policy. Zero shot generalizations were demonstrated on unseen scenarios."
}