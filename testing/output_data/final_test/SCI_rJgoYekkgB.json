{
    "title": "rJgoYekkgB",
    "content": "We investigate how individual attention heads in pretrained transformer language models capture syntactic dependency relations. By analyzing attention weights, we find that some heads can recover dependency types better than baselines on parsed English text. Fine-tuning BERT on different datasets does not substantially affect the patterns of self-attention. Pretrained Transformer models like BERT have shown excellent performance on language tasks, but analyzing attention weights may not reveal much syntactic knowledge. Researchers have tried to analyze the linguistic knowledge learned by BERT through various methods. The BERT model computes hidden representations for each token by attending to all tokens in an input sentence. Researchers aim to investigate if syntax is encoded by BERT's self-attention heads by extracting dependency relations from all attention heads. BERT's attention heads encode most dependency relation types with higher accuracy compared to baselines, regardless of fine-tuning on different datasets. BERT models show non-trivial accuracy for certain dependency types but do not significantly outperform right-branching trees in terms of UUAS. The attention heads of BERT reflect only a small number of dependency relation types, not capturing the full extent of syntactic knowledge. Previous work has focused on extracting syntactic trees from Transformer-based NMT models' attention heads. BID6 aggregate attention weights to form a single matrix for extracting constituency and dependency trees. In contrast, BID10 trains Transformer-based machine translation models on different language pairs and extracts dependency trees using the maximum spanning tree algorithm on the attention weights of the encoder. Concurrently, BID14 focuses on identifying confident attention heads of the Transformer encoder that serve specific functions related to relative positions, syntactic relations, and rare words. Previous work has shown that BERT learns significant syntax knowledge, with BID12 introducing a probing-style method to evaluate BERT's syntactic knowledge. BERT encodes syntax more than semantics, as shown by a structural probing model. The model maps hidden representations to an innerproduct space corresponding to syntax tree distance. Strong models like BERT and ELMo are better at reconstructing dependency trees compared to baselines. BERT is a Transformer-based masked language model pretrained on BooksCorpus and English Wikipedia, achieving stellar performance on various NLP tasks. The experiments were conducted on pretrained cased and uncased versions of the BERT-large model, consisting of 24 self-attention layers with 16 heads each. Performance was reported to be achieved by capturing attention weights for each head and layer. The study investigates the impact of fine-tuning syntax-related (CoLA) and semantic-related (MNLI) tasks on attention weights and dependency relations in BERT models. Experiments include CoLA-BERT and MNLI-BERT models, compared to randomly initialized BERT. Previous work suggests randomly initialized models perform well on NLP tasks. The study explores how fine-tuning syntax and semantic tasks affect attention weights and dependency relations in BERT models. Two methods are used to extract relations from attention weights in BERT, focusing on inter-word attention. Non-matching tokens are merged for compatibility, and attention weights are summed for corresponding columns and rows before applying extraction methods. The study focuses on extracting relations from attention weights in BERT models by merging non-matching tokens and summing corresponding columns and rows. Relations are extracted using a simple method based on the attention matrix, which may not form a valid tree structure. To extract valid dependency trees from attention weights in BERT models, a complete weighted directed graph is treated as a matrix of attention weight tokens. The Chu-Liu-Edmonds algorithm is then applied to compute the maximum spanning tree, resulting in a valid undirected dependency tree. Additionally, relative position baselines are computed to determine the most common positional offset between parent and child words for a given dependency relation. The accuracy of dependency relations extracted using different methods in BERT models is compared, showing that pre-trained and fine-tuned BERT models outperform random BERT and relative position baselines for most relation types. The self-attention weights in trained BERT models play a significant role in achieving these results. The self-attention weights in trained BERT models implicitly encode certain dependency relations, with semantic-oriented fine-tuning tasks encouraging effective long-distance dependencies. MNLI-BERT outperforms BERT and CoLA-BERT for clausal dependencies like advcl and csubj. MNLI-BERT also excels in maximum undirected unlabeled attachment scores for specific dependency types. MNLI-BERT outperforms BERT and CoLA-BERT for certain dependency types, achieving higher UUAS scores. The performance gap between BERT models and right-branching baselines is not substantial, casting doubt on whether BERT attention heads track syntactic dependencies. Fine-tuning on CoLA and MNLI does not significantly impact UUAS scores. The study investigates if BERT attention heads exhibit implicit syntax dependencies by analyzing dependency relations at all layers. The study examines the dependency relations from the attention heads of BERT at all layers. Two simple methods are used to extract dependency relations, revealing that some attention heads track over 75% of dependency types accurately. However, linguistically uninformed baselines outperform BERT on about 25% of dependency types, suggesting that BERT's learned dependency syntax may be trivial. Fine-tuning on CoLA and MNLI does not affect self-attention patterns, but fine-tuned models show different performance on the GLUE benchmark compared to BERT."
}