{
    "title": "B1lG42C9Km",
    "content": "In this study, a new intrinsic social motivation for multi-agent reinforcement learning (MARL) is proposed, where agents are rewarded for influencing another agent's actions using counterfactual reasoning. This approach leads to enhanced cooperation, emergent communication protocols, and more effective communication among agents, ultimately resulting in higher collective rewards. In this study, a new intrinsic social motivation for multi-agent reinforcement learning (MARL) is proposed, where agents are rewarded for influencing another agent's actions using counterfactual reasoning. This approach leads to enhanced cooperation, emergent communication protocols, and more effective communication among agents, ultimately resulting in higher collective rewards. Influence can be computed by equipping each agent with an internal model that predicts the actions of other agents, allowing for a decentralized approach without the need for a central controller. This represents a more general and scalable bias for MARL with independent agents. Intrinsic social motivation for multi-agent reinforcement learning (MARL) involves rewarding agents for influencing others' actions through counterfactual reasoning. This approach enhances cooperation, communication protocols, and collective rewards without the need for a central controller. Social interaction is a key aspect of human learning, often overlooked in the study of intrinsic motivation. In MARL, agents simulate alternate actions to influence others, rewarding high influence to encourage cooperation. Inspired by human cognition, newborns coordinate behavior based on actions of others. The proposed social influence reward is studied in Sequential Social Dilemmas, challenging MA environments with game-theoretic rewards. In MARL, agents use a game-theoretic reward structure similar to Prisoner's Dilemma. The paradoxical payoff structure challenges achieving cooperative social dynamics. A Model of Other Agents (MOA) network is attached to each agent to predict and reward influence without a centralised controller. In MARL, agents aim to maximize their own reward without a central controller or sharing reward functions. A social reward function based on observing agents' behavior can lead to cooperative behavior in scenarios like autonomous driving. In MARL, agents aim to maximize individual rewards without a central controller. Each agent chooses actions to form a joint action that transitions the environment. Agents receive rewards based on their actions and seek to maximize future rewards using A3C for training. The policy is learned via REINFORCE with baseline BID33. Agents consist of convolutional and fully connected layers, an LSTM network BID7, and linear layers outputting \u03c0 k and V \u03c0 k (s). Social influence intrinsic motivation modifies an agent's reward function to R k = \u03b1E k + \u03b2I k, where E k is the environmental reward and I k is the causal influence reward. I k is computed by generating counterfactual actions to assess their impact on other agents' behavior. Counterfactuals estimate the probability of outcomes under different actions in a given situation. The text discusses the concept of counterfactuals in causal inference, where the marginal policy of B is compared to the conditional policy of B given A's action to measure the causal influence of A on B. This helps determine how much B's planned action distribution changes because of A's behavior. The causal influence reward for agent A is related to the mutual information between the actions of agents A and B. This reward is connected to empowerment in single agent RL, rewarding A for having high mutual information with B's actions. By sampling independent trajectories from the environment, A is rewarded for having empowerment over B's actions. The social influence reward is calculated based on the mutual information between agents' actions. The policy's ability to approximate this reward depends on the learning dynamics. The approach involves a sequential ordering of agents' actions, with influencers receiving rewards and influencees being influenced. The influencers choose actions first, which are then given to the influencees. This allows for better credit assignment and easier learning of high influence actions. Experimentation with different measures shows the influence reward is robust. Human children quickly learn to use communication to influence others in cooperative activities, indicating a capacity to form shared goals. The capacity to form shared goals with others allows humans to engage in cooperative activities. In addition to environmental reward, a social influence reward is used to train agents to communicate using a communication policy \u03c0c and a communication value function Vc. Counterfactuals are employed to assess the influence of an agent's communication message on another agent's action in the next timestep. Communication between agents plays a crucial role in influencing each other's actions. For communication to be effective, it must provide valuable information that helps the receiving agent maximize its reward. Selfish agents tend to ignore ungrounded communication, while valuable information can cause a change in intended actions. To compute the causal influence reward, the probability of the receiving agent's next action given a counterfactual scenario is essential. This information is obtained through a centralized controller accessing other agents' policies. To achieve independent training in Multi-Agent Reinforcement Learning (MARL), each agent is equipped with its own internal Model of Other Agents (MOA). The MOA is trained to predict other agents' next actions based on the current action and the agent's view of the state. This allows agents to compute social influence rewards by imagining counterfactual actions and using the MOA to predict their effects. The MOA in Multi-Agent Reinforcement Learning allows agents to predict the effect of their actions on others and give themselves rewards for influential actions. This approach improves upon previous models by enabling agents to consider counterfactual scenarios and communicate influence without the limitation of only some agents being influencers. In Multi-Agent Reinforcement Learning, agents can influence each other through communication and mutual influence. Nodes are conditioned on and interventions are made by replacing a specific node with counterfactuals. Social influence rewards are given when the influenced agent is within the influencer's field-of-view. This method aims to estimate influence in the next timestep by considering visible actions and accurate estimates. Sequential Social Dilemmas (SSDs) are spatially and temporally extended multi-agent games with a payoff structure similar to Prisoner's Dilemma. Traditional RL agents struggle to coordinate in these dilemmas due to the paradoxical reward structure. Two SSDs, a public goods game Cleanup, are experimented with in this work. In this work, two Sequential Social Dilemmas (SSDs) are explored: a public goods game Cleanup and a tragedy-of-the-commons game Harvest. Apples provide rewards in both games, and agents can punish each other. Various attempts have been made to develop intrinsic social motivation rewards, such as hand-crafted rewards specific to foraging environments and emotional intrinsic rewards based on agents' perception of their neighbors' cooperativeness. In complex settings with long-term strategies, SSDs are investigated. Hughes et al. (2018) introduced inequity aversion motivation, penalizing agents for rewards differing too much from the group. Prosocial reward shaping shows training a single agent to optimize for others' rewards can improve collective outcomes. Training agents to learn emergent communication protocols has been another focus, with selfish agents struggling to use communication effectively. In theory, communication is proportional to common interest, with prosocial or hand-crafted agents more likely to communicate effectively. Self-interested agents struggle to learn to communicate. Social influence rewards can encourage agents to communicate in complex environments. Curiosity-based intrinsic motivation can motivate agents to prefer vocalizing sounds imitated by others. Curiosity may be a sufficient motivation for agents or children to learn to communicate with others. Our MOA network is related to work on machine theory of mind, training agents to model the impact of their policy on other agents and incorporating it into their learning rule. Causal influence and counterfactual reasoning have been used to measure coordination between agents in a multi-agent setting. The curr_chunk discusses various approaches in multi-agent reinforcement learning, including using counterfactuals and mutual information for reward shaping and designing social rewards. It also mentions developing agents that maximize empowerment over states in pursuit of specific goals. The curr_chunk discusses maximizing transfer empowerment by maximizing mutual information between agent's actions and player's future state. It compares this approach to training agents with social influence reward in different settings and evaluates their performance against standard A3C agents. The curr_chunk discusses the results of training influence with a centralised controller and the gradual increase of social influence reward over steps. It also mentions an additional experiment in a simplified environment to showcase the effects of social influence on fostering cooperative behavior. Figures 6(a) and 6(d) show the outcomes of this training method. The influence model with visible actions and social influence reward leads to significantly higher collective reward in both games. Trajectory analysis of high scoring models in Cleanup and Harvest reveals interesting behavior, such as a single agent trained with social influence reward shown in a video. A single agent trained with social influence reward shows unique behavior in pursuing apples on the map, using only two moves: turning left in place and moving right to traverse the map. The agent moves strategically towards apples, exhibiting moments of high influence with other agents. The influencer agent strategically signals the presence or absence of apples to other agents by using its own actions as a form of communication, similar to a binary code. This action-based communication allows the influencer to gain influence over other agents in the environment. Training agents to communicate through influence rewards led to cooperative behavior and emergent communication, similar to the bee waggle dance. Using an explicit communication channel with social influence reward resulted in significantly higher collective rewards for the agents in both games. The optimal hyperparameter settings showed that training the communication head with zero extrinsic reward was effective. Speaker consistency and two measures of instantaneous coordination were introduced to analyze the communication behavior learned by the agents. The communication behavior learned by the agents was analyzed using two measures of instantaneous coordination based on mutual information. These measures are all instantaneous and cannot capture long-term dependencies. Models trained with influence reward showed more consistent communication and coordination, especially in high influence moments. Agents trained with influence reward show more consistent and meaningful communication and coordination, especially in high influence moments. Baseline agents without influence reward exhibit little coordination through communication. Highly coordinated behavior is observed among influence agents during influential moments. Influence is sparse in time, with agents communicating meaningful information to gain influence when it becomes relevant for the listener to act upon it. The relationship between individual agents' rewards and the degree of influence they exert is examined. Agents that are more influenced tend to achieve higher task rewards in both Cleanup and Harvest games, supporting the hypothesis that communication messages should help the listener maximize their own environmental reward. This indicates successful transmission of useful information between agents. In investigating the effectiveness of influence reward without a centralised controller, agents were trained with a Model of Other Agents (MOA) network. The training period was extended to allow the MOA model time to train, and the policy LSTM was conditioned on other agents' actions. A comparison was made with an ablated version that did not use the MOA module for reward computation. Results showed that agents trained with a MOA module achieved a higher collective reward. The Model of Other Agents (MOA) module improved collective reward for agents compared to the A3C baseline. MOA agents outperformed influence agents in Cleanup tasks, showing effective cooperation. The MOA method allows agents to influence each other, leading to better reward signals and cooperative behavior. Influence models achieved higher collective rewards than the previous state-of-the-art. Influence models achieved higher collective rewards than the previous state-of-the-art scores for Cleanup and Harvest environments. The experiments showed that an intrinsic social reward based on influencing other agents improved cooperation and led to higher collective return in social dilemmas. The influence reward also drove agents to learn an emergent communication protocol through their actions, confirming the connection between maximizing influence and maximizing mutual information between agents' actions. However, it is important to consider the limitations of the influence reward. The influence reward may not always lead to cooperative behavior, as it depends on various factors such as the environment and task specifics. While influence is important for cooperation, it may not be sufficient on its own. Agents can gain influence through communication rather than threatening behavior, as it is a more effective way to influence others without sacrificing their own rewards. Agents benefit from being influenced by communication messages, gaining higher individual rewards as the messages contain valuable information. Communication protocols learned through influence reward are more effective. Influence reward enhances communication protocols, leading to higher collective return for agents. It can be computed by predicting other agents' actions and simulating their effects. This advancement in multi-agent social motivation allows agents to understand the impact of their actions on others without needing access to their reward function or a central controller. Using counterfactuals could further improve agents' ability to comprehend the consequences of their actions on others. In multi-agent networks, influence can drive coordinated behavior and act as a regularizer to integrate information. Sequential social dilemma games like Harvest and Cleanup require cooperation to sustainably collect limited resources. In Cleanup, agents cooperate to harvest apples sustainably by cleaning a river or consuming apples produced by others. They can also fine nearby agents. Schelling diagrams illustrate relative payoffs and game-theoretic properties in multi-agent settings. A proof-of-concept experiment tested the influence reward in a special environment. In a special environment, one agent is trapped in a box while the other has the choice to open the box or consume apples. The vanilla A3C agent acts selfishly, with the purple agent rarely choosing to open the box. In contrast, an agent trained with a social influence reward opens the box in 88% of trajectories, allowing both agents to consume apples. Videos of both behaviors are available. The purple influencer agent in the Box trapped environment chooses to open the box early in the trajectory, allowing the other agent more time to collect rewards. The influence reward incentivizes prosocial behavior, as seen in Figure 11. The experiment demonstrates that letting another agent out of a trapped box results in high influence. Video evidence of this behavior is available at https://youtu.be/Gfo248-qt3c. The experiment showed that an agent gains high influence by releasing another agent from a trapped box. Models were trained with specific parameters, including a convolutional layer, fully connected layers, and an LSTM. The social influence reward can be calculated using divergence measures like JSD or pointwise mutual information. The PMI term measures the influence of one agent's action on another's action. Different comparison functions can be used to compute influence. In addition to comparing influence, various hyperparameters can be tuned for each model through random search. Optimal entropy reward and learning rate are searched for, with annealing from lr init to lr final. Parameters for varying influencers, influence reward weight, and curriculum steps are found to be effective. Experimenting with giving influence reward to the influenced agent is also explored. The influence reward is not used in other experiments to allow for independent training. The hyperparameters for each model's best performance are shown in Table 4. The influence models significantly outperformed previous work on inequity aversion. The speaker consistency metric measures the correspondence between a speaker's action and their goal. The speaker consistency metric measures the correspondence between a speaker's action and their communication message. A moment of high influence is shown between the purple influencer and magenta influencee in the Cleanup game. The purple influencer's actions signal to the magenta agent whether apples have appeared. Table 4 displays the final collective reward obtained by each model tested in the experiments. The experiments show that agents can out-perform state-of-the-art results without needing to view other agents' rewards. Collective reward may not always reflect cooperative behavior, as seen in the Harvest game where one agent fails to learn. This leads to a high collective reward but makes sustainable harvesting easier. Random elements were eliminated for the results presented in the paper. In the paper, random seeds in Harvest were eliminated if an agent failed to learn. An alternative strategy was presented, weighting collective reward by the Gini coefficient to measure inequality of returns. Influence models showed the highest performance even with this new metric. The influence reward was also shown to be robust to hyperparameter settings. In FIG5, the top 5 hyperparameter settings for each experiment were plotted to show the robustness of the reward. Training explicitly prosocial agents optimized for collective reward, resulting in better outcomes. Agents were designed to optimize for a combination of their own reward and the collective reward of all agents. The reward function for each agent was defined as the sum of their individual reward and a weighted sum of all other agents' rewards. Hyperparameter search was conducted over the mentioned parameters. Agents trained to optimize for collective reward achieved higher collective rewards in both Cleanup and Harvest games. The optimal value for the weight placed on collective reward was found to be 0.85. However, individual returns for these agents were significantly low, with some agents never receiving any reward in the Cleanup game. Training agents for collective reward requires them to view rewards obtained by other agents. The social influence reward allows agents to achieve cooperative behavior without the need for assumptions."
}