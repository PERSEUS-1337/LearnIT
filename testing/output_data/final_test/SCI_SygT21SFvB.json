{
    "title": "SygT21SFvB",
    "content": "In this work, the study focuses on the generalization of neural networks in gradient-based meta-learning by analyzing objective landscapes. Experimental results show that meta-test solutions become flatter, lower in loss, and further from the meta-train solution as meta-training progresses. The study also challenges the correlation between generalization and flat minima in gradient-based meta-learning. Additionally, it is found that generalization to new tasks is linked to the coherence between adaptation trajectories in parameter space. In this study, the focus is on understanding the characteristics of loss function landscapes in gradient-based few-shot meta-learning. Various meta-learning approaches have been proposed to address few-shot learning, and the study aims to analyze the correlation between meta-test gradients and generalization. The research explores the properties of optimization landscapes related to generalization in gradient-based meta-learning. The experimental work focuses on using the Model Agnostic Meta-Learning (MAML) algorithm to study gradient-based meta-learning. Insights include the adaptation of meta-test solutions becoming flatter and reaching lower support loss values as training progresses. Generalization to new tasks is not correlated with the flatness of minima in the context of gradient-based meta-learning. The experimental work explores the correlation between generalization to new tasks and coherence between adaptation trajectories and metatest gradients in the context of Model Agnostic Meta-Learning (MAML). Initial steps are taken to propose a regularizer for MAML training based on these observations. The curr_chunk discusses the importance of saddle points versus local minima in high dimensional landscapes, the role of overparametrization in generalization, and the flatness of minima of the loss function found by stochastic gradient-based methods. Various studies have measured flatness using different methods such as the spectral norm of the hessian of the loss and the determinant of the hessian of the loss. Researchers have found that flatter minima lead to better generalization. In the context of gradient-based meta-learning, the study explores the properties of objective landscapes in the setting of adapting to tasks sampled from a distribution. The model is trained on a set of training tasks and evaluated on testing tasks, focusing on k-shot learning for classification tasks. In gradient-based meta-learning, the model adapts to a task with few support samples and is evaluated on new target samples. MAML learns initial parameters through second-order approximation, while First-Order MAML omits these second-order derivatives. For finetuning baseline, the model is trained in a standard supervised learning setup. During evaluation on meta-test tasks, the model's final layer is replaced with the appropriate size for the task, and all model parameters are optimized. Visualizations show metrics measuring properties of objective loss landscapes. The average cosine similarity of trajectory directions between meta-train and meta-test tasks is used to measure generalization in gradient-based meta-learning. Generalization is defined as the model's ability to achieve high accuracy on testing tasks after optimizing parameters from a given meta-training parametrization. The optimization landscapes and adaptation trajectories are evaluated to understand the model's performance. The study focuses on measuring the adaptation trajectories and properties of landscapes during meta-training. The analysis includes evaluating the curvature of loss at adapted meta-test solutions. The study measures the curvature of loss at adapted meta-test solutions to determine if flatter minima lead to better generalization for meta-test tasks. The curvature is evaluated using the hessian matrix and spectral norm. The study analyzes the curvature of loss at adapted meta-test solutions using the hessian matrix to determine if flatter minima lead to better generalization for meta-test tasks. The study examines adaptation trajectories of models to tasks and their coherence for generalization. It defines trajectory displacement and direction vectors, as well as a metric for coherence between adaptation trajectories. The study characterizes adaptation trajectories at meta-test time and measures the coherence of meta-test gradients in relation to adaptation trajectories. The problem of symmetries within neural network architectures is highlighted, where different parameterizations can represent the same function. This issue is more prevalent in networks with a higher number of parameters. Ultimately, the focus is on the functional differences that the function undergoes. In the context of adaptation trajectories at meta-test time, coherence of meta-test gradients is measured in relation to adaptation trajectories. The study emphasizes the functional differences that functions undergo, highlighting the issue of symmetries within neural network architectures, especially in networks with a higher number of parameters. To simplify analyses, coherence is measured between trajectories of only one step (T = 1), and target accuracy is measured at meta-test solutions obtained after one step of gradient descent. In contrast to previous sections, the study focuses on the correlation between meta-test gradient vectors and target accuracy. The average inner product between meta-test gradients at a meta-train solution is defined as a metric for generalization on meta-test tasks. Experimental results show that coherence of adaptation trajectories and meta-test gradients correlates with performance on few-shot classification datasets. The study focuses on the correlation between meta-test gradient vectors and target accuracy in few-shot classification problems using Omniglot and MiniImagenet datasets. Three gradient-based meta-learning algorithms are used: MAML, First-Order MAML, and a Finetuning baseline. Experimental setup closely follows previous work, with experiments conducted on various ways and shots for both datasets. The experiments included 5-way 5-shot, 20-way 1-shot, and 20-way 5-shot scenarios, each repeated five times. Hyperparameters were chosen based on previous work for meta-learning algorithms. The MAML baseline kept most hyperparameters unchanged to compare pre-training effects. The optimizer ADAM was used, with hyperparameter search conducted on mini-batch size. Results showed that MAML initially found sharper solutions that became flatter with more epochs. The experiments involved various scenarios with different shot and way settings, repeated five times. Hyperparameters were chosen based on previous work. Results showed that MAML initially found sharper solutions that became flatter with more epochs. In contrast, the correlation between flatness of solutions and generalization was not significant. In this section, experiments were conducted using the same setup as in Section 5.1, with 500 tasks sampled after each meta-training epoch. First-Order MAML analyses were dropped for Omniglot experiments due to similar performance to Second-Order MAML. A correlation was observed between coherence of adaptation trajectories and generalization, with higher inner product linked to higher target accuracy on new tasks. Similar observations were made in other settings as well. In experiments analyzing adaptation trajectories and generalization, a correlation was found between trajectory directions and target accuracies. The relationship between network distances and generalization appeared less linear in some settings, possibly due to challenges in measuring distances in parameter space. Results on objective landscapes at meta-train solutions were presented. Results show that coherence between meta-test gradients is linked to generalization, consistent with observations on adaptation trajectories. Experimental data on E[g] further supports this correlation, leading to a proposal to regularize gradient-based meta-learning. Additionally, experimental results on E[gTi gj] for few-shot regression tasks are presented. In Few-shot regression tasks, a two-layer MLP is trained to fit meta-training sine functions with few support samples. Generalization is measured by low Mean Squared Error on meta-test sine functions. Despite MAML's popularity for meta-training, there is a significant generalization gap between meta-train and meta-test tasks. Regularizing MAML based on adaptation trajectories is proposed to address this gap. Based on observations on adaptation trajectories, a regularization term is added to MAML to improve generalization. The regularized solutions are obtained by reducing the angle between direction vectors. This variant of MAML with regularized updates was used in \"Omniglot 20-way 1-shot\" classification, resulting in increased meta-test accuracy. Regularized MAML improves meta-test accuracy by reducing angle between direction vectors. Meta-test solutions become flatter and lower in loss as metatraining progresses. The study shows that as metatraining progresses, meta-test solutions become flatter even as generalization declines, challenging the correlation between generalization and flat minima. Generalization to new tasks is linked to coherence between adaptation trajectories and meta-test gradients. The findings suggest potential for regularization in MAML meta-training to improve performance on various datasets and meta-learning scenarios. The architecture used in the study consists of 4 modules with specific layers for convolution, batch normalization, activation, and pooling. Different configurations were used for Omniglot and MiniImagenet datasets. The loss function employed was cross-entropy. Omniglot dataset has 1623 classes with 20 instances each, while MiniImagenet dataset has 64 training classes. The MiniImagenet dataset includes 64 training classes, 12 validation classes, and 24 test classes, each with 600 instances of size 84 \u00d7 84. The models are trained and tested using MAML and First-Order MAML with specific experimental setups. Inner loop updates are done with full batch gradient descent, ADAM optimizer is used for meta-update, and meta-learning rates are specified. Adaptation to meta-test tasks is performed with the same number of steps as meta-training. Mini-batches of 16 and 8 tasks are used for 1-shot and 5-shot settings. In k-shot learning, support samples comprise k \u00d7 m samples for an m-way classification task. Each meta-training epoch consists of 500 iterations. The finetuning baseline uses the same hyperparameters for ADAM optimizer during meta-training and meta-test adaptation. Training hyperparameters for mini-batch size and number of iterations per epoch are searched. Experiments run for 100 epochs to limit meta-overfitting and maximize meta-test accuracy. Finetuning models see 100 times less training data per epoch compared to MAML training. During training, mini-batches of 64 images with 25 iterations per epoch were used for 1-shot learning, and mini-batches of 128 images with 12 iterations per epoch for 5-shot learning. Xavier initialization was used for the final layer weights at meta-test time. The performance of models trained with MAML and First-Order MAML for few-shot learning settings on Omniglot and MiniImagenet are shown in Figures 10 and 11, depicting target accuracies on meta-train and meta-test tasks. The relation between target accuracy on meta-test tasks and average inner product between meta-test gradients evaluated at meta-train solution is shown in Figure 12."
}