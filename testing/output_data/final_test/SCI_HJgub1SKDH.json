{
    "title": "HJgub1SKDH",
    "content": "One of the unresolved questions in deep learning is the nature of the solutions discovered by networks with different initializations. Surprisingly, solutions reached by the same network architecture show similar learning dynamics, with examples learned in the same order. Heterogeneous collections of neural network architectures also exhibit this pattern, with more powerful architectures achieving higher accuracy. The study demonstrates that neural network architectures can continue to learn and improve accuracy, even when tested on different datasets. The research includes a comprehensive analysis of various networks, showing a consistent pattern of similarity in learning complex problems. This similarity is not due to optimization but is linked to effective generalization in deep learning models. The success of deep networks in solving classification problems effectively, reaching human-level precision, is not well understood. Different solutions are obtained using variants of Stochastic Gradient Descent (SGD) with random weight initialization and sampling of mini-batches, demonstrating similar performance reliably. To advance understanding, comparing different network instances is necessary. Most comparison approaches are based on deciphering internal representations of learned models. We propose a simpler approach to comparing networks based on their classifications of data. Each network instance is represented by 2 binary vectors capturing train and test accuracy. Different instances of neural networks achieve similar accuracy, predicting that their test-based vector representations should have similar norms. Features of deep networks reliably capture perceptual similarity across different instances. The proposed approach compares deep neural networks classifiers based on their similarity in representation vectors. The analysis shows a high level of similarity between trained networks, with consistent accuracy and patterns across different instances. The analysis reveals that deep neural networks classifiers exhibit similar accuracy and classification patterns across different instances, regardless of optimization methods, hyperparameters, architecture, or dataset. This similarity is consistent even with varied test data, including out-of-sample images, randomly generated images, and artificial images. The results are reproducible across domains and were confirmed using BiLSTM. The analysis shows that deep neural networks classifiers exhibit consistent accuracy and classification patterns across various instances, even with different training samples. Results were reproduced using BiLSTM with attention for text classification, revealing a dynamic learning process where network instances evolve similarly over time and epochs. The analysis reveals that deep neural networks classifiers exhibit consistent accuracy and classification patterns across various instances. When classifying ImageNet with modern CNN architectures, networks initially misclassify most examples and gradually learn to correctly classify them in the same order. This learning process is consistent across different network architectures like VGG19, AlexNet, DenseNet, and ResNet-50. Deep neural networks classifiers exhibit consistent accuracy and classification patterns across various instances, learning examples in the same order but at different paces. This phenomenon extends to simpler learning paradigms like deep linear networks, SVM, and KNN classifiers. However, when labels are randomly shuffled, the agreement between classifiers disappears, indicating that networks that generalize are more similar than those that memorize. The similarity in learning dynamics between networks that generalize and those that memorize is not due to convergence to similar solutions or the use of gradient descent. Different network instances follow their own paths while converging to the final model. This is demonstrated through training shallow CNNs on an artificial dataset and using SGD to train linear classifiers on overlapping Gaussian distributions. Each classifier follows a unique path to reach the optimal solution. Neural network architecture f is analyzed for consistency when repeatedly trained on dataset X from scratch. Different extents of training epochs are considered, creating collections of instances of f. Consistency is measured by comparing predictions of instances within each collection throughout the learning process. The consistency score of an example (x, y) is defined for each epoch e, measuring classifiers' agreement when x is correctly classified. The consensus score measures the agreement of classifiers on classifying examples, while the consistency score measures agreement when examples are correctly classified. The consensus score is determined by the largest number of classifiers classifying each example with the same label. The consistency score results in a bi-modal distribution, quantified using a measure suggested by Pearson. The distribution of consistency scores is analyzed using a measure suggested by Pearson for RV X, indicating how bi-modal X is. The mean consensus score and the distribution of consistency scores determine the similarity of classifiers. The study focuses on CNNs trained on visual tasks and explores the effects of factors like resampling train data and classifying out-of-sample test data. In this section, the distribution of consistency scores among classifiers obtained from a single NN architecture is analyzed. Initially, all networks are random variables, leading to a normal distribution of consistency scores. However, after a few epochs, the distribution transforms into a bi-modal distribution. The distribution of consistency scores among classifiers changes dramatically to a bi-modal distribution, with most examples being misclassified by all networks. Learning proceeds in a specific order, insensitive to network initialization and mini-batch sampling, indicating networks capture similar functions in corresponding epochs. The distribution of consistency scores among classifiers changes to a bi-modal distribution, with most examples being misclassified by all networks. Consensus score measures consistency regardless of true or false labels, showing that all network instances classify examples similarly, even when misclassifying. The study investigated various public domain architectures like AlexNet, DenseNet, and ResNet-50 for ImageNet, VGG19, and st-VGG for CIFAR-10 and CIFAR-100, along with handcrafted networks for other datasets. Results can be replicated by changing hyper-parameters such as learning rate, optimizer, batch size, etc. The analysis extends to include network instances generated by different architectures and comparisons with other classifiers are discussed in section 4.3. The study analyzed various architectures like AlexNet and ResNet-50 for different datasets. Comparisons with other classifiers were discussed. The analysis extended to include network instances generated by different architectures, focusing on the consistency of learning pace between ResNet-50 and AlexNet. The study compared the learning pace of points in different collections trained on the same dataset. The accessibility score, representing the average consistency of a point's learning, was correlated across collections to compare the order of learning. For collections generated by ResNet-50, the correlation was almost 1. Linear operators in Convolutional Neural Networks, like Oja (1992), play a crucial role in deep learning investigations. The study investigated the bi-modal behavior of linear CNN architectures trained on the small-mammals dataset. The linear networks showed weaker performance compared to non-linear networks, with a bi-modal distribution of consistency scores throughout the learning process. The bi-modality in the linear case was even more pronounced than in the non-linear case. In the study, linear CNN architectures on the small-mammals dataset showed weaker performance compared to non-linear networks. The bi-modal distribution of consistency scores was more pronounced in the linear case. Linear and non-linear networks learn examples in a similar order, as seen in Fig. 7b. The common learning pattern was observed across various neural network architectures. Boosting based on linear classifiers as weak learners also exhibited a similar dynamic aspect in training accuracy increase over time. In various machine learning paradigms, including SVM, KNN classifier, perceptron, decision tree, random forest, and Gaussian na\u00efve Bayes, there is a strong correlation with the order of learning observed in neural networks. The bi-modal distribution of consistency scores weakens as the similarity between train and test data distributions decreases. This phenomenon is discussed further in Appendix F. The bi-modal distribution of consistency scores is no longer seen in neural networks when learning to see Gabor patches. A dataset of artificial images with 12 overlapping classes of Gabor patches was used to train st-VGG models, resulting in a shift towards a normal distribution of consistency scores. However, at convergence, the bi-modal characteristics partially re-appear on the test data. Bi-modal characteristics re-appear on test data with random labels, indicating successful generalization. Training accuracy can reach 100% without dropout regularization. The distribution of consistency scores becomes Gaussian, centered around mean accuracy. Bi-modality is not due to gradient descent optimization. The fully connected neural network architecture with 2 intermediate layers, ELU, and dropout is trained to discriminate points from overlapping Gaussian distributions. The consistency scores distribution resembles that of independent networks. Neural networks learn similar classification functions and exhibit similar learning dynamics across different architectures. The study found that different CNN architectures exhibit similar learning patterns when classifying examples, regardless of their size or hyper-parameters. The similarity in classification extends to out-of-sample test data but decreases as the gap between train and test data distributions increases. This pattern holds true across different architectures, showing that data is learned in the same order despite variations in learning speed. The study found that different CNN architectures exhibit similar learning patterns when classifying examples, regardless of their size or hyper-parameters. Stronger architectures start by learning examples weaker architectures classify correctly, followed by more difficult examples. The order in which data is learned seems to be an internal property of the data, not an artifact of stochastic gradient descent. The deeper the network and the more non-linearities it has, the more similar the learning progress becomes in different network instances. The study found that different CNN architectures exhibit similar learning patterns when classifying examples, regardless of their size or hyper-parameters. Stronger architectures start by learning examples weaker architectures classify correctly, followed by more difficult examples. The deeper the network and the more non-linearities it has, the more similar the learning progress becomes in different network instances. This suggests a reduction in the number of degrees of freedom in the learning process, leading to a more uniform way of learning the dataset. This counter-intuitive result is strongly correlated with effective generalization in neural networks. Most training protocols use regularization to prevent unlimited data memorization, leading neural networks to fit easier examples first. The hypothesis is that networks prefer learning simpler hypotheses over memorizing all data. This aligns with the idea of effective generalization in neural networks. The direct comparison of neural representations is a challenging problem due to the large number of parameters and underlying symmetries. Various non-direct approaches in the literature compare subsets of similar features across networks, showing that some features are consistently learned. Methods like SVCCA efficiently compare layers and networks, demonstrating that converged networks are similar and converge from earlier to deeper layers. The study by Raghu et al. (2017) shows that networks which generalize are more similar than those that memorize, with similarity increasing with network width. Various machine learning methods present examples to learners in specific orders, such as curriculum learning, self-paced learning, and active learning. The focus here is on analyzing the order in which examples are learned, rather than altering it. Additionally, the study does not aim to improve initialization methods but instead examines the properties of network instances generated by the same methodology. The study focuses on analyzing the order in which examples are learned, rather than altering it. It also experiments with handcrafted networks, including st-VGG, a simplified version of VGG with specific layers and filters. When training with random labels, dropout layers are removed to facilitate learning. The study used a batch size of 100 and trained the network with SGD optimizer and cross-entropy loss. The st-VGG architecture had a learning rate of 0.05 with decay every 20 epochs. A smaller version of st-VGG was created with 4 convolutional layers, max-pooling, and dropout layers. The fully connected layer in the MNIST architecture has 128 units with 0.5 dropout, followed by an output layer matching the number of classes in the dataset. The architecture includes 2 convolutional layers with 32 and 64 filters, max-pooling, and dropout. The fully connected network used a learning rate of 1 for 12 epochs with AdaDelta optimizer and a batch size of 100. When experimenting with fully connected networks, a 4-layer architecture was used with 1024 units in 2 fully connected layers and a softmax output layer. A dropout of 0.5 was applied after each fully connected layer. Different numbers of fully connected layers were tested with similar results. For textual data, GloVe embeddings were used with a BiLSTM layer of size 300, 0.25 dropout, and recurrent dropout. An attention layer and fully connected layers of size 256 with dropout were also included. Adam optimization with a learning rate of 0.005 and a batch size of 256 were used for optimization. The small-mammals dataset used in the study is a super-class of CIFAR-100. The small-mammals dataset used in the paper is a subset of CIFAR-100, containing 2500 train images divided into 5 classes equally, and 500 test images. Results from this dataset were replicated on larger datasets like CIFAR-100, CIFAR-10, and ImageNet. The cats and dogs dataset is a subset of CIFAR-10, with 20000 train images and 2000 test images, focusing on 2 classes for a binary problem. Additionally, a Gabor dataset with 12 classes was created for experimentation. The Gabor dataset used in the paper contains 12 classes of Gabor patches, each with 100 images varying in size, orientation, parity (odd/even), RGB channel, and base orientation. The dataset will be published upon acceptance. The Gaussian dataset used in the fully connected case has 2 classes sampled from multivariate Gaussians with different means and variances. The dataset for face recognition task included 10 classes from VGGFace2 with 600 train images and 89-243 test images per class. Each image was resized to 64 \u00d7 64 \u00d7 3. The Stack Overflow dataset consists of 39K training samples and 1K test samples tagged with 20 programming languages. The experiments with natural datasets showed consistent results across epochs. Different sets of networks were trained independently for each epoch to ensure fair comparison. The induced class hierarchy by consistency scores in training examples typically forms a hierarchical structure over different classes. In training experiments with natural datasets, consistency scores show a hierarchical structure over classes. 100 instances of st-VGG were trained on small-mammals dataset, revealing a specific order in learning classes. Initially, only 2 classes reach full consistency, with more emerging as learning progresses. Classifier errors show a pattern of using fewer labels at first, becoming more specific later on. Individual image consistency scores evolve throughout the learning process. The consistency scores of individual images evolve throughout the learning process, showing patterns of increasing, decreasing, or maintaining a score of 1 or 0. Some examples exhibit different learning patterns, such as starting with a high score then dropping to 0. The duration of a score of 0 correlates with the difficulty of learning. Experiments were conducted on different architectures using N = 100 instances, exploring various hyper-parameters and optimization techniques. Results showed consistent qualitative outcomes across different architectures trained on the same dataset. The experiments included comparisons between ResNet-50 and DenseNet, as well as AlexNet and DenseNet on the ImageNet dataset. Boosting linear classifiers using AdaBoost with k weak linear classifiers trained on the small-mammals dataset showed improved performance with more classifiers. Accuracy for easy examples was significantly higher than general accuracy and did not improve with increasing k, suggesting that easy examples can be classified linearly and are learned first by both boosting and CNNs. The small-mammals dataset was used to train various classifiers such as SVM, KNN, perceptron, decision tree, random forest, and Gaussian na\u00efve Bayes. These classifiers under-performed compared to a CNN architecture with an accuracy of 0.56. Easy examples were learned first by all methods, resulting in lower accuracy for difficult examples. During the learning process, neural networks tend to learn examples that have already been learned by other paradigms."
}