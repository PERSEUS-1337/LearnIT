{
    "title": "rkgpy3C5tX",
    "content": "Meta-learning, also known as learning-to-learn, is effective in solving problems with limited data in supervised and reinforcement learning. Current methods lack uncertainty quantification, which is crucial in data-scarce scenarios. A proposed meta-learning approach efficiently uses hierarchical variational inference to learn a prior distribution over neural network weights, enabling accurate uncertainty estimates in contextual bandit and few-shot learning tasks. Deep learning has excelled with large amounts of labeled data, but human intelligence can learn new concepts from few experiences. Meta-learning trains models to perform well on various tasks and apply that knowledge to new tasks efficiently, advancing few-shot learning capabilities. Meta-learning models need to perform well on tasks with minimal training data. It is essential for these models to have good predictive uncertainty, expressing high confidence in correct predictions and low confidence in unreliable ones. Bayesian methods offer a way to reason about uncertainty, providing deep learning models with accurate predictive uncertainty. In this paper, the authors extend the work of BID0 on hierarchical variational inference for meta-learning. They aim to combine the flexibility and scalability of the meta-learning framework of BID5 with the uncertainty quantification of the Bayesian model of BID0. The focus is on efficiently amortizing variational inference to improve predictive uncertainty in deep learning models. Hierarchical variational bayes formulation used in BID0 for meta-learning involves a global latent variable \u03b8 and episode-specific variables \u03c6 i. Hierarchical variational inference is used to lower bound the likelihood of the data. Amit & Meir (2018) optimize this problem via mini-batch gradient descent on variational parameters \u03bb i for each episode. This approach is limited to problems with few training episodes and small \u03c6 i. Amortized Bayesian meta-learning scales meta-learning with amortized variational inference by computing variational parameters on the fly instead of storing and computing each parameter for a large number of episodes. This approach is beneficial for problems with a large number of episodes, such as few-shot learning, where deep networks are required. The model predicts \u03bb i from D i using amortized variational inference. Instead of training an encoder for each episode, a good initialization is found for inference. Variational parameters are obtained through gradient descent steps from a global initialization. The model predicts local variational parameters \u03bb i from dataset D i using amortized variational inference. The global initialization \u03b8 serves as both the initialization for the variational parameters and the prior parameters. Tying the prior and initialization reduces the total parameters needed. Backpropagation through the computation of q can be done to compute updates for \u03c8, although it requires computing the Hessian. The backpropagation step for computing updates for \u03c8 involves efficiently computing the Hessian using fast Hessian-vector products. This allows for learning a global initialization of variational parameters, assuming a setting with many more episodes than data points. Uncertainty in global latent variables is kept low by using a point estimate, eliminating the need for global variational parameters. In few-shot learning, the optimization problem simplifies to finding the solution \u03b8* without the need for global variational parameters. Training examples are called the support set, and test examples are the query set. Evaluation involves determining the variational distribution q(\u03c6i) and measuring performance on corresponding test sets. During training and evaluation, a modified objective is used that incorporates support and query set split. Each episode only has access to data from the support set to compute the variational distribution. This conditioning potentially gives a weaker lower bound for all training datasets but improves performance during evaluation by avoiding mismatch in how the variational distribution is computed. During training and evaluation, the distribution is computed differently. The specific model implementation involves using fully factorized Gaussian distributions for priors and posteriors. The SGD process corresponds to Bayes by Backprop with learned priors. Optimization is done via mini-batch gradient descent. The training and evaluation process involves averaging gradients over multiple episodes, with KL-divergence terms calculated analytically and expectations approximated by sampling from the approximate posterior. Variance reduction techniques, such as the Local Reparametrization Trick and Flipout, are crucial for training stochastic neural networks used in the model. The training process involves using techniques like Local Reparametrization Trick (LRT) BID17 and Flipout BID35 to generate independent weight samples for each example. Replicating data in few-shot learning settings allows for multiple weight samples without significantly increasing run time. Meta-learning is approached as either empirical risk minimization (ERM) or Bayesian inference in a hierarchical graphical model. Recent developments in variational inference have allowed for efficient approximate inference in complex models and large datasets, enabling the scaling of Bayesian meta-learning. This approach involves casting meta-learning as Bayesian inference in a hierarchical graphical model, providing a principled framework to reason about uncertainty. Hierarchical Bayesian methods can now handle complex models and large datasets, thanks to these advancements. Our method, closely related to BID5 and Bayesian variants of MAML, focuses on quantifying uncertainty in task-specific weights for tasks like contextual bandits and few-shot learning. Unlike BID14, who use a point estimate for most layers, we maintain uncertainty in task-specific weights. The method uses Stein Variational Gradient Descent to approximate the posterior over weights in the final layer with an ensemble, avoiding Gaussian restrictions. The expressiveness of the posterior depends on the number of particles in the ensemble, with memory and computation requirements scaling linearly and quadratically. Sharing parameters across particles is necessary for linear scaling to larger datasets. Other recent work includes Neural Processes for uncertainty quantification and Bayesian decision theory for an amortization network to output the variational distribution. Our method utilizes an amortization network to output the variational distribution over weights for each few-shot dataset. It does not require a priori distinction between global and task-specific parameters, allowing for easier application to new problems. The learned prior in the model shows that many weights are essentially shared across episodes, simplifying the model's implementation. Our proposed model is evaluated on contextual bandits and few-shot learning benchmarks, compared primarily against MAML. Unlike MAML, our model avoids overconfidence by maintaining uncertainty. We also compare against Probabilistic MAML for few-shot learning. The contextual bandit task involves selecting actions based on observed contexts, such as the wheel bandit problem. The wheel bandit problem involves selecting actions to minimize cumulative regret in a synthetic contextual bandit setup with a scalar hyperparameter controlling exploration. The unit circle in R2 is divided into 5 areas based on \u03b4, with different reward distributions for each region. The agent at each time step must choose an arm among 5 based on the point X = (x1, x2) inside the circle. The optimal arm in the wheel bandit problem depends on the high-reward region X is in, with each region having an assigned optimal arm. The problem becomes more difficult with increasing \u03b4, requiring more exploration to locate the high-reward regions. Thompson Sampling is a classic approach for balancing exploration and exploitation in bandit problems by sampling reward functions from a posterior distribution. Thompson Sampling explores more when posterior has high variance, turning to exploitation when distribution is more certain. BID28 compares Thompson Sampling on different models for contextual bandit problems like the wheel bandit. Meta-learning methods in BID9 involve pre-training with randomly generated data across \u03b4 values. Evaluation using Thompson Sampling shows mean and standard error for cumulative regret across 50 trials. In BID9, meta-learning methods involve pre-training with randomly generated data across \u03b4 values. Evaluation is done on specific instances of the wheel bandit problem, where models like MAML develop a prior over model parameters to get a head start. The meta-learning methods in BID9 involve pre-training with randomly generated data across different \u03b4 values. Evaluation is done on specific instances of the wheel bandit problem, showing that our model outperforms MAML as \u03b4 increases and more exploration is required. Our model shows better cumulative regret compared to MAML, especially with fewer time steps, indicating faster convergence to optimal actions. We visualize the learned prior p(\u03c6 | \u03b8) in FIG1, highlighting differences with MAML. The standard deviation of rewards for specific arms varies, aiding exploration to determine the \u03b4 value in a new problem. Our exploration focuses on determining the \u03b4 value in a new problem. MAML is limited to learning expected reward values and lacks appropriate exploration capabilities. We conduct few-shot learning on CIFAR-100 and miniImageNet datasets, splitting classes for training, validation, and testing. Using a convolutional architecture, we achieve classification accuracies on both datasets. The study compares classification accuracy of a model with MAML in few-shot learning experiments on CIFAR-100 and miniImageNet datasets. The model consists of 4 convolutional layers and a fully-connected layer. Downweighting the inner KL term improved performance. Results show accuracy and confidence intervals, with differences in implementation details affecting miniImageNet results. Our model achieves comparable to slightly worse classification accuracy than MAML and Probabilistic MAML on benchmarks. Reliability diagrams are used to measure predictive uncertainty by plotting expected accuracy against model confidence. A well-calibrated model will have bars align closely with the diagonal line, indicating accurate predictions. The Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) are used to measure model calibration. Reliability diagrams and error scores show that our model is consistently better calibrated than MAML and Probabilistic MAML. Another method to assess predictive uncertainty is by testing the model's confidence on out-of-distribution examples. The model's uncertainty estimates are evaluated by plotting empirical CDF curves on out-of-episode examples. Our model shows better uncertainty estimates compared to MAML-based models. Additionally, the prior distribution learned in tasks involving deep convolutional networks is visualized. The standard deviation of filters in convolutional networks increases from lower to higher layers, reflecting different learning speeds on new episodes. This variation indicates which weights are general and which need to be quickly modified to capture new data. The method described efficiently uses hierarchical variational inference for scalable meta-learning across many training episodes and large networks. It involves learning a prior distribution over network weights to approximate posterior with Bayes by Backprop. Bayesian interpretation helps reason about uncertainty in various few-shot learning tasks. Future work could explore more expressive prior distributions for improved uncertainty estimates. Pseudocode for meta-training and meta-evaluation is provided in algorithms 1 and 2. The method efficiently uses hierarchical variational inference for scalable meta-learning across many training episodes and large networks. Hyperparameters for the model for few-shot learning experiments are provided in Table 4."
}