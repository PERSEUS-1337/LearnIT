{
    "title": "HJgR5lSFwr",
    "content": "This paper addresses limitations of mutual information estimators by redefining the cost using generalized functions from nonextensive statistical mechanics. Variational based estimators outperform previous methods in high dependence high dimensional scenarios. Our approach enables the estimator to capture changes in mutual information over a wider range of dimensions and correlations of input variables. The mutual information is crucial in various fields like machine learning and finance. Estimating it accurately is challenging, especially with limited samples. Existing methods rely on estimating probability density from available samples, but new approaches using generalized functions show promise in capturing changes in mutual information across different dimensions and correlations of input variables. New approaches based on variational bounds, inspired by Donsker & Varadhan (1983), outperform traditional methods in machine learning setups with high-dimensional and high-dependence scenarios. These estimators use neural networks to maximize a lower bound of mutual information, improving accuracy in estimating probability density from limited samples. In this work, new variational lower bounds on mutual information are proposed, inspired by nonextensive statistical mechanics. Generalized versions of logarithm and exponential functions are reviewed to control the trade-off between variance and bias of the estimator, outperforming previous methods in capturing trends in high-dimensional and high-dependence scenarios. The text discusses new variational lower bounds on mutual information, utilizing generalized logarithm and partition functions to improve trend capture in high-dimensional scenarios. It references the work of Poole et al. (2018) for a comprehensive review of variational estimation methods. The approach involves introducing a variational distribution to estimate mutual information when the distributions p(x|y) and p(x) are unknown, relying on samples from these distributions. The Barber & Agakov (2003) lower bound on mutual information is derived through this method. The Barber & Agakov (2003) lower bound on mutual information is obtained by utilizing the Kullback-Liebler divergence. The variational distribution converges to the true distribution when p(x|y) = p(x|y). To simplify the optimization problem, p(x|y) is structured according to Poole et al. (2018), with f as the critic, Z as the partition function, and K as the number of samples. The softmax function \u03c3(x; y) maps the critic's output to a probability, using the average instead of the partition function due to the unknown number of states. By replacing equations and nesting expectations, a tight bound for a(y) = Z(y) is derived. The Nguyen et al. (2010) bound, I NWJ (X, Y), is a tight bound for mutual information when the critic approaches the optimal critic f * (x, y) = 1 + log p(x|y) p(x). The I NWJ estimator performs well in high dimensional high dependence scenarios but exhibits high variance. To reduce variance, the critic can depend on multiple samples and use a Monte Carlo estimator for the partition function, resulting in the noise contrastive estimator I NCE (X, Y). The noise contrastive estimator I NCE (X, Y) is introduced as a solution to the high variance of the I NWJ estimator. It is upper bounded by log K, where K is the batch size. To address this, a nonlinear interpolation between I NWJ and I NCE is proposed by Poole et al. (2018), using a secondary critic and an interpolation parameter \u03b1. This results in the Poole et al. (2018) bound I \u03b1. The Poole et al. (2018) bound I \u03b1 is obtained by a nonlinear interpolation between I NWJ and I NCE, increasing the estimator's upper bound to log(K/\u03b1). While these estimators perform well in high dimensional scenarios, they still have limitations. I NCE addresses the variance issue by averaging the partition function, bounding the estimator by log(K). The I \u03b1 estimator combines the two approaches and can reach higher values. The Poole et al. (2018) bound I \u03b1 is a combination of I NWJ and I NCE, offering higher values but with variance close to I NWJ. A generalization of exponential and logarithm functions from nonextensive statistical mechanics is presented, along with a Boltzmann-Gibbs class definition of entropy in terms of probability sets. This expression has garnered attention in various fields. In information theory, the Boltzmann-Gibbs entropy was generalized by Tsallis in 1988, introducing a parameter q to control the weight of individual probabilities. The q-logarithm is used in the definition of Tsallis generalized entropy, where q < 1 amplifies smaller probabilities and q > 1 amplifies larger ones. This generalization is useful for more complex systems like the brain. The Tsallis entropy, a generalization of classical entropy, is used to describe complex systems like brain activity, financial markets, and black holes. It has evolved into nonextensive statistical mechanics, utilizing generalized logarithm and exponential functions. The q-operators and q-algebra are defined, leading to new variational bounds. The generalized mutual information is also discussed. The generalized mutual information is defined using Tsallis statistics to overcome limitations of previous variational bounds. It differs from classical mutual information due to generalization, with the ability to choose weight for large values in probability mapping. The triplet (1, 2, 3) is shown as an example, with different probabilities based on the parameter q. This will be used to enhance estimator performance in the next section. The generalized mutual information is defined using Tsallis statistics to overcome limitations of previous variational bounds. It differs from classical mutual information by allowing for a weight to be chosen for large values in probability mapping. The behavior of exp q (x) is similar to log q (x) with respect to the generalization parameter q, and they are inverses of each other. Normalizing the triplet (1, 2, 3) using the q-generalization of the partition function attributes higher probability mass to the largest value as q increases. To obtain a variational bound on the generalized mutual information, the definition is multiplied and divided by the variational distribution p(x|y). The inequality holds for all values of q and is tight when p(x|y) = p(x|y). The expression can be simplified by selecting p(x|y) as a generalized partition function. Different values of q yield different functions, with q = 0 giving the counting measure and q = 1 giving the softmax function. Setting q of the partition function equal to the Tsallis divergence, we obtain the Nguyen, Wainwright, and Jordan bound. The proposed lower bound on mutual information, named I NES, is derived based on nonextensive statistical mechanics. It is obtained by replacing the optimal critic with the multisample optimal critic in the inequality, resulting in an expression that is iteratively reindexed and averaged. This bound is similar to the original I NCE but overcomes the limitation of being upper bounded by log q (K). The proposed lower bound on mutual information, I NES, is derived from nonextensive statistical mechanics. It overcomes the limitation of being upper bounded by log q (K) by choosing a generalization q < 1 for the logarithm. The generalization of the logarithm and partition function can be set independently to control how the critic output is mapped to a probability. Using Gaussian distributed random vectors, the estimator's performance is compared with previous works. The mutual information can be calculated using a specific formula, with 3000 samples used for each experiment. The proposed lower bound on mutual information, I NES, is derived from nonextensive statistical mechanics. For each experiment, 3000 samples are used to estimate mutual information and train the critic in batches of 512 samples. Results show that for low dimensions, most estimators capture the trend, but for dimensions greater than 10, I NWJ and I NCE saturate. I NES outperforms I \u03b1, which requires both critics to converge. The returned value of I NES is not a classical mutual information but has similar properties. Estimators perform better over a wider range of input correlations. The study aimed to enhance mutual information estimators in high dependence, high dimensional scenarios. The I NES estimator showed consistent performance across a wide range of correlations, with higher variance than I NCE but lower than I NWJ. By tuning the generalizations q, the trade-off between bias and variance of the estimator can be controlled. The proposed I NES estimator improves upon existing methods by tuning generalizations q to control bias and variance trade-off. It captures dependence between input variables and is applicable in machine learning tasks like feature selection and generative adversarial networks. In q-algebra, operations are defined as follows: x \u2295 q y = x + y + (1 \u2212 q)xy and x q y = x \u2212 y 1 + (1 \u2212 q)y. Exponential and logarithmic identities have a different form: exp q (x + y) = exp q (x) \u2297 q exp q (y) and log q (xy) = log q (x) \u2295 q log q (y)."
}