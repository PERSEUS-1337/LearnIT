{
    "title": "rJ4vlh0qtm",
    "content": "Multi-agent collaboration in real-world problems often requires a distributed setting with local communication and information aggregation. Previous studies in multi-agent reinforcement learning have focused on communication architecture but suffer from rigid structures. A new approach, spontaneous and self-organizing communication (SSoC) learning scheme, treats communication as an explicit action to adapt to complex and unpredictable scenarios. SSoC enables agents to organize communication effectively by allowing them to decide when and who to send messages based on observed states. This dynamic communication channel emerges in a self-organizing manner, leading to intelligent message passing among agents. Experiments show that SSoC promotes effective collaboration tactics not mastered by other methods in various real-world applications involving multiple agents. Recent advancements in deep reinforcement learning have shown promising results in various challenging scenarios such as robotic manipulation, visual navigation, and game playing. However, deep RL still faces challenges in multi-agent learning scenarios. Two extreme solutions for modeling multiple agents are treating them as a single entity or as independent learners. The centralized approach offers optimality guarantees but has limitations in solving multi-agent tasks. The advantage of centralized approaches in multi-agent reinforcement learning is the guarantee of optimality, similar to a single agent Markov decision process. However, the assumption of a global controller knowing everything about the environment is impractical. Independent multi-agent reinforcement learning treats agents as independent entities, leading to non-stationarity issues. Existing approaches like Meanfield and Commnet have predefined communication structures, limiting flexibility. Recent methods like VAIN and ATOC focus on when, where, and how communication is initiated. The proposed architecture, named \"Spontaneous and Self-Organizing Communication\" (SSoC) network, aims to improve communication flexibility in multi-agent reinforcement learning. Unlike existing methods like VAIN and ATOC, which have limitations in message flow range and communication time, SSoC introduces learnable spontaneous communication behaviors and self-organizing message flow among agents. The \"Spontaneous and Self-Organizing Communication\" (SSoC) network enables agents to communicate spontaneously through the action of \"Speak\", sending messages to partners within the communication scope. Unlike existing approaches, SSoC agents only communicate when necessary and stop transferring messages if they are useless, learning a self-organizing communication policy to maximize collaborative rewards. The SSoC network facilitates spontaneous communication among agents to maximize collaborative rewards. Communication is dynamic and self-organizing, with agents sending messages based on real needs. This process forms a communication route without predefined channels, promoting efficient information transfer. The SSoC network enables spontaneous communication among agents through a \"Speak\" action that controls message transfer. The \"Speak\" policy is learned through reinforcement learning to optimize message propagation for higher rewards. This approach leads to novel communication patterns and enables complex collaborative strategies among agents. The SSoC network facilitates spontaneous communication among agents through a \"Speak\" action, optimizing message propagation for higher rewards. Collaborative strategies are enhanced by visualizing communication efficiency with a heat map. Performance gains are expected compared to existing methods on tested tasks. The VAIN BID4 and BID15 models use a pair-wise communication structure for tasks like multi-object tracking. However, these models face scalability issues due to the growing state-action space with more agents. Other approaches, like BID14 and BID18, aim to establish a global communication channel for all agents to improve communication effectiveness. BID17 introduces a broadcasting communication channel among agents using hidden states as messages, but it has limitations such as producing redundant information and using a single net for all agents. In contrast, BID19 proposes the Meanfield algorithm for large-scale multi-agent problems, promoting local communication between agents and a mean-agent of neighbors. However, its capability is limited for complex tasks. Recent studies explore a \"centralized/communicated training + decentralized/independent execution\" approach. Recent studies have been exploring a \"centralized/communicated training + decentralized/independent execution\" approach in multi-agent systems. Different methods have been proposed, such as using a global critic for centralized training, adapting actor-critic methods with centralized action-value functions, and utilizing recurrent attention models for better local communication. However, challenges remain in establishing explicit communication and information processing modules for effective collaborations, especially for learning complex and long-range communications. SSoC network enables long-range communication through multi-step message propagation in a distributed partially observable MARL environment. Each agent can only see a neighboring circular area and aims to reach a winning state by taking a sequence of actions based on observed states and messages from partner agents within a local range. SSoC network allows agents to communicate by sending messages, referred to as \"Speak\" actions, to neighboring agents. Only agents taking a \"Speak\" action can be heard by partners within their communication range. Partners can use received messages to determine their actions and pass messages to their neighbors if they also \"Speak\". This communication path forms naturally among all agents through multiple steps. SSoC network establishes a \"global\" communication channel through self-organization, allowing agents to send binary \"Speak\" signals to control message passing. The \"Speak\" action is determined by each agent based on its observation, enabling communication in a distributed MARL scenario. The \"Speak\" action in SSoC network is self-organizing, initiated by agents based on their observations. This spontaneous communication is not predetermined or manually designed, allowing agents to learn when and who to communicate with. Each agent combines received messages with their own thoughts to output actions, creating a global communication channel beyond local limitations. This self-organizing communication addresses key challenges in multi-agent communication. The SSoC network introduces a new approach to multi-agent communication. It consists of an \"agent stream\" and a \"message stream\" where agents generate actions based on their thoughts and received messages. The message stream aggregates messages from agents within communication range using a learnable weighting unit. This self-organizing communication system addresses challenges in multi-agent communication. The SSoC network utilizes a self-weighted aggregator (LWU) to collect messages from neighbor agents. These messages are then combined with the agent's own \"thought\" using a message composition unit (MCU) before being sent to other agents. The policy network consists of 3 fully-connected layers, and a binary \"Speak\" action determines message transmission. The MCU incorporates a learnable gating mechanism for message merging. The ACU is a mechanism that allows agents to decide how much of their own \"thought\" and other agents' information to include in their output message. It functions similarly to MCU but focuses on outputting the agent's action without temporal recurrence. The agent's action is updated using regular policy gradient with baseline, and the \"Speak\" action involves an additional \"speak\" policy gradient. Output messages are stored in a buffer for reuse in the next time step, with messages taken out for feed-forward sampling and backward propagation during training. Experiments are conducted on multi-camera intelligent surveillance task and large-scale battle task in a distributed collaborative MARL environment. Agents have local observations and take actions independently, with a shared reward for model training. Two state-of-the-art methods are selected as baselines for comparison. Multiple agents cooperation is needed in real-life scenarios, such as intelligent cameras tracking target people in airports or railway stations by changing their pose actively. In a simulation task called multi-camera intelligent surveillance, agents control cameras to track moving target balls in an airport or railway station. Cameras can adjust their angles to capture the targets as long as possible, sharing information with nearby cameras in a distributed setting. The objective is to actively monitor and capture the moving balls by changing angles. The cameras aim to capture moving balls by adjusting angles cooperatively. The task is formulated as a multiagent reinforcement learning problem with specific objectives, states, actions, and rewards. The camera agent needs to learn without prior knowledge of the ball's information. In a multi-camera intelligent surveillance task, the camera agent learns to adjust its policy and send necessary information to local partners. When the ball is captured, it sends a message to the next camera for better learning. If the ball is out of view, no message is sent. Results of SSoC are compared with other algorithms in a 30-time step episode training for 5000 episodes. SSoC agents learn a smart strategy to capture the ball continuously until it moves out of sight. They communicate useful messages to neighboring agents, enabling a collaborative strategy. Agents can speak \"1\" to transfer messages and remain silent when necessary. The learned policy is visualized in a video demo for better understanding. This scenario is part of the MAgent environment BID21. In the large-scale battle task of the MAgent environment BID21, SSoC controls a group of agents to eliminate opponents. The task is challenging due to the large number of agents and distributed setting. SSoC outperforms baselines in mean rewards and mean kill. It also shows a higher win rate against Commnet, Meanfield, and Independent. SSoC demonstrates superior performance in distributed MARL tasks, outperforming baselines in win rate. The heatmap analysis reveals the effectiveness of SSoC's communication strategy in achieving success in battles, showcasing meaningful message flow between agents. The agents in group b start communication by taking \"Speak\" actions at the 10th frame. Messages are sent to group b through several steps, with agents gathering nearby to move right and confront enemies at the 15th frame. Eventually, two groups join forces to eliminate most enemies, showcasing the effectiveness of self-organizing communication in learning a better collaborative policy. The proposed scheme enables multi-step message transferring, allowing for long-range reinforcement policies to be learned. Most agents only communicate when necessary, rather than constantly sending messages like in previous models. In this paper, a SSoC network for MARL tasks is proposed, allowing agents to learn when to initiate communication through a \"Speak\" action. This spontaneous communication action helps establish a dynamic self-organizing structure, leading to better collaborative policies and improved communication efficiency. Future work will focus on enhancing the learning of the \"Speak\" action to optimize communication flow. The paper proposes a SSoC network for MARL tasks, enabling agents to learn when to communicate through a \"Speak\" action. Future work will enhance the learning of this action to optimize communication flow."
}