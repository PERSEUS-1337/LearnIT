{
    "title": "Skgxcn4YDS",
    "content": "LAMOL is a method for lifelong language learning that replays previous tasks without needing extra memory. It outperforms previous methods and can handle five different language tasks sequentially with only one model. The source code for lifelong learning is available at https://github.com/jojotenya/LAMOL. Lifelong learning aims to accumulate interconnected knowledge between tasks to prevent catastrophic forgetting in machine learning models. It is a vital step towards artificial general intelligence. Lifelong language learning is crucial for artificial general intelligence. Previous studies focused on tasks like sentiment analysis, conversational agents, and word representation learning. However, these tasks were similar in nature. To address fundamentally different tasks, LAMOL - Language Modeling for Lifelong language learning is proposed. Many NLP tasks can be viewed as question answering, so multiple tasks are tackled with a single approach. The approach of Language Modeling for Lifelong language learning (LAMOL) addresses multiple NLP tasks by training a language model to generate answers based on context and questions. While pre-training the LM on unlabeled data is beneficial, catastrophic forgetting still occurs when training on a stream of tasks. To prevent forgetting, a generator is used to generate examples seen before, allowing the learner to also learn from previous tasks. LAMOL prevents forgetting by using a language model as both learner and generator. It generates samples from previous tasks, eliminating the need for an extra generator. This method is similar to multitask training but generates data internally. Key advantages include no extra memory requirements, flexibility in training on additional tasks, and outperforming baselines in experiments. Our method outperforms baselines and state-of-the-art methods by a considerable margin, approaching the multitasking upper bound within 2-3%. Adding task-specific tokens during pseudo-sample generation evenly splits samples among tasks, stabilizing LLL, especially beneficial for training on numerous tasks. Analyzing the impact of pseudo-samples on LAMOL's performance, we provide open-source code for further research in lifelong learning. Regularization, architecture, and data are key aspects in lifelong learning research, with methods focusing on minimizing deviation from trained weights when updating for new tasks. Regularization methods in lifelong learning estimate parameter importance and add it as a constraint to the loss function. Various approaches like Elastic Weight Consolidation (EWC), Online EWC, Synaptic Intelligence (SI), Memory Aware Synapses (MAS), and Incremental Moment Matching (IMM) assign importance to parameters differently. EWC calculates sensitivity using a Fisher information matrix, while SI assigns importance based on contribution to total loss change. MAS estimates importance through model output gradients, and IMM matches weight moments between tasks. Progressive neural networks assign dedicated capacity for each task inside a model, freezing weights after completion. Columns in the neural network are added for new tasks while transferring knowledge from old tasks. Towards Training Recurrent Neural Networks for Lifelong Learning combines Gradient episodic memory and Net2Net, learning tasks in easy-to-hard order using a curriculum-based setting. The model learns tasks in easy-to-hard order using a curriculum-based setting. It addresses the forgetting problem with the GEM method and expands to a larger model through the Net2Net approach if needed. PathNet reuses subsets of a neural network for knowledge transfer between tasks, while PackNet prunes and re-trains the network iteratively to accommodate multiple tasks in a single model. When resources are limited, model expansion is restricted. Some methods allocate capacity for tasks based on architecture, reducing practicality. Weight restrictions are imposed through data distribution of old tasks. Two approaches involve keeping real samples or distilling knowledge from old data to prevent weight deviation during new task training. GEM preserves real samples for optimization, while A-GEM is a more efficient version. A-GEM is a more efficient version of GEM that achieves the same or better performance. Learning without forgetting minimizes parameter alteration by recording outputs from old task modules before updating. Shin & Kim and Kemker & Kanan encode data from old tasks into a generative model system, replaying pseudo-data during training. d'Autume et al. investigates episodic memory performance on NLP problems, distilling knowledge into episodic memory for replay. LAMOL is a method that combines a language model and a question answering model to generate context, questions, and answers. It uses pseudo-old samples during training to prevent catastrophic forgetting. The data format depends on the training objective, with samples framed in a SQuAD-like scheme. The LM format for training as a QA model involves decoding the answer after reading context and question. For training as an LM, decoding all three parts is learned given a generation token. Special tokens like ANS, EOS, and GEN are added. Sequential training on tasks leads to catastrophic forgetting, so pseudo samples are generated before training on a new task. The model generates pseudo samples representing previous tasks before training on a new task. Samples with more than one answer are discarded. During training, samples are formatted for both QA and LM formats, and losses are minimized together. Using the same GEN token for all tasks can lead to forgetting old tasks. To address the issue of forgetting old tasks in LLL, the model replaces the GEN token with task-specific tokens for each task. This ensures that all previous tasks receive an equal share of generated pseudo samples. Evaluation metrics are assigned to each task, with details provided in Table 1 and Appendix A. All methods utilize the smallest pre-trained GPT-2 model. The LAMOL model uses the smallest pre-trained GPT-2 model for training tasks, with specific parameters set for sampling and loss weight. Real data is used to replace pseudo-samples, ensuring an equal distribution among previous tasks. A summary of averaged scores on five tasks is provided, with different variations of LAMOL and Multitasked models serving as upper bounds. The LAMOL model uses real data for training tasks, ensuring equal distribution among previous tasks. Different methods like fine-tuning, multitask learning, regularization-based methods, and Gradient Episodic Memory (GEM) are compared for lifelong learning. GEM involves sampling data from previous tasks during training. The GEM approach retrieves data from memory for gradient calculation in optimization steps. Improved memory-based parameter adaptation (MBPA++) includes sparse experience replay and local adaptation for LLL. GPT-2 model shows good performance on text classification datasets, outperforming BERT-based model. GPT-2 has potential for superior LLL performance if catastrophic forgetting is prevented. In a small-scale experiment on three datasets (SST, QA-SRL, WOZ), different task orders were tested for their impact on model performance. Results showed that task order significantly affects performance, with some methods like LAMOL 0.2 GEN performing better than others. The order of tasks is crucial, as seen in the significant drop in performance for WOZ when trained after other tasks. When using LAMOL, the performance of old tasks remains consistent throughout training. Increasing the sampling ratio \u03b3 improves performance, especially when increased from 0 to 0.05. Task-specific tokens are beneficial when \u03b3 = 0.05 to retain the presence of the first task. A better LLL method has a smaller standard deviation, indicating less impact from task order. Task-specific tokens also help stabilize performance. The proposed LAMOL with \u03b3 > 0 shows the ability to retain learned knowledge across tasks. In a sequential training scenario with five tasks, LAMOL outperforms all baselines by a large margin and approaches within 2-3% of the multitasked upper bound on average. Performance improves with higher \u03b3 values and the use of task-specific tokens. The method of using real samples is more sample-efficient compared to pseudo-samples. The quality of pseudo-data may affect the model's ability to generate high-quality samples. Task-specific tokens sometimes fail to constrain the model, leading to inconsistencies in generated examples. Improving this issue could enhance the performance of using task-specific tokens. Test scores of each method on different tasks are illustrated in Figure 3 during training. When using LAMOL, the model remembers nearly perfectly, showing high transferability between tasks like SQuAD and QA-SRL. Forward transfer is retained with replaying pseudo-data, leading to improved scores on tasks like WikiSQL and SST. The model's ability to transfer knowledge between tasks is highlighted, with surprising improvements seen when training on QA-SRL for tasks like WikiSQL. The WikiSQL score significantly improves when training on QA-SRL for Fine-tuned and MAS after WikiSQL is forgotten during SST training. Comparison with MBPA++ shows higher scores, but LAMOL 0.2 TASK still outperforms. Experimenting with \u03b3 values influences LLL performance, with and without task-specific tokens. In this experiment, the influence of \u03b3 on LLL performance with and without task-specific tokens was studied. The results showed that using task-specific tokens mitigates the vanishing distribution issue, with better performance seen in models generating more samples. However, the performance gain diminishes when the sampling ratio \u03b3 is between 0.1 to 0.3. The implementation of MBPA++ can be found at https://github.com/Daikon-Sun/EM-in-LLL. LAMOL is proposed as a simple yet effective method for LLL based on language modeling, achieving LLL with a single LM without additional components or keeping old examples. Any pre-trained LM can be used to leverage unlabeled text for LLL improvement, with the flexibility to add more tasks as needed. Five tasks from decaNLP are included, such as Question Answering using the SQuAD dataset for evaluation. Test datasets are hidden from the host in the task discussed by Socher (2018), requiring models to be uploaded to generate results. The development set was used to test the metric, with a training set size of 87,599 and a development set size of 10,570. Semantic Parsing task involves translating sentences into SQL queries with logical forms provided by WikiSQL. The training set size is 56,355, and the test set size is 15,878. Sentiment Analysis task uses Stanford Sentiment Treebank with movie reviews. Treebank (SST, binary version) dataset contains movie reviews for binary sentiment analysis. The training set has 6,920 samples, and the test set has 1,821 samples. Semantic Role Labeling -QA-SRL task involves question answering with a training set of 6,414 samples and a test set of 2,201 samples. Goal-Oriented Dialogue -English Wizard of Oz (WOZ) task focuses on restaurant reservation with a training set of 2,536 samples and a test set of 1,646 samples. The curr_chunk discusses text classification tasks and datasets from MBPA++, including News Classification, Sentiment Analysis, Wikipedia Article Classification, and Questions and Answers Categorization. The dataset includes a summary of averaged scores on five tasks, with the best performance highlighted. The United States conducted a wide-ranging war in Afghanistan since 9/11, resulting in casualties among American servicemen, civilians, and Afghan soldiers. The conflict also involved French military actions in Libya and Egypt in the mid-19th century. The Gaddafi family relocated to Egypt in 1856, and the army returned to Benghazi in 1857. Gaddafi's army returned to Benghazi. The review of a movie was negative. Juan Valderrama stated that the president's nominees would be appointed by the president. The conflict also involved French military actions in Libya and Egypt in the mid-19th century. The first two films in a series were made by two different people working together. The films include Chaos Squad1 and Squad1. The films feature main characters played by the same cast members as in the original series. Reviews for the films were positive. On June 19, 2012, former heavyweight champion Arthur Lang defended his title against Alexander Green in the final of the world heavyweight championship. Lang was only twenty-five years old at the time of his first appearance on the TV series. The movie features two fascinating characters who are popular in Austrian films. The review mentions that the film is not as good as expected, with both positive and negative aspects. Examples generated by LAMOL with task-specific tokens: squad1, wikisql, sst, srl, and ans. Annotations correspond to each task-specific token of SQuAD, WikiSQL, SST, and QA-SRL. The upper frame shows the normal situation, while the lower frame displays generated contents inconsistent with their task-specific token."
}