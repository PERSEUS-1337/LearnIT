{
    "title": "ByxtC2VtPB",
    "content": "Adversarial examples can easily fool deep networks due to locally non-linear behavior. Mixup in training improves generalization and model robustness against perturbations by introducing global linearity. Mixup-trained models passively defend against attacks in inference, but a new approach called mixup inference actively breaks locality via global model predictions. MI mixups input with random clean samples to transfer perturbations, improving adversarial robustness for models trained by mixup. Adversarial examples exist in various domains, highlighting the need to enhance model robustness against attacks. Previous methods have attempted to address the issue of classifiers being unstable on data manifolds by introducing transformations on input images. These include adding Gaussian noise to keep processed inputs close to the original ones. However, these strategies can be evaded due to the local randomness of the perturbations. Another approach involves applying non-linear transformations to the inputs. Many methods aim to improve adversarial robustness in training by inducing stable behavior through data augmentation on adversarial examples. However, these methods can be computationally expensive and may degrade model performance on clean inputs or under general transformations. In contrast, mixup training introduces globally linear behavior. The mixup training method introduces globally linear behavior in-between data manifolds, improving adversarial robustness. It is computationally more efficient than other methods and can maintain state-of-the-art performance on clean inputs. Previous work has focused on embedding the mixup mechanism in the training phase. The paper introduces mixup inference (MI) for actively defending against adversarial attacks by utilizing the globally linear behavior of mixup-trained models. MI performs a global linear transformation on inputs by mixing them with a clean example, leading to robustness improvements through perturbation shrinkage and geometric intuition. Mixup inference (MI) defends against adversarial attacks by shrinking perturbations and introducing global randomness, making it less vulnerable to adaptive attacks. MI's global linearity ensures the recovery of input identity proportional to the mixup ratio. Experimental evaluation on CIFAR-10 and CIFAR-100 shows effectiveness against oblivious and adaptive attacks. The MI method efficiently defends against adversarial attacks in inference, including both oblivious and adaptive attacks. It is also compatible with other mixup variants like the interpolated AT method. The paper introduces notations, mixup training formula, and adversarial attacks in detail. In the adversarial setting, data pairs (x, y) are augmented to triplets (x, y, z) with an extra binary variable z. The variable z is hidden during inference, and a perturbation \u03b4 crafted by adversaries should not change the true label y = y 0. Adversarial examples are off the data manifolds, and p -norm adversarial attacks have a preset threshold. The empirical risk minimization principle is commonly used in supervised learning. The mixup training mechanism minimizes the loss function by combining input-label pairs from the training dataset. It induces globally linear behavior of models, improving generalization performance and adversarial robustness. Compared to adversarial training methods, mixup requires less computation while maintaining state-of-the-art performance. The mixup training mechanism induces globally linear behavior in models, improving generalization and adversarial robustness with less computation. The mixup inference (MI) method utilizes the globality of mixup-trained models in the inference phase to break the locality of adversarial perturbations. In mixup inference (MI), a label y s is sampled, followed by sampling x s from p s (x|y s) and mixing it with x to get asx = \u03bbx + (1 \u2212 \u03bb)x s. MI is executed N times to obtain F MI (x), with a fixed mixup ratio \u03bb as a hyperparameter. In mixup inference (MI), a well mixup-trained model F can be denoted as a linear function H on the convex combinations of clean examples. The mixup-trained classifier F predicts the one-hot vector of label y. The prediction vector can be compactly denoted as a non-linear part G(\u03b4; x) of F for inputs off the data manifolds. The output of x in MI is given by Eq. (3) and Eq. (4) when the execution times N tend to infinity. In mixup inference (MI), a well mixup-trained model F can be denoted as a linear function H on the convex combinations of clean examples. The mixup-trained classifier F predicts the one-hot vector of label y. The prediction vector can be compactly denoted as a non-linear part G(\u03b4; x) of F for inputs off the data manifolds. The output of x in MI is given by Eq. (3) and Eq. (4) when the execution times N tend to infinity. Now, investigating the y-th and \u0177-th components of F (x) reveals how they differ and impact classification or detection of adversarial examples. The sampling label y s controls the MI output F (x) for each execution, with different distributions resulting in different versions of MI. Two easy-to-implement cases are considered: MI with predicted label (MI-PL) where the sampling label y s is the same as the predicted label \u0177. In mixup inference (MI), the predicted label (MI-PL) is discussed, where the sampling label y s is the same as the predicted label \u0177. Simplified formulas are provided in Table 1 for clarity. The model's performance changes with and without MI, focusing on the formula of interest. The robustness improving condition (RIC) is defined as an improvement in robustness if the prediction value on the true label y increases while it on the adversarial label \u0177 decreases after performing MI when the input is adversarial. In mixup inference (MI), the detection gap (DG) is used to measure the ability to detect the hidden variable z. Different versions of MI are analyzed for their properties and benefits in defense strategies. MI-PL operation ideally does not affect predictions on clean inputs but can be beneficial for adversarial inputs. MI-PL can be used as a general-purpose defense or a detection-purpose defense against adversarial inputs. It aims to improve general-purpose robustness by satisfying RIC and involves mechanisms like input transfer to defend against adversarial attacks. The stochasticx 0 induces global and diverse randomness, making it harder for adaptive attacks in the white-box setting. Perturbation shrinkage with a factor \u03bb tightens the upper bound on attack ability. Empirical results show that a smaller perturbation threshold weakens attacks. MI-PL improves robustness against adversarial attacks by defending against prediction degradation. In Fig. 2, empirical evidence shows that existing adversarial attacks, such as the PGD attack, satisfy certain properties. MI-PL and MI-OL can improve robustness against adversarial attacks by defending against prediction degradation and inducing global randomness. MI-OL can improve robustness against adversarial attacks by defending against prediction degradation and inducing global randomness. MI-OL can defend a broader range of attacks than MI-PL, as verified in Fig. 2. In detection-purpose defenses, MI-PL performs better than MI-OL in practice. In this section, experimental results on CIFAR-10 and CIFAR-100 are provided to demonstrate the effectiveness of MI methods in defending adversarial attacks using ResNet-50 and momentum SGD optimizer. Training is conducted for 200 epochs with a batch size of 64 and specific learning rates for different methods. The attack method used is untargeted PGD-10 with specific parameters. The code is available at an anonymous link. In this subsection, the untargeted PGD-10 attack with specific parameters is evaluated using MI methods on CIFAR-10. The hyperparameter \u03b1 for mixup and interpolated AT is set to 1.0, and defenses with randomness are executed 30 times for averaged predictions. Empirical results show that MI-PL and MI-OL improve robustness under the attack, as demonstrated in the following sections. The performance of the method is evaluated under oblivious-box attacks, where the defense mechanism is not known to the adversary. Results show that using MI-PL improves detection of adversarial attacks compared to direct confidence detection. Classification accuracy on oblivious adversarial examples from CIFAR-100 is provided in Table 3. The number of iteration steps for attacks is indicated, with accuracy represented by \u2264 1%. Parameter settings for each method can be found in Table 5. Comparison with previous defenses like adding Gaussian noise or random rotation is done. Performance on CIFAR-10 and CIFAR-100 is reported in Table 2 and Table 3. Hyperparameters are carefully selected to ensure fair comparisons between methods. Our MI method improves model robustness by inducing global linearity, demonstrated through hyperparameter grid search. MI-Combined variant combines MI-OL and MI-PL for adversarial input detection. Ablation studies confirm the exploitation of global linearity in mixup-trained models. The MI method enhances model robustness by promoting global linearity in mixup-trained models, improving adversarial accuracy even under strong adaptive attacks. The generated perturbations are averaged over adaptive samples to address clipping issues, as shown in Fig. 4. Additional background information related to adversarial attacks is provided in this section. Adversarial attacks in deep learning have been successful, but human imperceptible perturbations can fool high-performance models like DNNs. The projected gradient descent (PGD) method is commonly used for crafting adversarial examples. Different threat models are introduced in the adversarial setting to address these attacks. In the adversarial setting, threat models involve assumptions about the adversary's goals, capabilities, and knowledge. Goals can range from fooling classifiers in untargeted mode to misclassifying specific examples in targeted mode. Adversarial examples require bounded perturbations under different norms. Adversary's knowledge levels vary, with defense methods evaluated against oblivious adversaries. Defense methods are evaluated against different types of adversaries: oblivious adversaries are unaware of the defense and attack based on the classification model, white-box adversaries know the defense scheme and parameters, and black-box adversaries have limited access. Experiments focus on the oblivious and white-box settings, as previous work shows randomness is effective against black-box attacks. The most widely used framework for adversarial training is the saddle point framework. The interpolated AT method combines adversarial training (AT) with mixup, training on interpolations of adversarial and unperturbed examples. It aims to achieve higher accuracy on clean inputs while maintaining robustness. The \u03bbOL parameter is varied in an ablation study, providing technical details on the method and experiment implementation. The MI method is generally compatible with models trained with induced global linearity. There is a gap between empirical results and theoretical formulas due to mixup acting as a regularization. To improve MI performance, stronger regularization can be imposed, such as training with mixup for more epochs. Adaptive attacks are designed for the MI method following Athalye et al. (2018). In designing adaptive attacks for the MI method, the expected model prediction is calculated based on a fixed hyperparameter \u03bb. The gradients of the prediction with respect to the input x are determined using a series of examples in adaptive PGD attacks. The hyperparameter settings for the experiments are provided in tables 4 and 5. The original methods by Guo et al. (2018) were designed for ImageNet models but were adapted for CIFAR-10 and CIFAR-100 in this study. Experiments were mainly conducted on the NVIDIA DGX-1 server with eight Tesla P100 GPUs."
}