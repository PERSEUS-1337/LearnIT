{
    "title": "BJg1fgBYwH",
    "content": "The proposed SAFE-DNN enhances classification robustness by incorporating unsupervised learning of low-level features using SNN with STDP. Experimental results show improved noise robustness for various DNN architectures on CIFAR-10 and ImageNet subset without compromising accuracy. This is crucial for deploying DNNs in autonomous systems like autonomous vehicles and robotics, ensuring reliable classifications even with noisy data. Pixel perturbation can lead to errors in CNNs trained with SGD, degrading classification accuracy. Two main approaches to improve robustness are image de-noising networks, which preprocess images but may reduce accuracy for clean images, and training on specific noise types. Advanced de-noising networks can generalize to different noise types but are not suitable for real-time applications. An alternative approach is to develop a classification network that is inherently robust to input perturbations, such as training with noisy data or introducing noise to network parameters. These methods do not increase computational or memory demand. The paper proposes a new class of DNN architecture that integrates unsupervised neuro-inspired learning with supervised training to create a system resilient to input perturbations without the need for noisy data. This approach aims to maintain classification accuracy for clean images while being computationally efficient. The paper introduces a hybrid network that combines supervised DNN training with unsupervised SNN learning through STDP. This integration aims to enhance the DNN's robustness to input perturbations by extracting low-level features from spatial correlations while ignoring locally uncorrelated noise in pixels. The paper introduces a hybrid network architecture called SAFE-DNN, which combines supervised DNN training with unsupervised SNN learning through STDP. This integration aims to improve image classification under noisy input by extracting low-level features while preserving accuracy for clean images. The architecture includes a spiking convolutional module within a DNN pipeline and utilizes a novel frequency-dependent stochastic STDP learning rule for local competitive learning. The proposed methodology transforms the STDP-based spiking convolution into an equivalent CNN using a special neuron activation unit (SAU). This allows for supervised training in a single deep network after freezing the STDP-learnt weights. The SAFE-DNN architecture is implemented with different deep networks like MobileNet, ResNet, and DenseNet to demonstrate its versatility. Experimentation is conducted on CIFAR10 dataset. The SAFE-DNN architecture demonstrates robust classification under input noise without prior knowledge of perturbation, complementing de-noising networks with minimal computational overhead. It can be integrated into resource-constrained autonomous platforms for real-time processing. The SAFE-DNN architecture hybridizes STDP and SGD during learning to create a single hybrid network operating as a DNN during inference. It uses a spiking neural network with biologically plausible neuron and synapse models, such as the Leaky Integrate Fire (LIF) model, to capture the firing pattern of real biological neurons. In SNN, neurons are connected by synapses with conductance determining the strength of the connection. Learning occurs through spike-timing-dependent-plasticity (STDP) algorithm, involving long-term potentiation (LTP) and long-term depression (LTD) operations. LTP is triggered when post-synaptic neuron spikes closely after a pre-synaptic neuron spike, indicating causality between the events. The synapse undergoes LTD if there are spikes before the pre-synaptic spike arrives or without receiving a pre-synaptic spike at all. The weight update process in a DNN involves gradient descent to compute new weights. Parameters like \u03b1, \u03b2, G max, and G min are tuned based on network configurations. Weight optimization is described using cross entropy loss and gradient descent rate \u03b7. The gradient in equation (3) is derived from output prediction probabilities and ground truth, back-propagated through the network using the chain rule. This global learning approach improves higher level feature detection and classification accuracy but makes it challenging to enforce local constraints during training. The network must learn to consider local spatial correlation to improve robustness to input noise. This local learning will help ignore noisy pixels during low-level feature extraction and prevent noise propagation through the network. The motivation behind SAFE-DNN is to address this issue. The SAFE-DNN approach aims to incorporate local learning of features in a Spiking Neural Network (SNN) to prevent noise propagation through the network. The modulated conductance in SNN is determined by the spike timing difference and modulation probability function, enabling weight modulation based on input signals within the same receptive field. The SAFE-DNN approach utilizes Spike Timing-Dependent Plasticity (STDP) in a Spiking Neural Network (SNN) to learn spatial correlations between pixels in a local region, enabling robust feature extraction that is resistant to noise. The network architecture includes spiking convolution modules alongside conventional CNN layers to achieve this goal. The spiking convolution module is placed at the front for robust extraction of local features, while auxiliary CNN layers ensure global learning. The output feature maps are concatenated for input to the main CNN module responsible for higher-level feature detection and classification. This integration of global and local learning enhances the overall performance of the network. The SAFE-DNN network integrates global and local learning using a convolutional module. It drops the first convolution layer and one block from the original architecture, utilizing the remaining layers as the main CNN module. Three configurations based on MobileNetV2, ResNet101, and DenseNet121 are tested, showing minimal overhead compared to baseline networks. In the dynamical system of SNN, neurons transmit information through spikes, requiring input signal conversion to spike trains and multiple time steps for neuron response. The SNN model used in SAFE-DNN requires multiple time steps for neurons to respond to input stimulus, unlike conventional DNNs. To address this issue, the spiking convolution module is adapted to a single-time-step response system to avoid slowing down training and inference. The training process involves two stages: STDP-based learning in isolation for the spiking convolution module, followed by migrating network parameters to the SAFE-DNN module. The network parameters are migrated to the spiking convolution module of SAFE-DNN, where a conversion process takes place. Batch normalization is added after the convolution layer, and a special activation unit (SAU) replaces the basic spiking neuron model to preserve non-linear properties. The entire SAFE-DNN is then fully trained using statistical methods, while the weights in the spiking convolution module remain fixed. Network inference is performed using the SAU for modeling neurons instead of the baseline LIF. The SAU is used instead of the baseline LIF for modeling neurons in the second stage of training. A frequency-dependent stochastic STDP algorithm is proposed to address the associative potentiation issue in STDP by dynamically adjusting the probability of LTP/LTD based on input signal frequency. The algorithm for frequency-dependent stochastic STDP adjusts LTP/LTD probabilities based on input signal frequency. Parameters \u03c4 d and \u03c4 p, \u2206t (t post \u2212 t pre ), \u03b3 p, \u03b3 d, f max, f min, and f control the probabilities. LTP probability is higher with smaller \u2206t, while LTD probability is higher with larger \u2206t. The window for inducing LTP or LTD is narrower for weak input compared to strong input. The architecture of the spiking convolutional module is shown in Fig. 3, resembling conventional DNN but with differences. Input images are converted to spike trains with frequencies from f min to f max. Connections between spiking neurons follow STDP learning rule, with cross-depth inhibition in the convolution layer. FD stochastic STDP shows better learning capability than conventional STDP. The spiking convolutional module architecture includes cross-depth inhibition in the convolution layer to achieve competitive local learning behavior for robust low-level features. In a multiple-layer network, a layer-by-layer learning procedure is used due to the diminishing spiking frequency, adjusting neurons in the first layer to provide higher spiking frequency by lowering the spiking threshold V th. The effect of changing V th in a spiking neural network (SNN) is illustrated in Fig.4. Neurons in the first layer receive input from images, facilitating learning in subsequent layers through spike conversion. Spike frequency is determined by input value X and perturbation \u03be, with total spikes received calculated as N spike = F * T input. Spiking neuron responses show unique non-linearity, with perturbations within a certain range not causing extra spikes.\u03b4 value varies with input frequency. The Special Activation Unit (SAU) is designed to provide robustness to small input perturbations by being a step function. Three baseline networks are compared with SAFE-DNN, which is only tested with noisy images. Visualization shows improved local feature extraction with FD stochastic STDP in deep networks. Two SAFEMobileNetV2 models are trained with FD stochastic STDP and deterministic methods. The SAFEMobileNetV2 models, trained with FD stochastic STDP and deterministic STDP, are tested on noisy input with AWGN noise. The embedding space between fully connected layers is compared, showing better clustering and accuracy with FD stochastic STDP. The SAFE-DNN architecture is compared with standard MobileNetV2 and MobileNetV2-\u00b5, which replaces the spiking convolution module with regular DNN layers. MobileNetV2-\u03bb is constructed by replacing activation functions in the first three layers of MobileNetV2-\u00b5. The comparison between SAFE-MobileNetV2 and MobileNetV2-\u00b5 involves replacing activation functions in the first three layers without re-training. The networks are trained on CIFAR10 dataset and tested with clean and noisy images. SAFE-MobileNetV2 maintains good separation between feature mappings for each class even with noise up to 25 dB, unlike MobileNetV2-\u00b5. Table 2 displays the accuracy of different network variants on CIFAR-10. Noise in images significantly reduces classification accuracy for baseline DNNs. Training with noise (30dB) improves robustness, especially when inference noise matches training noise. Clean images suffer accuracy degradation. Average filtering boosts accuracy in high noise conditions (<20 dB SNR) but hampers performance with mild to no noise due to loss of feature details in CIFAR-10 images. In the CIFAR-10 dataset, average filtering leads to loss of feature details in images. However, SAFE-DNN with all three architectures shows improved performance in noisy conditions compared to the original network. For example, at 20 dB SNR, SAFEMobileNetV2 achieves good performance while the original network drops below 40% accuracy, resulting in a significant 50% gain. This trend is consistent across different noise levels, with SAFE-DNN performing similarly to networks trained with noise at around 30 dB SNR and outperforming them at higher noise levels. In clean images, SAFE-DNN performs on par with baseline networks. Additionally, in a test on a subset of ImageNet related to traffic, the subset containing 20 classes and 26,000 training images, the same baseline networks as in CIFAR-10 were used. In a test on a subset of ImageNet with 20 classes and 26,000 training images, baseline networks from CIFAR-10 were used. Noise training at 25 dB SNR improved robustness but affected clean image accuracy. DensNet121 showed more noise robustness than MobileNetV2 and ResNet101. SAFE-DNN implementations of all three networks exhibited better robustness over all noise levels without affecting clean image classification accuracy. Comparing top 5 accuracy results, SAFE-MobileNetV2 maintained above 80%. SAFE-MobileNetV2 outperforms baselines with over 80% accuracy even at 5 dB SNR. Testing with different noise structures (Wald, Poisson, and salt-and-pepper), noise-trained DNNs show better robustness than baseline and average filtering. Performance drops when noise levels are not aligned during inference. Noise-trained networks perform poorly when noise levels are mis-aligned. MobileNetV2 networks are tested on ImageNet subset with different noise distributions. SAFE-MobileNetV2 shows robustness to various noise structures without specific training. Adversarial perturbation testing is done using black-box method on DNNs trained conventionally. The study introduces SAFE-DNN, a deep learning architecture combining spiking convolutional networks with STDP based learning for robust feature extraction. Results show improved robustness to input perturbations, including adversarial attacks, but not white-box attacks. Integration with adversarial training methods is suggested for future work. SAFE-DNN enhances robustness to input perturbations without prior noise knowledge, compatible with various DNN designs, and suitable for real-time autonomous systems in noisy environments."
}