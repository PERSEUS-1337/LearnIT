{
    "title": "ryGcXQK8LS",
    "content": "In recent years, significant progress has been made in identifying computational principles underlying neural function. Evidence suggests that a synthesis of these ideas could lead to an understanding of how neural computation emerges from innate dynamics and plasticity, potentially leading to new AI technologies. The advantages of these principles for computation and their potential benefits for AI are discussed, highlighting the opportunity for novel AI formulations based on our current understanding of the brain. Sparse spike coding in real neurons is computationally beneficial and more powerful than real-valued counterparts. Self-organizing networks represent feedforward input structure using mechanisms like spike timing dependent plasticity (STDP) and homeostatic mechanisms. Feedback connections build predictive models by learning to invert feedforward representations. Oscillations and quasi-chaotic transitions continuously reconfigure neural circuits based on task needs. The curr_chunk discusses how various mechanisms such as inhibitory processes, spike conduction delays, and dopamine modulation play crucial roles in neural circuits for tasks and learning. Additionally, neuromodulators like acetylcholine, noradrenaline, and serotonin have significant effects on brain function. These principles are well-established in neuroscience. The curr_chunk discusses how brains create internal simulations of 'what if' scenarios and future plans through predictive models, oscillations, and dynamic circuit reconfiguration. It also mentions stochastic sampling, dopamine's role in model-based reinforcement learning, and the importance of prediction of reward and temporal-difference learning. The curr_chunk explains that high-level cognitive functions are essentially 'internal actions' preceded by neural operations. Subcortical circuits like the basal ganglia and cerebellum control well-trained movements. Computational principles apply across all cortex regions, with functional differentiation mainly through structural connectivity. The brain controls information flow, stores, recalls, organizes, and transforms information to achieve specific outcomes. It adapts flexibly to unforeseen conditions by activating neural assemblies in response to sensory input and internal state. Neurons and assemblies respond without centralized control, influenced by plasticity mechanisms that bias activity towards rewarding outcomes. The brain adapts to unforeseen conditions by activating neural assemblies in response to input and internal state. These assemblies perform computations shaped by self-organization and reward learning, maintaining activity within useful bounds. Neural connections support intricate activity patterns, coupling in novel ways to form creative and fluid intelligence-related patterns. The brain's neural assemblies adapt to unforeseen conditions by performing computations shaped by self-organization and reward learning. These patterns underlie the brain's combinatorial computational power, which is not based on a program but on efficient coding and stochastic sampling. Engineering-style reductionist simplifications do not provide insights into neural function, as the brain is a bootstrapped physical dynamical system where neurons respond to incoming patterns of spikes. The brain's dynamic complexity and emergent properties challenge us to understand how it controls the body and interacts with the world. New theories are needed to explain how plasticity shapes brain dynamics and facilitates connections between brain regions. This understanding can lead to better brain-inspired AI, which differs significantly from artificial neural networks in terms of training difficulty and data requirements. AI is challenging to train with large amounts of data, while the brain uses transfer learning efficiently with minimal samples. AI systems are energy-intensive, unlike the brain which operates on low power. AI requires pre-training and complete retraining for new information, whereas the brain learns continuously through transfer learning. AI is vulnerable to adversarial input, unlike the brain which generalizes well due to self-organization and predictive feedback. AI systems are limited to the tasks they are trained for, while the brain can perform multiple tasks and switch between them. The brain generates causal models using STDP, predictive feedback, and working memory. Current AI is primarily based on big data and correlation analysis, with exceptions that draw inspiration from the brain to improve deep learning capabilities. However, many advantages of neural processing are still untapped. SNNs are more powerful than ANNs, ideal for spatiotemporal patterns and causal models. They outperform DL in some aspects, requiring fewer training samples and generating efficient representations. DL requires large amounts of training data to cover the full input space explicitly, leading to high energy consumption. SNNs, on the other hand, use unlabelled data and generate efficient representations through parts-based decompositions and STDP. Their spike-based nature allows for powerful temporal representations and dynamic neural circuit reconfiguration. Neural dynamics in brains and SNNs allow for stochastic sampling and dynamic reconfiguration of circuits to match computational needs. Brains and SNNs are suited for embodied interactions with the real world, offering advantages over DL in handling complex real-world problems. The true computational power of the brain lies in the integration of neural computation principles. The integration of neural computation principles, including oscillations, spike-time coding, and reinforcement learning, has not been attempted at scale. Combining these principles with self-organizing plasticity could lead to a network that can generalize and respond flexibly to new inputs. This synergistic combination may provide a deeper understanding of neural dynamics and underlying mechanisms, with evidence-based and implementable principles in neural circuit models and AI prototypes. The curr_chunk discusses the limitations of current AI systems driven by Deep Learning and proposes that unsupervised learning, similar to how the brain operates, may be the way forward. It emphasizes the importance of the brain's flexibility and computational capacity rooted in the activity dynamics of its components. The potential for creating revolutionary computational systems with moderate resources and effort is highlighted. The curr_chunk emphasizes the brain's ability to seamlessly merge bottom-up discriminative and top-down generative computations in perceptual inference. It challenges the notion that brains process information, suggesting they create it instead. This has implications for experiments finding low-dimensional manifolds in population firing rates, indicating that working memory and stochastic sampling only create the appearance of a simple representation. The curr_chunk discusses how the brain uses stochastic sampling of interpretations from afferent evidence to accumulate information in complex spatiotemporal patterns, rather than relying solely on firing rates. This process allows for the narrowing of probability distributions and constraints on sampling, leading to an increase in firing rates. The network activity representing firing rates actually stores a significant amount of information about context, stimuli, responses, and brain state. By studying the dynamics and self-organization of functional networks in the brain, we gain insight into its true nature as the embodiment of the mind. The brain's structural overlaps and cross-connections between hierarchies create complex patterns of competition and coupling. Neural computation involves a continuous superposition of transient dynamic states mediated by competition, coupling, self-organization, and reward within and between neural assemblies. Neural assemblies are activated by recognized input, influencing other neurons' activities. Once the input disappears or brain state changes, the assembly shuts down until receiving keying input again. This mechanism ensures efficient brain processing by quickly responding to good matches and slowing down for poor matches. STDP, RL, and homeostatic mechanisms play a role in this process. Neural assemblies are activated by recognized input, influencing other neurons' activities. STDP, RL, and homeostatic mechanisms cause the recruitment of more neurons to represent common inputs and well-trained tasks. This process increases the fidelity and discriminability of representations and enhances processing speed. This approach combines relevant biophysical principles in a 'sideways-in' manner. The brain's complex functional properties emerge from interactions at different levels, with neurons, synapses, and spikes being the primary modelling level. Ion channels, neurotransmitter release, and membrane dynamics are abstracted as mathematical functions. Networks and population dynamics exhibit emergent properties, with spike-time coding, self-organization, and reward learning playing key roles. Research topics such as coding, short term plasticity, reward learning, and oscillations have been extensively investigated. Understanding how these principles synergistically combine is crucial for a rich understanding of neural function. Next-generation AI can benefit from brain-inspired neural processing by focusing on SNN mechanisms. Extensive progress in this direction can be expected with similar attention given to SNNs as has been given to ANNs in the last 10 years."
}