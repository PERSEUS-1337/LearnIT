{
    "title": "r1lqA-OSvV",
    "content": "SOSELETO (SOurce SELEction for Target Optimization) is a new method that uses a source dataset to solve a classification problem on a target dataset by assigning weights to source samples. This approach allows the target to select the most informative source samples for its classification task, acting as a regularization to mitigate overfitting. SOSELETO can be applied to transfer learning and training on datasets with noisy labels, showing state-of-the-art results. Deep learning has shown success with large training sets, but struggles with data-poor problems. Transfer learning is a common solution, but SOSELETO proposes a new approach by assigning weights to source examples for target optimization. This method aims to select the most informative samples for classification tasks, acting as a regularization technique to prevent overfitting. The SOSELETO approach proposes optimizing a shared source/target representation through bilevel optimization. The source minimizes its classification loss at the interior level, while the target minimizes its loss with respect to source sample weights and its own classification layer at the exterior level. This method allows the target to select informative source samples for its classification task, acting as regularization to prevent overfitting. The optimization process involves training the shared representation, source and target classifiers, and source weights simultaneously. Transfer learning techniques involve feature extraction and fine-tuning. Domain adaptation focuses on knowledge transfer between source and target classes. Adversarial methods are used to improve domain adaptation performance. Partial transfer learning deals with partial overlap between classes. SOSELETO proposes optimizing a shared representation through bilevel optimization. In domain adaptation, BID7 proposes selecting a portion of the source dataset before training, while BID9 and Zhang et al. focus on unsupervised approaches. Our method is completely supervised and addresses classification with noisy labels. In the realm of deep learning, learning with label noise can lead to high accuracy with large data. Various methods have been proposed to address noisy labels, including introducing noise layers in CNNs, predicting clean labels and noise types simultaneously, and combining instance reweighting techniques. The problem of learning with label noise is addressed by utilizing a data-rich source set to classify a data-poor target set. Not all source examples contribute equally useful information for the target problem, and the goal is to let the algorithm determine the relevance of source images without preconceived notions. The algorithm chooses relevant source images for the target classification task by assigning weights to each source example. This approach addresses transfer learning and learning with label noise, allowing the algorithm to determine the relevance of source images without preconceived notions. The algorithm assigns weights to source examples to address transfer learning and learning with label noise. It combines source and target losses into a single optimization problem by creating a weighted sum, but issues may arise with the weight choice. The proposed approach uses bilevel optimization to address transfer learning and learning with label noise. By optimizing source features and classifier based on weights \u03b1, and minimizing target loss through source weights, overfitting is mitigated for small target sets. Stochastic optimization is adopted for solving the bilevel problem efficiently. The proposed approach utilizes bilevel optimization for transfer learning and learning with label noise. The iterative process involves taking gradient steps in the interior level problem and updating source example weights based on alignment with the target aggregated gradient. Weight values are clipped to be within the range [0, 1]. The proposed approach involves bilevel optimization for transfer learning and learning with label noise. Weight values are clipped to be within the range [0, 1]. SOSELETO algorithm is illustrated on a synthetic example with 20% of points having wrong labels. Clean points are labeled based on a threshold of 0.1 on the weights \u03b1. The SOSELETO algorithm correctly identifies clean and noisy points based on a threshold. It separates clean and noisy instances well, with mislabeled points near the separator. The algorithm's performance is robust to different thresholds. In a real-world scenario with CIFAR-10, 10K clean examples are set aside for pre-training. Our method, SOSELETO, outperforms previous methods on noisy CIFAR-10 datasets. We also demonstrate superior performance in transfer learning from SVHN 0-4 to MNIST 5-9, even with a small target dataset. LeCun et al. compared their results with various techniques, including target only, standard fine-tuning, Matching Nets, and Label Efficient Learning. They used the LeNet architecture and achieved superior performance on MNIST's test set with their SOSELETO method, outperforming all techniques except Label Efficient. SOSELETO is a technique for transferring SVHN 0-9 to MNIST 0-4, achieving over 92% accuracy with label noise. It involves joint training through bilevel optimization, optimizing the source loss with respect to network parameters and the target loss with respect to weights and its own classifier. The algorithm is effective for learning with label noise and transfer learning problems. SOSELETO is architecture-agnostic and can be extended beyond classification tasks. The algorithm involves alternating interior and exterior descent operations for source and target classifiers on mini-batch basis. Time-complexity requires both source and target batches for each iteration. Target derivatives are evaluated over a target mini-batch. SOSELETO requires both a source batch and a target batch for each iteration, doubling the time compared to the ordinary source classification problem. In terms of space-complexity, in addition to network parameters, source weights \u03b1 need to be stored. The relative space increase depends on the source dataset size and number of network parameters, typically around 3%. To ensure \u03b1 j \u2208 [0, 1], a new variable \u03b2 j \u2208 R and a piecewise linear sigmoid function \u03c3(\u00b7) can be used. Update Equation (4) can be replaced accordingly. In SOSELETO, a piecewise linear sigmoid function is used to update the source weights \u03b1 to ensure they are within [0, 1]. The Jacobian \u2202\u03b1/\u2202\u03b2 is defined to replace Equation (4) with an update equation for \u03b2. The Jacobian is easily computed analytically, ensuring \u03b2 values stay within [0, 1]. SOWETO is an approximation for a bilevel optimization problem, with conditions for convergence to a local minimum of the target loss L t demonstrated. The change in target loss from iteration m to m + 1 is examined to show convergence. In SOSELETO, a piecewise linear sigmoid function is used to update the source weights \u03b1 to ensure they are within [0, 1]. The change in the target loss from iteration m to m + 1 is examined by analyzing the evolution of the weights \u03b1. By setting the learning rate \u03bb \u03b1 sufficiently large, the target loss can be decreased. Ignoring certain terms and setting v = Q T \u2202Lt \u2202\u03b8, a condition for convergence to a local minimum of the target loss L t is demonstrated. In SOSELETO, a piecewise linear sigmoid function is used to update the source weights \u03b1 to ensure they are within [0, 1]. A standard bound on the L1 norm of a vector is used to analyze the evolution of the weights \u03b1 and decrease the target loss. By examining the \u03b1-values chosen on convergence, the effective noise level can be adjusted by thresholding the training samples. This analysis is visualized in FIG3 with three plots showing the relationship between the number of samples and the noise level. SOSELETO effectively filters out incorrect labels at different noise levels, with about 4%, 10%, and 20% incorrect labels for 30%, 40%, and 50% noise levels respectively. The large slopes of the curves on the graph indicate SOSELETO's capability to prune unhelpful instances at train time. Additionally, SOSELETO improves classification by utilizing different digits from a different dataset (SVHN 0-4) for MNIST 5-9. An experiment was conducted using SVHN 0-9 to MNIST 5-9, with a partial overlap in classes between the datasets. Increasing the source set led to a significant accuracy improvement to 90.3%. Measuring the percentage of \"good\" instances didn't show a strong correlation with labels. Labels 7-9 had slightly higher weights, but there was no strong evidence that labels 5-9 were more useful than 0-4. The usefulness of an instance determined by SOSELETO was more tied to its appearance, such as whether the digit is centered. In the experiment using SVHN 0-9 to MNIST 5-9, increasing the source set improved accuracy to 90.3%. The usefulness of an instance, determined by SOSELETO, is tied to appearance factors like centering, size, blur, and rotation. Some instances were wrongly labeled, with 3-5% errors found in \"bad\" instances. Instances labeled as \"0\" in SVHN dataset were especially problematic, with clear errors visible. Highly weighted instances had fewer errors."
}