{
    "title": "BJxI5gHKDr",
    "content": "In this work, the focus is on in-domain uncertainty for image classification and exploring standards for quantification. A broad study of different ensembling techniques is conducted, introducing the deep ensemble equivalent (DEE) to show equivalence to few independently trained networks. Deep neural networks (DNNs) are popular for classification, with predictive performance often measured in accuracy. Deep neural networks (DNNs) for classification are often evaluated based on accuracy, but they can produce inaccurate probability estimates. This has led to a focus on uncertainty estimation with DNNs, including out-of-domain uncertainty measured on data not from the training distribution. Resistance to data corruptions and higher uncertainty on out-of-domain data were explored in a recent study. Ensembles of deep neural networks are used for uncertainty estimation and improving the quality of deep learning models. Two main directions in training ensembles of DNNs involve stochastic computation graphs and obtaining separate snapshots of neural network weights. Methods based on stochastic computation graphs introduce noise over weights or activations of deep learning models. Methods like data augmentation, dropout, variational inference, batch normalization, Laplace approximation, deep ensembles, snapshot ensembles, and fast geometric ensembles introduce noise or obtain separate snapshots of neural network weights to improve deep learning models. In this paper, the focus is on assessing the quality of in-domain uncertainty estimation in deep learning models. The study evaluates modern DNN ensembles on CIFAR-10/100 and ImageNet datasets, introducing the deep ensemble equivalent score for interpretability. The research highlights the limitations of common metrics in the field and addresses pitfalls in uncertainty estimation methods. The study evaluates the performance of different methods for ensembling deep neural networks on image classification tasks. Methods that explore different optima of the loss function show promising results, while those that only focus on a single optimum fall behind. The research also mentions other relevant settings where probabilistic estimates are crucial, such as regression, image segmentation, language modeling, active learning, and reinforcement learning. In-domain uncertainty is the focus, excluding out-of-domain uncertainty. Methods trained on clean data with simple data augmentation are considered, avoiding the use of out-of-domain data or complex augmentation techniques like mixup and adversarial training. Conventional training procedures using stochastic gradient descent and batch normalization are employed. The text discusses the use of standard optimization techniques in deep learning, avoiding more advanced methods like superconvergence and stochastic weight averaging. Various works focus on approximating and accelerating prediction in ensembles, including distillation, fast dropout, and deterministic variational inference. The emphasis is on the raw power of these techniques rather than comparing training times of different ensembling methods. The text discusses the predictive distribution of stochastic computation graphs and the importance of ensembling techniques. Different metrics are used to measure the quality of uncertainty estimation in classification problems. The text discusses the predictive distribution of deep neural networks, focusing on the softmax function and temperature parameter. It introduces the concept of confidence in classifiers and the average test log-likelihood as a metric for measuring uncertainty in deep learning models. Temperature scaling, also known as calibration, is a method to adjust the temperature parameter in deep neural networks to improve the log-likelihood. It can help find an optimal temperature for the test data, leading to better performance compared to using the default temperature of 1. Ensembling techniques can also benefit from temperature scaling, as the choice of temperature can impact the overall ordering of methods and the best ensembling method. Temperature scaling, or calibration, adjusts the temperature parameter in deep neural networks to improve log-likelihood. It is particularly impactful on experiments with data augmentation on ImageNet. The log-likelihood at the optimal temperature is called the calibrated log-likelihood, which can be estimated without a validation set. Log-likelihood shows a strong correlation with accuracy, indicating that accuracy influences uncertainty measurement. A model with higher accuracy tends to have a higher log-likelihood, even if uncertainty quality is lower. The Brier score, similar to log-likelihood, penalizes incorrect predictions based on assigned probabilities. It is sensitive to softmax distribution temperature and correlates empirically with log-likelihood. Misclassification detection is often assessed using AUC-ROC and AUC-PR metrics in binary classification problems. The papers discuss using uncertainty criteria like confidence or predictive entropy for prediction scores. However, these metrics cannot directly compare misclassification performance across different models due to each model inducing its own binary classification problem. AUCs for misclassification detection cannot be directly compared. Accuracy-confidence curves are used to measure the performance of misclassification detection, but they heavily rely on calibration and confidence values. Comparing AUCs for misclassification detection between different models is not valid, as each model creates its own binary classification problem. This comparison is only appropriate for out-of-domain data detection problems where objects and targets remain fixed for all models. However, this condition breaks down in the detection of adversarial attacks, as different models have different inputs after an attack. Accuracy-confidence curves heavily rely on calibration and confidence values, making comparisons between models challenging. To address this, one can use accuracy-rejection curves, which are less sensitive to temperature scaling and allow for a more meaningful comparison of rejection ability. The area under the accuracy-rejection curve (AU-ARC) can be computed as a scalar metric for easy comparisons. A probabilistic classifier is considered calibrated if predicted class probabilities match true class probabilities according to the data distribution. Expected Calibration Error (ECE) is a metric that estimates model miscalibration by binning probability scores and comparing them to average accuracies within these bins. Recent research highlights issues with ECE, including bias, inability to be directly optimized, and limited estimation of miscalibration. Thresholded Adaptive Calibration Error (TACE) was proposed as a solution to issues with ECE, such as bias and limited estimation of miscalibration. TACE disregards low predicted probabilities, adapts bin locations for equal object distribution, and estimates miscalibration across all classes. Thresholded Adaptive Calibration Error (TACE) addresses issues with ECE but is not reliable for comparing different models due to bias. It is sensitive to the number of bins and threshold parameters. Two common ways to perform temperature scaling using a validation set are dividing the public training set or splitting the public test set. In order to reduce variance in estimating metrics like log-likelihood and Brier score, a \"test-time cross-validation\" method is used by dividing the test set into two parts and computing metrics with optimized temperature. Ensembling techniques considered include deep ensembles, snapshot ensembles, fast geometric ensembling, SWA-Gaussian, and cyclical. Various techniques such as FGE, SWAG, cSGLD, VI, dropout, and test-time data augmentation were chosen to cover a diverse set of approaches for improving predictive performance. These techniques involve distributions over parameters of computation graphs, allowing for averaging predictions across parameters to approximate the predictive distribution. For example, a deep ensemble can be represented as a mixture of independently trained snapshots, while a Bayesian neural network utilizes a fully-factorized Gaussian approximate posterior distribution over weight matrices. The fully-factorized Gaussian approximate posterior distribution over weight matrices and convolutional kernels \u03c9 is represented as q VI (\u03c9) = N (\u03c9 | \u00b5, diag(\u03c3 2 )), with data augmentation parameterized by random crop coordinates and image flipping. Data augmentation is studied in combination with ensembling techniques, not as a separate method. The approximation requires K independent forward passes through a neural network, making the test-time budget comparable across methods. Deep ensembles provide independent samples from different modes of the loss landscape, making it a strong baseline for ensembling techniques. The calibrated log-likelihood (CLL) is used as the main measure of uncertainty estimation performance for the ensemble. The Deep Ensemble Equivalent (DEE) is defined for ensembling methods using CLL as the main measure of uncertainty estimation performance. PyTorch is used for implementation, closely matching the quality of reported methods. SSE and cSGLD outperform other techniques, showing near-linear scaling of DEE with the number of samples. Weight-space trajectories of cSGLD and SSE suggest their effectiveness. In experiments, SSE typically outperforms cSGLD due to a larger training budget. SSE collects one snapshot per cycle while cSGLD collects three, making samples from SSE less correlated. Both methods can be adjusted for different trade-offs between training budget and DEE-to-samples ratio. FGE and SWAG, being more \"local\" methods, perform worse than SSE. In comparison to SSE and cSGLD, FGE and SWAG, as \"local\" methods, perform worse but still outperform \"single-snapshot\" methods like dropout, K-FAC Laplace approximation, and variational inference. FGE and SWAG cover a single mode with snapshots, providing a better fit for local geometry. Data augmentation is crucial for training modern DNNs. Test-time data augmentation is important for training modern DNNs and has been used to enhance the performance of convolutional networks. While it is commonly used for tasks like ImageNet challenge, its impact on ensembling techniques in deep learning is less explored. The study shows that test-time data augmentation can improve accuracy but decrease the log-likelihood of deep ensembles on CIFAR-10. Temperature scaling is required to reveal the true performance of the method. Our experiments demonstrate that ensembles may be miscalibrated by default but still provide superior predictive performance after calibration. Temperature scaling is essential for improving the predictive uncertainty of ensembling methods, highlighting its importance. Comparison of log-likelihoods of different ensembling methods without temperature scaling may not provide a fair ranking. Many common metrics for measuring uncertainty are unreliable or unsuitable for comparing methods. Popular ensembling techniques require many samples for averaging but are similar to independently trained models. Deep ensembles outperform other methods by exploring different modes in the loss landscape, crucial for predictive performance. Methods stuck in a single mode cannot compete with those exploring various modes. Test-time data augmentation is a strong baseline for in-domain uncertainty estimation and can improve methods without increasing training time or model size. Deep ensembles with few members outperform methods based on stochastic computation graphs, highlighting the importance of exploring different modes in the loss landscape for predictive performance. The presence of unreliable metrics hinders fair comparisons among different uncertainty estimation methods, emphasizing the need for more reliable benchmarks in various setups. Reliable benchmarks are crucial for uncertainty estimation in neural networks. Conventional models use cross-entropy loss with weight decay regularization during training. The optimization problem involves maximizing the likelihood function and incorporating L2 regularization as a Gaussian prior distribution. The objective is to perform maximum a posteriori inference in a probabilistic model. In a probabilistic model, a SoftMax-based likelihood is used with a fully-factorized zero-mean Gaussian prior distribution. The model is applied to all ensembling techniques with fixed weight decay parameters for each architecture. Networks on CIFAR-10/100 datasets are trained using SGD optimizer with specific hyperparameters. The models used specific hyperparameters and a unified learning rate scheduler for optimization epochs. Data augmentation techniques were applied, and the number of optimization epochs was increased to address underfitting issues. Dropout layers were adjusted to improve training efficiency. In experiments, ResNet50 models were trained on ImageNet dataset with specific hyperparameters and data augmentation techniques. The training process included increasing the number of epochs to 130, resulting in a top-1 error rate of 23.81 \u00b1 0.15. Training one model on a single NVIDIA Tesla V100 GPU took approximately 5.5 days. Deep ensembles were also utilized to average predictions across independently trained networks. Deep Ensemble involves training multiple networks with the same hyperparameters and applying dropout techniques to improve predictions on CIFAR and ImageNet datasets. The use of dropout layers in VGG and WideResNet networks was consistent with the original papers, with dropout rates of 0.5 and 0.3 respectively. Training with dropout required 400 epochs instead of 300 for deterministic cases. Variational Inference approximates the posterior distribution with a tractable variational approximation by maximizing the variational lower bound. In variational inference, fully-factorized Gaussian approximation is used for the probabilistic model, which remains consistent with conventional training. Variational inference is applied to both convolutional and fully-connected layers, with weights' variances parameterized by log \u03c3. To prevent underfitting, pre-training is utilized by initializing weights with a snapshot of a pretrained model. The weights of pretrained conventional models are used as a snapshot, and log \u03c3 is initialized with a model-specific constant. The KL-divergence is scaled based on a model-specific parameter \u03b2, with weight decay implemented as part of the optimizer. Different \u03b2 values were used for different networks on CIFAR-10 and CIFAR-100 datasets. The log-variance initialization was set to -5 for all models, and parameters \u00b5 were optimized using conventional SGD. A separate Adam optimizer was used to optimize the log-variances of the weights. Training was conducted for 100 epochs, equivalent to 400 epochs of training. The Laplace approximation uses the curvature information of the loss function to construct a Gaussian approximation to the posterior distribution. The Fisher Information Matrix is used as an approximation to the true Hessian for scalability. K-FAC Laplace uses the whole dataset to construct an approximation to the empirical Fisher Information Matrix and applies a \u03c0 correction to reduce bias. The optimal noise scale for K-FAC Laplace is determined using a held-out validation set and is different when using test-time data augmentation. Snapshot Ensembles collect samples from a training trajectory to construct an ensemble. Cyclical Stochastic Gradient Langevin Dynamics (cSGLD) is a state-of-the-art ensembling method for deep neural networks, similar to Snapshot Ensembles. It uses SGD with a cosine learning schedule and parameters specific to different datasets like CIFAR-10/100 and ResNet50 on ImageNet. Monte Carlo family method similar to SSE, uses SGD with learning rate schedule, introduces gradient noise and captures snapshots per cycle for efficient sampling from posterior distribution over neural network weights. Parameters include cycle length of 50 epochs, max learning rate of 0.5, batch size of 64, and 3 epochs with gradient noise per cycle. Outperforms original paper's choices for CIFAR-10 and CIFAR-100. Cyclical SGHMC shows slightly better performance than cyclical SGLD but couldn't be reproduced with various SGD momentum values. FGE Fast Geometric Ensembling (FGE) is an ensembling method similar to SSE, collecting samples from a training trajectory to construct an ensemble. Optimal noise scale for K-FAC Laplace varies for different datasets and architectures. Model pretraining is done with SGD for 160 epochs, followed by a desired number of FGE cycles with one snapshot per cycle collected. FGE involves a desired number of cycles with snapshots collected, changing learning rates and cycle lengths for different networks. SWAG is an ensembling method fitting a Gaussian distribution to model weights and sampling from it. Training includes a pretraining stage with varying learning rates and continues with a constant rate for most models. The empirical covariance matrix rank for estimating Gaussian distribution parameters is set to 20. SSE outperforms deep ensembles on CIFAR-10 with the WideResNet architecture, suggesting a more suitable learning rate schedule. Changes to the learning rate schedule for WideResNets are planned for future revisions. ResNet110 and ResNet164 performance metrics are provided. Results before and after data augmentation on CIFAR10 and ImageNet datasets are shown in Table 7 and Table 9, respectively. ResNet164, VGG16, and WideResNet models were evaluated, with varying error percentages reported for each model."
}