{
    "title": "H1g0piA9tQ",
    "content": "Current machine learning algorithms can be easily fooled by adversarial examples. One solution is to use models that employ confidence thresholding to avoid errors by not making predictions when unsure. The MaxConfidence family of attacks is proposed to evaluate such models, showing good results in practice, especially against linear models. Simple defenses perform well on MNIST but not on CIFAR, suggesting MNIST should be retired as a benchmark dataset for adversarial attacks. The authors propose retiring MNIST as a benchmark dataset for adversarial robustness research and release code for evaluations in the cleverhans library. They caution ICLR reviewers to avoid de-anonymizing submissions. The success-fail curve illustrates the failure rate on adversarial examples and the success rate on clean examples using different confidence thresholds. Upper and lower bounds are provided for thresholds where attacks are not optimal. The MaxConfidence attack outperforms a traditional attack in terms of maximizing loss and failure rate in the presence of confidence thresholding. The staircase pattern in the success-fail curve is due to few examples lying near the middle, and linear connections between points may falsely imply feasibility of certain points. Adversarial training performs worse than the undefended baseline on semantic adversarial examples. Regularized models slightly outperform undefended baselines, showing the benefits of generic regularization combined with confidence thresholding. Adversarial examples are intentionally designed inputs to fool machine learning models, which are highly vulnerable to such attacks. Defending against adversarial examples often involves refusing to classify them. In this work, metrics and attack methods are presented for evaluating models that use confidence thresholding to refuse classification of some examples. Models based on confidence thresholding have the potential to enhance adversarial robustness by addressing limitations of adversarial training. Confidence thresholding is a promising approach to improve adversarial robustness in models. It offers a simple way to trade performance on clean data for resistance to potential attacks. This method could potentially reduce the computational cost of adversarial training and show feasibility on datasets like MNIST. Our proposed evaluation methods allow for a flexible tradeoff between performance on clean data and robustness to adversarial examples by adjusting a threshold. Adversarial training may lead to reduced accuracy on natural data, but models using confidence thresholding can achieve higher accuracy with reduced coverage. Many proposed defenses focus on detecting and refusing to classify adversarial examples, but some have been found to be ineffective. In the context of evaluating models for robustness to adversarial examples, confidence thresholding offers a different approach compared to detection methods. It allows classifiers to shut off for inputs that are not common in natural data, reducing the risk of overfitting to a specific adversarial example concept. However, evaluating models based on confidence thresholding remains a challenge. The suggestion is to analyze models using a tradeoff curve that balances success on natural data and failure on adversarial data. Models utilizing confidence thresholding need to be benchmarked accordingly. In the context of evaluating models for robustness to adversarial examples, confidence thresholding offers a different approach compared to detection methods. It allows classifiers to shut off for inputs that are not common in natural data, reducing the risk of overfitting to a specific adversarial example concept. A new family of attacks called MaxConfidence is proposed to benchmark models using confidence thresholding. Coverage and accuracy are metrics used to evaluate machine learning models, with a tradeoff between the two. Most machine learning papers evaluate accuracy at 100% coverage, but reducing coverage can increase accuracy in remaining examples. In evaluating models for robustness to adversarial examples, confidence thresholding allows classifiers to shut off for uncommon inputs, reducing overfitting risk. The MaxConfidence attack benchmarks models using this approach, optimizing tradeoffs between accuracy and coverage. The threshold for covering an example is chosen based on the probability of the most likely class exceeding a certain value. This method is optimal for classifiers that return a response only if they believe they are more likely to be correct than incorrect. In this work, a new evaluation methodology is proposed for adversarial situations, focusing on measuring the success rate on the natural test set and the failure rate on adversarial inputs. The goal is to assess how well a classifier can be used for its intended purposes and how effectively an adversary can control it. The success-failure curve shows the tradeoff between success and failure rates of a model at different confidence thresholds. Different applications may require prioritizing success or avoiding failure, and designers can use these curves to choose the appropriate threshold. The curve for Model A on the MNIST dataset is shown in FIG0, illustrating how success rate grows faster than failure rate as the confidence threshold decreases. The success-failure curve can be generated by sorting probabilities and plotting success and failure rates for different thresholds. Defenses based on incomplete coverage may not be suitable for applications requiring continuous performance, like autonomous delivery drones relying on machine learning systems for flight control. In applications where incomplete coverage is acceptable, such as adversarial attacks against speech recognition systems, it is not necessary for the system to always produce output. Different types of inputs beyond norm balls can be used to evaluate failure rates on adversarial data. The MaxConfidence attack is introduced to address shortcomings of previous attacks against confidence thresholding models. It is optimal against linear models and offers an optimality guarantee even in theoretical settings. The MaxConfidence attack is optimal against linear models and offers an optimality guarantee under certain assumptions. It is shown to cause the most crossentropy loss possible against logistic regression under an L \u221e constraint. However, this optimality guarantee does not extend to softmax regression. The attack set S(x) must be convex for the MaxConfidence attack to work effectively, solving a different convex optimization problem at each step. The MaxConfidence attack is optimal for k class linear models, providing a certificate of optimality at convergence. Previous untargeted attacks aimed to maximize the negative log probability assigned to the true class, but were not optimal. For k > 2, there can be multiple non-equivalent local maxima, affecting adversarial training performance. Various targeted attacks already exist for maximizing the model's output for a specific class, such as BID19, BID7, and BID12. The MaxConfidence attack is a targeted adversarial machine learning technique that aims to find the maximal failure rate by increasing the attacker's cost only with hits on a specific target class. It uses an optimization algorithm to run targeted attacks on wrong classes, choosing the one that results in the most confidence on a wrong class. When no optimal optimization algorithm is guaranteed, attack performance can be enhanced by attack bundling, using various algorithms or multiple calls to stochastic optimization algorithms. Care must be taken when implementing gradient-based optimizers for numerical stability. Examples of suitable optimization algorithms include those used in existing targeted attacks. The MaxConfidence attack requires careful implementation for numerical stability, utilizing log space and a stable log-softmax function. The attack aims to find adversarial inputs that are misclassified by a classifier based on confidence thresholding. If any attack restricted to a certain input space can find a misclassified example, the MaxConfidence attack will also succeed in finding one. The MaxConfidence attack aims to find misclassified examples by utilizing a confidence threshold. It achieves optimal failure rates when sampling from a distribution. The proof involves cases where the attack finds covered and misclassified examples, uncovers examples, or finds correctly classified examples. The MaxConfidence attack aims to find misclassified examples using a confidence threshold. It assumes x is covered and correctly classified, with p model (y = y* | x) > t. If there exists an example x in S(x) that is covered but incorrectly classified as belonging to a false class f = y*, a contradiction arises when t \u2265 1/2. The MaxConfidence attack may not be optimal when the confidence threshold is relaxed. A counterexample shows that the attack fails in cases where the model misclassifies inputs without concentrating probability on any single wrong class. Requiring the threshold to be greater than or equal to 1/2 forces the model to concentrate probability on a class for the attack to be effective. If the threshold is less than 1/2, the attack can still be used to bound the failure rate under optimal attack, but it is no longer guaranteed to be optimal. The optimal failure rate must be less than or equal to the coverage of the MaxConfidence attack. The MaxConfidence attack may not be optimal when the confidence threshold is relaxed. Case 3 presents an opportunity for a stronger attack to find misclassified examples. The failure rate is bounded by MaxConfidence's failure rate and coverage. In practice, attacks provide lower bounds on failure rate but not upper bounds due to imperfect optimization. Success-fail curves for CIFAR-10 models show different performance compared to MNIST. Regularized and adversarially trained models are somewhat comparable on semantic adversarial examples. The regularized and adversarially trained models intersect at (success=70%, failure=9%). Regularized model is better for success, while adversarially trained model is better for avoiding failure. The optimization algorithm may underestimate the true maximum confidence. The optimization algorithm may underestimate by approaching 1 2 from below as c(x) approaches 1 2 from above. Despite this, it still obtains the optimal failure rate. Most optimization algorithms are expected to suffer from extreme underestimation periodically, making our failure rate estimates lower bounds. Future verification methods may help create success-fail curves that upper bound the failure rate. On MNIST, three different models were studied on adversarial examples, including an adversarially trained model and two versions of \"Model A.\" Model A, a simple convolutional network, showed different success-fail curves when strongly regularized compared to the undefended baseline. Success-fail curves reveal hidden benefits of models with good confidence estimates. Regularized Model A performs reasonably well against MaxConfidence with 40 steps of gradient descent. Adversarial training is more effective but costs 40X more and is specialized for the threat model. Models are also evaluated on semantic adversarial examples. In the study, the goal is for the model to have low confidence on unusual negative images while maintaining high confidence on clean data. Adversarially trained models perform worse than undefended baseline on this task, highlighting a limitation of adversarial training. Confidence thresholding methods can achieve generically low confidence on inputs differing from the training distribution without needing to enumerate all contingencies. Results from experiments on CIFAR showed that confidence thresholding on a regularized model was not as effective against L \u221e adversarial examples compared to adversarially trained models. On MNIST, the regularized model outperformed the adversarially trained model in classifying semantic adversarial examples. However, on CIFAR-10, the adversarially trained model performed better at one end of the curve while the regularized model performed better at the other end. The adversarially trained model was found to be worse than a baseline using no defense other than confidence thresholding on semantic adversarial examples. The study compared the performance of a regularized model and an adversarially trained model on MNIST and CIFAR-10 datasets. Results showed that robustness properties on MNIST did not transfer well to CIFAR-10. The authors recommend evaluating on more than just MNIST and retiring it as a benchmark for adversarial examples. Adversarial training on one type of out-of-distribution data can worsen performance on other types, compared to a baseline using confidence thresholding as the only defense. The evaluation methodology introduced success-fail curves for different confidence thresholds and an optimal attack against linear classifiers. Confidence thresholding with simple regularization showed robustness to L \u221e attacks on MNIST. The study demonstrates that confidence thresholding can enhance robustness against various attacks without the need to specify each attack type. The evaluation methodology aims to aid in designing and testing cost-effective defenses against adversarial examples. Model A was fine-tuned to improve success-fail curves compared to the baseline. The study fine-tuned Model A through trial and error to improve success-fail curves compared to the baseline. The model architecture includes Conv2D layers with ReLU activation and was designed based on performance on clean and L \u221e adversarial examples, not semantic adversarial examples. The study fine-tuned Model A through trial and error to improve success-fail curves compared to the baseline. The model architecture includes Conv2D layers with ReLU activation and was designed based on performance on clean and L \u221e adversarial examples, not semantic adversarial examples. The convolutional network contains downsampling and residual layers, two convolutional layers, and a fully connected layer for outputting logits. Regularization methods were experimented with to improve confidence estimates in the adversarial setting, with traditional techniques found to be ineffective. In the study, Model A was fine-tuned through trial and error to enhance success-fail curves. The model architecture includes Conv2D layers with ReLU activation and was optimized for clean and L \u221e adversarial examples. Various regularization methods were tested to improve confidence estimates in the adversarial setting, with extreme label smoothing, variable label smoothing, and noisy logit pairing proving effective. Label smoothing with different values of \u03b4 was used in class 5 for a 10-class problem, showing improved success-fail curves. By applying larger values of \u03b4, the model achieved better results, especially when using different label smoothing for clean and noisy examples. This approach aimed to encourage the model to have local maxima of confidence. The approach used in the study involved applying label smoothing with different values of \u03b4 to encourage the model to have local maxima of confidence. Specifically, \u03b4 = 0.3 was used for clean examples and \u03b4 = 0.4 for noisy examples. Additionally, penalizing the mean squared error between the logits on clean and noisy examples was found to improve success-fail curves. This method, known as noisy logit pairing, encourages the model to learn smoothing functions and is equivalent to weight decay in logistic regression models. Noisy logit pairing in logistic regression is equivalent to weight decay, with a coefficient of 1 used for the logit pairing loss."
}