{
    "title": "ryGpEiAcFQ",
    "content": "A Synaptic Neural Network (SynaNN) is built with synapses and neurons, incorporating a nonlinear synapse function inspired by neuroscience research. The concept of surprisal space is introduced, showing the relationship between inhibitory probability functions. The derivative of the synapse in surprisal space corresponds to the negative Bose-Einstein distribution. A fully connected synapse graph is constructed as a synapse block in the network, enabling synapse learning with gradient descent and backpropagation algorithms. In a proof-of-concept experiment, an MLP model with synapse network hidden layers was trained and tested on MNIST data. Synapses, crucial in biological neural networks, are points where neurons connect and learn. A probability model for synapse behavior was proposed, incorporating excitatory and inhibitory probabilities. The concept of surprisal space was introduced, representing the synapse function as the sum of excitatory and inhibitory functions in this space. The inhibitory function in surprisal space was analyzed, showing it as the topologically conjugate function. The derivative of this function was linked to the negative Bose-Einstein distribution. A fully connected synapse graph was constructed, with a gradient formula derived for gradient descent learning. Weight updating in learning involved adding the negative Bose-Einstein distribution value. Finally, a program implementing Multiple Layer Perceptrons (MLP) for MNIST was designed and tested. The program implemented a Multiple Layer Perceptrons (MLP) for MNIST and achieved near equal accuracy to standard MLP. Neuron scientists have explored complex synapse models with rich properties, including biological synapse and computing synaptic conductances. Various synapse models based on differential equations have been proposed and simulated using Spiking Neural Networks (SNN). Synapses in these models act as linear amplifiers with adjustable coefficients. The paper proposed a simple synapse model for a neural network inspired by neuroscience to solve optimization problems. The model allowed for learning through gradient descent and backpropagation algorithms. The authors provided a detailed proof of the formula of gradients in the appendix. The Bose-Einstein distribution was derived from Bose statistics in statistical physics. A logarithmic matrix was used to represent a fully-connected synapse network. The concept of surprisal from Information Theory was introduced, defining surprisal as -log(p) in surprisal space. In surprisal space, a synapse function was constructed by adding an excitatory identity function and an inhibitory function. The inhibitory function was determined to be \u2212log(1 \u2212 e \u2212x ), and the structure and classification were of interest. A commutative diagram for the synapse inhibitory function was constructed, following the rediscovered procedure. This method did not require knowledge of category theory, but focused on mapping two spaces with corresponding points using continuous and inverse mapping functions. The synapse inhibitory function \u2212log(1 \u2212 e \u2212x ) was found to be a topologically conjugate function in the target space. This function allows the synaptic neural network to exhibit the same dynamical behavior in both probability space and surprisal space. The gradients of the loss function were proven using basic calculus, demonstrating that the synaptic neural network can effectively utilize the backpropagation algorithm. Additionally, the negative Bose-Einstein distribution was applied in surprisal space for parameter updating. The text discusses the application of the negative Bose-Einstein distribution in updating parameters for synaptic neural networks. It introduces a neuroscience-inspired synapse model and function, defines surprisal space, and explores the inhibitory part of a synapse. The text also mentions the representation of fully-connected synapses as a synapse tensor and expresses synapse learning through gradient descent as a negative Bose-Einstein distribution. The synapse consists of inputs from excitatory and inhibitory channels, an output channel, and forms a graph to connect neurons. Changes in neurons and synaptic membranes explain interactions between neurons and synapses. Chemical tokens affect the control channel of the chemical gate through a random process. A probabilistic model is suitable for the computational model of the biological synapse. The Na+ and K+ channels in a synapse control excitatory and inhibitory effects, respectively. Neurons may have multiple types of channels on their membrane, but experiments show only two types in a synapse. Neuronal firing generates spiking pulses, with frequency reflecting stimulation strength. The strength of stimulation in a synapse is reflected by the rate (frequency) of neuronal firing. Different types of chemical channels in the synapse membrane control excitatory and inhibitory effects. The synapse model includes properties such as open probabilities of excitatory and inhibitory channels, which affect the activation of connected output neurons. The activation of the connected output neuron is determined by the probabilities of excitatory and inhibitory channels, controlled by parameters \u03b1 and \u03b2. The joint probability distribution function S(x, y; \u03b1, \u03b2) defines the probabilities of activation. Surprisal measures the surprise when a random variable is sampled in bits, nats, or hartleys. Surprisal is a measure of surprise in bits, nats, or hartleys when a random variable is sampled. It is a fundamental concept in information theory and can be represented as the negative logarithmic probability of the occurrence of a value x. The Surprisal Function is defined as I(x) = -log(x) where x is in the open interval (0,1) of real numbers. The Surprisal Function I(x) is a bijective mapping from the open interval (0,1) of real numbers to the real open interval (0, \u221e). It is a homeomorphism and has an inverse function I^(-1)(u) = e^(-u). The Surprisal Space S is the mapping space of the Probability Space P with the negative logarithmic function. The Surprisal Synapse LS(u, v; \u03b8, \u03b3) is defined as -log(S(x, y; \u03b1, \u03b2)). The Surprisal Synapse LS(u, v; \u03b8, \u03b3) is defined as (\u2212log(\u03b1x)) + (\u2212log(1 \u2212 \u03b2y)). Theorem 1 discusses topologically conjugate functions and the proof involves building a commutative diagram with a homeomorphism. The topologically conjugated function G of function F is defined when I \u2022 F = G \u2022 I, with G(u) = \u2212log(1 \u2212 e \u2212u) being the conjugate function of 1 \u2212 x. They share dynamics, fixed points, and I(x) = \u2212log(x) is continuous in (0,1). The surprisal synapse involves the addition of excitatory and inhibitory functions in surprisal space. The commutative diagram reveals the structure and topological conjugacy of the function \u2212log(1\u2212e \u2212u), connecting synaptic neural networks to category theory, topology, and dynamical systems. The connection between surprisal synapse and the topologically conjugate function is explored, along with a link to the Bose-Einstein distribution. The BED function is defined as BED(v; \u03b3) = 1 / (e^(\u03b3+v) \u2212 1). The BED function BED(v; \u03b3) = 1 / (e^(\u03b3+v) \u2212 1) is defined with parameters v \u2208 S, \u03b3 \u2208 S, and v + \u03b3 \u2265 ln(2). It represents the probability of boson particles remaining in energy level v with initial value \u03b3. Synapses in a biological neuron connect dendrites to axons, forming a synapse graph on dendritic trees. The gradient of the surprisal synapse LS(u, v; \u03b8, \u03b3) connects statistical physics to neural networks. The synapse graph on dendritic trees of a neuron consists of synapses connecting inputs and outputs. Each synapse has excitatory and inhibitory inputs. In a fully-connected synapse graph, outputs are linked in a chain, with parameters defining the connections. The synapse graph on dendritic trees of a neuron consists of synapses connecting inputs and outputs. Each synapse has excitatory and inhibitory inputs. In a fully-connected synapse graph, outputs are linked in a chain, with parameters defining the connections. The synapse tensor formula Eq.9 relates distribution vectors and parameter matrices in the context of fully-connected synapse graphs. The text discusses the application of matrix multiplication and logarithm in the context of synapse graphs on dendritic trees. It also mentions the use of cross-entropy as a loss function and gradient descent for parameter optimization in neural networks. The fully-connected synapse graph outputs through a softmax activation function. The gradient over parameters is computed in the surprisal space. The loss function in surprisal space is calculated using the target and output vectors. The log function is removed in the surprisal space, and without an activation function, there is a direct relationship between the variables. Error back-propagation can be used for gradient descent in synapse learning. The learning of synaptic neural networks follows Bose-Einstein statistics in the surprisal space, as shown in the paper \"Memory as an equilibrium Bose gas\". A Synaptic Neural Network implementation called SynaMLP with Multiple Layer Perceptrons (MLP) is illustrated, with input, hidden, and output layers for down-sampling and classification. The SynaMLP is an implementation of a synapse tensor in neural networks, serving as a replacement for weight layers. It utilizes activation functions and is trained using the MNIST dataset with Python, Keras, and Tensorflow. The synapse tensor is designed as a class for this purpose. The SynaNN MLP, a replacement for weight layers, achieved a test accuracy of around 98% similar to traditional MLP. A softmax activation function was applied to avoid NAN errors. The Synaptic Neural Network (SynaNN) handles a probability distribution. The structure and construction of synapse network, as well as BE distribution in gradient descent learning, were analyzed. The surprisal space involves the addition of identity and topologically conjugate functions of inhibitory synapses. The surprisal synapse function is defined as LS(u, v; \u03b8, \u03b3) = (\u03b8 + u) + (I \u2022... The surprisal synapse function LS(u, v; \u03b8, \u03b3) = (\u03b8 + u) + (I \u2022 F \u2022 I \u22121 )(\u03b3 + v) can be implemented using physical or chemical components in non-linear synaptic neural networks. Various synapse functions are explored in neural network research and applications."
}