{
    "title": "H1g6osRcFQ",
    "content": "Computer simulation is used for training robotic control policies for complex tasks like locomotion, but transferring these policies to real hardware is challenging due to differences in environments. Domain randomization is a common approach for transfer learning, but it relies heavily on accurate system identification. This paper introduces a new method that uses domain randomization to transfer control policies to unknown environments by learning a family of policies with different behaviors in simulation. The best policy is then selected based on task performance in the target environment, eliminating the need for system identification. The method introduced in the paper uses domain randomization to transfer control policies to unknown environments by learning a family of policies with different behaviors in simulation. The best policy is selected based on task performance in the target environment, eliminating the need for system identification. Recent developments in Deep Reinforcement Learning (DRL) have shown potential to learn complex robotic controllers automatically with minimal human intervention. Transfer learning from a simulated environment to the real world is a promising approach to address the high sample complexity of DRL algorithms for agile tasks like locomotion. Learning in a simulated environment allows for safe exploration of various situations, although there may be model discrepancies between the simulation and real-world environments. Efforts have been made to bridge the Reality Gap, the model discrepancy between physics simulation and the real world, to improve the performance of trained policies in target environments. Researchers aim to develop more accurate computer simulations and train policies that can succeed in various simulated environments with different dynamics. This involves training robust or adaptive policies in environments with randomized dynamics to enhance their capabilities. A robust policy trained under varied dynamics may perform well if the simulation approximates real-world dynamics. However, adaptive policies excel by identifying and adjusting to different dynamics, leading to higher performance across dynamic systems. Yet, when target dynamics differ significantly from training dynamics, sub-optimal results may occur due to inaccurate identification models. In this work, a new method is introduced that combines the versatility of an adaptive policy with the task performance in the target environment. The algorithm involves training a family of policies optimized for different dynamic parameters, allowing for direct policy optimization based on task performance. The algorithm involves training a family of policies optimized for different dynamic parameters, allowing for direct policy optimization based on task performance. Transfer of a policy learned in one simulator to another simulator is demonstrated, with differences in simulation results due to constraint solvers. Latency is added to mimic real-world scenarios, increasing the difficulty of the transfer. Our method can overcome actuator modeling errors and transfer policies to challenging problems. While DRL can learn control policies in simulation, few algorithms successfully transfer them to the real world. Researchers propose optimizing simulation models using real-world data, but fitting highly agile control problems remains a drawback. Training robust policies with domain randomization can improve the transfer of control policies to the real world, especially for highly agile and high-dimensional problems. This approach involves randomizing appearance and dynamic parameters to enhance policy robustness. The design of the randomization parameters is crucial for different tasks to prevent the policy from learning a conservative strategy or failing to learn the task. Training adaptive policies with current and past observations as input can help identify dynamic parameters online and apply actions suitable for different system dynamics. These policies have been used for sim-to-real transfer in tasks like in-hand manipulation and non-prehensile manipulation. Instead of training a single robust or adaptive policy, multiple policies are trained for randomized environments and combined linearly for deployment in the real world without further fine-tuning. Our method uses dynamic randomization to train policies with different strategies for varying dynamics, optimizing directly in the target environment instead of relying on simulation. Recent works have also explored training policies in a source environment and fine-tuning in the target environment, such as MAP-Elite which learns a set of controllers and applies Bayesian optimization for fast adaptation to hardware damages. BID27 and BID4 developed policies that adapt to new environments by utilizing learned representations. Unlike prior work, they optimized only the dynamics parameters input to the policy, allowing for adaptation with less data and sparse reward signals. The motor skill learning problem is formulated as a Markov Decision Process. Reinforcement learning involves finding a control policy that maximizes expected rewards in a Markov Decision Process (MDP). In the context of transfer learning, adapting a policy from a source MDP to a target MDP is crucial. This adaptation is particularly important in Partially-Observable Markov Decision Processes (POMDPs) where only partial information is available. Transfer learning involves adapting a policy from a source MDP to a target MDP. A new method is proposed for transferring a policy learned in a simulated environment to a target environment with unknown dynamics. The algorithm consists of two stages: learning a family of policies and optimizing strategy. The first stage involves learning policies for specific dynamics, merging them into one to cover the space efficiently. Our method merges policies trained under similar dynamics into one neural network, training them simultaneously. A policy \u03c0 : (o, \u00b5) \u2192 a considers both robot observation o and physical parameters \u00b5. Each rollout randomly selects new physical parameters, creating a family of policies parameterized by \u00b5. We search for the optimal strategy in the \u00b5 space for the target environment, learning a mapping between experiences under source dynamics and \u00b5. Our method involves merging policies trained under similar dynamics into one neural network, considering both robot observation and physical parameters. By learning a mapping between experiences under source dynamics and \u00b5, we aim to find an optimal strategy in the target environment based on accumulated rewards rather than experience similarity. This approach efficiently searches the space of dynamic parameters to reduce the number of samples needed from the target environment. Our method involves using Covariance Matrix Adaptation (CMA) to efficiently search the space of dynamic parameters in the target environment. Samples are drawn from a Gaussian distribution over the space of \u00b5, and fitness is evaluated based on rollouts in the target environment. The mean and covariance matrix of the distribution are updated iteratively to improve performance. Our method utilizes Covariance Matrix Adaptation (CMA) to search dynamic parameters efficiently in the target environment. Policies are trained for locomotion control tasks and transferred to environments with different dynamics to overcome the reality gap. Testing includes generalization to variations in body mass, terrain slope, and end-effector materials. Randomized dynamic parameters during training are denoted as dim(\u00b5) for clarity. The dynamic parameters randomized during training are denoted as dim(\u00b5). Proximal Policy Optimization (PPO) is used for optimizing the control policy. Three trials with different random seeds are run for each example, reporting the mean and one standard deviation for the total reward. The method Strategy Optimization with CMA-ES (SO-CMA) is compared to three baseline methods: Robust, Hist, and UPOSI BID38. The robust policy is a feedforward neural network that learns actions for all training environments but cannot identify dynamic parameters from its input. In contrast, an adaptive policy is given a history of observations as input, allowing it to identify the environment being tested and choose actions adaptively. Training an LSTM policy directly using PPO is less efficient than using a feed-forward network with history input. In experiments, a feed-forward network with a history of 10 observations is used to represent the adaptive policy \u03c0 adapt, compared to UPOSI. In comparison to UPOSI, our method decouples the learning of an adaptive policy into training a universal policy via reinforcement learning and a system identification model via supervised learning. We expect UPOSI and Hist to achieve similar performance in theory, but in practice, UPOSI is expected to learn more efficiently due to the decoupling. We use a history of 10 observations as input to the online system identification model and continue to train the baseline methods after transferring to the target environment for fair comparison. Additionally, we compare our method to policies trained directly in the target environments, serving as an 'Oracle' benchmark. The Oracle policies for various environments were trained with a specific number of samples. The study investigates the effectiveness of SO-CMA in transferring to unknown environments and the impact of dim(\u00b5) on policy performance. Experiments were conducted with different values of dim(\u00b5) and randomized parameters for the robot. The study evaluates the performance of different methods in transferring policies to the MuJoCo Hopper environment with varying values of dim(\u00b5). SO-CMA successfully transfers the policy with good performance when dim(\u00b5) is 5 and 10, while baseline methods struggle to adapt with the same sample budget. In investigating the generalization of SO-CMA to differences in joint limits, the study varied the ankle joint limit for the MuJoCo Hopper and compared transfer performance with different dim(\u00b5) values. Results showed that SO-CMA outperformed baseline methods, especially with higher dim(\u00b5) values. Additionally, different initializations of the policy network led to varied gaits in a biped robot constrained to a 2D plane in the Walker2d environment. The study focused on analyzing the performance of transfer learning algorithms in different gaits, such as hopping and running, with varying robustness to environmental changes. To ensure comparability, a symmetry loss was used to achieve a symmetric running gait in all policies. Modeling errors were mimicked by adding an 8ms latency to the MuJoCo simulator. Policies were trained with randomized friction coefficients, restitution coefficients, and joint damping. Transfer performance was evaluated with varying sample numbers and foot masses in the MuJoCo Walker2d environment. The study compared the performance of SO-CMA to baselines using 30,000 samples in the target environment. Results showed SO-CMA outperformed Hist and UPOSI, similar to Robust. Policies were trained for HalfCheetah environment with transfer from DART to MuJoCo, adding a 50ms latency. Dynamic parameters were randomized during training, with results shown in FIG4 (a) for sample numbers and in FIG4 (b) for varying ground slopes. In the study, SO-CMA outperformed Robust and Hist, achieving similar results to UPOSI in environments with varying ground slopes. The importance of accurate actuator models for successful policy transfer from simulation to real-world settings for quadruped robots was highlighted. The experiment aimed to determine if SO-CMA could overcome errors in actuator models using a linear torque-current relation during policy training in a simulated environment. During policy training, a linear torque-current relation is used for the actuator model, which is then transferred to an environment with a more accurate non-linear relation. The robust policy tends to make the quadruped sink to the ground, while SO-CMA successfully transfers a policy trained with a crude actuator model to an environment with realistic actuators. Transferring policies from rigid-body to deformable object environments can enhance learning efficiency. Transferring policies from rigid-body to deformable object environments can enhance learning efficiency. SO-CMA successfully controls the robot to move forward in a soft Hopper environment with a deformable foot, while baseline methods fail. The method can transfer policies trained in one environment to a notably different one with a low amount of samples. SO-CMA outperforms baseline methods by consistently transferring policies across different environments, with the key factor being the sensitivity of the controller to the task's dynamic parameters. When the reality gap is large, UPOSI may fail to produce accurate estimates, leading to non-optimal actions. Hist struggles with transfer due to similar limitations as UPOSI and the challenge of training with a large input space. Baseline methods may succeed with dense reward signals, but sparse signals like distance traveled before falling may be more common in practice. Our method, SO-CMA, using an evolutionary algorithm, can handle sparse rewards effectively, showing a significant performance gap compared to baseline methods. We propose a policy transfer algorithm that learns a family of policies in a source environment and selects the best-performing policy for the target environment. This approach can overcome modeling errors and transfer policies from simulation to real hardware, with potential for further exploration of different approaches. Further investigations could explore different approaches for learning policies with diverse behaviors, such as unsupervised learning methods like BID11 and latent representation learning like HCP-I policy by BID4. Adding memories to the policy could extend its applicability to time-varying environments. Strategy optimization options like CMA-ES have shown promise, but reducing sample requirements in the target environment is desirable. One potential direction is to use simulation-learned models for optimization initialization. Both DART and MuJoCo are physically-based simulators used for transferring controllers from simulated robots to real hardware. Despite their similarities, there are key differences that make transferring policies between them challenging. In DART, contact handling is done through solving a linear complementarity problem to prevent object penetration, while MuJoCo allows objects to penetrate and separates them with increasing impulse. DART aims to solve joint limit constraints exactly, while MuJoCo uses a soft constraint formulation. In MuJoCo, a diagonal matrix \u03c3I n is added to the joint space inertia matrix to stabilize the simulation, not modeled in DART. A comparison of the Hopper example in DART and MuJoCo shows significant differences in robot control outcomes despite identical initial states and control signals. In experiments using Proximal Policy Optimization (PPO) in OpenAI Baselines, policies were trained with a feed-forward neural network. Modifications were made to the Walker2d environment to improve locomotion gaits, such as reducing the torque limit of the ankle joint. The text discusses modifications made to improve locomotion gaits in experiments using Proximal Policy Optimization (PPO) in OpenAI Baselines. Dynamic randomization settings and modeling errors were tested in different environments, including DART and MuJoCo. Latency in signals was also examined in the experiments. In experiments using Proximal Policy Optimization (PPO) in OpenAI Baselines, modifications were made to improve locomotion gaits. Latency in signals was examined, with a focus on the delay between sending out an observation from the robot and executing the corresponding action. The value of delay was below 50ms, with examples using 8ms and 50ms. Actuator modeling errors were addressed by fitting a more accurate actuator model, and varying parameters like foot mass in Walker2d and slope in HalfCheetah to create different target environments for testing. The torso mass ranges from 2 to 9kg. In the HalfCheetah example, the ground slope is varied to create different testing environments by rotating the gravity direction. Modeling errors include treating deformable objects as rigid ones, with a deformable box modeled using soft shape object in DART. The stiffness of the deformable object is set to 10,000 and damping to 1.0. PPO is run for 500 iterations in the source environment, sampling 40,000 steps per iteration to update the policy. In the target environment, fine-tuning of the Robust and Adaptive policy is done by sampling 2,000 steps at each iteration of PPO. A smaller batch size is used due to training on only one dynamics and limited sample budget. This amounts to 50 iterations of PPO updates with a maximum rollout length of 1,000. During fine-tuning, the CMA-ES implementation in Python is used to generate samples for evaluating the policy in the target environment. Experimentation with Strategy Optimization with Bayesian Optimization and Model-based Optimization is also conducted to find the best parameters for the policy. Bayesian Optimization is a gradient-free method that incrementally builds a Gaussian process model to estimate search parameter loss. It balances exploration and exploitation to draw new samples for evaluation. Testing on Hopper and Quadruped examples shows comparable performance to CMA-ES, making it a viable choice. SO-BA is a viable choice for strategy optimization but is noisier and less computationally efficient than CMA-ES due to re-fitting GP models. Model-based methods like using neural networks to learn dynamics and optimize \u00b5 were attempted but faced challenges with error accumulation and latency issues. In experiments, a Long Short Term Memory (LSTM) network is used to learn the dynamics of the target environment. The LSTM outperformed feed-forward networks for strategy optimization. Results show that the LSTM-based model achieved similar performance to CMA-ES in Hopper DART-to-MuJoCo. Model-based methods offer more flexibility than CMA-ES and Bayesian optimization, allowing for time-varying parameters. However, learning an accurate model requires access to the full state of the robot and frequent training of the LSTM network, making it slower than other methods."
}