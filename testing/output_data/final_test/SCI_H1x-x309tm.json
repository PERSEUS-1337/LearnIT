{
    "title": "H1x-x309tm",
    "content": "This paper introduces an analysis framework and conditions for the convergence of Adam-type algorithms in non-convex stochastic optimization, with a convergence rate of $O(\\log{T}/\\sqrt{T})$. The new algorithm AdaFom is included in this analysis. Our results provide a comprehensive analysis for Adam-type methods in the non-convex setting, aiding practitioners in monitoring algorithm progress and convergence behavior. First-order optimization methods, such as gradient descent (GD) and stochastic gradient descent (SGD), have seen significant advancements in the past decade for solving machine learning problems. These methods update the solution iteratively using a descent direction and learning rate. Recent works have proposed accelerated versions of gradient descent and stochastic gradient descent, falling into three categories: momentum methods, adaptive learning rate methods, and adaptive gradient methods. Adam, a popular method for deep learning, belongs to the third category and is used to train deep neural networks. Despite its practical success, theoretical investigation of Adam-like methods for non-convex optimization is still lacking. The work by Reddi et al. (2018) highlighted convergence issues with Adam in convex settings and introduced AMSGrad as a corrected version. However, the analysis was limited to convex problems, while the most successful applications are for non-convex problems. There is a gap between theory and practice. The derived conditions are practical, simple to check, and can certify convergence or track algorithm progress. The conditions for algorithm convergence are essential and tight, showing how oscillation of the effective stepsize can impact convergence rate. Interpretations of these conditions explain why certain Adam-type algorithms can outperform SGD.notations and asymptotic notations are used for clarity. Stochastic optimization is popular in analyzing algorithms in machine learning, especially with mini-batch gradient evaluation. A generic problem involves minimizing a function f with a random variable \u03be representing data sample or noise. First-order optimization algorithms use unbiased noisy gradients, assumed to be bounded and independent at different times. The function f is assumed to be continuously differentiable with Lipschitz continuous gradient, allowing for non-convex functions. This departure from convexity is a key aspect discussed in the paper. Our work focuses on the generalized Adam algorithm, a departure from convexity assumptions in recent papers on Adam-type methods. Algorithm 1 involves step sizes, problem parameters, gradient estimates, and past gradients to calculate an effective step size. The generalized Adam algorithm, highlighted in Algorithm 1, introduces the concept of an effective step size using the vector \u03b1 t / \u221av t. Popular variants of Adam include RMSProp and AdaFom, with Adam considered a momentum version of AdaGrad. The difference lies in the momentum added to first and second order moment estimates. AdaFom can be seen as a variant of Adam with an increasing sequence of \u03b2 2. The convergence of AMSGrad with fast diminishing \u03b2 1,t was studied, while the version with constant \u03b2 1 or strictly positive b and for non-convex settings are unexplored. AMSGrad with constant \u03b2 1 has also been proven to converge. Algorithm 1 can solve \"finite-sum\" problems with smooth and possibly non-convex functions. Algorithm 1 can be extended to mini-batch cases with unbiased estimators for gradients. The analysis focuses on convergence to first-order stationary solutions with sublinear rate, especially in non-convex settings. The challenge lies in the biased gradients and adaptive learning rate used in Adam-type algorithms. The analysis focuses on convergence to first-order stationary solutions with sublinear rate in non-convex settings. It involves careful analysis of biased gradients and the use of inverse exponential moving average in adjusting the learning rate. The existing convex analysis does not apply due to different convergence criteria and constant momentum controlling parameter. Assumptions for convergence analysis are formalized, including access to bounded noisy gradients and unbiased noise. The bounded norm of gradient in A2 is equivalent to Lipschitz continuity of f, a common condition for convergence analysis. A3 is standard in stochastic optimization. The main result shows global convergence with sublinear rate using properly chosen weighting terms in Algorithm 1. The effective stepsize parameters \u03b1 t and v t impact convergence of Adam-type algorithms. The effective stepsize parameters \u03b1 t and v t influence the convergence of Adam-type algorithms. The convergence rate of Algorithm 1 is determined by certain conditions on the parameters \u03b1 t and v t. The condition \u03b1 t m t / \u221av t \u2264 G ensures that the change of x t at each iteration is finite. The condition \u03b1 t m t / \u221av t \u2264 G ensures finite change in x t. Theorem 3.1 guarantees convergence of Adam-type methods if s 1 (T ) grows slower than s 2 (T). The rate of growth of s1(T) can be dominated by different terms, such as Term A and B, which are related to the convergence of the algorithm. Term A quantifies the increase in the objective function due to higher order curvature, while Term B is related to the effective stepsizes. The condition for proper convergence is that s1(T) grows slower than s2(T). The lower bound on the summation of effective stepsizes reduces to \u03b1 t in Algorithm 1. Term B characterizes the oscillation of effective stepsizes, which can lead to non-convergence of Adam. Adaptive gradient methods can help reduce convergence speed compared to SGD. In certain cases, proper design of v t can reduce quantity compared with SGD. Adaptive gradient methods like AMSGrad offer flexibility in stepsizes, aiding in hyperparameter tuning. The bound FORMULA8 is tight, showing that certain algorithms can diverge due to high growth rates. Term A is crucial, illustrated in a one-dimensional optimization problem. In a one-dimensional optimization problem, the growth rate of different terms is shown in FIG1. Both SGD and Adam do not converge to a stationary solution. Using an example, the importance of Term B for the convergence of Adam-type algorithms is demonstrated. AMSGrad converges while Adam does not, as shown in FIG2, matching theoretical analysis with empirical results. The importance of Term B for the convergence of Adam-type algorithms is highlighted in the empirical results in FIG2. AMSGrad converges due to smaller oscillations in effective stepsizes, associated with Term B. The quantity DISPLAYFORM2 DISPLAYFORM3 DISPLAYFORM4 is also noted by (Huang et al., 2018), but their analysis is limited to convex optimization. Theorem 3.1 provides a general approach for designing the weighting sequence {v t} and analyzing convergence in Adam-type algorithms. The non-convergence of Adam is explained in (Reddi et al., 2018) and is consistent with the analysis in Section 3.2. Term A in (5) can also cause Adam to diverge. The convergence rates of AdaGrad and AMSGrad are analyzed in Theorem 3.1, with a focus on AMSGrad's performance with a constant momentum parameter. Corollary 3.1 and Corollary 3.2 provide convergence rates for AMSGrad and AdaFom, respectively. AdaFom is a more general algorithm than AdaGrad, with AdaGrad being a special case when a certain parameter is set to zero. The convergence rates of AdaGrad and AMSGrad are analyzed in Theorem 3.1, with a focus on AMSGrad's performance with a constant momentum parameter. Corollary 3.1 and Corollary 3.2 provide convergence rates for AMSGrad and AdaFom, respectively. AdaFom is a more general algorithm than AdaGrad, with AdaGrad being a special case when a certain parameter is set to zero. In the analysis, specific conditions and constants are defined for the algorithms, ensuring numerical stability and avoiding division by zero errors. The convergence rates of AMSGrad and AdaFom are analyzed, showing that AMSGrad's performance can be improved with a constant momentum parameter. The derived convergence rate involves an additional log T factor compared to first-order methods, but this can be mitigated by choosing an appropriate stepsize. The theoretical analysis focuses on the worst-case convergence rate of adaptive methods in nonconvex optimization. In this section, the empirical performance of Adam-type algorithms is compared on training two convolutional neural networks (CNNs), including AMSGrad, Adam, AdaFom, and AdaGrad. Two examples are provided: training a CNN on MNIST and CIFARNET on CIFAR-10. AMSGrad performs similarly to Adam, confirming previous results. AdaGrad's performance is worse due to the lack of momentum. AdaGrad's performance is worse than other algorithms due to the lack of momentum. AdaFom falls between AMSGrad/Adam and AdaGrad, acting as a momentum version of AdaGrad with a simpler adaptive learning rate. In a larger network training on CIFAR-10, Adam and AMSGrad perform similarly and outperform AdaFom. Mild conditions are provided for the convergence of Adam-type algorithms, including AdaGrad, AdaFom, and SGD. This paper discusses the convergence of Adam-type algorithms for non-convex optimization problems and the impact of oscillation of effective stepsizes on convergence rate. Momentum methods, such as Nesterov's accelerated gradient, consider the history of first-order information. Future research may explore constrained non-convex optimization. The Nesterov accelerated gradient (NAG) method constructs the descent direction using the difference between current and previous iterates. A generalization of NAG for non-convex stochastic programming has been studied. Heavy-ball (HB) methods form the descent direction vector through a decaying sum of previous gradient information. Stochastic variance reduced gradient (SVRG) methods integrate SGD with GD for a hybrid descent direction of reduced variance. An accelerated version of perturbed gradient descent (PAGD) algorithm with the fastest convergence rate among Hessian free algorithms has also been proposed. Adaptive learning rate methods like AdaGrad and Adadelta accelerate SGD by incorporating past gradients or second-order information into the learning rate. While AdaGrad works well for sparse gradients, its convergence is only analyzed in convex settings. Other methods like ESGD lack theoretical investigation but show practical convergence improvement. Adam, RMSProp, and Nadam are adaptive gradient methods that update descent direction and learning rate simultaneously. Adam, the most widely-used method for training deep neural networks, uses exponential moving averages of past gradients to update descent direction and adjust the learning rate. While Adam was shown to converge at a rate of O(1/ \u221a T ) for convex problems, recent work highlighted convergence issues and proposed a modified version called AMSGrad. AMSGrad is a modified version of Adam that avoids its pitfalls by utilizing non-increasing quadratic normalization. While AMSGrad has shown progress in understanding adaptive gradient methods, its convergence analysis only applies to convex problems. Additional experiments demonstrate how specific Adam-type algorithms can outperform SGD in certain situations, highlighting the benefits of adaptive gradient methods in solving non-convex problems. AMSGrad, a modified version of Adam, has shown progress in understanding adaptive gradient methods. It can potentially converge to optima in valleys with high curvature, requiring less hyperparameter tuning. Empirical evidence demonstrates its flexible stepsizes property in optimization problems. AMSGrad, a modified version of Adam, has a strong normalization effect allowing for larger step sizes. The growth rate of different terms is shown in figures with various step sizes. In the figures, AMSGrad converges while SGD and Adam oscillate. Diminishing step sizes affect algorithm performance, with AMSGrad consistently converging and Adam slowly converging. AMSGrad can converge with a larger range of stepsizes compared to SGD. The key quantity limiting convergence speed is T t=1 \u03b1 t g t / \u221av t 2. In some cases, AMSGrad does not perform well compared to SGD due to larger effective stepsizes and a problem-independent constant. The normalization factor \u221av t helps the algorithm converge easier by imitating the largest Lipschitz constant. In the experiment on MNIST, a convolutional neural network (CNN) with 3 convolutional layers and 2 fully-connected layers is considered. Filters of sizes 6 \u00d7 6 \u00d7 1, 5 \u00d7 5 \u00d7 6, and 6 \u00d7 6 \u00d7 12 are used in the convolutional layers. AMSGrad and Adam are compared in the experiment. The architecture of the CIFARNET used in the experiment includes two convolutional layers with 32 and 64 kernels, followed by max pooling and dropout. This is followed by two convolutional layers with 128 kernels, each followed by max pooling. The last layer is a fully connected layer with 1500 nodes. ReLU activation and stride 1 are used in all convolutional layers. The learning rate decreases over epochs for Adam and AMSGrad. The learning rates for Adam and AMSGrad start at 0.001 and decrease by 10 every 20 epochs. AdaGrad and AdaFom start at 0.05 and decrease to 0.001 after 20 epochs and 0.0001 after 40 epochs. These rates are optimized for performance. The convergence proof of Algorithm 1 is presented, with lemmas leading to Theorem 3.1. The proof of Lemma 6.2 is completed by utilizing the Lipschitz smoothness of \u2207f. The next series of lemmas bound the terms on the right-hand side of the equation. Lemma 6.3 states that under certain conditions, T1 can be bounded. The proof involves showing that mt is less than or equal to H through induction. Lemma 6.7 discusses the bounds for T2. Lemma 6.3 states that T1 can be bounded by showing mt \u2264 H through induction. Lemma 6.7 discusses bounds for T2. Further, in (9), z1 = x1 by definition. The second term in (26) is bounded, and T7 is defined as displayed. T8 and T9 are introduced for ease of notation in the subsequent inequalities. The second-last inequality in T9 is due to the similar reason as T7. Using Lemma 6.8, we can bound the second term in (26) and reparameterize g t as g t = \u2207f (x t ) + \u03b4 t with E[\u03b4 t ] = 0. The first term in RHS of (34) is the desired descent quantity, while the second term is a bias term to be bounded. The proof of Theorem 3.1 combines various lemmas to bound the expected descent of the objective function. The proof of Lemma 6.9 is then established through induction, showing the concavity of the function. The proof of Theorem 3.1 involves bounding the expected descent of the objective function by combining lemmas. Using x = \u03a3 ai and b = aT, we have to show log(x) \u2265 log(x - b) + b/x for b < x. The first iteration is checked with a1 = 1, completing the proof. Q.E.D."
}