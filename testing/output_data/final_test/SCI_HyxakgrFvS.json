{
    "title": "HyxakgrFvS",
    "content": "When training a neural network for a specific task, it is common to adapt a pretrained network instead of starting from scratch. This can be beneficial when there is limited training data, for lifelong learning scenarios, or to incorporate prior knowledge through preset weights. The main approaches for network adaptation include fine-tuning and using the pretrained network as a fixed feature extractor. Side-tuning is proposed as a straightforward alternative to fine-tuning and fixed feature extraction for adapting pretrained networks. It involves training a lightweight \"side\" network that is fused with the pretrained network using a simple additive process. Side-tuning is shown to be less prone to overfitting, yield better results with limited training data, and avoid catastrophic forgetting in lifelong learning scenarios. It has demonstrated promising performance in various tasks such as lifelong learning, reinforcement learning, imitation learning, NLP question-answering, and single-task transfer learning. Transfer learning through side-tuning involves utilizing a pretrained model to learn new tasks without compromising the base model's performance. The approach includes training a side model that complements the base model's representations for the target task. By using a combining operation like alpha-blending, side-tuning addresses limitations of fine-tuning, such as forgetting old information and overfitting. Side-tuning is a form of transfer learning that involves training a side model to complement the base model for new tasks. It is an additive approach that strategically adds parameters for each new task without constraints on the side network's structure. Unlike fixed feature extraction, side-tuning allows for adaptation of the base network over the agent's lifetime. Side-tuning is an additive transfer learning approach that strategically allocates parameters for new tasks without constraints on the side network's structure. It allows for adaptation of the base network over time and does not require regularization to prevent inter-task interference. Side-tuning is a simple approach for incremental learning that uses smaller side networks compared to the base network. It effectively addresses challenges like catastrophic forgetting and rigidity in adapting to new problems. In Section 4.2.2, it is discussed how network adaptation methods struggle with adapting to new problems due to constraints from previous problems. Section 4.2.3 introduces side-tuning as a solution that avoids these issues while maintaining high performance. Side-tuning is an additive approach that does not suffer from the drawbacks of substitutive methods like fine-tuning, making it effective for incremental learning. In the context of network adaptation methods discussed in the previous section, side-tuning is highlighted as an additive approach that avoids constraints from previous tasks. It involves modifying network weights or using task-specific parameters to solve new tasks effectively. Other methods include using off-the-shelf features with readout layers or custom connection schema. Incremental learning aims to perform well on a sequence of tasks by the end of training. Incremental learning involves tasks T1 to Tm, aiming to perform well on the entire set without catastrophic forgetting or slowing down learning speed. Different network adaptation approaches address these issues, such as Cheung et al. (2019) using orthogonal subspaces for tasks, Schwarz et al. (2018) adding a parameter regularization term per task, and Li & Hoiem (2016) imposing constraints on parameter updates. However, these approaches may slow down learning on later tasks or ignore useful transfer from previous tasks. Additive methods like side-tuning offer an advantage by not imposing constraints from previous tasks. Additive methods, including side-tuning, are advantageous as they avoid catastrophic forgetting and allow for transfer. Side-tuning shows good performance and scaling, demonstrated experimentally on iCIFAR and Taskonomy datasets. It is compatible with meta-learning and existing approaches, focusing on feature adaptation rather than rapid learning. Residual Learning leverages easier approximation for certain tasks. Residual Learning exploits easier approximation for tasks, used in ResNets and robotics. Additive learning concepts have been studied in various fields, such as infant development and adult affordances. Functional specificity exists within the brain. Functional specificity exists within the brain, with separate pathways conditioned on each other. Side-tuning combines a side model with a base model to compute representations for a target task. The base model provides core cognition or perception, while the side network is updated during training to learn residuals for gradient boosting. Side-tuning combines a side model with a base model to compute representations for a target task. The side network complexity can scale to the problem's difficulty, initialized with weights from the base network. The final representation is a combination of the base and side networks, with multiple options for the combination operator. Alpha blending, a combination operator in side-tuning, works well in practice by preserving input dimensions and being simpler than concatenation. It is a generalization of concatenation followed by a channel-collapsing operation. Varying alpha during training allows for a switch between feature extraction (\u03b1=1) and fine-tuning (\u03b1=0), resembling a common training curriculum in RL. In side-tuning, alpha blending is a combination operator that varies during training to switch between feature extraction (\u03b1=1) and fine-tuning (\u03b1=0), resembling a common training curriculum in RL. Alpha controls the weighting of the prior with the learned estimate and can be treated as a learnable parameter that determines how heavily to weight the base model. The value of alpha correlates with task relevance, and there is a tradeoff between bias and variance contributions when choosing between feature extraction or fine-tuning. Side-tuning in machine learning involves using alpha blending to switch between feature extraction and fine-tuning, balancing bias and variance. Alpha controls the weighting of the prior with the learned estimate, offering a way to control the importance of the prior in the model. Side-tuning reduces variance by including fixed features in the representation and allows updating via the residual side network, providing a consistent approach to model training. The pullback metric in differential geometry, known as perceptual loss in deep learning, allows for specifying explicit priors on outputs rather than latent representations. This method can be useful for knowledge transfer with limited training data and is used to prevent catastrophic forgetting. Elastic Weight Consolidation and Learning Without Forgetting address catastrophic forgetting in neural networks. Side-tuning avoids this issue by not updating the base network. In incremental learning, where performance on previous tasks is important, catastrophic forgetting is a major concern. Experiments involve dedicating a new side network to each new task. In incremental learning, side-tuning dedicates a new side network to each new task, avoiding catastrophic forgetting by not updating the base network. This approach outperforms existing methods in the literature while using fewer parameters on more tasks. The additive learning approach ensures that performance across the whole set can only increase as the agent sees more tasks. Side-tuning in additive learning prevents catastrophic forgetting by dedicating a new side network to each task without updating the base network. This approach outperforms existing methods by using fewer parameters on multiple tasks, ensuring performance improvement as more tasks are encountered. Continuous adaptation scenarios are also effectively handled by side-tuning, avoiding rigidity during training and maintaining task-specific performance even with a stream of tasks with undefined boundaries and minimal data per task. Side-tuning is a method for continuous adaptation that uses small, cheap side networks to constantly adapt the base network to input tasks. When a side network performs poorly, it is discarded, allowing for online adaptation. Side-tuning has shown success in various domains and outperforms existing incremental learning approaches on different datasets. In the context of continuous adaptation, side-tuning involves using small side networks to adapt the base network to input tasks. This method has shown success in various domains and outperforms existing incremental learning approaches on different datasets. Comparisons are made with methods like fine-tuning, parameter superposition, progressive neural network, and independent task networks. Neural Network (PNN) is a network adaptation approach that uses more learnable parameters than alternatives like saving separate ResNet-50 for each task. It achieves strong performance in incremental learning experiments for multiple tasks without forgetting or becoming rigid. The curr_chunk discusses how side-tuning with a multilayer perceptron outperforms existing incremental learning approaches on challenging datasets like Taskonomy and iCIFAR. It focuses on learning tasks from a single RGB image using ResNet-50 for the base network and a 5-layer convolutional network for side-tuning. The number of parameters used is significantly lower for side-tuning compared to other methods. The network parameters used for EWC and PSP are 24.6M, while for side-tuning it is 11.0M. Tasks are formed by partitioning CIFAR-100 classes into 10 sets, trained for 20k steps each. Side-tuning shows no catastrophic forgetting, with error rates remaining stable. Sample predictions show side-tuning outperforming EWC. Side-tuning demonstrates no catastrophic forgetting on tasks, with predictions closer to ground-truth compared to EWC. It learns new tasks easily, while EWC stagnates. Side-tuning outperforms alternatives on Taskonomy and iCiFAR datasets. Side-tuning significantly outperforms other methods on Taskonomy and iCIFAR datasets, achieving better average ranks with fewer trainable parameters. It experiences zero slowdown compared to EWC, which becomes increasingly rigid with the number of tasks. Side-tuning's performance remains superior even when other methods use smaller networks. The side-tuning method achieves the best average rank on Taskonomy, outperforming other methods like EWC and PSP. It is not domain or task-specific and shows good performance in various settings. Side-tuning's immunity from catastrophic forgetting and rigidity outweighs the benefits of larger networks used in other methods. Side-tuning method outperforms other lifelong learning tasks like fine-tuning on Taskonomy dataset for various target tasks. It shows good performance with limited data and is as effective as feature extraction or fine-tuning. The side network, trained alongside the base network, achieves similar results to standard fine-tuning. Side-tuning method, when trained alongside the base network, matches the adaptiveness of fine-tuning and outperforms learning from scratch in various tasks. It performs well with large amounts of data and shows similar or slightly better results than fine-tuning. Additionally, side-tuning adapts effectively to new tasks, even in non-convolutional architectures like BERT, outperforming feature extraction and scratch methods. In this section, Behavior Cloning is used to train agents to imitate experts in navigating to a target coordinate in the Habitat environment. The agents are evaluated in 14 held-out validation buildings, showing that side-tuning consistently matches or beats the best approach. Reinforcement Learning using a different algorithm (PPO) and direct interaction also yields similar trends. In Habitat environment, agents trained with side-tuning show comparable performance to other methods. Task relevance predicts alpha values, with curvature outperforming denoising in imitation learning. Alpha values are predictive of transfer performance, with the relative order being more important than the actual value. Side-tuning performs well in domains with abundant data. Side-tuning outperforms other methods in domains with abundant or scant data, showing superior performance even with intermediate amounts of data. Network size matters depending on the task, with side network size being more critical than base network size. Side-tuning offers advantages over alternatives even with a smaller base network. In experiments from Taskonomy and Habitat, side-tuning shows benefits in high and low-data settings. Extending side-tuning to do boosting may improve performance, but parameters might be better utilized in a deeper network. Good side network initialization can slightly boost performance, but differences were not statistically significant across tasks. Initialization differences were not statistically significant across tasks, but excluding simple tasks showed a significant difference. In reinforcement learning, fine-tuning often fails due to 'high variance' early updates. A stage-wise approach performed as well as keeping features fixed, with side-tuning being simpler. Testing the 'high-variance update' theory by fine-tuning showed promising results. The side-tuning framework is a simple yet effective approach for additive learning, showing benefits over vanilla fine-tuning. It is well-suited for incremental learning and outperforms existing methods with state-of-the-art neural networks in various domains. The approach is effective in preventing catastrophic forgetting and rigidity, with empirical results supporting its theoretical advantages. The na\u00efve approach to incremental learning in this paper made design decisions for network types. Flexible parameterizations for side networks, better forward transfer, learning when to deploy side networks, and using side-tuning to measure task relevance were discussed as ways to improve performance in incremental learning setups. In incremental learning setups, side-tuning is used to measure task relevance and improve performance. The base model architecture affects performance, with a small five layer convolutional network performing comparably to ResNet-50. Rectified Adam helps with high variance updates at the start of training. Ablation studies show that in data-scarce scenarios, using features is effective, while fine-tuning works well with abundant data. Side-tuning can perform as well as stronger approaches in both cases. In incremental learning setups, side-tuning is used to measure task relevance and improve performance. The base model architecture affects performance, with a small five layer convolutional network performing comparably to ResNet-50. Our data consists of 4M images on 12 single image tasks including curvature, semantic segmentation, reshading, keypoints, texture edges, and more. We pretrain on curvatures and train each task for three epochs using different loss functions. We use Adam optimizer with initial learning rate of 1e-4, weight decay of 2e-6, gradient clipping to 1.0, and batch size of 32. Performance is evaluated on a held out set of images after training specific tasks and all tasks are complete. Pretraining is done on CIFAR 10, then CIFAR100 is partitioned into 10 sets of 10 classes. Training is done on these tasks for 4 epochs with Adam optimizer, learning rate of 1e-3, batch size of 128. Question answering dataset SQuAD2.0 is used for training and testing, with a BERT transformer pretrained on a larger corpus. Fine-tuning is done on a single BERT transfer. The experimental setup involves training and testing agents in the Habitat environment with the Gibson dataset, which virtualizes 572 buildings. Agents navigate to nonvisual target destinations while avoiding obstacles and walls. Training and testing are done in separate sets of buildings, with a maximum episode length of 500 timesteps. The train and test spaces cover 15678.4m2 and 1752.4m2, respectively. In the Habitat environment with the Gibson dataset, agents navigate to target destinations while avoiding obstacles and walls. The episode length is 500 timesteps, and the target distance ranges from 1.4 to 15 meters. Both imitation learning and RL use different data, architecture, and optimization processes. In imitation learning, expert trajectories are collected, a neural network is trained for 10 epochs using cross entropy loss and Adam, and finetuning updates all weights. RL setup is borrowed from the same work for experiments. In experiments using the Gibson dataset in the Habitat environment, a single rollout worker is used due to computational constraints. Experience replay and an off-policy variant of PPO are employed to decorrelate batches. The agent receives rewards for reaching the goal, moving towards it, and incurs a small negative reward per timestep. The maximum episode length is 500 timesteps, with the target distance set between 1.4 and 15 meters. In experiments with the Gibson dataset in the Habitat environment, a five-layer convolutional network is used for base encoding, distilled from ResNet-50. Finetuning involves updating all weights, and feature extraction is done with the five-layer network. In teacher-student distillation, the goal is to minimize the distance between the teacher's output and the sum of the student's output and the teacher's output. Additional analysis provides alternative perspectives for lifelong learning. In experiments with the Gibson dataset in the Habitat environment, a five-layer convolutional network is used for base encoding, distilled from ResNet-50. Finetuning involves updating all weights, and feature extraction is done with the five-layer network. In teacher-student distillation, the goal is to minimize the distance between the teacher's output and the sum of the student's output and the teacher's output. Additional analysis provides alternative perspectives for lifelong learning. We provide insights on side-tuning and adapter networks in comparison to PNN, showing comparable performance in classification error and ranking. In experiments with the Gibson dataset in the Habitat environment, a five-layer convolutional network is used for base encoding, distilled from ResNet-50. Finetuning involves updating all weights, and feature extraction is done with the five-layer network. In teacher-student distillation, the goal is to minimize the distance between the teacher's output and the sum of the student's output and the teacher's output. Additional analysis provides alternative perspectives for lifelong learning. We provide insights on side-tuning and adapter networks in comparison to PNN, showing comparable performance in classification error and ranking. In the PNN, an alternate method (PNN3) with similar performance to side-tuning is found. Our method outperforms others quantitatively and matches closely with PNN. Various fusion methods are compared in iCIFAR, with late fusion performing better than early fusion and comparable to distributed fusion. Taskonomy analysis shows similar qualitative results. In experiments with the Gibson dataset in the Habitat environment, a five-layer convolutional network is used for base encoding, distilled from ResNet-50. Finetuning involves updating all weights, and feature extraction is done with the five-layer network. In teacher-student distillation, the goal is to minimize the distance between the teacher's output and the sum of the student's output and the teacher's output. Additional analysis provides alternative perspectives for lifelong learning. We provide insights on side-tuning and adapter networks in comparison to PNN, showing comparable performance in classification error and ranking. In the PNN, an alternate method (PNN3) with similar performance to side-tuning is found. Our method outperforms others quantitatively and matches closely with PNN. Various fusion methods are compared in iCIFAR, with late fusion performing better than early fusion and comparable to distributed fusion. Taskonomy analysis shows similar qualitative results. The methods do not vary much, with distributed and early fusion requiring knowledge about the information structure, while late fusion can consider each information column a black box, useful when the base information is non-neural network. Side-tuning with ground truth curvature achieves better rank on the Taskonomy dataset than other methods, utilizing black-box side information effectively. Sidetuning can be successful with black-box side information, improving performance compared to not using the inputs or using inputs from a neural network. Lifelong learning approaches lack a standard way to utilize this type of information."
}