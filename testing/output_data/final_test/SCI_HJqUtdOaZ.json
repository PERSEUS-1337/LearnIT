{
    "title": "HJqUtdOaZ",
    "content": "Automatic classification of objects is crucial in engineering and data mining. Complex classifiers can enhance accuracy, but feature combination is key. A structure similar to Feed-Forward Neural Network is used to optimize feature combinations with Genetic Algorithm. Linear and non-linear activation functions are employed for reliability. Experiments on UCI data sets show the effectiveness of the proposed system. The proposed linear and non-linear intelligent FFNN-based feature combination method enhances classifier reliability and results. The importance of classification in various fields like medicine, engineering, and computer science is highlighted. Applications include disease diagnosis, fault detection in power systems, and batch processes in chemical engineering. Extracting useful features from data is crucial for designing efficient classification systems. Feature combination methods, including dimension reduction techniques like Linear Discriminate Analysis (LDA) and Principle Component Analysis (PCA), are used to enhance the severability of data for classification purposes. These techniques aim to improve the effectiveness of classifiers by selecting the most relevant features and reducing the complexity of the feature space. In this paper, Feature combination methods are discussed, including the use of Feed-Forward Neural Network (FFNN) and Genetic Algorithm (GA) for optimization. The focus is on determining the most effective combination methods for specific applications and datasets, addressing questions about feature space dimension reduction and the comparison between linear and non-linear feature combination methods. Neural Network (FFNN) and Genetic Algorithm (GA) are used for optimization in feature combination methods. Linear Intelligent Feature Combination (LIFC) and Non-Linear Intelligent Feature Combination (NLIFC) systems are introduced for adaptive combination systems. Original features are mapped into a new feature space using semi-FFNN structure, and outputs are classified by minimum distance classifier. GA updates weights and biases of semi-FFNN structure, and correct recognition rate is evaluated. Overview of minimum distance classifier, FFNN structure, and GA are described in sections 2, 3, and 4. Proposed method and experimental results are presented in section 5. The curr_chunk discusses the Minimum Distance classifier as a simple classification method based on distance calculation between input data and available classes. Various distance calculation procedures are mentioned, with Euclidean distance being the most common. The formula for Euclidean distance is provided as well. The curr_chunk explains the concept of minimum distance classifier and the use of Artificial Neural Networks (ANNs) in solving complex problems. ANNs are modeled after the human brain and can learn non-linear functions, making them useful in practical applications like comparative diagnoses and controlling nonlinear systems. Artificial Neural Networks (ANNs) are used in various applications such as comparative diagnoses and controlling nonlinear systems. Different topologies, including feed-forward, are proposed for implementing ANNs in supervised learning. The feed-forward topology involves information being fed into the ANN via the input layer, where it is distributed to the main body of the network. Information is updated through synapse weights and activation functions in subsequent layers, ultimately reaching the output layer. This structure allows for information flow from input to output without feedback or disconnections between layers. Genetic Algorithm (GA) is a well-known evolutionary algorithm used in stochastic optimization techniques. It involves creating an initial population, evaluating an objective function, and producing offspring through crossover to find the best solution. Particles in population evolve to adapt to their environment, with those better at adaptation having a higher chance of survival. Each chromosome in GA represents a specific feature or behavior. In genetic algorithms, the best solution is obtained through crossover and mutation of offspring created from parents. Different methods can be used to enhance classification systems, such as feature combination to simplify discrimination by simple classifiers. Reducing feature space dimension is a common strategy in methods like LDA, PCA, and ICA. The proposed method in this paper focuses on applying linear or non-linear intelligent features mapping in a new solution space to increase the discriminative power of data. It involves reducing, combining, or increasing feature space dimensions based on the mapping concepts. Equation 1 provides a general overview of the process, where feature dimensions can be reduced, unchanged, or increased through a transfer function. The proposed method in the paper involves changing feature space dimensions and applying transfer functions to combine features in linear or non-linear formats. The transfer function can be a feedforward neural network structure. The output data is projected into one axis if m=1, and into multiple axes if m>1. The transfer function can be linear or non-linear, with linear transfer resulting in weighted summation of primary features, and non-linear transfer involving a non-linear activation function. The paper discusses the use of non-linear sigmoid transfer functions to optimize feature separability for classification systems. Genetic Algorithm (GA) is employed for optimizing weights and biases, followed by the use of a minimum distance classifier and cross-validation technique to calculate error recognition rate. The stop criteria is based on achieving the least error recognition rate. The study evaluates the stop criteria for Genetic Algorithm (GA) in optimizing weights and biases for classification systems. The fitness function of GA is the mapping system, with weights and biases as chromosomes. Four classification tasks using UCI datasets are used to test the proposed combination methods. The Iris dataset includes samples from three species with four features each. The Wine dataset results from a chemical analysis of wines from three cultivars in Italy. The dataset includes samples of wines from Italy, categorized into three classes based on 13 features. Another dataset consists of samples from different types of glass, categorized into nine features. The Ionosphere dataset contains radar data collected in Goose Bay, Labrador. The radar data from the Ionosphere dataset consists of \"Good\" and \"Bad\" returns, with \"Good\" returns showing structure in the ionosphere. Various conditions are considered for efficient classification systems, with classification using cross-validation applied ten times. The classification parameters of GA are consistent for all data sets. The study analyzed error recognition rates for various data sets using GA. Results were presented in tables showing minimum, maximum, and average correct classification rates. The method focuses on error recognition rate but reports correct recognition rate. The classification process was repeated 10 times under different conditions, such as feature projection and combination. The study analyzed error recognition rates for various data sets using GA. Results were presented in tables showing minimum, maximum, and average correct classification rates. Classification is done using linear and non-linear transfer functions, with different approaches for Iris, Wine, Glass, and Ionosphere datasets. Best classification rates were achieved with non-linear methods, with varying effects on feature space dimensions. The study compared the performance of a proposed method with LDA and PCA for dimensionality reduction. LDA reduces features to (C-1) dimensions, while PCA also reduces feature dimension. Results were compared with other classification rates from literature using the same datasets. The study compared the performance of a proposed method with LDA and PCA for dimensionality reduction. Results showed the importance of data and feature selection before using complex classifiers. The proposed methods (LIFC and NLIFC) provided high-quality features, improving classification rates for Iris, Wine, Glass, and Ionosphere datasets. Extracting useful knowledge and features from the dataset is crucial for designing an efficient classification system. The paper proposed intelligent feature combination for better results. The study proposed intelligent feature combination to enhance feature quality and used a minimum distance classifier for classification. Results showed that the combination method depends on the dataset and its features, with non-linear mapping reducing dimension for Iris and Ionosphere datasets, and increasing dimension for Wine dataset. The best result for Glass dataset was obtained through feature combination. The best result for Glass dataset is achieved through non-linear mapping of combined features without changing the dimension of the feature space."
}