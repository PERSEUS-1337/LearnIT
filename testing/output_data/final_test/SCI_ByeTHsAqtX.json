{
    "title": "ByeTHsAqtX",
    "content": "The gradient in large-scale deep learning scenarios converges to a small subspace spanned by top eigenvectors of the Hessian. This subspace is mostly preserved during training, suggesting that gradient descent mainly occurs within it. This phenomenon has implications for optimization and learning in deep learning models trained using stochastic gradient descent (SGD) and its variants. This paper investigates the dynamics of the gradient and the Hessian matrix during SGD in overparameterized deep learning models. The Hessian spectrum can indicate flat directions in the loss landscape. The Hessian spectrum analysis in deep learning reveals two components: a bulk with small eigenvalues and a top with larger positive eigenvalues. The tangent space at each point has a bulk subspace and a top subspace, with the gradient quickly aligning with the top subspace during training. The Hessian spectrum analysis in deep learning shows that the top eigenvectors evolve independently from the bulk eigenvectors during training, implying learning occurs in a slowly-evolving subspace. This phenomenon is consistent across various model architectures and datasets. The Hessian spectrum analysis in deep learning reveals that the gradient is concentrated in the top subspace, leading to a similar loss decrease when using the projection of the gradient onto this subspace. A toy model of softmax regression on Gaussians illustrates this phenomenon, showing that the top Hessian subspace contains the gradient while the bulk subspace has zero eigenvalues. The gradient is concentrated in the top Hessian subspace, with zero eigenvalues in the bulk subspace. Including a small amount of variance does not change this, although non-zero eigenvalues appear in the bulk subspace. The gradient lies mainly in the top subspace during deep learning, which remains preserved over training periods. Models with parameters denoted by \u03b8 and a cross-entropy loss function L(\u03b8) are considered. The gradient in the top Hessian subspace is concentrated, with zero eigenvalues in the bulk subspace. The gradient lies mainly in the top subspace during deep learning, which remains preserved over training periods. The gradient at a point \u03b8 can be decomposed into top and bulk subspaces, with the fraction in the top subspace given by a specific formula. The dynamics of gradient descent were analyzed in various works, including Saxe et al. (2013). The fraction of gradient concentrated in the top subspace grows quickly during training, indicating a driving mechanism. Experiments used stochastic gradient descent on different models and datasets, with measurements evaluated on the entire training set. Results were replicated in two independent implementations. In this section, the overlap between the gradient and the Hessian-gradient product during training is considered. The overlap ranges from [-1, 1] and can be used as a proxy measurement to determine if the gradient is mostly in the top subspace of the Hessian. Experimental results for various deep learning scenarios are presented in TAB0. The overlap between the gradient and the Hessian computed on a test set is large for various deep learning scenarios, including models with different architectures, activation functions, optimization algorithms, and training data variations. The overlap tends to be consistently close to one after some training, indicating that the gradient is mostly in the top subspace of the Hessian. The overlap between the gradient and the Hessian in deep learning scenarios tends to be large, indicating that the gradient is mostly in the top subspace of the Hessian. By choosing random vectors in the top Hessian subspace, an estimate for the overlap can be obtained, which aligns with empirical observations. The top Hessian subspace overlap with the gradient is significant, indicating preservation during training. Random vectors in this subspace show comparable overlap to the gradient, while full parameter space vectors have negligible overlap. The top subspace is maintained during training, as shown empirically. The regression data set was sampled from a sine function with Gaussian noise. SGD was used with a mini-batch size of 64 and \u03b7 = 0.1. Models were trained for a few epochs, and the overlap of subspaces was measured. The evolution of subspace overlap for different starting and future times was shown for classification tasks with k = 10 classes. After training models with SGD on a regression dataset, the evolution of subspaces was analyzed for classification tasks with 10 classes. The top subspace, spanned by the top k eigenvectors, showed significant overlap even after 100 steps, indicating little evolution during training. In contrast, the next k eigenvectors' subspace had quick decay in overlap, suggesting a weak time dependency. This observation supports the idea that gradient descent occurs in a small subspace, with the gradient concentrating in this subspace throughout training. Additional results on subspace evolution were provided in Appendix C. After training models on a regression dataset with SGD, the evolution of subspaces was analyzed for classification tasks with 10 classes. The top subspace showed significant overlap even after 100 steps, indicating little evolution during training. The next subspace had quick decay in overlap, suggesting a weak time dependency. Increasing the subspace dimension until d = 9 led to a fixed overlap, but at d = 10, it decreased monotonically. This suggests an interesting feature when the dimension equals the number of classes. In order to understand the effects, a toy example was worked out, capturing all effects. The overlap is the normalized inner product between projectors P (t) top and P (t ) top. The overlap between projectors P (t) top and P (t ) top is related to the Frobenius norm of the difference between them. The transition at the number of classes minus one, k \u2212 1, seems sharp in realistic deep learning examples. The toy model is illustrative but not a definitive explanation of the phenomenon. The text discusses a 2-class classification problem with Gaussian distributions and softmax regression. Small perturbations do not change qualitative results, and further generalizations are left for future work. The text discusses the cross-entropy loss in a 2-class classification problem with Gaussian distributions and softmax regression. Simplifying approximations are made, such as assuming orthogonal means and concentrating samples at specific points. The loss function is shown to have 2d-2 flat directions. The loss function in a 2-class classification problem with Gaussian distributions and softmax regression has 2d-2 flat directions. The Hessian has rank 2, with two nontrivial eigenvectors in the top subspace and its kernel in the bulk subspace. The gradient is always within the top subspace, and the optimization trajectory is solved analytically in late times. The solution involves parameters that span a 2d-dimensional space, with the Hessian having two positive eigenvalues in the top subspace. The gradient evolves during training within the top subspace, with the possibility of breaking degeneracy by adding biases to model parameters. Small sample noise can lead to non-zero eigenvalues in the Hessian spectrum. During training, the gradient aligns with the eigenvector of the Hessian with the smaller eigenvalue, especially in the case of a Gaussian mixture with multiple classes. The addition of biases can break degeneracy, resulting in one smaller eigenvalue. The gradient evolves towards the minimal eigenvector in the top subspace, driven by a single gradient descent step. The gradient aligns with the eigenvector of the Hessian with the smallest non-zero eigenvalue, especially when degeneracy is broken by biases. However, applying this argument to the entire Hessian spectrum would lead to the incorrect conclusion that the gradient should evolve into the bulk. This is because the bulk of the Hessian spectrum corresponds to flat directions. The Hessian spectrum shows flat directions where the gradient vanishes, implying a symmetry in parameter space. Introducing sample noise leads to small changes in the optimization trajectory. The Hessian spectrum reveals flat directions with vanishing gradients, indicating symmetry in parameter space. Using various parameters, including k > 2, the top-k subspace dynamics become more apparent. The loss equation and orthogonal mean vectors result in k(k \u2212 1) nontrivial directions, with the Hessian naturally rank k(k \u2212 1). The k(k \u2212 1) subspace is dominated by k top eigenvalues, particularly in linear regression with quadratic loss. The addition of noise in the loss function breaks translation symmetry, leading to fewer flat directions and more non-zero eigenvalues in the Hessian spectrum. This results in a spectrum closer to realistic examples, with the top subspace still dominated by two large eigenvalues. Noise can be seen as a small perturbation that does not change the overall conclusions analytically. Further analysis including sample noise is left for future work. During training, the Hessian splits into two subspaces, with the gradient residing in the subspace spanned by the top k eigenvectors. This concentration of learning in a small subspace with positive Hessian eigenvalues may explain the success of deep networks with nonconvex loss functions. Future research will focus on investigating this phenomenon further. Future research will focus on investigating the nature of the nearly preserved subspace, the hierarchy of eigenvalues in the top subspace, and how it mixes with itself in deep learning examples. The goal is to understand the different eigenvectors in this subspace and their potential relevance for feature extraction. Learning in the top subspace is crucial, as the decrease in loss is mainly due to the projection of the gradient onto this subspace. Further investigation is needed to explore the implications of this phenomenon. The loss on the current iteration will decrease by almost the same amount if the linear approximation holds. Updating with g top has a nonlinear effect on the dynamics, altering the spectrum or causing the top subspace to unfreeze. Further study is warranted on the relationship between the Hessian and the gradient, and the practical applications of second-order optimization methods. Newton's method and its traditional criticisms for large models are also discussed. The Hessian of large deep networks is highly-singular, making it infeasible to compute its inverse. Newton's method moves towards critical points, but a low rank approximation in the top subspace allows for a well-defined and computationally simple inversion. This approximation ensures descent towards optima with strictly positive eigenvalues in the top subspace. For efficient computation of the top eigenvectors of the Hessian, the Lanczos method is used without explicitly representing the Hessian. The eigenvalue spectrum of the Hessian naturally separates into \"top\" and \"bulk\" components, with the top containing the largest eigenvalues. The eigenvalues of the top subspace in a small fully-connected two-layer network trained on MNIST for 40 epochs are clearly visible, with 10 nontrivial eigenvalues. The dimension of the top subspace is tied to the classification task, as shown by studying datasets with different numbers of classes and synthetic datasets. For example, in a modified MNIST dataset with 2 class labels based on even or odd digits, only 2 large eigenvalues were observed, indicating a 2-dimensional top subspace. The study focused on the eigenvalues of the top subspace in a small fully-connected two-layer network trained on MNIST for 40 epochs. The network showed high training accuracy even after removing the correlation between input and labels. A two-layer fully-connected network on CIFAR100 was then analyzed to explore the transition between bulk and top subspaces, with a density plot of eigenvalues shown in FIG5. The x-axis is the log of the eigenvalue, with a focus on the transition from top to bulk by computing the top 1000 eigenvalues. The density plot in FIG5 reveals a clear feature around the 100th eigenvalue, suggesting a possible log-normal distribution. Further investigation is needed to characterize the spectral density of the Hessian. In FIG6, the maximal eigenvector changes during training for a fully-connected architecture on MNIST. Nonzero elements are spread across model parameters, with top layer weights only slightly larger than first hidden layer coefficients. In Figure 7, the eigenvector in the final layer of a fully-connected architecture trained on MNIST is shown to evolve over time with non-sparse coefficients. The eigenvectors are a complex linear combination of parameters, raising questions about their significance in learning dynamics and feature representation. Additionally, the evolution of the maximal eigenvalue is plotted for both the fully-connected and ResNet-18 architectures, showing initial growth followed by stabilization. The eigenvalues of the fully-connected MNIST model and ResNet-18 architecture show initial growth followed by decay. The eigenvectors in the final layer of the fully-connected model evolve with non-sparse coefficients, indicating meaningful changes in learning dynamics and feature representation. The eigenvector is changing in time and not dominated by any specific parameter. The size of the nearly-preserved subspace is related to the number of classes, as shown in the Hessian spectrum. The gradient lies in a subspace spanned by the top-k eigenvectors, which are nearly preserved during training. The size of the nearly-preserved subspace is related to the number of classes, as shown in the Hessian spectrum. Investigating whether the subspace is k-dimensional, results show interesting behavior as the subspace dimension increases past the number of classes. Subspaces with dimensions less than 10 seem to be preserved amongst themselves, while top 15 and top 20 subspaces are significantly less preserved. The study shows that subspaces with dimensions less than 10 are preserved amongst themselves, with the largest preserved subspace peaking around the number of classes. Additional structure is suggested as eigenvectors no longer rotate as much during training. The overlap of the gradient with eigenvectors is analyzed in different points of training. The study analyzes the overlap of the gradient with eigenvectors at different training points. Additional experiments show the gradient quickly converges to the top subspace and remains there. Various scenarios like changing learning rate, batch size, hidden layers, activation function, label permutation, and optimizer are explored. The reduced case of a 2-sample, 2-class problem learned using softmax-regression shows the loss function approaching its minimum value at a late training stage. The loss function has flat directions, limiting the rank of the Hessian to at most 2. The optimization trajectory is solved by training the model using gradient descent with a small learning rate limit. The parameters evolve in continuous time, showing a general solution for the optimization trajectory. The optimization trajectory for the 2-sample, 2-class problem using softmax regression reaches its minimum loss value in late training stages. The solution space has 2d-2 dimensions parameterized by constants c1,2 and \u03b81,2. The gradient converges to a constant vector independent of parameters as t approaches infinity. The Hessian has at most rank 2 with two non-trivial eigenvectors and eigenvalue (\u03b7t)^-1 in the limit t \u2192 \u221e."
}