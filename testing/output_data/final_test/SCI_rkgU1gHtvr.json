{
    "title": "rkgU1gHtvr",
    "content": "We propose estimated mixture policy (EMP), a novel method for off-policy policy evaluation with trajectory data generated by multiple behavior policies. EMP reduces variance in estimating state stationary distribution correction and provides induction bias for estimating state-action stationary distribution correction. Extensive experiments show significantly improved accuracy compared to state-of-the-art methods in both continuous and discrete environments. Off-policy policy evaluation methods (OPPE) use previously-collected trajectories to estimate the value of a new decision-making policy without interacting with the environment. This is particularly useful for scenarios where directly executing a novel policy is costly or risky. State-of-the-art methods for infinite-horizon off-policy policy evaluation focus on learning state stationary distribution corrections or ratios. State-of-the-art off-policy policy evaluation methods focus on estimating the likelihood ratio of long-term state visitation probabilities between target and behavior policies, avoiding high variance compared to classic importance sampling methods. Policy-aware methods require detailed information on behavior policy distributions, making them challenging to apply with pre-generated off-policy data. Nachum et al. (2019) introduced DualDice, a policy-agnostic method for off-policy data generated by multiple behavior policies. In contrast, this paper proposes EMP, a partially policy-agnostic method for off-policy policy evaluation with known or unknown behavior policies. EMP utilizes aggregated information from behavior policies without requiring detailed information on each individual policy. EMP is a method for off-policy policy evaluation that includes a pre-estimation step to learn a \"virtual\" policy. It obtains OPPE by learning state stationary distribution correction, remaining computationally cheap and scalable. EMP yields smaller mean square error than policy-aware methods and learns the state stationary distribution correction of smaller dimension compared to DualDice. The estimation of the mixture policy is an inductive bias for stationary distribution correction, potentially improving performance when pre-estimation is not costly. An ad-hoc improvement of EMP is proposed, showing significant enhancement in comparison to policy-aware and policy-agnostic methods across various control tasks. The general setting of OPPE in infinite horizon is introduced, followed by a review of OPPE methods based on importance sampling and stationary distribution correction learning for estimating infinite-horizon average reward in a Markov Decision Process environment. The policy interacts with the environment using a reward function and transition probability function. Off-policy policy evaluation estimates the expected reward of a target policy using data from different behavior policies. The data is generated by multiple behavior policies, with most literature focusing on the single-behavior-policy case. In the single-behavior-policy case, OPPE methods can be categorized into importance-sampling (IS) based and stationary-distribution-correction based methods. IS methods have shown promising results for short-horizon off-policy policy evaluation by using importance weighting to correct policy mismatches. The EMP method is inspired by the importance sampling literature, which suggests that using estimated behavior policy in the importance weighting can reduce mean square error. The EMP method differs from previous works as it is not IS-based and focuses on multiple-behavior-policy settings. State-of-the-art methods for off-policy policy evaluation use stationary-distribution correction to avoid variance issues from IS. When the behavior policy is unknown, estimating it is a natural idea. In this section, the BCH method is reviewed for behavior policy estimation in the stationary distribution correction learning task. Estimating the behavior policy is beneficial for extending the method to settings where the policy is unknown and reducing mean squared error even when the policy is known. The BCH method is reviewed for behavior policy estimation in the stationary distribution correction learning task. It solves a min-max problem using a kernel method to calculate the stationary distribution correction \u03c9. The BCH method, utilizing a kernel method, is enhanced by employing estimated behavior policy for improved asymptotic Mean Squared Error (MSE). This improvement is crucial for the performance of the EMP method, as discussed in Section 4. The stationary distribution correction is learned through a min-max problem, leading to smaller MSE with estimated behavior policy. The stationary distribution correction is improved by using estimated behavior policy, leading to better estimates. Theoretical guarantees and finite-sample error bounds are derived, showing the benefits of employing estimated behavior policy in the min-max problem. In this section, the EMP method is developed for OPPE in settings of multiple behavior policies, with theoretic variance reduction results established. The data from different behavior policies can be pooled together as if they are generated by a virtual \"mixture policy\" \u03c0 M, which is crucial for the EMP method derivation. The data from different behavior policies can be pooled together as if they are generated by a virtual \"mixture policy\" \u03c0 M. This mixture policy is a weighted average of the behavior policies, allowing for a distribution function that is a mixture of state-action-next-state tuple distributions generated by each behavior policy. The mixture policy \u03c0 M is defined as a weighted average of behavior policies, generating state-action-next-state tuples following a distribution based on the likelihood ratio of target policy over the mixture state distribution. The average reward can be estimated by \u03c9 M, and the state distribution generated by multiple behavior policies equals the state stationary distribution generated by \u03c0 M. By knowing \u03c0 M, we can learn \u03c9 M(s) via a min-max problem and estimate R \u03c0 using the state distribution generated by multiple behavior policies. In the context of estimating the average reward and state distribution in reinforcement learning, the EMP method aims to directly estimate the mixture policy \u03c0 M without learning behavior policies \u03c0 j and their stationary distributions. The estimated mixture policy belongs to a parametric family and is used to formulate a min-max problem to learn the state distribution correction \u03c9 M. The method involves pre-estimating \u03b8 M and replacing the exact mixture policy with the estimated one to learn \u03c9 M. The EMP method pools data from different policy behaviors to treat them as a single mixture policy, making it applicable to settings with minimal information on behavior policies. This pooling feature not only addresses the lack of behavior policy information but also leads to variance reduction intrinsically. Treating data separately according to behavior policies allows for the use of EMP or other OPPE methods to obtain stationary distribution corrections for each behavior policy. The EMP method utilizes multiple importance sampling (MIS) for variance reduction by optimizing the heuristic function. By pooling data and learning the optimal MIS weight, EMP achieves variance reduction in OPPE problems across various discrete-control tasks. In this study, EMP is evaluated on OPPE problems in discrete-control tasks Taxi, Singlepath, Gridworld, and continuous-control task Pendulum. The comparison is made with existing OPPE methods, validation of theoretical properties, and exploration of potential improvements. Codes will be released with the publication of the paper. The EMP method is compared with BCH, DualDice 2, and step-wise importance sampling in the single-behavior policy setting. A single behavior policy is learned for evaluating BCH and IS, generating trajectories for analysis. The EMP method is compared with BCH, DualDice 2, and importance sampling in a single-behavior policy setting. Trajectories are generated to estimate behavior policy and stationary distribution corrections. Results show that EMP converges faster and has smaller MSE compared to other methods in different environments. The EMP method outperforms BCH, DualDice, and importance sampling in different environments, showing smaller MSE. Performance improves with more trajectories and longer horizons. EMP consistently obtains smaller MSE than other methods. Comparing EMP with its variation EMP (single) shows that pooling data is beneficial for variance reduction. In this study, the EMP method is shown to outperform EMP (single) in reward estimation, indicating variance reduction benefits. A variation of EMP, KL-EMP, further reduces variance by re-weighting data samples based on KL-divergence between behavior and target policies. Results demonstrate improved performance with larger sample sizes and advocate for partial policy-awareness in off-policy policy evaluation. Theoretical and experimental results show the effectiveness of off-policy policy evaluation methods in reducing variance. Future studies aim to improve efficiency by estimating individual behavior policies and computing optimal weights. The use of reproducing kernel Hilbert space helps solve the mini-max problem of BCH, leveraging the reproducing property for closed form representation. In this appendix, the closed form representation of max f \u2208F L(w, f ) 2 is derived using the reproducing property. The behavior policy is assumed to belong to a class of policies with parameter space E \u03b8. The estimated behavior policy is obtained through maximum likelihood method. Regularity conditions on G are assumed for the estimation process. The BCH method estimates \u03b8 based on Assumption 2 and regularity conditions on G. Theorem 1 shows that, asymptotically, \u03b7 is the solution to the optimization problem. The proof involves deriving key equations and showing the validity of Corollary 1 under certain assumptions. The central limit theorem is applied to the maximum likelihood estimator \u03b8 under certain conditions. Propositions 1 and 2 relate to the distribution and average reward generated by a behavior policy. Theorems 1 and 2 discuss the stationary distribution correction and the convergence of the optimization problem. The proof of Theorem 2 follows from that of Theorem 1. Gridworld is a 4x4 grid environment with different states and actions. SinglePath has 5 states and 2 actions, with rewards based on transitions. Pendulum has a continuous state space and action space. Taxi is a discrete environment. For the continuous environment Pendulum, a neural network is used to model the policy. A two-layer MLP neural network with hidden layers of size 32, learning rate 0.001, and tanh activation function is trained to estimate the policy using MLE and Adam optimizer with batch size 128. In the EMP algorithm, weights are optimized based on KL-divergence between behavior and target policies to generate a new data buffer. Sampling is done based on these weights to update the data buffer. In the EMP algorithm, weights are optimized using KL-divergence between behavior and target policies to update the data buffer. Sampling is then performed based on these weights to generate new data. To implement this method for infinite state space, a neural network is used to estimate the policy from the subgroup of data generated by each policy. The numerical results demonstrate that using KL weights {w KL j } leads to smaller Mean Squared Error compared to using {w j } from the data sample. The policy-aware versions of EMP algorithms, such as EMP (single) and KL-EMP, have their analogues named BCH (pooled) and BCH (KL-polled). These versions implement variance reduction through policy estimation and utilize different methods to optimize the data pooling process. The BCH (KL-polled) method uses KL-divergence to optimize weights, outperforming policy-aware analogues."
}