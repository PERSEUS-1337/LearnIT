{
    "title": "ryg8WJSKPr",
    "content": "Delusional bias in approximate Q-learning can be mitigated by training Q-approximators with labels consistent with the greedy policy class. A penalization scheme is introduced to encourage Q-labels to remain consistent across training batches. Multiple Q-approximators can be generated and tracked to avoid premature policy commitments. Experimental results show improved performance in Atari games. Q-learning is essential in deep reinforcement learning success. Q-learning, a key component in deep reinforcement learning success, has seen recent advancements but still faces challenges with stability and convergence when combined with function approximation. Various modifications have been proposed to address these issues, but achieving both robustness and scalability remains difficult. Recent research has identified a source of error in Q-learning with function approximation. Lu et al. (2018) discovered delusional bias in Q-learning with function approximation, causing unbounded errors and undesirable behavior. They introduced a policy-consistent backup operator to manage multiple Q-value estimates and prevent premature convergence on specific policies. CONQUR (CONsistent Q-Update Regression) is proposed as a framework to address delusional bias in Q-learning with function approximation. It integrates policy-consistent backups with regression-based function approximation to minimize the effects of bias and allow scaling to practical problems. CONQUR introduces novel augmentations to standard Q-regression to enhance policy consistency across training batches. It includes a soft-consistency penalty, a search space over Q-regressors, and heuristics for guiding the search. Experimental results on the Atari suite show significant improvements over Q-learning, demonstrating the effectiveness of CONQUR in reducing bias and scaling to practical problems. The text discusses Q-learning with a function approximator to learn an optimal Q-function in a Markov decision process. Batch versions like DQN fit a regressor to training examples, offering data efficiency and stability over online methods. Batch Q-learning uses a sequence of data batches to estimate the Q-function, fitting regressors to training data to derive the greedy policy. Delusional bias occurs when backed-up value estimates are derived from unrealizable action choices. The delusional bias in Q-learning arises when backed-up values are not realizable due to inconsistent action choices. Lu et al. (2018) propose a non-delusional policy consistent Q-learning algorithm to address this issue and ensure learning of the optimal representable policy. The concept of policy consistency is crucial in this context. In Q-learning, an assignment associates actions with states. A policy is policy consistent if a greedy policy matches the assignment. State-action pairs can be equated to assignments. Q-learning uses labels generated by assuming maximizing actions are taken. Training involves bootstrapping on previous regressors. The PCQL algorithm prevents Q-learning from estimating values based on actions that are not policy consistent with earlier assignments. Q-labels are generated using actions that may not be maximizing relative to the prior regressor, and tradeoffs are made when deciding which actions to assign to different states. PCQL maintains multiple information sets for different state assignments during training, resulting in multiple Q-function estimates and hypotheses. It provides strong convergence guarantees but is a tabular algorithm that restricts the policy class and does not generalize Q-values. Theoretical guarantees come at a cost of exact policy consistency tests, which are not practical for large problems. The CONQUR framework addresses limitations of PCQL by introducing a practical soft-constraint penalty to promote policy consistency, a structured search space for regressors, and heuristic search schemes to find good Q-regressors. It works with training data divided into batches for training, accommodating online and offline RL scenarios. The CONQUR framework introduces a soft-constraint penalty to promote policy consistency and uses a structured search space for regressors. It works with training data divided into batches for training, accommodating online and offline RL scenarios. The action assignment \u03c3 D for a data batch D dictates the \"maximum\" action for generating a Q-label, enforcing strict \u0398-consistency as regressors. Enforcing strict \u0398-consistency as regressors are generated is computationally challenging. Maintaining consistency when generating new regressors imposes requirements on assignment and regression optimization. Enforcing consistency in regression optimization is computationally challenging. A penalty term is proposed to encourage updates of Q-regressors to be consistent with prior action assignments. This penalty function is added to the squared loss to ensure consistency with the underlying information set. The consistency buffer penalty is incorporated into the regression loss to promote consistency with prior action assignments, without the need for strict consistency testing. This penalty acts as a regularizer on the squared Bellman error, allowing a tradeoff between Bellman error and consistency. The consistency penalty in Q-learning update allows for flexibility in prior action assignments without strict consistency testing. The consistency buffer can be populated in various ways to balance between full consistency and adapting to more recent data during training. The proposed consistency penalty in Q-learning update aims to discourage inconsistencies in the greedy policy induced by the Q-estimator, regardless of the actual estimated values. This penalty differs from the temporal-consistency loss by Pohlen et al. (2018), as it focuses on tracking all \u0398-consistent assignments in a more practical manner. In CONQUR, information set tracking is reframed as a search problem with various strategies proposed for managing the search process. The search space and its properties are defined, along with discussing search procedures. Training data is divided into batches, with an initial Q-function estimate for bootstrapping labels. The regressor for each batch can be trained with actions assigned to successor states, leading to different updated Q-estimators. Conditions for \"reasonable\" action assignments are discussed, including strict consistency requirements of PCQL. The search space in CONQUR is defined as a generic space for finding policy-consistent Q-functions. Conditions for action assignments are discussed, with strict consistency requirements of PCQL relaxed for practical reasons. Nodes in the search tree are defined based on regressors and training data sets, with a depth-first recursive specification provided in the appendix. The search tree has an exponential branching factor. The search tree in CONQUR has an exponential branching factor, but the size can be bounded by the VC-dimension of the approximator. The number of nodes in the tree is at most VCDim(G), where G is the set of boolean functions defining feasible greedy policies. Different search methods are used instead of exhaustive search due to the impracticality of navigating the search space. Various search methods are used to explore the search space in CONQUR, aiming to reach a high-quality regressor at a leaf of the tree. Key considerations include child generation, node evaluation, and the search procedure. The impracticality of exhaustively traversing the search space is acknowledged, leading to the use of different search methods. In CONQUR, search methods are utilized to explore the search space efficiently for a high-quality regressor at a leaf node. Heuristics are employed to generate a small set of children, focusing on high-value assignments and ensuring diversity among them. The goal is to balance between maximizing Q-values and maintaining consistency in the assignments. In CONQUR, search methods aim to find a high-quality regressor at a leaf node by generating children with high-value assignments. Policy commitments at each stage constrain future assignments, emphasizing flexibility and consistency. Sampling action assignments using a Boltzmann distribution is a natural technique employed to achieve this goal. In CONQUR, search methods aim to find a high-quality regressor at a leaf node by generating children with high-value assignments. The consistency penalty is used to encourage consistency in constructing the regressor \u03b8 k for each child. The temperature parameter \u03c4 controls the focus on maximizing assignments versus diverse, random assignments. Stochastic sampling biases selection of high-value actions to states that occur early in the permutation, so a new random permutation is used for each child to ensure diversity. Scoring children is necessary to evaluate the quality of each child for expansion. Search methods in CONQUR aim to find a high-quality regressor at a leaf node by generating children with high-value assignments. Techniques such as using average Q-label, Bellman error, or loss incurred by the regressor can be employed to score children for expansion. Care must be taken when comparing nodes at different depths of the search tree, and rollouts of the induced greedy policy can be used to evaluate node/regressor quality. Different search procedures can be applied based on the scoring method used. In the CONQUR framework, various search procedures like best-first search, beam search, and local search can be utilized to generate and score children efficiently. A modified beam search strategy with backtracking and priority scoring is outlined to navigate the large search space in practical RL settings. This approach reduces the search tree size significantly while adding only a constant-factor slowdown. In Algorithm 2 (see Appendix B), key refinements are outlined for the search process in the CONQUR framework. The tree is grown in a breadth-first manner, alternating between expansion and dive phases. In the expansion phase, parent nodes generate child nodes with action assignments sampled from the Boltzmann distribution, optimizing the child's regressor using consistency-penalized Bellman error. The dive phase involves generating one child per parent with action assignments selected by the parent node's regressor. Consistency is promoted through consistency-penalized regression. The search starts with an expansion phase creating c children, where c is the splitting factor. The search process in the CONQUR framework involves expanding the tree with c children inheriting their parent's consistency buffer. Frontier nodes are selected for expansion based on scoring functions. Backtracking strategies and ensembling multiple Q-approximators have been considered to reduce instability and variance in reinforcement learning. In the CONQUR framework, the search process involves expanding the tree with children inheriting their parent's consistency buffer. Techniques like backtracking and ensembling Q-approximators are used to reduce instability and variance in reinforcement learning. Population-based methods inspired by evolutionary search have also been explored to improve hypothesis diversity and quality in RL. However, these techniques do not systematically target weaknesses like delusion in Q-learning. The performance of CONQUR is assessed using the Atari test suite, with experiments on the impact of consistency penalty and modified beam search. The study evaluates the impact of introducing soft-policy consistency in DQN and DDQN algorithms, termed DQN(\u03bb) and DDQN(\u03bb) respectively. The penalty coefficient \u03bb is defined to demonstrate the effectiveness of the consistency penalty. A small consistency buffer is maintained using the current data batch to prevent premature policy constraints. This modification can be easily applied to any regression-based Q-learning method. The study introduces soft-policy consistency in DQN and DDQN algorithms, DQN(\u03bb) and DDQN(\u03bb) respectively. The penalty coefficient \u03bb is adjusted during training to prevent premature policy constraints. Training and evaluating on 19 Atari games show that the best \u03bb varies depending on the game, with \u03bb = 0.5 generally working well. Increasing \u03bb can improve performance in some games like Gravitar, while \u03bb = 0.5 is optimal for SpaceInvaders. Consistency penalization improves DQN and DDQN performance by addressing delusional bias in learning policies. DQN(\u03bb) and DDQN(\u03bb) outperform baselines in most games, with \u03bb = 0.5 showing consistent improvement. Double Q-learning addresses maximization bias differently from consistency penalization. The CONQUR framework, when applied to DQN and DDQN, shows performance gains over baselines in various games. Utilizing pre-trained networks from the Dopamine package, CONQUR retrains only the last layer for testing purposes. This approach acts as a linear Q-approximator, achieving results comparable to full Q-network training. CONQUR is utilized with pre-trained networks from the Dopamine package, retraining only the last layer for testing purposes. It acts as a linear Q-approximator, showing performance gains over baselines in various games. Different parameters are tested, and regressors are trained using a specific loss function. The best performing policy is selected based on different scoring approaches. CONQUR, utilizing pre-trained networks from the Dopamine package, retrained only the last layer for testing. It acts as a linear Q-approximator, showing performance gains over baselines in various games. Different parameters are tested, and regressors are trained using a specific loss function. The best performing policy is selected based on different scoring approaches. In Table 1, a selection of games is shown. Rollouts and consistent-Bellman scoring are compared, with the latter being less computationally intense. CONQUR is compared with a pre-trained DQN and a \"multi-DQN\" baseline on 59 games, resulting in significant improvements to the pre-trained DQN. The retraining of the last layer of a pre-trained DQN network, as described in the Appendix D, led to significant improvements in policy performance. An average score improvement of 125% was achieved, with notable gains in games like Solaris, Tutankham, and WizardOfWor. CONQUR outperformed the multi-DQN baseline in 20 games by at least a 10% margin, with improvements ranging from 1-10% in 22 games. Some games showed little effect or a decline in performance. Results from comparing CONQUR and multi-DQN with 8 nodes show varying effects on game performance, with some games improving by 10% or more, while others perform comparably or worse. Increasing the number of nodes to 16 generally leads to better performance for CONQUR, with more games achieving higher scores. The impact of increasing nodes is illustrated in Fig. 3. The value of the best frontier node is shown to improve with CONQUR compared to multi-DQN in both Alien and Solaris games. CONQUR nodes outperform multi-DQN nodes, with significant score improvements. Varying \u03bb in Alien game shows performance improvement up to a certain point before declining. CONQUR, a framework for mitigating delusional bias in value-based RL, shows improved performance compared to multi-DQN in Alien and Solaris games. Stronger penalization in Atlantis degrades performance, emphasizing a trade-off between consistency and action assignments. CONQUR incorporates a tree-search procedure and consistency penalty to bias the value estimator for scalability. CONQUR improves existing approximators by removing delusional bias and applying a consistency penalty in DQN or DDQN. The framework offers flexibility for future research directions, including exploring different methods for nudging regressors to be policy-consistent and examining various search strategies within CONQUR. The methods described can be extended to continuous actions and soft max-action policies. There is a potential connection between maintaining multiple \"hypotheses\" and distributional RL. An example from a study shows how delusional bias affects Q-learning with function approximation. The optimal feasible policy in G(\u0398) takes a 1 at s 1 and a 2 at s 4. Online Q-learning with data generated using an \u03b5-greedy behavior policy converges to a fixed point corresponding to a \"compromise\" admissible policy. Q-learning fails to find a reasonable fixed-point due to delusion, as shown by the backups at (s 2 , a 2 ) and (s 3 , a 2). Q-learning struggles to find the optimal policy due to delusional bias, affecting the value estimation and action assignment. DQN and DDQN differ in how they use the Q-network for label generation. In DQN, the Q-network is used for both value estimate and action assignment, while in DDQN, it is only used for value estimation. The DQN and DDQN algorithms differ in their consistency of assignments. DQN maintains consistency for all batches, while DDQN can have inconsistent assignments due to updating the network at every step. Modifications are made to DqnAgent and DdqnAgent by adding a consistency penalty term to the original TD loss. The TF-Agents implementation of DQN training on Atari with default hyperparameters is used. The modified DQN and DDQN agents trained with consistency penalty on 15 Atari games are empirically evaluated using the TF-Agents framework. Results show the effects of varying \u03bb on both DQN and DDQN, with Table 3 summarizing the best penalties for each game. Training curves for the best penalization constants and a fixed penalization of \u03bb = 1/2 are also shown. During training, a frontier queue of size 8 or 16 is used to select top scoring leaf nodes for updates. Training batches are generated using \u03b5-greedy from the best node's regressor. Bellman error plus consistency penalty is used as the scoring function, with calibration for depth differences between leaf nodes. The scoring is adjusted by comparing current nodes in the frontier with their parents and scaling the difference by a constant of 2.5. During training, a frontier queue of size 8 or 16 is used to select top scoring leaf nodes for updates. Q-labels are generated using action assignments from Boltzmann sampling to create high-quality children. The expansion phase occurs at specific iterations, with the dive phase in between. The process involves multiplying the difference between current nodes and their parents by a constant of 2.5. During training, Q-labels are generated using Boltzmann sampling every fifth iteration, with the rest following standard Q-learning. Each iteration consists of 10k transitions, totaling 1M transitions over 100 iterations. RMSProp optimizer with a learning rate of 2.5 \u00d7 10 \u22126 is used, with 2.5k gradient updates per iteration and a batch size of 32. The target network is replaced every fifth iteration, rewards are clipped between [\u22121, 1], and discount value \u03b3 = 0.99 with \u03b5-greedy exploration (\u03b5 = 0.01). During training, Q-labels are generated using Boltzmann sampling every fifth iteration, with the rest following standard Q-learning. The evaluation process involves testing the agents every 10th iteration, with a cap on episode length at 108k frames. The evaluation policy uses \u03b5-greedy with \u03b5 = 0.001. CONQUR's training curves with 16 nodes under different penalization strengths are shown in Fig. 11, while Tables 4 and 5 summarize the highest policy values achieved for all 59 games under 8 and 16 nodes. CONQUR and the baseline show improvements in all 59 games under 8 and 16 nodes. CONQUR's advantage over the baseline is amplified, especially in more deterministic MDP environments. Results on 16 and 8 nodes use a splitting factor of 4 and 2, respectively. Training parameters include \u03b5-greedy policy for exploration and evaluation, with a mini-batch size for Qnetwork training. Calibration adjusts differences between nodes not selected during expansion or dive phases. The calibration process in CONQUR adjusts differences between nodes not selected during expansion or dive phases based on the average difference between frontier nodes and their parents, using a discount factor of 0.99 during training."
}