{
    "title": "r1lfga4KvS",
    "content": "Clustering is a key task in unsupervised learning and data mining, with k-means being a widely used algorithm. However, extending k-means to non-Gaussian data points with non-convex shapes is challenging. A novel algorithm called Extreme Value k-means (EV k-means) is introduced for this purpose, utilizing Extreme Value Theory (EVT) to transform the Euclidean space into an extreme value space. Tricks to accelerate Euclidean distance computation are also proposed for improved computational efficiency. Additionally, an online version of EV k-means is developed to cluster streaming data using Mini Batch k-means. The k-means algorithm is widely used in clustering data samples of high similarity. Extensive experiments validate the effectiveness of Extreme Value k-means (EV k-means) and online EV k-means on synthetic and real datasets, outperforming competitors in most cases. The k-means algorithm is commonly used for clustering data samples based on similarity. However, it has limitations such as high computational complexity and sensitivity to initial centroids. To address these issues, this paper aims to enhance the clustering ability of k-means. This paper enhances the clustering ability of k-means by measuring similarity between samples and centroids using GEV and GPD distributions. GEV and GPD help transform Euclidean space into a probability space, indicating the similarity of a sample to a centroid. BMM and POT methods are employed to fit the data for GEV and GPD, respectively. The Extreme Value k-means (EV k-means) algorithm utilizes GEV and GPD distributions to measure similarity between samples and centroids in clustering. It is a probability-based clustering algorithm that clusters samples based on the probability output from GEV or GPD. To accelerate computation, samples and centroids are expanded into tensors of the same shape and processed using GPU parallel computing. For streaming data, an online Extreme Value k-means based on Mini Batch kmeans is proposed, with parameters learned using stochastic gradient descent. The paper introduces Extreme Value k-means (EV k-means) using GEV and GPD distributions to improve clustering of non-convex data shapes. It proposes novel methods for accelerating Euclidean distance computation and transforming space into extreme value space. Online Extreme Value k-means is also presented for clustering streaming data, with parameters learned online. Experimental results validate the effectiveness of EV k-means algorithms. The experimental results demonstrate that EV k-means and online EV k-means outperform other algorithms across various datasets. Previous research on k-means has focused on aspects like determining optimal k, centroid initialization, and acceleration methods. Various approaches have been proposed for selecting the optimal k value and initializing centroids, with k-means++ being a popular choice. The curr_chunk discusses the use of density-based centroid selection in k-means, as well as methods to accelerate k-means such as using Markov chain Monte Carlo and triangle inequality. It also mentions the application of Extreme Value Theory (EVT) in various fields but highlights the lack of attention to combining k-means with EVT. Li et al. (2012) proposed using EVT for feature learning in k-means, but the current method differs significantly. The method discussed in the curr_chunk differs significantly from previous work on combining k-means with Extreme Value Theory (EVT). It involves computing distances from centroids to all data points and fitting GEV or GPD for each centroid, assigning data points based on probabilities. Additionally, the paper introduces GPD k-means and online Extreme Value k-means, not mentioned in previous research. The sum squared error is used to measure closeness between samples and centroids in clusters. The curr_chunk discusses the NP-hard problem of finding the global minimum in k-means clustering using Lloyd's algorithm. It explains the iterative process of assigning cluster labels and updating centroids until convergence. Additionally, it introduces the statistical aspects of Extreme Value Theory for analyzing extreme events in probability distributions. The asymptotic behavior of sample maximum and upper order statistics in Extreme Value Theory is explored, with a focus on the Fisher-Tippett Theorem. This motivates the Block Maxima Method for analyzing extreme events in probability distributions. The Block Maxima Method (BMM) divides the sample into blocks of length s to approximate block maxima with a three-parametric GEV distribution. The Pickands-Balkema-de Haan Theorem defines the maximum domain of attraction condition for distribution functions. The clustering results of GPD k-means are shown in three isotropic Gaussian blobs. The clustering results of GPD k-means show three isotropic Gaussian blobs, with colors indicating probability density. The upper order statistics provide information about the tail of the distribution. The peak-over-threshold (POT) approach approximates the conditional distribution for values above a threshold with a two-parametric GPD. The POT approach focuses on excess over the threshold to characterize tail features, while the BMM approximates the GEV distribution for large enough blocks. The BMM approximates the GEV distribution for large blocks, while the POT method uses all data beyond a threshold to fit the GPD, making full use of extreme data. The Extreme Value k-Means Algorithm proposes using EVT to measure similarity for k-means clustering, addressing the limitations of Euclidean distance for non-convex clusters. The paper proposes using Extreme Value Theory (EVT) to transform Euclidean space into a probability space for clustering non-convex clusters. By fitting GEV distributions using the BMM, the EVT can model maximum distances and reflect the probability of a distance belonging to a cluster, improving similarity measurements for k-means clustering. The GEV distribution is fitted for each cluster by computing the Euclidean distance between centroids and samples. The distances are divided into blocks to obtain block maximum sequences used to estimate GEV parameters. Maximum likelihood estimation is then used to estimate the parameters of the GEV distribution for each cluster. The GEV distribution parameters are estimated using maximum likelihood estimation for each cluster. The GPD parameters are also estimated using MLE. The probability of a sample belonging to a cluster is determined using the GEV and GPD distributions. The Extreme Value k-means (EV k-means) algorithm models the distribution classes of GEV and GPD to measure similarity between samples and centroids. It clusters samples based on probability output from GEV or GPD, with block size and threshold affecting MLE deviation and variance. Choosing hyperparameters requires balancing deviation and variance, done through grid search for block size and adaptive threshold setting. The GEV k-means algorithm sets block size and threshold adaptively using hyperparameter \u03b1 to indicate excess percentage. It fits a GEV distribution to the dataset, estimates parameters using MLE, assigns cluster labels based on maximum probability, and updates centroids iteratively. The GPD k-means algorithm uses POT to model excess distances exceeding a threshold, fitting a GPD distribution. It shows clustering results in isotropic Gaussian blobs, highlighting the importance of proximity to centroids. The main bottleneck is the computation of Euclidean distances, which is addressed by an accelerated computation method proposed in the paper. The Extreme Value k-means algorithm accelerates Euclidean distance computations between samples and centroids using GPU parallel computing. In the era of Big Data, clustering streaming data poses a significant challenge, necessitating the design of an Extreme Value k-means approach. The paper proposes an online Extreme Value k-means algorithm for clustering streaming data based on Mini Batch k-means. It fits the GEV distribution using mini batch data and updates the threshold and parameters of GPD dynamically. The algorithm randomly selects a mini batch, initializes parameters, computes Euclidean distances using an accelerated method, and updates values accordingly. The proposed algorithm updates parameters of GEV or GPD using Mini Batch kmeans clustering. It computes maximum values and excess, clusters samples, and minimizes negative log-likelihood function to update parameters. Performance is evaluated using ACC, NMI, and ARI metrics. The proposed algorithm updates parameters of GEV or GPD using Mini Batch kmeans clustering to compute maximum values and excess, cluster samples, and minimize negative log-likelihood function for parameter updates. Performance evaluation is done using ACC, NMI, and ARI metrics. The percentage of excess \u03b1 for GPD k-means Output clusters C by initializing centroids and performing transformations on X and C to obtain D, followed by GEV k-means and GPD k-means steps. The visualization of six synthetic datasets shows the results of Extreme Value k-means compared to other clustering methods. The clustering results of six datasets (D1-D6) using nine algorithms are shown, with ARI values ranging from -1 to 1. D1-D6 contain different types of clusters with varying sample sizes. The proposed algorithm is compared to others on two-dimensional synthetic datasets. D3, D4, and D5 datasets consist of clusters with varying sample sizes and shapes. Different algorithms, including k-means and spectral clustering, are used for clustering analysis. The centroids can be initialized randomly or using k-means++. Our four algorithms (GEV k-means (RM), GEV k-means (++), GPD k-means (RM), GPD k-means (++)) successfully cluster convex and non-convex clusters, outperforming k-means and k-means++ on non-convex but well-separated clusters. Additionally, our algorithms perform better than k-medoid, bisecting k-means, and spectral clustering on certain datasets. The proposed EV k-means was evaluated on nine real datasets, including iris, breast cancer, live disorders, heart, diabetes, glass, vehicle, MNIST, and CIFAR10. The MNIST dataset consists of 60,000 training grayscale images and 10,000 test images of handwritten digits 0 to 9, represented by 84-dimensional vectors. The CIFAR10 dataset contains 50,000 training and 10,000 test color images with 32x32 pixels, represented by 512-dimensional vectors. Each experiment was repeated 10 times with different random seeds, and the mean of the results was taken as the final outcome. Our proposed EV k-means algorithm shows comparable performance to other algorithms on real datasets, outperforming them on MNIST and CIFAR10. Online EV k-means has slightly smaller metrics compared to EV k-means, while Mini Batch k-means has significantly smaller metrics than k-means, with values on MNIST being 10%, 17%, and 8% smaller."
}