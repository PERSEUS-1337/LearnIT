{
    "title": "SyGT_6yCZ",
    "content": "The quality of visual features is crucial for recognition systems. Traditional methods like SIFT and HOG have been effective, but convolutional neural networks are now used for feature extraction. However, training time is a drawback compared to hand-designed features. A new method for training supervised convolutional models for feature extraction is proposed in this paper, maintaining high quality while being simple and fast. This approach is evaluated on various datasets and compared to state-of-the-art methods. High-quality image features are essential for accurate and scalable vision recognition tasks. Various approaches have been proposed for building visual features, including dictionary learning, scattering approaches, unsupervised bag of words methods, and unsupervised deep learning techniques. These methods aim to improve upon hand-crafted feature vectors like SIFT and HOG. Another successful alternative is using supervised Convolutional Neural Networks (CNN) for high-quality image feature extraction. In this paper, a Simple Fast Convolutional (SFC) feature learning technique is proposed to reduce the time required for supervised convolutional feature learning without sacrificing performance. The technique aims to address the drawback of the time-consuming training process of Convolutional Neural Networks (CNN) for high-quality image feature extraction. The proposed Simple Fast Convolutional (SFC) feature learning technique aims to reduce training time for supervised convolutional feature learning while maintaining performance. By combining SFC with classical classifiers like SVM and ELM, better performance was achieved compared to alternative methods, with significant reduction in training time. The evaluation was done on MNIST, CIFAR-10, and CIFAR-100 datasets, showing that SFC outperforms alternative approaches in terms of performance and training time efficiency. The proposed Simple Fast Convolutional (SFC) feature learning technique aims to reduce training time for supervised convolutional feature learning while maintaining performance. The innovation in learning feature fast involves designing a step decay schedule based on the total number of epochs predicted to train the model. This schedule is optimized for the specific training time available, ensuring high-quality features can be generated in as few as 10 epochs. The SFC method adjusts the step decay schedule according to the time constraints, allowing for efficient training of convolutional neural networks. The optimal step decay schedule is crucial for fast training of convolutional neural networks. Inspired by recent work, it is observed that deep networks undergo two learning phases - a fast initial training phase followed by a slower fine-tuning phase. During the initial phase, each layer learns to preserve input information, enhancing mutual information between layers and input/output. The subsequent phase stabilizes this mutual information, allowing layers to prioritize important information for output mapping. The optimal step decay schedule is crucial for fast training of convolutional neural networks. The first phase of network training requires a high learning rate for a large percentage of epochs to generate high-quality features quickly. In contrast, the second phase involves exponential decay of the learning rate for fine-tuning the models. The proposed SFC method involves setting the first learning rate decay after 60% of the predicted training epochs. After training the convolutional network and learning features, the last fully connected layer is dropped to expose nodes of the previous layer. The Simple Fast Convolutional (SFC) feature representation is obtained by applying the target image to the input layer of the model. Classical classifiers can then be used on the extracted SFC features. After training the convolutional network and learning features, the Simple Fast Convolutional (SFC) feature representation is obtained by applying the target image to the input layer. Classical classifiers can then be used on the extracted SFC features. The MNIST dataset consists of 10 handwritten digits classes with 60 thousand training images and 10 thousand test images. The CIFAR-10 dataset has 10 classes with 6000 color images each, while CIFAR-100 has 100 classes with 600 images each. The baseline model for MNIST is LeNet5 with modifications to the activation function and batch normalization. The proposed LeNet5 model with batch normalization showed high performance in training time and test accuracy with only 10 epochs. The VGG19 model with 19 layers and batch normalization was used for training on CIFAR-10 and CIFAR-100 datasets. Stochastic gradient descent with Nesterov acceleration and a learning rate of 0.1 were employed for optimization. Mini-batches of 128 images each were used for training. The experiments focused on producing fast solutions with high accuracy by using SVM and ELM classifiers without data augmentation. SVM used default values and Gaussian Kernels, while ELM had 1024 nodes in the hidden layer. The labels SVM and ELM were used to refer to experiments with the respective classifiers applied directly to raw image pixels. SVM SFC10 and ELM SFC10 referred to experiments with features trained for 10 epochs. In the experiments, different training schedules were compared for CNN classifiers, including SFC10, SFC30, and SFC100, as well as classical training schedules CNN 0.1 and CNN 0.01 with varying learning rates over 100 epochs. In experiments comparing SVM SFC10 with H-ELM BID17, SFC10 showed better accuracy and reduced training time. SFC10 also provided similar test accuracy to CNN 0.1 and CNN 0.01 but was about 10 times faster. The experiments on CIFAR-10 showed that using SFC features significantly reduced training time compared to SVM and ELM. The SFC features significantly reduce training time and improve SVM and ELM classifiers' performance on the test set by 30% and 50% respectively. SFC features outperform alternative approaches in terms of test accuracy. The results show that SFC can quickly learn high-quality representations, even if not aiming for the absolute maximum test accuracy. The comparison includes SVM SFC10, ELM SFC10, and the original VGG19 model, with all results averaged over 10 runs. The SFC feature learning approach significantly reduces training time and improves classifiers' performance on the test set. Results show that SFC produces high-quality features, outperforming alternative approaches in terms of test accuracy. SFC30 provides better test accuracy than CNN 0.1 and CNN 0.01, being three times faster. The comparative study on dataset CIFAR-100 demonstrates the superiority of SFC in feature representation. The SFC variants offer better test accuracy and faster training compared to classical approaches. Convolutional feature learning can be done quickly while still generating high-performance representations. The method allows for a balance between training speed and test accuracy. Transfer learning can further extend its application. Despite efforts to improve, supervised convolutional methods still yield state-of-the-art results for image feature generation. Experiments demonstrate that adjusting the learning rate decay can significantly accelerate the training of deep neural networks."
}