{
    "title": "HJaDJZ-0W",
    "content": "Recurrent Neural Networks (RNNs) are widely used in various domains like speech recognition and machine translation. Sparse RNNs reduce compute and memory requirements, making them easier to deploy on different devices. Two approaches are investigated to induce block sparsity in RNNs: pruning weights in a layer and using group lasso regularization. These techniques allow for creating block-sparse RNNs with sparsity ranging from 80% to 90% with minimal loss in accuracy, reducing model size significantly. Block-sparse RNNs can reduce model size by 10x, maintain high block sparsity up to 32x32, and eliminate overheads related to data storage and memory accesses, increasing hardware efficiency. Improvements in speech recognition, language modeling, and machine translation are attributed to large Recurrent Neural Networks (RNNs) trained on extensive datasets. Deploying these large models is resource-intensive, but pruning weights of deep neural networks can reduce memory and compute requirements. Unstructured sparsity in weight matrices may not provide expected speed-up on hardware platforms, leading to inefficiencies. Block sparsity, which saves indices of non-zero blocks instead of elements, can address these issues by reducing storage overhead and improving hardware utilization. Block sparsity, storing non-zero blocks instead of elements, reduces storage overhead and improves hardware utilization. Structured sparsity with two-dimensional blocks allows for faster processing units utilization. A block pruning approach is proposed to induce block sparsity in RNNs during training, creating a block-sparse RNN. Group lasso regularization is also used to induce block sparsity, and it can be combined with block pruning for effectiveness. Block pruning and group lasso regularization are successful in creating block-sparse RNNs, with a 9% to 17% loss in accuracy compared to the dense baseline. Model size reduces significantly for speech recognition, and block sizes can be scaled up to 32\u00d732. The approach works with LSTM cells for Language Modelling as well, without requiring hyper-parameter retuning. Several techniques have been used to reduce network size by pruning weights in neural networks, such as bias techniques, Hessian-based approaches, sorting, and thresholding. Some approaches maintain high accuracy while pruning, while others induce random sparsity. In contrast, our technique focuses on inducing structured sparsity without the need for re-training, keeping training time constant. Structured sparsity techniques have been applied in neural networks, including pruning CNNs with Scalpel and altering LSTM structures for smaller memory footprint. Regularization methods like group lasso have been used to induce sparsity in deep neural networks, but these approaches have not been extensively applied to RNNs trained on large-scale datasets. Our approach to pruning deep learning models for RNNs trained on large-scale datasets involves introducing random, unstructured sparsity by pruning blocks of a weight matrix instead of individual weights. This method is orthogonal to other techniques such as quantization and low-rank factorization, and can be combined with them. The approach to pruning deep learning models for RNNs involves introducing random, unstructured sparsity by pruning blocks of a weight matrix instead of individual weights. The process generates a block-sparse mask from a weight matrix based on a threshold, with more blocks pruned as training progresses. Six hyper-parameters determine the threshold, including start slope and ramp slope. To achieve 90% sparsity, weights from an existing dense model are used. To achieve block sparsity in deep learning models for RNNs, weights from a dense model are used. The start slope is adjusted based on the number of elements in a block, leading to block sparsity ranging from 85% to 95%. Pruning hyper-parameters are consistent across recurrent and fully connected layers. Group lasso is a weight regularization technique that induces block-sparsity by adding a loss term proportional to the 2 norm of weight groups. It can drive weights within certain groups to zeros, making it widely used for structured sparsity in deep learning models. Group lasso is extended to work with block pruning by matching groups to the 2D grid of blocks used in the pruning algorithm. It guides the selection of blocks to prune by driving less important weights towards zero. A small \u03bb g is used to avoid underfitting, and group lasso is turned off after around 40% of training epochs. It is related to 1 regularization and is explored in combination with weight pruning in different applications such as Speech Recognition and Language Modelling. In Speech Recognition and Language Modelling applications, block sparsity experiments were conducted on two different models from BID0. The models used CTC cost function and were trained on 2100 hours of speech data. Block pruning and Group Lasso with block pruning were used to introduce block sparsity in the models. The experiments involved pruning weights in recurrent and fully connected layers. In Speech Recognition and Language Modelling applications, block sparsity experiments were conducted on two different models from BID0. The models used CTC cost function and were trained on 2100 hours of speech data. Block pruning and Group Lasso with block pruning were used to introduce block sparsity in the models. The experiments involved pruning weights in recurrent and fully connected layers. In the recurrent layers, both linear and recurrent weights, as well as fully connected layers, were pruned. Biases, batch-normalization parameters, and weights in the convolutional and CTC layers were not pruned. No hyper-parameter changes were needed for sparse training runs. Nesterov Stochastic Gradient Descent (SGD) with momentum was used for training all models for 25 epochs. Results for different sparse models pruned with 4\u00d74 blocks were reported and compared with other pruning approaches. The impact of varying block size on model accuracy was discussed. The dense RNN model was initially pruned, reducing the parameter count by nearly 10\u00d7 using BP. The block-sparse RNN model with 1760 hidden units had an overall block sparsity of 89% with a CER of 17.93. Group lasso induces block-sparsity but results in worse CER compared to block pruning. High \u03bb g values in group lasso hurt model accuracy, leading to underfitting. Our block pruning approach outperforms group lasso and other pruning methods, achieving better accuracy with 80% sparsity. The sparse RNN model generated using iterative pruning BID9 is significantly better than the block-sparse model, but requires 2-3\u00d7 more training time. Random sparsity is less efficient than block sparsity in current hardware. To maintain accuracy, sparse models with more hidden units are trained in each recurrent layer. With our approach, we train sparse models with increased hidden units in each recurrent layer. The RNN sparse models with 2560 and 3072 hidden units are only slightly worse than the dense baseline model. The overall parameter count is reduced significantly, by 5\u00d7 and 2.5\u00d7 respectively. Similarly, the block-sparse GRU model can reduce parameter count by 11\u00d7 while maintaining high sparsity. Pruning larger GRU models also shows promising results in reducing accuracy loss while shrinking the model size. Block pruning can reduce the model size significantly while maintaining good accuracy. Larger sparse blocks reduce memory overhead and can take advantage of modern processors. The choice of block size depends on the hardware used for inference, with different processors supporting varying block sizes. The approach is agnostic to block size and can generate block-sparse models for arbitrary blocks. Block pruning experiments were conducted on the Penn Tree Bank dataset using a large LSTM model with 1500 hidden units. We pruned weights in the embedding, LSTM, and softmax layers, reporting that block pruning can reduce parameter count by nearly 3x while maintaining accuracy. A trade-off exists between sparsity and accuracy, with potential for reducing parameter count by 8.3x with a 5% loss in accuracy. The choice of model depends on desired memory and compute budget for inference. Further evaluation is needed for large-scale datasets. Sparse formats incur overhead in indexing, memory accesses, and array-data-paths compatibility, mitigated by larger block sizes. For example, using a block size of 16x16 reduces memory bloat to less than 1%. Block-sparse formats optimize memory access by storing blocks contiguously, enabling large coalesced accesses. Utilizing array-data-paths in modern processors, like the Volta V100 GPU, can result in up to 8x higher throughput. To maximize performance, the block size should match the hardware data-path size (16x16 or larger on V100). Inference performance is influenced by accuracy, evaluation latency, and memory requirements, highlighting the trade-off between unstructured sparsity and block sizes. In order to understand the trade-off between unstructured sparsity and block sparsity, benchmarking was done on GEMM speed-up and memory reduction for a single layer in the speech recognition RNN model using NVIDIA's CuSparse and CuBLAS libraries on a TitanX Maxwell GPU. Sparse matrices were represented in CSR or Block-CSR format, and memory savings were calculated using CSR and Block Sparse Row (BSR) from Scipy module in Python. Different block sizes were evaluated, with the best unstructured sparsity result obtained using iterative pruning from BID9. The unstructured sparsity model BID9 achieved the best accuracy but required longer training time and did not improve compute time relative to the dense model. Block-sparse models, on the other hand, can significantly reduce both compute and memory. Block-sparse models offer significant reductions in compute and memory requirements with a small loss in accuracy. For instance, 16x16 block sparsity can decrease memory consumption by 11\u00d7 and speed up compute by 3\u00d7 with a 10% accuracy drop. Block-sparse matrices outperform unstructured sparsity in speed-up for RNN and GRU layers, thanks to improved load balance and reduced irregular memory accesses. Future work aims to efficiently implement block-sparse kernels to leverage array-data-paths in modern processors. In experiments with block sparsity, models show a trade-off between sparsity and accuracy. Models pruned with block sparsity exhibit an \"accuracy cliff\" with significant accuracy loss beyond certain sparsity levels. Larger block sizes reach this cliff sooner, indicating a balance between sparsity and block size for optimal accuracy. Using block pruning and group lasso during training can create block-sparse RNNs with fewer parameters than dense models, reducing memory requirements. Investigating early pruning during training could lead to training sparse models, benefiting from sparsity for lower compute and memory demands. Implementing efficient block-sparse matrix multiplies for modern processors could increase speed during deployment. Prior to group lasso, 1 and 1/2 regularization was considered for inducing sparsity in the network. 1 regularization and 1/2 regularization are explored to induce sparsity in the network. Weight pruning algorithm is used along with regularization to guide pruning or produce sparsity directly. 1/2 regularization aims to drive unimportant weights towards zero while leaving large weights unchanged. In experiments with 1 and 1/2 regularization, the Deep Speech 2 Bidirectional RNN model is trained for 25 epochs on a 2000-hour dataset. Results are reported on a 2.9-hour test set. Higher \u03bb is needed for 1 regularization to achieve sparsity without pruning. Pruning characteristics and their impact on training and accuracy are discussed, with a pruning schedule shown in Figure 4a. The Bidirectional model trained with Block Pruning (BP) and Weight Pruning (WP) and Group lasso with block pruning (GLP) starts pruning after the first epoch at 2700 iterations. GLP achieves 90% sparsity earlier than BP, encouraging sparsity early in training. The 94% sparse model performs worse than the 89% sparse model in terms of output connections for neurons. The model with 94% sparsity performs worse than the 89% sparse model in terms of output connections for neurons. Figure 4 shows the pruning schedule for two layers in the network for WP, GLP, and BP models, with GLP and BP models using a block size of 4x4. Figure 4b plots the histogram of the number of output connections for all neurons in the network using block pruning with 4x4 blocks. The final recurrent layer before the CTC cost layer is layer 14, which is pruned less aggressively compared to initial layers in both block pruning and weight pruning. Increasing sparsity closer to the output leads to decreased accuracy, and the variance in sparsity across layers grows with block size, making it challenging to increase block size beyond 32x32 with the same pruning hyper-parameters for all recurrent layers."
}