{
    "title": "BkxOwVShhE",
    "content": "Batch normalization (batch norm) is commonly used to stabilize and speed up training in deep neural networks. It reduces the number of parameter updates needed for low training error but also decreases robustness to adversarial perturbations and noise. Substituting weight decay for batch norm can eliminate the vulnerability to input dimension. Batch norm causes exploding gradients and makes training less sensitive to hyperparameter choices. Model robustness to unseen data is a concern despite the ease of training with batch norm. The concern about model robustness to unseen inputs during deployment has led to various methods aiming to improve it. Understanding factors that reduce robustness is crucial for developing effective methods. This was discussed at the ICML 2019 Workshop on Deep Learning Phenomena, showcasing mini-batches from the \"Adversarial Spheres\" dataset in a deep linear network with batch norm at initialization. The use of batch norm in neural network architectures is common, but it may not enhance robustness as previously assumed. Different regularizers impact robustness differently, and the vulnerability to adversarial attacks can increase with input dimension. The importance of identifying regularizers in neural networks, such as batch norm, in relation to adversarial vulnerability and input dimension is highlighted. By adding regularization and removing batch norm, the relationship between vulnerability and input dimension is shown to be not inherent. The impact of batch norm on hidden layers' pre-activations in a neural network is briefly reviewed using notation from previous studies. Batch normalization parametrizes layer l with batch mean \u00b5 \u03b1 and variance \u03c3^2. It subtracts mean \u00b5 \u03b1 from pre-activation h l \u03b1i, divides by standard deviation \u03c3 \u03b1 plus constant c, then scales and shifts by \u03b3 \u03b1 and \u03b2 \u03b1. This procedure fixes first and second moments equally for all neurons \u03b1, suppressing information. Loss of information cannot be recovered by \u03b3 \u03b1 and \u03b2 \u03b1 due to non-local batch-wise nonlinearity. Batch normalization can harm the ability to distinguish between high-quality and low-quality examples, leading to a loss of information and reduced robustness in the input space. This is further amplified by stacking batch norm layers, affecting the ability to maintain relative distances and impacting adversarial and general robustness. The study evaluates the robustness of convolutional networks trained with and without batch norm using various datasets. White-box adversarial attacks are conducted using projected gradient descent. Different norms and parameters are used for each dataset, and test accuracy is reported for additive Gaussian noise. The study evaluates the robustness of convolutional networks trained with and without batch norm using various datasets. Test accuracy is reported for additive Gaussian noise, showing that batch norm increases clean test accuracy but reduces accuracy for different types of noise attacks. In CIFAR-10 experiments, models were trained with random 32x32 crops and horizontal flips. Batch norm improved clean test accuracy by 1.1% and reduced accuracy for noise attacks. Test accuracy dropped by 6% for additive noise and by 17.3% and 5.9% for PGD perturbations. Similar results were obtained on a new test set, CIFAR-10.1 v6. In CIFAR-10 experiments, batch norm slightly improves clean test accuracy by 2.0 \u00b1 0.3%, but leads to a drop in accuracy for cases with additive noise and PGD perturbations. Increasing the initial learning rate to 0.1 with batch norm facilitates training and improves clean test accuracy relative to standard initialization. The study evaluated the robustness of pre-trained ImageNet models, showing that batch norm improves accuracy on noise but reduces it for PGD perturbations. Deeper models with batch norm did not match the robustness of shallower models without batch norm. The study evaluated the robustness of pre-trained ImageNet models, showing that batch norm improves accuracy on noise but reduces it for PGD perturbations. Deeper models with batch norm did not match the robustness of shallower models without batch norm. On the training set, some runs performed as expected, while others were successful. The effect of increasing epochs on trends was shown. Independence between vulnerability and input dimension can be recovered through adversarial training with little to no trade-off through weight decay regularization. Loss scales roughly as predicted without regularization, but with weight decay, adversarial test results improve. The study evaluated the robustness of pre-trained ImageNet models, showing that batch norm improves accuracy on noise but reduces it for PGD perturbations. Deeper models with batch norm did not match the robustness of shallower models without batch norm. Training with weight decay resulted in improved adversarial test accuracy ratios. The study compared the robustness of MLP models with and without batch normalization. Batch norm improved accuracy on noise but reduced it for perturbations. Adding weight decay improved adversarial test accuracy ratios. Batch norm improves clean test accuracy and BIM-\u221e perturbation accuracy. Extreme batch norm settings degrade accuracy for perturbations. Weight decay increases accuracy for noisy and adversarially perturbed inputs. Batch norm sacrifices robustness for accelerated training and higher clean test accuracy. Input dimension does not inherently affect vulnerability. The VGG architecture was chosen for experiments on SVHN and CIFAR-10 datasets due to its simplicity and independence from batch norm. This allows for studying the effect of batch norm alone, without other architectural innovations or hyperparameter tuning. While models like Inception and ResNet have higher standard test accuracy, they may not be as suitable for analyzing batch norm's impact on robustness. The VGG architecture was chosen for experiments on SVHN and CIFAR-10 datasets to study the impact of batch norm alone. The choice of architecture is arbitrary, with a trade-off between test accuracy and robustness. PGD implementation was used with specific settings for adversarial attacks. Comparisons were made between PGD and BIM implementations, showing a reduction in test accuracy for perturbations. The study compared the impact of batch norm on VGG models for adversarial attacks. Results showed a decrease in test accuracy for perturbations, with consistent differences between PGD and BIM implementations. Increasing PGD iterations for deeper networks had minimal impact on accuracy. Models with more than 8 layers failed to train without batch norm. Fixup initialization was proposed to reduce model complexity. The study compared the impact of batch norm on VGG models for adversarial attacks. Results showed a decrease in test accuracy for perturbations. Fixup initialization was proposed to reduce model complexity. WideResNet with Fixup had a test accuracy of 94.6, while WideResNet with batch norm had a test accuracy of 95.9. Higher clean test accuracy on CIFAR-10 was observed with WRN compared to VGG, but gains were lost with moderate Gaussian noise. VGG outperformed WideResNet variants under noise conditions. The study compared the impact of batch norm on VGG models for adversarial attacks. Results showed a decrease in test accuracy for perturbations. Fixup initialization was proposed to reduce model complexity. VGG8 outperformed WideResNet variants under noise conditions, achieving 78.9 \u00b1 0.6 vs. 69.1 \u00b1 1.1 accuracy. The Fixup variant improved accuracy for noisy CIFAR-10 and CIFAR-10.1, as well as for PGD-\u221e and PGD-2 attacks. This work motivates techniques like Fixup to reduce batch normalization usage. The study compared the impact of batch norm on VGG models for adversarial attacks. Results showed a decrease in test accuracy for perturbations. BIM-\u221e was used with 10 steps and a step size of 5e-3 for crafting adversarial examples. The objective was misclassification, not targeted attacks. 2048 samples were selected from the validation set. The discrepancy between results in additive noise and white-box BIM perturbations for ImageNet raised questions about gradient masking. When targeting an unnormalized model, top 1 accuracy was 10.5% - 16.4% higher, and top 5 accuracy was 5.3% - 7.5% higher compared to vice versa. The study compared the impact of batch normalization on VGG models for adversarial attacks. Results showed a decrease in test accuracy for perturbations. When targeting an unnormalized model, top 1 accuracy decreased by 16.5% \u2212 20.4% compared to only 2.1% \u2212 4.9% for batch normalized networks. The pre-trained ImageNet models provided by PyTorch developers lack hyperparameter settings. Diamond shaped artifacts in the unnormalized case indicate training failures. In FIG3, outliers are removed and batch sizes from 5-20 are shown. Batch normalization limits trainable depth and robustness decreases with batch size for depths around 25 layers. Unnormalized networks show little relationship between accuracy, robustness, depth, and batch size. Unnormalized networks have lower peak accuracy and take longer to converge compared to batch normalized networks. The unnormalized networks take longer to converge, requiring 40 total epochs for training. They show higher clean test accuracy than batch normalized networks. Good robustness and clean test accuracy can be achieved simultaneously in unnormalized networks, while they are mostly non-overlapping in batch normalized networks. The logistic regression model is parameterized by weights and bias, with predictions optimized by stochastic gradient descent on the sigmoid cross entropy loss. The model was trained using SGD for 50 epochs with a constant learning rate of 1e-2 and a batch size of 128. Increasing the input dimension on variants of MNIST was studied, showing that the model is too simple for competitive test accuracy but serves as a first step towards extending to ReLU networks. Increasing the input dimension by resizing MNIST from 28 \u00d7 28 to various resolutions with PIL.Image NEAREST interpolation increases adversarial vulnerability in terms of accuracy and loss. Adversarial damage predicted to grow like \u221a d can be recovered through weight decay, with a small trade-off in standard test accuracy. Weight decay regularization is used to correct for loss scaling and recover input dimension invariant vulnerability with minimal degradation of test accuracy. Compared to PGD training, weight decay regularization does not require arbitrary hyperparameters, does not prolong training, and is less attack-specific. Adversarially augmented training is not used to convey robustness to unseen attacks and common corruptions. Models trained with weight decay obtained a 12% improvement. Our models trained with weight decay achieved 12% higher accuracy compared to batch norm on a small sample of MNIST examples. Weight decay reduces the generalization gap, even in batch-normalized networks where it is presumed to have no effect. Batch norm is not typically used on the last layer, but the loss scaling mechanism persists to a lesser degree. Robustness was evaluated on common corruptions and perturbations benchmarks, including 19 types of real-world effects grouped into categories like \"noise\" and \"blur\". Results for VGG variants and WideResNet on CIFAR-10-C are outlined, showing that batch norm increased error rates for all noise levels. Results for VGG8 and WRN-28-10 on CIFAR-10-C showed that batch norm increased error rates for all noise variants. The most impactful corruptions were Gaussian, Impulse, Shot, and Speckle, while others had mixed results. The study found that batch norm increased error rates for all noise variants in VGG13, with notable generalization gaps for Gaussian and Impulse noise. Overall, the mCE was reduced by 2.0 \u00b1 0.3% for the unnormalized network. For Pixelate, the mCE was reduced by 2.9 \u00b1 0.6%. Using \"Fixup\" initialization on a WideResNet 28-10 (WRN) decreased mCE by 1.6 \u00b1 0.4%. Generalization gaps for different noise types were: Gaussian-12.1 \u00b1 2.8%, Impulse-10.7 \u00b1 2.9%, Shot-8.7 \u00b1 2.6%, and Speckle-6.9 \u00b1 2.6%. JPEG compression had a mCE reduction of 4.6 \u00b1 0.3%. Some corruptions showed a positive gap for VGG8 but a negative gap for WRN, indicating batch norm improved accuracy for certain corruptions. The WRN model shows high standard test accuracy and relies heavily on texture information rather than shape cues. The model is considered over-fitted with minimal training error. The dataset involves classifying points on concentric circles. The batch-normalized network is highly sensitive to the learning rate \u03b7, making training less stable. Using SGD instead of Adam, the network's robustness is harmed, especially in the case of datapoints from two concentric circles. In a study on a two-hidden-layer fully connected network, the batch-normalized model fails to train effectively with a learning rate of \u03b7 = 0.01, while the unnormalized equivalent converges quickly. The experiment was repeated with five random seeds, showing the unnormalized network achieving zero training error for \u03b7 up to 0.1. The unnormalized network achieves zero training error for \u03b7 up to 0.1, while the batch-normalized network is untrainable at \u03b7 = 0.01. When evaluating robustness, 10,000 test points are sampled for each class, with noise applied. The model with batch norm classifies 94.83% correctly, while the unnormalized net obtains 96.06%. Visualizing the activations shows differences in clustering behavior between the two models over 500 epochs. In the final stage of training, clusters become tighter with batch-norm layers, affecting activation patterns and decision boundaries."
}