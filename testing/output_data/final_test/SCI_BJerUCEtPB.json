{
    "title": "BJerUCEtPB",
    "content": "Recent research has shown that CNNs are often overly sensitive to high-frequency textural patterns. A regularization scheme penalizing large differences between adjacent components within each convolutional kernel has been designed to address this issue. Applying this regularization to popular training methods results in models with smooth kernels that exhibit improved adversarial robustness. Additionally, the method appears to provide more perceptually-aligned gradients, enhancing interpretability in deep learning models for computer vision tasks. Recent research has investigated the limits of claims regarding the perceptive capabilities of neural networks, especially when faced with adversarial examples or cross-domain evaluations. Various methods have been explored to enhance the robustness of neural networks, including regularization schemes and training models with augmented examples through attack methods. In this paper, the focus is on reducing neural networks' reliance on high-frequency patterns in images by regularizing CNNs to prioritize low-frequency components. This approach aims to enhance model robustness and align gradients perceptually. In this study, the focus is on reducing neural networks' dependence on high-frequency patterns in images by prioritizing low-frequency components through regularization. The research demonstrates that filtering out high-frequency components with a smooth kernel can improve model robustness and align gradients perceptually. The study also shows empirical improvements over popular adversarial robust methods and uses model interpretation techniques to understand decision-making processes. The study focuses on reducing neural networks' reliance on high-frequency patterns in images by prioritizing low-frequency components through smooth kernel regularization. This approach helps produce models less sensitive to high-frequency patterns, as demonstrated in previous research. Smooth kernel regularization prioritizes low-frequency components in images, reducing neural networks' reliance on high-frequency patterns. This approach ensures that high-frequency components become negligible, as shown by Titchmarsh (1948) and Bracewell (1986) through the Convolution Theorem. The regularization term R0(w) is calculated to prioritize low-frequency components in images, reducing reliance on high-frequency patterns. Additional heuristics are applied to improve this regularization, such as subtracting the scale of w to prevent models from achieving a small value of R0(w) by directly scaling down every coefficient of w proportionally. The regularization term R0(w) prioritizes low-frequency components in images to reduce reliance on high-frequency patterns. Heuristics are applied to improve this regularization, such as dividing R0(w) by the scale of w. Empirical observations show that the regularization plays a significant role in early training stages but may hinder overall performance later. To address this, an exponential function is used to adjust the regularization effect based on the value. The final regularization focuses on encouraging smoothness over the two spatial dimensions of 2D images. Our implementation focuses on regularizing smoothness over the two spatial dimensions of 2D images by enumerating all pairs of neighbors to avoid double counting. The regularization term \u03bbR(w) is appended to various loss functions in our experiments, including the vanilla loss function, Trades loss, adversarial training loss, and a variation of logit pairing loss. Adversarial training involves fitting the model with adversarial examples generated at train time, while Trades loss fits the model with clean data. The text discusses the use of logit pairing loss in training models with augmented adversarial examples, as an alternative to fitting the model with clean data. The approach involves penalizing the KL divergence of softmax outputs to ensure consistency in comparison. In experiments, the model uses KL divergence in logit pairing loss to limit hypothesis space for robustness, based on Pinsker's inequality. The study empirically validates proposed methods with synthetic experiments and standard datasets like MNIST, FashionMNIST, CIFAR10, and Restricted ImageNet. Experiments use basic convolutional neural networks and ResNet models, applying new regularization to different losses. Saliency-based visualization methods are used to understand model interpretations for each class. The study applies new regularization to various losses, including vanilla, Trades, adversarial training, and logit pairing. Evaluation is done against different adversarial attack methods like FGSM, PGD, C&W, DeepFool, ADef, and Salt&Pepper using default parameters in Foolbox. The study applies new regularization to various losses, including vanilla, Trades, adversarial training, and logit pairing. Evaluation is done against different adversarial attack methods using default parameters in Foolbox. The regularization method is tested on a basic data set of four shapes to train a convolutional neural network. Different models are compared with and without regularization, with varying hyperparameters like \u03bb within {0.01, 0.1, 1.0, 10.0, 100.0} and \u03b3 within {0.1, 1.0, 10.0}. Regularization with a negative hyperparameter value of \u03bb = \u2212100.0 is tested to observe the effects on model regularization towards high-frequency kernels. Resulting models are labeled as VH, TH, AH, LH based on different losses. Figure 1 shows that the regularization leads to smoother kernels across all losses. Applying negative regularization results in more fluctuation in kernel weights. The visualization in Figure 1(b) confirms that smooth kernels have minimal high-frequency components. The frequency domain of kernels with regularization applied shows significant differences in low and high-frequency components. The visualization of the first four convolutional kernels of each model and their corresponding frequency domain are shown. Internal representations influenced by regularization focus on low-frequency signals for shape details, while negative regularization emphasizes high-frequency signals. Adversarial examples are used to test model deception. Adversarial examples generated by FGSM deceive models, with some examples altering decisions in a way understandable to humans. Regularized models show more convincing alterations in shape compared to non-regularized models. Regularization with a negative parameter (H) causes patches to behave in a shattered manner. Prediction accuracy over adversarial examples is reported in Table 1 for different attack methods. Search attempts for adversarial examples often fail under default hyperparameters in Foolbox, indicating model robustness. Regularization helps achieve the best adversarially robust models across various settings. Our regularization improves the robustness of models significantly for MNIST and FashionMNIST datasets, even with only vanilla training loss. The improvements are more pronounced for these datasets compared to non-regularized models. Performance gains are also observed for CIFAR10 and Restricted ImageNet, although less significant. Additional accuracy and curves are provided in the Appendix for thorough comparisons. The use of activation maximization further demonstrates model interpretation. Activation maximization is used to exaggerate the most representative features of a certain class in input images. Regularization significantly improves model interpretation, making patterns more observable for digits like 0, 2, and 3. Similar results are seen in the FashionMNIST dataset. Regularization, especially AR, enhances model interpretation across different cases in FashionMNIST and CIFAR10 datasets. AR stands out in interpreting various classes like \"Bag\" with a strap, \"bird,\" and \"deer,\" showing superior understanding compared to other methods. The regularization technique proposed in the study enhances model interpretation for various classes like \"Bag,\" \"bird,\" and \"deer\" in FashionMNIST and CIFAR10 datasets. The method focuses on low-frequency components of images, leading to more perceptually-aligned gradients and smooth convolutional kernels during training. This approach is supported by neuroscience literature emphasizing the connection between low-frequency components and shape recognition. The regularization technique proposed in the study enhances model interpretation for various classes in FashionMNIST and CIFAR10 datasets by focusing on low-frequency components of images. The model architecture varies for different datasets, with perturbation bounds set accordingly. The study uses a ResNet50 model with a perturbation bound of 0.005/1.0 for PGD. Pixel values are divided by the standard deviation during processing. Different ResNet50 models are used for Restricted ImageNet. Trades loss cannot be effectively trained on ResNet50 with NVIDIA 1080Ti hardware. Accuracy-epsilon curves are shown for different bounds. Adversarial examples are visualized for model evaluation, focusing on the most deceptive examples. Adef attack is used for generating adversarial examples in MNIST and FashionMNIST datasets. The Adef attack is preferred for generating adversarial examples as it aligns more visually with human perception."
}