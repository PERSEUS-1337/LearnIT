{
    "title": "Sklgs0NFvr",
    "content": "Despite concerns about machine learning systems relying on spurious patterns in training data, the term lacks a clear definition in standard statistical frameworks. Causality provides a clearer understanding, distinguishing between spurious associations due to confounding and direct or indirect effects. This paper focuses on NLP, introducing methods for training models that are insensitive to spurious patterns. By revising documents to align with a counterfactual target label, classifiers trained on original data perform differently from those trained on counterfactually-revised data. Combining datasets yields classifiers that perform well across sentiment analysis and natural language inference tasks. Statistical learning offers a framework for understanding what makes a document's sentiment positive or a loan applicant creditworthy. By focusing on associative questions rather than semantic ones, relevant features can be identified. The paper introduces methods for training models that are insensitive to spurious patterns in NLP, resulting in classifiers that perform well across various tasks. The paper discusses the limitations of purely associative learning in addressing issues such as spuriousness, artifacts, reliability, and discrimination in deep learning models. Researchers have found that deep neural networks in computer vision and NLP rely on surface-level texture or background clues, leading to concerns about annotation artifacts and bias. Issues of annotation artifacts and bias have been highlighted in various tasks, showing that models often rely on spurious associations. Some models for question-answering tasks may not be sensitive to the question choice, while in Natural Language Inference, classifiers trained on hypotheses only perform well. The concept of spuriousness within the supervised learning framework is not clearly defined, as ML systems are trained to exploit mutual information between features and labels without distinguishing between spurious and non-spurious associations. The curr_chunk discusses the distinction between spurious and non-spurious associations in causality, proposing a new dataset creation procedure inspired by causality philosophy. It mentions counterfactually augmenting datasets and editing documents to manipulate specific aspects. The curr_chunk discusses interventions to manipulate sentiment in documents to disentangle spurious and non-spurious associations. It proposes a human-in-the-loop system for counterfactually manipulating documents to improve classifier performance. The curr_chunk discusses using crowd workers to manipulate text for sentiment analysis and NLI tasks, collecting counterfactually-manipulated examples to improve classifier performance. The text discusses creating new datasets for sentiment analysis and NLI tasks by extending existing datasets. It shows that classifiers trained on original data fail on counterfactually-revised data, and spurious correlations can be broken by augmenting examples. Classifier accuracy significantly decreases when evaluated on original vs revised reviews. BERT and linear classifiers show a drop in performance when evaluated on revised reviews compared to original reviews. However, when retrained on a combined dataset, their accuracy improves. BERT is more resilient to performance drops but relies on spurious associations. Fine-tuning BERT on SNLI sentence pairs results in a significant drop in accuracy on revised pairs. A Bi-LSTM trained on hypotheses alone performs well on SNLI dataset but poorly on revised dataset. Training on combined dataset improves its performance. Several papers demonstrate cases where NLP systems struggle to learn human-defined differences, such as vulnerability to synthetic transformations and misclassification of paraphrased tasks. Studies show that simply replacing words can break ML-based NLI systems, and classifiers correctly classify hypotheses alone in about 69% of SNLI corpus. Several studies have highlighted issues with NLP systems, including the vulnerability to synthetic transformations and misclassification of paraphrased tasks. Researchers found that simply replacing words can disrupt ML-based NLI systems, with classifiers accurately classifying hypotheses in about 69% of the SNLI corpus. Additionally, various papers have identified biases in question-answering benchmarks and training data imbalances leading to unintended bias in resulting models. Other research has explored the impact of stylistic variation on sentiment analysis algorithms, revealing significant differences in sentiment scores for similar word pairs. Several papers explore richer feedback mechanisms for classification, such as annotators highlighting rationales in text. Zaidan et al. (2007) remove rationales to create contrast documents for training classifiers. Lu et al. (2018) programmatically alter text to address gender bias in word embeddings. Zmigrod et al. (2019) and Maudslay et al. (2019) propose methods to mitigate gender stereotypes in language by altering grammatical gender and substituting gendered words in text. They use different approaches, with Zmigrod et al. using a Markov random field for morphologically-rich languages like Spanish and Hebrew, while Maudslay et al. focus on probabilistic automatic substitution of gendered words in a corpus. Both studies aim to counteract gender bias in language. The platform recruited U.S. residents with specific qualifications to edit datasets. 713 workers participated, with 518 contributing edits to the final datasets. The original IMDb dataset had 50000 reviews, with 20000 reviews in the train split. After filtering out the longest reviews, 2500 were randomly sampled for editing. The dataset was then split into train/validation/test sets with 1707, 245, and 488 examples, respectively. The dataset was split into train/validation/test sets with 1707, 245, and 488 examples. Reviews were presented to workers for revision, with 2% being rejected after manual inspection. Common editing patterns were identified, and one revised review was chosen for each original review to construct the new dataset. For each review, access to its counterfactually-revised counterpart helps isolate the parts indicative of sentiment. Position indices of replacements or insertions are identified to create binary vectors representing edits. Inter-editor agreement is analyzed using Jaccard similarity, showing higher agreement on smaller reviews. Natural Language Inference involves a 3-way classification task with entailment, contradiction, and neutral labels for two sentences. The label in Natural Language Inference describes the relationship between sentences, with three possible labels: entailment, contradiction, and neutral. A sample of 1750, 250, and 500 pairs were randomly selected from the train, validation, and test sets of SNLI respectively, ensuring balanced classes. Edits for sentiment analysis included recasting facts, suggesting sarcasm, and inserting modifiers. The presentation of Atlantis' landscape and setting is predictable. \"Election\" is a highly anticipated thriller-drama with a shocking ending. Some parts of the story make sense, but there are no amazing visual effects. The film lacks action but still manages to surprise at the end. The curr_chunk discusses the process of revising hypotheses and premises for data collection. Workers were asked to make edits to either the hypothesis or premise while keeping the other intact. The data was verified by presenting pairs to three workers for a majority vote. Only pairs that received unanimous agreement were approved. The curr_chunk discusses the process of revising hypotheses and premises for data collection. Workers were asked to make edits to either the hypothesis or premise while keeping the other intact. The data was verified by presenting pairs to three workers for a majority vote. Only pairs that received unanimous agreement were approved. In the revised dataset, \u2248 9% of data was discarded, resulting in 6664 pairs in train, 800 in validation, and 1600 in test. Qualitative analysis revealed common patterns in edits. Workers were paid $0.65 per revision and $0.15 per verification, totaling $10778.14 for the study. Five models were used in the experiments, including Support Vector Machines (SVMs). The study used five models for sentiment analysis: SVMs, Na\u00efve Bayes, Bi-LSTMs, ELMo with LSTM, and fine-tuned BERT models. Implementation details for reproducibility were discussed, including training on TF-IDF bag of words and grid search for parameter identification. Bi-LSTMs were trained with a restricted vocabulary, maximum input length of 300 tokens, and 50-dimensional token embeddings. The model used for sentiment analysis includes a bidirectional LSTM with global max-pooling and fully-connected layers. Training is done for a maximum of 20 epochs with early stopping. ELMo-LSTM is used for contextualized word representations. The model for sentiment analysis includes character-based word representations and bidirectional LSTMs. ELMo is used to generate a 1152-dimensional hidden representation, followed by an LSTM with recurrent dropout. The model is trained for up to 20 epochs with early stopping criteria, using the Adam optimizer. BERT is also utilized, fine-tuned for each task with specific token length settings. The maximum token length for sentiment analysis is set at 350 and 50 for NLI. BERT is fine-tuned for up to 20 epochs with specific learning rates. Linear models trained on original reviews achieve 80% accuracy on original reviews but only 51% on revised reviews. BERT models show less accuracy drop compared to Bi-LSTMs when evaluated on combined datasets. Models trained on combined datasets perform well, showing a slight decrease in performance compared to models trained on original data. Experiments on sentiment analysis across various domains demonstrate that models trained on counterfactually-augmented data outperform models trained on original data in most cases. The study experimented with training models on passages where edited spans were removed, showing that SVM, Na\u00efve Bayes, and Bi-LSTM achieved high accuracy, while BERT performed poorly. The study also highlighted seemingly irrelevant words that were picked up as high-weight features by linear models, indicating the benefits of their approach. The study showed that combining original and revised datasets affected the predictive associations of certain terms. Models trained on original data performed slightly better on the original test set but failed on revised reviews. Retraining models on a combination of original and revised reviews led to increased accuracy in classifying revised reviews, emphasizing the importance of including counterfactually revised examples in training data. Fine-tuning BERT on a combination of RP and RH datasets improves accuracy on both sets but results in a drop in performance on the SNLI dataset. Fine-tuning BERT on a combination of RP and RH datasets leads to consistent performance on all datasets by forcing models to consider both premise and hypothesis. Combining original sentences with RP and RH further improves performance. Comparing this approach to fine-tuning on SNLI training set shows better results on RP and RH. Bi-LSTM trained on SNLI hypotheses only achieves 69% accuracy, dropping to 44% when retrained on a combination of original, RP, and RH data. The study found that using hypotheses associated with different truth values can lead to conflicting feedback during training, causing a drop in performance. Retraining models on original, RP, and RH data resulted in a significant accuracy drop, indicating hypersensitivity to domain. Models were able to distinguish between original and revised data with 77.3% accuracy using BERT. By revising documents with human feedback, insights on semantic concepts can be derived, leading to classifiers less reliant on spurious associations. This method shows promise in sentiment analysis and NLI tasks, where opinions and facts hold varying importance. Leveraging human-in-the-loop feedback helps in disentangling spurious associations, resulting in classifiers that perform better when faced with out-of-sample data. In future work, techniques will be extended to leverage human feedback for building more robust systems in question answering and summarization tasks."
}