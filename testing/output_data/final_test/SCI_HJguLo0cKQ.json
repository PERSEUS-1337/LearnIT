{
    "title": "HJguLo0cKQ",
    "content": "While deep learning has shown impressive results on challenging problems, researchers have found a vulnerability in neural networks in adversarial settings. Small perturbations to the input can cause these models to produce highly inaccurate outputs, making them unsuitable for safety-critical applications like self-driving cars. Recent studies have demonstrated that training with adversarially generated data can improve robustness against attacks. This paper explores how increasing the computational budget for defending against attacks can enhance the robustness of adversarially-trained models. Increasing the number of parameters in these models improves their robustness, and ensembling smaller models while training them adversarially as a single model is a more efficient use of resources. Ensembling smaller models while training them adversarially as a single model is a more efficient way to spend the budget. Adversarial training of the ensemble, rather than ensembling adversarially trained models, provides robustness against attacks. Deep neural networks have shown state-of-the-art performance but are vulnerable to small input changes leading to inaccurate outputs. Adversarial perturbations can be easily computed using standard optimization techniques. Adversarial attacks can be categorized based on the adversary's access to the model, with 'white-box' attacks having full access and 'black-box' attacks having no access. Despite new defenses, recent papers show that most are still vulnerable to attacks. Recent papers have shown that new defenses against adversarial attacks are still vulnerable, with non-gradient based attacks remaining effective. The question posed is whether increasing computational resources can enhance the adversarial robustness of a classifier. Various methods, such as using larger models, aggregating predictions from multiple runs, or employing portfolios of models, have been suggested to improve robustness. In this paper, the authors propose using adversarial training of ensemble models to improve adversarial robustness. Experimental results show that stronger adversaries can still attack ensembles successfully. The study analyzes the trade-off between robustness and computation, finding that increased computation or model size can enhance robustness. Adversarially trained ensembles are more robust than individual models, and can be made robust through treating the ensemble as a single model. In this study, the authors propose using ensemble models for adversarial robustness. They train the models using multi-step adversarial training, resulting in increased robustness to black-box and white-box attacks. This approach differs from previous work that used ensembles for adversarial perturbations. Attacks on neural networks involve generating adversarial examples by perturbing inputs to maximize loss. Adversarial inputs are created within a set of allowable inputs around a nominal input. This approach aims to increase robustness to attacks on neural networks. Adversarial examples are generated using projected gradient descent to update the input with noise, encouraging generalization of adversarial robustness. The fast-gradient-sign method uses the sign of the gradient to generate adversarial examples quickly. Ensembles of models trained with adversarial training are the focus of this paper. Ensembles of models are trained with adversarial training to increase robustness by minimizing a weighted sum of two losses, one on normal training data and the other on adversarially generated data. This approach aims to improve performance by attacking the network at each time-step. Ensembles of models are used to increase robustness to adversarial attacks by averaging the predictions of k separately parameterized models. This approach aims to combine weak models in a way that the ensemble performs better than any individual model. The use of a 'gating network' to generate data-dependent weights for each model was considered, but performance was found to be similar to a simple average. Ensembles of models aim to maintain diversity to prevent errors from cancelling each other out. This can be achieved by bootstrapping data or using different model types. In neural networks, diversity is maintained by initializing each model differently to converge to different parameter spaces. Ensembles of models aim to maintain diversity by initializing each model differently to converge to different parameter spaces. This approach is based on the insight that different initialization is sufficient for uncertainty estimation in neural networks. Taking a Bayesian approach to the classification problem involves a prior over model parameters, a likelihood of the data, and a probability of a label given an input and a model. The 'Bayes-optimal' classification of a new data point is optimal but intractable for all but small problems, so an approximation involves sampling initial parameters from the prior and running an iterative procedure. Ensembles of models aim to maintain diversity by initializing each model differently to converge to different parameter spaces. The approach involves sampling initial parameters from the prior and running an iterative procedure to maximize the likelihood. This process can be considered as sampling from the posterior over models. Adversarial ensemble training using PGD under \u221e norm constraint involves initializing neural network parameters and updating them for each model in the ensemble. The role of initialization is to sample from the prior over possible model parameters. Ensembles of models are used to improve classification performance and approximate the Bayes optimal classifier. Adversarially trained ensembles provide robustness to adversarial attacks by replacing the loss function with the mean of the loss over the models. This procedure increases robustness to adversarial inputs, as shown in empirical results. The exact procedure is outlined in Algorithm 1. The Baseline model is a Wide ResNet BID34 with 28 ResNet blocks, batch normalization, ReLU, and a linear layer for CIFAR-10 classes. Ensemble2 has twice the parameters of the baseline. SingleAdv benchmark adds adversarial training to the baseline with the same parameters. The SingleAdv benchmark, with the same parameters as the baseline, undergoes adversarial training using the Iterated Fast Gradient Sign Method. The schematic depiction in Figure 1 shows the comparison of different model classes in the paper, illustrating the inputs and outputs of adversarially trained networks. Ensemble2Adv is compared to SingleAdv and DoubleAdv benchmarks with similar parameter counts. SeparateEnsemble2Adv combines two separately trained SingleAdv models at test time to evaluate the advantage of adversarial training in ensembles. Model variations are illustrated in Figure 1 for replication of experiments. The models are trained on CIFAR-10 using similar hyperparameters to previous experiments. Training includes both clean images and adversarial examples generated by IFGSM. Evaluation is done by averaging cross-entropy losses for both types of images to obtain gradients for the model. During training, white-box adversaries with L \u221e perturbation of 8 are considered, using IFGSM, PGD, and evaluating with IFGSM7, PGD5, and PGD20 attacks. Black-box adversaries are also included in the evaluation using precomputed adversarial examples. The models, a Wide ResNet and VGG-like architectures, achieve 96.0% and 94.5% accuracy on CIFAR-10 clean test set. The PGD20 adversary fools the ensemble on 100% of the evaluation set, highlighting the importance of evaluating black box accuracies for robustness against attacks. The evaluation outliers are removed by computing a smoothed version of each time series and replacing points in the original time series with their smoothed version when the absolute difference exceeds three standard deviations. Evaluation accuracies for different seeds are interpolated and averaged across seeds per model class. In the evaluation of model performance, the average of the last 10 evaluation steps is reported for all models. Evaluation metrics at the time of best performance on FGSM5 are shown in TAB0. The evolution of evaluation accuracies for selected metrics is depicted in FIG1, highlighting the drop in accuracy as the number of PGD attack steps increases. Adversarially trained models consistently outperform non-adversarially trained ones, especially against weaker attacks like IFGSM5 and PGD5. Non-adversarially trained models show some recovery of robustness after training but this diminishes with further training. Models achieving some robustness against weak adversaries do not have true adversarial robustness. Ensemble2Adv models show stable robustness during training, with a slight drop in accuracy on CIFAR-10 test. They outperform SingleAdv and DoubleAdv models, with a 29% improvement over non-adversarially trained Ensemble2Adv. Despite dropping accuracies with increased attack steps, Ensemble2Adv maintains a 7-point gap over SingleAdv. The ensemble model Separate2Adv shows robustness to adversarial attacks closer to SingleAdv than Ensemble2Adv, despite having the same structure. Evaluation accuracies after 500 steps of PGD in TAB0 validate the results. An ensemble of adversarially trained models outperforms other setups at test time, possibly due to exposure to both clean and adversarially modified images during training. The adversarial training exploits the proximity of the original image to the model's decision boundary. SingleAdv is weaker due to fewer parameters, compromising the decision boundary's robustness. Ensemble2Adv outperforms DoubleAdv possibly because it is more robust to white box attacks. The DoubleAdv architecture tends to spread out thin \"tentacles\" due to its flexibility, while Ensemble2Adv, comprised of two separate models, is more robust to overfitting. This difference in behavior is illustrated in Figure 3, showing the responses of various architectures to adversarial training. The modification of models with adversarial training covers adversarial examples by modifying one model more than the other, creating a \"tentacle\" effect. Ensemble2Adv trained jointly outperforms SeparateEnsemble2Adv, as adversarial training weakens both submodels simultaneously. Decision boundaries for non-adversarially and adversarially trained models are illustrated in Figure 4. Figure 4 displays decision boundaries for various models, showing attack directions and resulting images. The decision regions are visualized in a 2-dimensional plane, with majority class represented by color. The models' decision regions are visualized in 2D planes, showing support for the hypothesis that adversarial training adds \"thickness\" around natural image points, making adversarial examples harder to find. Ensembling creates more consistent classes but introduces small \"pockets\" of other classes, which are removed by combining models to create large regions of the correct class around images. In this paper, an empirical study was conducted on the effect of increasing model parameters in adversarial training methods. It was found that ensembling smaller models improves robustness more than producing one larger model. The results show that this improvement is not solely due to ensembling or the robustness of the models, but specifically due to adversarial training as a single model. Further research should investigate scaling the number of models in an ensemble while controlling for parameters to determine if significant improvements can be achieved over minimal ensembles. This will help understand why such architectures are generally more robust than larger single models, especially under adversarial training."
}