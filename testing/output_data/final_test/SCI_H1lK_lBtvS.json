{
    "title": "H1lK_lBtvS",
    "content": "Anomaly detection is a fundamental problem in artificial intelligence, with recent classification-based methods showing superior results. A unifying view and an open-set method are proposed to relax generalization assumptions, along with the extension of transformation-based methods to non-image data using random affine transformations. The method achieves state-of-the-art accuracy across various data types and is validated on multiple datasets. Anomaly detection is crucial for both humans and artificial intelligence, with applications in detecting credit card fraud. Anomaly detection systems in artificial intelligence are used for various purposes such as discovering credit card fraud, detecting cyber intrusion, predicting maintenance needs for industrial equipment, and identifying stock market opportunities. The task involves classifying data as normal or anomalous to trigger further inspection when a different pattern is detected. Anomaly detection methods can be supervised, where training examples of normal and anomalous patterns are provided, but obtaining such supervision may not always be possible, especially in cyber security settings. In the context of anomaly detection systems, the semi-supervised scenario involves training an anomaly detector with normal examples and then detecting anomalies in test data containing both normal and anomalous examples. This approach is easier to implement than fully unsupervised methods and has been a focus of research in the field, with deep learning methods based on classification showing promising results. Deep learning methods for anomaly detection, particularly in the semi-supervised scenario, have shown promising results. Self-supervised classification-based methods, such as Deep-SVDD, have been proposed for anomaly detection using normal training data. A novel technique called GOAD unifies current state-of-the-art methods for anomaly detection based on classification. Our method transforms data into subspaces to learn features with larger inter-class separation than intra-class separation. The distance from the cluster center correlates with anomaly likelihood, allowing us to determine if a data point is normal or anomalous. We generalize transformation functions to include affine transformation for non-image data, improving anomaly detection on image and tabular datasets. Anomaly detection methods include reconstruction-based approaches using basis functions like nearest neighbors, PCA, K-means, neural networks, and GANs. Distributional methods also play a role in anomaly detection. Distributional methods in anomaly detection focus on modeling the distribution of normal data to identify anomalies. Various techniques like Gaussian models, non-parametric density estimates, and deep learning methods such as autoencoders have been used for this purpose. Deep features are easier to model than raw features, as shown by Yang et al. (2017). DAGMM, introduced by Zong et al. (2018), learns a probabilistic model jointly with deep features to shape the feature space. Anomaly detection can also be done through Classification-Based Methods, such as One-Class SVM (Scholkopf et al., 2000), which separates normal data from other regions. Learning good feature spaces for this separation is achieved through classic kernel methods and recent deep learning approaches. One challenge in unsupervised learning is providing an objective for learning relevant features, which can be addressed through self-supervised learning with neural networks. Learning good representations in a self-supervised way involves training a neural network on auxiliary tasks like video frame prediction, image colorization, puzzle solving, and image processing transformations. These methods have shown promise in detecting anomalies in images and improving feature space separation for anomaly detection. In this work, the authors introduce a method that extends the applicability of self-supervised methods to various data types and enhances robustness to adversarial attacks. They analyze semi-supervised classification-based methods for anomaly detection, aiming to build a classifier to distinguish normal data from anomalies in a high-dimensional space. Deep-SVDD is highlighted as a recent approach that learns to transform data into a feature space and fit a minimal hypersphere for anomaly detection. Learning an effective feature space for anomaly detection is challenging, as trivial solutions can result in the smallest hypersphere. Geometric-transformation classification (GEOM) transforms normal data into multiple subspaces using different geometric transformations. This method extends to all affine transformations, making it applicable to non-image data. An auxiliary task involves learning a classifier to predict anomalies. The GEOM method involves learning a classifier to predict transformation labels for data points, aiming to detect anomalies by assigning low probabilities to samples with high anomaly scores. However, the classifier is only valid for samples in the training set, posing a significant limitation. The GEOM method involves learning a classifier to predict transformation labels for data points to detect anomalies. However, the classifier is only valid for samples in the training set. Anomaly score is undefined for anomalies, but one way to overcome this is by training on anomalous data. This approach unifies one-class and open-set classification methods to address generalization issues. Our approach unifies one-class and transformation-based classification methods by learning a feature extractor using a neural network. Each subspace is modeled as a sphere with a center, and the classifier predicts transformation labels for data points. The centers are determined by the average feature over the training set. Training the feature extractor using the center triplet loss yielded better results. Using the center triplet loss, a feature space is learned to separate transformation subspaces. A normality score is calculated using probabilities, with a small regularizing constant added for uncertainty. At test time, each sample is transformed by M transformations, with the probability of normality being the product of these transformations. The probability that x is normal between transformations is calculated by the product of probabilities for transformed samples in their respective subspaces. Anomaly scores indicate the degree of anomaly for each sample, with higher scores representing more anomalous samples. Geometric transformations are tailored for CNNs to preserve pixel neighborhood, beneficial for unsupervised feature learning and anomaly detection in non-image datasets like tabular data commonly used for cyber security. Tabular data, commonly used for cyber security, consists of discrete and continuous attributes with no specific order. To enable transformation-based methods to work on various data types, the set of transformations needs to be extended to affine transformations. This class of transformations, including rotations, flips, and translations, is more general than mere permutations and allows for dimensionality reduction and random transformations. Random transformations are crucial for avoiding adversarial examples, as they reduce variance across different dataset types. Generalizing the set of transformations to the affine class allows for more flexibility in working with non-image data and makes it harder for adversaries to create effective attacks. The study introduces a general class of transformations to the affine class, enabling the use of unlimited random transformations for non-image data. Experiments on the Cifar10 dataset validate the effectiveness of the distance-based approach, showing promising results in terms of AUC. The method is compared with existing deep one-class methods, demonstrating its superiority in defending against adversarial examples. The distance-based approach outperforms the state-of-the-art method by Golan & El-Yaniv (2018) on the Cifar10 dataset, showing the importance of considering generalization behavior outside the normal training region. CNN architectures benefit from pixel order preserving transformations, while fully-connected networks do not. Adversarial Robustness: Networks can fully utilize random affine matrices and benefit from randomized transformations to defend against attacks where anomalies are modified to look normal. Three networks A, B, C are trained with different transformations, with C using randomly selected transformations. Adversarial examples are created using PGD based on network A, showing the effectiveness of randomized transformations on Cifar10 dataset. The average increase in performance of classifying transformations correctly on anomalies was 12.8% for network A, 5.0% for network B, and 3% for network C. Random transformations were shown to be beneficial. The method was evaluated against state-of-the-art anomaly detection techniques on four tabular datasets, including the Arrhythmia dataset from the UCI repository. The datasets include cardiac arrhythmia, thyroid, and KDD Intrusion Detection. Classes are designated as normal or anomalous based on specific criteria. Thyroid dataset focuses on hyperthyroidism, while KDD dataset simulates network attacks. The dataset consists of normal and 4 simulated attack types: denial of service, unauthorized access from a remote machine, unauthorized access from local superuser, and probing. It contains around 5 million TCP connection records with 41 attributes, 34 continuous and 7 categorical. Categorical attributes are encoded using 1-hot encoding. Two different settings are evaluated for the KDD dataset, one using the entire UCI 10% dataset where non-attack class is treated as anomaly, and the other where non-attack is normal and attacks are anomalous. In the reverse configuration, attack data is sub-sampled to 25% of non-attack samples. Methods are trained and evaluated on 50% of normal data and all anomalies. One-class SVM, LOF, and DAGMM are used for anomaly detection. DAGMM is a method introduced by Zong et al. (2018) that learns features with a mixture of Gaussian density models. They propose multiple variants of this technique, with the main variant being DAGMM. The method involves randomly sampling transformation matrices using the normal distribution, with each matrix having dimensionality L \u00d7 r. Implementation includes fully-connected hidden layers with 128 nodes and leaky-ReLU activations. Optimization is done using ADAM with a learning rate of 0.001, and each experiment is repeated 5 times. The decision threshold value is chosen following the protocol in Zong et al. (2018) to result in the correct number of anomalies. The experiments were conducted 5 times with recorded standard deviation. The Arrhythmia dataset was the smallest examined, with OC-SVM and DAGMM performing well. A linear classifier outperformed deeper networks, with early stopping after a single epoch generating the best results. The Thyroid dataset had a low anomaly to normal ratio and low feature dimensionality, with most baselines performing equally well. The UCI KDD 10% dataset is the largest dataset examined, with our method outperforming all baselines significantly. Large networks performed the best on this dataset, and early stopping was not needed. The KDD-Rev dataset is smaller than the KDDCUP99 dataset, with a quantitative comparison available in Tab. 2. The UCI KDD 10% dataset was the largest dataset examined, with our method outperforming all baselines significantly. Adversarial examples were less of a problem for tabular data due to the large number of transformations and relatively small networks. The paper also discusses increased adversarial robustness due to random transformations. This paper evaluates the robustness of a method in a semi-supervised scenario with anomalies in the training dataset. Results show significant outperformance compared to DAGMM and graceful degradation. The method also shows robustness to contamination in other datasets. The GOAD method demonstrates robustness to contamination and can generate any number of tasks. Anomaly detection performance on the KDD-Rev dataset improves with more tasks, stabilizing at 16 tasks. Larger numbers of transformations enhance performance on smaller datasets. The choice of margin parameter s is not critical, with a value of s = 1 recommended. The method is more robust than strong baselines in handling anomalies in the training set. Our method for detecting anomalies in unstructured data is robust to a small percentage of anomalies in the training set and degrades gracefully with contamination. It is beneficial for large datasets, particularly for the full KDDCUP99, but not necessary for smaller datasets. The approach can be used in a linear setting for performance critical operations and does not require domain knowledge. The method significantly improves anomaly detection over strong baselines. Our method significantly improves anomaly detection over strong baselines by generating random tasks. Performance increases rapidly with the number of tasks, especially for smaller datasets. More transformations are used for smaller datasets, leading to improved results. The approach can be extended to generate arbitrary randomized tasks for time-series and image data. Our method improves anomaly detection by aggregating scores from multiple classifiers. Comparisons with ensemble methods show GOAD outperforms FB-AE. Experiments with random transformations like random permutations and rotations show similar performance. The number of transformations used varies for different datasets. The performance of transformations is similar to affine transformations, with slight differences. Contamination experiments were conducted on 3 datasets, excluding Thyroid due to insufficient anomalies. Anomalies were split into train and test sets for evaluation. GOAD showed robustness to contamination on KDD, KDDRev, and Arrhythmia datasets. Results are shown in Fig. 3."
}