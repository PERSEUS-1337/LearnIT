{
    "title": "HkghV209tm",
    "content": "The new algorithms proposed for AMSGrad and Adam optimize deep neural nets by exploiting the predictability of gradients. They combine momentum and adaptive gradient methods with Optimistic Online learning, leading to faster training. Deep learning has proven effective in various tasks, with high-dimensional model parameters and large training data requirements. Fast algorithms are crucial for training high-dimensional deep neural nets with large data requirements. Recent algorithms like AMSGRAD, ADAM, RMSPROP, ADADELTA, and NADAM combine adaptivity and momentum concepts. ADAGRAD, an online learning algorithm, performs well with sparse gradients by adjusting learning rates based on gradient magnitudes, potentially improving data geometry exploitation. In this paper, a new algorithm is proposed that combines adaptivity and momentum approaches, inspired by optimistic online learning. This algorithm aims to exploit the geometry of data and past gradients for faster training of neural networks. The paper introduces new algorithms, NEW-OPTIMISTIC-AMSGRAD and NEW-OPTIMISTIC-ADAM, which combine adaptivity and momentum with optimistic online learning for training deep neural networks. These algorithms leverage a good guess of the gradient to accelerate training and outperform baselines in experiments. In this paper, OPTIMISTIC-algorithms are compared to baselines like ADAM-DISZ and AMSGRAD-DISZ. Both ADAM and AMSGRAD are ONLINE LEARNING algorithms using REGRET ANALYSIS. Converting online learning algorithms to offline optimization algorithms is possible through online-to-batch conversion. The paper provides a brief review of ONLINE LEARNING and OPTIMISTIC-ONLINE LEARNING. In online learning, a learner plays an action and receives a loss function in each round. The goal is to minimize regret, with no distributional assumption on the loss functions. No-regret algorithms aim to have regret approach zero as the number of rounds increases. Optimistic online learning is a recent branch of works in this paradigm. In online learning, the learner uses a good guess of the loss function before choosing an action to minimize regret. Optimistic-FTRL algorithm updates the guess based on the gradient vector. The regret of Optimistic-FTRL is analyzed under the assumption of convex loss functions. The regret of OPTIMISTIC-FTRL algorithm is analyzed under the assumption of convex loss functions. It can have much smaller regret than FTRL if the guess parameter is close to the gradient. However, if the guess is far from the gradient, the regret of OPTIMISTIC-FTRL could be worse than FTRL without optimistic update. Leveraging a good guess for updating the parameter can lead to fast convergence and small regret. Additionally, OPTIMISTIC ONLINE LEARNING has been shown to accelerate the convergence of some zero-sum games. ADAM is a popular algorithm for training deep nets, combining momentum and ADAGRAD ideas. ADAGRAD combines momentum with individual learning rates for different dimensions. The adaptive learning rate may accelerate convergence for sparse gradient vectors. However, the learning rate decay can be too fast when training deep nets. Ba (2015) suggests using a moving average of gradients divided by the root of the second moment for updating model parameters. ADAM fails at some online convex optimization problems. AMSGRAD is also mentioned. ADAM and AMSGRAD are compared in terms of their algorithms for online convex optimization problems. AMSGRAD fixes issues with ADAM by adding a step to guarantee a nonincreasing learning rate. Parameters for AMSGRAD are suggested as \u03b21 = 0.9, \u03b22 = 0.99, and \u03b7t = \u03b7/\u221at. A new algorithm, NEW-OPTIMISTIC-AMSGRAD, is proposed for training deep nets. NEW-OPTIMISTIC-AMSGRAD is a new algorithm proposed for training deep nets. It has an optimistic update that exploits the guess m t+1 of g t+1 to update w t+1. The update is subtle, as w t+1 is updated from w t\u2212 1 2 instead of w t. The learning rate on line 9 does not contain the factor of 4 1\u2212\u03b21 in practice. NEW-OPTIMISTIC-AMSGRAD introduces an optimistic update for training deep nets, utilizing a guess m t+1 of g t+1 to update w t+1. The algorithm includes adaptive learning rates, past gradient averages, and acceleration properties. To obtain m t, an extrapolation algorithm is used. The algorithm by Scieur et al. (2016) adapts classical extrapolation methods to estimate the fixed point using the last few iterates of a sequence. It involves obtaining a mini-batch stochastic gradient vector, solving for z, calculating c, and outputting an approximation of the fixed point x*. The algorithm by Scieur et al. (2016) adapts classical extrapolation methods to estimate the fixed point using the last few iterates of a sequence. In NEW-OPTIMISTIC-AMSGRAD, Algorithm 3 is used to obtain nonlinear iterates {x t} with theoretical guarantees on the distance from x*. The extrapolation method in NEW-OPTIMISTIC-ADAM helps achieve faster convergence by predicting the gradient accurately. NEW-OPTIMISTIC-ADAM is an algorithm derived from removing a step in Algorithm 2, resulting in an OPTIMISTIC-variant of ADAM. The regret analysis is provided, assuming the model parameter w is in d-dimensional space for unconstrained optimization. The algorithm utilizes a Mahalanobis norm and dual norm for optimization. The regret analysis of NEW-OPTIMISTIC-ADAM assumes a d-dimensional space for unconstrained optimization, utilizing Mahalanobis and dual norms. The constraint K in the regret definition is a finite norm ball containing optimal solutions. The regret is decomposed and analyzed, with a focus on bounding terms like g t and h t. The bound in Lemma 1 corresponds to AMSGRAD, with detailed proof provided in Appendix B. The proof of Lemma 1 is detailed in Appendix C, leading to the conclusion of Theorem with comparisons to AMSGRAD. Conditions are discussed where the bound is smaller than AMSGRAD, focusing on terms like g t and h t. The analysis shows potential smaller bounds under certain conditions. The last term in the inner sum is bounded by a constant c, leading to a smaller bound due to the non-decreasing denominator. The growth rate of v t\u22121[i] varies for different dimensions. The last term is O(log T), potentially better than in previous terms. NEW-OPTIMISTIC-AMSGRAD may have a smaller regret and better convergence rate than ADAM and AMSGRAD. Our work focuses on optimizing online learning by introducing OPTIMISTIC-MIRROR-DESCENT to accelerate convergence in bilinear zero-sum games. We also address the non-convergence issue in ADAM-DISZ and show that it suffers from the same problem as ADAM. The proposed method, AMSGRAD-DISZ, aims to improve convergence by introducing a step to the weighted second moment of ADAM-DISZ. Testing on various neural network architectures shows that NEW-OPTIMISTIC-AMSGRAD accelerates the convergence of AMSGRAD, enhancing the learning process efficiency. Parameters for AMSGRAD are set as recommended, and tuning the learning rate results in the best performance. NEW-OPTIMISTIC-AMSGRAD is compared to AMSGRAD with the same parameters for a fair comparison. The only tuning parameter for NEW-OPTIMISTIC-AMSGRAD is r, which is set to 15 for all experiments. The study compares different methods on MNIST, CIFAR10, and IMDB datasets, using a noisy version of MNIST for increased training difficulty. The experiments start with fully connected neural networks for multi-class classification on the MNIST-back-rand dataset. The dataset consists of 12000 training samples and 50000 test samples with random background inserted in MNIST images. A multi-layer neural network with hidden layers using ReLu's is used, with a mini-batch size of 128. Performance is evaluated using multi-class cross entropy loss. NEW-OPTIMISTIC-AMSGRAD shows improvement in convergence speed and training loss compared to AMSGRAD. AMSGRAD-DISZ performs similarly to AMSGRAD. Neural Networks (CNN) are crucial in deep learning applications like computer vision and natural language processing. NEW-OPTIMISTIC-AMSGRAD is tested in deep CNN's with dropout using the CIFAR10 dataset. The ALL-CNN architecture is implemented with specific layers and dropout probabilities. The cost function is multi-class cross entropy, and the batch size is 128. NEW-OPTIMISTIC-AMSGRAD accelerates learning, as shown in the training loss provided in FIG5. In natural language processing tasks, Recurrent Neural Networks (RNN's) with Long-Short Term Memory (LSTM) are commonly used to consider sequence dependency. NEW-OPTIMISTIC-AMSGRAD is tested in RNN's using the IMDB movie review dataset from BID16, showing improved performance in high data sparsity conditions. The model includes a word embedding layer with 5000 input entries and each word is embedded into a 32 dimensional space. The model includes a word embedding layer with 5000 input entries and each word is embedded into a 32 dimensional space. The output of embedding layer is passed to 100 LSTM units, connected to 100 fully connected ReLu's before reaching the output layer. Binary cross-entropy loss is used with a batch size of 128. NEW-OPTIMISTIC-AMSGRAD shows improved convergence speed, achieving training loss faster than vanilla AMSGRAD. AMSGRAD-DISZ performs less effectively on this dataset. NEW-OPTIMISTIC-AMSGRAD demonstrates good performance with sparse gradients. NEW-OPTIMISTIC-AMSGRAD combines optimistic learning and AMSGRAD to enhance optimization for deep neural networks. The optimistic step can be applied to other algorithms like ADAM and ADAGRAD. The algorithm's performance is affected by the parameter r, the number of previous gradients used. Choosing r too small may lead to off-track training initially. The algorithm NEW-OPTIMISTIC-AMSGRAD combines optimistic learning with AMSGRAD for deep neural network optimization. The parameter r, the number of previous gradients used, impacts performance. An ideal range is suggested as 10 \u2264 r \u2264 20, with r = 15 performing well in most tasks. An \"early start\" approach can make the algorithm more flexible by gradually increasing the number of past gradients used for prediction. After fixing the moving window size for optimistic prediction at 20 after iteration 21, experiments were conducted on NEW-OPTIMISTIC-ADAM and ADAM-DISZ. The results showed that adding an optimistic step improved the performance of the ADAM optimizer. Theorems 1 and 2 are proven by rearranging terms, using Young's inequality, and analyzing the update rule. Theorems show improvements in performance by adding an optimistic step to the ADAM optimizer. Theorem 2 presents a convex online learning problem where ADAM-DISZ has nonzero average regret. The loss function is linear, and the learner's decision space is defined. The goal is to show specific conditions for the execution of ADAM-DISZ. The loss function sequences are chosen for a new coordinate system where the initial point is 1. The update of ADAM-DISZ is discussed, showing that w 3t+4 = 1. If \u0175 3t+3 \u2265 1, then w 3t+4 is calculated accordingly. The update of ADAM-DISZ is discussed, showing that w 3t+4 = 1. The regret over T rounds would be (2C \u2212 4)T /3, which is not sublinear to T. The proof is completed by generalizing the result so that ADAM-DISZ does not converge for any \u03b2 1 , \u03b2 2 \u2208 [0, 1) such that \u03b2 1 < \u221a \u03b2 2. The regret analysis is provided for the OPTIMISTIC-ADAGRAD algorithm, introducing notations and assumptions. The Mahalanobis norm and distance generating function are defined, along with the 1-strong convexity property of \u03c8 t (\u00b7). The distance generating function \u03c8 t (v)\u2212 \u03c8 t (v), u\u2212v is defined, along with the associate dual norm \u03c8 * t (x) := x, diag{v t } \u22121/2 x. The model parameter w is in d-dimensional space, w \u2208 Rd. Algorithm 6 is analyzed for any convex set K, reducing to Algorithm 5 when K = Rd."
}