{
    "title": "rJlqoTEtDB",
    "content": "In this paper, a novel technique called PowerSGD is proposed to enhance the stochastic gradient descent method for training deep networks. PowerSGD involves raising the stochastic gradient to a power during iterations, introducing a power exponent parameter. PowerSGDM, which includes momentum, is also introduced. Convergence rate analysis is provided for both methods, with experiments showing faster initial training speed, comparable generalization ability to SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD acts as a gradient modifier through a nonlinear transformation. PowerSGD is a gradient modifier that enhances stochastic gradient descent for training deep networks. It involves raising the stochastic gradient to a power during iterations, introducing a power exponent parameter. Various stochastic optimization algorithms like SGD with Momentum, AdaGrad, RMSProp, and Adam have been proposed to train deep neural networks more efficiently. Despite the popularity of Adam, its generalization remains a topic of interest. Adaptive methods like AdaGrad, RMSProp, and Adam show faster convergence rates initially but plateau on testing data. AMSGrad was proposed as a variant of Adam to address convergence issues, with theoretical guarantees but similar generalization to Adam on test data. However, a performance gap still exists between AMSGrad and SGD. SGD is chosen over adaptive methods in recent works in natural language processing and computer vision, as it performs better. However, obtaining sharp convergence results for SGD in non-convex settings remains an active research topic. SGD iteratively updates model parameters by moving in the direction of the negative gradient, while SGDM brings a Momentum term for faster convergence. SGDM introduces a Momentum term for faster convergence compared to SGD. AdaGrad, a form of adaptive learning rate technique, updates parameters based on past gradients, leading to rapid decay in learning rate for dense gradients. RMSProp was proposed to address this issue by computing the exponential moving average of past squared gradients. The Adam optimizer, a derivative of AdaGrad and RMSProp, updates weights based on recent gradients' mean and root mean square. Research on linking discrete gradient-based optimization to dynamic system theory is gaining attention. The PoweredSGD optimizer enhances initial training and can be combined with learning rate schedules. The Powerball function modifies gradient terms in a nonlinear way, differentiating it from other techniques focusing on learning rates and momentum terms. In this paper, the authors propose modifying gradient terms using the Powerball function by Yuan et al. (2016). They systematically present methods for stochastic optimization with and without momentum, provide convergence proofs, and conduct experiments with deep learning models and benchmark datasets. Another related work by Bernstein et al. (2018) introduces a version of stochastic gradient descent using only the signs of gradients, which corresponds to a special case of PoweredSGD. The PowerSign optimizer proposed by Bello et al. (2017) is different as it involves conditional scaling of gradients. The paper introduces PoweredSGD, a stochastic optimizer that applies the Powerball function to the gradient, with a variant called PoweredSGDM for improved convergence and generalization. Convergence rates for both methods are proven to be the best known. The PoweredSGD and PoweredSGDM algorithms achieve the best convergence rates for SGD and SGDM on non-convex functions. Experimental studies demonstrate faster convergence rates in early training compared to adaptive gradient methods, with comparable generalization ability to SGD and SGDM. In this section, PoweredSGD and PoweredSGDM algorithms are proposed, combining Powerball function technique with stochastic gradient descent. Section 3 provides convergence results for non-convex optimization, while Section 4 presents experimental results demonstrating the superiority of the algorithms. Train a DNN with n free parameters can be formulated as an unconstrained optimization problem where f (\u00b7) : R n \u2192 R is a function bounded from below. SGD is efficient for high-dimensional optimization, updating the parameter vector x t \u2208 R n in the opposite direction of a stochastic gradient g(x t , \u03be t ) on mini-batches. PoweredSGD and PoweredSGDM converge and achieve competitive rates on non-convex functions, showing advantages over other optimizers for training deep networks. The PoweredSGD algorithm introduces a nonlinear transformation called the Powerball function, with a parameter \u03b3 to adjust its intensity. It is applied to the stochastic gradient term in the update rule, resulting in the PoweredSGD algorithm with an additional parameter \u03b3 \u2208 [0, 1]. When \u03b3 = 1, it becomes vanilla SGD. The algorithm combines the momentum trick with SGD to give SGDM, which shows better performance on non-convex functions. The PoweredSGD algorithm introduces a nonlinear transformation called the Powerball function with a parameter \u03b3. It combines with SGD to give SGDM, which shows better performance on non-convex functions. PoweredSGD with Momentum (PoweredSGDM) is proposed, with an update rule detailed in Algorithm 2. Convergence results of PoweredGD and PoweredSGDM in the non-convex setting are presented, assuming a stochastic first-order black-box oracle for gradient estimation. The PoweredSGD algorithm introduces a Powerball function with parameter \u03b3, combining with SGD to give SGDM for better performance on non-convex functions. PoweredSGDM is proposed with an update rule detailed in Algorithm 2. The main convergence result for PoweredSGD is stated, showing potential to outperform popular stochastic optimizers. Theorem 3.1 states that PoweredSGD with an adaptive learning rate and mini-batch size can lead to improved performance. The PoweredSGD algorithm introduces a Powerball function with parameter \u03b3 for better performance on non-convex functions. It can outperform popular stochastic optimizers by allowing the parameter \u03b3 to be tuned for different training cases. The convergence bound reduces to the best known rate for SGD when \u03b3 = 1, with an effective convergence rate of O(1/ \u221a T ). PoweredSGD can achieve convergence in the order O(1/T) when exact gradients are used. PoweredSGD introduces a Powerball function with parameter \u03b3 for improved performance on non-convex functions. It can outperform popular stochastic optimizers by tuning \u03b3 for different training cases. Convergence analysis for PoweredSGDM with adaptive learning rate and mini-batch size is presented, matching the best known rate of convergence in special cases. The upper bound for convergence rates in special cases (\u03b3 = 0, 1) continuously interpolates for varying \u03b3 in [0, 1] and \u03b2 in [0, 1). The key technical result enabling these findings is Lemma B.1 in the Supplementary Material, providing tight estimates of momentum terms. New convergence rates for \u03b3 \u2208 (0, 1) are introduced, with a different proof approach for \u03b3 = 0, 1 compared to previous studies. A large mini-batch (B t = T) is assumed for convergence results, aligning with previous analyses for \u03b3 = 0. The section aims to showcase the efficiency of PoweredSGD and PoweredSGDM algorithms through experiments on various model architectures and datasets, comparing them with SGDM, AdaGrad, RMSprop, and Adam. The experiments are divided into convergence and generalization tests, as well as Powerball feature experiments, with detailed setups provided in Table 1. The experiments conducted compare PoweredSGD and PoweredSGDM with other adaptive methods like Adam and RMSprop. Results show that PoweredSGD and PoweredSGDM exhibit better convergence rates and achieve better performance on test sets. The experiments compared PoweredSGD and PoweredSGDM with other adaptive methods like Adam and RMSprop, showing better convergence rates and performance on test sets. The proposed methods achieved better generalization performance than adaptive methods, although slightly worse than SGDM. Learning rates were tuned on a logarithmic scale around the default rate for each optimization method, with momentum set to 0.9. Based on experiments, different optimization methods like AdaGrad, RMSprop, and Adam have specific learning rates and parameters that need to be carefully tuned for optimal performance. Adaptive methods generally perform worse than non-adaptive methods, but tuning the initial learning rate can lead to significant improvements. To compare with adaptive methods, the best performing learning rate is chosen and tested with its closest neighbor. In experiments, different optimization methods like AdaGrad, RMSprop, and Adam require careful tuning of learning rates and parameters for optimal performance. The best performing learning rate is chosen and tested with its closest neighbor to iteratively update until performance cannot be improved further. Experiments on ResNet-50 and WideResNet models on CIFAR-10 and CIFAR-100 show that adaptive methods converge faster and perform better than non-adaptive methods. Experiments on CIFAR-100 with WideResNet and ImageNet with ResNet-50 models show that PoweredSGD and PoweredSGDM outperform other adaptive methods in terms of convergence rates and test accuracy. AdaGrad quickly plateaus due to excessive parameter updates. In deep learning, gradient vanishing can hinder training deep neural networks with SGD. PoweredSGD method effectively rescales stochastic gradients to alleviate this issue. Experiments on MNIST dataset with a 13-layer neural network show promising results. PoweredSGD and PoweredSGDM outperform adaptive methods in convergence rates and test accuracy on CIFAR-100 and ImageNet datasets. The PoweredSGD algorithm utilizes a 13-layer neural network with ReLU activation functions. It introduces the Powerball accelerated gradient descent algorithm, showing convergence properties for different values of \u03b3. The method is proven effective in rescaling stochastic gradients to address the issue of gradient vanishing in deep learning. The PoweredSGD algorithm utilizes a 13-layer neural network with ReLU activation functions and introduces the Powerball accelerated gradient descent algorithm. To analyze the convergence of PoweredSGD, understanding the relation between mini-batch size and variance reduction of SGD is crucial. The proof of Theorem 3.1 involves estimating the gradient of f using stochastic gradient oracles and applying Lipschitz continuity. The proof for the PoweredGDM scheme with an adaptive learning rate shows convergence with a specific update rule involving a momentum constant. The scheme reduces to PoweredGD when the momentum constant is 0. Theorem B.1 states conditions under which the PoweredGDM scheme can lead to convergence, with a formula for the number of iterations. The PoweredGDM scheme satisfies the L-Lipschitz continuity of \u2207 f. By choosing \u03b5 > 0, we can estimate the bound. For any \u03b2 \u2208 [0, 1), the bound can be minimized to give the theorem. The PoweredSGDM scheme also satisfies the L-Lipschitz continuity of \u2207 f. By choosing \u03b5 1 > 0, we can estimate the bound. Theorem B.1 is proven by estimating \u03b5 > 0 with L-Lipschitz continuity of \u2207 f. Setting \u03b5 1 = (1\u2212\u03b2 ) 2 L\u03b2 and (1\u2212\u03b2 ) 2 1+\u03b2 leads to the bound in the theorem. Theorems 3.1 and 3.2 show sharp estimates for convergence rates of PoweredSGD and PoweredSGDM. The estimates are in terms of stochastic gradients g t without loss of generality. The Powerball function can help amplify gradients when they approach zero, alleviating the vanishing gradient problem in deep neural network training. PoweredSGD and PoweredSGDM show sharp convergence rate estimates in terms of stochastic gradients. The Powerball function can amplify gradients to alleviate the vanishing gradient problem in deep neural network training. PoweredSGD was tested on deep networks with varying \u03b3 and learning rates, showing that when network depth exceeds 13 layers, adjusting the learning rate of SGD does not solve the issue. PoweredSGD, with the Powerball function, can effectively address the vanishing gradient problem. PoweredSGD with the Powerball function amplifies gradients to alleviate the vanishing gradient problem in deep neural network training. However, when the network exceeds 15 layers, both SGD and PoweredSGD struggle to train further due to the ratio of amplified gradients becoming too large. Combining PoweredSGD with other techniques may help address the vanishing gradient problem further. The Powerball function with a tunable hyper-parameter \u03b3 accelerates optimization by reducing large gradients and addressing the exploding gradient problem. Testing on ResNet-50 and DenseNet-121 with different \u03b3 values showed that \u03b3 between 0.5 to 1.0 yielded good test accuracy without significant loss. PoweredSGD transitions to vanilla SGD when \u03b3 = 1, and results are visualized in Fig. 5. PoweredSGD with a hyper-parameter \u03b3 can improve test accuracy compared to SGD, with \u03b3 = 0.8 being a suitable choice in most cases. The range of \u03b3 selection widens as the learning rate decreases, with \u03b3 between 0.4-0.8 providing enhanced robustness to changing learning rates. In the main part of the paper, experiments show that PoweredSGD can lead to faster initial training and is complementary to other techniques for improved learning. Preliminary experiments combining learning rate schedules with PoweredSGD demonstrate improved performance. In (Loshchilov & Hutter, 2016), PoweredSGD with warm restarts policy improves convergence and test accuracy on CIFAR-10 dataset with ResNet-50. Choosing a \u03b3 in the range of 0.4-0.6 provides better robustness to changing learning rates. PoweredSGDR achieved the lowest training error compared to other methods. PoweredSGDR achieved the lowest training error and improved test accuracy to 94.64% compared to PoweredSGD's 94.12%. The Powerball function's nonlinear gradient transformation is shown to be orthogonal and complementary to existing methods, potentially enhancing performance when combined with other techniques. Hyper-parameter settings for best test accuracy can be found in Table 3."
}