{
    "title": "BJeKwTNFvB",
    "content": "We propose a model for physical parameter estimation from video without labeled states or objects. Our approach combines vision-as-inverse-graphics and differentiable physics engines to discover objects and state representations. This enables long-term video prediction and model-predictive control, outperforming unsupervised methods in predicting systems with interacting objects. The curr_chunk discusses the value of integrating vision and physics in learning model-based control for systems like pendulums. It highlights the interpretability of the controller for goal-driven control and zero-data adaptation. The text also mentions the challenges of system identification and physical parameter estimation in physical modeling. In this work, the physics-as-inverse-graphics approach bridges the gap between unsupervised object discovery from video and learning physical dynamics. It utilizes a vision-as-inverse-graphics encoder-decoder system with Spatial Transformers to generate interpretable states like position and velocity for model-based control. Our contribution is a solution to unsupervised learning of physical parameters from video without ground-truth data. This allows for learning physical parameters and vision components jointly in an end-to-end fashion. The model can learn physical parameters without object or state supervision, showcasing the incorporation of dynamics priors for known physical systems. Incorporating dynamics priors with learnable parameters and vision improves model performance in video prediction and control tasks. The method is evaluated on various datasets and applied to vision-based control of an under-actuated pendulum, enabling data-efficient learning and zero-shot adaptation. The ability to extract interpretable states and parameters from pixels without supervision is highlighted, allowing for end-to-end control using goal-paramaterized policies and physical reasoning. The success of modern neural architectures is attributed to the ability to build inductive bias through structure. The success of modern neural architectures is attributed to the ability to build inductive bias through structure. Convolutional operations capture spatial correlations in images, recurrency allows for temporal reasoning, and spatial transformers provide spatial invariance in learning. However, incorporating high-level physical interaction knowledge, such as the underlying physics of a dynamic visual scene, is not yet considered by these architectures. Physics-as-inverse graphics introduces a framework to incorporate this information into learning for predicting the future of visual scenes or control tasks. In recent years, there has been increased interest in physical scene understanding from video, aiming to learn explicit physical dynamics by discovering and modeling objects in a scene with position as a latent variable. This builds on the neural vision-as-inverse-graphics literature, particularly using spatial transformers for rendering. Various models assume knowledge of equations governing system dynamics, but focus on individual object interactions. Our model is the first to offer end-to-end unsupervised physical parameter and state estimation, inspired by the Galileo model and Physics 101 dataset. It bypasses the difficulty of recognizing and tracking objects from video using a neural system. Belbute-Peres et al. (2018) found that a MLP encoder-decoder with a physics engine requires supervision with position/velocity labels to learn. Our work incorporates vision-as-inverse-graphics with physics to enable learning without labels. Unsupervised discovery of objects and dynamics from video is an area of interest, but models lack interpretable latent representations usable by a physics engine. Our work differs from previous studies by obtaining explicit cartesian, angular, or scale coordinates for objects in a scene, allowing direct feeding into a differentiable physics engine. Ehrhardt et al. (2018) developed an unsupervised model for consistent object locations but only for cartesian coordinates, not angles or scale. Our architecture includes an encoder estimating object positions, a velocity estimator for velocities, and initial conditions for a physics engine. The physics engine uses positions and velocities from the last input frame as initial conditions. It outputs positions for the decoder to predict images. Model-based control systems are more robust and sample efficient than model-free reinforcement learning. Hafner et al. (2019) developed a latent dynamics model (PlaNet) for planning from pixels, which is more efficient than A3C and D4PG. When used for control, visually grounded controllers are desired for known dynamics, providing transferability and generality. Byravan et al. (2018) use supervised learning to segment objects and learn feedforward models with REINFORCE to predict physical states. Unsupervised system identification is achieved from pixels, enabling data-efficient vision-actuated model-based control. Learning explicit physics from video requires identifying and representing objects in an image. The architecture for dynamics prediction involves representing object positions and velocities as latent states using a sequence-to-sequence video prediction model with an encoder, velocity estimator, physics engine, and graphics decoder. The encoder extracts object coordinates from a frame using a 2-stage localization approach with masks generated by a U-Net. The architecture for dynamics prediction includes unnormalized masks stacked and passed through a softmax to produce N + 1 masks. A 2-layer location network produces coordinate outputs from each masked input component. The velocity estimator computes the velocity vector of each object using a 3 hidden layer MLP. The physics engine contains a differentiable physics engine. The physics engine in this work contains differential equations governing the system with unknown physical parameters to be learned. It uses Euler integration with physical parameters and force applied to each object. The decoder takes input positions and outputs predicted images. The decoder is a critical part of the system, allowing the encoder, velocity estimator, and physics engine to train in an unsupervised manner. An unconstrained decoder can learn erroneous representations of Cartesian space, leading to incorrect associations between input vectors and object locations in the image. The representation is crucial for correct future predictions in the physics engine. Spatial transformers are used to ensure a correct correspondence between latent and pixel coordinates, improving on previous methods. The decoder's writing attention mechanism is guided by transformer parameters to place attention windows accurately in the image. The decoder uses a spatial transformer with inverse transformation parameters to achieve coordinate-consistent outputs. Each object is represented by learnable content and mask. The decoder architecture utilizes spatial transformers and masks to model depth ordering and capture occlusions between objects. The use of a constrained decoder ensures consistent object locations, while the encoder's training signal is derived from the physics engine. To improve encoder/decoder representation, a static per-frame autoencoder loss is added during training. Mean-squared error loss is used, and long-term prediction is evaluated by predicting additional frames during testing. The model outperforms Interaction Networks (IN), DDPAE, and VideoLSTM in frame prediction accuracy. Interaction Networks (IN), DDPAE, and VideoLSTM excel in extrapolation by incorporating explicit physics. The model is trained on scenes with various settings, including balls bouncing off edges, balls connected by a spring, and balls with gravitational pull. Different physical parameters are learned for each setting, such as spring constant and equilibrium distance for spring systems, and gravity constant or mass for gravitational systems. The model is trained on scenes with different settings like balls bouncing off edges, connected by a spring, and with gravitational pull. Various physical parameters are learned for each setting, such as spring constant, equilibrium distance, gravity constant, and mass. The datasets consist of 5000 sequences for training, 500 for validation, and 500 for testing. The model is compared to 3 strong baselines: DDPAE, VideoLSTM, and Interaction Network + Inverse-Graphics. The proposed model, using an Interaction Network for dynamics, successfully identifies physical parameters from pixels and outperforms other models in extrapolation range predictions. Our model's predictions are highly accurate even many steps into the future, outperforming other black-box models in the extrapolation range. The use of an explicit physics model is valuable in systems with non-linear yet well-defined dynamics. The difference in performance is attributed to the fact that some dynamics that are harder to predict do not appear during training but occur frequently during testing. Our model's accurate predictions into the future outperform other black-box models. The use of an explicit physics model is valuable for non-linear dynamics. The difficulty in predicting certain dynamics not seen during training affects performance during testing. The generality vs specificity tradeoff impacts model inference. Joint training with a coordinate-consistent decoder is crucial for success. In comparing different training methods on the 3-ball gravity dataset, joint training with prediction and autoencoder losses yields the best performance. The coordinate-consistent decoder is essential for accurate predictions, as it allows the model to learn encoders/decoders that can be correctly used by the physics engine. The prediction loss is crucial for ensuring that the forces between objects remain consistent even if their positions change. Learning the encoder/decoder along with the velocity estimator and physics engine on the prediction loss ensures that the forces between objects remain consistent even if their positions change. This allows for zero-shot adaptation to domain-shift in gravity and goal-driven control to balance. Further ablations on the decoder architecture show its ability to correctly render objects in unseen regions of the image. The method involves training a DDPG agent on the latent space of an autoencoder with 320k images for zero-shot adaptation to domain-shift in gravity and goal-driven control for balancing a pendulum. The application includes identifying dynamical parameters and states of a physical system from video, enabling vision-based planning and control, demonstrated on the pendulum from OpenAI Gym. The model is trained using 5000 sequences of 14 frames with random initialization and actions. The physical parameters to learn are gravity and actuation coefficient. Model-predictive control is performed using the learned parameters and a cross entropy method. The model is compared to an oracle model and other state-of-the-art RL methods. Our model-based RL method successfully recovers gravity and force coefficient values from vision alone, enabling correct planning and control. It demonstrates data efficiency comparable to PlaNet and outperforms DDPG in speed. The explicit physics in our model allows for counter-factual reasoning and zero-shot adaptation to new environmental parameters. Our model can exploit reasoning to succeed over a wide range of gravities without re-training. It enables goal-paramaterized control, reaching any specified target angle directly. The capabilities are provided automatically by our model's interpretable representation, not requiring further adaptive learning like reward-based or implicit physics approaches. The approach presented here shows promising results in physical parameter estimation, long-term video prediction, and MPC. However, limitations need to be overcome for real-world application, such as the need for improved object representation to model real-world scenes effectively. The paper discusses the use of a sequence-to-sequence architecture for modeling scenes with moving objects. It suggests future work on formulating the model in a probabilistic manner for better state filtering and smoothing during inference. Additionally, it mentions the challenge of adapting the model to varying scene backgrounds. The paper introduces a model that integrates physics into deep learning for object discovery and accurate long-term prediction. This approach allows for unsupervised object tracking, system identification, and flexible control. The model relies on a coordinate-consistent decoder for image reconstruction from physics, addressing challenges in incorporating structure into deep learning models. The equations of motion for different systems are described, including 2-balls and 2-digits spring, 3-balls gravity, and Pendulum following OpenAI Gym environment equations. An actuation coefficient a is introduced in the physics engine model, along with g. Training is done using RMSProp with specific parameters for different datasets. The image sizes vary for different datasets, with content and mask variables generated by a neural network. Fully-connected decoders have limitations in decoding object poses not seen during training. The decoder's lack of inductive bias results in incorrect extrapolation in image space. By comparing different decoder models in a spatial extrapolation experiment, it was found that a black-box decoder has insufficient extrapolation ability. In contrast, a rendering decoder can correctly decode states not seen during training. Our rendering decoder can accurately decode states not encountered during training, giving models like ours an advantage in data efficiency. However, this advantage does not extend to inferring states from images with objects in unseen regions due to limited de-rendering biases in the encoders used. The model proposed assumes knowing the number of objects in the scene. When using an incorrect number of object slots, the model's performance is subpar. With too few slots, objects are missed, and with too many slots, interactions between objects and empty slots confuse the model. Results show confusion in dynamics due to incorrect number of object slots in the physics engine for the 3-body gravitational system. Left side shows contents and masks learned for 2 object slots, while the right side shows contents and objects learned for 4 object slots."
}