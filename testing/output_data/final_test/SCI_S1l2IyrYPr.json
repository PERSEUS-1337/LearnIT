{
    "title": "S1l2IyrYPr",
    "content": "Recent improvements in large-scale language models have led to advancements in generating syntactically and semantically consistent text for various applications. However, training on large corpora can result in models internalizing social biases present in the data. This paper aims to quantify and reduce biases in language models by analyzing how changes in sensitive attributes affect the sentiment of generated text. Fairness metrics from the fair machine learning literature are used for evaluation on different corpora. State-of-the-art Transformer-based language models exhibit biases learned from data, as shown in extensive evaluation on news articles and Wikipedia. Proposed regularization methods improve fairness metrics without affecting perplexity and semantic similarity, advancing the development of fairer language models. Text representation learning methods trained on large unlabeled corpora have led to improved model performances on various tasks, but recent studies have found human-like biases in word embedding models. Both context-free and context-dependent word embedding models exhibit human-like semantic biases, including gender and race. Language models, such as Transformer-based models, have been shown to inherit biases from web corpora. This study focuses on analyzing systematic variations in sentiment scores generated by a Transformer-based language model trained on Wikipedia and news articles. The analysis includes control variables like country names, occupations, and person names in the context. The study analyzes sentiment biases in a Transformer-based language model trained on Wikipedia and news articles, focusing on variations in sentiment scores with control variables like country names, occupations, and person names. Two methods are proposed to reduce counterfactual sentiment biases, involving embedding and sentiment similarity. Experimental results show a reduction in biases while maintaining generation capability, measured by perplexity and semantic similarity. Our method addresses various fairness specifications in language models by quantifying systematic counterfactual sentiment biases and proposing methods to reduce them. We demonstrate the efficacy of embedding and sentiment similarity-based techniques in training language models to be invariant to input transformations. The study uses a sentiment classifier to measure biases and acknowledges the need for an unbiased evaluator in future research. Language models estimate the probability of a sequence of tokens occurring in natural language. Using conditional probabilities, sequences can be generated with long-range dependencies. Transformer-based models are used to learn these probabilities, which have been shown to scale effectively. Bias in Natural Language Processing Systems is a prevalent issue, with NLP models exhibiting cultural associations and social biases. These biases are observed across various NLP tasks, such as gender bias in coreference resolution, image captioning, and text classification. Sentiment analysis also shows systematic biases related to race and gender in over 200 systems. In a study from 2018, systematic biases based on race and gender were found in over 200 systems. Occupational gender bias in word embeddings has been addressed through measuring projections onto gender-related subspaces. However, Gonen & Goldberg (2019) noted limitations to this approach. Lu et al. (2018) proposed counterfactual data augmentation as a remedy for occupation-specific gender biases in language modeling. Qian et al. (2019) regularize a generative language model to predict similar log-probabilities for gendered options. In contrast to prior works on gender bias in language models, this study probes the output of language models using sentiment analysis. The models are trained on large datasets like English news articles and WikiText-103, with a focus on Transformer-based models similar in size to GPT-2. The study does not rely on gendered word pairs for data augmentation or approximating linear gender subspaces. A fundamental group fairness definition is \"equality of odds\", requiring equal false positive and false negative prediction rates across demographic subgroups. Postprocessing methods can superficially satisfy group fairness but may impact individual fairness. Causal inference tools are used in fairness research to address issues like \"corrective discrimination\". Counterfactual fairness necessitates consistent model predictions. Counterfactual fairness, like individual fairness, ensures consistent model predictions before and after intervention on sensitive attributes in causal graphs. The goal is to achieve counterfactual fairness by de-biasing input representations in the latent space of text generative models, promoting fair representations and independence between sensitive attributes and prediction outputs. The study demonstrates biased sentiment in a large-scale language model by conditioning it with different occupations, showing systematic sentiment differences. This bias highlights the need to address fairness in model predictions by de-biasing input representations. Our proposed approach aims to reduce counterfactual sentiment biases in language models based on predefined specifications of sensitive attribute variables. This involves generating counterfactual inputs by replacing sensitive tokens with tokens from randomly chosen attributes, while keeping non-sensitive tokens unchanged. This process is done using a sentiment classifier and a pretrained language model. The language model generates a sentence sentiment score based on input x, with a distribution denoted by PS(x). Sentiment classification involves predicting \u0177 = S > \u03c4 using a decision threshold \u03c4. Fairness concepts like \"demographic parity\" and \"demographic disparity\" are used to measure fairness between subgroups. The concept of fairness in sentiment analysis involves measuring demographic disparity by comparing positive sentiment rates between different subgroups. To achieve \"Strong Demographic Parity,\" the raw output distributions need to match instead of relying on a predetermined decision threshold. Statistical disparity is computed by averaging over random choices, with the Wasserstein-1 distance used to measure sentiment bias in counterfactual evaluation. The counterfactual fairness specification for sentiment biases involves measuring Wasserstein-1 distance between output sentiment distributions. It addresses individual fairness by treating similar individuals similarly based on non-sensitive tokens. This specification evaluates the output distribution of a generative model, different from prior work on individual predictions in NLP models. Fairness evaluation includes measuring individual and group fairness metrics from sentiment scores distributions on the evaluation set. The evaluation set measures sentiment scores using Wasserstein-1 distance for individual and group fairness metrics. Individual fairness is computed by averaging the distance between sentiment score distributions of evaluation sentences and their counterfactual sentences. Group fairness involves separating sentences into subgroups based on sensitive attributes and measuring the distance between sentiment distributions of generated sentences within each subgroup. The Total Group Fairness metric is calculated by measuring the Wasserstein-1 distance between sentiment distributions of generated sentences within subgroups and the entire evaluation set. To reduce counterfactual sentiment bias, language models are trained to produce similar sentiment distributions for perturbed prefixes with tokens from different groups. The approach explores fairness through embedding and sentiment similarity using hidden features from a language model to represent future generated sequences. The goal is to ensure that the embedding of sequences are close enough to reduce counterfactual sentiment bias. The \"embedding similarity\" approach aims to ensure that the embeddings of sequences are close enough to reduce counterfactual sentiment bias. This is achieved by defining fairness loss as a distance between embeddings and using the cosine distance for comparison. The method involves averaging the last 2 layers' embeddings to capture high-level semantics effectively. However, enforcing embedding similarity may lead to a small difference between embeddings, reducing regularization effectiveness. The regularization effectiveness of enforcing embedding similarity can be compromised by its strong nature, potentially causing the model to generate similar texts for all inputs. To address this, an alternative method using sentiment classifiers is proposed to eliminate sentiment biases. By applying the sentiment classifier to hidden representations and measuring the sentiment similarity, fairness can be achieved without sacrificing model performance. The text discusses using sentiment classifiers to measure sentiment similarity in a multi-dimensional space, aiming to regulate sentiment differences without impacting model perplexity. A three-step curriculum training scheme is employed to implement the proposed embedding similarity and sentiment similarity approach, which can potentially improve language models' performance on test sets. The text discusses a curriculum training scheme to implement embedding and sentiment similarity approaches. It involves training a language model with cross-entropy loss, training a sentiment classifier using extracted features, and continuing language model training with fairness loss based on embedding or sentiment similarity. The language model is trained on negative log-likelihood or cross-entropy loss for predicting the next token of input. A \"debiasing step\" is included in the process. Two TransformerXL models are trained on Wikipedia and English news articles datasets. The training data for GPT-2 models is not used. The WikiText-103 dataset consists of over 28,000 articles from Wikipedia. The language model is trained on Wikipedia and English news articles datasets using TransformerXL models with different parameters. The model achieved 17.06 perplexity on the WikiText-103 dataset and 17.46 perplexity on the WMT-19 dataset. A sentiment classifier is trained using a 3-layer MLP network with hidden layer size 128 for sentiment feature projection. Labels for sentence sentiment are generated using the Google Cloud sentiment analysis API. The sentiment classifier achieves over 98% test accuracy on both datasets by keeping sentences with high sentiment scores. To measure counterfactual sentiment biases, sensitive attributes like Country, Occupation, and Name are examined. Each category has specific sensitive tokens detailed in Appendix A. In Appendix A, sentence templates are designed to evaluate counterfactual sentiment biases for sensitive attributes. Each template consists of a sentence prefix with a placeholder for a sensitive token. An external sentiment classifier is applied to generated sentences, and a simpler counting-based sentiment classifier is used for evaluation. The counting-based sentiment classifier is introduced to avoid bias in sentiment analysis systems. It counts positive and negative opinion words to determine sentiment scores, which helps in gauging sentiment without sensitive attributes. This method is less accurate but less prone to biased judgments compared to blackbox sentiment models. The text discusses the trade-off between relevance and fairness in a sentiment analysis system. It highlights the importance of balancing generation quality and fairness by evaluating metrics like perplexity on the test set with sensitive attributes. The model's performance can deteriorate if biased towards fairness, ignoring sensitive attributes, impacting the original language model's performance. The text discusses evaluating language model performance on a subset of the test set with sensitive attributes to measure semantic similarity. It suggests using a universal sentence encoder to calculate cosine similarity and identify irrelevant sentences based on a threshold. Monitoring semantic similarity ratio can indicate if generated sentences capture original semantics effectively. The text discusses training language models with different losses and regularization strengths on WMT-19 and WikiText-103 datasets. Performance is reported in Tables 2 and 3, showing reduced bias in fairness metrics using sentiment API and counting-based sentiment scores. The proposed approaches achieve reduced bias in both individual and group fairness metrics by using different regularization parameters for the fairness loss. Strong regularization can reduce bias but also slightly decrease semantic similarity scores. Balancing the trade-off between model performance is possible by choosing different regularization parameters. Very strong regularization can result in models generating almost identical texts for different attributes. The proposed methods maintain a similar level of perplexity on the test set with sensitive attributes. The proposed methods aim to reduce bias in individual and group fairness metrics by adjusting regularization parameters. Strong regularization can decrease bias but may also lower semantic similarity scores. Balancing model performance involves choosing appropriate regularization parameters. Very strong regularization can lead to models generating similar texts for different attributes. The perplexity on the test set with sensitive attributes remains consistent. Table 2 and Table 3 show the performance of language models trained on different datasets, with metrics such as perplexity and fairness measures. Lower numbers are better for most metrics. The proposed methods aim to reduce bias in fairness metrics by adjusting regularization parameters, which can impact semantic similarity scores. The study compares language models trained on different datasets using metrics like perplexity and fairness measures. The results show that bias in fairness metrics is lower in WikiText-103 compared to WMT-19, possibly due to the data source and model characteristics. Additionally, qualitative examples are provided in Table 4. The study compares language models trained on different datasets using metrics like perplexity and fairness measures. Qualitative examples are provided to show differences in model performance. In this paper, the focus is on assessing and mitigating bias in large-scale transformer-based language models. The study evaluates biases in terms of individual and group fairness metrics using embedding-similarity and sentiment-similarity based methods. The research aims to address counterfactual sentiment biases in language models. The study focuses on reducing counterfactual sentiment biases in language models using embedding-similarity and sentiment-similarity based methods. The proposed framework aims to address various fairness specifications and is a step towards deploying fairer language models. Future work includes extending the framework to study biases related to religion, ethnicity, age, or multiple-attribute cross-subgroups. Additional experimental details for training and evaluating the models are provided. The study focuses on reducing counterfactual sentiment biases in language models through debiasing techniques. Training details include using different optimizers, learning rates, batch sizes, and training steps for two datasets. Language model debiasing involves reducing learning rates and adjusting training steps. In the study, language model debiasing techniques are used to reduce counterfactual sentiment biases. Sample generation involves sampling sentences per template with specified sensitive attributes to estimate fairness metrics. A model trained with large embedding similarity regularization produces almost identical outputs for different occupations, with low semantic similarity scores. The study uses language model debiasing techniques to reduce sentiment biases in different occupations. Models with sentiment similarity achieve higher semantic scores and better individual fairness compared to embedding similarity models. The proposed approaches significantly improve individual fairness compared to baseline models. The study discusses gender biases in occupation, specifically highlighting biases towards certain occupations such as \"editor\", \"teacher\", \"guard\", \"CEO\", and \"secretary\" in generated samples. The model captures the distinction between counterfactual attributes by showcasing distinct words in the samples. The study highlights gender biases in occupation, showcasing distinct words in generated samples to capture the distinction between counterfactual attributes. Specifically, it defines distinct words for category a between categories a and b as arg max w p(w|a)/p(w|b), with examples shown in Table 9."
}