{
    "title": "HJloElBYvB",
    "content": "In this paper, the authors investigate the behavior of compression and prediction terms in the Information Bottleneck objective. They study multiple phase transitions in the IB objective and observe sudden jumps in prediction accuracy with increasing strength between compression and prediction terms. The authors introduce a definition for IB phase transitions as a qualitative change in the loss landscape, showing a connection with learning new classes. They derive a formula for IB phase transitions using second-order calculus of variations and relate it to the Fisher information matrix for parameterized models. The authors present a formula for IB phase transitions, showing a connection with learning new classes. They introduce an algorithm for discovering phase transition points and verify its accuracy in predicting phase transitions in datasets like MNIST and CIFAR10. The Information Bottleneck (IB) objective trades off model compression with predictive performance using a Lagrange multiplier \u03b2. It has been effective in various scenarios such as improving robustness against adversarial attacks, learning invariant and disentangled representations, and facilitating skill discovery in reinforcement learning. The Information Bottleneck (IB) objective with a Lagrange multiplier \u03b2 balances model compression and predictive performance. In Wu et al. (2019), the authors study the learnability transition of the IB objective, observing how the transition from a trivial to a nontrivial representation relates to the dataset structure. Training Variational Information Bottleneck (VIB) models on CIFAR10 dataset at different \u03b2 values helps understand how I(Y ; Z) and I(X; Z) vary. The study focuses on the Information Bottleneck (IB) objective with a Lagrange multiplier \u03b2, analyzing the phase transitions and their dependence on the dataset structure. The IB objective involves a trade-off between prediction-loss and complexity, with insights gained from observing discontinuous behavior in I(X; Z), I(Y ; Z), and accuracy on the CIFAR10 dataset with 20% label noise. Understanding these transitions is crucial for a better grasp of the IB objective and its relationship with dataset structure and learned representations. In this work, the focus is on the Information Bottleneck (IB) objective with a Lagrange multiplier \u03b2, analyzing phase transitions and their dependence on the dataset structure. The study aims to understand the behavior of prediction loss and model complexity with varying \u03b2, and how they relate to the dataset. The techniques developed in the IB setting may also shed light on the two-term tradeoff in other learning objectives. Key contributions include identifying qualitative changes in the IB loss landscape, introducing a quantity G[p(z|x)] to prove a theorem for IB phase transitions, and revealing the interplay between the IB objective, dataset, and learned representation. Our work introduces a theoretical formula addressing Information Bottleneck (IB) phase transitions, along with an algorithm to find these transition points. The theory and algorithm accurately predict phase transitions in categorical datasets, class learning onset in MNIST, and transitions in CIFAR10 experiments. The Information Bottleneck Method provides a numerical solution for the optimal encoder distribution P(Z|X) using the Blahut-Arimoto Algorithm. The IB functional is solved numerically for the optimal encoder distribution P(Z|X) with various extensions and interpretations. Previous theoretical analyses include copula functions and generalization bounds with IB. Phase transitions have been observed in various learning domains with different learning objectives, such as in the latent representation of \u03b2-VAE and the Deterministic Information Bottleneck. These transitions are utilized to determine the optimal number of clusters for geometric clustering and in binary classification tasks. Valuable insights for IB are provided in this work. This work provides insights on phase transitions in the Information Bottleneck (IB) setting, specifically focusing on the interplay between the IB objective, its phase transitions, the dataset, and the learned representation. It addresses all IB phase transitions in the most general setting, beyond the limitations of jointly Gaussian variables. The paper discusses phase transitions in the Information Bottleneck (IB) setting, focusing on the IB objective, its interplay with the dataset, and learned representation. It introduces the concept of relative perturbation function for encoding distribution p(z|x). The perturbed probability (density) is p(z|x) = p(z|x) (1 + \u00b7 r(z|x)) for some > 0. Second variation: Let functional F [f (x)] be defined on a normed linear space R. Adding a perturbative function \u00b7 h(x) to f (x) expands the functional F [f (x) + \u00b7 h(x)] as a linear functional of \u00b7h(x), denoted as the first variation, and a quadratic functional of \u00b7 h(x), denoted as the second variation. The perturbation function \u00b7 h(x) can be seen as an infinite-dimensional \"vector\" with amplitude and direction. The \u03b4 2 IB \u03b2 [p(z|x)] represents the local \"curvature\" of the IB objective w.r.t. p(z|x), indicating a phase transition when the convexity changes from a minimum to a saddle point near the optimal solution p * \u03b2 (z|x) as \u03b2. The perturbation function expands the functional F [f (x) + \u00b7 h(x)] as a linear and quadratic functional of \u00b7 h(x). The \u03b4 2 IB \u03b2 [p(z|x)] indicates a phase transition from a minimum to a saddle point near the optimal solution p * \u03b2 (z|x) as \u03b2 increases. The encoder p \u03b8 (z|x) is parameterized by a parameter vector \u03b8, such as weights and biases in a neural net. The encoder p \u03b8 (z|x) is parameterized by a parameter vector \u03b8 = (\u03b8 1 , \u03b8 2 , ...\u03b8 k ) T \u2208 \u0398, e.g. weights and biases in a neural net. An infinitesimal change of \u03b8 induces a relative perturbation, from which the threshold function can be computed using the conditional Fisher information matrix. The infimum is attained at a specific value of \u2206\u03b8. This lemma shows that each term of G[p(z|x)] can be replaced by a bilinear form with the Fisher information matrix of the respective variables. Lemma 0.2 helps understand G[p(z|x)] for parameterized models, linking phase transitions to model parameters. Theorem 1 provides a condition for IB phase transitions, with phase transition points defined by roots of an equation. The proof for Theorem 1 is in Appendix D, and Section 4 will analyze it in detail. The focus is on understanding G[p(z|x)] and the phase transition condition from the perspectives of Jensen's inequality and representational maximum correlation. Theorem 1 gives a condition for IB phase transitions, involving an optimization problem. Jensen's inequality is used to explain the equality between different variables A, B, and C. The goal is to minimize intra-class difference while maximizing inter-class difference, leading to clustering of perturbation values based on class. It is conjectured that there are at most |Y| - 1 phase transitions in classification problems. Theorem 1 discusses conditions for IB phase transitions in classification problems, aiming to minimize intra-class difference and maximize inter-class difference. It is conjectured that there are at most |Y| - 1 phase transitions. Additionally, a deeper understanding of G[p(z|x)] is gained by introducing representational maximum correlation and conditional maximum correlation concepts. These concepts are defined and related to G[p(z|x)] in Theorem 2. Theorem 2 delves into the concept of maximum correlation in the orthogonal space of a learned representation, specifically focusing on the representational maximum correlation and conditional maximum correlation. It further explores the relationship between G[p(z|x)] and the phase transition condition outlined in Theorem 1. The text discusses the maximum correlation between X and Y in the orthogonal space of a learned representation. It highlights how the correlation decreases as the representation captures more information about X. A phase transition occurs when the correlation reduces, leading to a search for maximum correlation orthogonal to the learned representation. This concept is similar to canonical-correlation analysis (CCA). In Information Bottleneck (IB), each phase transition corresponds to learning a new nonlinear component of maximum correlation between X and Y in Z, orthogonal to the previously-learned Z. Classes that are less difficult, with larger maximum correlation between X and Y, are learned earlier in classification tasks. An optimal relative perturbation function r(z|x) can be decomposed into two factors, focusing on perturbing a specific point z * in the representation space. The focus is on perturbing a specific point in the representation space and finding the \"conspicuous subset\" in X space. The connection between G[p(z|x)] and the singular value of the Q X,Y |Z matrix is revealed. The phase transition condition is equivalent to a nonlinear eigenvalue problem. An algorithm is derived to estimate phase transitions efficiently for a given model architecture and dataset. The algorithm presented in Algorithm 1 efficiently estimates phase transitions for a given model architecture and dataset by training a neural network to estimate p(y|x) and using an iterative approach to converge to a phase transition point. The algorithm efficiently estimates phase transitions by using the IB algorithm to reach optimal p * \u03b2 (z|x) at \u03b2, then estimating G[p * \u03b2 (z|x)] using SVD. The process iterates until convergence at the phase transition point. The theory and Algorithm 1 are tested on categorical datasets and MNIST/CIFAR10 datasets with label noise to predict phase transitions accurately. The Blahut-Arimoto IB algorithm is used to compute optimal p * \u03b2 (z|x) for categorical datasets, with phase transitions observed at \u03b2. Algorithm 1 converges quickly to these points, supporting the conjecture of phase transitions in classification problems. Experiments on MNIST with noisy labels further validate the algorithm's performance. The algorithm is tested on a 4-class MNIST dataset with noisy labels, showing the path it takes to find phase transition points quickly and precisely. Per-class accuracy is also plotted, demonstrating the algorithm's ability to converge within a few iterations. The algorithm quickly converges to phase transition points, discovering 3 in total. Each transition corresponds to learning a new class, with the last class being harder due to larger label noise. Per-class accuracy is plotted, showing alignment with observed learning patterns. The confusion matrix for label noise shows the ordering of classes by difficulty, aligning with the representational maximum correlation prediction. The algorithm identifies phase transitions accurately but may miss later transitions due to the gap between variational IB and true IB objectives. The CIFAR10 experiment is also investigated. In the CIFAR10 experiment, the algorithm accurately identifies phase transitions in IB as \u03b2 varies, predicting 9 transitions in total. The experimental setup details are provided in Appendix I, stretching the limits of the discrete approximation to the continuous representation being learned. The algorithm can pinpoint dense transitions not easily visible in the I(Y ; Z) vs. \u03b2 curve alone. This work introduces a formula for IB phase transitions and explores the relationship between the IB objective, dataset, and learned representation. An algorithm is presented to find phase transitions, which accurately predicts transitions in categorical datasets and experiments with MNIST and CIFAR10. This study is a first step towards understanding phase transitions in the Information Bottleneck. Our approach in the Information Bottleneck is applicable to other \"trade-off\" objectives like \u03b2-VAE and InfoDropout. Lemma 2.1 is proven, which is crucial for the lemmas and theorems in this paper. The IB objective can be expanded by performing a relative perturbation on the conditional probability distributions. The 0th-order term is IB \u03b2 [p(z|x)], with higher-order terms for n \u2265 2. The KL-divergence between p(z|x) and p(z|x)(1 + \u00b7 r(z|x)) is discussed. Using Jensen's inequality, conditions for equality are derived. The lemma is proven, crucial for the paper's theorems. The IB objective can be extended by perturbing conditional probability distributions. To estimate G[p(z|x)] empirically, Monte Carlo importance sampling is used with samples {x j } \u223c p(x) and {z i } \u223c p(z) = p(x)p(z|x). The empirical \u011c[p(z|x)] is calculated using different distributions for importance sampling. Parameterized distribution p \u03b8 (z|x) changes to p \u03b8+\u2206\u03b8 (z|x) after a perturbation \u2206\u03b8 on \u03b8. The parameterized distribution p \u03b8 (z|x) changes to p \u03b8+\u2206\u03b8 (z|x) after a perturbation \u2206\u03b8 on \u03b8, where \u03b8 = (\u03b81, \u03b82, ...\u03b8 k ) T. The field \u0398 is closed under subtraction, so \u2206\u03b8 \u2208 \u0398. The first-order variation terms vanish, and the remaining terms are all similar. When r(z|x) is globally shifted by r(z|x) \u2190 r(z|x) + s(z), the numerator of G[r(z|x); p(z|x)] is then calculated. After perturbing the parameterized distribution p \u03b8 (z|x) by \u2206\u03b8, the formula for G[p(z|x)] simplifies. By using Cauchy-Schwarz inequality, we can further analyze the behavior of p(z|x) on phase transitions. The distribution p(z|x) is studied in relation to phase transitions. In this section, the behavior of p(z|x) on phase transitions is studied using a categorical dataset. The first phase transition separates x = 2 from x \u2208 {0, 1}, while the second phase transition separates x = 0 from x = 1. Each phase transition allows for distinguishing subsets of examples and learning new classes. The MNIST training examples with class 0, 1, 2, 3 are used with a hidden label-noise matrix. The conditional entropy bottleneck (CEB) is used as the variational IB objective. The study uses the conditional entropy bottleneck (CEB) as the variational IB objective. Training involves starting at \u03b2 = 100 for 100 epochs, annealing down to the target \u03b2 over 600 epochs, and continuing training for another 800 epochs. Neural nets with specific architectures are used for the encoder, classifier, and backward encoder. Adam is used as the optimizer with a specific learning rate schedule. Noisy labels are generated using the CIFAR10 class confusion matrix. The study utilized the conditional entropy bottleneck (CEB) for variational IB objective. Training involved starting at \u03b2 = 100 for 100 epochs, annealing down to the target \u03b2 over 600 epochs, and continuing training for another 800 epochs. Specific neural net architectures were used for the encoder, classifier, and backward encoder, with Adam as the optimizer and a specific learning rate schedule. Noisy labels were generated using the CIFAR10 class confusion matrix. The models trained for 600 epochs with Adam optimizer, starting at a base learning rate of 10^-3. Learning rate was reduced at 300, 400, and 500 epochs. Convergence to final accuracy occurred within 40,000 gradient steps. Figure 5 shows p(y|x) for the categorical dataset. Accuracies in Figure 4 are averaged across five passes over the training set. |Z| = 50 in Alg. 1."
}