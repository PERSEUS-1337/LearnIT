{
    "title": "BJxD11HFDS",
    "content": "The paper introduces the Factorized Multimodal Transformer (FMT) model for multimodal sequential learning, addressing the challenge of modeling spatio-temporal dynamics across different modalities. FMT factorizes intramodal and intermodal dynamics to improve modeling of multimodal phenomena without overfitting, even in low-resource settings. The Factorized Multimodal Transformer (FMT) model addresses difficulties in training, such as overfitting, even in low-resource setups. FMT has attention mechanisms with a full time-domain receptive field to capture long-range multimodal dynamics. The model outperforms previous models on datasets with language, vision, and acoustic modalities, setting new state-of-the-art results. Multimodal scenarios, like face-to-face communication, use language, vision, and acoustic modalities together for communicative intent. Learning from multimodal sequential data is a challenging research area in machine learning. Various approaches, including graphical models, RNNs, and Transformer models, have been proposed for this task. Transformer models, with their self-attention mechanism, have shown superior performance in highlighting related information across different modalities. The Factorized Multimodal Transformer (FMT) is a new transformer model designed for multimodal sequential learning. It efficiently models relations between different modalities within one network by increasing the number of attention units. This self-attention mechanism is capable of highlighting related information across sequences, making it a strong neural component for finding relations in multimodal data. The Factorized Multimodal Transformer (FMT) efficiently models interactions between modalities in a single transformer network using Factorized Multimodal Self-attention (FMS) units. It is evaluated on challenging multimodal language data and compared to existing approaches for multimodal sequential learning in sentiment analysis, emotion recognition, and personality traits recognition. Previous work in this area focuses on modeling multimodal sequential data within machine learning. Previous work in the field of machine learning has focused on modeling multimodal sequential data. Two main categories of models include early fusion, which concatenates features at the input level, and late fusion, which learns ensembles of weak classifiers from different modalities. However, early fusion models often lack the necessary components to effectively deal with multimodal data, leading to suboptimal performance. Hybrid methods combining early and late fusion have been utilized in modeling multimodal data. Specific models designed for multimodal data include Multi-view HCRFs, Multi-view LSTMs, Memory Fusion Network, and Multi-attention Recurrent Networks. Generic fusion techniques like Tensor Fusion have also been proposed for various models. The Transformer model, known for its self-attention mechanism, has shown superior performance in NLP tasks compared to RNN-based or convolutional architectures. It efficiently extracts short and long-range dependencies in input sequences. Transformer models have been successfully applied in various machine learning areas, including NLP and computer vision. The Factorized Multimodal Transformer 1 (FMT) extends transformer to multimodal domains for structured sequences. It includes Multimodal Transformer Layers (MTL) with Factorized Multimodal Self-attentions (FMS) for intramodal and intermodal factors. S1 and S2 are summarization networks enhancing attentions efficiently in FMT. The Factorized Multimodal Transformer (FMT) extends transformer to multimodal domains with Multimodal Transformer Layers (MTL) and Factorized Multimodal Self-attentions (FMS). Resampling using a reference clock aligns modalities {L, V, A} based on word timestamps. The dataset consists of inputs x and labels y, with N samples and T timestamps. Zero paddings unify sequence lengths, and A denotes input dimensionality at each timestep. The Factorized Multimodal Transformer (FMT) model involves passing each modality through an embedding layer with positional embeddings added. The output dimensions are denoted as ex = eL + eV + eA. The model then utilizes Multimodal Transformer Layers (MTL) to capture factorized dynamics and align time-asynchronous information across modalities using Factorized Multimodal Self-attentions (FMS). Specialized self-attentions are used within FMS to control the high dimensionality of intermediate attention outputs. The Factorized Multimodal Transformer (FMT) model utilizes Multimodal Transformer Layers (MTL) with specialized self-attentions in Factorized Multimodal Self-attentions (FMS) to control attention outputs. Multiple FMS have the same time-domain receptive field, unlike transformer models that split sequences based on attention heads. Each FMS unit in the model contains 7 distinct attentions for 3 modalities. The Factorized Multimodal Transformer (FMT) model utilizes Multimodal Transformer Layers (MTL) with specialized self-attentions in Factorized Multimodal Self-attentions (FMS) to control attention outputs. FMS units contain 7 distinct attentions for different modalities, each with unique receptive fields. The attentions extend to the length of the sequence, allowing for asynchronous relations within and across modalities. Each attention is controlled by Key, Query, and Value operations, followed by residual addition and normalization. The Factorized Multimodal Transformer (FMT) model utilizes Multimodal Transformer Layers (MTL) with specialized self-attentions in Factorized Multimodal Self-attentions (FMS) to control attention outputs. FMS units contain 7 distinct attentions for different modalities, each with unique receptive fields. The output of FMS is high-dimensional, and the goal is to reduce it using a 1D convolutional network. The network summarizes information across all modality dimensions and timesteps, resulting in an output of dimensionality R T \u00d7ex. The Factorized Multimodal Transformer (FMT) model utilizes Multimodal Transformer Layers (MTL) with specialized self-attentions in Factorized Multimodal Self-attentions (FMS) to control attention outputs. Multiple FMS units can be used inside a MTL to efficiently extract diverse multimodal interactions existing in the input data. Each FMS output goes through a feedforward network for each timestamp. Various possible interactions within a multimodal input, such as unimodal, bimodal, or trimodal interactions, need to be highlighted and extracted. Multiple FMS units are necessary to capture all these interactions without diluting attention. FMT achieves superior performance over baseline models for multimodal sentiment analysis on the CMU-MOSI dataset. The model reports binary accuracy (BA), F1 score, mean-absolute error (MAE), and Pearson correlation coefficient (Corr). The feedforward network in FMT adds residual connections and normalizes the output. Multiple Factorized Multimodal Self-attentions (FMS) units are used to capture diverse interactions in the input data efficiently. The experimental methodology includes tasks, datasets, computational descriptors, and comparison baselines for multimodal language analysis. The secondary summarization network S2 is used in conjunction with a Gated Recurrent Unit (GRU) for prediction. Various inherently multimodal tasks and datasets are studied in this paper, focusing on complex sequential multimodal signals. The experiments focus on multimodal sentiment analysis and emotion recognition using CMU-MOSI and IEMOCAP datasets, respectively. The datasets present challenges due to speaker diversity, topic variations, and low-resource setup. The study focuses on emotion recognition using the IEMOCAP dataset, consisting of 302 videos with speakers displaying Happy, Sad, Angry, and Neutral emotions. FMT outperforms baseline models in POM dataset for multimodal personality traits recognition. The POM dataset contains 1,000 movie review videos annotated for various personality and speaker traits, including Confident, Passionate, Dominant, Credible, and more. The abbreviations for these traits are used throughout the paper. The FMT and baseline models use computational descriptors for language, visual, and acoustic modalities. P2FA alignment model aligns text and audio at word level, GloVe embeddings are used for word representation. Emotient FACET extracts visual features, COVAREP extracts acoustic features like fundamental frequency and glottal source parameters. The FMT model uses various parameters for glottal source, voiced/unvoiced segmenting, formants, spectral parameters, and Mel Cepstral Coefficients. It is compared to the MV-LSTM baseline model, with key distinctions being the number of transformers used and the direction of interactions. MulT is a model that combines two attentions in one, without trimodal factors or direct unimodal paths. FMT attentions have full time-domain receptive fields, while MulT splits input based on heads. Performance measures for models include binary accuracy, multiclass accuracy, F1 score, Mean-Absolute Error, and Pearson Correlation Coefficient. Hyperparameter space search for FMT and baselines is discussed. The results show that FMT outperforms other models in sentiment analysis on the CMU-MOSI dataset, emotion recognition on IEMOCAP, and personality traits recognition on the POM dataset. The performance of FMT is superior across all tasks, except for Happy emotion recognition. FMT outperforms baselines in personality traits recognition. Ablation experiments show all factors, modalities, and components are necessary for best performance. The model with 6 FMT units achieves the highest performance. In this paper, the Factorized Multimodal Transformer (FMT) model is introduced for multimodal sequential learning. FMT utilizes a Factorized Multimodal Self-attention (FMS) within each Multimodal Transformer Layer (MTL) to capture intra-model and inter-modal dynamics in asynchronous multimodal sequences. Performance comparisons with baseline approaches are conducted on three publicly available datasets for multimodal sentiment analysis and emotion recognition. The Factorized Multimodal Transformer (FMT) model achieved superior performance in multimodal sentiment analysis, emotion recognition, and personality traits recognition compared to previous models. The hyperparameters of FMT include learning rate, structure of summarization network, number of MTL layers, number of FMT units, and dropout. Models are trained for a maximum of 200 epochs. The study explores the impact of the number of MTL layers on FMT performance and the effect of increasing the number of attention heads on the original transformer model. Results show that superior performance is not solely achieved by increasing attention heads. The study shows that increasing the number of attention heads does not necessarily lead to superior performance. FMT with 1 FMS unit achieves higher performance than OTF. Handling up to 3 modalities (language, vision, acoustic) with 7 attentions in each FMS is manageable for successful training. However, as the number of modalities increases, modeling becomes more challenging. The complexity can be managed in FMT due to factorization in FMS. Two approaches are proposed for handling a high number of modalities. Two approaches are proposed for handling a high number of modalities: reducing factors based on domain knowledge and assumed dependencies, or using a greedy approach similar to stepwise regression to iteratively add important factors. These methods allow the model to manage a higher number of modalities with a balance between performance and overparameterization."
}