{
    "title": "SJeBwn2usm",
    "content": "In this paper, the authors address the challenge of deploying deep neural networks in low memory environments by using techniques like filter pruning and low-rank decomposition for model compression and reducing compute resources during inferencing. Our approach achieves up to 57% higher model compression compared to Tucker Decomposition or Filter pruning alone for GoogleNet, reducing Flops by up to 48% for faster inferencing. Deep neural networks are widely used in various AI applications, with a focus on CNNs for object recognition and image classification. Transfer learning and filter pruning are key techniques for reducing compute and memory requirements in trained CNNs. In this paper, the focus is on transfer learning and the effectiveness of filter pruning in achieving model compression. By combining these techniques, up to 57% higher compression and 48% reduction in Flops can be achieved for models like GoogleNet. The methodology of combining filter pruning with tensor decomposition is described in Section 2, experimental results are presented in Section 3, and conclusions are in Section 4. The approach involves varying the ranks of the output core tensor and factor matrices. Filter pruning is a standard method for compressing deep neural networks like GoogleNet. The approach involves filter pruning followed by low-rank decomposition using Tucker."
}