{
    "title": "rJlBQkrFvr",
    "content": "In reinforcement learning, intrinsic rewards help agents learn to interact with their environment in a task-generic way. A method for computing intrinsic curiosity rewards using a latent variable model is introduced, improving performance on games with sparse extrinsic rewards. The method remains effective even in stochastic environments. Using curiosity as an exploration policy in reinforcement learning has many benefits, such as helping agents discover how to gain extrinsic rewards in sparse reward scenarios. Exploration also leads to building more robust policies and a general understanding of the environment, allowing agents to adapt better to changes. However, novelty-driven exploration methods can be distracted by randomness. In this paper, a new method for agent curiosity is presented to address distractions caused by randomness in exploration. The method utilizes a conditional variational autoencoder to model the environment and ensure robust performance in sparse reward environments and under stochasticity. The study introduces a perception-driven approach to curiosity using a conditional variational autoencoder (CVAE) to model dynamics and compute intrinsic rewards. It incorporates additional techniques to regularize for stochastic dynamics and uses a Bayesian metric for surprise, improving the ability to capture complex visual dynamics. The study introduces a Bayesian metric for surprise to capture complex visual dynamics and down-weight stochastic elements in learning dynamics models. Autonomous agents use this metric as an intrinsic reward in playing Atari games, outperforming alternate reward schemes. The method remains effective even with stochastic action realization in the environment. In this work, the authors combine a CVAE with deep reinforcement learning approaches to build a perception model that computes visual novelty through conditional log-likelihood estimation of observed states. They review perception-driven methods to encourage curiosity and demonstrate successful performance beyond baseline methods. The authors combine a CVAE with deep reinforcement learning to compute visual novelty through conditional log-likelihood estimation. This approach is shown to explore efficiently in various environments, utilizing Bayesian surprise as intrinsic reward. The surprise is computed via importance sampling from a latent variable model, allowing for complex multimodal distributions over images. Several methods maximize information-theoretic measures of agent behavior to learn skills without supervision. Co-Reyes et al. (2018) maximize entropy of agent trajectories, Pong et al. (2019) maximize entropy of possible goals, and Houthooft et al. (2016) use KL divergence between dynamics models as intrinsic reward. An alternative approach measures surprise in terms of next state entropy, providing a new perspective on exploring complex distributions over images. In an alternative approach, a stochastic model for sequential data based on variational inference techniques is used for high-dimensional data like videos. This model is leveraged to efficiently estimate surprise in videos by constructing a perception model using a conditional variational autoencoder (CVAE). The generative model is conditioned on the last embedded state to generate an estimation of the next embedded state. The generative model in the architecture conditions on the last embedded state and action to provide a visual model of the environment. State embeddings are computed by a neural network submodule from image frames. The approach is derived from conditional variational autoencoders, with generative and prior distributions defined for the latent space. The CVAE model learns a latent representation with a prior distribution of p(z) \u223c N (0, I). The empirical lower bound of the conditional log-likelihood and objective function is defined using defined distributions. The perception model includes a neural network predicting a t from \u03c6 t and \u03c6 t+1 to account for dynamics in the environment. The perception model by Pathak et al. (2017) regularizes for environment dynamics and stochasticity in agent actions. The error in action prediction is used as the loss for the action-prediction network. Hyperparameters are tuned to weight the contribution of each loss, and Bayesian surprise is defined as the curiosity an agent should have about an observation. Bayesian surprise is defined as the curiosity an agent should have about an observation based on the conditional likelihood given the agent's world model. The negative of this conditional probability estimate is used as a reward for agents, with the form r t = \u2212 log p(\u03c6 t+1 |\u03c6 t , a t ). This approach allows for capturing multimodal distributions over images using importance sampling from the latent space of a CVAE architecture. The text discusses using importance sampling from the latent space of a CVAE to estimate conditional likelihoods for formulating surprise. The reconstruction loss of the model is used to compute conditional probability, which is equated to Bayesian surprise. This surprise is then used as intrinsic reward input for a reinforcement learning algorithm, specifically in the context of exploring Atari video games as simulation environments. Atari games are commonly used for curiosity approaches in reinforcement learning due to their complex visual environments. In this study, intrinsic rewards are combined with extrinsic rewards in training agents to play different Atari games. The proximal policy optimization (PPO) algorithm is utilized for its performance with minimal hyperparameter tuning. The effectiveness of this reward combination is compared to an alternate prediction-based exploration bonus. In this study, agents are trained to play Atari games using a combination of intrinsic and extrinsic rewards. The effectiveness of this approach is compared to agent behavior trained solely on extrinsic rewards. The impact of curiosity-reward on learning is tested on games with varying levels of extrinsic reward sparseness. In this study, agents are trained to play Atari games using a combination of intrinsic and extrinsic rewards. The effectiveness of this approach is compared to agent behavior trained solely on extrinsic rewards. Montezuma's Revenge, Pitfall, and Private Eye are challenging games for deep reinforcement learning algorithms due to sparse extrinsic rewards. Results show comparable performance to ICM when extrinsic rewards are readily available, but using curiosity can substantially improve performance in these cases. Using curiosity to improve performance in games with sparse extrinsic rewards is less significant compared to when rewards are abundant. PPO achieved the highest extrinsic reward in Pitfall but failed to find a successful game strategy. Our method outperforms the combination of ICM and extrinsic rewards in Montezuma's Revenge and Private Eye, with low variance at convergence in Private Eye. Comparing results before any working strategy is learned is premature, as exploration may require incurring temporary negative rewards. Our method demonstrates improved ability to learn complex scene dynamics in the absence of extrinsic rewards, balancing novelty and exploration effectively. We conduct a sticky action experiment to test the model's robustness to stochasticity, showing promising results. Our model shows comparable performance to ICM in the presence of sticky actions, with slightly better results. Results from training 3 seeds for each method over 10 million timesteps are summarized in Table 2, with the best performance bolded. We introduced a novel method using a CVAE to compute curiosity and demonstrated its effectiveness in learning scene dynamics without extrinsic rewards. Our approach using a CVAE allows agents to learn tasks effectively in environments with sparse rewards while maintaining robustness to stochasticity. The method we use as our baseline, ICM, struggles with stochasticity in scenes due to significant changes between sequential frames. This requires a different approach to handle. Comparing models for curiosity and exploration in deep reinforcement learning involves considering both the dynamics model and intrinsic reward metric. In this paper, models and intrinsic reward metrics are compared for curiosity in deep reinforcement learning. Different metrics such as KL divergence between latent distributions are proposed. Future work could explore the impact of intrinsic reward metrics on robustness to stochasticity. The perception model generates image embeddings that construct realistic frames from video games, showing improved environment perception with our model. Increasing ability to perceive the environment is linked to decreasing intrinsic rewards. An experiment was set up using a visual decoder to reconstruct images from learned embeddings. The decoder was trained on game images and image embeddings, then used to reconstruct images from predicted embeddings. The experiment was conducted on Kung Fu Master, a visually complex Atari game. The reconstructed images from the perception model were compared to grayscale, rescaled images from the game simulations. The experiment used a visual decoder to reconstruct images from learned embeddings in the Atari game Kung Fu Master. High intrinsic rewards were linked to poor reconstructions, while low rewards were associated with good reconstructions. The scenes shown were from early in training when the agent only partially perceived the environment. As the perception model improved with additional training, the relationship between intrinsic reward and image reconstruction error weakened. The correlation between intrinsic reward and image reconstruction error weakens as the model recognizes distinct transition dynamics. The success of the CVAE model in perceiving the game environment in Kung Fu Master is validated. The experiments leveraged the PPO algorithm for implementation. We implemented the PPO algorithm in the Atari simulation environment using infrastructure code from Pathak et al. (2017). Training for 10 million time steps took about 7 hours on a 2080 Ti NVIDIA GPU, processing 425 frames per second. Hyperparameters for loss weights were tuned using intrinsic reward only, exploring different weight values through a grid search. After tuning loss weights using intrinsic reward only, we optimized the combination of intrinsic and extrinsic reward weights. We experimented with different scales of latent space dimension and listed the hyperparameters used in Table 3. The intrinsic and extrinsic reward weights were the same for ICM and our method, but we tuned them separately for each model. All hyperparameters required in ICM were taken from Pathak et al. (2017) except for reward weighting. The perception model used in the study reconstructed game scene images and calculated intrinsic reward and image reconstruction error for each pair."
}