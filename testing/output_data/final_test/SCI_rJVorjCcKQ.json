{
    "title": "rJVorjCcKQ",
    "content": "The paper introduces Slalom, a framework for high-performance execution of Deep Neural Networks (DNNs) in Trusted Execution Environments (TEEs) by efficiently partitioning computations between trusted and untrusted devices. Slalom securely delegates execution of linear layers in a DNN from a TEE to a faster, untrusted processor, maintaining integrity and privacy for outsourced ML computations. Slalom evaluates DNNs in Intel SGX enclave, delegating work to untrusted GPU for increased throughput in verifiable and private inference. TEEs like Intel SGX offer integrity and privacy guarantees for ML computations in untrusted environments, crucial for cloud-based ML APIs and ML-as-a-Service platforms. Intel SGX, ARM TrustZone, and Sanctum provide a solution by using TEEs to isolate sensitive code for outsourced ML computations, ensuring strong privacy and integrity guarantees. TEEs outperform pure cryptographic approaches but come with a performance cost compared to untrusted alternatives like GPUs or server CPUs. Intel SGX, for example, incurs overhead for memory-intensive tasks and has limitations in exploiting multi-threading. The Slalom framework proposes efficient DNN inference in trusted execution environments like SGX or Sanctum. It allows outsourcing linear layers to untrusted GPUs without compromising integrity or privacy. Security of Slalom is formally proven, and it increases throughput by 6\u00d7 to 20\u00d7 compared to running all computations in SGX. The GPU significantly boosts throughput and energy efficiency for verifiable inference, as well as private inference. Challenges remain for efficient verifiable training of DNNs in TEEs, with a focus on security properties in an outsourcing scheme between a client and server executing a DNN on provided data. Privacy is maintained in the outsourcing of machine learning models through Trusted Execution Environments (TEE) like Intel SGX, ARM TrustZone, or Sanctum. Enclaves in TEEs isolate computations, providing security guarantees and performance benefits compared to baseline schemes. Remote attestations can be produced for verification using the manufacturer's public key. Hardware enclaves from Intel SGX are utilized in experiments with Slalom, offering an efficient solution for ML outsourcing. TEE's like Intel SGX provide a secure solution for outsourcing machine learning models. SGX's security outperforms cryptographic protocols significantly, but there are limitations such as a 128 MB PRM restriction and performance issues with multi-threaded DNN evaluations. Current SGX enclaves cannot compete in terms of performance or energy efficiency for DNN computations. Slalom outperforms baseline TEE scheme for verifiable inference and private outsourced computations, especially when server has access to model F. It excels in speeding up DNN inference by outsourcing work from the TEE, making it viable for irregular intervals of user data processing with high throughput. The idea is to speed up DNN inference in TEEs by outsourcing work to a faster untrusted processor, aiming to make the combined cost cheaper than evaluating the full DNN in the TEE. Existing protocols have overheads that make outsourcing inefficient, so new designs are needed to overcome this barrier. To enhance DNN inference in TEEs, outsourcing protocols are designed to leverage the TEE's knowledge of the model and selectively outsource computations. This approach allows for interactive outsourcing with high communication but better efficiency, focusing on non-linearities that are a small fraction of DNN computation. Linear operators, the main bottleneck in DNNs, can be evaluated in the TEE for improved performance. Linear operators, the main computational bottleneck in DNNs, can be efficiently delegated using a secure scheme. Integrity is verified using a randomized check with a significant reduction in multiplications. Input privacy for outsourced linear operators can be achieved with linearly homomorphic encryption, but the overhead is too high compared to computing the function directly in a TEE. The proposed approach involves using a two-stage method based on symmetric cryptography to efficiently compute linear functions in a Trusted Execution Environment (TEE). In the offline phase, the TEE generates pseudorandom elements and pre-computes a value, then in the online phase, it encrypts the input and outsources the computation to a faster processor. This method reduces communication and interaction between the TEE and the co-processor. Slalom is a three-step approach for outsourcing DNNs from a TEE to a faster device. It involves quantizing inputs and weights, outsourcing linear layers using Freivalds' algorithm, and encrypting inputs for privacy. The focus is on feed-forward networks with various layers and activations, with the potential for extension to other architectures. The Slalom approach involves outsourcing DNN computations to a faster device using quantization, Freivalds' algorithm, and encryption for privacy. It focuses on feed-forward networks with different layers and activations, with the possibility of extending to other architectures like residual networks. The DNN computations are converted to integers and embedded in the field Z p of integers modulo a prime p to avoid wrap-around. Floating point numbers are converted to fixed-point representation for efficiency using fake quantization. Double-precision floats are used for arithmetic modulo p to reduce precision loss. Slalom's approach to verifying the integrity of outsourced linear layers involves using double-precision floats to reduce modular reductions. Freivalds' Algorithm can be applied to linear layers by batching, where the output is checked against a random vector. As batch size increases, the verification cost is amortized. Batched verification of linear layers involves using Freivalds' check for full convolutions, even for single-element batches. The complexity for evaluating and verifying linear functions is detailed in Table 2, with different layers such as \"Fully Connected\" and \"Convolution\". The verification cost is amortized as batch size grows, but it is prohibitive in SGX enclaves due to limited memory. The text discusses obtaining an outsourcing scheme for linear layers with optimal verification complexity and weight compression in a memory-limited TEE model. It leverages precomputed DNN weights and secret randomness reuse to reduce verification complexity. The verification scheme with preprocessing is based on a reformulation of a lemma, requiring a certain number of multiplications for the check. The text discusses using precomputed blinding factors for privacy in outsourced computations in a TEE model. The TEE uses a PRNG to generate blinding factors and stores encrypted unblinding factors. During online phase, the TEE regenerates blinding factors and decrypts outputs using precomputed unblinding factors, incurring overheads. The Slalom protocol incurs overheads due to computations on untrusted devices using double-precision arithmetic, data exchange between processors after each layer, and loading precomputed unblinding factors efficiently. The security of Slalom is guaranteed by formal definitions and proofs in Appendix B, with the protocol being secure for outsourcing computations. Slalom is a secure outsourcing scheme for computations between a TEE and an untrusted co-processor S, ensuring privacy and t-integrity. It is evaluated on Intel SGX hardware for performance improvements in tasks like ImageNet inference. The scheme allows for increased performance by outsourcing linear layers while maintaining security. Designing a lightweight C++ library for feed-forward networks based on Eigen, implementing the forward pass of DNNs with various functionalities. Performance comparable to TensorFlow on CPU. Code available at https://github.com/ftramer/slalom. Slalom performs arithmetic over Z p, ensuring integrity with Freivalds' check. Chance of fooling the TEE on incorrect DNN evaluation is less than 1 in 22 billion for a 50-layer DNN. Slalom uses AES-CTR and AES-GCM for privacy, running on an Intel Core i7-6700 Skylake processor with 8GB of RAM. Computation is outsourced to a Nvidia TITAN XP GPU. Challenges for efficient parallelization are discussed. Evaluation includes synthetic benchmarks and ImageNet classification with various models. The evaluation focuses on throughput and energy efficiency, comparing Slalom with MobileNet and other models. Micro-benchmarks include matrix products, convolutions by VGG16, and separable convolutions by MobileNet. The data is pre-loaded for all operations. The evaluation compares Slalom with MobileNet and other models in terms of throughput and energy efficiency. Various verification strategies show significant speedups over direct computation, especially for square matrices up to 2048 dimensions. Outsourcing results for convolutions can lead to large savings, particularly with preprocessing and an increase in the number of channels. The evaluation compares Slalom with MobileNet and other models in terms of throughput and energy efficiency. Savings increase with the number of channels, especially for large c out. Batched verification is effective for operators with small memory footprints. \"Truly\" separable convolutions are faster to verify. VGG16 has a larger memory footprint than SGX's PRM, resulting in a significant overhead during forward passes. By securely storing preprocessed products in the enclave, memory footprint is reduced, leading to a 20.3\u00d7 increase in throughput. VGG16 network fits in the PRM, achieving higher throughput for in-enclave forward passes. MobileNet shows 3.6\u00d7 to 6.4\u00d7 speedups with Slalom for verifiable inference. Private DNN inference with outsourced linear layers involves blinding and unblinding inputs and outputs, which is costly in SGX due to extra memory reads and writes. Speedups of 13.0\u00d7 and 5.0\u00d7 for VGG16 and fused MobileNet without intermediate activations are achieved for private outsourcing. Speedups of 10.7\u00d7 and 4.1\u00d7 are seen when ensuring integrity. Slalom's improvements are even higher in non-resource-constrained or multi-threaded environments. Extending Slalom to Deep Residual Networks is straightforward, involving verifying linear layers and computing activations. The privacy-preserving Slalom variant in FIG4 shows promising performance numbers. The ResNet implementation from Keras BID6 shows minimal decrease in accuracy with quantization. Benchmarks for different ResNet variants demonstrate significant speedups for verifiable inference and privacy. The 18 and 34-layer ResNets use 3x3 convolutions, while larger models utilize pointwise convolutions. Slalom's speedup over the baseline increases as layers are added to ResNet architecture due to duplicating layers with the largest number of channels. Techniques for secure outsourcing of DNN inference may also apply to DNN training. Training deep neural networks (DNNs) with Slalom poses challenges such as the need for a more flexible quantization scheme for weight changes during training. Preprocessed random vectors for Freivalds' check cannot be reused indefinitely, suggesting the use of very large batches for efficient verification. Additionally, techniques for protecting input privacy do not apply to training as weights change after every batch, and Slalom does not hide model weights from untrusted processors, which may be necessary for private training. The paper explores the efficiency of evaluating a DNN in a Trusted Execution Environment (TEE) for integrity and privacy. They introduce Slalom, a framework that securely outsources linear layers to a GPU while preserving privacy using encryption. Slalom boosts inference throughput without compromising security for canonical DNNs like VGG16, MobileNet, and ResNet variants. Securely outsourcing matrix products from a TEE has applications in ML beyond DNNs. The approach of outsourcing work from a TEE to a faster co-processor could be applied to other problems with fast verification algorithms. SGX enclaves isolate program execution from other processes on the same host, ensuring encrypted and authenticated memory. While SGX covers many attack vectors, it does not address side-channel attacks explicitly. The text discusses side-channel attacks on SGX enclaves, which have been a concern due to data-dependent code behavior. These attacks undermine privacy by observing enclave computations. While DNN computations are less vulnerable to these attacks, recent Spectre attacks have shown vulnerabilities in SGX. Mitigations are being developed, but a truly secure solution may require architectural changes like the proposed Sanctum processor. The text discusses secure outsourcing schemes for deep neural networks (DNNs) between a client and a server, without formally modeling SGX's security. It defines a scheme with offline preprocessing and an online protocol, focusing on outsourcing protocols where the TEE acts as a client. Interested readers can refer to formalisms in other papers. The online outsourcing protocol involves inputs (F, x, st) initiated by C, who outputs a value y \u2208 Y or aborts. Key properties of a secure outsourcing scheme include correctness, t-integrity, input privacy, and efficiency in online computation. The secure outsourcing scheme involves hiding the model F from either the server S or the client C. Model privacy guarantees that the server's views are indistinguishable when outsourcing the model. Commit-and-prove schemes are used to ensure that the outputs are produced with the same model. The outsourcing scheme is defined with random values. The outsourcing scheme ensures model privacy by using secure cryptographically secure pseudorandom number generators. The integrity of the scheme is maintained by replacing vectors with random values, leading to a bound on the output probability. The outsourcing protocol ensures model privacy by using secure pseudorandom number generators. By replacing pre-computed blinding vectors with random values, the scheme maintains integrity and guarantees output probability bounds. The protocol involves setting up a secure communication channel between the client and server, with the server hosting the Trusted Execution Environment (TEE) and initializing the Slalom model. The TEE receives model F from S and runs the Slalom protocol to securely compute y = F(x). The TEE sends y and a commitment to F to C over a secure channel. Different outsourcing approaches are compared, with a baseline running a DNN in a TEE for security guarantees. High-end GPUs provide higher throughput but lack security. SafetyNets BID15 and Gazelle BID26 achieve integrity and privacy using cryptographic approaches. SafetyNets and Gazelle achieve integrity and privacy using cryptographic approaches without a TEE. SafetyNets does not hide the model from either party, while Gazelle leaks some architectural details to the client. SafetyNets evaluates a 4-layer TIMIT model at 13 images/sec on a notebook CPU, while Gazelle evaluates an 8-layer CIFAR10 model at 450 images/sec in an enclave. Gazelle evaluates a single image in 3.5 sec with 300MB of communication between client and server. Comparing approaches with different hardware, throughput alone is not the fairest metric. When comparing different approaches for privacy and integrity, it is important to consider energy efficiency as a key metric. For example, using a GPU for computations can be more energy efficient compared to using an enclave with SGX CPUs. The power consumption of the GPU is higher, but it achieves significantly higher throughput, making it more energy efficient overall. Additionally, when evaluating the cost of running both the enclave and GPU in a system like Slalom, the power consumption attributed to Slalom is a combination of both components, with the GPU accounting for a smaller percentage of the total power consumption. The power consumption attributed to Slalom is roughly 35.5W, combining CPU and GPU usage. Slalom achieves 4\u00d7-20\u00d7 higher throughput than the baseline and is 3.4\u00d7-17.1\u00d7 more energy efficient. Common linear operators in deep neural networks are described, including convolutional layers and fully-connected layers. A convolution involves combining two linear operators to transform input data into an intermediate form, with a separable convolution using two kernels to produce an output. The cost of a layer is determined by the number of multiplications required. Depthwise convolution involves independent convolutions with filters applied to a single input channel, while pointwise convolution is a matrix product with specific dimensions. In our evaluation, we analyze the performance of two DNNs on ImageNet with and without quantization. Quantization results in a slight drop in accuracy. We also consider model parameters, input/output sizes, and communication between trusted and untrusted co-processors in Slalom. The models used are pre-trained from Keras BID6. Accuracies are computed on the ImageNet validation set using pre-trained models from Keras BID6. Slalom performs modular arithmetic over a field Z p in the TEE, leveraging standard floating point operations for computational efficiency. The main computations involve inner products over Z p for Freivalds' check. A quantization scheme ensures DNN values can be represented in Z p for p 2 24. Inner products are computed by casting elements to doubles, as a single multiplication in Z p would exceed the range of integers representable as floats. Single or double precision floats are preferred on Intel architectures for efficient SIMD instructions. In the evaluation, a tradeoff exists between the number of repetitions of Freivalds' check and the size of the set S for random values. With k = 2 repetitions, setting S = [\u22122^19, 2^19] allows for bounded multiplications and accumulation of terms before modular reduction. Increasing k further is not beneficial due to the cost of more inner products outweighing the savings from reducing modulos. The outsourcing scheme's performance is evaluated without Intel SGX's quirks. In the evaluation, the performance of the outsourcing scheme is assessed without Intel SGX's quirks. Rerunning evaluations outside of SGX's enclave mode shows significant speedups in computation verification and better viability of batched verification. Benchmarks for VGG16 and MobileNet demonstrate savings in integrity with various secure outsourcing strategies. In experiments outside of SGX enclave mode, significant speedups in computation verification were observed with batched verification. Additional blinding for privacy preservation led to speedups of 3.9\u00d7 and 8.1\u00d7 for MobileNet and VGG16. Despite attempts at achieving parallelism in SGX, no speedup was achieved for VGG16. The experiments did not achieve parallel speedup for VGG16 due to memory limitations. For MobileNet, less than a 1.5\u00d7 speedup was achieved using up to 4 threads. The DNN library does not support intra-operation parallelism, but implementing a thread pool for SGX could be beneficial. Eigen's multi-threading support was used to speed up operations, and custom OpenMP code was used for parallelizing dot products. Figure 7 presents multi-threaded micro benchmarks on an untrusted CPU, showcasing results for matrix products and convolutions using 4 threads. Multi-threading increases the gap between direct and verified computations of matrix products, with efficient parallelization of dot products. Linear speedups are achieved for verifiable separable convolutions, but depthwise convolutions comparison is omitted due to lack of multi-threaded implementation. Memory-access overheads in SGX pose uncertainties. It is uncertain if similar speedups could be achieved using intra-op parallelism in an enclave due to memory-access overheads in SGX, but it is worth exploring this avenue."
}