{
    "title": "rJedNwij2r",
    "content": "Real-world Relation Extraction (RE) tasks are challenging due to limited training data or class imbalance. Data Augmented Relation Extraction (DARE) uses GPT2 to generate examples for specific relation types, improving F1 score by up to 11 points. DARE achieves new state-of-the-art in three biomedical RE datasets, surpassing previous results by 4.7 F1 points on average. Relation Extraction (RE) has gained importance with the rise of knowledge graphs and their applications. Recent works focus on supervised RE using predefined relation categories, integrating Transformer-based architectures for language modeling. These advancements aim to enhance the discriminative power of RE models. In this work, the authors explore using GPT-2 to generate new training samples for Relation Extraction tasks. They fine-tune GPT-2 for each relation type, generate data, combine it with the gold dataset, and fine-tune a BERT model for RE. Extensive experiments show that their approach, DARE, outperforms baselines and the state-of-the-art on biomedical RE benchmark datasets. The study demonstrates significant improvements in Relation Extraction using GPT-2 for training data augmentation. Various approaches in the literature address challenges such as class imbalance and limited data in RE tasks. Approaches to deal with imbalance in classifier level include penalizing misclassification errors differently for each class based on class frequency or adjusting prior class probabilities. Methods include undersampling the majority class, oversampling the minority class, and balanced bagging. Oversampling approaches for textual data are limited compared to those for image data. Approaches to address imbalance in classifier level include penalizing misclassification errors differently for each class based on class frequency or adjusting prior class probabilities. Methods involve undersampling the majority class, oversampling the minority class, and balanced bagging. Textual data oversampling methods are somewhat limited compared to image data due to text semantics relying on word token order or structure. Various techniques like synonym replacement, topic models, data augmentation, and LSTM-based approaches have been proposed to generate new samples for tasks like visual question answering and language lifelong learning. In the framework of Language Lifelong Learning (LLL), a method is proposed to prevent catastrophic forgetting by finetuning GPT-2 to generate training samples for previous tasks when training on a new task. This approach does not require domain expertise or training a model from scratch, making it easily adaptable to any domain with low resource requirements. The GPT-2 model is briefly introduced before detailing this oversampling technique for text. The GPT language model and BERT language model are deep neural network architectures pre-trained on vast amounts of textual data. GPT-2 comes in different sizes with varying parameters, and the second largest model is used in experiments for a balance between accuracy and hardware requirements. A RE classifier based on BERT is employed in this work, following a similar principle as Devlin et al. (2018) with a special token (CLS) for classification. The method introduced involves masking entity mentions with generic entity types and generating new training data by splitting the dataset into subsets based on relation types. GPT-2 is finetuned on each subset to generate new sentences, which are filtered and combined into a synthetic dataset. An ensemble of relation extraction classifiers is then trained on this synthetic dataset and the original dataset. In experiments, a ratio is set to generate instances in Dsynth equal to the number of gold instances for a relation multiplied by the ratio. Early attempts at finetuning over the whole D by adding special tokens for relation types did not improve results. The empirical evaluation of the method involved using the second largest GPT-2 model with 774M parameters. The experiments were conducted on a machine with a GPU V100-16GB, using HuggingFace's Transformers library for implementation. Finetuning of GPT-2 was done with Adam optimizer, sequence length of 128, batch size of 4 (equivalent to 8 with gradient accumulation), and a learning rate of 3e \u2212 5. Finetuning for 5 epochs was done on all datasets and relation types, with generation parameters set for optimal hyperparameter values in future. The study involved finetuning a GPT-2 model on 500k PubMed abstracts followed by dataset-specific finetuning for relation extraction. A pre-trained BERT model was used as the RE classifier, finetuned on gold or gold+generated datasets. The AdamW optimizer, sequence length of 128, batch size of 32, and learning rate of 2e \u2212 5 were utilized for 5 epochs. A softmax layer was used for predictions, with relation types assigned based on a threshold maximizing the micro-F score on the validation set. Further optimization of hyperparameters is suggested for future work. In experiments, an ensemble of twenty classifiers is trained for DARE, with each classifier trained on both gold set and a sub-sample of generated data to reduce noise. Evaluation is done on three RE datasets from the biomedical domain, including the BioCreative V CDR corpus for chemical-disease relations. The dataset is split into train, development, and test sets, focusing on intra-sentence relations. The dataset is available in the GitHub repository for replication purposes. The CDR dataset, also known as DDI2013, contains drug-drug interaction annotations at the sentence level from MedLine abstracts and DrugBank documents. The ChemProt dataset covers chemical-protein interactions with five relation types, provided with a train-development-test split. Both datasets suffer from class imbalance and limited number of positives. The datasets DDI2013 and ChemProt have class imbalance and limited positives. Two baselines, balanced bagging and class weighting, are considered for these scenarios. An ensemble of ten models is used for both baselines, with class weights set based on the rarest class. GPT-2 is finetuned on in-domain data from the CDR dataset to test its effectiveness before being finetuned per relation type. In this experiment, GPT-2 models were finetuned per relation type using in-domain data from the CDR dataset. The results showed significant improvement in generating new training examples. The study focused on dealing with imbalanced datasets by sampling different numbers of positive examples and combining them with negative instances. The experiment compared the performance of a balanced bagging ensemble and DARE using these datasets. In this experiment, GPT-2 models were finetuned to generate training data for imbalanced datasets. The results showed DARE's significant advantage over balanced bagging, especially with few positive samples. DARE can boost classifier performance by generating training data of arbitrary sizes. The next experiment focuses on studying the effect of different sizes of generated data on DARE's performance. The experiment focused on determining the optimal ratio of generated examples for training classifiers. A short experiment was conducted using the CDR dataset with different sizes of generated data. Results showed that adding more data does not always improve classifier performance due to noisy patterns in the generated data. Adding more generated data does not necessarily improve classifier performance, as noisy patterns in the generated data can have a greater influence than those in the gold data. DARE is compared against the state-of-the-art and two baselines, with results shown in Table 4. Micro-F score is reported for multi-class datasets to allow for comparison with previous works. Per class results for DARE are also provided in Tables 5 and 6 in Appendix A. DARE is compared against the state-of-the-art and baselines using BERT-large as the base classifier, showing a steady advantage across all datasets. The baselines perform roughly on par, with DARE outperforming them by 2 to 5 F1 points. DARE leverages GPT-2 generated data to improve upon state-of-the-art and baselines in Relation Extraction. The method involves finetuning GPT-2 per relation type to generate new training data, leading to significant improvements in F1 score points, particularly in dealing with class imbalance or limited data settings. Our work leverages GPT-2 generated data to enhance Relation Extraction performance, achieving up to 11 F1 score points improvement in imbalance or limited data settings. Additionally, we demonstrate state-of-the-art results on three biomedical RE benchmarks, with potential for further advancements in Natural Language Understanding tasks."
}