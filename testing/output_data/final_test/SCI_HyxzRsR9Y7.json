{
    "title": "HyxzRsR9Y7",
    "content": "The success of deep reinforcement learning algorithms like policy-gradients and Q-learning depends on having informative rewards at each timestep. Sparse rewards lead to sub-optimal performance due to credit assignment challenges. Trajectory-based methods like cross-entropy and evolution strategies don't need per-timestep rewards but struggle with high sample complexity. A new self-imitation learning algorithm is introduced to address the efficiency of RL algorithms in sparse and episodic reward settings. The algorithm proposed minimizes divergence in policy optimization by using Jensen-Shannon divergence, leading to a policy-gradient approach with shaped rewards from experience replays. It performs well in environments with sparse and episodic rewards, outperforming existing algorithms. Additionally, a solution to limitations in self-imitation learning is suggested using Stein variational policy gradient descent with the Jensen-Shannon kernel to learn diverse policies, showing effectiveness in challenging continuous-control tasks. Deep reinforcement learning (RL) using deep neural networks has shown superior performance in various problem domains like computer games, continuous control, and robotics. Policy gradient methods and Q-learning algorithms optimize neural networks to solve sequential decision problems effectively. Algorithms like BID28 capture the temporal structure of sequential decision problems by decomposing them into supervised learning tasks. However, sparse or delayed reward signals can lead to inferior performance and inefficient sample complexity, known as the temporal credit assignment problem BID44. This issue is exemplified in games like Atari Montezuma's revenge, where rewards are sparse, making neural network training inefficient and exploration challenging. Many real-world problems have sparse or episodic rewards, leading to challenges in learning efficient policies. Alternative algorithms like global optimization methods have been explored for policy search, focusing on cumulative rewards over entire trajectories. Examples include the cross-entropy method and evolution strategies. These methods aim to improve sample efficiency in learning policies. Our contribution is a new algorithm based on policy gradients, aiming to outperform existing RL algorithms in sparse and episodic reward settings. By minimizing the Jensen-Shannon divergence between the current policy's visitation and the distribution induced by experience replay trajectories, we enhance performance in challenging reward environments. The Jensen-Shannon divergence is minimized in a policy-gradient algorithm with shaped rewards learned from experience replays, enhancing credit assignment in sparse and episodic settings. The algorithm involves self-imitation learning without external demonstrations. Additionally, the Stein variational policy gradient is applied with the Jensen-Shannon kernel to learn multiple diverse policies simultaneously, improving the self-imitation framework. The benefits of the addition to the self-imitation framework are demonstrated by tackling difficult exploration tasks with sparse and deceptive rewards. Various policy learning algorithms utilize divergence minimization, such as Relative Entropy Policy Search (REPS) and Guided Policy Search (GPS). MATL uses adversarial training for efficient transfer learning. In Guided Policy Search (GPS, BID21), a parameterized policy is trained by constraining the divergence between the current policy and a controller learned via trajectory optimization. Learning from Demonstrations (LfD) aims to train a control policy to mimic a demonstrator's trajectory distribution. Human data has been used in various approaches for self-driving cars and drone manipulation, along with Deep Q-learning to improve performance in Atari and robotics tasks. Maximum entropy IRL framework has been utilized to learn cost functions for optimal demonstrations, and an imitation-learning algorithm (GAIL) has been developed to minimize the divergence between agent's rollouts and external expert data. Our algorithm, GAIL, differs from prior work by using the learner's past experiences as demonstration data instead of external supervision. Exploration in RL involves count-based methods using state-action visitation counts and bonus rewards for rarely visited states. Techniques like approximation and pseudo-count estimation in large state spaces aid in exploration. Intrinsic motivation, such as information gain or prediction error bonuses, can also enhance exploration. Hindsight Experience Replay adds extra goals and rewards to a Q-learning algorithm. In recent work, the algorithm proposes using past good trajectories for exploration by storing (s, a) pairs and returns, reusing them if the return is higher than the current state-value estimate. The approach focuses on self-imitation through divergence-minimization to learn dense rewards for policy optimization, enhanced with SVPG. This method differs from prior work by incorporating a modification of SVPG to train a diverse ensemble of agents with good exploratory skills. The text discusses self-imitating learning in reinforcement learning (RL) and extends the method to learn multiple diverse policies using Stein variational policy gradient with Jensen-Shannon kernel. It explains the RL setting with an unknown system dynamics model and stochastic policies over high-dimensional, continuous state and action spaces. The agent interacts with the environment using a policy \u03c0 mapping observations to actions. The text discusses reinforcement learning in high-dimensional, continuous state and action spaces. The RL objective is to maximize the expected sum of rewards. The state-visitation distribution for a policy is defined, and the policy return is calculated using the state-action visitation distribution. The policy gradient theorem is used to determine the ascent direction. The behavior of a policy is characterized by the state-action visitation distribution, which determines the expected return. Distance metrics on a policy should be defined with respect to the visitation distribution, and policy search aims to find policies with high reward. Policy optimization involves minimizing the divergence between the visitation distribution of the current policy and an expert policy to imitate it. In policy optimization, the goal is to find a policy that imitates an expert policy by minimizing the divergence between their state-action visitation distributions. This is done by maintaining a subset of highly-rewarded trajectories and optimizing the policy based on the empirical distribution of the visitation distribution. The Jensen-Shannon divergence is used to measure the difference between the two distributions. In policy optimization, the Jensen-Shannon divergence is used to measure the difference between state-action visitation distributions of two policies. An approximate gradient of D JS w.r.t the policy parameters can be obtained to optimize the policy. A simple approach to construct replay memory M E using high-return past experiences is introduced to represent a mixture of deterministic policies. In policy optimization, the Jensen-Shannon divergence measures the difference between state-action visitation distributions of policies. A priority queue list is used to store top-k trajectories based on total reward. Sampling noisy actions from Gaussian policies is sufficient for locomotion tasks. The policy optimization procedure is augmented to handle more challenging environments. In policy optimization, the algorithm utilizes self-imitation learning from experience replay to enhance exploration and create a diverse ensemble of policies. The gradient estimator of D JS is similar to policy gradients but replaces the true reward function with per-timestep reward. This allows for interpolation between D JS gradient and standard policy gradient, resulting in a net gradient on the policy parameters. The interpolated gradient in policy optimization involves using a mixture policy represented by parameterized networks for densities, trained through D JS optimization. This results in a reward function that guides the learner towards expert behavior in regions frequented more by the expert. In sparse or deceptive reward environments, weighting Q r \u03c6 heavily enables successful learning by imitation, even in dense reward environments the two gradient components can be combined for policy optimization. The algorithm for self-imitation is detailed in Appendix 5.2. However, a limitation of self-imitation is that the quality of trajectories in the replay memory relies on good exploration by the agent, as seen in a maze environment example. In sparse or deceptive reward environments, weighting Q r \u03c6 heavily enables successful learning by imitation. Self-imitation can lead to sub-optimal policies due to local minima in the policy optimization landscape. Stochasticity in the environment can make it difficult to recover the optimal policy just by imitating past top-k rollouts. To overcome these pitfalls, training an ensemble of self-imitating agents is proposed. To enhance exploration in challenging environments, an ensemble of self-imitating agents is trained to visit different regions of the state-space, avoiding deceptive reward traps and increasing the probability of finding the optimal policy. This approach is based on Stein variational policy gradient (SVPG) to learn diverse policies and enforce exploration in high dimensional spaces. Stein variational gradient descent (SVGD) enforces diversity on the parameter space by approximating q with an ensemble of policies. It iteratively updates policies to balance exploitation and exploration, using a Gaussian RBF kernel with dynamically adapted bandwidth. The proposed improvement to Stein variational gradient descent (SVGD) involves using a statistical distance metric, specifically the JS kernel, to compare policies. This JS kernel incorporates the state-action visitation distribution and temperature to update policies in each iteration. The algorithm also includes estimating the gradient of the JS distance and applying it to each policy. In this section, the study compares self-imitation with standard policy gradients under different reward distributions. State-value function networks are used as baselines to reduce variance. The study benchmarks high-dimensional locomotion tasks using unimodal Gaussian policies and PPO algorithm. The study compares self-imitation with standard policy gradients using the PPO algorithm on locomotion tasks with different reward distributions. Performance is evaluated with a single agent, and the combination with SVPG exploration for multiple agents is discussed separately. In practical settings where designing a reward function is difficult, a method of providing rewards at the last timestep of an episode is used. This approach involves probabilistically masking out rewards at each timestep independently for each new episode, resulting in non-zero feedback at only a few timesteps. The learning curves on three tasks with episodic rewards are plotted, with a hyper-parameter \u03bd controlling the weight distribution between environment rewards and shaped rewards from r \u03c6. The baseline PPO agents use \u03bd = 0. The baseline PPO agents use \u03bd = 0, while self-imitating (SI) agents use \u03bd = 0.8 for credit assignment. PPO agents struggle with episodic rewards due to difficulty in credit assignment, while Self-Imitation benefits from shaped rewards for each timestep, leading to successful learning. The study compares the performance of self-imitating (SI) agents with baseline PPO agents in high-dimensional control tasks. SI agents with \u03bd = 0.8 achieve higher average scores across various reward settings, indicating the effectiveness of self-imitation in learning with sparse rewards. The results show that self-imitation can be integrated with standard policy gradients for successful learning. The study compares self-imitating agents with PPO agents in high-dimensional control tasks, showing that self-imitation is effective in learning with sparse rewards. CEM and ES algorithms perform poorly, with ES being sample-inefficient. Self-imitation can lead to sub-optimal policies, while the SVPG objective improves performance by training an ensemble with explicit policy repulsion. The study compares self-imitating agents with PPO agents in high-dimensional control tasks, showing self-imitation's effectiveness with sparse rewards. An ensemble of 8 self-imitating agents, denoted as SI-independent, gets trapped in local minima, leading to sub-optimal imitation. In contrast, SI-interact-JS, another instantiation of the algorithm, explores wider by sharing information for gradient calculation among agents. The study compares self-imitating agents with PPO agents in high-dimensional control tasks, showing self-imitation's effectiveness with sparse rewards. SI-interact-JS explores wider portions of the maze, with multiple agents reaching the green zone of high reward. Kernel matrices for the ensembles show dissimilar policies for SI-interact-JS compared to SI-independent. Self-imitation is tested in harder exploration problems in high-dimensional, continuous state-action spaces with modified MuJoCo tasks. The study compares self-imitating agents with PPO agents in high-dimensional control tasks, showing self-imitation's effectiveness with sparse rewards. SI-interact-JS explores wider portions of the maze, with multiple agents reaching the green zone of high reward. Kernel matrices for the ensembles show dissimilar policies for SI-interact-JS compared to SI-independent. Self-imitation is tested in harder exploration problems in high-dimensional, continuous state-action spaces with modified MuJoCo tasks. The performance of PPO-independent, SI-independent, SI-interact-JS, and SI-interact-RBF algorithms on tasks is plotted in FIG3. A survival bonus is given to bots that fall over, causing premature episode termination. SI-independent agents rely on action-space noise from the Gaussian policy parameterization to find high-return trajectories, which are added to demonstrations. The study compares self-imitating agents with PPO agents in high-dimensional control tasks, showing self-imitation's effectiveness with sparse rewards. In SparseHopper and SparseHalfCheetah, PPO-independent gets stuck in local optima, while SI-interact-JS encourages diverse exploration, leading to higher scores. SI-interact-RBF performs less effectively, indicating the superiority of the JS-kernel for exploration. The study compares self-imitating agents with PPO agents in high-dimensional control tasks, showing self-imitation's effectiveness with sparse rewards. Approaching policy optimization for deep RL from JS-divergence minimization perspective leads to a self-imitation algorithm improving standard policy-gradient methods. Significant performance gains are observed for high-dimensional, continuous-control tasks with episodic and noisy rewards. Ensemble training with SVPG objective and JS-kernel is proposed as a solution for potential limitations of self-imitation approach. Benefits of a self-imitating, diverse ensemble for efficient exploration and avoidance of local minima are demonstrated through experimentation. Future work includes improving the algorithm using exploration techniques in RL and applying parameter-space noise or curiosity-driven exploration to each agent. The algorithm for training diverse agents could be used in hierarchical RL and RL without environment rewards. Further investigation is needed for utilizing exact state-action densities for policy optimization. The current policy is approximated to be constant in a neighborhood around \u03b8, and the gradient is calculated using the policy gradient theorem. The sensitivity of self-imitation to \u03bd and the capacity of M E are shown in experiments on Humanoid and Hopper tasks with episodic rewards. A higher value of \u03bd boosts performance with episodic rewards. In experiments with episodic rewards, a higher value of \u03bd improves performance. Different values of C outperform baseline PPO. D JS repulsion can aid exploration in sparse environments and support hierarchical reinforcement learning. Pre-training with D JS repulsion in the Swimmer environment allows for acquiring diverse skills for later use in challenging tasks. Policy-gradients are calculated using the norm of velocity as rewards, with JS-kernel for exploration. Comparing 8 interacting agents with 8 independent agents, paths taken by the Swimmer show more variety. In the downstream task of Swimming+Gathering, the bot collects green dots while avoiding red ones. Pre-training a diverse ensemble improves performance on tasks, even without rewards. In a Hopper task with no rewards, weak supervision in the form of trajectory length is required. Policy parameters are parameterized by \u03b8. The text discusses optimizing policy parameters using Stein variational gradient descent (SVGD) to generate diverse, high-return policies efficiently. SVGD perturbs particles in the policy ensemble to reduce KL-divergence and converge to the optimal policy-parameter distribution. The perturbation for particle \u03b8 i is determined by a positive definite kernel function. Using a target distribution and JS-kernel, the gradient direction for ascent is calculated. The repulsion factor pushes \u03c0 \u03b8i away from \u03c0 \u03b8j, and similar repulsion can be achieved by using a reversed gradient w.r.t \u03b8 i. In the implicit method, a parameterized discriminator network is trained using state-action pairs from different policies to approximate the ratio. To reduce computational burden, explicit modeling of state-action visitation density is used by training networks for each policy. The agent uses log ratios as exploration rewards in the policy gradient theorem, with state-value function networks as baselines. Each agent in a population trains multiple networks for different rewards, including self-imitation rewards. Evaluation is provided for a method called self-imitation learning (SIL), with a loss function that buffers transitions and returns for training. The algorithm reuses stored returns if they are higher than current state-value estimates. The performance of PPO+SIL on MuJoCo tasks with different reward distributions is evaluated using 5M timesteps. Results show that performance suffers under episodic and masked reward settings compared to dense rewards. Our approach, PPO+SIL, utilizes dense, per-timestep rewards for credit assignment, leading to learning good policies even under episodic and noisy settings. Replay memory is used to store past good rollouts, similar to off-policy RL methods like DQN BID28. Performance evaluation of Twin Delayed Deep Deterministic policy gradient (TD3) algorithm on tasks with episodic and noisy rewards shows that TD3 suffers with episodic and noisy reward settings, indicating that off-policy algorithms do not exploit past experience effectively in such scenarios. The TD3 algorithm struggles to learn a good policy even with dense rewards. A new exploration baseline, EX 2, uses implicit state-density estimation for novelty-based exploration. TRPO is used as the policy gradient algorithm on hard exploration MuJoCo tasks. Results are averaged over 3 separate runs."
}