{
    "title": "Sk7cHb-C-",
    "content": "We propose an unsupervised method for building dynamic representations of sequential data, particularly observed interactions. The method acquires representations of input data and its dynamics through a hierarchical generative model with two levels. The model is a Bayesian network with switching variables in the higher level, actively exploring the latent space guided by knowledge and uncertainty. No encoder or inference models are used as the generators also serve as their inverse transformations. The method evaluates dynamic representations of sequential data in two scenarios: static images and videos. Results show improved performance over architectures without temporal dependencies. The system extracts data dynamics correlating with ground truth actions, aiding in characterizing interactions and building adaptive systems. Interactions like human activities in videos or vehicle-mounted camera data can benefit from understanding and differentiating repeating behaviors over time. The learning system aims to separate diverse types of dynamics in observed sequences, such as actions in videos or human activities, by dynamically adapting internal representations to changing data. This process involves representing each observation, like frames in a video, to differentiate behaviors over time. The learning system aims to represent observations, like frames in a video, to abstract the dynamics of the observed actions. In an unsupervised framework, the representations are defined based on the observed dynamics, with the goal of acquiring representational states and their dynamics simultaneously. The definition of representations is central to understanding how learning occurs. The representational process in learning is dynamic, adapting to changing environments and uncertainty in sensory data. It involves an active interpretation of observed data, opposed to a passive transformation into static representations. Implementing this requires systems like neural networks to adapt over time, with recurrent networks allowing previous states to influence current inputs. Neural networks can adapt to changing scenarios by modifying internal states based on feedback from prediction errors, allowing for active interpretation of the environment. This dynamic process enables the system to actively adjust to new information and uncertainties in sensory data. The ideas of actively interpreting and adapting to observed data coincide with dynamic views on conceptual representations in cognitive science. Concepts are seen as dynamic, distributed structures that are flexible, experience-dependent, and modality-specific. This flexibility is crucial for adapting to diverse situations and impacts the design of artificial learning agents. Representations in cognitive science are dynamic and time-dependent, emphasizing the importance of actively interpreting and adapting to the world. Concepts are viewed as flexible, distributed, and context-dependent, with simulation playing a fundamental role in concept acquisition and processing. This approach aligns with ideas on predictive coding and grounded cognition. The text discusses the relationship between predictive coding, the free energy principle, and generative models in cognitive science. It explores how the brain minimizes prediction error and updates internal beliefs using Bayesian methods and generative models to account for contextual reactions and causality. The text proposes a generative model that acts as an encoder and its own inverse model, updating internal states using prediction error. Different approaches based on predictive coding and prediction error minimization have been presented, focusing on training recurrent networks with hierarchical architectures encoding abstract representations of observed sequences. Other works have proposed similar approaches using convolutional neural networks and convolutional LSTMs. BID16, BID2, BID10, and BID5 propose hierarchical architectures for unsupervised and semi-supervised learning using recurrent neural networks with LSTMs. These models focus on predicting future frames, classifying actions, and minimizing prediction errors. However, none of these methods explicitly build variational representations. Recent works combine recurrent NNs with other models for more comprehensive approaches. Recent works have proposed models that combine recurrent NNs with state space models, using ideas from Bayesian networks and Kalman Filters. These models learn dynamic latent variables from prediction error signals and use variational Bayes or Variational Auto-Encoders to model data as probability distributions. The goal is to define internal states that represent different dynamics explicitly. The goal is to define internal states that represent different dynamics explicitly using generative models like VAE or a new architecture proposed by BID1. This architecture aims to estimate a latent representation of data without the need for an encoder, similar to GANs, resulting in more compact models with a direct coupling between representational states and data through the generator. The model presented is a hierarchical Bayesian network that does not rely on an encoder but only on the decoder or generator. It aims to modify distributions based on prediction error during the inference process, similar to the method by BID1. The architecture generates a prior of Z t+1 from the current state through a transition model A t, based on a generative model. The hierarchical Bayesian network generates transition models A t from current states S t at the dynamics level. Observations X t are generated from latent variables Z t at the observation level. The model adapts to constant inputs and changing input data over time, updating internal states to represent inputs and dynamics simultaneously. The model is based on predictive coding, updating Z t based on prediction error to converge to the best distribution representing X t. During training and inference, Z is optimized through backpropagation, exploring the latent space guided by uncertainty until a certain point in time. The model utilizes predictive coding to update Z t based on prediction error, converging to the best distribution representing X t. Z t is adjusted over time to reduce prediction error and match the best states for generating X. The error is defined as the mean Log-Likelihood over all predicted components in X. The model uses predictive coding to update Z t based on prediction error, aiming for stability by defining corrections in terms of momentum. Normalized corrections are used to prevent instabilities or oscillations around the optimal point during training. The model uses predictive coding to update Z based on prediction error, with normalized corrections to prevent instabilities. The correction diminishes as prediction error decreases, and values of \u03c3 Z are updated based on the correction. Loss is calculated as mean log-likelihood with Kullback-Leibler divergence regularization. The model utilizes predictive coding to update Z by minimizing the loss function with Kullback-Leibler divergence regularization. It includes a transition model to encode how states Z change over time, with a generative model that produces the transition model A t . The transition is considered linear, where Z t+1 = A t Z t + C, with C modeled by the sampling of Z t and S t to encode variance in observations and transitions. The model utilizes predictive coding to update Z by minimizing loss function with Kullback-Leibler divergence regularization. It includes a transition model to encode how states Z change over time, with a generative model that produces the transition model A t. The segmentation of observations over time is achieved by S, with updates for expected values of S calculated in relation to prediction error. The text discusses the normalization and non-linearity in modeling switching variables with S. It introduces a normalized correction \u2206S, a weight for change determination, and a sigmoid function for introducing non-linearity. The expression includes moving averages of expected value and variance, with a focus on prediction error. The parameters \u03b1, \u03c3 Lw, and \u03b2 are defined to activate a sigmoid function when prediction error exceeds normal levels. pN Lt tracks prediction error over time, activating the sigmoid if it rises above the threshold. The system activates the sigmoid when the prediction error exceeds normal levels, switching between transition models to reduce errors. The variance of the situation representation is updated to reflect changes, with a positive number set for experiments. The loss function includes prediction error and regularization to minimize divergence between prior and posterior of Z over T time steps. A term is added to keep transition model values low. The generative model uses a neural network with specific layers and filters to generate observations. The generator model consists of fully connected layers to produce a matrix A. The variance of the prior Z is estimated by another neural network with fully connected layers. The update process is separate from the general system. The generative model utilizes a neural network with specific layer sizes, disconnected from general backpropagation. The models are optimized using the AdaMax algorithm. Two experiments evaluate constant input and prediction based on changing transition models. Internal representations evolution is tested for best reconstruction quality and prediction error. Results are compared to a VAE with different architecture. The VAE is a generative model with representations determined by probability distributions and a defined loss function. Sharper images can be achieved with GANs and complex loss functions, but do not meet the criteria of variational representations. The dataset used is CelebA Dataset with aligned faces. Results show that active exploration of the latent space leads to better color details in images. The method proposed involves training with video sequences to show high correlations between changes in values of S and initial action times. Results are visualized by aligning video frames to values of \u00b5 St over time, depicting actions such as handing and receiving a ball, placing it on a table, and returning it. The model successfully segments actions in video sequences, aligning changes in values of \u00b5 St with predefined actions. The method involves representing dynamic data in two levels, learning generative models through active adaptation. The model successfully segments actions in video sequences based on prediction error propagation. Two experiments demonstrate the model's ability to adapt dynamically to data, leading to improved results compared to static models. The model can extract semantics similar to ground truth from videos of actions performed in a given scenario. The text suggests that interpreting the world is an active process, and better machine learning algorithms may involve understanding representations as a dynamic interaction with the environment or data evolution."
}