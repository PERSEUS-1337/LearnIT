{
    "title": "SJdCUMZAW",
    "content": "In this study, the focus is on solving the challenging task of grasping and stacking objects using deep reinforcement learning techniques. By enhancing the Deep Deterministic Policy Gradient algorithm with off-policy data and replay, high-performance control policies for dexterous manipulation can be achieved. The results suggest the potential for training successful stacking policies on real robots, addressing a fundamental challenge in robotics. Recent advancements in neural networks and deep learning have shown promise in solving control problems for robotics. While learning approaches offer flexibility and generality, they often require large amounts of data, which can be impractical for real robot applications. One solution is to incorporate task-specific knowledge to restrict the generality of the controller. In this paper, the focus is on learning precise manipulation skills using deep reinforcement learning in simulation. The goal is to stack a Lego brick onto another using a robotic arm and gripper, with the aim of eventually transferring these skills to real robotics hardware. The paper focuses on learning manipulation skills using deep reinforcement learning in simulation. It involves stacking a Lego brick onto another using a robotic arm and gripper, requiring mastering sub-skills like reaching, grasping, lifting, and stacking with precision and generalization. The paper builds on the Deep Deterministic Policy Gradient (DDPG) algorithm for reinforcement learning. The paper introduces a distributed version of the Deep Deterministic Policy Gradient (DDPG) algorithm to improve data efficiency and overcome computational bottlenecks in learning manipulation skills. It also presents strategies for designing effective shaping rewards and biasing the distribution of solutions for complex tasks. The paper introduces a distributed version of the Deep Deterministic Policy Gradient (DDPG) algorithm to improve data efficiency and overcome computational bottlenecks in learning manipulation skills. It also presents strategies for designing effective shaping rewards and biasing the distribution of solutions for complex tasks. The contributions allow for learning robust policies for the full stacking task in less than 10 million environment transitions, equivalent to less than 10 hours of interaction time on 16 robots. Additionally, using states from demonstration trajectories as start states for learning trials enables learning the full task with 1 million transitions, less than 1 hour of interaction on 16 robots. This is the first demonstration of end-to-end learning for a complex manipulation problem involving multiple freely moving objects, suggesting the possibility of learning such skills directly on real robots. Reinforcement learning (RL) approaches in robotics involve interactions with the environment guided by a reward signal. Policy search methods are commonly used due to their ability to handle continuous action spaces. However, they can struggle with complex tasks due to scalability issues. Guided policy search methods (GPS) use a teacher algorithm to optimize trajectories, improving data efficiency. Reinforcement learning in robotics involves interactions with the environment guided by a reward signal. Policy search methods, like guided policy search, optimize trajectories using a neural network policy for data efficiency. Model-free approaches, such as value function methods with neural networks, enable effective data reuse without requiring full access to the state space or a model of the environment. Recent advancements in deep learning have led to new end-to-end training methods for challenging control problems. The paper demonstrates that neural network approximators can be used for robotic manipulation tasks in the real world. Experimental rigs allow parallelized data collection using multiple robots simultaneously. Demonstration data plays a crucial role in robot learning by providing suitable cost functions and speeding up learning through kinesthetic teaching. The section explains the learning problem and summarizes the DDPG algorithm, which is related to other Q-function based RL algorithms. The RL problem involves an agent interacting with an environment to maximize rewards. The agent observes the state, takes actions based on a policy, transitions to a new state, and receives rewards. The goal is to maximize the expected sum of discounted rewards. DPG is a policy gradient algorithm for continuous action spaces that improves the policy function by backpropagating the action-value gradient from a Q-function approximation. DPG is a policy gradient algorithm that maintains a parametric approximation to the action value function Q \u03c0 associated with a policy \u03c0. DDPG incorporates experience replay and target networks to improve the original DPG algorithm. Experience is collected into a buffer for mini-batch updates, and target networks are used to compute y t in the algorithm. The DDPG algorithm maintains parameters \u03b8 and \u03c6 to compute y t, improving stability. Q-function enables off-policy learning, separating data collection from network updates. The task involves picking up and stacking Lego bricks, decomposed into grasping and stacking subtasks. The arm starts in a random configuration in each episode. In a physically plausible simulation, the arm is set up similar to a real-world Jaco arm. Episodes last 7.5 seconds with rewards given for task completion. Observations include arm and finger joint angles, brick positions, and gripper distances. Continuous actions control joint velocities. The arm and finger joints' velocities are set in experiments. Using only the raw state of the brick and arm configuration increased the number of environment interactions needed. Learning rate is optimized for each experimental condition, and 10 agents with different initial network parameters are trained and evaluated. The x-axis in the plots represents the number of environment transitions seen so far, and the y-axis represents episode performance. In this section, two methods for extending the DDPG algorithm are studied, showing significant effects on data and computation efficiency. Multiple mini-batch replay steps are explored to improve convergence of deep neural networks in reinforcement learning tasks. The modified DDPG algorithm performs a fixed number of mini-batch updates per iteration, impacting the solution to tasks. Video examples of policies solving tasks can be found at https://www.youtube.com/watch?v=7vmXOGwLq24. The DDPG algorithm was modified to include a configurable number of update steps, known as DPG-R, which significantly impacted the performance on tasks like Grasp and StackInHand. Increasing the number of update steps improved results, with successful policies for stacking and grasping achieved after a certain number of interactions. The DDPG algorithm was modified to include a configurable number of update steps, known as DPG-R, which significantly impacted the performance on tasks like Grasp and StackInHand. Increasing the number of update steps improved results, with successful policies for stacking and grasping achieved after a certain number of interactions. Additionally, insufficient training may lead to underfitting of the policy, affecting data quality and overall slow learning. Other aspects of network architecture can also impact learning speed. Increasing the learning rate cannot replicate the effect of multiple replay steps. In practice, attempts to adjust the learning rate to improve training stability have been found to be ineffective. Asynchronous DPG involves increasing the number of update steps relative to environment interactions, enhancing data efficiency but also significantly increasing compute time. Parallelizing computation can address the scalability issue when network updates dominate overall run time. In robotics setups, where interaction collection dominates run time, parallelizing training and environment interaction through an asynchronous DPG version can be beneficial. The asynchronous DPG algorithm involves actors and critics sharing network parameters, using asynchronous updates without synchronization. It utilizes the Adam optimizer with local non-shared first-order statistics and a single shared instance of second-order statistics. The pseudo code for the asynchronous DPG-R is shown in algorithm box 1. The asynchronous DPG algorithm involves actors and critics sharing network parameters for updates without synchronization. The pseudo code for asynchronous DPG-R includes initializing a replay buffer, selecting actions, updating shared critic and policy parameters, and updating target networks every S steps. Performance comparison of ADPG-R with different update steps and 16 workers is shown in Figure 2. Increasing the ratio of update steps per environment steps improves data efficiency in the asynchronous DPG-R algorithm. The use of multiple workers does not affect overall data efficiency for StackInHand but reduces it by half for Grasp. Figure 3 shows data plotted as a function of environment steps per worker, representing optimal wall clock efficiency. Theoretical wall clock time for 16 workers is significantly lower for StackInHand and Grasp. Distributing neural network training and data collection across multiple computers and robots reduces overall run time of experiments. Asynchronous DPG is extensively used for the experiments, with a sparse reward function for successful task completion. In the context of reducing run time for experiments using multiple workers, the text discusses the challenges of naive exploration in complex tasks and proposes shaping rewards to guide learning. The focus is on developing suitable reward functions for compositional tasks like the Stack task, offering a general recipe for similar tasks. Shaping rewards guide learning in compositional tasks like the Stack task by providing increasing rewards as subtasks are completed. Different composite rewards, such as Grasp shaping and Reach and grasp shaping, are considered in addition to the original sparse task reward. The agent receives rewards based on proximity to brick 1, grasping brick 1, and completing the full task. The reward functions combine sparse and distance-based components. The rewards increase as subtasks are completed, guiding the agent towards task completion without explicitly instructing on how to grasp or stack. In the previous section, a strategy for designing effective compositional reward functions to ease exploration burden was discussed. However, designing such rewards can still be error-prone, leading to unexpected failure cases. A complementary strategy is proposed to embed prior knowledge into the training process by initializing the learning agent at states reflecting the compositional nature of the task. This approach aims to improve exploration and address challenges related to reward design. Initializing episodes with states reflecting the compositional nature of the task can improve exploration and address challenges related to reward design. Suitable states can be defined manually or obtained from a human demonstrator or a previously trained agent, resembling apprenticeship learning without requiring complete trajectories or demonstrator actions. The approach does not require complete trajectories or demonstrator actions. Policies with mean return over 100 perform the full Stack from different starting states. Instructive start states allow learning even with sparse rewards. Two methods are used for generating starting states: manually defined initial states or randomly sampled from successful demonstration trajectories. The experiments in FIG3 show improved results with various reward functions combined with instructive start states. Even the second simplest reward function (Grasp shaping) produced controllers that can solve the task. Using pre-trained controllers with full composite shaping as start states also improved learning with basic sparse rewards. In combination with basic sparse rewards, an appropriate start state distribution led to a significant performance improvement in learning without the need for shaping rewards. Training policies for all seeds was achieved in less than 1 hour of interaction time on 16 simulated robots, indicating that simpler reward functions can be effective. The simplest reward function was found to be sufficient to solve the task in the final experiment. The DDPG algorithm has been extended to improve learning efficiency and reduce wall clock time for complex continuous control tasks. Through decoupling network updates from environment interaction and parallelizing data acquisition, robust policies were successfully learned for manipulation tasks. This approach suggests that modern learning methods can tackle challenging control problems effectively. The DDPG algorithm has been extended to improve learning efficiency for complex continuous control tasks. Initial experiments show good correspondence between simulation and real-world performance, but additional instrumentation may be needed for real-world application. The algorithms and techniques presented offer important advancements for robotics hardware in the lab. The algorithms and techniques presented offer important guidance for applying deep reinforcement learning methods to dexterous manipulation on a real robot. DDPG is related to other model-free RL algorithms like NAF and DQN, which use experience replay and target networks to stabilize learning with neural networks. The DDPG and DQN algorithms use mini-batch updates for the Q-function and policy, distinguishing them from NFQ and NFQCA. NFQ and NFQCA perform less frequent updates with full re-fitting of the Q-function and policy network after each episode. NFQCA is data efficient but may face challenges with large networks or observation spaces. DPG is a deterministic version of the stochastic value gradients family. In this section, further details are provided on composite reward functions derived from the state vector of the simulation. These reward functions are defined based on various quantities such as the height of a brick above a table and the positions of different sites and hand pinch site. The pinch site of the hand is defined by x,y,z positions where the fingertips would meet when closed. Conditions for subtask completion include reaching Brick 1 within a virtual box, grasping Brick 1 above a table surface by a threshold, and stacking Brick 1 on Brick 2 with a displacement constraint. The rotation matrix projects a vector into the coordinate system of brick 2, ensuring the box constraint is considered relative to brick 2's pose. The composite reward includes distance-based shaping components guiding the hand to brick 1 and then to brick 2. These shaping components can be easily implemented with hardware such as a contact sensor or a visual system. The reward functions are implemented using these components. The reward functions implemented in the system are based on various shaping components and predicates to determine subtask completion, resulting in different composite reward functions. These functions include Stack, Grasp shaping, Reach and grasp shaping, and Full composite shaping, each providing rewards based on the completion of specific subtasks."
}