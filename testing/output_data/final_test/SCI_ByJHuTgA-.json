{
    "title": "ByJHuTgA-",
    "content": "Ongoing innovations in recurrent neural network architectures have led to state-of-the-art results on language modelling benchmarks. Reevaluating popular architectures with large-scale hyperparameter tuning, it was found that properly regularised standard LSTM architectures outperform newer models, achieving new state-of-the-art results on various datasets. The deep learning research community evaluates model quality through empirical studies, where hyperparameter values play a crucial role in model performance. In this paper, a black-box hyperparameter optimization technique is used to compare language modeling architectures like LSTMs, Recurrent Highway Networks, and NAS. Flexible model families are specified with control over embedding sizes, recurrent cell sizes, regularisation, and learning. In a study comparing language modeling architectures like LSTMs, Recurrent Highway Networks, and NAS, it was found that LSTMs outperformed newer models when hyperparameters were properly controlled. This highlights the importance of carefully managing hyperparameter variation to avoid replication failures in deep learning. Despite the computational cost, it is possible to achieve reliable results with careful controls. The findings suggest that unreliable evaluation may not be unique to specific tasks like machine translation, emphasizing the need for thorough hyperparameter control in deep learning research. The research emphasizes the importance of carefully managing hyperparameter variation in deep learning. Three recurrent architectures are discussed: Long Short-Term Memory, Recurrent Highway Network, and NAS, each with their own strengths and performance levels. More consensus on experimental methodology and attention to hyperparameter sensitivity is needed in the research community. The study compares different recurrent architectures like LSTM, Recurrent Highway Network, and NAS BID27. They focus on model comparisons and avoid techniques that lower perplexities but are unrelated to the architecture's merits. Other studies show the benefits of Neural Cache BID5 and Dynamic Evaluation BID6. The models with LSTM or NAS cells have standard components like input embedding lookup tables and stacked recurrent cells with skip connections for optimization. The curr_chunk discusses the use of dropout in RHN models, with different types of dropout applied to various connections. The models are depicted as a single horizontal \"highway\" for processing recurrent states through time. The presence of an optional down-projection reduces the number of output embedding parameters. The RHN state is passed between layers in a single highway, unlike LSTMs which have separate recurrent connections for each layer. Dropout variants are applied to all models, except for intra-layer dropout in RHNs. Models are compared on three datasets: Penn Treebank, Wikitext-2, and Enwik8. The Enwik8 corpus from the Hutter Prize dataset is used for training word level models with specific batch size and truncated backpropagation. The model starts with a zero state at the beginning of training and test time, with a bias towards easily starting from this state. Optimization is done using Adam with customized parameters. The exponential moving average for gradient means in Adam is close to RMSProp without momentum, allowing for larger learning rates. Batch size is 64, learning rate decreases if validation performance doesn't improve. Different settings for Enwik8 include truncated backpropagation with 50 time steps, \u03b22 = 0.99, = 10 \u22125, and batch size 128. Evaluation uses best validation perplexity checkpoint, with word-based datasets showing worse results with training batch size. Throughout the experiments, the training batch size had a negative impact on word-based datasets, resulting in a 0.3 increase in perplexity. However, Enwik8 was not significantly affected due to its larger evaluation and training sets. Preliminary tests suggested that MC averaging could slightly improve perplexity and bits per character, similar to BID3 results but much more computationally expensive. Therefore, mean-field approximation for dropout was used at test time. Hyperparameters were optimized using Google Vizier BID4, a black-box hyperparameter tuner, focusing on learning rate, input embedding ratio, and various dropout parameters. For deep LSTMs, tuning hyperparameters like input dropout, state dropout, output dropout, weight decay, and intra-layer dropout is crucial. Thousands of evaluations are needed for convergence. Models are compared based on total trainable parameters rather than hidden units. The tuner controls the tradeoff between embedding and recurrent cell parameters. The actual parameter budget determines cell and embedding sizes. Enwik8 has few embedding parameters due to a small vocabulary size. LSTMs and RHNs of various depths were tested. In experiments comparing LSTMs and RHNs of different depths with varying parameter budgets, results show that even a shallow LSTM with 10M parameters outperforms a 24M RHN. NAS architecture performs well on the dataset, with models tuned for Penn Treebank also showing reasonable performance on Wikitext-2. In experiments comparing LSTMs and RHNs of different depths with varying parameter budgets, results show that even a shallow LSTM with 10M parameters outperforms a 24M RHN. NAS architecture performs well on the dataset, with models tuned for Penn Treebank also showing reasonable performance on Wikitext-2. Our results on this dataset are below the previous state of the art, with our best result of 65.9 comparing favorably to the Neural Cache BID5. RHNs lag behind all models significantly, while NAS architecture may have overfitted to Penn Treebank. Our models match the smaller RHN of BID26, with NAS lagging behind. Improvements were made on two datasets through careful model specification and hyperparameter optimization, with larger gains seen for LSTMs compared to RHNs. The original RHN experimental conditions may have been more extensively tuned, leading to the observed differences. NAS benefited minimally from tuning due to similarities with BID27, with the slight edge attributed to the suboptimality of grid search. The three recurrent cell architectures are closely matched on all three datasets, with minuscule differences on Enwik8 where regularisation matters the least. Results support the claims that capacities of various cells are very similar and differences result from trainability and regularisation. NAS, one of the best human designed and one machine optimised cell, was the top performer among thousands of candidates. Down-projection was found to be very beneficial for some depth/budget combinations, improving results on Penn Treebank by about 2-5 perplexity points at certain depths. The experiments evaluated the contribution of different features in LSTM and RHN models at various depths and parameter budgets. Untying input and output embeddings worsened perplexities by 6 points, while removing variational dropout had a negative impact on RHN models. The deep LSTM also experienced a loss in perplexity without intra-layer dropout. The experiments compared the impact of different dropout methods on LSTM and RHN models. Results showed no consistent advantage between recurrent dropout and variational dropout. Various sources of noise were identified, leading to concerns about overfitting. Models were retrained with different initialisation seeds to assess the severity of these issues. The experiments retrained models with different seeds to evaluate the impact of noise on validation and test scores. Results showed significant variance due to different seeds, with a notable effect on perplexity levels. The tuner could only partially fit the noise, indicating limitations in reproducing good results consistently. Our findings suggest that a gap in perplexity of 1.0 is a statistically robust difference between models trained on Penn Treebank and Wikitext-2 datasets. The distribution of results was approximately normal with the same variance for all models, so we report numbers in a tabular form instead of plotting the distribution. The validation loss was plotted against hyperparameter settings in a violin plot BID8. The majority of settings near the best values produced perplexities within 3.0. The hyperparameter surface behaved well, and a grid search was considered for the remaining six hyperparameters. The study explored hyperparameter tuning for LSTM models, showing that LSTMs are not significantly affected by changes in gate configurations. The results indicated that the number of parameters can be reduced without impacting performance significantly. The study found that LSTMs are insensitive to changes in gate configurations and benefit from bounded cell states. As deep neural language models replace shallower ones, the need for careful comparison of deep architectures has increased due to smaller effect sizes and the risk of faulty conclusions in a rapidly growing research community. This paper highlights the challenges in comparing deep neural architectures due to smaller effect sizes and the risk of faulty conclusions. It suggests establishing reliable baselines as benchmarks for future work and emphasizes the tradeoff between computation and result reliability. Solutions include reducing hyperparameters, better optimization strategies, and defining computational budgets for model evaluation."
}