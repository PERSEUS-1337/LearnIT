{
    "title": "B1ae1lZRb",
    "content": "Deep learning networks have achieved state-of-the-art accuracies on computer vision tasks, but deploying these models on resource-constrained systems is challenging due to their compute and memory requirements. Techniques like low precision numerics and knowledge distillation can help reduce these requirements. A new approach called Apprentice combines these techniques to improve the performance of low precision networks, achieving state-of-the-art accuracies on ImageNet dataset with ternary precision and 4-bit precision for various ResNet architectures. The study explores applying knowledge distillation techniques to different stages of the train-and-deploy pipeline for high-performing deep neural networks in computer vision. These networks have significant compute and memory requirements during training and inference, with memory footprint dominating during activation and model storage. Training is done on large datasets with big batch sizes, while inference batch sizes are typically small. The complexity in compute, memory, and storage necessitates careful management during the training phase. During the training phase of deep neural networks in computer vision, complexity arises in compute, memory, and storage requirements. The networks are trained on CPU and/or GPU clusters in a distributed computing environment. Deployment of trained models on resource-constrained inference systems like portable devices or sensor networks poses challenges due to constraints on memory, compute, and power. Quantization and model compression have emerged as popular solutions for such scenarios, where a low-precision version of the network model is generated and deployed on the device. The low-precision version of the network model is deployed on devices to reduce compute and storage requirements. Existing works in low-precision DNNs often sacrifice accuracy. Different schemes involve transferring knowledge from a full-precision network to train a low-precision network, resulting in faster convergence but similar accuracies. Apprentice networks with lowered precision and knowledge distillation techniques can marginally improve the accuracy of low-precision networks. The techniques presented in the paper focus on obtaining low-precision DNNs using knowledge distillation. These techniques result in improved accuracy for low-precision models, setting new state-of-the-art accuracies for ResNet models at ternary and 4-bit precision. The contributions include producing low-precision models that surpass previous accuracy levels and help models converge faster. These accurate low-precision models are beneficial for deployment on resource-constrained systems and cloud-based platforms. During the inference phase of DNNs, memory is allocated for input and output feature maps as well as weight tensors. Lowering the precision of weight tensors can reduce memory requirements during deployment, especially when using small batch sizes. This approach is illustrated for various networks running image patches. Additionally, reducing the precision of weight tensors can help lower the memory footprint and working set. During deployment, lowering memory footprint by reducing accesses to off-chip memory improves performance and energy savings. Low-precision compute simplifies hardware implementation, replacing complex circuits with simpler elements like xnor and popcount logic. This helps lower inference latency and energy budget. Low-precision models reduce compute and data requirements, but may lead to degraded accuracy. Research on low-precision DNNs focuses on reducing precision of weights for efficient inference. Various methods like Binary connect (BC) and Ternary-weight networks (TWN) target precision reduction. These methods serve as baselines for comparing network accuracies in literature. Research on low-precision DNNs focuses on reducing precision of weights for efficient inference. Methods like TWN, INQ, XNOR-NET, DoReFa, and TTQ target weight quantization. While some methods show accuracy loss when quantizing below 8-bits, others like BID10 advocate for 16-bits for training on CIFAR10. Activation quantization generally hurts accuracy, with XNOR-NET and DoReFa showing significant accuracy degradation. Knowledge distillation techniques involve using a teacher-student strategy to transfer information from a large deep network to a shallower student network. BID3 and BID2 demonstrate compressing ensemble information into a single network and studying shallow, wide topologies. BID16 introduces the concept of temperature to transfer knowledge by adjusting logits before Softmax function. FitNets BID27 extend the use of intermediate hidden layer outputs as target values for training a deeper, thinner student model. Net2Net BID5 utilizes a teacher-student network system with a function-preserving transformation approach to accelerate the training of a larger student network. BID43 and BID42 use attention and an information metric, respectively, to transfer knowledge from one network to another. BID1 proposes a reinforcement learning-based approach for compressing a teacher network into an equally capable student network. Knowledge distillation is introduced as a method for compressing a teacher network into a student network, achieving a compression factor of 10x for ResNet-34 on CIFAR datasets. Other techniques for model compression include pruning, hashing, and weight sharing, which reduce parameter memory footprint. Efficient hardware support is necessary for realizing the benefits of these schemes during runtime. In knowledge distillation, parameters W T and W A represent the teacher and student networks, respectively. The cost function consists of terms for improving the teacher and student networks, as well as a knowledge distillation term. A temperature factor \u03c4 is used to soften the probability distribution of the teacher network's logits. Cross-entropy function is used with weighting factors \u03b1 = 1, \u03b2 = 0.5, and \u03b3 = 0.5 for transfer learning. The effect of varying parameters is studied in experiments. In knowledge distillation, the teacher network's depth and precision are varied to improve efficiency. Model compression aims to enhance network efficiency. Apprentice combines both techniques to boost network accuracy and runtime efficiency. Three schemes are explored to obtain a low-precision student network, including joint training with a full-precision teacher network and distilling knowledge from it. The text discusses three schemes for obtaining a low-precision student network through knowledge distillation. The TTQ scheme achieves state-of-the-art accuracy with ternary precision for weights and full-precision for activations. Top-1 error rates of 28.3% and 25.6% were achieved for ResNet-34 and ResNet-50 models on Imagenet-1K dataset. For 2-bits weight and full-precision activations, models achieved 28.3% and 25.6% Top-1 error rates. BID22's work set the baseline for 2-bits weight and 8-bits activation models, with 29.24% Top-1 error for ResNet-50. WRPN scheme BID23 reported the highest accuracy for 4-bits precision. Implementing this scheme for 4-bits weight and 8-bits activations resulted in 29.7% and 28.4% Top-1 error rates for ResNet-34 and ResNet-50 models on Imagenet-1K. The first investigated scheme involves training a full-precision teacher network with a low-precision student network using ResNet topology. In BID3 and BID16, the student network trains while distilling knowledge from the teacher network. They jointly train with the teacher guiding the student towards higher accuracy logits. The pre-activation version of ResNet is implemented in TensorFlow BID0. Ternary precision is used for low-precision numerics, with weights quantized into {-1, 0, 1}. The WRPN scheme BID23 is used to quantize weights and activations to 4-bits or 8-bits, without lowering the precision of the first and final layers. During training and fine-tuning, gradients are maintained at full-precision. Lowering precision of layers degrades accuracy, as observed in prior works. Results with ResNet-18 show accuracy drop by 3.5% with ternary and 4-bits precision. Distillation technique improves accuracy of low-precision models. The accuracy of low-precision configurations improves significantly with distillation technique. Pairing a larger full-precision ResNet model with ResNet-18 also enhances accuracy. The best full-precision accuracy was achieved with a student ResNet-18 and ResNet-101 as the teacher. The gap between full-precision ResNet-18 and the best ternary weight ResNet-18 is only 1%. Regularization with low-precision and distillation is hypothesized to improve accuracy. The accuracy of the student ResNet-18 model surpasses the baseline with \"8A, 4W\" configuration. The low-precision student networks, guided by a teacher network, show significant improvement in accuracy compared to not using a teacher network. The gap between low-precision and full-precision network accuracies is narrowed, with some configurations even surpassing the baseline accuracy. Distillation techniques are shown to enhance accuracy in larger models like ResNet on ImageNet dataset. Results with ResNet-34 and ResNet-50 student networks, guided by a teacher network, show significant improvement in accuracy compared to not using a teacher network. The Apprentice scheme closes the gap between full-precision baseline networks and low-precision variants, with accuracy numbers improved by 1.5%-3%. In scheme-A, a ternary ResNet-34 student network paired with a full-precision ResNet-18 is 8.5x smaller in size, with a final trained accuracy 2.7% worse than the full-precision model. The distillation scheme works best when the teacher network is higher in accuracy than the student network, regardless of capacity. Using a larger teacher network reaches a saturation point in benefits. Joint training of low-precision and high-precision networks raises concerns about the small network's influence on the large network's accuracy. The joint cost function matches the smaller network's scores with the teacher network's predictions, adding a term to the total loss function. The joint cost function is added to the total loss function, affecting the learning capability of larger networks due to smaller network impairments. However, in practice, the accuracy of the teacher network remains consistent when jointly trained with a student network. This could be attributed to the choice of \u03b1, \u03b2, and \u03b3 values for the Softmax function and hyper-parameters. The hyper-parameters used in the experiment were \u03b1 = 1, \u03b2 = 0.5, and \u03b3 = 0.5. Training directly on the teacher network logits eliminated the need to experiment with \u03c4 values. A \u03c4 value of 1 yielded the best results when training on soft targets. Small \u03c4 values are more effective when the student network is significantly smaller than the teacher network. Experimenting with different configurations of \u03b1, \u03b2, and \u03b3 resulted in lower performance models compared to the original parameters. Different loss functions were also tested for the third term in equation 1. In the distillation process, various loss functions were tested, but no improvement in accuracy was observed compared to the original cost function formulation. Future work will explore different hyper-parameter values and loss functions. Despite this, distillation proved effective in achieving high accuracy low-precision models, surpassing previously reported figures. For instance, the best ResNet-18 model with 2-bits weight achieved a 31.5% error rate, outperforming the TTQ scheme by 2%. The Apprentice network achieved a 27.2% Top-1 error at this precision level. In Scheme-A, a trained teacher network is used as a baseline for training a low precision student network from scratch. Scheme-B involves pre-computing and storing logit values for input images to save forward pass computations in the teacher network. Scheme-B involves pre-computing and storing logit values for input images to save forward pass computations in the student network. This allows the student network to learn \"dark knowledge\" from the teacher network trained on private data, achieving similar accuracy with fewer epochs. The training accuracies are comparable to those reported in previous tables, and low-precision student networks learn faster. Figure 5 shows Top-1 error rates for different configurations in the experiment suite. Scheme-C, similar to scheme-B, fine-tunes the student network with full precision training weights before training, resulting in faster convergence with a lower learning rate due to a good initial point. Training with a low learning rate is crucial for optimal accuracy, with a suggested approach of 1e-3 for 10-15 epochs, followed by 1e-4 for 5-10 epochs, and then 1e-5 for 5 epochs. Some configurations stabilize after 40-50 epochs, while others benefit from scheme-B with warm startup. Distillation may not be necessary for AlexNet, as techniques from previous studies can achieve comparable results. The potential of distillation on larger networks is explored in this paper. In this paper, the authors explore a scheme for improving accuracy in larger networks. They compare different schemes (A, B, and C) and find that scheme-C yields slightly better accuracy. For ResNet-50 student network, ternary weights improve accuracy by 0.6% compared to scheme-A, setting a new state-of-the-art error rate of 24.7%. This brings the ternary ResNet-50 within 0.9% of the baseline accuracy. Scheme-C for ResNet-50 model with 4-bits weight and 8-bits activations is 0.4% better than scheme-A, closing the gap to be within 1.3% of full-precision accuracy. Ternary precision models reduce model size by a factor of 2/32 compared to full-precision models, showing how one can achieve a performant model with ternary precision. Many works focus on network pruning and sparsification techniques targeting full-precision models. Ternary precision models reduce model size significantly compared to full-precision models. Sparsity is crucial for effective sparse models, with ternary models being about 50% sparse. Prior works on sparsification of full-precision networks have achieved less sparsity and accuracy compared to ternary models. Ternary precision models are considered state-of-the-art in accuracy and model size reduction. The text discusses improving the accuracy of low-precision networks to bridge the gap with full-precision models. Three schemes based on knowledge distillation are presented to enhance accuracy in resource-constrained systems. The goal is to simplify inference deployment on systems with limited resources and low latency requirements. The study experiments with Apprentice scheme on CIFAR-10 dataset, using various depths of ResNet topology. The network consists of 6n+2 weight layers with different numbers of filters in each set of layers. Lowering precision impacts the network as its depth varies. The impact of lowering precision on ResNet varies with depth. Larger networks show diminished impact compared to full-precision accuracy. Ternarizing a model at ResNet-110 yields similar error rates as full-precision, while at ResNet-20, there is a 0.8% gap in error rates. Lowering both weights and activations leads to significant accuracy degradation. The Apprentice scheme helps reduce this gap. The impact of lowering precision on ResNet varies with depth, with larger networks showing diminished impact compared to full-precision accuracy. Experimentation was done with different schemes, including joint training of teacher and student networks, compression using distillation scheme, and fine-tuning without distillation. The error rates for different configurations lie between those shown in FIG6 and FIG7. The error-rate for the scheme discussed lies between the error-rates in FIG6 and FIG7. Scheme-A gives 0.7% better accuracy at low-precision configurations compared to the discussed scheme. Some works suggest wider layers or larger models to recover accuracy at low-precision. Future work could explore low-precision models with less layer widening factor, such as 1.10x or 1.25x, to improve inference latency while maintaining accuracy. Investigating hyper-sparse network models without accuracy loss using distillation based schemes is also a promising area for further research."
}