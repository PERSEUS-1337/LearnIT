{
    "title": "Hyg0vbWC-",
    "content": "We demonstrate that generating English Wikipedia articles can be treated as a multi-document summarization task. Utilizing extractive summarization to identify key information and a neural abstractive model to create the article. Our decoder-only architecture allows for scalable attention to long sequences, enabling the generation of coherent multi-sentence paragraphs and complete Wikipedia articles. The model can extract relevant facts when provided with reference documents. The sequence-to-sequence framework has shown success in natural-language tasks like machine translation. Recent work has focused on abstractive text summarization of news articles. While prior work dealt with single-document summarization, we are exploring multi-document summarization. Extractive summarization has been the main focus, selecting key information from related documents to form summaries. In this work, the focus is on abstractive neural methods for multidocument summarization using English Wikipedia as a supervised machine learning task. The Transformer architecture BID18 is modified to only consist of a decoder, showing better performance with longer input sequences compared to RNN and Transformer encoder-decoder models. Neural abstractive summarization was pioneered in BID15 with headline generation models using the English Gigaword corpus. RNN-based encoder-decoder models perform well on this task in both ROUGE and human evaluation. An abstractive summarization dataset is proposed in BID9, which is more challenging than headline generation. The dataset for abstractive summarization has fewer parallel examples compared to other datasets. Various techniques are used to improve performance, but guidelines for creating story highlights are unclear. Neural abstractive models are also trained on multi-document data from Wikipedia, resulting in larger summaries with stylistic uniformity. The input documents for extractive summarization may vary in style and source. The ROUGE-1 recall scores show that our dataset is less suitable for extractive methods compared to other summarization datasets. Previous work has utilized Wikipedia for machine learning tasks such as question answering, information extraction, and text generation. The BID16 project involves generating Wikipedia articles extractively from reference documents using learned templates. They use all article types and search engine references, including documents from the References section of Wikipedia articles. Previous work on neural abstractive summarization has relied on RNNs, but the Transformer architecture has shown promising results in machine translation tasks. The Transformer architecture shows promising results in medium-length input sequence summarization, with modifications to handle longer sequences. The study involves using Google search results to supplement Wikipedia articles, removing clones and refining search results for each article. The dataset, WikiSum, is described with a focus on article properties and the importance of supplementing with web search results due to low citation counts. The dataset WikiSum is significantly larger than previous summarization datasets, with 1865750, 233252, and 232998 examples for train/development/test subsets. Due to memory constraints, an extractive summarization is used to select input text before training an abstractive model for generating Wikipedia text. This two-stage process mimics how humans summarize long documents by highlighting key information. The dataset WikiSum is large, with over 1.8 million examples for training. Extractive summarization is used to select input text before training an abstractive model, mimicking how humans summarize long documents. Three extractive methods are investigated, along with a trivial and cheating method, to assess the importance of this stage. TextRank and SumBasic are two methods used for extractive summarization. TextRank creates a weighted graph based on word overlap, while SumBasic uses word frequencies to score sentences. Additionally, a cheating extractor is implemented to assess the quality of extraction. In the abstractive stage of data representation, paragraphs are concatenated in order with the title, encoded using sub-word tokenization, and truncated for input sequence formation. Abstractive models learn to write articles by treating it as a sequence transduction problem, with LSTM encoder-decoder with attention used as a baseline. In the abstractive stage of data representation, paragraphs are concatenated with the title, encoded using sub-word tokenization, and truncated for input sequence formation. A stronger baseline model, the non-recurrent Transformer model, is introduced with a modification for long sequences that combines input and output sequences into a single \"sentence\" for training as a standard language model. The model predicts the next word given the previous ones, with error signals propagated from both input and output time-steps during training. The Transformer model combines input and output sequences for training, optimizing easier with longer sequences. Attention is based on query (Q) and key (K) and value (V) pairs. Local attention divides sequence tokens into blocks for efficient processing. Memory-compressed attention allows for independent attention in blocks of tokens, reducing memory cost and keeping activations linear with sequence length. This modification uses strided convolution to decrease the number of keys and values while maintaining the number of queries. Unlike local attention, memory-compressed attention can exchange information globally across the entire sequence, enabling processing of sequences 3x in length. Our final architecture consists of a 5-layer network alternating between local-attention (L) and memory-compressed attention (M) layers. We also included one mixture of experts (MoE) layer to increase capacity. Evaluation is based on perplexity and ROUGE-L F1 metrics. ROUGE-L F1 is more suitable for abstractive models as it balances ROUGE-Recall and ROUGE-Precision. The text discusses optimizing for perplexity to improve ROUGE and human judgment in abstractive summarization tasks using the tensor2tensor library. Models trained with Transformer encoder-decoder architecture showed minimal overfitting and did not require early-stopping. The Transformer Decoder (T-D) and T-DMCA models were used for decoding, with enhancements in T-DMCA. Extractive methods like tf-idf, SumBasic, and TextRank were evaluated without an abstractive model, showing similar ROUGE-L F1 scores. However, the best abstractive model significantly outperformed extractive methods in terms of ROUGE-L F1 and linguistic quality. Smart extraction was found to be crucial for final performance. From the previous context, it is evident that smart extraction is crucial for final performance. The extractive method, tf-idf, shows a significant gap compared to other methods. Future work could focus on improving the extraction step to enhance overall performance. Additionally, combining datasets yields the best results, with citations and search results contributing complementarily. The abstractive model architecture, seq2seq-attention, performs poorly based on the given data. The Transformer architectures outperform seq2seq-attention on the task, with the Transformer-Decoder showing improved performance up to a length of 4000. Using T-DMCA modifications, training was possible up to a length of 11000, with the MoE-layer enhancing performance at high lengths. The best model achieved a perplexity of 1.90 with 256 experts at length 7500. The T-DMCA model outperforms other models in linguistic quality evaluation, scoring significantly better on various dimensions compared to baseline models. Despite some repetition of phrases, the abstractive model shows high fluency and coherence. The extractive model also performs well, particularly in non-redundancy. The T-DMCA model outperforms other models in linguistic quality evaluation, scoring significantly better on various dimensions compared to baseline models. Human evaluation scores show T-DMCA as the best model, with tf-idf-only and seq2seq-attention models scoring lower. Automatic metrics correlate with human preference, especially for models with large performance gaps. The T-DMCA model outperforms other models in linguistic quality evaluation, scoring significantly better on various dimensions compared to baseline models. Future work should focus on improving the extractive stage and extending decoder-only architectures to learn from larger L while maintaining model capacity. Comparing with BID16 is challenging due to differences in reported results and input/output articles used. The T-DMCA model performs better on American Actors than Diseases, showing higher ROUGE-1 scores. Different models show improvements in fluency, accuracy, and complexity as perplexity decreases. The T-DMCA model offers a succinct alternative to Wikipedia, mentioning key facts about a law firm. The T-DMCA model can translate names into multiple languages accurately and can generate entire Wikipedia articles. It is possible to train the model on combined input-output sequence lengths of approximately 12000 using the T-D architecture. The T-DMCA model can generate Wikipedia articles by training on reference tokens. The model organizes articles into plausible sections and exhibits global coherence. It inserts factual information and can be further developed for full-article abstractive summarization tasks. The study introduced a two-stage extractive-abstractive framework for large-scale summarization. A new decoder-only sequence transduction model was proposed for the abstractive stage, outperforming traditional architectures on long sequences. The researchers will release the URLs used in their experiments and provide code for content extraction from the CommonCrawl dataset. The researchers used an open-source library for training abstractive models and will release their modeling code extensions. They evaluated model outputs based on linguistic quality using five dimensions. Human evaluation involved comparing model outputs side-by-side in a randomized manner. In Table 6, 3 raters scored 25 examples each to compare model preferences. A human evaluation tool asked raters to choose between model outputs. The extractive method used was tf-idf."
}