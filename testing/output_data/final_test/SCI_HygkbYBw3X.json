{
    "title": "HygkbYBw3X",
    "content": "In this paper, a novel method is introduced to interpret recurrent neural networks (RNNs), specifically long short-term memory networks (LSTMs) at the cellular level. A systematic pipeline is proposed for interpreting individual hidden state dynamics within the network using response characterization methods. The method can identify neurons with insightful dynamics, quantify relationships between dynamical properties and test accuracy, and interpret the impact of network capacity on a network's dynamical distribution. The generalizability and scalability of the method are demonstrated through evaluation on different benchmark sequential datasets. The paper introduces a method to interpret LSTM networks at the cellular level by analyzing response characteristics. It can predict network dynamics, rank cells based on their contribution, and provide reproducible metrics of interpretability. The methodology is evaluated on various datasets, showing scalability to large networks. Key contributions include a novel algorithm for systematic LSTM analysis. The paper presents a novel algorithm for interpreting LSTM networks through response characterization. It evaluates the method on different datasets for classification and regression tasks, providing detailed interpretations at both single cell and network scales. The analysis enables insights into deep neural network interpretability. Various methods have been proposed for interpreting deep networks through feature visualization and neural activation analysis. Feature attribution, such as computing saliency maps, can help understand the contributions of hidden neurons at different scales. Dimensionality reduction techniques can also be used to simplify high-dimensional feature maps. Recent advancements in interpretability methodologies for deep neural networks include methods like LSTMVis, which relates hidden state dynamics patterns of LSTM networks to larger networks to explain individual cell functionality. A systematic framework has been introduced to combine interpretability methodologies across multiple network scales, enabling exploration over various levels of interpretability. There is still room to incorporate techniques such as robust statistics, information theory approaches, gradients in correlation-domain, and response characterization methods. In this paper, interpretability methodologies for deep neural networks, such as LSTMVis, are discussed. The visualization of hidden states in RNNs on text datasets reveals interpretable cells detecting language syntax and semantics. Gradient-based attribution evaluation methods help understand RNN functionality in localizing key words in text. However, interpreting the network may require detailed prior knowledge about the data content, posing challenges in generalizing to other sequential data forms like time-series. In our study, interpretability for RNNs is enhanced through the attention mechanism, allowing the network to focus on specific parts of the input space. Hidden-state visualization is crucial for understanding the network's internals. MAC cells introduced by Hudson et al. enable interpretable machine reasoning engines, performing accurate reasoning directly from the data. However, applying these modifications to arbitrary network architectures, especially LSTM, can be challenging. In the context of enhancing interpretability for RNNs through attention mechanisms and hidden-state visualization, the extension to arbitrary network architectures, particularly LSTM, is challenging. Rigorous studies on the dynamics of RNNs, including LSTM, have been conducted, with analyses on long-term dynamics and gate ablation. A novel response characterization method is introduced to understand LSTM hidden state dynamics. A vanilla recurrent neural network (RNN) is formulated as a control dynamical system with internal state h, input x, and real matrices representing weights. The activation function \u03c3 is crucial for the network's operation. Long short term Memory (LSTM) networks, a type of gated-recurrent neural network, use three gating mechanisms to handle training challenges. They have distinct outputs for different initial states and unique internal parameter settings. Information flow in LSTM networks involves static and time-varying behavior, creating a complex system. In this paper, a technique is proposed to find hidden observable dynamics within LSTM networks using response characterization. The approach enables a systematic and quantitative understanding of the network's dynamics on both macro and micro scales. By analyzing the output of the system with various inputs, a computational pipeline is established for reasoning about the hidden units' behavior. The paper proposes a technique to uncover hidden dynamics in LSTM networks by characterizing their responses. This method allows for a detailed analysis of the network's behavior at different scales, revealing insights such as component strength, reactiveness to input changes, and phase alignment. The approach involves analyzing the network's responses to step and sinusoidal inputs to gain a deeper understanding of its dynamics. The study uses LSTM units to approximate dynamics of individual cells by extracting input and recurrent weights. Response metrics include initial and final response, response output change, and delta response to determine LSTM unit strength in terms of output amplitude. The settling time of the step response is the elapsed time from the input change to when the output reaches a 90% threshold. Computing settling time for LSTM units helps identify \"fast units\" and \"slow units\", predicting active cells. Amplitude and frequency of cyclic response signal determine output difference and cycle rate. Response frequency is computed by evaluating power spectral density. Amplitude metric ranks LSTM cells based on output contributions, effective for RNNs trained on cyclic datasets. The correlation metric in LSTM units captures phase-alignments between input and output signals, enabling analysis of internal kinetics. Decomposing LSTM networks into individual cell components allows for analysis based on response characterization metrics, creating two scales of dynamic interpretability. The analysis focuses on creating two scales of dynamic interpretability within LSTM networks. It involves identifying individual cells exhibiting fast vs slow responses, quantifying their contribution to the system, interpreting phase-shift and alignment properties, and analyzing the effect of network capacity on overall dynamics. This approach serves as both a deployment and debugging tool to pinpoint undesired dynamics within the network. Response characterization techniques can be applied at both the individual cell and aggregate network scale. In this study, full distributions over individual network components are analyzed to understand network dynamics at a global scale. Results of LSTM training on various sequential datasets are presented, showcasing the applicability of the algorithms to a wide range of temporal sequence problems. Our algorithms demonstrate scalability in handling deeper network structures and are applied to various temporal sequence problems. By analyzing LSTM networks on sequential datasets, we identify cells contributing significantly to decision-making. LSTM cells are trained on sequential MNIST data, with inputs as pixel sequences and outputs as digit classifications. Individual cells are isolated to compute their responses, aiding in understanding network dynamics. The interpretation of LSTM cell behavior is visually demonstrated in FIG1, showing fast and slow phases of neuron activation in response to input sequences. Fast-cells react immediately to fast-input dynamics, while slow-cells respond slightly later. The delta-response distribution indicates a 50% ratio of inhibitory and excitatory dynamics, confirmed by input-output correlation criteria. Neurons exhibit antagonistic behavior to sine-wave inputs, highlighting network dynamics. The LSTM cells in the network showed a linear transformation with some cells shifting phases faster than their inputs. The amplitude distribution indicated varying amplitudes, with high amplitudes contributing more to the network's decision-making. The study also explored the generalization of these effects on different datasets. The study analyzed the dynamics of different networks (seq-MNIST, Stock-Net, CO2-Net, Protein-Net) with a network size of 128. Protein-Net exhibited the fastest dynamics, while Stock-Net and CO2-Net showed similar settling time profiles. Sine frequency remained constant across all networks. The delta response and correlation metrics indicate the distribution of inhibitory and excitatory behavior in individual cells within the network. Neurons in all networks maintain a rate of 44% excitatory and 56% inhibitory dynamics, with high absolute amplitude neurons being significant contributors to the output's decision. Most neurons exhibit a low absolute delta-response value, except for MNIST, indicating cells with equivalent influence on output accuracy. Sine-amplitude remains constant for most neurons in Stock and CO2-Nets. The distribution of inhibitory and excitatory behavior in individual cells within the network is indicated by delta response and correlation metrics. Neurons in all networks maintain a rate of 44% excitatory and 56% inhibitory dynamics, with high absolute amplitude neurons being significant contributors to the output's decision. Most neurons exhibit a low absolute delta-response value, except for MNIST, indicating cells with equivalent influence on output accuracy. Sine-amplitude remains constant for most neurons in Stock and CO2-Nets, while for seq-MNIST net and Protein-net, there is a gradually increasing trend with weak values. Individual cell-ablation analysis on LSTM networks was performed to evaluate the impact on output accuracy and performance. Neurons with higher sine amplitude in CO2-Net tend to disrupt the output more. Neurons with higher sine amplitude disrupt the output more, while those with high absolute delta response values are significant in output prediction. In seq-MNIST-Net, neurons with high absolute delta response or sine amplitude reduce output accuracy. Protein-Net neurons contribute equivalently to output accuracy. Stock-Net shows weak values in these metrics, with some outliers predicting dominant neurons. Ablation analysis confirms constant mean-misclassification error rate for all neural ablations. In Fig. 4F, outliers at the tails of the distribution predict dominant neurons, particularly neurons 120 to 128. Ablation experiments in Fig. 4E show higher impact on output for neurons 1 to 40 and 100 to 128. Increasing LSTM network capacity results in different learned dynamics. Increasing LSTM network capacity results in varied learned dynamics, with response amplitude decaying proportionally to the number of LSTM cells. Settling time distribution remains constant across different network capacities. The network's ability to learn time delay constants is consistent across different capacities. Protein-Net shows unique behavior with increased capacity, leading to overfitting and rapid settling time decay. A method for response characterization in LSTM networks was proposed to predict cell-contributions at both cell and network levels. The study proposed a method for response characterization in LSTM networks to identify cell contributions at both cell and network levels. The methodology establishes a novel building block for interpreting LSTM networks, demonstrating that practical sub-regions of dynamics can be reached by response metrics. The algorithm has been open-sourced to encourage further exploration of LSTM cell dynamics and model kinetics. Future work aims to extend the approach to more data modalities and analyze the training phase of LSTMs for interpreting learning dynamics."
}