{
    "title": "HkgGWM3som",
    "content": "Adversarial examples are inputs that cause a model to make mistakes, differing from an oracle's output. These attacks are well-studied in classification and computer vision, but in natural language processing, defining successful attacks varies. A unified model for NLP tasks is proposed, introducing the concept of adversarial gain to measure output changes relative to input perturbations. This model can be applied to different feature spaces and distance conditions for attack or defense strategies. The notion of adversarial gain is proposed as a way to evaluate adversaries and defenses in the context of adversarial examples. It is rooted in stability and manifold theory, and can be applied across different feature spaces and distance conditions. The adversarial gain is defined as a measure of perturbation in a model's output relative to changes in the input, based on distance metrics and feature transformations. It aims to limit the impact of adversarial examples on the model's response, similar to the concept of stability in L2 incrementally stable systems. The adversarial gain aims to limit model response perturbation based on worst-case adversarial inputs relative to initial conditions. Various problems emphasize stability in different distance metrics and feature spaces, leaving the definition broad. The adversarial gain aims to limit model response perturbation based on worst-case adversarial inputs relative to initial conditions. The text discusses using the GigaWord dataset for text summarization with specific parameters and attack vectors. The text discusses using InferSent embeddings and cosine distance to measure distance in a feature space for adversarial examples. It shows how adversarial gain can give the model leeway in cases where input meaning changes significantly, indicating attack effectiveness. The text discusses adversarial examples for sentiment classification using various techniques such as modifying word embeddings and flipping characters/words in a sentence to change class confidence. Different kinds of attacks, including black-box attacks, are explored for their effectiveness. The text reviews existing works on adversarial attacks, focusing on human perception and the ability to discern attacks from original text. It highlights the ability to change a network's prediction with imperceptible perturbations, leading to misclassification of slightly different examples. In examining various works on adversarial attacks, it has been shown that meaning preservation is not guaranteed. Prior studies have used samples from generated attacks to assess meaning preservation, but not systematically. Some examples show that negation of phrases can alter meaning, highlighting the importance of evaluating meaning preservation using a defined embedding space and cosine distance. This evaluation is crucial in understanding how attacks can change meaning away from its original context. The evaluation criteria consider changing meaning away from the original context. Text-based attacks were examined, focusing on attacks with open-source code availability. Two specific attacks were selected: Seq2Sick for text summarization and word-level sentiment classification attack. The code used for these attacks can be found at the provided link. The dataset used is the same as in a previous study. The dataset used for text-based attacks includes neutral labels and removes a similarity requirement on replacement words to generate attacks for all samples. This approach allows for the addition of words by replacing padding characters, expanding the set of possible attacks beyond previous restrictions."
}