{
    "title": "S1lIMn05F7",
    "content": "Deep neural networks excel in image classification tasks but are vulnerable to adversarial attacks. Adversarial examples can manipulate neural networks into providing incorrect outputs. A new defense mechanism using generative adversarial networks (GAN) is proposed in this paper to combat black box attacks effectively. The paper proposes a defense mechanism using generative adversarial networks (GAN) to combat black box attacks effectively, on par with state-of-art methods like ensemble adversarial training. Deep neural networks have been used for various tasks like image classification, speech recognition, and playing video games. Adversarial attacks on convolutional neural networks have led to the development of methods like JSMA, PGD, and C&W's attack. There are white box attacks where attackers have complete knowledge of the target network, and black box attacks where they have limited information. Defensive methods have been proposed to counter adversarial attacks. Various defensive methods have been proposed to mitigate the effect of adversarial examples, including adversarial training, defensive distillation, randomization at inference time, and thermometer encoding. This paper introduces a defensive method based on generative adversarial networks (GANs) that generates adversarial noise to misclassify input images. The approach uses discriminative and generative networks to create adversarial noise for misclassifying input images. The discriminative network classifies clean and adversarial examples, while the generative network generates perturbations to fool the discriminative network. This method shows robustness against black box attacks and performs similarly to state-of-art defense methods in experiments. The paper introduces a novel approach using discriminative and generative networks for creating adversarial noise to misclassify input images. It explores the joint training of these networks and the effectiveness of different generative networks in attacking a trained discriminative network. The rest of the paper discusses related works, presents the defensive method in detail, showcases experimental results, and concludes with a review of attack and defense methods in neural network training. The paper discusses methods for generating adversarial examples in neural networks, including the use of a neural network model trained for classification. Different optimization techniques such as box-constrained L-BFGS and the fast gradient sign method are introduced to find small perturbations that cause misclassification. The projected gradient descent method is highlighted as an iterative version of the fast gradient sign method. The strongest attack using first-order gradient information is the Projected Gradient Descent (PGD). BID25 introduced a Jacobian-based saliency-map attack model for generating adversarial examples with minimal pixel changes. BID21 demonstrated a universal small image perturbation that fools all natural images. BID25 also showcased the first black-box attacks against neural network classifiers. Various defensive methods, such as distillation and foveation-based mechanisms, have been proposed to mitigate the impact of adversarial examples. Adversarial training was first proposed by BID29. Adversarial training, proposed by BID29, involves training models with original and perturbed images to reduce the impact of adversarial examples. Methods like FGS and PGD are used to generate adversarial perturbations. Cascade adversarial training, as presented in BID22, combines adversarial examples from defended networks with those being trained. Recent works explore using GANs for generating and defending against adversarial examples, such as BID27 and BID10 using GANs to learn input distribution manifold for defense. Our approach involves learning the manifold of input distribution with GAN for defense, projecting input examples onto this manifold before classification to filter out adversarial noise. Unlike BID1 and BID32, we co-train the discriminative network with the adversarial generative network in a minimax game instead of using a fixed discriminative network. In contrast to previous approaches, our method involves training the discriminative network with the adversarial generative network in a minimax game to filter out adversarial noise. The generative network aims to model a distribution of unlabeled training examples in generative adversarial networks (GAN). The generative network transforms a random input vector into an output similar to training examples, while the discriminative network distinguishes real examples from generated ones. Both networks are trained jointly with gradient descent, aiming for the generative samples to be indistinguishable from real data. The approach involves using GAN to generate adversarial noise for training the discriminative model. This method differs from previous approaches by training against a dynamic generative noise network. In this work, a noise network is used to train robust discriminative neural networks by generating adversarial noise. The additive noise \u2206x is modeled as G(x), where G is a generative neural network that generates instance-specific noise based on the input x. Unlike white box attack methods, G does not need to know the parameters of the discriminative network it is attacking. G can take in inputs like a Gaussian random vector or the class label to generate adversarial noise. The text discusses a minimax game between a discriminator network D and a generator network G, focusing on perturbations based on the \u221e norm. The generator network can add noise to inputs using a tanh layer for normalization. Different norms can be accommodated by adjusting the normalization layers. The text explains the tradeoff between minimizing risk on clean examples and adversarial examples under maximum perturbation. It discusses the use of a neural network as an adversary to model adversarial noise, with a focus on optimizing for risk under white box attacks. In defending against black box attacks, the adversarial noise is constrained not to depend on the discriminator network parameters. The generative network parameter is shared across all examples, aiming to create powerful attacks for the discriminator to train against. Previous work has shown that there are effective classes of attacks that can be launched against trained classifiers. Unlike traditional GANs, both the discriminative and generative networks are of interest in this setting. The paper focuses on stabilizing GAN training using gradient regularization. By minimizing a regularized objective, the gradient norm is controlled to prevent it from growing excessively during updates. This approach aims to improve the stability and convergence of GAN training. The paper discusses stabilizing GAN training by controlling the gradient norm to prevent excessive growth during updates. This helps improve stability and convergence. The update process involves using SGD with a step size and approximating the Hessian-vector product for faster computation. The update process for the generative network in GAN training involves setting the step size to capture the curvature at the scale of the gradient ascent algorithm. Gradient regularization is not added for the generative network parameters, as it is found that regularization for the discriminative network is sufficient for stabilization. Training both networks from scratch with random weight initializations is effective in reaching good saddle point solutions without the need for pre-training. In GAN training, the discriminative networks can overpower the generative network if simultaneous parameter updates are performed. To address this, multiple gradient steps are taken on the generative network for each update of the discriminative network, allowing the generative network to learn to map inputs to adversarial noises more effectively. This strategy aims to make the generative network more powerful so that the discriminative network has a strong adversary to train against. In experiments, 5 gradient steps are taken on each mini-batch. The tradeoff parameter \u03bb is fixed at 1, and the gradient regularization parameter \u03b3 is set at 0.01. The adversarial network approach is implemented using Tensorflow BID0 on machines with 4 GTX1080 Ti GPUs. Attacks focus on fast gradient sign (FGS) and projected gradient descent (PGD) methods. The FGS attack computes the adversarial image by projecting onto the feasible range of rescaled pixel values. For the PGD attack, fast gradient sign attack is iterated multiple times with projection, using random initialization near the starting point neighborhood. Parameters include a uniform random vector, step size, and number of PGD steps. Inputs for MNIST are black and white images of digits scaled between 0 and 1, rescaled to the range of [-1,1]. Perturbations are studied at = 0.3. Implementation details are available at the provided GitHub link. In previous work, perturbations of = 0.3 were studied using a simple convolutional neural network similar to LeNet5 as discriminator networks. An encoder-decoder network was used for the generator. Training methods included SGD with specific learning rates and momentum, batch size of 64, and 200k iterations for discriminative networks. Generative network used fixed learning rate and weight decay. Running more updates on the generative network improved the robustness of the discriminator. Running 5 updates on the generative network improves the discriminator's robustness. Adversarial training with PGD performs best under white box attacks, with accuracies above 90%. The PGD model shows a small drop in accuracy on clean examples compared to the standard model. Black box attack accuracies are generated using FGS and PGD attacks on surrogate models A', B', and C'. The study compares models A', B', and C' trained with different random seeds to their standard counterparts. Black box attacks are most effective on models trained with the same method. Adversarial PGD outperforms adversarial network on white box attacks but performs similarly on black box attacks. Adversarial examples from PGD and adversarial networks do not transfer well to the undefended standard model. The undefended model maintains accuracies between 85-95%. The Street View House Number data uses original training set augmented with 80k extra images. No preprocessing is done on the images except scaling to [-1,1]. The study focuses on perturbations of size 0.05 for white box and black box attacks on SVHN images. They use discriminative networks adapted to 32x32 images and a generator based on ResNet. Different optimization techniques are employed for discriminative and generative networks, with specific parameters for each. The white box attack accuracies of the models are shown in Table 2. Adversarial PGD performs best against PGD attacks. The adversarial network approach has the best accuracies on clean data and against FGS attacks, and also improved accuracies against PGD over standard training. Adversarial examples generated from the adversarial PGD model have the strongest attack power across all models. In the experiment, adversarial examples are most effective against models trained in the same way. CIFAR10 inputs are scaled to [-1,1] and augmented. Perturbations of size 8/256 are studied. Discriminative networks are trained with batch size 64 and learning rate \u03b7 D = 0.1 for 100k iterations. Generative network uses Adam with learning rate \u03b7 G = 0.002. Weight decay is 1E-4 for standard training and 1E-5 for adversarial networks. White box accuracies of different models under attack are shown in TAB1. The PGD model has the best accuracy under PGD. Our adversarial model performs well against FGS and PGD attacks, maintaining accuracy on clean examples. It outperforms the standard model in adversarial scenarios. However, it suffers from lower baseline accuracy on clean data. The wider version of ResNets with higher accuracies were tested on CIFAR10 and CIFAR100, showing similar results. Ensemble adversarial training was compared on the datasets, using adversarial examples to improve model performance. The quality of solutions depends on the type of adversarial examples included. The study compared ensemble adversarial training with an adversarial networks approach on MNIST, SVHN, and CIFAR10 datasets. Adversarial networks performed better on white box attacks and all black box attacks across different training methods. The study compared ensemble adversarial training with adversarial networks on MNIST, SVHN, and CIFAR10 datasets. Adversarial networks showed better performance on white box attacks, while black box accuracies varied depending on the dataset and type of attacks used. The effectiveness of training against a static set of adversaries versus a dynamically adjusting adversary remains inconclusive and requires further research. The study explores the impact of generative network capacity on saddle point solutions and their use as adversarial attack methods. Different architectures (G1, G2, G3) are tested, including narrow and wide autoencoder networks attacking an undefended discriminator. Wide autoencoder proves more effective in attacking the discriminator. The wide autoencoder is more powerful than the narrow autoencoder in attacking undefended discriminator networks. It performs well as a white-box attack method, closely matching PGD's attack power on undefended models. Additionally, as a black-box attack method, it outperforms PGD on certain models. However, defended models trained with PGD show resilience against these attacks. Generator networks using random noise inputs are effective against undefended models, reducing accuracies by over 30%. After training, G(y) acts as a set of class conditional filters for attacking models. Experiments attacking models trained with adversarial PGD were conducted using generative networks. Results are in the Appendix. Different generative networks were co-trained with a discriminative network on CI-FAR10, showing similar performance in white box and black box attacks despite varying attack powers. The similarity in decoder portions may explain their convergence to similar quality saddle points. Adversarial PGD training works best on white box attacks, but there is a tradeoff between accuracies on clean data and adversarial examples due to finite model capacity. Training for standard accuracy and training for adversarial accuracy are two different problems. Adversarial examples generated from PGD are difficult to train against, making adversarial PGD training disadvantaged in black box attack situations compared to other methods like ensemble adversarial training. Defending against black box attacks involves hiding the training method to prevent adversarial examples. The transferability of adversarial examples and the impact of network architecture on solution quality are still not well understood. Training dynamics like step size and iterations have a significant effect on solution quality at saddle points. The impact of network architecture on solution quality at saddle points is significant. Exploring different generative network architectures could lead to varied saddle points. There may be connected flat regions in GAN minima similar to neural network loss landscapes. Training dynamics can influence GAN solutions and their robustness properties. The approach can be extended with multiple networks and ensemble adversarial training. The study proposes an adversarial network approach for robust discriminative neural networks against adversarial noise. Future work includes extending experiments to ImageNet and exploring architecture choices for generative and discriminative networks. Generative networks in the study use encoder-decoder networks G0 and G1, and decoder networks G2 and G3 with different inputs. The networks are parameterized by a factor k for the number of filters. Default values are k = 64 and k = 16 for networks using labels. Additional results are presented for CIFAR100 and Wide ResNet on CIFAR10. The study explores adversarial network approach for robust neural networks against adversarial noise. Experiments are conducted on larger capacity models, showing increased accuracies overall. Despite existing accuracy gaps, the adversarial network approach also shows a small gap compared to standard training. The study explores adversarial network approach for robust neural networks against adversarial noise, showing increased accuracies overall on larger capacity models. The models are weakest against attacks trained with the same method but with a different random seed. Our adversarial network approach performs well across different attacks, even though it may not always be the winner for each individual attack. Additional experiments are conducted using different generative networks to attack networks trained with adversarial PGD and our adversarial networks approach. The study explores adversarial network approach for robust neural networks against adversarial noise, showing increased accuracies overall on larger capacity models. Various generative networks were tested in attacking a network trained with adversarial PGD, with results showing that the adversarial PGD network is very robust. The strongest attack came from a more restrictive generative network using only the label as input, which was successful in transferring to other networks. However, none of the generative networks could learn much from the robust adversarial PGD network in generating adversarial examples. Our adversarial network was found to be less robust than adversarial PGD under white box attack. The autoencoder(64 filters) network, while not as robust as adversarial PGD under white box attack, can still reduce accuracy from over 90% to 53%. It is more robust than the undefended network and transfers well to other networks, reducing accuracy to 46%."
}