{
    "title": "BJB7fkWR-",
    "content": "Many deep reinforcement learning approaches struggle with sharing knowledge between visually distinct games that have the same underlying structure. This paper introduces a new method to learn game state embeddings regardless of visual differences, using techniques from multi-task learning and domain adaptation to place diverse game states on a shared embedding space. Games are commonly used to experiment with new approaches in AI research, providing a variety of problem scenarios. Initially, board games like backgammon and chess were used, but now video games offer diverse environments for AI benchmarking. These systems utilize neural networks trained on raw pixel values to interpret game states and learn optimal strategies. However, the sensitivity to visual changes limits knowledge transfer between visually distinct games. The use of visuals in games limits knowledge transfer between visually distinct games, leading networks to learn each game individually without overlap. Learning representations is crucial in AI research, with applications like word embeddings and style transfer. Word embeddings involve predicting word context and using hidden layer outputs to represent words. In AI research, word embeddings involve using hidden layer outputs to represent words based on their context. This knowledge transfer allows for solving complex tasks without retraining from raw input. The proposed architecture in the paper improves knowledge representation across visually distinct tasks by learning independent representations that can be shared between them. The paper presents a method for sharing knowledge between visually distinct games using domain adaptation techniques in reinforcement learning. It outlines the training environment, desired outcomes, resulting representations, and future work to extend the approach. The curr_chunk discusses the significance of using games as benchmarks for AI, highlighting their ability to test high-level reasoning and planning. Games like Deep Blue and AlphaGo have showcased advancements in AI. Games also provide a platform for agents to process experiences quickly and learn efficiently. The curr_chunk discusses competitions and projects using game engines to teach AI, such as the General Game Playing Competition and Project Malmo using Minecraft. It also mentions the General Video Game Playing Competition and OpenArena for training AI across various tasks. The curr_chunk discusses using the ID Tech 3 engine for AI training in tasks like path finding and laser tag. It also explores neural evolution and hierarchical approaches in FPS game AI design. Reinforcement learning is highlighted as a method for training agents through rewards to improve their actions in environments. Temporal difference (TD) is mentioned as a technique in this area. Temporal difference (TD) is a key technique in reinforcement learning, allowing for prediction of future rewards through experience. Actor-critic methods utilize TD, with the actor learning a policy and the critic evaluating the TD error to adjust action probabilities. This approach aims to maximize future rewards by learning from past experiences. Actor critic methods in reinforcement learning aim to reduce the probability of choosing certain actions based on past experiences. They require minimal computation to select actions and can learn explicitly stochastic policies. For example, they could learn the optimal policy for games like rock, paper, scissors. Deep Q networks (DQN) and Asynchronous Advantage Actor-Critic networks have advanced deep reinforcement learning in playing games from screen buffer data. A3C combines actor-critic methods with deep reinforcement learning for optimal decision-making. In our experiments, we chose to use A3C networks, which combine actor-critic methods with deep reinforcement learning. A key aspect of A3C is that the value and policy are calculated by separate networks, but they can share lower level layers. The policy is learnt online, without the need for an explore vs exploit parameter. A3C uses tricks like subtracting entropy from the error rate to encourage exploration and prevent convergence on suboptimal policies. A3C is an asynchronous method that prevents early convergence on suboptimal policies by allowing each thread to learn from its own set of experiences. It can train on CPUs without needing GPUs for acceptable training times, eliminating the need for network freezing and tricks used in DQN. Multiple threads update the global network with different experiences, enhancing learning efficiency. A3C outperforms DQN in Atari 2600 games by updating the global network with various experiences. UNREAL networks improve feature extraction through auxiliary tasks. Multi-task learning shares knowledge by using shared layers for different tasks, resulting in versatile data representations. Multi-task learning (MTL) involves sharing extracted features among tasks to leverage learned features. MTL in deep reinforcement learning includes approaches like using DQN for navigation tasks in the Malmo environment, switching tasks after each training episode to align embeddings across similar tasks. Domain adaption is an approach used to align embeddings for tasks with differing data sets but the same underlying structure, such as in image datasets like the Office dataset. It is also applied in natural language processing, like sentiment analysis across various domains. Originally, a hard-coded loss was used for these tasks, but more recent approaches have evolved. In recent approaches, adversarial networks are used to produce a loss for domain adaption embedding spaces. The Office dataset demonstrates supervised learning of real world representations, dealing with varying image quality and lighting conditions. The research sets the foundation for improving representations in deep reinforcement learning agents across visually distinct games. Each game results in separate embedding spaces in multi-task learning. Domain adaption aims to create shared embedding spaces from different raw data. Domain adaption involves creating shared embedding spaces from diverse datasets to enable knowledge transfer between visually distinct games. By developing a neural network system capable of playing multiple visually distinct games with a common structure, it becomes possible to share knowledge across different data sources. This approach leads to more generalized AI systems and allows for more efficient learning from new datasets. Our approach aims to learn new datasets efficiently by using deep reinforcement learning systems to play games from raw pixel values and a reward function. We focus on learning underlying representations and utilize the A3C algorithm, known for its success in various games. By altering visual representations while keeping game structures consistent, we aim to compare our method with current domain adaption approaches and basic A3C training. Our approach utilizes deep reinforcement learning to train on different visual representations of the same game in order to create a shared embedding manifold. The experiment involves using the game GridWorld with three distinct renderers to render the game in different ways. The experiment involved training on different visual representations of the same game using deep reinforcement learning. Three renderers were used in the game GridWorld to render the player, pickup, and goal in different ways on a 4x4 grid. The agent received rewards for collecting the pickup and reaching the goal, with images produced by the renderers being 40x40 pixels in size. The reward function focused on positive actions towards the goal to speed up convergence to a good policy. The experiment involved training on different visual representations of the same game using deep reinforcement learning. The goal is to have the system learn to play games and separate the game state from the visual representation. The approach compares to two other methods, with a shared upper layer architecture for multiple games. Input images are 8bit greyscale, 40x40 pixels, with separate convolutional layers for each game. The neural network architecture for the games consists of layers with different kernel sizes and step sizes. The output includes a probability distribution for actions and a value estimation used for training. Error gradients are calculated using a formula involving the policy network output, actual reward, estimated reward, entropy, and regularization term. The action selection is influenced by the prediction error in the state value. The neural network architecture includes a network for predicting state values and another for classifying renderers. The architecture involves optimizing an adversary network to predict which renderer produced a game state embedding. The network consists of input and hidden layers, with a softmax output layer during training. Cross entropy loss is minimized to improve classification, while convolutional layers are updated to remove renderer-specific data. The neural network architecture involves optimizing an adversary network to predict the renderer of a game state embedding. Convolutional layers are updated to remove renderer-specific data, with a penalty added to embeddings during training to enforce distance from global distributions. The loss for the convolutional layers in the training batch is optimized using a combination of A3C loss, discounted embedding loss, and discounted adversarial networks loss. The adversarial networks loss is subtracted from the convolutional layers loss to maximize error. The system is expected to have good generalizability due to the A3C reinforcement learning base. A relatively small \u03bb A is used when calculating gradients for the convolutional layers to allow for games with different underlying similarities. The network with fully shared parameters was unable to reach the same performance as the network with separate convolutional layers per renderer. The fully shared network only reached 10, while our network achieved close to optimal performance in around 3 million examples. The fully shared network did not perform as well as the network with separate convolutional layers per renderer, only reaching 10 average excess moves per game. Results were consistent across multiple runs, with initial performance affected by random weight initialization. Performance of both networks decreased in early training stages, possibly due to untrained adversarial network influence. Overlap between renderers is visible in Figure 6. Our system better aligns separate renderers with the same overall structure, while the fully shared parameter version shows differing shapes and angles in the embeddings. The network with separate convolutional layers successfully aligns embeddings and encodes information about optimal moves and game states, including the presence of pickups. This emergent property was not imposed during training but was observed in the network's results. Our system can effectively separate game states based on different tasks, such as pickups and heading towards the goal, using learned embeddings. The embeddings capture essential information about gameplay regardless of visual differences in the game renderings. This is achieved through deep reinforcement learning with adversarial networks, allowing for successful domain adaptations. The network can handle visually diverse inputs better than a network with fully shared parameters. Using adversarial networks with separate convolutional layers, we can create shared embedding spaces for visually distinct inputs in games. This approach improves learning efficiency on new visually distinct tasks and can handle diverse inputs better. Future plans include utilizing this knowledge to enhance learning on new games with different complexities and reducing task similarities. An experiment using 3 games from the Arcade Learning Environment is proposed for reinforcement learning. The curr_chunk discusses the use of the Arcade Learning Environment (ALE) for reinforcement learning on Atari 2600 games like Zaxxon, Assault, Beam Rider, and Space Invaders. These games involve moving an entity at the bottom of the screen to shoot upwards and destroy entities from the top. The goal is to transfer knowledge between deep neural network systems, regardless of their architectures, to improve learning efficiency."
}