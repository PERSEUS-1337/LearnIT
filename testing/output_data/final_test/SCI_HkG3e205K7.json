{
    "title": "HkG3e205K7",
    "content": "Deep latent variable models have gained popularity with scalable learning algorithms. The IWAE bound introduced by Burda et al. is tighter with more samples, but the typical gradient estimator performs poorly with increased samples. Roeder et al. proposed an improved gradient estimator, but it is biased, which can be efficiently estimated with the DReG estimator. The DReG estimator improves training techniques for latent variable models by reducing variance in IWAE, RWS, and JVI gradients. It enhances performance on various modeling tasks for deep generative models with latent variables. The combination of nonlinear function approximators and probabilistic approaches in expressive models can capture complex distributions. To address intractability in marginalizing latent variables, a common approach is to maximize a tractable lower bound like the variational evidence lower bound (ELBO). A factorized variational family is often used for tractability, but it can lead to overly simplistic models. Introducing a multi-sample bound, IWAE, can provide a tighter bound on the likelihood, improving training techniques for latent variable models. The IWAE bound provides a tighter bound on the likelihood, improving training techniques for latent variable models. BID31 proposed a lower-variance gradient estimator for the IWAE bound, which was speculated to be unbiased but was proven to be biased. A new unbiased estimator, the IWAE doubly reparameterized gradient (DReG) estimator, is introduced as a computationally efficient replacement that does not suffer from diminishing signal-to-noise ratio as the number of samples increases. In this work, DReG estimators are derived for IWAE, RWS, and JVI, showing improved scaling with sample size. The estimators are evaluated on various tasks, demonstrating significant variance reduction and improved performance. The goal is to learn a latent variable generative model with intractable marginal likelihood. The ELBO is maximized using a variational distribution q(z|x) in an amortized inference setting. The importance weighted autoencoder (IWAE) bound improves model expressiveness by reducing the coupling with z 1:K \u223c q\u03c6(z|x). The IWAE bound converges to log p\u03b8(x) as K \u2192 \u221e under certain conditions. The IWAE bound estimator is used for gradient estimation, with the signal-to-noise ratio deteriorating as K increases. This affects the performance of learned models on practical problems, especially for model parameters (\u03b8). The IWAE bound converges to log p\u03b8(x) as K approaches infinity. The IWAE bound converges to log p\u03b8(x) as K increases, regardless of q\u03c6. The signal-to-noise ratio of the inference network gradient estimator may not necessarily decrease as K \u2192 \u221e, as the variance of the gradient estimator can decrease faster. This motivates the search for lower variance gradient estimators for \u03c6. The STL estimator introduces bias when K > 1. By efficiently estimating the first term in Eq. 3 with the reparameterization trick, we can focus on one of the K terms. The inner expectation resembles a REINFORCE gradient term, where we interpret wi j wj as the \"reward\". This leads to a well-known equivalence between the REINFORCE gradient and the reparameterization trick gradient. The reparameterization gradient estimator has lower variance than the REINFORCE gradient estimator as it directly uses the derivative of f. When z i is not reparameterizable, a control variate can be used. The algorithm using the single sample Monte Carlo estimator simplifies the terms and cancels out unnecessary ones. The IWAE doubly reparameterized gradient estimator (IWAE-DReG) is introduced for the inference network gradient, showing improved performance compared to the standard IWAE gradient estimator. The estimator exhibits O( \u221a K) scaling behavior for both generation and inference network gradients. Alternative training algorithms for deep generative models are reviewed, including the RWS method introduced by Bornschein & Bengio in 2014. The RWS method introduced in 2014 uses importance sampling for latent variable models. It approximates the gradient of the log marginal likelihood with a self-normalized importance sampling estimator. The RWS update maximizes the IWAE lower bound for \u03b8 and optimizes a separate objective for the inference network. The wake update minimizes the KL divergence from p \u03b8 (z|x) to q \u03c6 (z|x). The wake update of the inference network approximates the intractable expectation using self-normalized importance sampling with z i \u223c q \u03c6 (z i |x). This update avoids diminishing SNR as K increases but may lead to instability or divergence. The algorithm RWS-DReG utilizes a single sample Monte Carlo estimator for the wake update. The inference network gradient estimator from BID31 combines the IWAE gradient estimator and the wake update, showing promising results for further exploration. The algorithm DReG(\u03b1) combines IWAE-DReG and RWS-DReG using convex combinations. BID27 introduces Jackknife Variational Inference (JVI) as a family of estimators that trade bias for variance. JVI shows reduced bias compared to IWAE, leading to better log marginal likelihood estimates with fewer samples. The JVI estimator is a linear combination of K and K \u2212 1 sample IWAE estimators, allowing the use of the doubly reparameterized gradient estimator. Previous work introduced a generalized framework of Monte Carlo objectives (MCO) and showed that the tightness of an MCO is related to the variance of the underlying estimator of the marginal likelihood. The IWAE bound has issues with gradient estimators for multi-sample lower bounds, leading to degraded performance with large numbers of samples. RWS BID4 does not suffer from this issue and can outperform models trained with the IWAE bound. Understanding gradient estimators of tighter bounds is crucial. Wake-sleep is an alternative approach for fitting deep generative models, applied to generative modeling of images. The DReG estimators are evaluated by measuring variance and signal-to-noise ratio of gradient estimators on various tasks including generative modeling of images. The generative model with specific distributions is considered, and gradient estimators' bias and variance are analyzed. In evaluating DReG estimators, bias and variance of gradient estimators are analyzed using signal-to-noise ratio (SNR). The bias of gradient estimators relative to the expected value is computed, highlighting the importance of considering additional evaluation measures. As K increases, SNR of IWAE-DReG estimator increases while SNR of standard gradient estimator of IWAE decreases. Bias is observed in the STL estimator, with statistical tests confirming bias in biased estimators like STL. The unbiased IWAE-DReG estimator shows increasing SNR with K and has the lowest variance among estimators. It is used in training generative models for binarized MNIST digits with a specific architecture. The inference network parameterized a Gaussian distribution over z using two deterministic layers. Dynamically binarized MNIST dataset was used for improved gradient estimators. Models trained with different gradient estimators showed reduced variance and improved performance. Biased estimators STL and RWS-DReG performed best, with RWS-DReG slightly outperforming STL. RWS outperformed IWAE as K increased. Experiments with convex combinations of IWAE-DReG were also conducted. The study experimented with convex combinations of IWAE-DReG and RWS-DReG on a dataset, finding that heavily weighting RWS-DReG led to the best performance. The doubly reparameterized gradient estimator improved test performance on the Omniglot dataset. Additionally, the study explored structured prediction tasks using a conditional latent variable model. The study compared performance by varying the convex combination between IWAE-DReG and RWS-DReG. They evaluated their method by modeling a conditional prior distribution on a binarized MNIST digit. The architecture included two deterministic layers to parameterize Gaussian and Bernoulli distributions. The study introduced doubly reparameterized estimators for updates in IWAE, RWS, and JVI, showing unbiased variance reduction and improved performance. Biased estimators like STL and RWS underperformed compared to unbiased IWAE estimators, with RWS becoming unstable later in training. The convex combination between IWAE-DReG and RWS-DReG was varied to compare performance. The study introduced DReG estimators for IWAE, RWS, and JVI, showing improved performance with same computational cost. A convex combination of IWAE-DReG and RWS-DReG performed best, with weighting being task dependent. Future work aims to automatically adapt weighting based on data, and the IWAE-DReG estimator is surprisingly simple, suggesting potential for general MCOs. The right plot compares performance as the convex combination between IWAEDReG and RWS-DReG is varied. The expected value of the IWAE gradient of the inference network collapses to zero with rate 1/K, while its standard deviation is only shrinking at a rate of 1/ \u221a K, resulting in the SNR of the inference network gradients going to zero at a rate O( DISPLAYFORM0 , worsening with K. The IWAE-DReG estimator's standard deviation scales like K^-3/2, resulting in an overall scaling of O(\u221aK) for the inference network gradient's SNR, improving with K. The SNR of the IWAE-DReG estimator improves similarly in K for both inference and generation networks. The delta method approximation of Var(g(X, Y)) scales like K^-3, with the standard deviation scaling like K^-3/2. Shared parameters between p and q can be utilized in the IWAE and IWAE-DReG frameworks, allowing for adaptation to the shared parameter setting in RWS. The IWAE framework allows for a modified RWS wake update using surrogate objectives for each scenario, facilitating proper gradient updates with respect to \u03b8. The modifier notation for p \u03b8 (x, z i ), q \u03b8 (z i |x), and w i is introduced for clarity, enabling the use of stopped gradients for different variables."
}