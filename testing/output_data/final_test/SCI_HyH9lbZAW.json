{
    "title": "HyH9lbZAW",
    "content": "Recent efforts in combining deep models with probabilistic graphical models have shown promise in providing flexible and interpretable models. A variational message-passing algorithm is proposed for variational inference in these models, making three key contributions. This includes structured inference networks that incorporate the graphical model's structure in variational auto-encoders, establishing conditions for fast amortized inference, and deriving a variational message passing algorithm for efficient natural-gradient inference. This method simplifies and generalizes existing approaches for deep structured models, enabling structured, amortized, and natural-gradient inference simultaneously. Recent work combines deep neural networks (DNNs) and probabilistic graphical models (PGMs) to create powerful and interpretable hybrid models. Efficient algorithms are needed to extract useful structure from data for real-world applications. Deep learning typically uses stochastic-gradient methods like back-propagation for fast inference, while PGMs use different algorithms. The curr_chunk discusses combining deep learning and probabilistic modeling by designing algorithms that merge their strengths, such as message-passing, natural-gradient, and amortized inference. The proposed method in the paper simplifies and generalizes previous work in this area. The proposed method in the paper simplifies and generalizes the method of BID10 by introducing Structured Inference Networks (SIN) that incorporate PGM structure in standard inference networks used in variational auto-encoders (VAE). SINs mimic the structure of generative models using parameters \u03c6, with the main difference being the reversed arrows between y n and x n compared to the model. A variational message-passing algorithm is derived, with messages reducing to stochastic gradients for deep components. The SAN algorithm enables Structured, Amortized, and Natural-gradient updates for deep components and PGM. It simplifies and generalizes the method of BID10, modeling data vectors with local latent vectors using neural networks and a PGM. The code for reproducing results is available at https://github.com/emtiyaz/vmp-for-svae/. The structured variational auto-encoder (SVAE) combines a probabilistic graphical model (PGM) and a neural network (DNN) with parameters \u03b8 NN and \u03b8 PGM. Unlike VAE, SVAE uses a structured prior p(x|\u03b8 PGM) to extract useful data structure, such as a mixture-model prior for grouping data into clusters. This approach, introduced by BID10, offers a more tailored prior distribution compared to the standard Gaussian prior used in VAE. The structured variational auto-encoder (SVAE) combines a probabilistic graphical model (PGM) and a neural network (DNN) with parameters \u03b8 NN and \u03b8 PGM. It uses a mixture-model prior for grouping data into clusters, making it easier to interpret than VAE. The main goal is to approximate the posterior distribution p(x, \u03b8|y) by using an inference network parameterized by DNN. The structured variational auto-encoder (SVAE) uses a DNN to model the encoder distribution, enabling faster training and testing. However, inaccurate predictions may occur when the inference network ignores the structure of the PGM prior, leading to errors in forecasting future observations. Inaccurate predictions may occur in the structured variational auto-encoder (SVAE) when the inference network ignores the prior structure of the probabilistic graphical model (PGM). A proposed solution is to use an inference network with the same structure as the model but replace its edges with neural networks. However, this approach may not be suitable for simple PGM models like linear dynamical systems, as using deep neural networks (DNNs) could lead to a significant increase in parameters, impacting speed and performance. BID10 suggests a method to incorporate the PGM structure in the inference network for SVAE with conditionally-conjugate PGM priors. BID10 proposes a two-stage iterative procedure to incorporate the PGM structure in the inference network for SVAE with conditionally-conjugate PGM priors. The first stage involves optimizing a surrogate lower bound to obtain \u03bb * x, which is then used in the second stage to optimize L(\u03bb * x (\u03b8, \u03c6), \u03b8) iteratively until convergence. The two-stage iterative procedure in BID10 incorporates the PGM structure in the inference network for SVAE with conditionally-conjugate PGM priors. The first stage optimizes a surrogate lower bound to obtain \u03bb * x, used in the second stage to iteratively optimize L(\u03bb * x (\u03b8, \u03c6), \u03b8) until convergence. The method may be challenging to implement and tune due to the implicitly-constrained optimization involved. The method of BID10 for SVAE with conditionally-conjugate PGM priors may be challenging to implement and tune due to various limitations. It requires long VMP updates to reach a local optimum, has restrictions on PGM prior assumptions, and is not applicable to complex PGMs with non-conjugate factors or constrained variables. To address these issues, structured inference networks (SIN) are proposed to simplify and generalize the algorithm of BID10. Structured Inference Networks (SIN) are proposed to simplify and generalize the algorithm of BID10 for VAE with non-conjugate PGM structures. SIN incorporates the PGM structure into the VAE inference network, enabling efficient amortized inference using stochastic gradients. A VMP algorithm is derived for natural-gradient variational inference on the PGM part while retaining efficiency in the DNN part. Factor is an exponential-family distribution with a graph structure similar to the PGM prior. The DNN term provides flexibility, while the PGM term incorporates the model's structure. Parameters include \u03c6 NN for DNN and \u03c6 PGM for PGM. Factors need to satisfy conditions for fast amortized inference: easy evaluation and differentiation of the normalizing constant log Z(\u03c6), and the ability to draw samples from SIN. Additional desirable feature is the ability to compute the gradient of x * (\u03c6) using reparameterization. The ability to compute the gradient of x * (\u03c6) using the reparameterization trick is a necessary feature for amortized inference. When certain conditions are met, a stochastic gradient of the lower bound can be computed similarly to a VAE. The variational lower bound includes terms from a structured PGM prior in the generative model, allowing for amortized inference similar to VAE. The ability to compute the gradient of x * (\u03c6) using the reparameterization trick is crucial for amortized inference. The gradients of Z(\u03c6) and x * (\u03c6) can be cheap or costly depending on the type of PGM, with computations being independent of N for GMM but infeasible for large N in LDS. In this paper, we focus on fast amortized inference for structured inference networks (SIN) that meet specific conditions. When the latent variables in probabilistic graphical models (PGM) are not highly correlated, Bayesian inference is computationally efficient. Examples of SIN meeting these conditions include using conjugate exponential-family distributions and linear dynamical systems as PGM priors. The latent variables in the PGM are modeled using an LDS with parameters A, Q, \u00b5 0, \u03a3 0. The inference network uses a Gaussian DNN factor. The SIN is a conjugate model allowing for efficient computation of marginal likelihood and distributions. The gradient of Z(\u03c6) can also be computed. The SIN model incorporates latent variables from the PGM, such as cluster indicators. The model uses a Gaussian distribution for the DNN part and allows for efficient computation of marginal likelihood and distributions. Sampling from the SIN involves marginalizing x n and sampling from conditional distributions. Our method can handle non-conjugate factors in the generative model, allowing for a tractable structured mean-field approximation using VMP. The gradients are simpler to compute compared to previous methods, making it easier to implement. Our method can handle non-conjugate factors in the generative model by replacing them with their closest conjugate approximations while ensuring the inference network captures the useful structure in the posterior distribution. This is illustrated using a Student's t mixture model to handle outliers in the data. The Student's t-distribution is not conjugate to the multinomial distribution, so we use a GMM factor in the inference network for simplifying inference. In theory, simplifying inference by choosing a simpler form of the inference network can be done even with non-conjugate factors, but the approximation error may be significant. Non-linearity is crucial for extremely non-linear dynamical systems. A VMP algorithm is derived for natural-gradient variational inference for \u03b8 PGM with an exponential-family prior. The algorithm can handle non-conjugate factors in the PGM part without affecting the efficiency of amortized inference on the DNN part. The mean-field approximation for DNN involves computing point estimates for \u03b8 NN and \u03c6 using a method that enables natural-gradient updates even with non-conjugate factors in the PGM. This method performs natural-gradient variational inference through mirror-descent updates with KL divergence. A VMP algorithm is derived based on this approach, starting with the variational lower bound expressed in terms of L. The lower bound for the mean-field approximation is expressed in terms of L SIN. Compute q(x|y, \u03c6) for SIN using an exact expression or VMP. Update \u03bb PGM using natural-gradient step. Mirror-descent update with KL divergence is used for q(\u03b8 PGM |\u03bb PGM ). Mean parameter \u00b5 PGM is denoted for \u03bb PGM. Minimal exponential family allows reparameterization of q. The update enables natural-gradient update for PGMs with both conjugate and non-conjugate factors, using closed-form maximization and stochastic-gradient methods for parameter updates. The update enables natural-gradient update for PGMs with both conjugate and non-conjugate factors, using closed-form maximization and stochastic-gradient methods for parameter updates. The rest of the parameters can be updated using a stochastic-gradient method, with separate computations for the PGM and DNN parts in the SAN algorithm. Our algorithm simplifies implementation by reusing existing software for the PGM and DNN parts. Experiments show that SAN algorithm yields similar results to BID10 method. Results for latent GMM are discussed, with SAN-TMM outperforming SAN-GMM even with 70% outliers. Figure 3 displays results for Pinwheel and Auto datasets. The Auto dataset is used to train models with 10 mixture components. The generative model's samples are shown in point clouds, colored by mixture component probability. True label data samples are also displayed. Our method is simpler and more general than BID10. Comparison is made with GMM and VAE methods. The VAE method (BID12) uses a DNN without clustering latent variables, while the SVAE method (BID10) applies a DNN and mixture model to cluster latent variables. These methods are compared to the SAN algorithm applied to a latent GMM model. All methods use a Normal-Wishart prior over GMM hyperparameters. Two datasets are used: a synthetic Pinwheel dataset and the Auto dataset from the UCI repository, which includes car information and a five-class label indicating the number of cylinders. The study compares different methods for clustering latent variables using DNNs and GMM models on two datasets. Training data is split into 70% for training and the rest for testing. Performance comparisons show that SAN converges faster than SVAE, with both achieving similar results and outperforming GMM. The implementation by BID10 did not perform well. The study compared clustering methods using DNNs and GMM models on two datasets. SAN performed better than SVAE, with both outperforming GMM. BID10's implementation was excluded due to poor performance. Results show SAN can learn meaningful clusters, while VAE struggles to cluster effectively. Our method enables flexible models that are easy to interpret, even with non-conjugate factors in the PGM. In a case study with a latent Student's t-mixture model, our approach outperformed GMM when dealing with outliers. Artificial outliers were added to the dataset, and our method showed improved performance compared to GMM. The experiment compares GMM, SAN on latent GMM, and SAN on latent TMM performance with increasing noise levels. Latent TMM outperforms other methods as noise level increases, even with 70% outliers. A proposed VMP algorithm simplifies BID10's algorithm for models with deep networks and graphical models, enabling structured, amortized, and natural-gradient updates under certain conditions. The paper discusses limitations in extending the method to models with dense correlations in latent variables, such as Gaussian process models. They plan to explore ideas from sparse Gaussian process models in the future. Implementing a message-passing framework that aligns well with deep learning is challenging, and they aim to investigate suitable platforms for integrating these algorithms. In SLDS, discrete variables are introduced and sampled using a Markov chain, with transitions defined for LDS conditioned on these variables. The paper discusses limitations in extending the method to models with dense correlations in latent variables, such as Gaussian process models. They plan to explore ideas from sparse Gaussian process models in the future. Implementing a message-passing framework that aligns well with deep learning is challenging, and they aim to investigate suitable platforms for integrating these algorithms. In SLDS, discrete variables are introduced and sampled using a Markov chain, with transitions defined for LDS conditioned on these variables. The SLDS prior is defined by dynamics with parameters A i and Q i for the i'th indicator, and a structured mean-field approximation is used for sampling. The section provides detailed derivations for the SIN shown in (12), including the normalizing constant Z(\u03c6) and sampling methods. It rearranges the SIN equation and expresses the joint distribution as a multiplication of marginals and conditionals. This simplification leads to the normalizing constant expression and a sampling approach for SIN. The text discusses the derivation of the SIN algorithm, focusing on completing squares and expressing terms as distributions. It simplifies the equation, calculates the normalizing constant, and outlines a sampling method. The algorithm is then applied to a latent LDS in an experiment. In an experiment, the SAN algorithm is applied to a latent LDS. Results show comparable performance to SVAE. Batch learning is used for LDS, while mini-batch updates are used for SVAE and SAN. The neural network architecture includes two hidden layers with tanh activation function. Model performance is measured in terms of mean absolute error for \u03c4-steps ahead prediction. Our method, compared to SVAE and LDS, shows robust performance in predicting ground-truth observations. Generated images from all methods in FIG5 demonstrate our method's ability to recover observations accurately."
}