{
    "title": "BklAyh05YQ",
    "content": "We propose a new method for training neural networks online in a bandit setting by modeling uncertainty only in the last layer of the network. This allows for a balance between exploration and exploitation, leading to better performance compared to other methods. Deploying systems in dynamic environments requires constant model updates with new data to maintain performance. Acquiring labeled data can be expensive, but user feedback can provide cheap alternatives. Learning in a contextual bandit setting involves balancing exploration of uncertain actions with exploiting confident actions for high rewards. Methods like Thompson sampling (TS) and Upper Confidence Bounds (UCB) offer theoretical benefits. Thompson sampling (TS) and Upper Confidence Bounds (UCB) offer ways to balance exploration/exploitation in a bandit setting, but require model uncertainty estimation. Neural networks in bandit settings need good uncertainty estimates, which can be challenging for large models. Estimating uncertainty on the last layer of neural networks, treated as linear models, is a straightforward approach. Bayesian last layer (BLL) methods involve updating only the last layer of a neural network, assuming the rest of the network provides good representations of inputs. This approach is model-agnostic and scalable, but requires regular updates to maintain low regret. Retraining the rest of the network is necessary to quickly incorporate new data and ensure low regret. In this paper, a new retraining objective for Bayesian last layer (BLL) methods is proposed to meet the criteria of quickly incorporating new data into the learned model while preventing the forgetting of previously learned information. The method optimizes the expected log-likelihood of both new and old data by utilizing uncertainty information gathered about the last layer, resulting in a more robust model. The proposed method optimizes the expected log-likelihood of new and old data by utilizing uncertainty information about the last layer. It improves regret on challenging bandit tasks and adapts quickly to new data without forgetting previous information. In this paper, the multiclass contextual bandit problem is studied, where the learner predicts a class label based on received context and receives a binary reward. Each class has an associated vector, and the probability of a class label is modeled using a logistic function and a neural network. The goal is to minimize regret by optimizing the expected log-likelihood of new and old data while utilizing uncertainty information about the last layer. The key to achieving low regret in the multiclass contextual bandit problem is to employ a policy that balances exploration and exploitation. By modeling the uncertainty in each z c at time t with a posterior distribution, exploration strategies like Thompson sampling or UCB can be effectively implemented. This can be done by holding f \u03b8 fixed and performing online Bayesian regression on the outputs of f \u03b8 when it is a neural network, which falls under the category of BLL methods known for their effectiveness in dealing with exploration problems involving neural networks. A scalable method for exploration problems involving neural networks is to update the feature functions efficiently in an online manner. The key question is how to update and learn f \u03b8. Offline training methods can be used for low regret, but for online performance, a training method for f \u03b8 must determine when to update the feature functions. In this paper, the focus is on updating feature functions efficiently in an online manner. Two solutions are discussed: sampling minibatches only from recent data may lead to overfitting, while sampling uniformly from all collected data may result in slow adaptation to new data. Consideration is given to the distribution over the last layer when training feature functions. The focus is on updating feature functions efficiently in an online manner by considering the distribution over the last layer. The goal is to optimize over all values of the last layer weighted by their probability to adapt or retain information when needed. This is achieved by deriving a local approximation that shows the ability to place density on last layers that do well on older data and fit well to new data. During online training, the focus is on optimizing feature functions efficiently by considering the distribution over the last layer. The objective is to maximize the likelihood by approximating it as a convolution between a logistic function and a Gaussian. This approximation allows for a closed form expression, which can be further simplified using a second order Taylor expansion. This method has been previously used and proven effective. The KL term in equation FORMULA3 can be approximated locally with a second Taylor expansion around the current value of \u03b8 = \u03b8 t\u22121. Utilizing properties of KL divergence and equation FORMULA2, the local KL approximation can be written as a ratio between expected data likelihoods given the last layer z distributed under different conditions. The optimization process involves regularizing the incorporation of new data by adjusting the hyperparameter \u03b2. The old data is used to calculate the Fisher information matrix F P, while the new data is used for optimization. This allows for parallel computation of the regularization term during online Bayesian regression. The iterative algorithm studied in this paper combines online Bayesian regression with network retraining. The algorithm alternates between an online phase, where data and posteriors are used to make decisions, and a network retraining phase. The algorithm receives feedback upon pulling the arm and updates the posterior over z. After T rounds, the learner outputs the updated posteriors over z and the data collected in this phase. In the offline/retraining phase, the algorithm takes in the collected data and posteriors over z, retrains the model using a specific method, and outputs the updated posteriors. The algorithm is compared against two other methods in the next section with results from various experiments on different datasets and models. Initial evaluation is done on challenging bandit tasks before moving on to higher dimensional problems. In BID17, higher dimensional problems and models are explored, including Natural Language Processing (NLP) and vision tasks using recurrent and convolutional models. The focus is on achieving good online and offline performance despite data distribution shifts. Details on hyperparameters, experimental setup, and datasets are provided in Appendix 7.4. Experiments are conducted 5 times and reported with mean and standard error. Bandit feedback is used for training the models. Our method involves marginalizing over all values of the last layer during neural network training. During neural network training, different methods are used to handle data shifts. The \"Sample New\" method optimizes for newly collected data only, while \"Sample All\" retrains using both old and new data. \"Full feedback\" involves training models with all labels. When evaluating offline accuracy, results for a model trained on shuffled batch data with full feedback are provided. Training online with marginalization sometimes performs comparably to offline training. Good online performance is confirmed on simpler problems in BID17. Results are presented for the Census, Jester, and Adult datasets, which are used for multiclass classification problems. The Census dataset has 9 classes, while the Adult dataset has 14 classes. The Jester BID5 dataset consists of jokes with user ratings. Each joke is treated as a bandit arm in the bandit problem, where a model recommends one joke based on a user context. The model uses linear regression to predict user ratings for the recommended joke, with the actual user rating as the reward. The study uses a two-layer MLP model to predict user ratings for jokes in the Jester BID5 dataset. Different retraining strategies are compared, with Marginalize and Sample New outperforming Sample All in terms of cumulative regret. Results show that both UCB and Thompson Sampling policies have similar performance, indicating that policies do not significantly impact training mechanisms. In the next section, the study evaluates a new technique on larger and more complex datasets, showing better performance than the Sample New method. The evaluation includes using the Bilateral Multi-Perspective Matching (BiMPM) model for paraphrase identification tasks to assess the algorithm's robustness to shifts in data distribution. To assess the algorithm's robustness to shifts in data distribution, two paraphrase identification datasets were combined: Quora Question Pairs dataset and MSR Paraphrase Corpus. The online training dataset was created by concatenating the MSR training set with a sample of 10,000 examples from the Quora training dataset. The regret values were reported using online algorithms on this dataset, while offline performance was reported on the MSR and Quora test sets. UCB was used as the search strategy for the bandit tasks, which included Multiclass analysis. The model learns to select instances from a pool in a bandit problem formulation. It aims to predict the expected reward for selecting an instance while minimizing regret. The instances are candidate paraphrase pairs, with a reward of 1 for a valid pair and 0 otherwise. The method involves offline retraining after 1,000 online rounds. The method involves retraining every 500 rounds for BiMPM implementation. Marginalize outperforms Sample All and Sample New techniques for multiclass and pool based tasks. Sample New has worse offline accuracy on Quora dataset but better on MSR due to adaptability. Batch train slightly outperforms Marginalize in offline accuracy for binary classification tasks. The method involves retraining every 500 rounds for BiMPM implementation. Marginalize outperforms Sample All and Sample New techniques for multiclass and pool based tasks. When the data distribution changes, Marginalize and Sample New adapt faster than Sample All. Marginalize achieves lower regret and higher offline accuracy for bandit settings. Evaluation using a convolutional neural network on CIFAR-10 dataset shows promising results for image classification tasks. The dataset is divided into images of animals and transportation. Two bandit tasks are analyzed: Multiclass and Pool. In the Multiclass task, each of the 10 classes in CIFAR-10 is assigned one arm for guessing the class. The Pool task involves turning CIFAR-10 into a binary classification task with a pool size of 5. The learner must select images from the pool for classification. For the image classification bandit task, a pool of images is presented to the learner who must select one belonging to the positive class. A reward of 1 is given for correct selections. Data is sorted to simulate domain changes. Results show that Sample New outperforms Sample All in cumulative regret but under-performs in the offline setting. The method used performs well in both cumulative regret and offline settings, but lower than batch train for the multiclass task due to the CNN architecture training requirements. In this paper, a new method for training neural networks in a bandit setting is proposed. The method addresses the challenges of exploration-exploitation by estimating uncertainty only in the last layer, allowing scalability to large models. By optimizing the network over all values of the last layer, the method outperforms traditional batch training methods. The method proposed in this paper optimizes the network over all values of the last layer, outperforming traditional batch training methods. Future work includes investigating more sophisticated methods for determining when to retrain the network and setting the weight of the regularization term automatically. The method proposed in this paper optimizes the network over all values of the last layer, outperforming traditional batch training methods. Future work includes investigating more sophisticated methods for determining when to retrain the network and setting the weight of the regularization term automatically. The derivation of the KL term is approximated locally around \u03b8 t\u22121 with a second order Taylor expansion. The UCB is straightforward when the covariance of the posterior over z is \u03a3. The UCB method optimizes the network over all values of the last layer, outperforming traditional batch training. The hyperparameters for different bandit tasks are presented in tables, with explanations for non-obvious values like Retrain Epochs and Update Frequency. In this section, the architectures used for each task are detailed. The Low dimensional tasks utilize a 2 layer MLP with a hidden layer dimension of 100 and ReLu activation functions. The paraphrase task uses the BiMPM architecture from a previous study. For the image classification task, a convolutional architecture with 64 filters and ReLu activations is employed. The architecture for the image classification task includes two 3x3 convolutional layers with 64 filters and ReLu activations, followed by max pooling and dropout layers. This is then followed by additional convolutional layers with 128 filters, max pooling, dropout, and fully connected layers with tanh activations. Tanh activations are used at the end to address issues with estimating uncertainty."
}