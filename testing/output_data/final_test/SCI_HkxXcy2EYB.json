{
    "title": "HkxXcy2EYB",
    "content": "Variational Autoencoders (VAEs) are powerful latent variable models, but the form of the approximate posterior can limit model expressiveness. The Hierarchical Discrete Variational Autoencoder (HD-VAE) introduces a hierarchy of variational memory layers using Concrete/Gumbel-Softmax relaxation. HD-VAE outperforms the Gaussian baseline on multiple binary image datasets with a limited number of latent variables. However, training very deep HD-VAE remains a challenge due to relaxation bias. Unsupervised learning is effective for leveraging raw unstructured data. Unsupervised learning leverages raw unstructured data through latent variable models like Variational Autoencoders (VAEs). VAEs use neural networks for approximate inference over latent variables, but face limitations with Gaussian priors. Proposed solutions include autoregressive flows and a hierarchy of latent variables to improve model expressiveness. Current state-of-the-art deep learning models are trained on web-scaled datasets, increasing the number of parameters for remarkable results. However, time complexity and GPU memory are limited resources that increase with neural network depth. Large memory layers and discrete variational distributions have been shown to be effective in improving model capacity and reducing computation time. The Hierarchical Discrete Variational Autoencoder (HD-VAE) is introduced as a VAE with a hierarchy of factorized categorical latent variables, trained using Concrete/Gumbel-Softmax, and relying on a conditional prior learned end-to-end. It utilizes a variational distribution parameterized as a large stochastic memory layer, aiming to improve model capacity and reduce computation time. The HD-VAE introduces a variational distribution parameterized as a large stochastic memory layer, outperforming Gaussian-based models on binary image datasets in test log-likelihood. It defines a hierarchical model with factorized latent variables and explores the relaxation bias and its impact on latent variable configuration. The HD-VAE introduces a variational distribution with stochastic memory layers, optimizing the Evidence Lower Bound using the Concrete/GumbelSoftmax relaxation for approximate sampling of categorical variables. The HD-VAE uses a variational distribution with stochastic memory layers and the Concrete/GumbelSoftmax relaxation for approximate sampling of categorical variables. The relaxed samples follow a distribution that converges to the categorical distribution as the temperature parameter approaches zero. The surrogate objective maximized is not guaranteed to be a lower bound of the log likelihood. Relaxation bias is defined as the difference between the original ELBO and the maximized objective. If the function is Lipschitz for the samples, the relaxation bias is minimized. The relaxation bias is defined as the difference between the original ELBO and the maximized objective. For a one layer Ladder Variational Autoencoder, the relaxation bias grows with the number of discrete variables. Empirical results support this property, showing an increase in the relaxation bias with the number of stochastic units. In a study on HD-VAE, memory layers were transformed into a variational distribution, outperforming the Gaussian model across multiple datasets. The experiment showed that using variational memory layers led to a more flexible model than a VAE with a Gaussian prior. The optimization of latent variable models was found to be challenging, with higher KL measured for the discrete model, indicating well-tempered optimization. Increasing the depth of HD-VAE improves log-likelihood, but relaxation bias may increase with more discrete latent variables. Training HD-VAE on Binarized MNIST with different units and depths showed a monotonic increase in relaxation bias with total discrete latent variables. This may explain why HD-VAE with many latent variables is not competitive with Gaussian models. Variational memory layers were introduced as a design in this preliminary research. The design for variational memory layers can be used to build hierarchical discrete VAEs that outperform Gaussian prior VAEs. However, the relaxation bias grows with the number of latent layers, hindering the creation of competitive deep hierarchical models. Future work aims to improve the performance of HD-VAE by harnessing the relaxed-ELBO. Training involves mitigating posterior collapse using the freebits strategy, dropout to prevent overfitting, and linearly decreasing temperature during optimization. The Adamax optimizer is used with specific learning rates for different parameters. The HD-VAE model is trained with a learning rate of 2 \u00b7 10 \u22122 for memory values and a batch size of 128. Despite having many parameters, it is robust to overfitting due to sparse memory updates. Sparse CUDA operations are not utilized, leaving room for memory efficiency improvements. Truncating relaxed samples during training can benefit from sparse optimizations. The average training iteration time and memory usage for a 6-layer LVAE model are shown in Table 3. Table 4 displays the ELBO on binarized MNIST for LVAE models with varying layers and stochastic units. The ELBO on binarized MNIST for a LVAE model with different layers and stochastic units using relaxed and hard samples is reported. The model involves N categorical latent variables with K classes, and the relaxation bias is bounded. The adjusted Evidence Lower Bound for relaxed categorical variables is defined. The adjusted Evidence Lower Bound for relaxed categorical variables (relaxed-ELBO) is a tight lower bound of the ELBO. It is differentiable and may enable automatic control of temperature in \u03ba-Lipschitz neural networks. Designing these networks can be done using Weight Normalization or Spectral Normalization. Handling residual connections and multiple layers of latent variables is challenging, but in the case of a one layer VAE, only the VAE decoder needs to be constrained. In the case of a one layer VAE, the VAE decoder only needs to be \u03ba-Lipschitz constrained. The relaxed-ELBO can be extended to multiple layers of latent variables in the LVAE setting. The function H is introduced for categorical samples, and relaxing expressions during training can reduce gradient variance. The Concrete/GumbelSoftmax defines a proper continuous distribution for relaxed categorical variables. The Concrete/GumbelSoftmax defines a proper continuous distribution for relaxed categorical variables in models like VAE, LVAE, and BIVA. These models involve generative models with continuous latent variables and can be coupled with any variational distribution. The curr_chunk discusses different types of Variational Autoencoders, including Ladder Variational Autoencoder with Skip-Connections (Skip-LVAE) and Bidirectional Variational Autoencoder (BIVA)."
}