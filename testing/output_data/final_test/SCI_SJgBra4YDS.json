{
    "title": "SJgBra4YDS",
    "content": "Deep image prior (DIP) utilizes a deep convolutional network structure as an image prior for various image restoration applications. The proposed approach in this study divides convolution into \"delay-embedding\" and \"transformation\" and introduces a simple image/tensor modeling method called manifold modeling in embedded space (MMES). This method is implemented using a denoising-auto-encoder and multi-way delay-embedding transform for image completion and super-resolution. The MMES method provides competitive results in image/tensor completion and super-resolution compared to DIP. Prior information is crucial for image/tensor restoration, converting optimization problems from ill-posed to well-posed, and providing robustness against noise. Various priors such as low-rank representation, smoothness, sparseness, non-negativity, and statistical independence have been studied in computer science. Total variation (TV) is particularly relevant in today's computer vision problems. In computer vision problems, priors like total variation (TV), low-rank representation, and non-local similarity are commonly used for image modeling. The deep image prior (DIP) is a state-of-the-art unsupervised image restoration method that optimizes an untrained ConvNet to minimize loss between generated and observed images. In computer vision, priors like total variation, low-rank representation, and non-local similarity are used for image modeling. The deep image prior (DIP) optimizes an untrained ConvNet to minimize loss between generated and observed images by resisting \"bad\" solutions and descending towards naturally-looking images. This study aims to explain why ConvNet is essential as an image prior and translate the concept of \"deep image prior\" into simple terms. The convolution operation is divided into \"embedding\" and \"transformation\" for delay/shift-invariant feature capture. The \"transformation\" involves linear and nonlinear operations in a simple convolution process. A simplified network structure includes linear embedding H, nonlinear encoding \u03c6 r, decoding \u03c8 r, and linear backward embedding H \u2020. The encoder-decoder part is a multi-layer perceptron for manifold learning. The proposed network, characterized by Manifold Modeling in Embedded Space (MMES), is a simple multi-layer perceptron sandwiched between forward and backward embedding. Parameters \u03c4 and r correspond to kernel and filter sizes in ConvNet, encoding input image patches into a lower-dimensional space. The hidden tensor L has reduced representation ability compared to input/output images due to its structure. The MMES network is a special type of auto-encoder that uses Hankelization instead of convolution operations. It represents coordinates on the patch-manifold of an image and can be implemented with multiple convolution layers. The encoder-decoder part can be implemented with convolution layers. The study proposes a new image/tensor modeling approach, demonstrates its effectiveness, and relates it to the DIP. The prospect of interpreting the DIP as a \"low-dimensional patch-manifold prior\" is highlighted, building on previous work by Peyre and Osher. Our study utilizes autoencoder to reduce patch-manifold dimension, different from Yokota et al. (2018) who used low-rank tensor modeling. We explore tensor completion, super-resolution, and deconvolution tasks, unlike Yokota et al. (2018) who focused only on tensor completion. Another related work is group sparse representation (GSR) by Zhang et al. (2014a). The proposed MMES is motivated by essential and simple image modeling that can translate ConvNet/DIP. It has connections with ConvNet/DIP such as embedding, non-linear mapping, and training with noise. The deep geometric prior by Williams et al. (2019) utilizes a multi-layer perceptron for shape reconstruction, aiding in understanding DIP from a manifold learning perspective. The deep decoder reconstructs natural images using non-convolutional networks, emphasizing the relationship between convolutional layers in DIP and upsampling layers in the decoder. Unlike DIP, the deep decoder uses an under-parameterized network for image reconstruction. The deep decoder emphasizes the importance of locality in image models, stating that convolution and upsampling layers provide this essential feature. The MMES, which includes a decoder and inverse MDT, is closely related to the deep decoder. The essence of image models lies in their locality, which can be achieved through convolution, upsampling, or delay-embedding. This is why image restoration with deep convolutional networks is effective. The image restoration from a single image using deep convolutional networks has gained attention in zero-shot learning, internal learning, or self-supervised learning. Recent generative models like SinGAN and InGAN have been proposed to learn from a single image by imposing constraints on local patches to be natural. The MMES method also imposes a low-dimensional manifold constraint on local patches, different from adversarial training. This approach is explained starting from the concept of MMES and systematically deriving its structure. The proposed tensor reconstruction method is derived from the concept of MMES, involving linear operators, padding, and Hankelization. The method includes a low-dimensional manifold constraint on local patches for tasks like tensor completion, superresolution, and deconvolution. The proposed method involves a low-dimensional manifold constraint on local patches for tasks like tensor completion, superresolution, and deconvolution. It utilizes an autoencoder to define the manifold in Euclidean space, minimizing the distance between observation and reconstruction. The proposed method involves utilizing multi-layer perceptrons for smooth manifold learning. The optimization algorithm for tensor reconstruction involves redefining a tensor X as an output of a generator and transforming the Hankel structure of matrix H to guarantee its integrity. The proposed method utilizes multi-layer perceptrons for smooth manifold learning in the optimization algorithm for tensor reconstruction. The auto-encoding process involves encoding each column of an input matrix with (\u03c8 r ,\u03c6 r ) and transforming a matrix [g 1 , g 2 , ..., g T ] with Hankel structure using Hankelization of input tensor Z. The MMES network described in Eq. (4) involves forward embedding (H), encoding (\u03c6 r ), decoding (\u03c8 r ), and backward embedding (H \u2020) using an auto-encoder (AE) to define the manifold M r. The optimization problem includes a reconstruction loss and an auto-encoding loss, balanced by a trade-off parameter \u03bb > 0. The optimization problem involves balancing a reconstruction loss and an auto-encoding loss with a trade-off parameter \u03bb > 0. The value of \u03bb is gradually adjusted to prevent a large gap between the losses, ensuring L rec > L AE. The algorithm for tensor reconstruction and enhancement includes adapting \u03bb for optimal results. The convolutional structure of H and H \u2020 facilitates the calculation of L rec and L AE. The calculation flow of L rec and L AE can be easily implemented using neural network libraries like TensorFlow. Adam optimizer is employed for updating (Z, A r). Experimental results show similarities and differences between DIP and MMES in color-image inpainting, superresolution, and deconvolution tasks. Optional results on optimization behavior, hyper-parameter sensitivity, and 3D image completion are provided in the Appendix. In a toy example of signal recovery, a one-dimensional time-series signal is generated from the Lorentz system and corrupted. The corrupted one-dimensional time-series signal from the Lorentz system was recovered using subspace and manifold modeling in embedded space. The manifold modeling captured the structure of the Lorentz attractor better than subspace modeling, showing smooth pattern changes and non-local similarity relationships. The manifold modeling played a key role in \"patch-grouping\" in the proposed method. The proposed method utilizes global manifold modeling for image inpainting, comparing its performance with various unsupervised methods such as HaLRTC, TMac, tSVD, Tucker inc., LRTV, SPC, GSR, MDT-Tucker, and DIP. Detailed hyper-parameter settings are provided in the Appendix. The proposed method competes well with DIP in image inpainting, achieving high scores in PSNR and SSIM. Various methods were tested on images with missing ratios ranging from 50% to 99%, with low-rank priors struggling to recover highly incomplete images. SPC showed improvement over LRTV in capturing essential image properties, and MDT-Tucker further enhanced the results. The proposed method competes well with DIP in image inpainting, achieving high scores in PSNR and SSIM. Comparison with unsupervised image super-resolution methods like Bicubic interpolation, GSR, ZSSR, and DIP was conducted, with hyper-parameter settings carefully tuned for optimal performance. PSNR and SSIM values from computer simulation results are shown in Fig. 5(b), using different color image sizes for super resolution methods. Super resolution methods like GSR, DIP, and MMES were evaluated for up-scaling images, with DIP slightly outperforming GSR and MMES slightly outperforming DIP. Bicubic interpolation was found to be inferior. Results showed that GSR had smooth outlines but was slightly blurred, while DIP produced sharp images with jagged artifacts. ZSSR was weak for very low-resolution images. Visual comparisons in Fig. 7 highlighted the differences in image quality among the methods. The proposed MMES method showed improved image deblurring compared to DIP, with sharp and smooth outlines. Three color images were used for comparison, with results showing similar quality qualitatively and quantitatively. Interpretability in machine learning lacks a precise definition. Interpretability in machine learning refers to the degree to which a human can predict a model's results. Manifold learning, like auto-encoders, is a non-linear dimensionality reduction technique that efficiently solves the problem of high-dimensional data sets. Manifold learning algorithms aim to uncover the underlying parameters of low-dimensional manifolds in high-dimensional data sets. The MMES approach utilizes multi-way delay embedding transform to extract a low-dimensional representation of images. The algorithm optimizes a cost function to describe the manifold mathematically and address the issue of noise in data. The MMES approach aims to explain noise impedance in DIP by considering a sparse-land model where noise perturbs low-dimensional image manifolds. When fitting networks for noisy images, learning \"similar patches\" is crucial for reducing squared error. The MMES approach focuses on learning \"similar patches\" in image-patches to reduce noise and improve denoising. It uses an auto-encoder to map similar patches into close points on a low-dimensional manifold, preserving common components while reducing noise. This concept is related to noise impedance phenomenon explained by Alain & Bengio (2014). The MMES model and associated learning algorithm provide new insights for DIP by utilizing non-linear manifold modeling with deep learning. This approach has been successfully applied to time-series signals, natural color images, and tensors. The study applies Hankelization with AE to general tensor data reconstruction, introducing MMES as a novel image reconstruction model based on low-dimensional patch manifold prior. It connects ConvNet and DIP, supporting their use in various applications like tensor/image enhancement. The method bridges dynamical system analysis, deep learning, and tensor modeling, serving as a prototype that can be enhanced with regularizations, multi-scale extensions, and adversarial techniques. The curr_chunk discusses the concept of Hankel matrix and its relation to block Hankel matrix. It explains the process of Hankelization for a two-dimensional array and highlights the difference between the two matrix types. The text also mentions that a Hankel matrix is a special case of a block Hankel matrix when all elements are scalar. In this paper, the concept of Hankel structure is discussed, which is a special case of a block Hankel structure. Valid convolution is explained as a process involving delay embedding/Hankelization and linear transformation. Multiway-delay embedding transform (MDT) is introduced as a generalization of Hankelization, utilizing multi-linear tensor product and tensor reshaping. The study defines the multiway-delay embedding with an unfolding operator and padding operation. The study introduces multiway-delay embedding using an unfolding operator and duplication matrix. It involves Hankelization with reflection padding and multi-linear tensor product to construct an overlapped patch grid. The Moore-Penrose pseudo inverse is utilized for trimming and removing elements. Delay embedding and its pseudo inverse can be implemented using convolution with onehot-tensor windows. The neural network architecture of an auto-encoder can restrict the manifold M r by controlling the dimensionality of the latent space. Other possibilities include Tikhonov regularization. Delay embedding and its pseudo inverse can be achieved using convolution with one-hot-tensor windows. In this study, the focus is on denoising auto-encoder (DAE) as a fundamental method for reducing data entropy and implementing Tikhonov regularization. The DAE is chosen for its relationship with regularization techniques and its ability to decrease data entropy. The study involves designing an auto-encoder with control over the dimension and noise level, particularly for multi-channel or color image recovery. In the case of multi-channel or color image recovery, a special setting of the generator network is used to share the patch-manifold. Three channels of input are embedded independently, concatenated into three block Hankel matrices, and auto-encoded simultaneously. The inverted images are stacked as a color image and color-transformed using a convolution layer with a kernel size of (1,1). The input channels do not need to correspond to RGB, but are optimized for compact color representation. In Section 4.2, the performance of the proposed method was compared with various unsupervised image inpainting methods. Hyper-parameters were manually tuned for peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) in the experiments. The default architecture was used for DIP without exploring different network structures. The proposed MMES method adaptively selected patch-size \u03c4 and dimension r. Parameter settings for MMES are shown in Table 2. Noise level for denoising auto-encoder was set at \u03c3 = 0.05. The auto-encoder used the same architecture as shown in Fig. 13. The Adam optimizer had an initial learning rate of 0.01, decayed by 0.98 every 100 iterations, and stopped after 20,000 iterations for each image. Experimental settings were detailed in Section 4.3, comparing the proposed method with other unsupervised image super-resolution methods. In experiments, unsupervised image super-resolution methods like bicubic interpolation, GSR 8, ZSSR, and DIP were compared. Different parameters were used for up-scaling in MMES, with varying results in PSNR and SSIM values. Bicubic interpolation performed poorly, while ZSSR showed promise for smaller color images. The performance of various unsupervised image super-resolution methods was compared. GSR, DIP, and MMES were competitive, with MMES slightly outperforming DIP. The experiment involved recovering a 50% missing gray-scale image of 'Lena' using optimization algorithms with specific parameters. The optimization behaviors and sensitivity of MMES with different hyper-parameters were evaluated. The sensitivity of MMES was evaluated with three hyper-parameters: r, \u03c3, and \u03c4. Different settings of dimension r and noise standard deviation \u03c3 were tested for reconstructing a 99% missing image of 'Lena'. Results showed that low dimension (r = 1) led to blurred results, while high dimension (r = 64) resulted in peaks. An appropriate noise level (\u03c3 = 0.05) provided sharp and clean results. Learning with noise was found to have a positive effect. Varying patch sizes \u03c4 of (8,8) or (10,10) were suitable for recovering the missing image. Patch size is crucial for image recovery, with (8,8) or (10,10) being appropriate. If too large, patch variations complicate the structure, while too small limits information from the matrix H. Good patch sizes vary for different images and corruption levels. The estimation of the optimal patch size remains an open problem. In this section, results of MR-image/3D-tensor completion problem are shown. The MR image size is (109, 91, 91). Different percentages of voxels are randomly removed and recovered using the proposed method and DIP. The proposed method outperformed DIP in low-rate missing cases and was competitive in highly missing cases. The insufficiency of filter sizes may cause degradation of DIP in 3D ConvNet compared to 2D ConvNet. Computational times for MMES were significantly shorter than DIP in tensor completion."
}