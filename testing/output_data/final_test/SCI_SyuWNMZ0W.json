{
    "title": "SyuWNMZ0W",
    "content": "The maximum mean discrepancy (MMD) is a metric that measures the difference between two probability measures P and Q. It can be used for two-sample tests and to train neural networks. Biases in data sets, such as under-representation of certain demographics, can impact machine learning algorithms. Manipulating the estimating distribution Q can help address these biases. In this paper, an estimator is constructed for the MMD between probability measures P and Q using a biased sample selection mechanism. Methods for estimating the sample selection mechanism are suggested when unknown. The estimator can be used to train generative neural networks on biased data samples. Neural networks with stochastic input layers can be trained to sample from a probability distribution P for various applications such as generating illustrations, simulating molecular fingerprints, and synthesizing medical time-series data. The feedforward neural network acts as a generator mapping random noise inputs to an observation space X, with the resulting distribution Q determined by the network's architecture. Generative networks like GANs use different loss functions to train the generator, such as adversarially learned loss functions or distributional distances. The maximum mean discrepancy has shown good performance as a loss function by requiring the generator to reproduce the full range of data variation. Machine learning methods rely on data being a representative sample from the target distribution. Biases in data collection can lead to machine learning algorithms replicating and amplifying human biases. Steps should be taken to correct biases, such as gender or race biases in audio or image datasets. Machine learning methods rely on data being a representative sample from the target distribution. Steps should be taken to correct biases, such as altering demographics in a scene to fit a story-line. By penalizing the generator based on the difference between simulated data and the unbiased distribution, we can construct an estimator of the bias. Generative networks aim to simulate data similar to a target distribution. Estimators of bias can be constructed by penalizing the generator based on the deviation between observed and target distributions. Approximating the function linking target and empirical data distributions is discussed, along with bias reduction methods in survey sampling statistics. The efficacy of this approach is demonstrated in practice. Generative networks use neural networks to simulate data from a complex probability distribution. The neural network generator minimizes the difference between the simulated distribution and the target distribution by training on a loss function. Generative adversarial networks (GANs) are a common form of generative network where a neural network classifier is trained to differentiate between real data and simulated data. The generator in a neural network is trained to differentiate between data and simulations, returning a score for each class. The loss function of the generator is based on the score assigned to simulations under the true data class, increasing the chance of fooling the classifier. However, mode collapse can occur when the generator mimics only a subset of the data. To address this issue, recent works have incorporated estimators of distributional distances between distributions P and Q, focusing on overall distributions rather than just element-wise distances between samples. The maximum mean discrepancy (MMD) measures the distance between two distributions by projecting them into a reproducing kernel Hilbert space (RKHS) and looking at the maximum mean distance between the two projections. This distance reduces to zero if all moments of the two distributions are the same. Other distributional distances used include the Wasserstein distance and the Cramer distance. Generative networks may include adversarial components in their loss functions, such as the MMD-GAN, which adversarially learns parameters of the MMD metric. The MMD measures the distance between two distributions by projecting them into an RKHS. To use it as a loss function for training neural networks, we need to estimate the MMD from data and differentiate it with respect to network parameters. An unbiased estimator of the MMD can be constructed using samples from the distributions. The MMD has been effectively used in generative adversarial networks. The MMD has been successfully used as a loss function in generative adversarial networks. BID5 and BID13 propose training a neural network to minimize the MMD between simulations and data. The MMD-GAN generator aims to reduce the MMD between simulations and data by combining the kernel with a dynamically learned autoencoder. The estimator in Equation FORMULA2 provides an unbiased estimate of the MMD between two distributions, which can be used as a loss function to minimize the MMD between distributions in a generative network context. In practice, data may be biased due to sampling practices, leading to biased estimations. An estimator for MMD between distributions P and Q is proposed when access is only through biased sampling. This involves a related distribution T(x)P(x) where T(x) acts as a \"thinning function\". The thinning function T(x) is used to characterize the sample selection mechanism, ensuring that candidate samples x* are selected into the pool with probability T(x*). This framework can be applied to biased or unbiased samples by modulating the target distribution. The estimation problem is transformed into an importance sampling problem, with ways to estimate or specify the thinning function discussed in Section 4. Importance sampling is used to estimate the MMD between distributions P and Q, where samples from T(x)P(x) are weighted based on the likelihood ratio 1/T(x). The normalizing constant Z, dependent on T and P, is unknown due to the lack of an analytic form for P. Therefore, direct calculation of the MMD using Equation 2 is not feasible. The biased estimator M b in Equation 3, known as the weighted MMD estimator, is constructed using self-normalized importance weights. This bias decreases as 1/m with the number of samples m from T(x)P(x), and often has lower variance than the unbiased estimator in Equation 2. If the thinning function T(x) describing the sample selection mechanism is known, Equation 3 can be used directly in the generator's loss function. In practice, we may not have access to the thinning function T(x) mentioned in Equation 3. If classes are imbalanced, we can manually rebalance the dataset or estimate T by comparing subsets of data with the expected distribution. Even with a partially labeled dataset, we can estimate T using labeled examples. For instance, in an image dataset with more men than women, labeling a subset can help approximate T for better balance. In a two-class setting, estimating the thinning function T(x) can be done by setting T(men) = 1 and approximating T(women) using the sample ratio of men to women in the labeled subset. Logistic regression can be used to extrapolate values of T across the observation space X. More sophisticated regression tools may be needed for complex problems, but the goal remains function estimation from labeled data. Techniques like neural network function estimation or Gaussian process regression can be used to learn the thinning function T(x) from labeled exemplars. The issue of correcting biased sample selection mechanisms, though not well-addressed in deep learning literature, is common. Our estimator is related to inverse probability weighting (IPW), used in survey statistics to correct for biased sample selection. Each data point is assigned a weight based on its selection probability, counteracting undersampling of certain classes. This work addresses biased data in machine learning. This work addresses biased data in machine learning by exploring how biases in language manifest in word embeddings and image labeling datasets. Experimental settings are considered to estimate sampling bias, and an estimator for the MMD between biased and unbiased data distributions is constructed. The efficacy of the approach is demonstrated using a mixture of two Gaussians and a scaled logistic function. The study involves training a generator network using a logistic function, with data distributed accordingly. The network has six layers with three nodes each, trained using standard MMD and weighted MMD estimators. Simulation plots compare the outputs of the trained GANs. The study involves training a generator network with a logistic function using ADAM optimization. The standard MMD estimator replicates the empirical distribution well, while the weighted MMD estimator replicates the target distribution. The dataset used contains a sampling bias, with 80% zeros and 20% ones instead of the target 50/50 distribution. The function describing the bias must be estimated from the data. The dataset acknowledges a discrepancy but lacks a functional translation for T. The practitioner labels examples for each class and estimates the under-representation. An architecture based on MMD-GAN is used, incorporating an autoencoder to optimize the kernel for discrimination between data and simulations. The experiments are conducted on an adapted version of the original MMD-GAN Torch code. The experiments are conducted using an adapted version of the original MMD-GAN Torch code. A low-dimensional embedding of images is obtained using an autoencoder, and a thinning function T (x) is specified on the encoder space. The standard MMD estimator in the loss function is replaced with a weighted estimator calculated using the estimated T. Two scenarios for using the weighted estimator are discussed - training the GAN from the start or initializing network weights after training with the standard estimator. The experiments involved training two networks using a weighted estimator: one initialized to a pre-trained MMD-GAN and one randomly initialized. The biased sample selection method resulted in more zeros than ones in the data used for training. Simulations generated by minimizing the standard MMD estimator reflected this bias, while simulations trained using the weighted MMD estimator showed an increase in ones without affecting simulation quality. Logistic regression was used to classify simulated images as zeros or ones. The weighted MMD estimator produced a higher percentage of ones in simulated images compared to the standard MMD estimator. However, neither network reached a 50:50 ratio as desired, likely due to convergence issues. The weighted MMD estimator led to a higher proportion of ones in simulated images compared to the standard MMD estimator. This estimator can be used to manipulate distributions when only biased sampling mechanisms are available. The weighted MMD estimator can manipulate distribution of simulations learned by a generative network to correct sampling bias or change distribution. Thinning function can be estimated from labeled data, demonstrated in an experiment with partially labeled images. Next step is exploring more sophisticated thinning functions for complex settings."
}