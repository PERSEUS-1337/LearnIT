{
    "title": "HyxLRTVKPH",
    "content": "In the context of machine learning, budgeted training is introduced to address the limitations of current approaches for hyper-parameter tuning and neural architecture search due to practical resource constraints. The focus is on optimizing performance within a fixed resource budget by adjusting the learning rate schedule, with simple linear decay identified as a robust and high-performing approach. The study supports the claim through experiments on various datasets, highlighting the importance of budgeted convergence for a good schedule. Budget-aware learning schedules outperform existing approaches for fast convergence in the practical setting of budgeted training. Deep neural networks have significantly advanced machine learning tasks, particularly in computer vision. The performance improvements in deep neural networks are driven by labeled visual data and training architecture innovations. However, resource constraints for training are becoming a limiting factor as datasets continue to grow in size. Planning for training on datasets orders of magnitude larger than current ones is crucial, especially for practical applications like self-driving cars and medical analysis. Resource-constrained training is already a concern and may become impractical to visit every training example before running out of resources. Resource-constrained training is a widespread issue for practitioners with limited compute access. The problem of budgeted training involves maximizing performance within a fixed training budget. Adjusting the learning rate schedule and annealing it to 0 at the end of the budget significantly outperforms off-the-shelf schedules, especially for small budgets. This approach is shown to be effective for training schemes on ResNet-18 for ImageNet. The focus shifts to optimizing performance within a fixed training budget, specifically limiting the number of iterations. The assumption is made that the network architecture remains constant throughout training. Theoretical analysis of optimization algorithms focuses on asymptotic convergence and optimality, but there is work providing performance bounds that depend on the iteration number. The goal is to maximize performance within a fixed number of iterations, considering that the globally optimal solution may not be achievable within a limited budget. Data subsampling is a common strategy, but other approaches may be more effective. In this paper, the authors propose adopting budget-aware learning rate schedules as a more effective strategy than data subsampling. They suggest linearly decaying the learning rate to 0 at the end of the budget, which they find to be robust and competitive for general learning settings. Their findings are verified with state-of-the-art models on various datasets. The authors propose budget-aware learning rate schedules as a better strategy than data subsampling. They observe a correlation between learning rate and gradient magnitude, leading to budgeted convergence. This phenomenon suggests decaying the learning rate to zero at the end of training, rather than early on, for optimal optimization. The authors introduce budget-aware learning rate schedules as a superior approach to data subsampling. They emphasize the importance of decaying the learning rate towards the end of training for optimal optimization, showcasing the effectiveness of their linear schedule over other methods. The descent method dates back to Robbins & Monro (1951) and involves updating parameters using a learning rate schedule. In deep learning, there is no consensus on the role of the learning rate, with most theoretical analysis assuming a small and constant rate. In deep learning, the role of the learning rate is debated, with some suggesting large rates for moving over energy barriers and small rates for converging to local minima. Recent analysis on mode connectivity challenges this, showing paths between previously isolated minima. Despite lacking theoretical explanation, heuristic schedules like step decay and exponential are commonly used for practical purposes. The learning rate in deep learning is a debated topic, with heuristic schedules like step decay and exponential commonly used. Recent work proposes new schedules, but evaluation is often limited to CIFAR and ImageNet datasets. SGDR advocates for learning-rate restarts based on CIFAR results, but cosine decay is found to be more effective. Adaptive learning rate methods adjust learning rates based on local statistics of the cost surface. Despite theoretical advantages, they do not generalize as well as momentum SGD for larger benchmark tasks. Evaluating these methods under budgeted settings reveals that aggressive early descent is not ideal for good performance. Modifying learning rate schedules for budgeted settings can be done through early-stopping. In budgeted settings, modifying the learning rate schedule through early-stopping can significantly improve results. By tuning the learning rate based on the training budget, a budget-aware schedule can be created. This schedule is not solely dependent on the absolute iteration but also considers the training budget. This approach can lead to better performance, especially when dealing with tasks that have a limited number of epochs. In non-convex optimization, it is beneficial to systematically decay the learning rate in proportion to the total iteration budget. This can be achieved through Budget-Aware Conversion (BAC), where the rate schedule is rescaled based on the original budget used. This adaptation strategy has been proven effective in experiments and is a general strategy for training with limited schedules. Recent experiments confirm the effectiveness of Budget-Aware Conversion (BAC) for converting standard schedules to be budget-aware. BAC is applied to baselines by default in experiments, with classification accuracy numbers provided. Several recent learning rate schedules are implicitly defined as a function of progress and are budget-aware by definition. The poly schedule in Caffe and the htd schedule proposed by Hsueh et al. (2019) are compared with the cosine schedule by Loshchilov & Hutter (2017) in terms of learning rate schedules on CIFAR-10. The effectiveness of these schedules is evaluated based on classification accuracy. The linear schedule outperforms other schedules under various budgets in a budget-aware setting. It is parameter-free and shows excellent generalization across budgets, suggesting self-similarity in the cost surface of deep learning. The linear schedule in deep learning is self-similar in cost surface. It outperforms other schedules in budget-aware settings, showing excellent generalization. The comparison is done on CIFAR-10 dataset and vision benchmarks, with implementation details in the main text and additional information in the appendices. In the setting of neural architecture search, a large number of random architectures are evaluated using the CIFAR-10 dataset, which contains 70,000 tiny images. The dataset is split into training and validation sets, with ResNet-18 used as the backbone architecture. Different learning schedules, including constant, exponential, step decay, and htd, are studied to improve validation accuracy. In neural architecture search, various learning schedules are compared for improving validation accuracy. The group of budget-aware schedules, including cosine, poly, and linear, outperforms others across different budgets without requiring tuning. The linear schedule performs best overall, while the exponential schedule struggles to generalize. The linear schedule is suggested as the optimal budget-aware schedule for achieving robust performance. Experiments validate its effectiveness across tasks and budgets, showing similar performance to cosine and poly schedules on CIFAR-10. The linear schedule is recommended for robust performance across tasks and budgets, outperforming cosine and poly schedules on CIFAR-10. Evaluation on ImageNet using ResNet-18 shows top-1 accuracy with step decay schedule. The linear schedule is recommended for robust performance across tasks and budgets, outperforming step decay. Object detection and instance segmentation on MS COCO are evaluated using the challenge winner Mask R-CNN with a ResNet-50 backbone. The AP of the final model on the validation set is reported in the experiment. For training semantic segmentation on Cityscapes, the PSPNet model with a ResNet-50 backbone is used with a full budget of 400 epochs. The mIoU of the best epoch is reported, and a poly schedule is utilized. Step decay is added for complete evaluation, following the same setup as in the ImageNet experiment. The text discusses video classification on Kinetics using the I3D model with a step decay schedule. It compares the performance of linear schedule over step decay, showing a clear advantage of linear schedule, especially on ImageNet. The text discusses the effectiveness of using a linear schedule for budgeted training, showing improved performance compared to traditional step decay schedules. The linear schedule, despite being parameter-free, outperforms step decay with limited budgets and achieves comparable results with full budget settings. The dynamics of gradient norm correlate with the learning rate schedule, demonstrating the effectiveness of the linear schedule for various budgets. The text discusses the correlation between gradient norm dynamics and learning rate schedules in budget-aware training. It highlights the importance of decaying schedules to near-zero rates for effective convergence, termed as \"vanishing gradient\" phenomenon. This empirical analysis emphasizes the properties required for effective budget-aware learning schedules. The correlation between gradient norm dynamics and learning rate schedules in budget-aware training is crucial for effective convergence. Decaying schedules to near-zero rates may be more effective than early stopping, resonating with classic literature on SGD behavior. The influence of learning rate on gradient magnitude remains unclear, emphasizing the importance of not wasting the budget in training. The best model tends to be found towards the end of training, with different schedules producing optimal models at various points. This is beneficial for state-of-the-art models with expensive evaluation processes. Budget-aware schedules save compute by validating only on the last few epochs. The location of the model with the highest validation accuracy within the training progress is measured and averaged across different schedules and runs. Faster descent of the objective is desired in optimization methods. The text discusses the importance of not employing aggressive early descent in optimization methods, as large learning rates can hinder budgeted convergence. It compares AMSGrad to budget-aware linear schedules and finds that AMSGrad underperforms due to its overly aggressive nature. The text also mentions exploring periodic schedules like SGDR for optimization. SGDR explores periodic schedules with cosine scaling for warm restarts, aiming to escape local minima. However, its effectiveness has been questioned. Comparisons show that SGDR has faster descent but is inferior to budget-aware schedules for any budget. The correlation between full gradient norm and learning rate is observed in SGDR, but warm restarts do not improve budgeted performance. The paper introduces a formal setting for budgeted training, highlighting the need for methods achieving promising anytime and budgeted performance simultaneously. The paper introduces a formal setting for budgeted training, showing that linear schedules can achieve better performance. Learning rate schedules control gradient magnitude and optimize like simulated annealing. Budgeted training is applied in neural architecture search due to resource constraints. In a formal setting for budgeted training, linear schedules can improve performance by predicting the relative rank of different architectures. Budgeted training techniques have not been explored in neural architecture search literature, but adjusting the learning schedule, specifically using a linear schedule, can enhance rank prediction accuracy. Linear schedules can enhance rank prediction accuracy in budgeted training for neural architecture search. Random architectures are modified from ResNet-18 and trained on CIFAR-10 with various learning schedules under small budgets. Kendall's rank correlation coefficient is used to measure rank similarity between full-budget and limited-budget ranks. The experiment involved training random architectures on CIFAR-10 with different learning schedules under small budgets to enhance rank prediction accuracy in budgeted training for neural architecture search. The results showed that a smooth-decaying schedule, such as linear or cosine, is preferred over step decay for better estimating the full-budget rank between architectures. Stochastic noise was reduced by repeating each configuration 3 times, and the median accuracy was taken. The full-budget model was trained with a linear schedule, with similar results expected with other schedules. Among 100 random architectures, 21 could not be accurately ranked. Among 100 random architectures tested on CIFAR-10, 21 were untrainable. The remaining 79 models had validation accuracy ranging from 0.37 to 0.94, with a peak at 0.91. The best validation accuracy for each configuration was used, minimizing the impact of decay schedules. Linear schedules performed better under small budgets, showing closer performance to full-budget accuracy. This suggests linear schedules generalize well across different settings. The study compares budgeted performance of different schedules on random architectures. Linear schedule is found to be the most robust across architectures and budgets. In another section, the equivalent learning rate is derived for comparing AMSGrad with momentum SGD. In budgeted training, data subsampling is a common strategy to meet constraints. The study compares different schedules on random architectures, finding linear schedule to be the most robust. The equivalent learning rate is derived for comparing AMSGrad with momentum SGD. In budgeted training, data subsampling is a common strategy to limit data points seen during training. Random sampling is an effective method for constructing a subsampled dataset. Even a baseline budget-aware step decay can outperform offline subsampling. Different subset construction methods exist, such as core-set construction, but may be unsuitable for some scenarios. In budgeted training, data subsampling is a common strategy to limit data points seen during training. Random sampling is effective for constructing a subsampled dataset. The comparison of linear schedule against step decay for various tasks is discussed. The evaluation of poly schedule on Cityscapes is included, showing inconclusive results on the superiority of one over the other within the smooth-decaying family. Smooth-decaying methods outperform step decay with limited budgets. Smooth-decaying methods outperform step decay with limited budgets, as shown in the comparison between budget-aware linear schedule and adaptive learning rate methods on CIFAR-10. Adaptive methods are surpassed by linear schedule at each given budget, including AMSGrad, RMSprop, and AdaBound. Budget-aware linear schedule consistently outperforms adaptive methods for all budgets. In a study comparing budget-aware linear schedule and adaptive learning rate methods on CIFAR-10, it was found that smooth-decaying methods outperform step decay with limited budgets. The performance of AMSGrad, RMSprop, and AdaBound was surpassed by the linear schedule at each given budget. Additionally, it was observed that annealing an adaptive method, such as AMSGrad, boosts performance. The study also evaluated the periodical schedule of SGDR, which requires two parameters to specify the periods. The study compared budget-aware linear schedule and adaptive learning rate methods on CIFAR-10, finding that smooth-decaying methods outperform step decay with limited budgets. SGDR was evaluated with off-the-shelf and budget-aware settings, showing the importance of budget-awareness for better convergence. The study compared budget-aware linear schedule and adaptive learning rate methods on CIFAR-10, finding that linear schedule outperforms SGDR under both budget-aware and budget-unaware settings. Constant learning rates are guaranteed to converge for convex cost surfaces, while sequences that decay neither too fast nor too slow ensure convergence for non-convex problems. Convex optimization converges with \u03b1 t = \u03b1 0 /t. For non-convex problems, convergence to a local minimum is possible. There is no theory for learning rate schedules in general non-convex optimization. Comparison between linear and step decay with different initial learning rates shows the importance of tuning the initial learning rate as a hyper-parameter. Training is done on ImageNet with ResNet-18 architecture, using a base learning rate of 0.1 and weight decay of 5 \u00d7 10 \u22124. Video classification on Kinetics with I3D is also performed. Asynchronous batch normalization and batch size 128 are used for video classification on Kinetics with I3D. The evaluation is done on the 400-category version of the dataset. An open source codebase is utilized, implementing a variant of standard I3D with ResNet as the backbone. The configuration of run i3d baseline 300k 4gpu.sh is followed, with a base learning rate of 0.005 and weight decay of 10^-4. Training is done using 4 GPUs with asynchronous batch normalization and batch size 32. For object detection and instance segmentation on MS COCO, Mask R-CNN is used with a base learning rate of 0.02 and weight decay of 10^-4. The codebase uses a base learning rate of 0.02 and weight decay of 10^-4 for training with 8 GPUs. A warm-up mechanism increases the learning rate for 0.5k iterations to stabilize multi-GPU training. For semantic segmentation on Cityscapes, a PyTorch codebase with a base learning rate of 0.01 and weight decay of 10^-4 is adapted. Training includes various augmentations and patch-based testing. The model patches independently and tiles them to create a single output. Overlapped regions use the average logits of two patches. Training is done with 4 GPUs using synchronous batch normalization and a batch size of 12."
}