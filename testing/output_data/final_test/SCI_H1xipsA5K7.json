{
    "title": "H1xipsA5K7",
    "content": "Our algorithm can learn a two-layer neural network with a general class of input distributions. It guarantees recovery of the parameters A, W of the ground-truth network, given a symmetric input x. Our algorithm, based on the method-of-moments framework, extends tensor decomposition results and uses spectral algorithms to learn neural networks without complex optimization. It can robustly learn ground-truth networks with few samples from symmetric input distributions. Despite the success of deep learning in various tasks, understanding it theoretically remains a challenge due to the non-convex nature of neural network optimization. The efficiency of learning neural networks, especially when data is drawn from them, is still an open question with limited provable results. The paper presents a new algorithm for learning a two-layer neural network for a general class of input distributions. It aims to find a neural network that closely approximates the ground truth network, even in the presence of limited data samples. The algorithm makes assumptions on the network structure or input distribution to tackle the challenging problem of learning neural networks. The paper introduces a new algorithm for learning a two-layer neural network for complex input distributions like texts, images, or videos. The network structure involves two weight matrices and a ReLU activation function, making learning similar networks challenging even with simple input distributions. The paper introduces a new algorithm for learning a two-layer neural network for complex input distributions like texts, images, or videos. Despite the input distribution being Gaussian, standard optimization objectives can lead to bad local optimal solutions. A new objective function proposed by Ge et al. (2017b) avoids bad local minima by requiring the input distribution to be symmetric. This symmetry requirement can be seen as a \"factor-2\" approximation to an arbitrary input distribution, allowing for the augmentation of training samples with their negations to achieve symmetry. The algorithm introduced in the paper aims to learn a two-layer neural network for complex input distributions like texts, images, or videos. It is based on a method-of-moments approach and can effectively recover model parameters with exact correlations between x and y of order at most 4. The algorithm runs in poly(d) time and outputs a network that is effectively the same as the ground-truth network for any input x. The algorithm introduced in the paper aims to learn a two-layer neural network for complex input distributions like texts, images, or videos. It can effectively recover model parameters with exact correlations between x and y of order at most 4. The algorithm is robust to perturbations and can work with polynomially many samples, outputting a network that computes an approximation function to the ground-truth network. The algorithm presented in the paper focuses on learning a two-layer neural network for complex input distributions such as texts, images, or videos. It can recover model parameters with exact correlations between x and y of order at most 4, and is robust to perturbations. The algorithm works with polynomially many samples and outputs a network that approximates the ground-truth network. In the context of smoothed analysis, assumptions are made regarding the full rank of certain matrices, and a method to perturb the input distribution, W, and A is discussed to limit the power of the adversary. The paper discusses learning a two-layer neural network for complex input distributions with high probability. The algorithm provides an -approximation to the perturbed network efficiently and robustly, with experiments showing its effectiveness. Proof for polynomial number of samples and smoothed analysis are deferred to the appendix. Neural networks come in various styles, including non-standard networks with different activation functions. Works have explored learning networks with discrete variables, polynomial activation functions, and rank-1 tensor decomposition for convolutional neural networks. For Gaussian input, new objectives have been proposed to avoid bad local optima in two-layer neural networks. Several works have extended these findings to different settings. Several works have extended learning results to more general distributions, with Gao et al. (2018) proposing a way to design loss functions for neural networks with general input distributions. Their estimator can still require an exponential number of samples in dimension d. Du et al. (2017a) showed how to learn a single neuron or convolutional filter under certain conditions for the input distribution. Recent works have focused on using kernel methods to learn neural networks with bounded weights and input distributions. While previous studies have shown how gradient descent minimizes training error in over-parameterized two-layer neural networks, our work extends this to testing error as well. Unlike previous research on learning single neurons or filters, our study tackles the more complex task of learning a two-layer neural network using the method-of-moments approach. In this section, we describe the neural network model we learn, introduce notations for matrices and tensors, and define the distinguishing matrix. We consider two-layer neural networks with d-dimensional input, k hidden units, and k-dimensional output. The input x is i.i.d. drawn from a symmetric distribution D. The neural network model consists of input x drawn from distribution D, weight matrices W and A, output y generated using ReLU function \u03c3, and zero-mean noise \u03be. Hidden units h are computed as \u03c3(Wx), with every row vector of W having unit norm. The network function remains unchanged when scaling rows of W by c and columns of A by 1/c. The model is illustrated in Figure 1. In the vector space R n, inner product of vectors is denoted by \u00b7, Euclidean norm by \u00b7. Singular values of matrix A are denoted as \u03c3 1 (A) \u2265 \u03c3 2 (A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3 min(m,n) (A), with condition number \u03ba(A) = \u03c3 1 (A)/\u03c3 min (A). Spectral norm is denoted as \u00b7, Frobenius norm as \u00b7 F. A d-dimensional linear subspace S is represented by matrix S \u2208 R n\u00d7d, with projection matrices Proj S = SS and Proj S \u22a5 = I n \u2212 SS. The Kronecker product of matrices A and C is denoted as A \u2297 C. For a vector x, the p-fold Kronecker product is denoted as x \u2297p. The conversion between vectors and matrices is done using vec(A) for matrices and mat(x) for vectors. The distinguishing matrix, DISPLAYFORM0, is defined based on a weight matrix W and input distribution D, with columns indexed by ij where 1 \u2264 i < j \u2264 k. The distinguishing matrix D is a matrix indexed by ij where 1 \u2264 i < j \u2264 k. The augmented distinguishing matrix M is a d 2 \u00d7 (k 2 + 1) matrix with robustly full rank requirements. The columns of N D ij look at the expectation over samples with opposite signs for weights w i , w j. In Section C, the distinguishing matrix D is required to be full rank to prevent degenerate cases. A lower bound on \u03c3 min (M) is also justified in the smoothed analysis setting. An algorithm for learning two-layer networks is described, starting with a single-layer neural network approach. The detailed proof is deferred due to space constraints. In Section C, a lower bound on \u03c3 min (M) is justified in the smoothed analysis setting for the distinguishing matrix D to prevent degenerate cases. The detailed proof for learning a single-layer neural network algorithm is deferred to Section A in the appendix. The algorithm estimates correlations between x and y, and the covariance of x to recover the hidden vector w, despite y not being a linear function of x. Goel et al. (2018) observed that for a single-layer neural network, if x comes from a symmetric distribution, averaging between x and -x can eliminate non-linearities like ReLU. This insight leads to a method-of-moments algorithm for learning the weight vector w. The algorithm estimates correlations between x and y, and the covariance of x to recover w. The algorithm in Algorithm 1 simplifies learning the weights of a network with k outputs and hidden-units by reducing the two-layer problem to a single-layer problem. In the noiseless case, u y = (u A)\u03c3(W x) where u \u2208 R k and z i is the normalized i-th row of A \u22121. If u = z i, then u y = \u03c3(\u03bb i w i x), representing a single-layer neural network output with weight \u03bb i w i. The input/output pairs (x, z i y) correspond to single-layer networks with weight vectors {\u03bb i w i}. This approach is based on the observation that averaging between x and -x can eliminate non-linearities in a single-layer neural network. The algorithm simplifies learning weights of a network with k outputs and hidden units by reducing the two-layer problem to a single-layer problem. It focuses on finding vectors that generate pure neurons, transforming the problem to learning a single-layer network. The algorithm aims to identify properties that indicate a single neuron output, using moments estimation and sample checks. The algorithm simplifies learning weights of a network with k outputs and hidden units by reducing the two-layer problem to a single-layer problem. It focuses on finding vectors that generate pure neurons, transforming the problem to learning a single-layer network. The algorithm aims to identify properties that indicate a single neuron output, using moments estimation and sample checks. However, even if z = u y is not pure, it may still satisfy the property. To address this issue, a higher order moment is considered, where the distinguishing matrix columns are linearly independent objects. If the sum of extra terms is 0, the coefficients in front of each column must also be 0, ensuring that all coefficients are 0 unless both c i and c j are non-zero. The algorithm simplifies learning weights of a network with k outputs and hidden units by reducing the two-layer problem to a single-layer problem. It focuses on finding vectors that generate pure neurons, transforming the problem to learning a single-layer network. The main obstacle in solving the system of equations f(u) = 0 is that every entry of f(u) is a quadratic function in u. The system of equations f(u) = 0 is a system of quadratic equations, which can be solved by linearizing the function. The algorithm simplifies learning weights of a network with k outputs and hidden units by reducing the two-layer problem to a single-layer problem. It focuses on finding vectors that generate pure neurons, transforming the problem to learning a single-layer network. The key idea is to linearize the function by treating each degree 2 monomial as a separate variable, resulting in a linear space with a matrix T whose nullspace consists of specific vectors. By estimating the tensor T from given samples, the span of its smallest singular directions can be determined. The final step involves finding z i 's from the span of z i z i 's to further simplify the problem. The final step involves finding z i 's from the span of z i z i 's using simultaneous diagonalization, where z i 's are the only eigenvectors of XY \u22121. This procedure helps in simplifying the problem by identifying all the z i 's. The algorithm presented in Algorithm 2 aims to find all z i 's by assuming z i A = \u03bb i e i and addressing the issue of negative \u03bb i values. The main steps involve constructing a pure neuron detector, simultaneous diagonalization to obtain all z i 's, and solving the single-layer problem using Algorithm 1. The output includes weight matrices W and A expressed as degree-2 polynomials over u. The algorithm in Algorithm 2 aims to find all z i 's by assuming z i A = \u03bb i e i and addressing negative \u03bb i values. Steps involve constructing a pure neuron detector, simultaneous diagonalization for z i 's, and solving the single-layer problem using Algorithm 1. The output includes weight matrices W and A as degree-2 polynomials over u. Theorem 4 states that the network returned by the algorithm computes the same function as the original neural network when certain conditions are met. The algorithm in Algorithm 2 finds z i by assuming z i A = \u03bb i e i and addressing negative \u03bb i values. Steps involve constructing a pure neuron detector, simultaneous diagonalization for z i, and solving the single-layer problem using Algorithm 1. The output includes weight matrices W and A as degree-2 polynomials over u. Theorem 4 states that the network returned by the algorithm computes the same function as the original neural network under certain conditions. In this section, experimental results validate the robustness of the algorithm for Gaussian input distributions and symmetric distributions like symmetric mixtures of Gaussians. The implementation differs from the description in Section 3.3, with a more robust approach involving drawing random samples from a subspace and computing the CP decomposition. The CP decomposition of samples is done via alternating least squares, repeated 10 times to select the best one. Gradient descent is used to learn W after fixing A, ensuring overall error stability. Modifications are unnecessary with a large number of samples. The algorithm was successful with 10,000 input samples and random orthogonal matrices. Our algorithm successfully learns neural networks with random orthonormal matrices as ground truth, requiring only a few times the number of parameters as samples. The error in recovering the matrices is minimal, even when the dimensions vary. The normalized error remains low, showcasing the algorithm's robustness. The study demonstrates the algorithm's robustness to label noise and the stability of normalized error as the dimensions of A and W increase. The overall mean square error is shown in Figure 2, with results for different levels of conditioning of W and A. The algorithm achieves low error in recovering W, A, and outputs, even with varying dimensions. The algorithm's robustness to label noise and stability of normalized error as dimensions of A and W increase is demonstrated. The input distribution is a mixture of Gaussians with two components, showing recovery of the globally optimal solution regardless of distribution choice. Condition number of A or W is varied by sampling singular value decompositions. The algorithm's performance remains steady as long as matrices A and W are well-conditioned, eventually fluctuating but still recovering with sufficient accuracy. A new algorithm using method-of-moments and spectral techniques is designed to avoid complex non-convex optimization for neural networks, allowing for more general input distributions. The current work discusses open problems in neural networks, including the need to remove constraints on output dimensions and biases in hidden layers. Future work aims to extend results to more distributions and complex networks, exploring implications for optimization landscapes. The algorithm presented offers insights into finding global optimal networks efficiently, potentially leading to new optimization strategies. Additionally, missing proofs for lemmas in previous sections are provided. In this section, missing proofs for lemmas from Section 3 are provided. The noise case is discussed, along with Algorithm 3. Handling cases where the matrix A has more rows than columns is also briefly discussed. The use of symmetric distribution to eliminate non-linearities like ReLU is explained through a more general version of Lemma 6. In this section, missing proofs for lemmas from Section 3 are provided. The noise case is discussed, along with Algorithm 3. Handling cases where the matrix A has more rows than columns is also briefly discussed. The use of symmetric distribution to eliminate non-linearities like ReLU is explained through a more general version of Lemma 6. For any vector a \u2208 R d and non-negative integers p and q, we have DISPLAYFORM1 where the expectation is taken over the input distribution. Two cases are considered: p and q are both even numbers or both odd numbers. The first step in the algorithm is to construct a pure neuron detector based on Lemma 2 and Lemma 3. Proof of Lemma 2 follows from Lemma 6 by setting a = w, p = 2, and q = 0. Proof of Lemma 3 focuses on the second equation. In this section, missing proofs for lemmas from Section 3 are provided. The second equation is proven by rewriting \u0177 = ki=1 ci \u03c3(wix) = uy as uA = c. The proof involves transforming terms and using Lemma 6. The proof shows that for input x from a symmetric distribution, for any vectors a, b \u2208 Rd, we have aTb = E(abT). In this section, the proof of Lemma 4 is provided, showing that the null space of T is the span of {vec * (z i z i )}. The proof is divided into two cases, demonstrating that for any vector vec * (U ) in the null space of T, T vec * (U ) = 0. This is achieved by showing that A U A is a diagonal matrix when the RHS of the equation equals 0. The proof of Lemma 5 shows that the vector vec * (U ) belongs to the span of {vec * (z i z i )} due to the full column rank of the distinguishing matrix N and the symmetry of A U A. By finding all z i 's using simultaneous diagonalization, the two-layer network can be reduced to a single-layer one, allowing for the recovery of the first layer parameters w i 's. The vector d i \u2208 R k is the diagonal elements of D i. Matrix Q \u2208 R k\u00d7k has full rank and none of its rows are zero vectors. The event of zero valued elements in D X or D Y diagonal is of probability zero. The solution space of q i \u03b6 1 = 0 is a lower-dimension manifold in R k with zero measure. The solution space of DISPLAYFORM12 \u22121 is a lower-dimensional manifold in R 2k space with measure zero. In the noisy case, the event of duplicated diagonal elements in Y has zero probability. The noise only affects the first two steps of the algorithm, not the last two steps. The algorithm focuses on learning two-layer neural networks with noise input. The algorithm focuses on learning two-layer neural networks with noise input, where each entry is expressed as a degree-2 polynomial over u. The steps in Algorithm 3 are designed with robustness in mind, such as separating input samples into two halves to avoid correlations and computing the null space of T for the exact case. The algorithm focuses on learning two-layer neural networks with noise input, separating input samples into two halves to avoid correlations. The pure neuron detector is modified to cancel extra noise square terms, and a lemma states the modified detector. The augmented distinguishing matrix M must be full rank for finding the span. The algorithm focuses on learning two-layer neural networks with noise input. The augmented distinguishing matrix M must be full rank for finding the span. Theorem 5 states that under certain conditions, the network returned by the algorithm computes the same function as the original neural network. The algorithm focuses on learning two-layer neural networks with noise input. To finish the noise case, Lemma 9 needs to be proven. The proof involves deducing terms in the right-hand side of an equation step by step. The algorithm uses half of the samples to estimate empirical moments. The algorithm focuses on learning two-layer neural networks with noise input. It uses half of the samples to estimate empirical moments and involves running Algorithm 3 on the samples. The output is a combination of terms displayed in a specific formula. The algorithm can be extended to cases where the output dimension is greater than the number of hidden units. The pseudo-code for this general case is provided in Algorithm 4. If certain conditions are met, the network can be accurately learned using this algorithm. If matrix M is full rank and Algorithm 4 has exact moments, the network computes the same function as the original neural network. Ground truth parameters A and W generate samples with noise \u03be. The column span of E[yx] is the same as A. The columns of P form an orthonormal basis for A. The generated sample is (x, Py). Theorem 5 states Z^-1\u03c3(Vx) = PA\u03c3(Wx). Even without exact moments, empirical data can still be used. Algorithm 3 can learn parameters robustly even without exact moments, as long as empirical moments are estimated with enough samples. Theorem 7 proves the robustness of Algorithm 3 under certain conditions. The matrix has smallest singular values \u03c3 min (M) \u2265 \u03b1. For any small enough \u03b4 < 1, given poly \u0393, P1, P2, d, 1/\u03b3, 1/\u03b1, 1/\u03b2, 1/\u03b4 number of i.i.d. samples, Algorithm 3 outputs V, \u1e90\u22121 with probability at least 1 \u2212 \u03b4, DISPLAYFORM1 for any input x. The key lemmas prove each step of Algorithm 3 is robust to noise, showing that with a polynomial number of samples, we can approximate the span of vec*(zi zi)'s accurately. The null space of T is the span of vec*(zi zi) as defined in Algorithm 3. The null space of matrix T, as shown in Lemma 10, is the span of vec*(zi zi)'s. Using matrix perturbation theory, Lemma 11 demonstrates that the null space of T is robust to small perturbations. With a polynomial number of samples, the span of the k least singular vectors of T closely approximates the null space. The proof of this lemma involves bounding the spectral gap and the Frobenius norm of T - \u015c. The null space of matrix T is robust to small perturbations, as shown in Lemma 10. By using matrix perturbation theory, it is demonstrated that the span of vec*(zi zi)'s closely approximates the null space. The algorithm for finding zi's is similar to simultaneous diagonalization, ensuring robustness. With a polynomial number of samples, the eigenvectors of X\u0176 -1 can be found with high probability. The null space of matrix T is robust to small perturbations, as shown in Lemma 10. Algorithm 1 is analyzed, and when called from Algorithm 3, fresh samples are reserved to maintain independence. A modified proof shows Algorithm 1 remains robust if \u1e91 i \u2212 z i is small enough. Lemma 13 provides conditions for robustness, and with a polynomial number of samples, eigenvectors of X\u0176 -1 can be found with high probability. Theorem 7 is proven in Section B.4 using matrix perturbation bounds. Lemma 14 establishes a spectral gap in matrix T. With enough samples, the estimate T is close to the true nullspace. Lemma 15 provides conditions for robustness in finding eigenvectors of X\u0176 -1 with high probability. The text discusses the relationship between the least k right singular vectors of matrix T and its null space. Lemmas 14 and 15 provide conditions for robustness in finding eigenvectors of X\u0176 -1 with high probability. Theorem 7, proven using matrix perturbation bounds, establishes a spectral gap in matrix T. With enough samples, the estimate T is close to the true null space. The text discusses characterizing matrix T as the product of four matrices to lowerbound the k 2 -th singular value. Matrix F is used to transform vec * (U ) to vec(U ) for any symmetric k \u00d7 k matrix U. Matrix C is constructed by picking corresponding rows of A \u2297 A. The first matrix M is the augmented matrix. Matrix C is constructed using A \u2297 A to create a k2 \u00d7 k2 matrix. Matrix B is used to transform the augmented distinguishing matrix M into a d2 \u00d7 k2 matrix. The k2-th singular value of matrix T is lower bounded when \u03c3 min (M) \u2265 \u03b1 and \u03c3 min (A) \u2265 \u03b2, with the value being \u03b1\u03b2^2. Matrix T has rank k2 and its k2-th singular value is lower bounded by \u03b1\u03b2^2. The rank of T is exactly k2, as matrix B is full-column rank with rank k2 and matrix M is full column rank with rank k2 + 1. The product matrix MB is still full-column rank with rank k2, leading to T = MBF with rank k2. CF has full-row rank equal to k2, as \u03c3 min (A) \u2265 \u03b2 implies A \u2297 A is full rank. Matrix T has rank k2 and its k2-th singular value is lower bounded by \u03b1\u03b2^2. The smallest singular value of each individual matrix in the product characterization of T is lower bounded. The construction of C shows it consists of a subset of rows of A \u2297 A. The smallest singular value of matrix CF is lower bounded by the smallest singular value of matrix C. This is proven by showing that for any unit vector u, u CF \u2265 u C, implying \u03c3 min (CF) \u2265 \u03c3 min (C). Additionally, it is shown that the k2-th singular value of matrix T is the smallest non-zero singular value of T, denoted as \u03c3 + min (T). The proof involves standard matrix concentration inequalities and the requirement on the norm of x is for convenience. The proof involves standard matrix concentration inequalities and the requirement on the norm of x is for convenience. The lemma states that for certain conditions on x, A, \u03be, and \u03c3 min (E[xx ]), T \u2212 T F \u2264 , with high probability. The upper bound for T \u2212 T F is derived by bounding T \u2212 T 2 and showing that it is upper bounded. Given a polynomial number of samples, certain expressions are shown to be upper bounded with high probability. The lemma provides conditions for T - TF \u2264 , with high probability, based on matrix concentration inequalities. By bounding T - T2, it is shown to be upper bounded with a polynomial number of samples. Additionally, given certain assumptions on x, A, \u03be, and \u03c3 min (E[xx]), various expressions are upper bounded with high probability. Based on Lemma 24 and union bound, with a polynomial number of samples, various expressions are upper bounded with high probability, ensuring certain conditions hold with probability at least 1 \u2212 \u03b4. In this section, the algorithm's simultaneous diagonalization step is shown to be robust. Matrices S and \u015c, containing the least k right singular vectors of T and T respectively, have their Frobenius norm well bounded. However, the difference between S and \u015c is only small after appropriate alignment with a rotation matrix. If SS \u2212\u015c\u015c F \u2264 , a rotation matrix R \u2208 R k\u00d7k exists satisfying RR = R R = I k . Proof: S has orthonormal columns, \u03c3 k (SS ) = 1. By Lemma 35, a rotation matrix R exists. Let the k columns of S be vec, where D i is a diagonal matrix. Let Q be a k \u00d7 k matrix with diagonal elements of D i . Let vec * (X) = SR\u03b6 1 , vec * (Y ) = SR\u03b6 2 , where R is the rotation matrix in Lemma 16 and \u03b6 1 , \u03b6 2 are standard Gaussian vectors. Let D X = diag(QR\u03b6 1 ) and D Y = diag(QR\u03b6 2 ). It's easy to check that X and Y are well separated. Lemma 17 states that if A \u2264 P 1 , \u03c3 min (A) \u2265 \u03b2, then with probability at least 1 \u2212 \u03b4, sep( holds. Matrix Q is well-conditioned since . Let U be a k 2 \u00d7k matrix with columns consisting of... Matrix Q is well-conditioned and has k non-zero rows. Define U as a k 2 \u00d7k matrix with columns of vec(U i )'s and Q as a k 2 \u00d7 k matrix with columns of vec(D i )'s. A subset of rows of U forms an orthonormal matrix S, with \u03c3 min (U ) \u2265 \u03c3 min (S) = 1. Given \u03c3 min (A) \u2265 \u03b2, we have \u03c3 min (Q) \u2265 \u03b2 2. Additionally, for the smallest singular value of A \u2212 \u2297 A \u2212 , we have Q \u2264 \u221a 2P. By properties of Gaussians, we know q \u22a5 i,j , R\u03b6 1 is independent of q j , R\u03b6 1, allowing us to apply anti-concentration of Gaussians. With probability at least 1 \u2212 \u03b4/k 2, we have... The text discusses the relationship between the eigenvectors of matrices X\u0176 \u22121 and XY \u22121. It proves that these eigenvectors are close to each other under certain conditions. The text also mentions the use of Gaussian vectors and provides a proof for the closeness of the eigenvectors. According to Lemma 34, to bound the perturbation matrices F and G, we need to first bound EX, EY, \u03c3 min(Y), and \u03c3 min(Y\u0302). By analyzing the smallest singular values of X and Y, we can conclude that \u03c3 min(X) and \u03c3 min(Y) are at least poly(1/d, 1/P1, \u03b2, \u03b4) for small enough values. For small enough values, the perturbation matrices F and G are bounded by poly(d, P1, 1/\u03b2, 1/\u03b4). Lemma 17 states that with probability at least 1 - \u03b4/4, the separation of certain variables holds. Additionally, the eigenvectors of XY^-1 are the normalized rows of A^-1, with the only issue being the sign of z_i. The sign flip step of z_i is shown to be robust in the following lemma. For small enough values, with high probability, the sign flip of z_i is consistent with the sign flip of z_i. The algorithm is proven to be robust with a certain number of i.i.d. samples. The algorithm is proven to be robust with a certain number of i.i.d. samples, ensuring that the learned weight vector is close to the true weight vector with high probability. The algorithm is robust with a number of i.i.d. samples, ensuring the learned weight vector is close to the true weight vector with high probability. In Algorithm 3, the normalized rows of A^-1 are denoted as z_i, and the eigenvectors of X\u0176^-1 are denoted as \u1e91_i. It is shown that with a bounded z_i - \u1e91_i, a polynomial number of samples ensures v_i - v_i is also bounded. The algorithm ensures robustness with i.i.d. samples, where the weight vector is close to the true weight vector with high probability. By bounding z_i - \u1e91_i, a polynomial number of samples ensures v_i - v_i is also bounded. The noise term (\u1e91 i \u2212 z i ) y is not independent with the sample (x,\u1e91 i y), but using a separate set of samples for Algorithm 1 allows for i.i.d. samples (x,\u1e91 i y), enabling the use of matrix concentration bounds for robustness. The first term is bounded using matrix concentration bounds. With a polynomial number of i.i.d. samples, the algorithm ensures robustness by bounding the difference between matrices V and V\u0302. This leads to a bound on the difference between matrices Z and Z\u0302 as well. The algorithm ensures robustness by bounding the difference between matrices V and V\u0302 using matrix concentration bounds. This leads to a bound on the difference between matrices Z and Z\u0302 as well, with the help of standard matrix perturbation bounds. In smoothed analysis, small Gaussian perturbations make matrices A and W robustly full rank with reasonable probability. Using this framework, we show that the distinguishing matrix is robustly full rank. Two settings are considered: Gaussian input distribution with perturbed weights for matrix W, and fixed full rank weight matrix W. The smallest singular value of the augmented distinguishing matrix M depends polynomially on dimension and perturbation amount, ensuring a lower bound for \u03c3 min (M) when W is in general position. In the second case, a full rank weight matrix W is fixed, and an arbitrary symmetric input distribution D is considered. A simple perturbation D close to D is introduced, and it is shown that the smallest singular value of the perturbed augmented distinguishing matrix M is lower bounded. The input follows a standard Gaussian distribution, and the weight matrix W is perturbed accordingly. The smallest singular value of M is proven to be lower bounded with high probability. Theorem 9 states that for k \u2264 d/5 and a standard Gaussian input distribution, the smallest singular value of the perturbed augmented distinguishing matrix M is lower bounded with high probability. To achieve this, a perturbation method is defined using a random Gaussian matrix Q and a parameter \u03bb \u2208 (0, 1) to perturb the input distribution. This perturbation is necessary as certain symmetric input distributions may lead to degenerate problems. The random matrix Q generates a Gaussian distribution D Q with a random covariance matrix. Sampling a point in D Q involves sampling n \u223c N (0, I d ) and outputting Qn. The smallest singular value of the augmented distinguishing matrix M D is lower bounded after applying (Q, \u03bb)-perturbation. Theorem 10 shows that after perturbations with a random Gaussian matrix Q, the result holds with high probability. Theorem 9 states that for k \u2264 d/5 and input following a standard Gaussian distribution, with a perturbed weight matrix W, the augmented distinguishing matrix M satisfies a certain property with high probability. This is proven by considering the matrix M ij and its properties under Gaussian distribution. Lemma 19 states that for input x following a standard Gaussian distribution and weight matrix W with full-row rank, the angle between weight vectors w_i and w_j can be calculated. The leave-one-out distance is a metric related to the smallest singular value, easier to estimate. Lemma 20 by Rudelson & Vershynin (2009) shows how to lowerbound the smallest singular value of a matrix by its leave-one-out distance. To bound \u03c3 min ( M ), we need to lowerbound d( M ). The approach involves showing that even when conditioning on all columns except one, the distance between columns is significant due to randomness. However, obstacles include a special non-random column, complex coefficients in the matrix expression, and lack of independence between columns. Lemma 21 provides a stronger version of Lemma 20, allowing for a special column in the matrix. It shows that if the leave-one-out distance for all but one column can be bounded, the smallest singular value of the matrix is still lowerbounded as long as the columns have similar norms. The proof is deferred to Section C.2. Additionally, the coefficients are shown to be lowerbounded with high probability, allowing for conditioning on them. Lemma 22 states that weight vectors can be perturbed without becoming close to co-linear, with coefficients lowerbounded with high probability. The proof is deferred to Section C.2. The final obstacle is addressed using ideas similar to Ma et al. (2016) for decoupling randomness in columns. The proof in Ma et al. (2016) decouples randomness in columns. Event E1 is a bad event, but not conditioned on. Columns of M are defined as w_i,L for i < j. Focus is on proving large distance of column M_ij from span of other columns. Span V_ij is correlated with M_ij, so a subspace V_ij is defined to contain it. The subspace V_ij contains V_ij, and is independent with V_ij. The dimension of V_ij is at most a certain value. When bad events E1 or E2 do not happen, the distance between column M_ij and the span of other columns is at least an inverse polynomial. The norm of the columns is bounded by a certain value, leading to M_ij being less than or equal to a polynomial function of \u03c4, d, and \u03c1. The proof of Lemma 19 involves characterizing columns in the augmented distinguishing matrix. Assuming unit norm weight vectors, the subspace spanned by w_i and w_j is considered. Using matrix representations, the proof shows the orthogonal subspace and basis vectors for the subspace. The goal is to prove a certain projection equality, utilizing orthogonal properties. The proof involves characterizing columns in the augmented distinguishing matrix by showing the orthogonal subspace and basis vectors for the subspace. The goal is to prove a certain projection equality using orthogonal properties. The closed form for m_ij is computed within the subspace S_ij of dimension two. The proof involves characterizing columns in the augmented distinguishing matrix by showing the orthogonal subspace and basis vectors for the subspace. The closed form for m_ij is computed within the subspace S_ij of dimension two using the polar representation of two-dimensional Gaussian random variables. The smallest singular value of A is defined and analyzed, leading to the derivation of certain inequalities. The Cauchy-Schwarz inequality is used to bound the smallest singular value of matrix A and the angle between perturbed vectors. The proof involves showing the lower bound of the projection of one vector on the orthogonal subspace of another. This is done by characterizing the subspace spanned by the vectors and using a chi-squared random variable with (d-2) degrees of freedom. Starting from a well-conditioned weight matrix W, various inequalities are used to show bounds on different random variables with high probability. These bounds are derived using chi-squared random variables and union bounds, ultimately leading to a conclusion about the weight matrix W. At ICLR 2019, it is shown how to perturb a symmetric input distribution locally to ensure a minimum singular value of MD is at least inverse polynomial. Theorem 10 states that given a well-conditioned weight matrix W and symmetric input distribution D, applying (Q, \u03bb)-perturbations yields a perturbed input distribution D with certain properties. The proof involves analyzing the augmented distinguishing matrix for instances with weight W and input distribution N(0, QQ). Techniques developed in Section C.1, along with noise domination and subspace decoupling, are used to address new challenges. The smallest singular values of MDQ and MWQ are closely related, leading to the need to analyze the singular value of MWQ. The problem involves analyzing the smallest singular value of matrix MWQ, which is similar to Theorem 9 but with a non-i.i.d. Gaussian weight matrix WQ. Despite this difference, Theorem 9 can still be applied due to the dominance of noise in WQ. A simple claim is used to show that if a property holds for a Gaussian distribution with smaller variance, it will also hold for a distribution with larger variance. The text discusses the analysis of the smallest singular value of matrix MWQ, showing that properties hold for Gaussian distributions with smaller variance also hold for distributions with larger variance. The claim is made that N(\u00b5, \u03a3) is a mixture of N(x, I), leading to the conclusion that \u03c3 min (MDQ) is large. The worry is that mixing with D may reduce \u03c3 min (MD), but it is proven that this cannot happen with high probability. The proof of Theorem 10 involves partitioning the Gaussian matrix Q to handle a special column in the augmented distinguishing matrix. The augmented distinguishing matrix is defined as M D, a matrix with specific columns based on the distribution D. The (Q, \u03bb)-perturbation is discussed, and the analysis of M D Q is the focus in the first step of the proof. In analyzing M D Q, the matrix can be written as M D Q ij = E x\u223cD Q (w i x)(w j x)(x \u2297 x)1{w i xw j x \u2264 0}. The columns are similar to the augmented distinguishing matrix of a network with weight matrix W Q and input distribution N (0, I d ). The smallest singular value of M W Q is then analyzed, where W = W Q and Q is a Gaussian matrix with correlations within columns. The smallest singular value of M W Q is analyzed, with correlations within columns of W Q. The covariance matrix of W Q has the smallest singular value at least \u03c1 2. With probability at least 1 \u2212 exp(\u2212d \u2126(1)), the norm of every row of W Q is upper bounded by poly(\u03c4, d). Any convex property that holds for any N (0, \u03c1 2 I kd) perturbation must also hold for \u03a3 W Q. With probability at least 1 \u2212 exp(\u2212d \u2126(1)), \u03c3 min (M W Q) \u2265 poly(1/\u03c4, 1/d, \u03c1). The text discusses the relationship between matrices R, U, and C, showing that C is independent of P1 and P2. It then introduces matrix H and aims to prove a lower bound on its smallest singular value using leave-one-out distance. The text discusses the covariance matrix of submatrix \u0124, showing independence of rows in random matrix P2VW. It applies Claim 1 and Lemma 37 to prove a lower bound on the smallest singular value. With probability at least 1 \u2212 exp(\u2212d \u2126(1)), the covariance matrix of each row of P2VW has a smallest singular value of at least \u03b3. This matrix can be viewed as the summation of two independent Gaussian matrices, one with covariance matrix \u03b3I(d\u2212k). Claim 2 is then applied to lowerbound the smallest singular value of a random matrix K. The text discusses using matrix concentration bounds and perturbation bounds for robust and smoothed analysis. It also presents known results on matrix perturbations and concentration bounds, along with proving corollaries useful in the setting. The leave-one-out distance of a subspace S independent of K with dimension at most k^2 + 1 is shown to be at least poly(\u03b3, 1/d) when d \u2265 7k. The proof involves applying Lemma 31 to K \u2297 K and considering the orthonormal matrix U. The text discusses matrix concentration bounds and perturbation bounds for robust analysis. It presents known results on matrix perturbations and concentration bounds, proving useful corollaries. The alignment of subspace basis is addressed, showing that after appropriate alignment, S is close to \u015c. After appropriate alignment, matrices S and \u015c are close. For fixed matrices perturbed by Gaussian variables, the smallest singular value can be bounded. If all entries of matrix A are independently perturbed, the smallest singular value is bounded below."
}