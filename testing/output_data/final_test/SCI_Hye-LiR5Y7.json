{
    "title": "Hye-LiR5Y7",
    "content": "SOSELETO (SOurce SELEction for Target Optimization) is a new method that uses a source dataset to solve a classification problem on a target dataset by assigning weights to source samples. This helps the target dataset choose the most informative samples for its classification task and acts as regularization to prevent overfitting. The method can be applied to transfer learning and training on datasets with noisy labels, showing state-of-the-art results. Deep learning has led to state-of-the-art algorithms in various fields due to large datasets. However, in data-poor scenarios like medical imaging, transfer learning is used to apply deep learning techniques. Transfer learning involves transferring knowledge from a data-rich source to a data-poor target, often done in a two-stage technique. Transfer learning in deep learning involves a two-stage technique where a network is first trained on a source classification task and then adapted to a target classification task. This adaptation can be done through feature extraction or fine-tuning. The approach aims to leverage pre-trained network features for the target task, but it may overlook the importance of certain source examples. Transfer learning in deep learning typically involves a two-stage approach where a network is trained on a source task and then adapted to a target task. However, the importance of specific source examples for the target task may be overlooked. To address this, we propose SOSELETO, a method that learns to filter important examples from the source dataset during end-to-end training. Each source sample is assigned a weight, and a shared source/target representation is optimized through bilevel optimization. SOSELETO is a method that optimizes shared representation through bilevel optimization. The target selects informative source samples for its classification task, acting as regularization to prevent overfitting. The entire training process happens simultaneously. SOSELETO optimizes shared representation through bilevel optimization, selecting informative source samples for classification to prevent overfitting. It differs from instance reweighting for domain adaptation by focusing on transfer learning with minimal label overlap. SOSELETO is a general approach for transfer learning and domain adaptation, without assuming label overlap between source and target. It can also be used for training with noisy labels, where a large noisy dataset is the source and a smaller clean dataset is the target. SOSELETO is a transfer learning and domain adaptation algorithm that can be applied to problems where a large noisy dataset is the source and a small clean dataset is the target. The algorithm assigns high weights to samples with correct labels and low weights to those with incorrect labels, implicitly denoising the source for accurate classifier training. The paper is organized into sections discussing related work, the SOSELETO algorithm, experimental results on transfer learning and training with noisy labels, and a conclusion. Transfer learning techniques include feature extraction and fine-tuning, while domain adaptation methods are also explored. Transfer learning techniques involve transferring knowledge between source and target classes, with domain adaptation focusing on aligning source and target via feature space statistics. Recent papers address domain adaptation scenarios where source and target classes are different, including cases of partial overlap or complete differences. Some propose selecting a portion of the source dataset for improved performance. In contrast to previous works on domain adaptation, a recent study focuses on selecting source examples similar to the target dataset using filter bank descriptors before training. Another recent work addresses few-shot learning with a large number of unlabelled examples, using adversarial and entropy-based losses for alignment and training. Instance reweighting for domain adaptation is a known technique, but this work allows for different label spaces between the source and target datasets. Our proposed method for domain adaptation allows for different classes in the source and target datasets, without assuming similarity of distributions or optimizing for it. Unlike previous works, our approach is completely supervised and uses a gradient-based re-weighing scheme to handle noisy labels effectively. Learning with noisy labels in classification is a well-known issue in machine learning. Deep learning studies have shown that large datasets with label noise can still achieve high accuracy. Different approaches have been proposed, such as adding a noise layer to adapt the output to noisy labels or using a probabilistic graphical model to infer clean labels and noise types. In the context of learning with noisy labels in classification, recent work includes using two separate CNNs to infer noise from images. Other approaches involve training separate networks simultaneously and analyzing CNNs' resistance to noise theoretically. Bilevel optimization problems have a nested structure, with the interior level being a standard optimization problem. Bilevel optimization is a standard optimization problem where the exterior level's objective is a function of the optimal arguments from the interior level. It has been extensively studied in mathematical programming. Bilevel optimization has applications in machine learning and computer vision. There are two datasets: a data-rich source set for learning extensively and a data-poor target set for classifier training. The goal is to learn a classifier on the target set, denoted DISPLAYFORM1, using the source set to solve the target classification problem. Not all source examples contribute equally useful information; images related to dogs, wolves, cats, and objects with similar textures as dog fur are helpful, while images of airplanes and beaches are less likely to be relevant. The algorithm chooses relevant source images for the target classifier network. The source and target networks have the same architecture but different parameters. The source and target share features but not classifiers. The source network is denoted as F (x; \u03b8, \u03c6 s ), while the target network is denoted as F (x; \u03b8, \u03c6 t ). The features are shared between the source and target networks. The weighted source loss is determined by \u03b1 j \u2208 [0, 1] for each source training example, with a per example classification loss. The target loss is standard. This formulation addresses transfer learning and learning with label noise, where high weights indicate relevant knowledge for the target task. The target dataset shares a classifier and label space with the clean dataset. High weights indicate reliable source examples without label noise. Combining source and target losses into a single optimization problem poses challenges. Bilevel optimization is a more promising approach, finding optimal features and source classifier by minimizing the source loss with weights \u03b1. The bilevel optimization approach aims to minimize source loss by adjusting weights \u03b1, while also minimizing target loss indirectly through controlling which source examples are included. This method serves as a form of regularization to prevent overfitting, especially when dealing with a small target dataset. The challenge lies in solving the optimization problem at every point in time, making implementation somewhat complex. The proposed procedure suggests taking a gradient descent step for the interior level problem at each iteration, aiming to improve features \u03b8 while keeping source weights \u03b1 fixed. This iterative approach addresses the inefficiency of solving the optimization problem at every step, aligning with standard deep learning practices of gradual loss improvement. The proposed iterative approach involves taking a gradient descent step for the interior level problem to improve features \u03b8 while keeping source weights \u03b1 fixed. The update scheme aligns with deep learning practices and involves aligning source example weights with the target aggregated gradient. Weight constraints are enforced by requiring \u03b1 j = \u03c3(\u03b2 j), where \u03b2 j \u2208 R and \u03c3 : R \u2192 [0, 1] is a sigmoid-type function. The proposed iterative approach involves updating features \u03b8 while keeping source weights \u03b1 fixed. The update scheme aligns with deep learning practices by aligning source example weights with the target aggregated gradient. Weight constraints are enforced by requiring \u03b1 j = \u03c3(\u03b2 j), where \u03b2 j \u2208 R and \u03c3 : R \u2192 [0, 1] is a sigmoid-type function. The update equation for \u03b2 modifies Equation (4) and is summarized in Algorithm 1, with descent equations for source and target classifiers \u03c6 s and \u03c6 t. The operation is done on a mini-batch basis to avoid conflicts in weight updates. Time-complexity involves processing both a source batch and a target batch in each iteration. The proposed iterative approach involves updating features \u03b8 while keeping source weights \u03b1 fixed. SOSELETO requires about twice the time as the ordinary source classification problem due to processing both a source batch and a target batch in each iteration. Regarding space-complexity, in addition to network parameters, we need to store the source weights \u03b1, resulting in a relative space increase of about 3%. Convergence properties of SOSELETO are uncertain as it is an approximation to a bilevel optimization problem. In Appendix B, sufficient conditions for SOSELETO convergence to a local minimum of the target loss L t are demonstrated. Implementation details include using SGD optimizer without learning rate decay, setting \u03bb \u03b1 = 1, and initializing \u03b1-values to 1. The range for \u03b1-values is clipped to [0, 1.1]. Specific settings for experiments are discussed in relevant sections. A synthetic experiment with 500 points in R 2 is used to illustrate SOSELETO's performance in learning with noisy labels. In a synthetic experiment with 50 points, SOSELETO correctly identifies clean and noisy instances based on a threshold of 0.1 on the weights \u03b1. Clean points above the threshold are labeled in green, while noisy points below the threshold are labeled in green. SOSELETO correctly identifies noisy labels by assigning small weights below 0.1, with 92 out of 100 points having such weights. The algorithm accounts for 98.4% of points using this threshold. A plot in FIG0 (e) shows the evolution of mean weight vs. training epoch for clean and noisy instances, with clean instances having a mean weight of 0.8 and noisy instances having a mean weight of 0.05 after 100 epochs. In a real-world setting, noisy labels are identified using small weights below 0.1. A noisy version of CIFAR-10 is used with an overall noise level selected, and a label confusion matrix is created based on this level. Experiments are conducted for various noise levels using a small clean dataset. In contrast to using a smaller clean dataset of 5K examples and 45K noisy samples, BID33 and BID39 set aside 10K clean examples for pre-training. Results are compared with state-of-the-art methods on CIFAR-10 test set for different noise levels. SOSELETO achieves state of the art performance on CIFAR-10's test set with noise levels of 30%, 40%, and 50%, outperforming competitors by 2.6% to 3.2% absolute improvement. It uses only half of the clean samples compared to other methods. Further analysis is done by examining the \u03b1-values chosen on convergence. SOSELETO effectively filters out incorrect labels in samples with 30%, 40%, and 50% noise levels. It shows a significant improvement in performance compared to competitors on CIFAR-10's test set. The study focuses on transfer learning with disjoint label sets and a small target dataset. The source dataset is a subset of SVHN with digits 0-4, while the target dataset is a small subset of MNIST with digits 5-9. There is no overlap between source and target classes, making it a true transfer learning problem. The small target set size adds further challenge. The study compares results with different techniques, including training on the target dataset only. The study compares different transfer learning techniques for a small target dataset with disjoint label sets. Various methods are evaluated, including target-only training, standard fine-tuning, Matching Nets, and Label Efficient Learning. The approaches differ in their use of unlabelled target data, with SOSELETO standing out for not utilizing any additional data. The LeNet architecture is used for most methods, while SOSELETO is configured with specific settings like batch sizes and regularization parameters. The study compares transfer learning techniques for a small target dataset with disjoint label sets. SOSELETO outperforms other methods, including few-shot techniques, without using unlabelled data. SVHN images were resized to match MNIST size for evaluation. SOSELETO is considered more useful than other transfer learning techniques for small target datasets with disjoint label sets. It involves a two-stage procedure where the classifier is first trained on the source dataset and then used to classify unlabelled data, resulting in improved accuracy above 92%. This technique exploits the source dataset through joint training using bilevel optimization. The algorithm SOSELETO is optimized through bilevel optimization, showing effectiveness in learning with label noise and transfer learning. Future research may explore incorporating domain alignment for datasets with overlapping labels. The technique is architecture-agnostic and applicable to various computer vision tasks beyond classification. In future research, it is important to explore incorporating domain alignment for datasets with overlapping labels in computer vision tasks. The algorithm SOSELETO is optimized through bilevel optimization, showing effectiveness in learning with label noise and transfer learning. Additionally, a set of sufficient conditions for SOWETO to converge to a local minimum of the target loss L t is demonstrated. The algorithm SOSELETO is optimized through bilevel optimization, showing effectiveness in learning with label noise and transfer learning. A set of sufficient conditions for SOWETO to converge to a local minimum of the target loss L t is demonstrated by examining the change in the target loss from iteration m to m + 1 using the evolution of the weights \u03b1."
}