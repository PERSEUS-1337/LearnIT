{
    "title": "S191YzbRZ",
    "content": "In genomics, predicting Transcription Factor Binding Sites (TFBSs) is a challenging task due to the variety of Transcription Factors (TFs) involved. Two main mechanisms for TF binding are sequence-specific motifs and co-binding effects. A new deep architecture called Prototype Matching Network (PMN) is proposed to mimic TF binding mechanisms by extracting prototypes for each TF and learning interactions with genomic sequences using LSTM. PMN outperforms baselines on a TFBS dataset with 2.1 million genomic sequences. The proposed Prototype Matching Network (PMN) outperforms baselines on a dataset with 2.1 million genomic sequences for predicting Transcription Factor Binding Sites (TFBSs). This deep learning architecture introduces prototype learning and considers TF-TF interactions, accurately modeling the underlying biology of gene expression control. Enabling machines to comprehend genomes is a crucial goal in computational biology. Accurate models for identifying and describing TF binding sites are essential in understanding cells. ChIP-seq technologies have provided genome-wide binding site maps for multiple TFs, but computational methods are still necessary due to limitations in experiments. TFs bind to sequence-specific motifs on genomes, which are crucial for understanding genome functioning and evolution. In understanding genome functioning and evolution, TFs bind to sequence-specific motifs on genomes. Motifs act as blueprints for TF binding, but co-binding effects with other TFs can also influence binding. To address these challenges, a deep-learning model called prototype matching network (PMN) is proposed to automatically extract motif-like features and model co-binding patterns for predicting TF binding sites. Many bioinformatics studies have used position weight matrices (PWMs) to predict TFBSs by matching sequences against PWMs. However, convolutional neural network (CNN) models have outperformed PWM-matching. A proposed PMN model is inspired by \"prototype-matching\" and combines feature matching with prototype matching for more accurate recognition of objects. The current text discusses the use of prototype-matching in TFBS prediction, proposing a novel prototype-matching loss to automatically learn prototype embeddings for each TF. Previous deep learning studies have not considered motif-matching using a similarity measure or including co-binding effects among TFs in data modeling. Genomic sequence-based TFBS prediction is viewed as a multi-label sequence classification task. The text discusses the challenges of multi-label sequence classification in TFBS prediction due to the large number of TFs involved. The proposed model PMN aims to address these obstacles by incorporating co-binding effects and learning prototype embeddings for each TF. The proposed model PMN addresses challenges in TFBS prediction by using a support set of prototypes to match against new test samples. Unlike previous models, PMN focuses on learning the support set and uses a combinationLSTM to model how test samples match to embeddings. The PMN model combines few-shot matching and prototype feature learning to model TF-TF interactions in an end-to-end deep learning architecture. It uses a combinationLSTM to update input sequence embeddings and represent high-order label combinations through a weighted sum of prototype embeddings, mimicking underlying biology. This approach aims to learn prototype embeddings and model co-binding effects, introducing large-scale prototype learning in TFBS prediction. This paper introduces large scale prototype learning using a deep learning architecture for TF-TF interactions. The model significantly outperforms existing TFBS prediction baselines on a reference TFBS dataset with 2.1 million genomic sequences. The learned prototypes capture cooperative effects not previously modeled, and the model uses a combination of CNN and LSTM to match input sequences against prototypes. The final output is based on a concatenation of the LSTM output and matching read vector. The proposed model combines few-shot matching with large-scale prototype feature learning for TFBS prediction. It extends matching models to a multi-label task for genomic sequence classification and uses an attention LSTM module to model label interactions. The model is designed to mimic biological processes and classify DNA sequences as positive or negative binding sites for transcription factors. The model combines few-shot matching with large-scale prototype feature learning for TFBS prediction. It extends matching models to a multi-label task for genomic sequence classification and uses an attention LSTM module to model label interactions. The input sequence is encoded using a 3-layer CNN to produce sequence embeddings, matched to TF prototype vectors for motif representation. Interactions among TFs are modeled to make predictions. The model uses a combinationLSTM to match sequence embeddings to TF prototypes for TFBS prediction. The combinationLSTM incorporates TF interactions and uses \"hops\" for classification. The combinationLSTM utilizes \"hops\" to update the output vector based on matching TFs simultaneously. It accepts a constant x at each hop, updating the read vector and output hidden state based on cosine similarity with prototypes. The similarity score is transformed using a sigmoid function weighted by a hyperparameter to produce the final matching prediction. The LSTM with K hops updates the output vector by combining prototype vectors based on matching scores. It models TF combinations and fine-tunes to find TF binding combinations. The output vector can match multiple prototypes simultaneously, influencing other TFs in a sequential manner. The study introduces a prototype matching loss to force prototypes to correspond to specific TFs, using a linear transform and sigmoid function to calculate binding probabilities. It also utilizes classification and prototype matching losses to classify sequences and match prototypes to positive binding sequences. The study introduces a prototype matching loss to correspond to specific TFs, using RNN and LSTM to model label combinations dynamically. The loss is computed from final weights to attend to TFs at different hops before making decisions. The study introduces a prototype matching loss to correspond to specific TFs, using RNN and LSTM to model label combinations dynamically. The loss is computed from final weights to attend to TFs at different hops before making decisions. The hyperparameter \u03bb controls the mapping of prototypes to TFs, with the final loss being a combination of classification and prototype matching losses. The model was trained using Adam with a batch size of 512 sequences for 40 epochs, with results based on the best performing validation epoch. Dropout was used for regularization, and TF binding preferences are typically represented by position weight matrices derived from position frequency matrices. The curr_chunk discusses the limitations of motif-based PWMs in capturing subtle sequence signals in ChIP-seq data. It mentions the importance of TFBS prediction in bioinformatics and the use of discriminative approaches like string kernel methods for accurate results. The text also touches on the general category of biological sequence classification and the role of sequence analysis in bioinformatics. The curr_chunk discusses the use of Attention RNN and Memory Matching Network for tasks involving sequential inputs. It introduces a framework for tasks with non-sequential inputs using content-based attention with associative memory. The models focus on iterative attention processes over multiple 'episodes' to improve performance. The curr_chunk introduces the \"matching network\" (MN) model for few-shot learning, utilizing a dynamic attention mechanism over memory vectors. It compares hidden states with support set elements to find the closest match. The model uses prototype vectors instead of images for support set matching. The MN model and our model both use an LSTM for learning interactions in the support set. MN uses softmax attention, while we use sigmoid attention for multi-label output. Prototype theory suggests objects are recognized as a whole, with prototypes as abstract representations. Our method is inspired by prototype-matching theory, focusing on testing prototypes rather than exact features. The BID35 model introduces prototypical networks for zero and one-shot learning, where data points cluster around a single prototype for each class. Each prototype embedding is the average embedding of examples in that class, acting as a linear classifier with Euclidean distance metric. However, it lacks a recurrent-attention mechanism over the support set and does not consider interactions among classes. In contrast, our method learns embeddings for each label, playing crucial roles in classification. In multi-label classification, prototypes are learned for each label to model interactions among labels. BID31 proposed learning k-prototypes within each class, requiring a k-means classifier to group intra-class embeddings. Problem transformation methods for multi-label classification involve learning binary classifiers for each label. In multi-label classification, problem transformation methods involve learning binary classifiers for each label. BID30 proposes a chaining method to model label correlations, while BID49, BID7, and BID9 use graphical models to capture dependencies. The state-of-the-art model for object recognition leverages LSTM to characterize high-order label dependencies. The LSTM is used to model long-term dependency in a sequence for label prediction. The RNN generates an embedding for the next predicted label based on the current label prediction. StarSpace BID48 learns entity embeddings for multi-label outputs, while our method extracts relationships among outputs directly. The dataset was constructed from ChIP-seq experiments in the ENCODE project database, providing binding affinity p-values for specific locations in the human genome. We extracted 200-length windows surrounding peak locations for 86 transcription factors in the human lymphoblastoid cell line (GM12878) from the human genome. Positive binding peaks were identified based on a p-value threshold of 1. Binding site windows with >50% overlap with TF windows were considered positive. A total of 2,084,501 binding site windows were identified, covering about 14% of the human genome. Data validation was done using chromosomes 1, 8, and 21 as a validation set, chromosomes 3, 12, and 17 as a test set, and the rest for training. The dataset consists of binding site windows for 86 transcription factors in the GM12878 cell line. Positive training windows for each TF range from <1% to 23% of samples. About 40% of windows have multiple TF bindings, with an average of 5 TFs per window. The PMN model was tested using a 3-layer CNN model with specific kernel sizes. A summary of the dataset is shown in TAB4 and a per-TF summary in FIG0. The PMN model, compared to single-label and multi-label CNN models, outperforms based on all 3 metrics across 86 TFs in the GM12878 cell line. The PMN also significantly outperforms both CNN models using a pairwise t-test. Additionally, a PMN model without LSTM is implemented to demonstrate the effectiveness of the combinationLSTM module. In the PMN model, a softmax function is used for attention and a sigmoid function for the final output in multi-label classification. The model incorporates LSTM over K hops and variations of prototype loss. The combination of CNN, prototypes, and combinationLSTM is referred to as PMN, with K = 5 hops used for combinationLSTM. The model is compared against a baseline single-task model for each TF. The joint CNN model outperformed single label CNN models in auROC and auPR metrics for TFBS prediction. However, the improvement was not significant based on a one-tailed pairwise t-test. The joint model is faster than individual models for each TF but does not model interactions among TF labels. The PMN model, incorporating LSTM over K hops and prototype loss, outperformed both baseline CNN models. The PMN model, incorporating LSTM over K hops and prototype loss, outperformed baseline CNN models in all metrics. The PMN models showed faster convergence and better performance than CNN models, possibly due to accurate modeling of co-binding interactions. The per-epoch mean auROC results in FIG1 demonstrate the superiority of PMN over CNN models. Additionally, Table 5 shows TF clustering information and TFs belonging to the same cluster. The subsequent column displays TFs in the same cluster as TF-A, sharing target genes according to TRRUST database BID10. It shows the number of overlapping target genes and p-values for TF-TF cooperativity. Notably, TFs further from TF-A in Cluster 1 and 2 have lower p-values than closer TFs. The PMN model learns a prototype for each TF, representing motif-like features, processed by combinationLSTM capturing TF information. The combinationLSTM processes prototypes of TFs to capture TF-TF cooperativity information. Hierarchical cluster analysis identified clusters of TFs working together. TRRUST database BID10 shows TFs regulating the same target genes and their cooperativity significance. The combinationLSTM processes TF prototypes to capture cooperativity information. Hierarchical cluster analysis identifies TF pairs with significant cooperativity. Learned prototypes guide model predictions and provide insights into TF-TF cooperativity in gene regulation. Sequence analysis is crucial in bioinformatics for understanding how TFs bind to DNA. Researchers propose a novel prototype matching network (PMN) for learning motif-like features in TF proteins binding to DNA. The PMN uses a combinationLSTM to model label dependencies and mimic biological effects like co-binding. Results on a dataset of 2.1 million genomic strings show that the PMN outperforms baseline variations, validating design choices. The PMN model is a general classification approach applicable beyond TFBS applications, as demonstrated on the MNIST dataset with convincing results. The PMN model, designed for learning motif-like features in TF proteins binding to DNA, shows promising results on the MNIST dataset. Future directions include extending PMN for different cell types and adding more domain-specific features for better genomic sequence representations. The PMN model, designed for learning motif-like features in TF proteins binding to DNA, shows promising results on the MNIST dataset. When compared to a standard CNN model on MNIST, the PMN model does not show a drastic improvement in performance but converges faster and produces better separated embeddings. This is likely due to the PMN's use of a similarity metric and its ability to update its output based on which number prototypes it matches to. The PMN model can update its output based on which prototypes it matches to, without being constrained to only one prototype."
}