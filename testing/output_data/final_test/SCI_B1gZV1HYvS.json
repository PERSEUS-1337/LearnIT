{
    "title": "B1gZV1HYvS",
    "content": "In multi-agent systems, complex behaviors arise from high correlations among agents. Previous work on modeling interactions from demonstrations is limited by assuming independence among policies and rewards. In this paper, the Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL) is introduced for modeling multi-agent interactions. CoDAIL approximates opponents' policies to regenerate similar interactions, allowing for decentralized training and execution. Experimental results show that CoDAIL outperforms existing multi-agent imitation learning methods in regenerating complex interactions. The code for CoDAIL is available at \\url{https://github.com/apexrl/CoDAIL}. Multi-agent reinforcement learning (MARL) is commonly used for the multiagent learning problem in systems with adaptive agents. MARL has shown progress in tasks like strategy games and traffic light control. A key challenge in MARL is defining a good learning goal due to correlated rewards among agents. Imitation learning, like behavior cloning, can be a solution for learning policies directly from demonstrations. Generative adversarial imitation learning (GAIL) offers a model-free approach without compounding error, making it effective and scalable. However, real-world multi-agent interactions pose challenges due to strong correlations among agents' policies and rewards. In scenarios like a football coach aiming to win the league, targeted tactics against various opponents and team situations are crucial. The multi-agent environment exacerbates compounding issues. In this study, the problem of modeling complex multi-agent interactions from offline demonstrations is explored to generate online policies that replicate similar behaviors. Previous research in multi-agent imitation learning has focused on isolated reward structures and independent per-agent policies, overlooking the high correlations among agents. This paper proposes a multi-agent imitation learning framework with correlated policies to approximate opponents' actions in order to address challenges in modeling multi-agent interactions. The study introduces a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL) to address challenges in modeling multi-agent interactions. The framework allows for decentralized training and execution, treating demonstrator interactions as Nash Equilibrium solutions. Experimental results show that CoDAIL outperforms other multi-agent imitation learning methods in recovering correlated multi-agent policy interactions. The study introduces CoDAIL, a decentralized adversarial imitation learning algorithm for modeling multi-agent interactions. CoDAIL outperforms other methods in recovering correlated multi-agent policy interactions in various scenarios. Markov game (MG) is defined as an extension of Markov Decision Process (MDP) with N agents, action spaces, state transitions, initial state distribution, and discounted factor. In Markov games, the reward function for each agent depends on joint agent actions, leading to optimal policies depending on others' policies. Nash equilibrium (NE) is extended to -Nash equilibrium (-NE) in Markov games, where every NE is equivalent to an -NE with a specific value. Imitation learning aims to learn policies directly from expert demonstrations. Imitation learning aims to learn policies directly from expert demonstrations, with behavior cloning (BC) and inverse reinforcement learning (IRL) being the main approaches. In multi-agent settings, demonstrations are interrelated trajectories sampled from interactions among all agents. Recent work focuses on learning policies without estimating the reward function, as IRL is less efficient due to resolving an RL problem inside the learning loop. Recent work in imitation learning aims to learn policies without directly estimating the reward function. GAIL utilizes Generative Adversarial Networks (GAN) to treat IRL as the dual problem of occupancy measure matching. In multi-agent tasks, each agent makes independent decisions, but the cumulative return depends on the joint policy \u03c0. One common method is to decouple the \u03c0 assumption. Recent work in multi-agent settings aims to address the vulnerability of assuming conditional independence of actions from different agents in the joint policy. By considering opponents and decoupling the joint policy as a correlated policy conditioned on state, agents can make more stable decisions to maximize cumulative rewards. In multi-agent settings, the goal is to maximize cumulative rewards against opponents with demonstrated policies through reinforcement learning. An IRL procedure is defined to find a reward function that ensures the demonstrated joint policy outperforms others. Occupancy measure is introduced to navigate agent interactions, and the distribution of pairs is formulated from the agent's perspective. Proposition 1 states that IRL for demonstrator opponents involves a dual form of occupancy measure matching problem with regularizer \u03c8. By setting \u03c8 = \u03c8GA, a GAIL-like imitation algorithm can be obtained using adversarial training procedures of GANs. However, accessing the policies of demonstrator opponents is impractical, so accessible counterparts are proposed to address this issue. To address the deficiency of accessing demonstrator opponents' policies, Proposition 2 suggests using accessible counterparts with importance sampling. This method helps quantify opponent actions, although estimating densities can lead to variance in learning. Setting \u03b1 = 1 in implementation has shown no significant impact on performance. The approach is similar to Kostrikov et al. (2018) and can be applied to various policy settings without prior considerations. The multi-agent imitation learning framework built allows the discriminator to learn without constraints. The multi-agent imitation learning framework allows the discriminator to learn implicit goals for each agent by updating policies and discriminators alternately. Opponents' actions are estimated using opponent modeling to find the optimal response, as accessing opponent joint policies is unrealistic. In a multi-agent imitation learning framework, opponents' actions are estimated using opponent modeling to find the optimal response. A function is constructed to approximate opponents for each agent, allowing for training in a fully decentralized manner. The algorithm, Decentralized Adversarial Imitation Learning with Correlated policies (Correlated DAIL), utilizes supervised learning to infer opponents' models and can be easily scaled to a distributed algorithm. The reinforcement learning objective against demonstrator counterparts is essentially equivalent to reaching a Nash Equilibrium (NE). The RL procedure can be seen as a single-agent problem when fixing the policies of other agents. Similarly, the IRL process becomes a single-agent problem, recovering an optimal reward function that achieves the best performance. This leads to the -NE solution concept, where random policies with vast entropy are not always considered sub-optimal. The CoDAIL approach aims to achieve a close-to-optimal solution by removing random policies from the candidate policy set and controlling the hyperparameter \u03bb. It assumes that demonstrated policies represent an -NE solution concept that can be controlled by \u03bb under specific reward functions. This contradicts the claim that NE is incompatible with MaxEnt IRL, as demonstrated opponents in multi-agent MaxEnt IRL can lead to optimal actions. The multi-agent MaxEnt IRL is equivalent to finding an -NE with demonstrator opponents. Previous methods like MADDPG, COMA, and MA Soft-Q lack in modeling complex interactions due to independent policy assumptions. Opponent modeling is crucial for rational behavior, but prior works in multi-agent imitation learning struggle with learning from complicated demonstrations. Prior works in imitation learning struggle with learning from complex demonstrations and are limited by specific reward assumptions. Parameter Sharing Generative Adversarial Imitation Learning (PS-GAIL) extends GAIL for multi-agent problems but does not leverage Markov games' properties. Various works in Markov games are constrained by tabular representation, known dynamics, and specific reward structures for different game types. Recently, researchers have developed multi-agent GAIL (MA-GAIL) and multi-agent adversarial inverse reinforcement learning (MA-AIRL) to address challenges in modeling agent interactions. However, these approaches struggle with correlated policies and lack a fully decentralized training procedure. In contrast, our approach can generalize correlated policies from demonstrations without needing to know specific opponent policies. Some works focus on modeling multi-agent interactions by learning policy representations based on interactions and generalization tasks. Grover et al. (2018) learn a policy representation function from agent interactions, while Kuhnt et al. (2016) and Gindele et al. (2015) use Dynamic Bayesian Models to describe physical relationships among vehicles in autonomous driving scenarios. In this paper, the focus is on imitation learning with correlated policies in autonomous driving scenarios. The approach involves opponent modeling to infer others' actions using historical trajectories. The method is tested on Particle World Environments, a benchmark for evaluating multi-agent algorithms. The study evaluates multi-agent algorithms in various cooperative and competitive scenarios, such as cooperative-communication, cooperative-navigation, keep-away, and predator-prey tasks. The study evaluates multi-agent algorithms in cooperative and competitive scenarios like keep-away and predator-prey tasks. To compare interaction quality, demonstrations are obtained from correlated policies using a MARL algorithm with opponents modeling. The modified ACKTR algorithm includes an opponents model and conditioned policy for each agent, transforming centralized learning to decentralized. Experts are not required for the designated environments. In the study, CoDAIL is compared with other algorithms like MA-AIRL, MA-GAIL, and NC-DAIL in various tasks without prior reward structure. Training involves obtaining demonstrator policies, generating demonstrations, and using surrogate rewards from discriminators. Training procedures are pre-trained via behavior cloning with 200 episodes of demonstrations. Tab. 1 and Tab. 2 display the results. CoDAIL outperforms other algorithms in cooperative and competitive tasks by achieving smaller reward gaps, indicating robust imitation learning capabilities. Conflict goals in competitive tasks lead to more complex interactions compared to shared goals in cooperative tasks. MA-GAIL and NC-DAIL show similar performance, suggesting the surrogate reward structure is less significant in multi-agent scenarios. MA-AIRL does not perform well in some environments, including Predator-prey scenarios. Raw rewards are listed in Appendix C, with hyperparameter sensitivity results in Appendix D. Evaluating interactions between agents involves collecting positions over state-action tuples. Experiments start from different initial states but with the same seed and run for 100 episodes. Position distributions are estimated for each agent in episodes with a maximum of 50 timesteps. CoDAIL outperforms other baseline methods in generating interaction data with minimum gap to demonstration interactions, as shown in Tab. 3. MA-GAIL and NC-DAIL perform similarly in modeling complex interactions, while MA-AIRL performs the worst, even worse than random agents in Predator-prey scenarios. Visualizations of interactions for demonstrator policies and learned policies show differences in position distributions. The density distribution of positions (x, y) and marginal distributions of x-position and y-position are plotted. Interaction densities of demonstrators and CoDAIL agents are highly similar, while other learned agents fail to recover the demonstrator interactions. Different policies can interact to earn similar rewards but still have vast differences in their generated interactions. The real reward may not be the best metric to evaluate the quality of modeling demonstrated interactions or imitation learning. In this paper, a decentralized adversarial imitation learning algorithm called CoDAIL is developed to model complex multi-agent interactions via imitation learning. It allows for decentralized training and execution, showing better performance in modeling correlated interactions compared to other state-of-the-art methods. Future work will focus on covering more imitation learning tasks and modeling latent variables of policies for diverse multi-agent imitation learning. The experiments on Communication-navigation showed that changing the training frequency of D and G affects the total reward difference between learned agents and demonstrators. The choice of \u03bb had little impact on performance due to the discrete action space. The density and distribution of agents' positions were analyzed in Cooperative-navigation experiments under the same random seed. KL divergence was used to measure differences in policies. In Predator-prey experiments, the density and marginal distributions of agents' positions were analyzed in 100 repeated episodes with different initialized states. KL divergence was used to compare generated interactions with demonstrators."
}