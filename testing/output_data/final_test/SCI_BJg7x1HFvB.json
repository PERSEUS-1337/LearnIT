{
    "title": "BJg7x1HFvB",
    "content": "Recent developments in natural language representations have led to the creation of large and expensive models that utilize vast amounts of text for self-supervised pre-training. Model compression techniques have been proposed to reduce the cost of applying these models to downstream tasks. However, the simple approach of pre-training and fine-tuning compact models has been overlooked. This paper demonstrates the importance of pre-training for smaller architectures and shows that fine-tuning pre-trained compact models can be competitive with more complex methods. Additionally, the Pre-trained Distillation algorithm, which transfers task knowledge from large fine-tuned models through knowledge distillation, further improves performance. Through extensive experiments, the interaction between pre-training and distillation is explored under model size and unlabeled task data properties. Sequentially applying these techniques on the same data has a compound effect. To reduce computational costs, 24 pre-trained miniature BERT models will be made publicly available. The two-staged training approach of self-supervised learning on a general-domain text corpus followed by end-task learning has enabled advancements in language understanding. The goal is to achieve similar gains with restricted memory. Our goal is to achieve gains with restricted memory by training small models directly using language model pre-training and task fine-tuning. This approach has been overlooked in the NLP community but shows promising results. The surprising finding is that pre-training+fine-tuning is a competitive method for building compact models. Knowledge distillation is used for model compression, where a compact student learns from a highly accurate teacher. This approach involves masked LM pre-training, task-specific distillation, and optional fine-tuning, referred to as Pre-trained Distillation (PD). In a study comparing Pre-trained Distillation (PD) with other methods, it outperformed or matched more complex approaches. The paper's significant contribution lies in extensive experiments exploring PD and its baselines in various conditions, focusing on model size and performance. In a study comparing Pre-trained Distillation (PD) with other methods, it outperformed or matched more complex approaches. The paper's significant contribution lies in extensive experiments exploring PD and its baselines in various conditions, focusing on model size and performance. Investigating model size and unlabeled data quality, the study found that pre-trained students leverage depth better than width, and Pretrained Distillation is more robust to variations in the transfer set than standard distillation. Additionally, chaining LM pre-training and task-specific distillation on the same dataset showed a compounding effect, outperforming either operation in isolation. The study found a compounding effect when chaining LM pre-training and task-specific distillation on the same dataset, indicating complementary learning aspects. The authors will make their 24 pretrained miniature BERT models publicly available to accelerate research on accurate models within memory and latency constraints. The focus is on exploring the parametric form of compact models and training data characteristics to build effective models for NLP tasks. In NLP tasks, a highly accurate but large model serves as the teacher, while compact models known as students satisfy resource constraints. Labeled data consists of training examples for NLP tasks, which are challenging to produce. During distillation, a teacher model transfers knowledge to a student model using unlabeled transfer data. This data set includes input examples similar to the labeled set but without true labels, making it easier to produce and larger in size. However, task-relevant input text is not readily available for certain NLP tasks like natural language inference and question answering. Additionally, input data distribution shifts over time for deployed systems, making existing unlabeled data outdated. Unlabeled language model data enables unsupervised learning of text representations through pre-training with a masked language model objective. These corpora are vast and contain millions of words, serving as a resource for building compact models using Pre-trained Distillation (PD) algorithm. The distinction between labeled, unlabeled, and transfer datasets is functional, with the same corpus potentially being part of multiple sets. The compact model training process involves pre-training on a masked LM objective, distillation on soft labels, and optional fine-tuning to improve robustness. Pre-training captures linguistic phenomena from a large corpus, while distillation leverages teacher expertise. Fine-tuning helps mitigate mismatches between transfer and labeled sets. The two-step algorithm is referred to as PD, and the three-step algorithm as PDF. Test results are evaluated on the GLUE server using the best-performing model on dev. Results for MLM pre-training followed by fine-tuning (PF) are provided for anchoring. The meta score is computed on 6 tasks only and is not directly comparable to the GLUE leaderboard. The large teachers are treated as black boxes, produced by pre-training and fine-tuning, with potential knowledge transfer through distillation. It is unclear whether pre-training the student would bring additional benefits. Patient Knowledge Distillation (Sun et al., 2019a) and DistilBert (Sanh, 2019) are two approaches that leverage pre-training and distillation to build compact models. They depart from traditional methods by initializing the student from the bottom layers of a pre-trained model and incorporating task-specific distillation. The training objective considers both teacher output and intermediate layers, making assumptions about student and teacher architectures. In a parallel line of work, DistilBert applies truncation-based initialization for the student, continues LM pre-training via distillation from a more expensive LM teacher, and fine-tunes on task data. LM distillation is computationally expensive due to the softmax operation over the entire vocabulary. The initialization strategy constrains the student to the teacher embedding size. Table 2 summarizes differences between concurrent work and Pre-trained Distillation (PD). An experiment is performed with a 6-layer BERT student and a 12-layer BERT BASE teacher, both with an embedding size of 768. For distillation, the transfer set matches the labeled set. Results on 6 GLUE tasks show that PD performs best. Pre-training+fine-tuning is competitive with larger datasets. PF maintains generality and simplicity but lacks leverage on unlabeled data. Further analysis explores model sizes and unlabeled data properties. All models follow the Transformer architecture. In Section 4, models follow the Transformer architecture and input processing of BERT. Models are denoted by their hidden layers (L) and hidden embedding size (H). The number of self-attention heads is fixed to H/64 and feed-forward size to 4H. End-task models have a linear classifier on top. Experimenting with 24 student models, sizes and latencies vary. The most expensive student is 3 times smaller and 1.25 times faster than the teacher, while the cheapest is 77 times smaller and 65 times faster. Results are reported on 5 selected students, but conclusions hold across all 24 models. Knowledge Distillation (Figure 4b) transfers information from a highly-parameterized teacher model to a more compact student model by exposing the student to soft labels for better generalization. Soft labels are class probabilities produced by the teacher model, controlled by a constant called temperature. The soft labels from a teacher model enable better generalization for training 24 students of various sizes. Pre-training+Fine-tuning leverages large unlabeled corpora to pre-train models before fine-tuning for end tasks, showing benefits for compact architectures. Sentiment classification is performed on various datasets including Amazon Book Reviews and SST-2. The algorithm is tested on unlabeled transfer data from Amazon Movie Reviews. The results are compared with prior work on the GLUE leaderboard. Natural language inference involves classifying pairs of sentences as entailment, contradiction, or neutral. MNLI is chosen as the target dataset, supplemented with SNLI and QQP. Textual entailment is similar to NLI but focuses on binary classification. The RTE dataset is much smaller than MNLI, providing a robustness test for transfer data. Experiments are conducted to understand the success of Pre-trained Distillation. Experiments show that LM pre-training is essential for maximizing student potential in Pre-trained Distillation. Comparing PD with pre-trained word embeddings to PD with pre-trained word embeddings and Transformer layers proves the importance of pre-training Transformer layers. In Pre-trained Distillation, pre-training Transformer layers is crucial for maximizing student potential. Less than 24% of the gains from PD can be attributed to pre-trained word embeddings, with the rest coming from pre-training Transformer layers. Students initialized via LM pre-training outperform those initialized from the bottom layers of pre-trained models. Truncating deep pre-trained models, especially for shallow students, leads to degraded performance. Pre-trained Distillation emphasizes the importance of pre-training Transformer layers for maximizing student potential. Truncating deep pre-trained models, especially for shallow students, can lead to degraded performance. Prioritizing depth over width is recommended, especially with pre-trained students. A comparison of 24 student model architectures on SST-2 shows how different students utilize model capacity. They are sorted by hidden size and number of layers to determine the best student for a fixed parameter size budget. The quality of randomly initialized students is closely correlated with the number of parameters. Pre-trained models are much more effective at utilizing more parameters and depth, as shown by the sharp drops in performance when moving to wider but shallower models. Truncating pre-trained models can lead to suboptimal parameter distribution, with deeper models performing better than wider ones. In this section, the importance of distillation and fine-tuning after LM pretraining is highlighted. Pre-trained Distillation outperforms baselines on NLP tasks, showing the value of using a large unlabeled LM set for pre-training and distillation. Pre-trained Distillation achieves superior performance on NLP tasks using a large unlabeled LM set for pre-training. It outperforms baselines, particularly excelling on the Amazon Book Reviews corpus with significant model size reduction and speed-up. Additionally, Distillation improves Transformer TINY by over 5% absolute on RTE, showcasing its effectiveness in compressing models compared to traditional distillation methods. Pre-trained Distillation outperforms baselines on NLP tasks, especially on Amazon Book Reviews, with model size reduction and speed-up. It excels on sentence-pair tasks like MNLI and RTE, but performs better on Amazon Book Reviews with a large transfer set. Distillation is consistently best overall, showing robustness to transfer set size. Distillation for speech recognition relies on a large transfer set, as shown in experiments on Amazon Book Reviews. With a smaller model and less transfer data, Pre-trained Distillation matches the teacher model's performance, outperforming traditional distillation methods. The impact of domain shift on distillation is explored, with a focus on out-of-domain data degrading distillation. An algorithm is proposed that is more robust to mismatches between training and transfer sets. Domain shift is measured using the Spearman rank correlation coefficient, with a procedure outlined for computing corpus similarity between datasets. The impact of domain shift on distillation is explored, with a focus on out-of-domain data degrading distillation. The study shows that distillation requires a large transfer set, with PD achieving the same performance as TransformerMINI with a smaller transfer set. The effect of domain shift is measured by varying the source of unlabeled text used for distillation. Transfer set domains range from not task-related to related categories like movie reviews. Results in Figure 9 demonstrate a correlation between accuracy and the Spearman coefficient for distillation and PD. PD outperforms distillation when using reviews from unrelated products as a transfer set. Even when pre-training and distilling on the same dataset, PD still outperforms the baselines. The interaction between pretraining and distillation is investigated by applying them sequentially on the same data. Pre-training and fine-tuning with D LM = X and pre-trained distillation with D LM = D T = X show that the compound effect still exists. For MNLI, setting D LM = D T = NLI* and distilling students on D T = NLI* results in PD outperforming PF by 2.2% on average. Even when pretraining and distilling on the same data, PD is better than training strategies applied in isolation, indicating that the two methods learn different linguistic aspects beneficial for the end task. In recent research, there has been a shift towards fine-tuning methods for large pre-trained representations for end tasks. While feature-based unsupervised representations have been successful in compact models, the pretraining+fine-tuning approach has not been extensively studied for small models. This work builds on model compression and knowledge distillation, experimenting with transferring information from a teacher to a student model through sharing intermediate layer activations. In this study, the use of large-scale general domain text for pre-training is shown to be successful. Additionally, techniques like pruning and quantization are explored for deriving smaller models, with potential gains expected to complement the pre-training process. Distillation with unsupervised pre-training is also discussed, where pre-trained word embeddings are used as inputs to students. Prior work has focused on pre-training student models using various techniques such as ELMo embeddings and context-independent word embeddings. Some studies have initialized Transformer students from BERT model layers and continued pre-training via distillation from a more expensive LM teacher. Other approaches involve deriving a single model for multiple tasks through distillation. However, the impact of unsupervised learning on student models in relation to model size and transfer set domain has not been thoroughly analyzed."
}