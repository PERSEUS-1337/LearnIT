{
    "title": "HyxTJxrtvr",
    "content": "Understanding object motion in computer vision involves segmenting and tracking objects over time. While there has been progress in instance segmentation, current models struggle to track objects and reason in both 3D space and time. Our model introduces a spatio-temporal embedding loss for video instance segmentation, incorporating a temporal network to capture motion and temporal context. It also estimates monocular depth to ensure time-consistent embeddings. The model excels in tracking and segmenting instances, even with occlusions and missed detections, outperforming on the KITTI Multi-Object and Tracking Dataset. Predicting actor motion in dynamic scenes is crucial for intelligent systems, mimicking how humans track moving objects using appearance, relative distance, and temporal consistency. The importance of incorporating temporal consistency in object segmentation and tracking is highlighted in computer vision. Datasets like COCO, Cityscapes, and Mapillary Vistas have advanced instance segmentation, but lack temporal annotations for training video models. Real-world navigation requires understanding object motion through segmenting instances, estimating depth, and tracking objects over time. Incorporating temporal consistency in object segmentation and tracking is crucial in computer vision. The KITTI Multi-Object and Tracking Dataset provides video instance segmentation annotations for training models. A new spatio-temporal embedding loss, along with deep temporal convolutional neural networks and self-supervised depth loss, produces consistent instance segmentations over time. The novel contributions include a spatio-temporal embedding loss for video instance segmentation, a temporal model for improved consistency over time, disambiguation of objects with self-supervised monocular depth loss, and handling occlusions without IoU based instance correspondence. The method advances state-of-the-art on the KITTI MultiObject and Tracking Dataset by using embedding-based instance segmentation, capturing inter-object relations with multi-modal cues. The Multi-Object Tracking (MOT) challenge aims to infer object trajectories using appearance, motion, and interaction cues. Researchers have developed a \"tracking-by-detection\" paradigm to connect detections across video frames. Large-scale tracking datasets are essential for training deep networks but are costly and time-consuming to collect. Vondrick et al. (2018) introduced video colourisation as a self-supervised method for visual tracking, using a pointing mechanism to copy colors from a reference frame. This model is more robust than optical flow based models, especially in complex scenes. Voigtlaender et al. (2019) extended multi-object tracking to include segmentation, introducing the KITTI MOTS dataset with pixel-level instance segmentation on over 8,000 video frames. The researchers extended Mask R-CNN with 3D convolutions for temporal information and an association head for association vectors. They collected datasets for video instance segmentation and introduced a synthetic dataset for object spatial extent estimation. Instance segmentation methods using embeddings map all pixels of an instance to a high-dimensional space. In 2018, embedding-based instance segmentation methods address limitations of region-proposal methods by mapping pixels of instances to a high-dimensional space. A spatio-temporal embedding loss is proposed to extend instance embedding to video, transforming pixels into unique locations based on appearance, context, and motion. The loss includes attraction and repulsion terms to ensure proximity of pixels from the same instance. The loss function includes attraction and repulsion terms to ensure proximity of pixels from the same instance in the video. It also includes a regularisation term to prevent instance centers from diverging too much from the origin. The attraction radius \u03c1a and repulsion radius 2\u03c1r are defined to control the embedding distances within and between clusters. The spatio-temporal embedding loss includes attraction, repulsion, and regularisation terms. Inference involves assigning pixels to instances using the mean-shift algorithm. Relative distance of objects helps segment instances in space and time. Depth estimation with supervised methods requires high-quality annotated data. Depth estimation in challenging environments can be improved by leveraging self-supervised depth losses from monocular videos. A separate pose estimation network is trained with a depth network to infer ego-motion during training. Scenes are assumed to be mostly rigid, with appearance changes attributed to camera motion. Training signal comes from novel view synthesis, with pixels violating assumptions masked to avoid infinite holes during inference. The training signal for depth estimation comes from novel view synthesis, generating new images from different camera poses. The view synthesis loss is calculated using the predicted depth and camera transformation. The projection error function includes L1, SSIM, and smoothness terms. The source image pixel coordinates are obtained using the camera intrinsic matrix and mapping to camera coordinates. Our model architecture consists of three components: an encoder, temporal model, and decoders. Frames are encoded for a more powerful representation, the temporal model learns scene dynamics, and decoders output the reconstructed image using a sampling mechanism to reduce projection errors and improve view synthesis. The model architecture includes an encoder, temporal model, and decoders. The temporal model learns scene dynamics using a causal 3D convolutional network, while the decoders output instance embedding and depth prediction. The temporal model in the architecture slowly accumulates information over time from previous encodings, trained efficiently with convolutions for parallel computations. Decoders map the temporal encoding to instance embeddings, pushing together embedding values belonging to the same instance in a high-dimensional space. The architecture accumulates information over time by comparing mean embeddings of segmented instances, using a distance threshold for matching. Segmented embeddings are clustered using mean shift to find dense regions, improving clustering over time. Embeddings have a lifespan corresponding to the model's sequence length for smooth variation. Pose network uses ResNet and mask network uses an encoder-decoder model. The architecture utilizes a ResNet for the pose network and an encoder-decoder model based on ResNet for the mask network. Experimental results show the method's performance on the KITTI Multi-Object Tracking and Segmentation dataset, containing 8,008 frames with instance segmentation labels for 26,899 cars. The dataset allows for training video instance segmentation models with consistent instance ID labels across time. The KITTI MOTS dataset is the focus due to its consistent video instance segmentation. Input images are halved to 3 \u00d7 192 \u00d7 640 resolution for encoding. The resulting encoding is 128 \u00d7 24 \u00d7 80, with decoders mapping temporal encoding to instance embedding. The decoders map temporal encoding to instance embedding with specific dimensions and train with a sequence length of 5 for 0.5 seconds of temporal context. Loss function parameters are set, and multi-object tracking and segmentation metrics are defined for quality assessment. Instance segmentation ensures each pixel is assigned to one instance for accurate predictions. The set IDS of ID switches is defined as the set of ground truth masks whose predecessor was tracked by a different ID. Following Voigtlaender et al. (2019), MOTS metrics include MOTSP, MOTSA, and sMOTSA for multi-object tracking and segmentation precision. The model is compared to baselines for video instance segmentation, such as Single-frame embedding loss and spatio-temporal embedding loss. Average precision (AP) is also measured. The curr_chunk discusses the comparison of different models on the KITTI MOTS validation set, focusing on static detection metrics and temporal consistency. The best model shows significant improvement in temporal consistency metrics (MOTSA and sMOTSA) compared to baseline approaches. Our best model significantly improves temporal consistency metrics (MOTSA and sMOTSA) over baselines by incorporating temporal context and local motion for a better spatio-temporal embedding. Additionally, the model's ability to predict both spatio-temporal embedding and monocular depth results in the best performance, outperforming Mask R-CNN on temporal metrics despite lower detection accuracy. The model demonstrates temporal consistency quality by relying on mask segmentation for instance clustering. Using ground truth masks improves performance significantly. Clustering with MeanShift algorithm is affected by noisy embeddings, but clustering with ground truth mean boosts evaluation metrics. Our model utilizes ground truth masks for instance clustering, outperforming a model using ground truth instance embedding mean clustering. The clustering algorithm accumulates embeddings from past frames, creating an attraction force for consistent instance matching. Instance tracking is achieved by matching detected instances to previous instances based on embedding distance. The spatio-temporal embedding changes smoothly over time, but collapses when trained on sequences that are too long. The model can effectively learn short-term motion features, but struggles with long-term cues due to loss preventing smooth shifting of the embedding. The optimum sequence length for instance segmentation on the dataset is five, providing consistent clustering across frames in both space and time. The model demonstrates robust tracking through various scenarios, including partial and full occlusion, noisy detections, and visualizations of RGB input, ground truth segmentation, predicted segmentation, and monocular depth. Additional examples and failure cases are available in the appendix. The embedding is visualized in 2D and 3D, showing mean shift clustering and incorporating depth context to improve quality, especially in complex scenarios like occlusion. A new spatio-temporal embedding loss is proposed for consistent instance segmentation over time, with the model effectively tracking occluded instances. Our model effectively tracks occluded instances and missed detections by leveraging temporal context, advancing state-of-the-art in video instance segmentation on the KITTI Multi-Object and Tracking Dataset. The encoder is a ResNet-18 convolutional layer with 128 output channels, while the temporal model consists of 12 residual 3D convolutional blocks. The decoders for instance embedding and depth estimation have 7 convolutional layers with specific channel configurations. The model includes 7 convolutional layers with specific channel configurations for instance embedding and depth estimation, as well as 3 upsampling layers. During training, a depth masking technique is applied to remove pixels that violate the rigid scene assumption. The pose network utilizes a ResNet-18 model followed by 4 convolutional layers to output a 6-DoF transformation matrix. Additionally, a separate mask network is trained to mask the background. The video instance segmentation model benefits greatly from depth estimation, resulting in a more structured embedding and improved object tracking in challenging scenarios like occlusion. Without depth estimation, the model incorrectly tracks objects, while with depth estimation, it learns a consistent embedding based on appearance and 3D geometry, leading to better results."
}