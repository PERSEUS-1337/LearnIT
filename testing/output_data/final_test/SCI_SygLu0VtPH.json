{
    "title": "SygLu0VtPH",
    "content": "Evolutionary-based optimization approaches have shown promise in various domains, but less so in solving 3D tasks directly from pixels. This paper introduces Deep Innovation Protection (DIP), a method for training complex world models in 3D environments. By using multiobjective optimization, DIP reduces selection pressure on specific components, allowing other components to adapt. The evolved networks learn a model of the world without the need for forward-prediction loss, similar to how the brain models the world through evolution for survival. The current world model approaches are often rewarded for predicting future states of the environment. This paper explores the emergence of representations through artificial evolution, focusing on the world model architecture introduced by Ha & Schmidhuber (2018) with visual, memory, and controller components. The paper explores the world model architecture with visual, memory, and controller components. It questions the scalability of training these components end-to-end using a genetic algorithm, as demonstrated in a 2D car racing domain. The study shows that a simple genetic algorithm fails to solve the more complex 3D VizDoom task, prompting the search for missing ingredients to evolve more powerful world models. The paper discusses the optimization of heterogeneous neural networks like world models as a co-evolving system of multiple subsystems. It suggests reducing selection pressure on individuals whose visual or memory system was recently changed to allow time for readaptation, inspired by a morphological innovation protection method. The approach allows for scalable co-optimization of controllers and robot body plans, finding a solution to the VizDoom: Take Cover task. The emergent world models predict events crucial for agent survival without explicit training. DIP enables evolution to orchestrate training of components in heterogeneous architectures, inspiring research on representations from non-gradient-based optimization approaches. Innovations in world models require careful tuning of components to work together effectively. The agent model includes a sensory component, memory component, and controller based on a world model approach. The sensory component compresses sensory information into a representative code, which is then used by the memory component to predict future codes. Both components feed into a controller to determine the agent's actions. The approach in this paper trains heterogeneous neural systems end-to-end by reducing selection pressure on individuals with recently changed modules, allowing other components to adapt. Mutations in the world model can affect the VAE, MDN-RNN, or controller, and selection pressure is reduced to give the controller time to readapt. This approach is inspired by multi-objective morphological innovation protection. In their approach, inspired by multi-objective morphological innovation protection, Cheney et al. (2018) utilize the NSGA-II optimization method to track the age of mutations in the VAE or MDN-RNN. The age objective resets to zero when a mutation occurs, favoring individuals with less time to adapt. The second objective is the accumulated reward during an episode, with pseudocode provided in Algorithm 1. This method differs from traditional multiobjective optimization by using age to prioritize individuals with less adaptation time. In this paper, the age of individuals in the population is tracked to measure adaptation time of the controller component. The visual and memory components are optimized through a genetic algorithm in an end-to-end fashion, without individual evaluation. The focus is on neural representations that emerge to solve tasks, following the original world model approach. In the experiments, an agent is trained to solve car racing and VizDoom tasks from 64\u00d764 RGB pixel inputs using a genetic algorithm. A visual component produces a latent code z t at each time step, which is combined with the hidden state h t of the memory component to determine the agent's next action. The agent model is trained with a multiobjective genetic algorithm using input (z t , h t ) to determine actions. In CarRacing-v0, the agent navigates procedurally generated tracks, receiving rewards for visiting track tiles. The network controlling the agent has outputs for steering, acceleration, and braking. In VizDoom:Take Cover, the agent avoids fireballs by strafing left or right to stay alive for 2,100 timesteps. The network has an output for strafing. In a genetic algorithm approach, the agent's controller outputs actions for left and right strafing or standing still to survive in a domain. The NSGA-II method selects individuals for the next generation through tournament selection without crossover. The population size is 200, and randomness is accounted for by evaluating the top three individuals of each generation multiple times. Different approaches are compared, with the age objective reset when components preceding the controller change. The age objective is set to zero if the controller changes in the network. Different setups test optimizing neural models in tasks like CarRacing-v0 and VizDoom: Take Cover. One setup resets age if the MDN-RNN changes, while another assigns a random age objective. The Standard GA without innovation protection is also compared. In a study comparing different setups for optimizing neural models in tasks like CarRacing-v0 and VizDoom: Take Cover, the Standard GA without innovation protection was found to perform significantly worse in the more complex VizDoom domain. This suggests that fine-tuning individual components in the overall world model architecture is crucial for more complex tasks. The study found that innovation protection in the DIP approach outperformed other methods in the VizDoom task, achieving an average score of 824.33. Saliency maps were used to analyze the agent's decision-making process, showing attention to walls, fireballs, health, and ammo indicators. The perturbation-based saliency maps measure how model output changes with altered input. The agent focuses on walls, fireballs, and monster positions. Increasing diversity in the population improves performance, but selectivity resetting the age objective is more effective. Protecting innovations upstream in the network is crucial for downstream components. The study investigates the type of world model that can emerge without optimizing for forward prediction or reconstruction loss. Insights into learned representations are gained using t-SNE. The study investigates the type of world model that can emerge without optimizing for forward prediction or reconstruction loss. Insights into learned representations are gained using t-SNE dimensionality reduction technique to visualize the inner workings of deep neural networks. The compressed 32-dimensional vector of the VAE and the hidden states of the MDN-RNN are of particular interest, as they are fed into the controller that decides on the agent's action. Different combinations of sequences of these latent vectors collected during one rollout are visualized in two dimensions, showing distinct classes for moving left and right. The temporal dimension captured by the recurrent network proves crucial. The recurrent network's temporal dimension is crucial for decision-making in the agent's actions. Analyzing the learned temporal dynamics involves looking at the variance of activation levels to determine their importance in the agent's controller component. The LSTM activations in the agent's controller component indicate critical situations where the agent needs to pay attention to predictions. The forward model reacts to fireballs when the agent is in their line of impact, crucial for survival. Evolutionary innovations in solving the VizDoom task involve learning to avoid fireballs in the first 30 generations. In generation 30, the agent learns to pay attention to fireballs and starts avoiding them by moving left or right. By generation 56, the agent can differentiate well between situations, surviving the entire episode. Weight distances of the best-performing networks are analyzed, showing the VAE component has the steepest decrease in distance. The VAE component shows the steepest decrease in distance, with a noticeable jump around generation 60. The MDN-RNN is optimized slowest, suggesting that DIP can orchestrate the training of heterogeneous world model architectures in an automated way. Generation 34 shows the agent starting to move left and right, with saliency maps of specific game situations. Starting from generation 34, the agent begins moving left and right, with more pronounced saliency maps. By generation 56, the compressed learned representation allows the agent to infer the correct action almost always. In generation 145, a champion is discovered with a visual encoder and LSTM mapping for left and right strafing actions. Recent studies have shown that combining RL algorithms with deep neural networks can be effective, with evolutionary-based methods emerging as a promising alternative in some domains. In recent studies, Salimans et al. demonstrated that evolution strategies can perform well in Atari and MuJoCo tasks. Simple genetic algorithms have also shown comparable performance to deep RL methods. Earlier approaches evolved neural networks for RL tasks in lower-dimensional spaces, but end-to-end training remains simpler compared to supervised learning networks. Complex agent models may involve training different network components separately. In the world model approach, a variational autoencoder is trained on 10,000 rollouts to compress sensory data, followed by training a recurrent network to predict the next latent code. A smaller controller network is then trained to perform the task, taking input from both the VAE and recurrent network. Evolutionary approaches for solving 3D tasks from pixels have been challenging, with notable indirect encoding and recurrent controller approaches existing. The approach introduced in this paper involves safe mutations to evolve large-scale deep networks for a 3D maze task, reducing selection pressure on mutated neural networks. This method complements the Diversity Inducing Policy (DIP) and could be combined with it in the future. In evolutionary computation, various methods like novelty search, quality diversity, and speciation have been developed to encourage diversity. The concept of age has been used in genetic algorithms to maintain diversity, with approaches like Age-Layered Population Structure and combining age with a multi-objective approach. DIP rewards performance and low age by resetting individuals' age when their sensory or memory systems mutate. Learning dynamical models has focused on gradient descent-based methods, with recent work including PILCO and Black-DROPS. The paper introduces deep innovation protection (DIP) to reduce selection pressure in neural architectures for learning dynamical models from high-dimensional pixel images. It demonstrates that a world model representation can emerge without explicit rewards, showing that components upstream in the neural network can change dynamically. The neural model learned to represent situations requiring similar actions with similar codes. It predicted useful events for survival without forward-prediction loss. A genetic algorithm with DIP solved 2D and 3D domains without specialized learning methods. Comparing emergent representations from evolutionary and gradient descent-based optimization approaches is of interest for future research. The DIP approach showed lower average scores in 3D domains compared to the original world model paper. Training each component separately may result in higher performance, especially in complex domains where random rollouts may not provide all relevant experiences. Combining DIP with training inside the world model itself is a potential future direction, but the evolved representation may not be directly optimized for predicting the next time step. The text discusses the potential of using hallucinated environments for training in reinforcement learning. It suggests evolving neural architectures along with network weights for more complex tasks. The approach is based on evolution but could also be applied to gradient descent-based methods. The genetic algorithm parameters were set to 0.03 for experiments. The genetic algorithm parameter \u03c3 was set to 0.03 for experiments. The agent model uses the same architecture as the original world model approach. The sensory model is implemented as a variational autoencoder compressing input to a latent vector z. The VAE takes an RGB image of size 64 \u00d7 64 \u00d7 3 through four convolutional layers. The decoder processes a tensor of size 1 \u00d7 1 \u00d7 104 through four deconvolutional layers. The network processes input through four deconvolutional layers with specific sizes. It combines a LSTM network with a Gaussian model for outputs. An analysis was done on cumulative reward per age and number of individuals at different ages. The average reward increases with age, but fewer individuals are present at higher ages. The results suggest a competition between minimizing age and increasing cumulative reward, leading to the need for a multi-objective optimization approach. Perturbation-based saliency maps are calculated using a Gaussian blur to add uncertainty to specific locations in the game image. This helps the agent become less certain about certain elements, such as fireballs. The intensity map S(i, j) is determined by comparing the policy output \u03c0 of the original image I with a modified image I that has Gaussian blur added at location (i, j)."
}