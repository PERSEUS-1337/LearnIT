{
    "title": "rJxycxHKDS",
    "content": "We introduce a deep learning framework for unsupervised domain adaptation that allows different domains to undergo varying operations to achieve a common feature representation for recognition. This approach contrasts with existing techniques that apply the same operations to all domains, even in multi-stream architectures with unshared parameters. The deep learning framework introduced allows for flexible domain adaptation, addressing the domain shift problem in image understanding. Domain adaptation aims to map images to domain-shift-immune features for improved classifier performance. In the context of deep learning, the standard approach is to find domain-invariant features using a single architecture for both source and target domains. Recent work focuses on allowing domains to undergo different transformations to achieve this, using two networks with the same architecture but different weights. This novel paradigm offers even more flexibility for domain adaptation. In this paper, a novel paradigm for domain adaptation is introduced, allowing different domains to undergo unique computations while selectively sharing subsets. This approach enables networks to automatically adapt to varying complexities in domains, such as simpler synthetic images versus more complex real-world images. A multibranch architecture is developed to send data through multiple network branches in parallel, providing the necessary flexibility for domain-specific computations. The multibranch architecture allows data to flow through parallel network branches with trainable gates for flexibility in combining outputs. Each domain has its own set of gates for domain-adaptive computations, sharing some computations while keeping others domain-specific. This learning strategy adjusts specific computations for each domain, extending to more than two domains. Domain Adaptive Multibranch Networks (DAMNets) adjust computations for each domain, outperforming Ganin & Lempitsky (2015) and Rozantsev et al. (2019). Code will be made publicly available. Deep learning-based methods have advanced domain adaptation, with DAMNets showing superior performance. Learning-based methods are leading in performance for deep domain adaptation by finding a domain-invariant data representation through various approaches such as measuring distribution similarity using Maximum Mean Discrepancy and leveraging class labels or anchor points for alignment. Another popular method involves training a classifier to recognize the domain and using adversarial training. Recent research has focused on adversarial domain adaptation techniques, where a classifier is used to recognize the domain of a sample and features are adjusted to prevent discrimination. Various approaches have been developed for classification, semantic segmentation, and active learning. Some methods involve sharing weights between networks processing source and target data, while others propose training small networks to map weights from the source to the target. Recent research has focused on adversarial domain adaptation techniques, where a classifier is used to recognize the domain of a sample and features are adjusted to prevent discrimination. In this paper, the authors argue that network capacity should adapt to each domain's complexity. They propose a Dynamic Network Architecture (DAMNet) that uses domain-specific gates to allow different computations for the source and target data. Recent research has emphasized the importance of adapting network architecture to each domain's complexity. The Dynamic Network Architecture (DAMNet) utilizes domain-specific gates to enable distinct computations for source and target data, distinguishing it from previous methods focused on single-domain scenarios. Our deep domain adaptation approach, DAMNet, automatically adjusts computations for different domains by utilizing multibranch networks. Training in the domain adaptation scenario involves adapting computations for source and target data, distinguishing it from single-domain scenarios. DAMNet architecture utilizes multibranch networks for domain adaptation, adjusting computations for different domains. It extends traditional deep neural networks by replacing operations with multiple parallel branches. The DAMNet architecture uses multibranch networks for domain adaptation, with learnable gates controlling the relative importance of branch outputs. These gates are controlled by activation weights, allowing the network to modulate the computational graph of each network block. The DAMNet architecture utilizes multibranch networks for domain adaptation, with learnable gates controlled by activation weights to modulate the computational graph of each network block. Sigmoid functions with adaptive steepness are used to encourage the network to select or discard branches, with a cooling schedule applied during training to firm up gate decisions. This approach allows for flexibility in learning general additive relationships without requiring aggregated outputs to be a convex combination of branch outputs. A Multibranch Network concatenates multiple computational units, requiring matching output shapes for aggregation within each unit. Computational units are typically groups of semantically related layers, such as convolutions and pooling. The goal is domain adaptation, using labeled images from a source domain to train a model for a target domain with a different data distribution. The Multibranch Network aims for domain adaptation by training a model for a target domain with different data distribution using labeled images from a source domain. Gates are extended to define sets for both domains, encoding samples into feature vectors through computational units. Outputs are combined in a domain-specific manner based on activation weights. The Multibranch Network facilitates domain adaptation by encoding samples from different domains into a common space through computational units with activation weights. The network can adjust the amount of computation for each domain by setting specific activation weights, allowing for flexibility in sharing weights and utilizing different computations. The scheme adapts computation for each domain by introducing fixed, domain-specific gates that act as multiplexers. A small network then operates on the encodings to return class assignments, allowing for the same set of operations on all samples. This formulation can be extended to more than two domains by assigning one set of gates per domain, enabling the exploitation of annotated data from different source domains. In a generalized case, the model introduces governing sets of gates with activations for different domains, allowing for the handling of multiple target domains simultaneously. The gate parameters are jointly optimized with other network parameters during training using back-propagation and a composite loss function to encourage correct classification and align domain distributions. The second term in the loss function involves domain confusion strategy for distribution alignment, utilizing an auxiliary domain classifier network to predict domain probabilities. This technique incorporates adversarial training directly into the model. The Domain-Adversarial Neural Network (DANN) method by Ganin & Lempitsky (2015) integrates adversarial training into back-propagation, training the domain classifier and feature extractor simultaneously. After training, inactive branches with low activations are pruned, leading to a more efficient network. This approach is compared against the Residual Parameter Transfer (RPT) method by Rozantsev et al. (2019) for processing target data. The Residual Parameter Transfer (RPT) method by Rozantsev et al. (2019) is compared with the Domain-Adversarial Neural Network (DANN) method by Ganin & Lempitsky (2015). RPT relies on domain confusion loss for fair comparison. Different network architectures are adapted for various adaptation problems, with parameters initialized from source domain training. Training is done on predefined splits. To train the networks, Stochastic Gradient Descent with momentum and variable learning rate is used. Gate parameters are initialized to prevent favoring a specific branch. Gradient clipping is applied to avoid exploding gradients. Activation plasticity is modulated linearly during training. Data preprocessing involves mean subtraction. During training, the network is exposed to all image data from the source and target domains, with annotations only from the source domain. The method is evaluated in image recognition using domain adaptation benchmarks. Our method focuses on image recognition using domain adaptation benchmarks such as Digits (MNIST, MNIST-M, SVHN) and Office datasets. We compare our DAMNet approach with DANN and RPT for image classification tasks. DAMNets show improved accuracy with large domain shifts, especially with multiple source domains. The method is general and compatible with any feed-forward network architecture, as demonstrated with LeNet for digit recognition datasets. The study focuses on image recognition using domain adaptation benchmarks like Digits and Office datasets. The DAMNet approach is compared with DANN and RPT for image classification tasks, showing improved accuracy with large domain shifts, especially with multiple source domains. The method is general and compatible with any feed-forward network architecture, demonstrated with LeNet for digit recognition datasets. The architecture includes LeNet and SVHNet for small images, extended to multibranch architectures with ResNet-50 as the backbone network. The study compares the DAMNet approach with DANN and RPT for image recognition using domain adaptation benchmarks. Results show DAMNet outperforms baselines in all cases, including digit recognition and Office-Home datasets. The multibranch network architecture is based on ResNet-50, achieving improved accuracy with large domain shifts. Our method consistently outperforms others in various scenarios, including synthetic SVHN cases and using ResNet-50 backbone on upscaled images, achieving close to 80% accuracy. The multibranch network architecture based on ResNet-50 also shows improved accuracy with large domain shifts in the Office-Home dataset. Our approach achieves close to 80% accuracy on difficult adaptation tasks, outperforming baselines in various scenarios. Training on multiple source domains and leveraging synthetic domains improves performance, reaching accuracy comparable to full supervision on the target domain. Despite challenges with RPT dataset, gate dynamics analysis shows how our networks learn domain-specific branch assignments. Our DAMNet model shows successful adaptation in different domains, with gate activations evolving over training epochs. The network understands similarities between domains, utilizing similar computations when appropriate. Evaluation on the UAV-200 dataset demonstrates our method's effectiveness in detecting drones from video frames. Our domain adaptation method leverages synthetic and real examples of drones to predict patches' classes in real images. The network architecture follows a supervised setup with AdaDelta optimizer, cross-entropy loss function, and average precision evaluation metric. Multibranch computational units are defined with convolutions, nonlinearities, and pooling operations. The method outperforms others in terms of average performance. Our method, DAMNets, outperforms others in average precision, validating its effectiveness for domain adaptation using synthetic data. We replace the encoding strategy of MCD with DAMNet, resulting in improved performance. Combining MCD with DAMNet shows enhancements over using MCD alone, as shown in Table 3. The study evaluates the effects of adding extra branches with different capacities to the network for domain adaptation. A modified multibranch SVHNet is trained for adaptation between MNIST and SVHN, replacing the second branch with a similar branch using 1x1 kernels. The evolution of gates' activations in a multibranch ResNet-50 network is shown for the Office DSLR + Webcam Amazon domain adaptation problem. The study explores the effects of adding extra branches with different capacities to the network for domain adaptation. Gates for source domains are shown in the top row, while the target domain has its own gates in the bottom row. Different branches are used for processing data in the first computational unit, but shared computations are utilized for similar source domains in later units. The target domain maintains its own branches to accommodate its distinct appearance until reaching a domain-agnostic representation in conv 5x. The simpler domain-MNIST uses significantly fewer parameters. The gate evolution reflecting this is provided in Appendix C. The study explores the benefits of adding extra branches for domain adaptation. Networks quickly ignore extra branches when not beneficial. A domain adaptation approach with trainable domain-specific gates is introduced. Experimental evidence supports the approach over weight untying. The study introduces a domain adaptation approach with trainable domain-specific gates, demonstrating benefits over weight untying. The framework allows for arbitrary branch architectures, with potential for future research in combining the approach with neural architecture search strategies. The dataset descriptions include MNIST with black and white images of handwritten digits, MNIST-M with synthetic images, and SVHN with natural scene images of numbers from Google Street View. MNIST has 60,000 training and 10,000 testing examples, MNIST-M has 59,001 training and 9,001 testing images, and SVHN has 73,257 training and 26,032 testing images. The SVHN dataset contains images of numbers from Google Street View with clutter and distractors. The Office dataset consists of images from 31 object categories in office environments, captured from different domains. The multibranch SVHN network adapts its processing complexity based on the image content. The multibranch LeNet network adapts its branches for computational units 1 and 3, modifying the architecture to simplify computation. The second branch is altered to reduce parameters while maintaining shape similarity. The network assigns branches based on visual complexity differences in domains, without forcing gates to open or close. Domain adaptation results are compared in Table 4. Our DAMNet approach is compared with DANN and RPT for domain adaptation in image classification tasks. Different domain combinations show varying degrees of domain shift, with some being more challenging. DAMNets show significant accuracy improvement with large domain shifts, especially with multiple source domains. Ganin & Lempitsky (2015) and Rozantsev et al. (2019) evaluated using a ResNet-50, with results reported as Average Precision."
}