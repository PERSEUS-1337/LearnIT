{
    "title": "BJxpbREKvB",
    "content": "Few-shot learning is a challenging task in machine learning where novel classes are learned with only a few examples. In a study using the mini-ImageNet dataset, fine-tuning achieved higher accuracy in 1-shot tasks and comparable accuracy to state-of-the-art algorithms in 5-shot tasks. The method also outperformed common few-shot learning algorithms in high-resolution single-domain and cross-domain tasks. The retraining process was stabilized with a low learning rate, leading to improved accuracy. The retraining process can be stabilized with a low learning rate, adaptive gradient optimizers increase test accuracy during fine-tuning, and updating the entire network improves accuracy in large domain-shift scenarios. Previous studies have shown high image classification performance with deep networks and big datasets, but few-shot learning remains a challenging task with lower accuracy in novel class classification. Few-shot tasks in machine learning have lower classification accuracy compared to many-shot tasks due to the need for networks to adapt to novel classes with only a few examples. Fine-tuning deep networks with a large number of trainable parameters can lead to overfitting on novel classes. Various methods have been explored to prevent overfitting, but the performance of a simple fine-tuning approach has not been well studied. In this study, the performance of a fine-tuning method is analyzed, showing higher classification accuracy than common few-shot learning methods. Experimentally, it is found that a low learning rate stabilizes retraining, using an adaptive gradient optimizer increases test accuracy, and updating the entire network improves accuracy with large domain shifts. The mini-ImageNet dataset is used for evaluation in few-shot image classification tasks. The study evaluates a fine-tuning method's performance on high-resolution mini-ImageNet and cross-domain datasets, showing improved accuracy. The method achieves higher classification accuracy in the common low-resolution single-domain task. Our fine-tuning method outperforms common few-shot learning algorithms in 1-shot tasks and matches the state-of-the-art method in 5-shot tasks for low-resolution single-domain tasks. It also achieves higher accuracy in high-resolution single-domain and cross-domain tasks compared to common few-shot learning methods. Additionally, the study highlights the benefits of a low learning rate, using an adaptive gradient optimizer like Adam, and updating the entire network for increased test accuracy during large domain shifts. Few-shot learning involves learning novel classes with only a few labeled examples, known as N-way K-shot learning. A network is pretrained using base classes with numerous labeled examples, while novel classes are used for retraining and testing. Validation classes help determine the learning rate and epochs needed for retraining. Various few-shot learning algorithms have been proposed to improve test accuracy during large domain shifts. To date, several few-shot learning algorithms have been developed, categorized into learning discriminative embedding, learning to learn novel classes, and data augmentation. MatchingNet and ProtoNet use metric-based classification, while RelationNet utilizes a trainable metric with convolutional and FC layers for higher accuracy. Weight imprinting by Qi et al. initializes novel classes effectively. Few-shot learning algorithms focus on training networks to adapt to novel classes efficiently. Weight-imprinting and fine-tuning methods are commonly used for this purpose. Meta-learning approaches, such as those by Ravi & Larochelle and Finn et al., aim to enable networks to quickly adapt to new classes by updating network weights or training adaptable parameters. These algorithms enhance few-shot classification performance by facilitating fast adaptation to novel classes. Few-shot learning algorithms focus on training networks to adapt to novel classes efficiently. Data-augmentation-based approaches generate synthetic examples of novel classes to overcome data deficiencies. Wang et al. (2018) integrated a feature generator in few-shot learning to create synthetic data using only a few novel examples, improving few-shot learning performance. Networks can adapt to novel classes using naive data-augmentation methods. In few-shot learning, networks can adapt to novel classes by using naive data-augmentation methods like image flipping and jittering. The mini-ImageNet dataset, a subset of ImageNet, consists of 100 classes with 600 examples each, split into base, validation, and novel classes. Images are resized to 84 \u00d7 84 for computational efficiency. Recent studies have used a higher-resolution version of the dataset with 224 \u00d7 224 images for deeper networks. The authors proposed a cross-domain dataset with a larger domain shift between base and novel classes. They used high-resolution images from the mini-ImageNet dataset and CUB-200-2011 dataset for evaluation, along with ResNet and VGG feature extractors. The study utilized VGG-16 feature extractors without FC layers, replacing the last layer with GlobalAveragePool2d. Two classifiers were used: a simple FC layer and a normalized classifier with weight imprinting. The method was evaluated on low-resolution mini-ImageNet dataset and high-resolution mini-ImageNet and cross-domain datasets. In this study, tasks using low-resolution single-domain, high-resolution single-domain, and cross-domain datasets were identified. The classifier weight columns are normalized, and classification is based on inner products. Initial weights for novel classes can be obtained by including their feature vectors in the classifier weight. The classifier weights are normalized, and a scale factor is applied to the output to improve softmax activation. Pretraining of networks with base classes using Adam optimizer was done for 600 epochs. Input images were preprocessed with random-resized cropping, color jittering, and channel-wise mean subtraction. In this study, three fine-tuning methods were compared for a low-resolution single-domain task. The methods involved updating the entire network, updating the classifier weight and batch-normalization statistics, and updating only the classifier weight to prevent overfitting. The second method, based on a previous study, focused on updating only the batch-normalization statistics to prevent overfitting. This approach is similar to meta-transfer learning, where updating only scales and biases of network parameters can prevent overfitting. Authors proposed MTL to prevent overfitting and efficiently adapt to unseen tasks. Noguchi & Harada's method was chosen for its simplicity. Initial classifier weights for novel classes were obtained before fine-tuning the networks. Networks were retrained using mini-batch learning with a batch size of N K in the N-way K-shot learning scenario. Few-shot classification accuracy was evaluated through mean accuracy calculations of 600 trials with randomly sampled classes. Confidence intervals of the mean accuracy were also calculated. In the validation and test phases, input images were preprocessed by resizing and center-cropping before fine-tuning the network. Performance of the method in a 5-way low-resolution single-domain task was evaluated, with different classifiers and fine-tuning strategies. Performance of our method in the 5-way high-resolution single-domain task was evaluated using different classifiers and fine-tuning strategies. The classification accuracy for novel classes was not available in the pretraining phase. The classification accuracy for novel classes was not increased by fine-tuning the network. Few-shot classification accuracies for different tasks are shown in Tables 1, 2, and 3. In the 1-shot learning task, using VGG-16 and a normalized classifier increased accuracy by 6%, but updating the entire network did not further improve it. However, updating the entire network in the 5-shot task improved classification accuracy. The study found that fine-tuning the feature extractor could reduce within-class differences with multiple novel examples. Results showed varying robustness against low-resolution inputs depending on the feature extractor used. For instance, ResNet-152's accuracy decreased by 11.0% compared to VGG-16's 4.3% decrease. The validity of the low-resolution mini-ImageNet dataset for evaluating few-shot learning performance was questioned. Performance in the 5-way cross-domain task was also evaluated. Table 3 shows the performance of the method in the 5-way cross-domain task using different classifiers and fine-tuning strategies. Table 4 compares the method with conventional approaches, highlighting the highest accuracy achieved for each task. The comparison of results in Table 3 and high-resolution single-domain task shows a decrease in performance in the cross-domain task due to larger domain shift between base and novel classes. This difference was reduced by fine-tuning. In the 5-shot learning task using VGG-16, fine-tuning the entire network reduced the difference in classification accuracy between single-domain and cross-domain tasks from 15.9% to 6.9%. Comparative evaluation in Table 4 shows our results outperforming previous methods in 1-shot low-resolution single-domain tasks. Our method achieved higher classification accuracy than conventional methods in both high-resolution single-domain and cross-domain tasks. The difference in accuracy between the 5-shot tasks was only around 5% with our method, while conventional methods saw a decrease of over 10% in the cross-domain task. This demonstrates the success of our approach using ResNet-18, normalized classifier, and Adam optimizer. In the study, the ResNet-18 model, normalized classifier, and Adam optimizer were utilized for the 5-shot cross-domain task visualization. Learning rates of 0.01, 0.001, and 0.0001 were set for four trials with randomly selected validation classes and support sets. The focus was on the transition of validation accuracy rather than the accuracy itself to mitigate the impact of randomly selected classes and samples. The fine-tuning method was effective in achieving high few-shot classification accuracy, with discussions on improving its performance by using a low learning rate for stabilization during retraining. Using a low learning rate for fine-tuning stabilizes the retraining process and improves classification accuracy. Updating the entire network during large domain shifts enhances performance. Optimizers like Adam impact few-shot classification performance. Setting a lower learning rate, such as 0.0001, stabilizes the retraining process compared to higher rates. Optimizers like Adam, Adamax, Adadelta, Adagrad, and RMSprop affect few-shot classification performance. Higher accuracies are achieved with adaptive gradient optimizers, especially with normalized classifiers. Local minima obtained by Adam may lack generalization ability, but this does not hold true for few-shot learning. The reasons behind this phenomenon are left for future research. Updating the entire network for adaptation to a large domain shift is shown to achieve higher accuracy, especially with a normalized classifier. In a low-resolution single-domain task, fine-tuning method outperformed common few-shot learning methods in the 1-shot task and matched the state-of-the-art method in the 5-shot task. The study also evaluated the method in high-resolution single-domain and cross-domain tasks. Our method outperformed common few-shot learning methods in high-resolution single-domain and cross-domain tasks. Experimentally, we found that a low learning rate stabilizes retraining, adaptive gradient optimizers like Adam improve test accuracy, and updating the entire network is beneficial for large domain shifts. These insights will aid the community in tackling few-shot learning challenges."
}