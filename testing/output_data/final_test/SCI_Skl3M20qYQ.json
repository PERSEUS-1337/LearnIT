{
    "title": "Skl3M20qYQ",
    "content": "Learning disentangling representations of the independent factors of variations in an unsupervised setting is a challenge. The paper introduces Non-Synergistic VAE, a new approach that penalizes synergistic mutual information in latents to encourage information independence and disentangle latent factors. This approach can be easily added to the VAE framework, maintaining a lower bound on the log likelihood. The paper introduces Non-Synergistic VAE, a new approach that penalizes synergistic mutual information in latents to encourage information independence and disentangle latent factors. Comparing with Factor VAE, it implicitly minimizes the synergy of the latents. Good representations improve performance for transfer learning and multi-task learning tasks by capturing explanatory factors. Compositional representation should create new elements from primitive concepts, resulting in an infinite number of new representations. The model should identify white as a primitive element and construct multiple representations from it. A disentangled representation is defined as sensitive to generative factors while being invariant to others, factorized, and interpretable. It should be able to learn factors like position, scale, or color independently. Disentangled representations are beneficial for tasks like domain adaptation. Variational autoencoders (VAEs) have been used for tasks like domain adaptation and learning disentangled primitives in an unsupervised manner. Modifications like \u03b2-VAE and Factor VAE have been introduced to enhance the disentanglement process by increasing latent capacity and encouraging independence of latent variables. Other approaches rely on information bottleneck techniques for improving disentangled representations. The text discusses various approaches, including \u03b2-VAE and Generative Adversarial Networks, for improving disentangled representations in models like VAEs. It also explores the concept of synergy and how it relates to achieving independence within latent variables. The text discusses the importance of computing multivariate synergistic information and its relation to unique and redundant information in the framework. It describes decomposing information provided by random variables and subsets, emphasizing the partial mutual information in unique components. The text discusses decomposing information provided by random variables and subsets into unique, redundant, and synergistic components. It introduces the PI-diagram for visualization and mentions the exponential increase in terms with more sources. The best measure for synergy is still a topic of research, with a focus on synergy metrics for neural codes. The synergy metrics for neural codes involve three types of independence: activity, conditional, and information independence. Measures of synergy for sets of information sources consider the mutual information encoded in responses from different stimulus features. The decomposition of information into unique, redundant, and synergistic terms is discussed in the context of population coding. The synergy metric in neural coding measures information independence by considering the mutual information encoded in responses from different stimulus features. It is based on the idea that synergy should be defined as the \"whole beyond the maximum of its parts\". This metric is derived from previous work by Williams & Beer (2010) and differs from other metrics by specifically considering the mutual information I max in a group. The synergy metric in neural coding measures information independence by considering the mutual information encoded in responses from different stimulus features. It is based on the idea that synergy should be defined as the \"whole beyond the maximum of its parts\". This metric is derived from previous work by Williams & Beer (2010) and differs from other metrics by specifically considering the mutual information I max in a group of latents A i. The I max can be expressed in terms of the KL divergence. The motivation of our contribution is inspired by the concept that synergy is not desirable for disentanglement in the VAE framework, as we aim for latents to be independently informative about the data. By penalizing synergistic information within the latents and the data, we encourage the disentanglement of underlying factors of variation. The hypothesis is inspired by information in previous studies, focusing on the disentanglement of latent variables in the VAE framework. Latents are denoted as Z and observations as X, with A i representing a subset of latents. The distribution p(A i |x) is intractable, so an approximate distribution q \u03c6 (A i |x) is used. The KL divergence is computed similarly to the VAE framework, with a focus on the number of dimensions for the random variable z. In the VAE framework, the KL divergence is computed based on the number of dimensions for the random variable z. The Synergy metric considers a subset of latents, expressed with a penalized term in the ELBO formulation. The KL term in the ELBO loss is decomposed when using the aggregate posterior, leading to a penalty on synergy in the loss function. In the VAE framework, the KL divergence is computed based on the number of dimensions for the random variable z. The Synergy metric considers a subset of latents, expressed with a penalized term in the ELBO formulation. To optimize, only the second term is used to maximize the subset of latents with the most mutual information per outcome. A mini-batch approximation is used due to computational intensity, with Imax as the KL term of the synergy term. The final version includes parameters such as batch size, latent dimension, weight of synergy loss, discount factor, optimizer, and reparametrization function. The algorithm utilizes a greedy policy for selecting latent values and is trained on the dsprites dataset for disentanglement. The architecture and optimizer used are similar to Factor VAE BID16. The Non-Syn VAE model was trained using the same architecture and optimizer as Factor VAE BID16. Qualitative testing involved traversing the latents and plotting mean activations, showing disentanglement of factors. Both models performed similarly in the test. The synergy term was computed from Non-Syn VAE in Factor VAE to assess disentanglement, with results showing state-of-the-art performance. In this paper, a model inspired by information theory and neuroscience fields achieves disentanglement of data variations. Results show state-of-the-art performance close to FactorVAE BID16. Drawing ideas from information theory, models like BID23 and BID1 have used the information bottleneck in the VAE framework. Looking to neuroscience and information theory for inspiration is essential. Replicating biological models is not necessary, but analyzing intuition is valuable. In this paper, a model inspired by information theory and neuroscience achieves disentanglement of data variations by adapting known main mechanisms of the brain to the models."
}