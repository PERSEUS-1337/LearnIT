{
    "title": "HygUBaEFDB",
    "content": "Many real applications require learning multiple tasks from different data sources/modalities with unbalanced samples and dimensions. Existing deep multi-task learning approaches face challenges in handling heterogeneous input dimensions and network architectures. A flexible knowledge-sharing framework is proposed to jointly learn tasks from distinct sources, allowing each task to have its own network design while sharing through partially shared latent cores. This framework effectively transfers knowledge among tasks. The proposed framework allows for effective knowledge transfer among tasks with different data sources and modalities, enhancing performance in scenarios with limited data. Multi-task learning boosts overall performance by learning related tasks simultaneously, particularly in deep learning applications like computer vision and natural language processing. One key challenge in deep Multi-Task Learning (MTL) is the restriction to multi-label learning, limiting applicability to scenarios with distinct data sources and unbalanced sample sizes. The challenges in deep Multi-Task Learning (MTL) include limited training data and high dimensionality in tasks from different domains. For example, classifying hand-written digits (MNIST dataset) is similar to recognizing hand-drawn characters (Omniglot dataset), but Omniglot is harder with fewer training samples and higher dimensionality. Similarly, predicting binary attributes from human face images (CelebA dataset) is related to age group classification using human photos in the wild (Adience dataset), with Adience being more difficult due to unprocessed wild images and fewer samples. Jointly learning these tasks can improve multi-task representation. In deep Multi-Task Learning (MTL), it is beneficial to jointly learn tasks with limited training data and high dimensionality. Different tasks, such as classifying hand-written digits and recognizing hand-drawn characters, can share representations. For CNN settings, shared weights are used at lower layers, while relatedness between tasks is modeled by tensor normal priors at the top layers. In TRMTL, layer-wise weights are separately encoded for different tasks, with a subset of latent cores tied across tasks. In TRMTL for CNN, spatial cores in tensorized convolutional kernels are shared. In deep Multi-Task Learning (MTL), shared cores in tensorized convolutional kernels are used for better feature extraction. Existing MTL models are limited to multi-label learning with shared training inputs, hindering knowledge transfer between tasks with different network architectures. This limitation stems from the lack of knowledge-sharing mechanisms to address discrepancies in input data dimensions and layer-wise structures. In deep Multi-Task Learning (MTL), knowledge-sharing mechanisms like hard and soft parameter sharing are used to connect tasks. Hard sharing involves sharing parameters at lower layers, while soft sharing learns one DNN per task with regularization terms connecting tasks. However, a common issue is that tasks must have identical network architectures, leading to sub-optimal performance. Ideally, each task should encode both task-specific and task-independent portions at shared layers. In deep Multi-Task Learning (MTL), knowledge-sharing mechanisms like hard and soft parameter sharing are used to connect tasks. To address the limitation of tasks needing identical network architectures, a latent-subspace knowledge-sharing mechanism is proposed. This mechanism allows each task to be associated with a distinct data source, enabling better conveyance of private knowledge. The framework is realized via tensor ring multi-task learning (TRMTL), offering a new distributed knowledge-sharing mechanism for tasks with varying architectures. The proposed framework for deep Multi-Task Learning (MTL) utilizes tensor ring decomposition to enhance the performance of MTL models by compactness and expressive power. High-order tensors are used to represent multi-way arrays of real numbers, with TR decomposition breaking down tensors into 3rd-order latent cores. This approach allows for linking latent cores in a circular manner, providing a new distributed knowledge-sharing mechanism for tasks with varying architectures. The tensor ring (TR) format generalizes the tensor train (TT) format by relaxing the border rank condition, making it more flexible for low-rank approximation. TR ranks are equally distributed on cores, resulting in a more compact model compared to TT. The framework for deep Multi-Task Learning (MTL) uses TR decomposition to link latent cores in a circular manner, enhancing performance by compactness and expressive power. The tensor ring representation layer (TRRL) utilizes a sequence of latent cores, with some tied across tasks for task-independent knowledge and others treated as private cores for task-specific knowledge. TR allows for a more compact representation of matrices, establishing a correspondence between matrix and tensor elements. The TR-matrix format is defined with a trace operation and latent cores denoted as G(k). The proposed knowledge-sharing framework involves partitioning layer parameters into task-independent TR-cores and task-specific TR-cores. This allows for a more compact tensor network format and finer sharing granularity across tasks. The layer's weights are reformulated in terms of TR-cores using TRRL, and the input tensor is transformed into the output tensor with a common subset of TR-cores. The proposed knowledge-sharing framework involves partitioning layer parameters into task-independent TR-cores and task-specific TR-cores, allowing for finer sharing granularity across tasks. The layer's weights are reformulated using TRRL, and the input tensor is transformed into the output tensor with a common subset of TR-cores. The sharing is carried out in a distributed fashion, with each weight element determined by common and private latent factors. Our model allows for efficient sharing of weights in a distributed fashion using TR format. It can be implemented with various tensor network representations, such as Tucker, TT, PEPS, and MERA. The model can also be extended to convolutional kernels. TRRL is similar to TR-based weight compression but uses different 4th-order latent cores. Our TRMTL-CNN framework allows for efficient weight sharing by sharing spatial cores in the tensorized kernel. Experimental results show that TRMTL outperforms single task learning and other methods like MRN and DMTRL. The layer-wise weight is tensorized before sharing, ensuring equal sizes of cores for knowledge sharing. In TRMTL, knowledge sharing is measured by the number of shared cores, with TR-ranks assumed to be identical across layers. TR-ranks are chosen through cross validation. Unlike DMTRL, TRMTL is flexible in core sharing, allowing for prior knowledge incorporation. Vision tasks often share more cores at lower layers. In TRMTL, knowledge sharing is measured by the number of shared cores, with TR-ranks chosen through cross validation. Vision tasks often share more cores at lower layers. Authors in (Zhao et al., 2017) demonstrate that distinct cores control image resolution at different scales. They preferentially share features from detailed to coarse scale, selecting cores in a left-to-right order. C number of cores is tuned via cross validation, with a greedy search applied layer by layer to reduce the searching space. In TRMTL, knowledge sharing is measured by the number of shared cores, with TR-ranks chosen through cross validation. Vision tasks often share more cores at lower layers. Authors in (Zhao et al., 2017) demonstrate that distinct cores control image resolution at different scales. They preferentially share features from detailed to coarse scale, selecting cores in a left-to-right order. C number of cores is tuned via cross validation, with a greedy search applied layer by layer to reduce the searching space. Another practical option is to prune the searching space by following the guidance that C tends to decrease as the layers increase. For certain CNN based architectures, the special case TRMTL-CNN is adopted. Cores produced by tensorized convolutional kernel have specific roles, with spatial dimensions cores shared and input/output cores being task-specific. In tests, C is 1 due to small spatial kernels, eliminating the need for tuning this hyper-parameter. Initial validation is done on MNIST dataset, classifying odd and even digits. Various sharing styles and hyper-parameter C effects on performance are examined. In TRMTL, knowledge sharing is measured by the number of shared cores, with TR-ranks chosen through cross validation. Vision tasks often share more cores at lower layers. Authors in (Zhao et al., 2017) demonstrate that distinct cores control image resolution at different scales. They preferentially share features from detailed to coarse scale, selecting cores in a left-to-right order. C number of cores is tuned via cross validation, with a greedy search applied layer by layer to reduce the searching space. Another practical option is to prune the searching space by following the guidance that C tends to decrease as the layers increase. For certain CNN based architectures, the special case TRMTL-CNN is adopted. Cores produced by tensorized convolutional kernel have specific roles, with spatial dimensions cores shared and input/output cores being task-specific. In tests, C is 1 due to small spatial kernels, eliminating the need for tuning this hyper-parameter. Initial validation is done on MNIST dataset, classifying odd and even digits. Various sharing styles and hyper-parameter C effects on performance are examined. Performance patterns are analyzed in three categories: 'bottom-heavy', 'top-heavy', and others. The 'bottom-heavy' patterns show better transferability between tasks with unbalanced training samples, outperforming the other categories. TRMTL achieves high accuracy with significantly fewer parameters compared to other models like STL and MRN on the MNIST dataset. The reduction in parameters is attributed to tensorization and sparser TRRL with lower ranks. Validation on the Omniglot dataset confirms the effectiveness of knowledge transfer from data-abundance tasks to data-scarcity tasks within the same data domain. The Omniglot dataset consists of 1,623 unique characters from 50 alphabets divided into 5 tasks. Task C has sufficient samples while the other 4 tasks are limited. TRMTL effectively utilizes knowledge from task C to boost accuracies of all tasks. The sharing mechanism is also tested on the UFC11 dataset with positive results. The RNN is implemented using one-layer LSTM with input length of 190 and TR-cores are shared between tasks via TRMTL, significantly improving performance. Sharing all 4 TR-cores achieves the best recognition precisions. In this section, the key advantage of TRMTL method in handling multiple tasks with different network architectures is demonstrated. Task A involves classifying hand-drawn characters from the first 10 alphabets, while task B is recognizing hand-written digits. Task A is more challenging due to fewer training samples per character. The architecture specifications of TRMTL using a 4-layer MLP for Omniglot and MNIST combination are shown in Table 3, highlighting the distinct network structures for each task. TRMTL method demonstrates effectiveness in handling multiple tasks with different network architectures. Task A and task B have their own network structures, with some shared cores across layers. DMTRL requires converting heterogeneous inputs into equal-sized features with unshared weights in a hidden layer. TRMTL performs well on the MNIST task, while TRMTL-200 and 211 outperform competitors on the Omniglot task. DMTRL struggles due to its architecture limitations. TRMTL is also effective in handling data from distinct domains like the Office-Home dataset. The dataset used in TRMTL consists of over 10,000 images from different domains like Art, Clipart, Product, and Real World, with tasks A to D assigned to recognize 65 object categories. TRMTL variants outperform competitors by over 5% in accuracy for the toughest task A when 80% samples are available, credited to a sharing mechanism for object identity across tasks. TRMTL shows superior performance in shared high-level features using non-identical architectures, with TRMTL-HT outperforming TRMTL-HM by at least 2% in accuracy. Testing on Adience and CelebA datasets, TRMTL directly applied to raw images via CNNs for age group prediction and facial classification tasks. TRMTL significantly outperforms other methods on the hard task A by jointly learning on two domains of data with heterogeneous architectures. Our TRMTL framework achieves superior performance on task A compared to other methods, while also demonstrating strong accuracy on task B. TRMTL-HM shows lower accuracies than TRMTL-HT, indicating that compromising on a uniform CNN design across tasks can lead to decreased performance. The framework effectively shares low-level features across different architectures, relying on manual selection of shared cores. The hyperparameter search space may expand rapidly with an increase in the number of layers. The hyperparameter search space expands rapidly with an increase in the number of layers. A more sophisticated option is to automatically select sharable core pairs with the highest similarity based on perturbation-induced error changes. Tensorization in our TRMTL framework allows for finer core sharing and higher compression ratios compared to DMTRL. In this work, a novel knowledge sharing framework for deep Multi-Task Learning (MTL) is introduced, where multiple tasks involving different data domains can benefit from shared latent cores via tensor network format. Empirical verification shows state-of-the-art results in improving overall task performance. The tensorial counterpart of deep multi-task representation learning (DMTRL) extends the concept of task-specific mixing coefficients using tensor factorization. DMTRL utilizes a 3rd-order weight tensor W (or a 5th-order filter tensor K in the case of CNN) that is factorized into TT-cores. These TT-cores represent common knowledge and task-specific information, distinguishing TRMTL from DMTRL by tailoring heterogeneous network structures. Our TRMTL can customize network structures for different tasks, unlike DMTRL which lacks flexibility in handling task variations. TRMTL generalizes DMTRL in various aspects through four major types of generalizations (G1-G4). Firstly, TRMTL tensorizes the weight into a higher-order weight tensor for more latent cores. Secondly, TRMTL allows for any sharing pattern at distinct layers, unlike DMTRL's strict requirements. Thirdly, TRMTL offers more flexibility in sharing knowledge at a finer granularity. TRMTL generalizes DMTRL in various aspects through four major types of generalizations (G1-G4). TRMTL allows for any sharing pattern at distinct layers and each task may have its individual input domains. It further generalizes TT to TR-format and incorporates a more general tensor network framework into the deep MTL context. TRMTL is a compact method for multi-task learning that outperforms other non-latent-subspace methods. It requires fewer parameters and is advantageous for tasks with small sample sizes. The network architecture and experimental results on the MNIST dataset are detailed in Table 6, where TRMTL achieves the best results and shows robustness to small tasks. Our TRMTL model achieves robust performance in pattern selection, with '410' and '420' patterns showing similar good results. The architecture for the Omniglot Dataset is similar to the previous experiment, utilizing TRRL for the last two convolution layers and first fully connected layer. The best sharing pattern is '432', and Table 8 provides network specification details. Figure 7 illustrates accuracy changes for tasks with and without datarich task aid. Table 9 summarizes performance of compared methods with different fractions of training data, showing TRMTL's overall best performance. Our TRMTL model achieves robust performance in pattern selection, with '410' and '420' patterns showing similar good results. In this section, we conduct more experiments on the CIFAR-10 dataset, assigning 10 classes into 3 tasks. We test the performance of different models in transferring knowledge from data-abundant tasks to data-scarcity tasks within one data domain. The experiments involve insufficient training samples for each task (5%, 10%, or 50%) using a specific architecture. Our TRMTL model outperforms competitors in accuracy, especially with limited training samples. Results show TRMTL ('4431' and '4421') patterns excel in most cases, with a 1.7% precision increase in task A when task C data is at 100%. The TRMTL-4431 model shows a 1.7% precision increase when task C data is at 100%, and a further enhancement up to 5.5% when both task B and C training samples are fully available. Compared to MRN, which only improves by 0.4% and 3.0% in similar scenarios, our model demonstrates clear task-specific features that benefit downstream classification tasks. The visualization of high-level features using t-SNE shows clustered features separated for different classes, highlighting the advantage of our method in handling tasks with heterogeneous inputs within a single data domain. The tasks involve assigning input images with different spatial sizes or channels on the CIFAR-10 dataset. DMTRL requires converting heterogeneous inputs into equal-sized features using a hidden layer with unshared weights. MLP with 4 hidden layers is used to showcase the impact of heterogeneous inputs. Architectures for different spatial sizes and channels are detailed in Tables 11 and 12. The TRMTL model demonstrates improved precision with fully available training samples for tasks B and C. The text chunk discusses input and output modes for different spatial sizes and channels in a model, with varying configurations for FC4 input and output modes."
}