{
    "title": "H1xscnEKDr",
    "content": "We study defending deep neural network approaches for image classification from physical attacks. Adversarial training and randomized smoothing are not effective against certain attacks. We introduce a new adversarial model, rectangular occlusion attacks, and show that training with this new attack improves robustness against physical attacks. The effectiveness of deep neural networks in various fields has led to the emergence of adversarial example attacks, where carefully perturbing pixels in an image can fool the network. Research has focused on defending against these attacks by proposing techniques for more robust neural network models or detecting adversarial inputs. Physical attacks on deep neural networks have been demonstrated by implementing adversarial perturbations in physical objects captured by a camera and fed through a deep neural network classifier. Three significant physical attacks include fooling face recognition with adversarially designed eyeglass frames, fooling stop sign classification with adversarially crafted stickers, and the universal adversarial patch attack causing targeted misclassification of any object with an adversarially designed sticker. Defending against these attacks has received considerable attention. The text discusses the lack of effective methods to defend against physical attacks on deep neural networks, specifically focusing on the eyeglass frame attack on face recognition and the sticker attack on stop signs. Conventional approaches like adversarial training and randomized smoothing show limited effectiveness against these physical attacks, highlighting the need for a novel abstract attack model to better capture the nature of such attacks. The text introduces a new attack model for physically realizable attacks, focusing on rectangular occlusion attacks. Algorithms are developed to compute adversarial occlusions, and neural network models are trained to be robust against these attacks. Experimental results show that this approach is more effective than traditional defense methods against physical attacks on deep neural networks. The standard solution approach for hardening neural networks against adversarial examples with l \u221e -norm perturbation constraints is adversarial training. Despite recent advances, adversarial training remains the most effective method. Randomized smoothing is a technique for obtaining robustness against l 2 -norm attacks in neural networks. Methods for detecting adversarial examples have shown mixed results, with recent work focusing on physical adversarial examples. However, detection alone is weaker than robustness. Recent efforts have also characterized neural network robustness to realistic perturbations like translations and rotations. Adversarial examples involve modifications of input images that cause misclassification by neural networks. Common approaches aim to solve an optimization problem to generate these examples. State-of-the-art attacks include those by Carlini & Wagner and the projected gradient descent attack by Madry et al. Physical attacks on neural networks involve modifying the actual object being photographed to deceive the network. These attacks are implemented in the physical space, have low suspiciousness by making subtle modifications, and cause misclassification by deep neural networks. Physically realizable attacks on neural networks involve simulating attacks digitally rather than implementing them physically. These attacks are considered stronger adversarial models compared to actual physical attacks. Three types of physically realizable attacks are considered. Physically realizable attacks on neural networks involve implementing attacks physically rather than digitally. Three types of attacks are considered: adding adversarial noise to eyeglass frames for face recognition, using stickers on a stop sign to cause misclassification, and designing patches with adversarial noise for misclassification of objects. Many proposed approaches for making deep learning robust have been defeated by more sophisticated attacks. The focus is on principled defense approaches that have not been broken, including robust learning and randomized smoothing. Robust learning aims to minimize a robust loss, with techniques like adversarial training by Madry et al. (2018) and curriculum adversarial training (Cai et al., 2018) being effective in practice. The implementation of adversarial training involves gradually increasing adversarial noise levels to achieve robustness. Another technique involves adding Gaussian noise to inputs during both training and prediction to construct a smoothed classifier. This method aims to achieve provably robust classification by perturbing inputs with isotropic Gaussian noise. Most deep learning approaches focus on adversarial robustness by introducing bounded perturbations. Conventional robust ML methods like adversarial training and randomized smoothing are effective against different types of attacks. The question is raised if these methods are robust against physically realizable attacks, similar to findings in malware evasion. This is the first investigation in computer vision applications and deep neural networks. In the first investigation of its kind in computer vision and deep neural networks, we experiment with robust ML approaches to defend against physically realizable attacks involving adversarial masking of objects. This includes adversarial training and randomized smoothing techniques applied to face recognition and stop sign classification attacks. Various variations of adversarial training are considered based on the l \u221e bound imposed on the adversary. Adversarial instances were generated using PGD for adversarial training. Attacks with \u2208 {4, 8} were considered, with curriculum adversarial training doubling the attacks. Models were learned for \u2208 {4, 8, 16, 32}. Different iterations of the PGD attack and noise levels were used. White-box dodging attacks were applied on face recognition systems. In white-box dodging attacks on face recognition systems, adversarial perturbations inside eyeglass frames were computed using a learning rate of 20 and momentum value of 0.4. Adversarial training showed limited effectiveness beyond 20 attack iterations, with the best performance achieved at 8 iterations. The best adversarial robustness is achieved with adversarial training using 8 iterations, regardless of using 7 or 50 PGD iterations. Non-adversarial accuracy drops by about 20% compared to the original model. Curriculum adversarial training has higher non-adversarial accuracy but is less robust. Randomized smoothing is ineffective against physical attacks, with accuracy dropping below 20% even for weak attacks. Following Eykholt et al. (2018), the LISA traffic sign dataset is used for experiments, with 40 stop signs as test data for untargeted attacks. The same settings and optimizer are applied as in the original attacks. Adversarial training with PGD iterations is ineffective when = 32, leading to error rates above 90%. Curriculum adversarial training shows higher non-adversarial accuracy but is less robust. The best performing adversarial training with = 16 has high performance on adversarial data and works well on clean data. Variants of adversarial training only marginally outperform the original model against the stop sign attack. Randomized smoothing shows inconsistent performance, with the best variant outperforming adversarial training with accuracy slightly above 60% even for stronger attacks. Randomized smoothing results in significant degradation of effectiveness on adversarial instances, nearly 40% compared to clean data. Conventional robust ML models perform poorly against physical attacks due to the mismatch between the attack model and realistic physical attacks. The evidence suggests that conventional attack models are not effective against real-world physical threats. The key element in physical attacks is the introduction of adversarial occlusions. The constraint is to avoid suspicion, limiting the size but not necessarily the shape or location of the occlusion. The introduction of adversarial occlusions in physical attacks limits the size but not necessarily the shape or location of the occlusion. A simple abstract model of occlusion attacks is proposed, involving a fixed-dimension rectangle that can be placed anywhere in the image with l \u221e noise inside. This model reflects common physical limitations and is abstract in nature. The rectangular occlusion attack (ROA) model involves placing stickers on an object to obscure identification, with the attack being untargeted to defend against physical attacks. The process includes identifying a region in the image for the rectangle and generating perturbations within that region. See Appendix C for illustrations. The rectangular occlusion attack (ROA) model involves placing stickers on an object to obscure identification. To efficiently generate fine-grained adversarial perturbations within a specific region, an exhaustive search is used to find the optimal position for a grey rectangle that maximizes loss. This approach decouples the search for the rectangle position from the perturbation generation process, improving efficiency. To speed up the search process for the rectangular occlusion attack (ROA), the gradient of the input image is used to identify candidate locations with high gradient magnitude. A subset of these locations is selected for the sticker placement, reducing the number of loss function evaluations. The resulting classifiers, termed Defense against Occlusion Attacks (DOA), are robust to abstract adversarial occlusion attacks and offer an alternative to traditional robust machine learning for defending against physical attacks. Full details of the algorithms for computing ROA are provided in the appendix. The Defense against Occlusion Attacks (DOA) is evaluated for its effectiveness against physically realizable attacks, including the adversarial patch attack on face recognition and traffic sign data. Two rectangle dimensions are considered: 100 \u00d7 50 and 70 \u00d7 70 pixels. This evaluation serves as a lower bound on robustness to actual physical attacks, which have additional practical constraints like being robust to multiple viewpoints. The study evaluated the Defense against Occlusion Attacks (DOA) effectiveness against physical attacks using two rectangle dimensions: 100 \u00d7 50 and 70 \u00d7 70 pixels. Adversarial noise was generated inside the rectangle using PGD iterations and ROA with different parameters. DOA adversarial training was conducted for 5 epochs. Results showed DOA's performance against the eyeglass frame attack compared to conventional methods. The study compared the effectiveness of DOA against physical attacks using different rectangle dimensions. Results showed DOA yielded more robust classifiers compared to conventional methods. Changing the dimensions of the rectangle had minimal impact as long as enough PGD iterations were used. The evaluation was repeated with traffic sign data and the stop sign attack. DOA was evaluated against physical attacks using different rectangle dimensions, showing robust classifiers. Results on traffic sign data and stop sign attacks demonstrated DOA's effectiveness, with over 90% robust accuracy for exhaustive search and around 85% for gradient-based variants even against stronger attacks. DOA remains highly effective at classifying stop signs and traffic signs, with around 95% accuracy on the full task. Adversarial patch attacks are effective when covering 10% or more of the image, but DOA remains robust even with 20% coverage. The new threat model proposed, rectangular occlusion attacks (ROA), combined with adversarial training, achieves high robustness against physical attacks in deep learning for image classification. Questions remain about certifying robustness against ROA and exploring other types of occlusions for practical robustness in computer vision applications. The study focuses on the practical robustness of deep learning in computer vision applications like autonomous driving and face recognition. A dataset with 2622 subjects was used, with 10 selected subjects for face recognition. The final dataset had 300-500 images per subject, processed to 224x224 pixels. The dataset was split into training, validation, and test sets with a ratio of 7:2:1 for each subject. The VGGFace convolutional neural network was utilized for the study. The study utilized the VGGFace convolutional neural network model for face recognition, with 10 selected subjects and a dataset containing 300-500 images per subject. Transfer learning was applied with adjustments to the fully connected layers. Pytorch implementation included converting images to BGR, using pretrained weights, and optimizing with Adam Optimizer. Batch size was set to 64, with a learning rate drop every 10 epochs and validation set accuracy monitoring for model performance. After training for 30 epochs, the model achieved 98.94% accuracy on test data. A subset of LISA with 47 U.S. traffic signs was selected, focusing on 16 high-quality signs for training. Evaluation was done on 40 stop signs to assess robustness against stop sign attacks. Data was processed to 32x32 pixels using the LISA-CNN architecture. The LISA-CNN architecture was used to process data to 32x32 pixels. The model achieved 98.69% accuracy on the validation set and 100% accuracy in identifying stop signs in the test data. Adversarial training and randomized smoothing were shown to degrade more gracefully against attacks they were designed for, including variants of projected gradient descent for l\u221e and l2 attacks. The projection operator Proj clips the result to be feasible, x t is the adversarial example in iteration t, \u03b1 is the learning rate, and L(\u00b7) is the loss function. For l2 attacks, PGD involves normalizing the perturbation \u03b4 = x t+1 \u2212 x t with \u03b4 2 \u2264 Kolter & Madry (2019). Experiments were conducted on face recognition and traffic sign datasets, focusing on adversarial perturbations to all sign images. Results show that curriculum adversarial training with = 16 is generally the most robust, with a tradeoff between accuracy on non-adversarial data and robustness. The results of using randomized smoothing on face recognition and traffic sign data show a high level of robustness against adversarial attacks, with a relatively limited drop in performance. Curriculum adversarial training with a value of 16 is generally the most robust, striking a balance between accuracy on non-adversarial data and robustness. The algorithm for rectangular occlusion attacks (ROA) involves finding the optimal position for a grey rectangle in the image and adding high l\u221e noise within that rectangle. The algorithm includes parameters for image dimensions and a stride parameter to speed up location computation. The ROA attacks involve finding the optimal position for a grey rectangle in the image and adding adversarial noise inside it using the l \u221e version of the PGD attack. The algorithm specifies a mask matrix to constrain the perturbation area, with parameters for iterations and learning rate. Physically realizable attacks follow a common pattern of specifying a mask and introducing noise within that area. The physically realizable attacks involve solving an optimization problem to find the optimal perturbation area. Different techniques are used such as initializing eyeglass frames with colors and stickers on stop signs with random noise. Gradient values are adjusted and clipped to stay within valid ranges during the attack process. The Adam optimizer was used with a 0.1 learning rate and default parameters for adversarial attacks. Adversarial perturbations were limited to a specified mask area, using gradient ascent to maximize the log probability of the targeted class. A square patch was implemented instead of a circular one, with random position and direction, a learning rate of 5, and 100 attack iterations per image. The attack region varied from 0% to 25%. The face recognition dataset consisted of 27 images for patch design, with attacks run over 20 epochs. The attack was conducted using a patch designed with a limited number of images for each dataset. Adversarial patches were evaluated on the validation set without the targeted class images. Examples of the eyeglass attack on face recognition and the stop sign attack were shown, demonstrating successful model predictions. The effectiveness of Defense-Oriented Adversarial (DOA) methods against l \u221e attacks is evaluated in the context of face recognition and traffic sign classification. Results show that DOA is largely ineffective against these attacks as it assumes the attacker only modifies a small proportion of the scene. DOA is effective in generalizing to various physical attack patterns, including a union of triangles and circle, a single larger triangle, and a heart pattern. The larger patterns, especially the heart pattern, are suspicious as they cover a significant portion of the image."
}