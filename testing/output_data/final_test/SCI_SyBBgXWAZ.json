{
    "title": "SyBBgXWAZ",
    "content": "Generative models like VAEs and GANs are trained with fixed prior distributions in the latent space. After training, the Generator can be sampled for exploration, like interpolating between samples or exploring differences. However, existing latent space operations can cause a distribution mismatch with the prior distribution. To address this, distribution matching transport maps are proposed to ensure that latent space operations preserve the prior distribution. Our experimental results confirm that the proposed operations improve sample quality compared to original methods. Generative models like Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) are popular for unsupervised learning. In GANs, a generator and discriminator are trained adversarially to produce synthetic samples. VAEs minimize the variational lower bound of data likelihood. Both models are trained with a fixed prior distribution. In generative models like VAEs and GANs, a trained generator G maps latent samples z to output samples G(z) to match the data distribution. However, common arithmetic operations on latent samples cause a distribution mismatch with the fixed prior distribution, impacting the model's performance. The text discusses the issue of distribution mismatch in generative models like VAEs and GANs when performing arithmetic operations on latent samples. It proposes using distribution matching transport maps to address this problem and showcases an interpolation operator that avoids distribution mismatch between samples of a uniform distribution. The text discusses distribution mismatches in generative models like VAEs and GANs during arithmetic operations on latent samples. It proposes using distribution matching transport maps to solve this issue and presents an interpolation operator that avoids distribution mismatch between samples of a uniform distribution. The text discusses using linear interpolation in GANs and VAEs to explore latent space representations and manipulate generated samples. It also addresses the issue of distribution mismatch during operations on samples. The text discusses addressing distribution mismatch in latent space representations by using spherical linear interpolation to improve sample quality, as observed by White (2016). This approach aims to maintain consistency with the prior distribution, but may not scale well to higher dimensions. The text proposes a method to preserve the prior distribution when using sample operations, such as linear interpolation. It involves finding a warping function that matches the distribution of the prior, using optimal transform maps to minimize modifications to the original operation. The text discusses using implicit models like GANs and VAEs to learn a generator that approximates a fixed prior distribution. Operations on latent samples are then explored, with examples provided in Table 1. The analysis is done through the underlying random variable, treating the output as a statistic. The text explores operations on latent samples, such as linear interpolation, to analyze how the trained generator changes when creating related samples from source samples. It is shown that the distribution of the generated samples may differ from the original latent samples. The text discusses the distribution mismatch that occurs when using linear interpolation and other operations on latent samples. It proposes a strategy using optimal transport to construct distribution-preserving operators. The text discusses using optimal transport to compute minimal modifications to distribution-mismatched operators, ensuring fidelity to the original operator. This strategy aims to preserve distributions when applying operations on latent samples. The text introduces the problem of optimal transport in a simplified setting, focusing on minimizing cost functions between probability distributions. Kantorovich's relaxed problem is discussed as a precursor to solving the original problem posed by Monge. The text discusses minimizing a cost function between probability distributions by finding the optimal transport plan from one distribution to another. It emphasizes the use of joint probability distributions with specified marginals to relax the deterministic relationship between variables. The text discusses the relaxation of a deterministic relationship between variables in the context of minimizing a cost function between probability distributions. It aims to find an analytical or efficient numerical solution by choosing a cost function that allows for a deterministic mapping. The focus is on constructing operators with i.i.d. components. Theorem 1 states that if the distributions p x and p y have i.i.d. components and the cost c is additive, then the minimization problems simplify into scalar problems. Theorem 2 provides a known solution for these scalar problems under certain constraints. The optimal transport map from distribution p X to p Y on R with finite cost in (KP-1-D) is defined by the Cumulative Distribution Function (CDF) of X and its pseudo-inverse. The mapping is non-decreasing and known as the monotone transport map from X to Y. By combining Theorems 1 and 2, a concrete realization of Strategy 1 can be achieved by choosing a suitable cost function and computing the monotone transport map. The CDFs F Z and F Y are used to compute the component distribution p Y of the result y of the operation. Monotone transport maps for linear interpolation are shown in FIG2. Detailed calculations and examples for various operations are provided in Appendix 5.3 for both Uniform and Gaussian priors. The Gaussian case has a simple resulting transport map for additive operations, validated through numerical simulations. In Figure 1, 1 million pairs of points in two dimensions are sampled from a uniform prior. The matched interpolation gives midpoints identically distributed to the prior, while linear interpolation condenses towards the origin. The spherical interpolation creates a distribution with a \"hole\" circling around the origin. There is a significant difference in vector lengths between the prior and midpoints of linear interpolation. In this section, the differences in generator output based on sample operations in traversing the latent space of a generative model are compared. The study includes linear interpolation, SLERP, and a proposed matched interpolation. The matched interpolation preserves the prior distribution and aligns perfectly, unlike the other methods. The study used DCGAN BID12 generative models trained on LSUN bedrooms and CelebA datasets. The study used DCGAN generative models trained on LSUN bedrooms, CelebA, and an icon dataset to evaluate different interpolation methods in latent space. The models were trained with different latent space dimensions and prior distributions. Additionally, iWGAN with gradient penalty was used to produce inception scores for CIFAR-10 dataset. The study begins with a classic example of 2-point interpolation. The study evaluated interpolation methods in latent space using DCGAN generative models trained on LSUN bedrooms, CelebA, and an icon dataset. Linear interpolation produced inferior results compared to SLERP and matched interpolation. Differences between interpolation methods were subtle for CelebA dataset, with the most divergence occurring at the midpoint of interpolation. In comparing interpolation methods in latent space using DCGAN generative models, linear interpolation showed inferior results compared to SLERP and matched interpolation. The most significant divergence was observed at the midpoint of interpolation, highlighting a loss of detail and increased artifacts in the linear version. Vicinity sampling and random walk in the latent space also demonstrated the advantages of matched interpolation over linear methods. The property of matched vicinity sampling allows for a random walk in the latent space, starting from a point drawn from the prior distribution. The repeated application of this operation results in divergence from the prior distribution, leading to unrecognizable output images. This highlights the importance of respecting the prior distribution when operating in latent space. Repeated application of operators that do not comply with the prior distribution can lead to divergence from the original distribution, as confirmed quantitatively using the Inception score BID16. Comparisons show that linear interpolation results in significantly lower scores compared to matched operations, while the SLERP heuristic gives similar scores. This highlights the importance of respecting the prior distribution in latent space operations for Generative Models. Latent space operations in Generative Models can cause distribution mismatch from the prior distribution, leading to lower quality samples. To address this, optimal transport can be used to modify the operations while preserving the prior distribution. The resulting matched operators produce higher quality samples and have the potential to become standard tools for evaluating generative models. The analysis explores differences in norm between linear interpolation midpoint and prior points in generative models. Latent space with a prior distribution is examined, showing points clustering on a spherical shell due to Central Limit Theorem. This property is well-known for i.i.d Gaussian entries. The text discusses the concentration of mass in corner points for a Uniform distribution on a hypercube, and how the distribution of a midpoint operator can be approximated. It also mentions that the distribution of y^2 can be approximated with N, and how y mostly lies on a spherical shell with a different radius than z. The shells intersect at regions with vanishing probability for large d, making y^2 a strong outlier compared to z^2. The proof is shown for the Kantorovich problem. In the context of discussing concentration of mass in corner points for a Uniform distribution on a hypercube, the text explains how to compute DISPLAYFORM0 and DISPLAYFORM1 for joint distributions. It shows that (KP) and (KP-1-D) are equivalent up to a constant, and minimizing one will minimize the other. The text also mentions illustrating matched operations for linear interpolation and vicinity sampling with uniform or Gaussian priors. The text discusses computing the uniform transport map and monotone transport map for random variables with i.i.d components. It mentions using a non-decreasing mapping to transform distributions and provides an alternative method for computing the monotone transport map. The text discusses computing the uniform transport map and monotone transport map for random variables with i.i.d components. It mentions using a non-decreasing mapping to transform distributions and provides an alternative method for computing the monotone transport map. In this case, linear interpolation between two points is considered, and the convolution of component distributions is calculated. The CDF for the linear interpolation is derived, and a method for obtaining new points close to a given point is described. The text discusses computing the monotone transport map for random variables with i.i.d components. It mentions using linear interpolation between two points and adjusting the variance of the resulting distribution. Another operation mentioned is the \"analogy\" where the difference between two points is applied to a third point. To compute the transport map for random variables with i.i.d components, one can use linear interpolation between two points and adjust the resulting distribution's variance. Another method is applying the difference between two points to a third point."
}