{
    "title": "SJzYdsAqY7",
    "content": "Deep convolutional neural networks (CNNs) are commonly used in various applications but require significant computational resources. Pruning techniques and Winograd convolution are methods to reduce CNN computation, but they cannot be directly combined due to the sparsity issue. To address this, a new pruning method called spatial-Winograd pruning is proposed to achieve high Winograd-domain weight sparsity without altering network structures. Our proposed spatial-Winograd pruning method efficiently transfers spatial-domain sparsity into the Winograd domain without the need for retraining. By using an importance factor matrix, we can adjust weight importance and gradients in the Winograd domain, allowing for effective retraining without changing the network structure. This approach achieves Winograd-domain sparsities of 63%, 50%, and 74% for models on CIFAR-10, CIFAR-100, and ImageNet datasets, respectively. The high computation cost of CNN models limits deployment of larger and deeper models. Two methods to reduce computation are pruning techniques and Winograd/FFT convolution. Pruning removes redundant weight parameters, while Winograd and FFT convolution transform computation into different domains, achieving speedups and requiring fewer flops. In this paper, the focus is on the Winograd convolution, which offers faster computation than FFT-based approaches. However, pruning techniques and Winograd convolution are not directly compatible as sparse weight matrices lose sparsity after the transformation. To address this, BID7 proposes pruning and retraining directly on Winograd-domain weights, but this requires a significantly smaller learning rate and is challenging for deep networks. Another approach, Winograd-ReLU pruning BID9, moves the ReLU function into the Winograd domain to increase sparsity but requires changes in the network structure. In this paper, a new pruning method called spatial-Winograd pruning is proposed to improve the sparsity of Winograd-domain weights without altering the network structure. It involves spatial structured pruning and Winograd direct pruning, achieving higher sparsity compared to previous methods. The proposed spatial-Winograd pruning method aims to increase sparsity in Winograd-domain weights efficiently. It involves structured pruning to transfer spatial-domain sparsity to the Winograd domain, avoiding retraining. Additionally, a new approach measures the importance of Winograd-domain weights based on their impact on output activations, allowing for faster retraining without changing the network structure. The Winograd convolution operates in the spatial domain, using element-wise multiplications instead of convolution operations. It works on 2D input and weight tiles to generate output tiles. The weight filter and input tile are converted to the Winograd domain for processing, resulting in the Winograd-domain output. The Winograd convolution transforms weight filters and input tiles into the Winograd domain to generate output tiles, reducing the number of multiplications required. Pruning is also used to reduce CNN computation, with iterative pruning and retraining proposed to achieve up to 5x computation reduction. BID14 and BID15 prune networks in a structured way by clustering weights into groups for efficient computation acceleration. However, Winograd convolution poses challenges for conventional pruning algorithms. Research attempts have been made to address this issue, such as directly masking out Winograd-domain weights and training spatial-domain weights using backpropagation. Inconsistencies between spatial and Winograd domains can lead to accuracy loss or low sparsity in networks like AlexNet for large datasets. The sparse Winograd convolution proposed by BID7 addresses accuracy loss and low sparsity in networks like AlexNet for large datasets. It stores weight values in the Winograd domain, achieving >90% sparsity on AlexNet but struggles with deep networks. BID9 introduces Winograd-ReLU pruning, moving the ReLU function into the Winograd domain to further reduce computation. In this paper, a new approach called spatial-Winograd pruning is proposed to achieve high Winograd-domain weight sparsity on deep CNN models without altering network structures. This method involves spatial structured pruning and Winograd direct pruning, aiming to address the challenges of utilizing weight and input sparsity for CNN acceleration on general-purpose processors. Spatial-Winograd pruning involves spatial structured pruning and Winograd direct pruning. In spatial structured pruning, weights are pruned in a structured way and retrained to regain accuracy. The pruned model's weights are then transferred to the Winograd domain for further pruning and retraining. This process is iteratively executed until the desired sparsity is achieved or the model loses too much accuracy. The first step, spatial structured pruning, clusters spatial-domain weights affecting the same Winograd-domain weight into groups, removes less important groups, and retrains the pruned network to maintain accuracy. This method helps transfer spatial-domain sparsity into the Winograd domain. The structured pruning method transfers spatial-domain sparsity into the Winograd domain by generating masks to indicate redundant weights. Each Winograd-domain weight is a weighted sum of spatial-domain weights, with importance measured using a maximum norm function. In this paper, the structured pruning method transfers spatial-domain sparsity into the Winograd domain by generating masks to indicate redundant weights. The maximum norm function is used to determine redundancy, with weights below a threshold being removed. After spatial pruning, retraining can be done using conventional algorithms like stochastic gradient descent. The structured pruning method transfers spatial-domain sparsity into the Winograd domain by generating masks to indicate redundant weights. The threshold for spatial pruning is gradually increased to increase network sparsity. In spatial structured pruning, both pruning and retraining steps are performed in the spatial domain to avoid Winograd-domain retraining. Winograd direct pruning removes unimportant weights based on their impact on output activations. The pruned network is then retrained in the Winograd domain with an importance factor matrix to adjust weight gradients. In Winograd pruning, a mask matrix is generated for each filter to indicate redundant weights. The output is calculated using weight coefficients and removing weights minimizes changes in output activations. The importance of each weight is measured to avoid significant changes in output activations. In Winograd pruning, a mask matrix is generated for each filter to indicate redundant weights, minimizing changes in output activations. The importance of each weight is measured by the expected value of ||\u2206O| Qi,j || 2 2, simplifying to a relative importance factor matrix F determined by m and n. This allows for the generation of a mask matrix M Winograd with a specific threshold t Winograd. In Winograd pruning, a mask matrix is created to identify redundant weights for each filter, minimizing changes in output activations. The weight importance is determined by the absolute value of |Qi,j|, equivalent to using Q^2i,j. Winograd retraining adjusts the gradients of Winograd-domain weights using the importance factor matrix F to prevent divergence during conventional SGD retraining. The study evaluates spatial-Winograd pruning on datasets like CIFAR-10, CIFAR-100, and ImageNet using PyTorch. The proposed method is compared to Winograd-ReLU pruning on models like VGG-nagadomi. The study evaluates spatial-Winograd pruning on datasets CIFAR-10, CIFAR-100, and ImageNet using models like VGG-nagadomi, ConvPool-CNN-C, and ResNet-18 BID3. The input tile size is set to 6 for 3\u00d73 kernels to achieve higher computation speedup. Lower input tile sizes can lead to higher sparsity due to less correlation between spatial-domain weights. The VGG-nagadomi model on CIFAR-10 dataset with 8 convolutional layers and batch normalization has a prediction accuracy of 93.96%. The study evaluates spatial-Winograd pruning on datasets CIFAR-10, CIFAR-100, and ImageNet using models like VGG-nagadomi, ConvPool-CNN-C, and ResNet-18 BID3. Pruning the first convolutional layer with a fixed Winograd-domain sparsity of 20%, the model achieves a sparsity of 60% with <0.1% accuracy loss. Spatial-Winograd pruning allows for a Winograd-domain sparsity of 63% without changing the network structure. The ConvPool-CNN-C model on CIFAR-100 dataset with 9 convolutional layers and 3x3 kernels has a prediction accuracy of 69.95%. The study evaluates spatial-Winograd pruning on datasets CIFAR-10, CIFAR-100, and ImageNet using models like VGG-nagadomi, ConvPool-CNN-C, and ResNet-18 BID3. Pruning the first convolutional layer with a fixed Winograd-domain sparsity of 20%, the model achieves a sparsity of 60% with <0.1% accuracy loss. Spatial-Winograd pruning allows for a Winograd-domain sparsity of 63% without changing the network structure. The ResNet-18 model is tested on ImageNet, achieving a sparsity of 50% with no accuracy loss using spatial-Winograd pruning. The original model achieved a top-1/top-5 prediction accuracy of 69.82%/89.55%. Despite a decrease in accuracy to 66.67%/87.42% with Winograd-ReLU pruning, the relative accuracies reported in BID9 were used as the baseline. Pruning was applied to 16 convolutional layers in the residual blocks with Winograd-domain sparsity, maintaining the first convolutional layer and downsample layers. Results showed that WinogradReLU pruning achieved a sparsity of 70%/65% with <0.1% top-1/top-5 accuracy loss. Winograd direct pruning was also applied to models with 65% and 70% Winograd-domain sparsity, resulting in minimal accuracy loss. Applying Winograd direct pruning to the model with 70% Winograd-domain sparsity can achieve a higher sparsity of 74%/72% with minimal accuracy loss. The use of the importance factor matrix in Winograd direct pruning adjusts weight importance and gradients for different locations of Winograd-domain weights. However, the accuracy drop is faster compared to spatial structured pruning due to limited accuracy recovery through Winograd retraining. The importance factor matrix is tested in Winograd pruning and retraining. Adjusting weight importance with the matrix reduces accuracy loss during pruning. Using the matrix can decrease accuracy loss from 22% to 10% when pruning to 76% sparsity. It is also effective in Winograd retraining after spatial structured pruning. In this study, a new pruning method called spatial-Winograd pruning is introduced to enhance weight sparsity without altering network structures. The method involves spatial structured pruning and Winograd direct pruning. By adjusting gradients with an importance factor matrix, the convergence during retraining is significantly accelerated, reducing accuracy loss to 0.2% after only 10 epochs. This approach outperforms retraining without gradient adjustment, which only reduces accuracy loss to 0.7%. The spatial-Winograd pruning method involves spatial structured pruning and Winograd direct pruning to enhance weight sparsity without changing network structures. It utilizes an importance factor matrix to adjust weight gradients during retraining in the Winograd domain, allowing for effective retraining with minimal accuracy loss. The method achieves high sparsities in Winograd domain on CIFAR-10, CIFAR-100, and ImageNet datasets. Equations 3 and 9 are determined by input tile size m and weight filter size n. The tensor S is calculated using an equivalent transformation with vectors a and b, and matrix C. The matrix S is obtained using the weight transform matrix G, where each element in the Winograd-domain weight filter Q is calculated. The coefficient tensor S and H can be calculated based on specific equations. The coefficient tensor H is calculated based on specific equations, with different sparsities experimented in different layers using spatial-Winograd pruning. Pruning sensitivity of each convolutional layer is tested to determine which layers need pruning. Only the second convolutional layer in each residual block of ResNet-18 is pruned, as it is more sensitive to pruning than the first layer. In spatial structured pruning, the pruning threshold for the second convolutional layer in each residual block is determined based on pruning sensitivity. The threshold is gradually increased until a 2% drop in validation accuracy is observed. A multiplier \u03b2 is used to adjust the threshold, leading to higher sparsity with a larger \u03b2. The same strategy is applied in Winograd direct pruning for choosing thresholds in different layers. Detailed sparsity distribution analysis is conducted for the pruned ResNet-18 model, focusing on 2D weight filters. Each Winograd-domain weight matrix is treated as a 2D filter, with the example of the last convolutional layer (7-b) examined. The last convolutional layer (7-b) is analyzed with a uniform sparsity of 74% across all pruned layers. More than half of the filters have all weights removed, with a significant number having exactly 20 weights removed. The sparsity distribution inside the filters reveals that the border part of the 6x6 filter, containing 20 weights, is less important than the central part. The central part of the filters is more important than weights in the border part, as removing them leads to a larger difference in output activations. Sparsity distribution inside filters with at least one weight remaining shows a similar pattern."
}