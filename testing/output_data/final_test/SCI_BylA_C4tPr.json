{
    "title": "BylA_C4tPr",
    "content": "Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data, with a focus on simple undirected graphs. However, handling multi-relational graphs, where each edge has a label and direction, is more challenging. Existing approaches suffer from over-parameterization and only learn node representations. CompGCN is a novel framework that embeds nodes and relations in a relational graph, leveraging entity-relation composition operations. It scales with the number of relations and generalizes existing multi-relational GCN methods. Evaluation on tasks like node classification, link prediction, and graph classification shows promising results. Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data, with a focus on simple undirected graphs. Traditional neural network architectures are limited to handling only Euclidean data, while GCNs have been proposed to address this shortcoming and have been applied to various domains. CompGCN is a novel framework that embeds nodes and relations in a relational graph, achieving superior results in tasks like node classification, link prediction, and graph classification. Most existing research on Graph Convolutional Networks (GCNs) focuses on learning node representations in simple undirected graphs, but multi-relational graphs, such as knowledge graphs, are more common. Current GCN approaches for relational graphs suffer from overparameterization and only learn node representations, making them unsuitable for tasks like link prediction. Initial efforts to learn relation representations in graphs have shown performance improvements in tasks like node classification. COMPGCN is a novel framework for incorporating multi-relational information in Graph Convolutional Networks, leveraging composition operations from knowledge graph embedding techniques to embed nodes and relations in a graph. It generalizes existing multi-relational GCN methods and scales with the number of relations in the graph. Extensive experiments demonstrate its effectiveness in tasks like node classification, link prediction, and graph classification. Graph Convolutional Networks (GCNs) generalize Convolutional Neural Networks (CNNs) to non-Euclidean data. GCNs were first introduced by Bruna et al. (2013) and later made scalable through efficient localized filters in the spectral domain (Defferrard et al., 2016). Various extensions of GCNs have been formulated recently. Most existing GCN methods follow the Message Passing Neural Networks (MPNN) framework for node aggregation. The proposed method in COMPGCN is an instantiation of the MPNN framework and has been made available with the source code and datasets on GitHub. Our proposed method is a specialized instantiation of the MPNN framework for relational graphs. Previous methods have focused on direction-specific filters and relation-specific filters, but our method is a more generic framework that can leverage any type of relation in multirelational graphs. In this section, the performance of COMPGCN on link prediction is evaluated using methods of translational, semantic matching based, and neural network based KG embedding approaches. The overview includes Graph Convolutional Networks (GCNs) for undirected graphs. Graph Convolutional Networks (GCNs) are discussed for undirected graphs and their extension to directed relational graphs. GCNs encode the immediate neighborhood of each node in the graph using normalized adjacency matrices and activation functions. Multiple GCN layers can be stacked to capture multi-hop dependencies in the graph. For multi-relational graphs, GCNs can be applied with multiple relations to enhance the representation of nodes. The GCN formulation for multi-relational graphs includes direction-specific weight matrices to address over-parameterization. The proposed method, COMPGCN, represents a multi-relational graph with relation-specific parameters and an overall architecture shown in Figure 1. The COMPGCN model represents a multi-relational graph with initial relation features and allows information flow in both directions. It extends edges and relations with inverse edges and weighted relations. Representing relations as vectors helps alleviate over-parameterization and allows for better utilization of GCNs on relational graphs. The COMPGCN model incorporates relation embeddings into GCN formulation using entity-relation composition operations from Knowledge Graph embedding approaches. It restricts operations to non-parameterized ones like subtraction, multiplication, and circular-correlation, with potential extension to parameterized operations like Neural Tensor Networks and ConvE. The choice of composition operation impacts the quality of the model. The COMPGCN model integrates relation embeddings into GCN using non-parameterized composition operations like subtraction, multiplication, and circular-correlation. The choice of composition operation significantly affects the model's quality. In future developments, superior composition operations for Knowledge Graphs can enhance COMPGCN's performance. COMPGCN integrates relation embeddings into GCN using direction-specific weights and learnable transformation matrices. It scales with increasing number of relations by using a variant of basis formulations. Table 1 contrasts COMPGCN with other methods in terms of features and parameter complexity. In COMPGCN, relation representations are expressed as a linear combination of basis vectors, with learnable scalar weights. Unlike Relational-GCN, COMPGCN uses embedding vectors instead of matrices and defines basis vectors only for the first layer, sharing relations through transformations in later layers. This parameter-efficient model can be extended to k-stacked layers. In COMPGCN, relation representations are expressed as a linear combination of basis vectors with learnable scalar weights. The model outperforms existing methods on various metrics in link prediction tasks on knowledge graphs like FB15k-237 and WN18RR. In our experiments, we evaluate COMPGCN on datasets FB15k-237 and WN18RR for link prediction tasks. We also assess Node Classification on MUTAG and AM datasets, and Graph Classification on MUTAG and PTC datasets. Comparison is made against Relational-GCN methods for relational graphs. In evaluating COMPGCN, comparisons are made with Relational-GCN methods for relational graphs like R-GCN, D-GCN, and W-GCN. Additionally, comparisons are also made with various task-specific baselines for link prediction tasks, including TransE, DistMult, ComplEx, KBGAN, ConvE, ConvKB, and SACN. In this section, various graph classification methods are compared, including Feat, WL, RDF2Vec, PACHYSAN, DGCNN, and GIN. The performance of COMPGCN on link prediction and the impact of different GCN encoders and compositional operators are also evaluated. The scalability of COMPGCN with the number of relations in the graph is examined. In this section, the performance of COMPGCN is evaluated on link prediction tasks using baseline methods. COMPGCN outperforms existing methods on FB15k-237 and WN18RR datasets in multiple metrics. The potential use of rotation operation in a complex variant of COMPGCN for further performance improvement is suggested for future work. In this study, different GCN methods are evaluated as encoders for link prediction tasks. COMPGCN is tested with various composition operators inspired by TransE, DistMult, and HolE. Results are summarized in Table 4, showing promising performance similar to previous research. Future work includes exploring ways to further improve performance. Utilizing Graph Convolutional based methods as encoders improves performance for most score functions. COMPGCN outperforms baseline GCN methods, showing a relative increase in MRR with different score functions. COMPGCN learns entity and relation embeddings jointly, leading to more expressive representations. It performs best with ConvE for link prediction. The impact of composition operators on link prediction is also discussed. In Table 4, composition operators are compared, showing that more complex operators like circular-correlation outperform simpler ones. The scalability of COMPGCN is analyzed with varying numbers of relations and basis vectors, outperforming existing methods. Experiments are conducted on the FB15k-237 dataset using the best performing model (ConvE + COMPGCN (Corr)). The performance of COMPGCN improves with an increasing number of relation basis vectors. With B = 100, the model's performance becomes comparable to using individual embeddings for each relation. The best performing model with B set to 50 also outperforms baselines in all settings. Additionally, using 5 relation basis vectors (B = 5) shows comparable performance to using separate vectors for each relation in the dataset. In the dataset, COMPGCN with a limited basis shows comparable performance to the full model across different numbers of relations. It outperforms R-GCN consistently and is more effective at encoding multirelational graphs. Experimental results for node and graph classification tasks are presented in Table 5. COMPGCN outperforms baseline methods on node classification and shows comparable performance on graph classification. It leverages composition operators from Knowledge Graph embedding techniques to embed nodes and relations in a graph effectively. The proposed framework demonstrates an average improvement of 3% on both node and graph classification tasks compared to baseline methods. Knowledge Graph embedding techniques are used in COMPGCN to jointly embed nodes and relations in a graph. The method improves performance on all types of relations, with an average improvement of around 10% on MRR for one-to-one relations compared to the best baseline. For one-to-many, many-to-one, and many-to-many relations, the corresponding improvements are 10.5%, 7.5%, and 4%. This shows that COMPGCN is effective at handling both simple and complex relations. Results on link prediction by relation category on FB15k-237 dataset show that COMPGCN improves performance on all types of relations compared to existing methods. The datasets used in the experiments include FB15k-237 and WN18RR, which are subsets of larger datasets with inverse relations removed. The curr_chunk discusses datasets used for node classification and graph classification tasks, including MUTAG for node classification and graph classification, AM dataset for artifact categorization, and details of the datasets used for different tasks. The curr_chunk provides details on datasets used for node classification, link prediction, and graph classification tasks. It includes information on the PTC Srinivasan dataset consisting of 344 chemical compounds for labeling based on carcinogenicity. Implementation details for each task are presented, including the use of COMPGCN in the PyTorch geometric framework for training link prediction models with binary cross entropy loss. For node classification and graph classification tasks, binary cross entropy loss with label smoothing is used. Validation accuracies are reported across 10 folds cross-validation. Adam optimizer is used for training with Xavier initialization for parameter initialization. Number of GCN layers ranges from 1 to 3. Hyperparameters for link prediction task: Glorot & Bengio (2010) initialization, GCN Layer (K) {1, 2, 3}, Learning rate {0.001, 0.0001}, Batch size {128, 256}, Dropout {0.0, 0.1, 0.2, 0.3}. Refer to Section A.3 for more details."
}