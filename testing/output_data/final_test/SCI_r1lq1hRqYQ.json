{
    "title": "r1lq1hRqYQ",
    "content": "Reinforcement learning is used for control problems, but creating reward functions is challenging. Language-conditioned reward learning (LC-RL) uses language commands as reward functions through inverse reinforcement learning. This approach is more transferable than language-conditioned policies to new environments. Our model uses deep neural networks to learn rewards from natural language commands, which transfer to new tasks and environments. Reinforcement learning requires specifying objectives as reward functions, which can be challenging, especially with complex image-based observations. Language-conditioned policies often lead to poor performance compared to using language as reward functions. Building natural language interfaces for reinforcement learning agents involves mapping observations and language commands to actions to perform tasks. This approach requires the policy to understand both the task in the physical world and the language command itself. Generalization to new environments is crucial for the system's performance. In building natural language interfaces for reinforcement learning agents, the key is to convert language-defined goals into reward functions. This allows the agent to learn how to plan and perform tasks independently through reinforcement learning, without relying on pre-defined policies for different environments. In this work, the feasibility of grounding free-form natural language commands as reward functions using inverse reinforcement learning (IRL) is investigated. Learning language-conditioned rewards presents unique computational challenges, with the method based on an exact MaxEnt IRL procedure requiring full knowledge of environment dynamics to train a language-conditioned reward function represented by a deep neural network. The study explores using inverse reinforcement learning (IRL) to ground natural language commands as reward functions for deep neural networks. By utilizing dynamic programming methods during training, a reward function can be learned without requiring knowledge of environment dynamics. This approach allows for evaluating reinforcement learning agents during test time without understanding the underlying environment dynamics. The method is tested on indoor house navigation tasks with natural language commands, showing generalization to new tasks and scenes. Learning a language-conditioned policy for reinforcement learning leads to poor performance and lacks generalization. Different approaches include behavioral cloning with supervised learning, rewarding completion of tasks, hand-designing reward functions, semantic parsing, and converting language into executable actions. In a related task, agents must answer questions based on instructions. In an embodied question-answering task, agents must produce answers based on information in the environment. A hybrid approach is used, combining supervised learning with rewards for completing tasks. Policy-based approaches show worse generalization performance, relying on zero-shot generalization at test time. One reason to choose a policy-based approach is the inability to run reinforcement learning in new environments. Another approach is learning a mapping from language to reward functions. Several works apply IRL procedures to the language grounding problem. In the context of language grounding, various works explore learning language-conditioned reward functions. Different methods like BID14 and BID25 utilize IRL or task-completion classifiers, with varying approaches to reward function learning. Some methods involve hand-designed symbolic reward functions, while others use generic, differentiable function approximators for handling diverse observations, including raw images. Our work applies language-conditioned inverse reinforcement learning to environments with image observations and deep neural networks, building off the MaxEnt IRL model. This model considers an entropy-regularized Markov decision process and partially-observed environments, showing that our rewards generalize to novel tasks and environments. Inverse reinforcement learning (IRL) aims to infer the reward function from expert demonstrations, where trajectories are observed with probabilities proportional to the exponentiated returns. Learning a reward function is equivalent to fitting an energy-based model to the maximum likelihood objective. The gradient to update the reward function is calculated using a specific formula. The language-conditioned IRL problem aims to learn a reward function that generalizes across multiple tasks. Previous works have explored multi-task scenarios in Bayesian or meta-learning settings. Tasks are formalized as MDPs, where each task may have different state spaces, dynamics, and reward functions. The multi-task objective involves optimizing q-iteration and the forward algorithm to update reward functions across tasks with shared observation and action spaces. Observations are 32x24 images from different environments, allowing for a common reward function despite varying state spaces. The text discusses using a shared reward function and language commands as proxies for context in a multi-task setting. Stochastic gradient descent is used for computational efficiency, and language-conditioned reward functions are learned using maximum causal entropy IRL. The algorithm allows for learning tasks from inferred reward functions in new environments using model-free RL algorithms. Our algorithm, summarized in Algorithm 1, updates the reward function using Maximum Entropy IRL gradient BID26. The stochastic gradient update is adapted for a single task \u03be, where the reward is a function of observation, action, and language. This approach allows for evaluating the reward without knowing the underlying state space and dynamics of the environment, similar to training a robot in known environments like a laboratory. Training a robot in known environments like a laboratory allows for computing optimal policies exactly using Q-iteration. This approach updates the reward function using Maximum Entropy IRL gradient, enabling evaluation of rewards without requiring knowledge of the environment's dynamics. The network architecture includes a language input represented by a sequence. The agent has two main input modalities: language input represented by one-hot vectors and panoramic image observations. The language embedding is generated using an LSTM network, while the image embedding is created by passing images through a CNN with shared weights. The image embedding is formed by combining CNN outputs from different image directions, followed by element-wise multiplication and processing through a fully-connected network to generate a reward output. The max global-pooling architecture in the CNN helps select objects in a scene, with language embedding modulating feature attention. Evaluation is done in simulated indoor environments using the SUNCG BID22 dataset. The SUNCG dataset provides 3D house environments for tasks like navigation and pick-and-place. Tasks involve navigating to specific locations or moving objects. Semantic labels are used as input. Each environment corresponds to a 3D scene. The agent in the kitchen environment receives observations in the form of a language command and a panoramic image. The panoramic view helps the agent navigate and interact with objects. The agent has 4 actions: move forward, turn left or right, and interact with objects based on language commands. Language commands are generated using a preset grammar and include object and location names. The agent in the kitchen environment receives language commands using a preset grammar, involving tasks like \"go to X\" or \"move X to Y\". The interact action is specific to the PICK task, allowing the agent to pick up or drop objects. The state space is limited to one object and two locations per task, increasing the state-space by a factor of 3. The model must learn to detect objects in different locations across environments, rather than memorizing specific object-location associations. The dataset is split into training, \"task\" test set (novel object-location combinations in familiar houses), and \"house\" test set (tasks in new houses). This split evaluates the model's generalization abilities with varying levels of difficulty. The dataset contains 1413 tasks with 14 objects and 76 house layouts. There are 1004 tasks in the training set, 236 in the \"task\" test set, and 173 in the \"house\" test set. Two methods for reward learning are evaluated: LC-RL, which learns a shared reward function using demonstration and language pairs, and \"Reward Regression\", which regresses directly onto ground-truth rewards for oracle performance evaluation. The evaluation shows oracle performance on the task without requiring ground-truth rewards. Success rates are compared using Q-iteration and DQN methods. The experiment tests reward learning in novel environments without needing dynamics knowledge. The experiment compared different reward learning methods in novel environments, including epsilon-greedy exploration and reward shaping with a state-based potential. Results were also compared against baselines using GAIL and AGILE, highlighting the difficulty of the RL problem even with proper reward learning. The model struggled to learn rewards using a reinforcement learning-based policy optimizer and could only solve simpler environments. The experiment compared reward learning methods in novel environments, showing the gap between sampling-based and exact solvers. GAIL-Exact performed comparably to LC-RL on training but worse on test environments. GAIL's discriminator doesn't match the true reward function, leading to poorer performance in new environments. Optimal behavioral cloning baseline was also compared, trained using Q-iteration for fair comparison. The experiment compared reward learning methods in novel environments, showing the gap between sampling-based and exact solvers. GAIL-Exact performed comparably to LC-RL on training but worse on test environments. Both LC-RL and Reward Regression were able to learn reward functions that generalize well. Additional details and results can be found in the provided links and appendices. Both LC-RL and Reward Regression learned reward functions that generalize to novel tasks and house layouts, with Reward Regression outperforming LC-RL due to oracle supervision. Common errors included rewarding reaching the goal without completing tasks. Ambiguity in language commands and lack of agent clarification impacted performance on tasks. The cloning baseline showed poor performance on training and testing environments, struggling to learn across multiple environments due to high-level language descriptions. The task is challenging for a policy-learning agent as it requires mapping language to house layouts instead of following step-by-step instructions. DQN with epsilon-greedy exploration was used for re-optimization of learned rewards. Adding a shaping term based on the value-function improves results in re-optimizing learned rewards, but requires ground-truth knowledge of the environment dynamics. Rewards learned through regression are easier to re-optimize than those learned through IRL, possibly due to the noisy nature of IRL rewards. Language-conditioned reward learning (LC-RL) is introduced as a method for training neural networks to learn reward functions in interactive environments. This approach outperforms policy-learning in test environments by enabling agents to learn and interact within the environment rather than relying on pre-existing policies. In LC-RL, agents learn reward functions in interactive environments, outperforming policy-learning by enabling learning within the environment. The agent has a 30 time-step limit to complete tasks, receiving a reward of 10 for success. Demonstrations are sampled from the optimal policy with a discount factor of \u03b3 = 0.99. Training involves 10 demonstrations per environment for IRL, optimized with Adam at a learning rate of 5 * 10 \u22124. The CNN architecture includes 5x5 and 3x3 convolutions with 16 and 32 filters respectively, with an embedding size of 32. Increasing filter or embedding sizes did not significantly impact performance. Architecture selection was done through hyper-parameter sweep. The architecture selection for the image embedding involved a hyper-parameter sweep, comparing global pooling, fully connected layer, and FiLM BID20. The combination of language and image embeddings was tested with point-wise multiplication or pooling versus concatenation. An example of a learned reward and value function from the IRL model is shown for a task involving moving a fruit bowl to the bathroom. The rewards/values are shown before and after acquiring an object, directing the agent from the fruit bowl to the bathroom. Different methods like IRL, GAIL, and reward regression are used to learn rewards. Tasks include moving a fruit bowl to the bathroom, with high values in blue and low in red. A birds-eye view highlights the object in green and the agent with a green triangle. In general, rewards learned by IRL and GAIL tend to be noisy and contain small artifacts, which can be detrimental when using RL to reoptimize the learned reward. The running time for dynamic programming algorithms scales with the size of the state space, and reward evaluation becomes a major bottleneck in runtime. One major optimization made to the algorithm is caching computation on repeated observations to speed up reward evaluation, which was identified as a bottleneck in runtime. This optimization reduces the need to evaluate rewards for all states and actions, making the process more efficient. Caching repeated computation significantly speeds up reward computation, resulting in 10-100 times speedups depending on the environment's structure."
}