{
    "title": "H1lac2Vtwr",
    "content": "Fine-tuning with pre-trained models like BERT has shown great results in language tasks. However, BERT ignores information between layers during fine-tuning and can improve in emphasizing local contexts. To address this, SesameBERT is proposed, which extracts global information through Squeeze and Excitation and enhances local information with Gaussian blurring. The effectiveness of SesameBERT was demonstrated on the HANS dataset. SesameBERT outperformed BERT on the GLUE benchmark and HANS evaluation set, showcasing the effectiveness of the approach in detecting shallow heuristics in models. Unsupervised pretrained models have dominated the NLP field, involving pretraining and fine-tuning steps to learn word embeddings and optimize parameters for downstream tasks. Traditional word embeddings are learned from large unstructured textual data, representing each word with an independent vector. Pretrained contextual representations like BERT, OpenAI GPT, and ELMo have gained attention in NLP for their ability to learn sentence-level information and generate multiple word embeddings. BERT is a leading method in this field, surpassing conventional word-embedding techniques by extracting more comprehensive semantic information. BERT, a state-of-the-art network, utilizes the Transformer encoder architecture and outperforms other models on the GLUE benchmark. Self-attention is effective in extracting latent meanings of sentence embeddings. This study aims to enhance contextualized word embeddings for a classifier by introducing the SESAME architecture during fine-tuning. The effectiveness of selective layer approaches in BERT usage remains unclear. The study introduces the SESAME architecture to enhance contextualized word embeddings for a classifier during fine-tuning. It focuses on the Squeeze and Excitation method for feature recalibration and the self-attention mechanism for capturing global dependencies. The proposed strategy involves Gaussian blurring to emphasize local contexts in sentence embeddings through convolution. The study introduces the SESAME architecture to enhance contextualized word embeddings for a classifier during fine-tuning. It focuses on the Squeeze and Excitation method for feature recalibration and the self-attention mechanism for capturing global dependencies. Through experiments, SesameBERT outperformed the BERT model in most GLUE tasks and showed improvement in the HANS dataset designed to diagnose fallible structural heuristics. The study introduces SesameBERT, which performs well in the HANS dataset, indicating it doesn't rely solely on heuristics. The final model proved competitive in various tasks, utilizing contextualized word representations like ELMo to capture word meanings in different contexts. The study introduces SesameBERT, a model that performs well in the HANS dataset by utilizing contextualized word representations like ELMo. It discusses the challenges of layer selection in learning how to apply language models like BERT and the Squeeze and Excitation method introduced to enhance network representations. Convolutional neural networks use filters to extract features from images, but lack interdependencies between channels. Squeeze and Excitation allows for feature recalibration using global information. Self-attention networks capture global dependencies without considering distances. Gaussianblurring method focuses on local contexts while capturing long-range dependencies. In neural networks, Gaussian bias is used to improve model performance by performing element-wise operations in a specific layer. A recent study raised concerns about neural networks adopting shallow heuristics instead of learning underlying generalizations. For example, in computer vision and natural language inference, models may be misled by contextual heuristics. In the field of natural language inference, models may predict contradictory labels due to the presence of the word \"not\" in training sets. The study aimed to enhance SesameBERT's robustness across all training sets by using HANS datasets to identify structural heuristics. The focus was on BERT, an encoder architecture with multiple Transformer layers, each containing self-attention and feed-forward sublayers. The self-attention mechanism involves querying dictionary entries with key-value pairs. The self-attention mechanism in BERT involves query, key, and value pairs for generating output. Attention weights help understand the importance of key-value pairs in generating the output. Pretrained BERT model is used for fine-tuning and creating contextualized word embeddings. In this study, the application of Squeeze and Excitation (Hu et al., 2018) was proposed for extracting contextualized embeddings from various layers. The output of the encoder layer was concatenated to form feature maps before feeding into the classifier. Global spatial information was squeezed into a layer descriptor using global average pooling in the squeeze step. In this study, the Squeeze and Excitation method was used to extract contextualized embeddings from different layers. The output of each layer was concatenated to form a three-dimensional tensor, which was then weighted and fed into the classifier. The excitation step aimed to capture layer-wise dependencies using a gating mechanism with fully connected layers. The Squeeze and Excitation method was utilized to extract contextualized embeddings from different layers. The model transformed input sequences into queries, keys, and values for multihead attention, enabling joint information attendance from various representation subspaces. The rescaled feature maps were concatenated to form u, and a weighted average layer u avg was calculated. The text discusses the use of Gaussian blur for capturing local dependencies in attention outputs. Different strategies are explored to apply convolutional operations, enhancing the central word's information with neighboring words weighted by a Gaussian distribution. This approach aims to improve the localness of attention outputs through a parameter-free 1D convolution operation. The text explores enhancing local dependencies in attention outputs using a parameter-free 1D convolution operation with a Gaussian kernel. It focuses on the interaction of cross-value vectors rather than cross-query vectors, resulting in improved attention output representation. The SesameBERT model utilizes a multihead mechanism to capture distinct linguistic properties in each head. Based on BERT with 12 layers, locality modeling is applied through Squeeze and Excitation to exploit global information and local properties. Evaluation was done through multiple classification tasks, comparing results with a reimplementation of BERT in TensorFlow. Parameters such as batch size, learning rate, and number of epochs were kept similar to the original BERT model for reproducibility. The paper discusses training models on the GLUE benchmark using a GPU, specifically the NVIDIA Tesla V100. The GLUE benchmark consists of nine NLU tasks, and the study includes details on the datasets and metrics used. A new evaluation set called HANS is used to identify structural heuristics in models. The MNLI dataset relies on lexical overlap heuristics to determine label contradictions, while the HANS dataset exposes models that rely on these heuristics. BERT performed well on MNLI but poorly on HANS, indicating its use of heuristics. MNLI has three labels, while HANS translates Contradiction or Neutral instances differently. The HANS dataset has two labels: Entailment and Non-entailment, targeting Lexical overlap, Subsequence, and Constituent heuristics. It serves to measure progress and visualize model shortcomings. Experiment results of baseline and proposed models show Gaussian blurring on attention outputs. SesameBERT was fine-tuned on 9 downstream tasks with different blur kernel sigmas. The study experimented with Gaussian blur kernels of different sigmas alongside BERT on GLUE datasets. SesameBERT outperformed the original BERT-Base model in tasks like RTE and AX. Results were uploaded to the GLUE server for evaluation, showing improvements in accuracy. The study compared SesameBERT with BERT-Base, ELMo, and OpenAI GPT using GLUE scores. Different layers showed varying abilities in capturing word relationships. Lower layers had lower accuracy due to deficient context in word embeddings. Ablation studies were conducted to analyze attention output layers. The ablation studies revealed that higher layers benefit more from Squeeze and Excitation, while lower layers benefit from Gaussian blurring to capture short-range dependencies. SesameBERT outperformed other methods in higher layers, indicating its effectiveness in capturing word relationships. In higher layers, using Squeeze and Excitation alongside Gaussian blurring helps self-attention models capture global information. BERT achieved 84.6% accuracy on the MNLI-m dataset, higher than SesameBERT. Both models performed well on heuristics labeled as Entailment, but BERT struggled with Non-entailment cases. The proposed method outperformed BERT in cases of \"Lexical overlap\" by almost three times. BERT struggled with all three cases, indicating reliance on shallow heuristics. The paper suggests using Gaussian blurring to capture local contexts in self-attention networks to prevent heuristic adoption. SesameBERT, a fine-tuning approach based on BERT, aims to improve model performance. SesameBERT, a fine-tuning approach based on BERT, aims to improve self-attention networks by finding high-quality attention output layers, utilizing Squeeze and Excitation to extract information, and applying Gaussian blurring to capture local contexts. Experimental results on GLUE datasets show SesameBERT outperforms the BERT baseline model, with improved weight distributions and effectiveness of Gaussian-blurring approaches. Additionally, SesameBERT shows advantages over BERT in learning desired information rather than shallow heuristics, as demonstrated on the HANS dataset."
}