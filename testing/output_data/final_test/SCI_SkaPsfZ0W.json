{
    "title": "SkaPsfZ0W",
    "content": "Graph Convolutional Networks (GCNs) have been successful in semi-supervised learning on graph-structured data. A new model, Network of GCNs (N-GCN), combines GCNs with unsupervised learning of graph embeddings using random walks. N-GCN trains multiple GCNs over node pairs discovered at different distances in random walks and optimizes the classification objective. The N-GCN model achieves state-of-the-art performance on node classification tasks like Cora, Citeseer, Pubmed, and PPI, and is resilient to adversarial input. GraphSAGE methods like N-SAGE propose resilience to adversarial input perturbations in semi-supervised learning on graphs for real-world applications such as predicting user interests in social networks or screening patients for cancer. Convolutional Neural Networks learn hierarchical filters for significant improvements in Computer Vision tasks, motivating researchers to extend convolutions spatially. Researchers have extended convolutions from spatial to graph-structured domains, creating Graph Convolutional Networks (GCNs) for semi-supervised learning on graphs. The goal is to recover labels for unlabeled nodes using node features, known labels, and the graph structure. The model for semi-supervised node classification builds on the GCN module proposed by BID14, operating on the normalized adjacency matrix\u00c2. The proposed extension of GCNs, N-GCN, combines information from different step-sizes by feeding each module a different power of the normalized adjacency matrix. The output of all GCN modules is then combined into a classification sub-network, which is jointly trained on the upstream objective. The N-GCN model combines information from different step-sizes by feeding each module a different power of the normalized adjacency matrix. The output of all GCN modules is then combined into a classification sub-network, which is jointly trained on the upstream objective. This approach achieves state-of-the-art results in semi-supervised node classification. The classification sub-network weights shift towards GCN modules utilizing higher powers of the adjacency matrix in the presence of input perturbations, effectively widening the receptive field of the convolutional filters. The paper discusses how explicit random walks enhance the representational power of vanilla GCN's in semi-supervised graph learning tasks. It also compares the proposed method with closely-related methods and includes experimental evaluation. Traditional label propagation algorithms learn a model that transforms node features into node labels using the graph as a regularizer term. Graph Convolution generalizes convolution from Euclidean domains to graph-structured data. BID7 introduces graph convolution for graph-structured data by transforming filters and signals to the Fourier domain, multiplying them, and then transforming back. The method involves a low-rank approximation of the eigendecomposition using truncated Chebyshev polynomials. BID14 proposes a rank-1 approximation. They suggest a multi-layer Graph Convolutional Networks (GCNs) for semi-supervised graph learning, where each layer computes a transformation. A two-layer Graph Convolutional Network (GCN) model is defined using vertex features X and normalized adjacency, with trainable weights W (0) and W (1). The model utilizes a softmax function for classification and a non-linearity function \u03c3(\u00b7) for signal processing. The GCN parameters are trained to minimize cross-entropy error over labeled examples, resulting in an output matrix R N. Node Embedding methods represent graph nodes in a continuous vector space by learning a dictionary Z \u2208 R N \u00d7d with d-dimensional embeddings per node. Modern graph embedding methods, inspired by Skipgram models on text corpora, use random walks to learn node embeddings. Each random walk generates a sequence of nodes, which are then converted to textual paragraphs and passed to a word2vec-style embedding learning algorithm. This learning-by-simulation approach is equivalent, in expectation, to minimizing cross-entropy error over labeled examples in GCN models. The learning-by-simulation approach in BID20 is equivalent to decomposing a random walk co-occurrence statistics matrix D. This method weights the importance of nodes based on their connectivity and distance. Unlike traditional node embedding methods, random walk methods optimize by minimizing a loss on representing random walk co-occurrence statistics D. Graph Convolutional Networks and random walk graph embeddings are both powerful techniques. BID14 utilizes GCNs for semi-supervised node classification by using the adjacency matrix for training and inference. Recent work has shown that random walk statistics can be powerful for learning unsupervised node representations that preserve graph structure. Under certain conditions, GCN models can learn random walks, particularly with specific identity conditions in the model. The GCN model operates on random walk statistics to better utilize information across distant nodes, unlike traditional methods that rely on adjacency matrices. This approach allows for more effective learning and preservation of graph structure. The proposed method suggests feeding a K-degree polynomial of adjacency matrices to multiple instantiations of GCN, allowing for better utilization of information across distant nodes. By adding self-connections and converting directed graphs to undirected ones, the eigendecomposition of symmetric matrices remains valid, enabling efficient approximation of multiplication in the Fourier domain. The latent representation of each GCN for node v is described by a row in the output, with a consistent latent dimensionality Ck for simplicity. The proposed method involves feeding a K-degree polynomial of adjacency matrices to multiple instances of GCN with a consistent latent dimensionality Ck. The output of all K GCNs is combined and fed into a classification sub-network for joint training via backpropagation. This allows the model to learn a combination of features using raw adjacency, random walks, and input features X. The classification network is represented as a fully-connected layer, with the output of the K GCNs concatenated along the column dimension. A fully-connected layer f_fc is added with trainable parameter matrix W_fc, jointly trained with GCN parameters. The proposed method involves using a classification network with softmax attention to learn a combination of features from multiple instances of GCN. The attention model is parametrized by a vector m and is similar to the \"Mixture of Experts\" model. By setting the number of output channels for all GCNs equal to the number of classes, cross entropy loss terms can be added to force all GCNs to be independently useful. The parameter vector m can be set manually using the validation split. The proposed method involves setting the parameter vector manually using the validation split for reasonable K values. Different choices for the parameter vector can be made, such as setting m 0 to a small value and the remaining values to a harmonic series. The output of GCNs consuming lower powers of \u03b1 should be weighted highly if a node's information is captured by its direct or nearby neighbors. The model minimizes cross entropy between the output and training labels, and intermediate supervision can be applied to make all GCNs independently useful. The N-GCN derivations assume one GCN per power, but the implementation feeds every to r GCN modules. The derivation applies to various graph models including GraphSAGE. Algorithms are provided for creating networks of arbitrary graph models, such as GCN and SAGE. The full Network of GCN model (N-GCN) is defined by combining Algorithms 1 and 2. The original algorithms GCN and SAGE can be recovered using N-GCN and N-SAGE with specific parameters. The text discusses modifying algorithms for creating networks of various graph models like GCN and SAGE. It includes details on recovering original models and experimental setups on different datasets. The text discusses datasets for Protein-Protein Interactions (PPI) and citation networks. It includes details on node representation, edge connections, and feature vectors extracted from article abstracts for predicting article subjects. The datasets have specific training, validation, and evaluation node allocations. The PPI graph consists of 24 subgraphs representing different human tissues, with 20 for training, 2 for validation, and 2 for testing. Baseline numbers for citation datasets and PPI are copied from previous studies. Various algorithms like LP, SemiEmb, ManiReg, DeepWalk, ICA, Planetoid, and GCN are used for evaluation. Node classification performance using various algorithms on different datasets is reported in Table 2. Results from previous studies are copied for rows (a) through (g), while rows (h) and (i) are from another dataset. Rows (j) through (l) are generated using the authors' code, and rows (m) and (n) represent their models. The test accuracy is reported for the run with the highest validation accuracy. Our implementation includes competitive baselines like SAGE and GCN. Our methods and baselines, including GCN, SAGE, and DCNN, are implemented using TensorFlow. We use 2-layer GCN and SAGE modules with specific output dimensions. The DCNN baseline has one layer with 16 dimensions per node. We apply dropout and L2 regularization to all models. Node classification accuracy results are shown in Table 2, with models trained using Adam optimizer for 600 steps. The study compares different models for node classification accuracy, using hyperparameters r, K, and choice of classification subnetwork. The models are trained with Adam optimizer for 600 steps, capturing parameters at peak validation accuracy. Results on the Pubmed dataset show that convolution-based methods work well with few training examples, while random walk methods perform better with more data. The proposed methods combine convolution and random walks, showing effectiveness in both scenarios. The study compares different models for node classification accuracy, combining convolution and random walks to achieve state-of-the-art results on all datasets. Analyzing the impact of K and r on classification accuracy, it is found that adding random walks improves model accuracy by providing additional information. Testing under feature noise perturbations shows practical implications for real-world applications. The study compares models for node classification accuracy, combining convolution and random walks for state-of-the-art results. Removing features shows that methods like N-GCN, N-SAGE, and DCNN outperform GCN and SAGE. Attention weights shift towards higher powers of features when maliciously dropped. The field of graph learning algorithms is rapidly evolving. The study compares models for node classification accuracy, combining convolution and random walks for state-of-the-art results. BID8 defines graph convolutions as a K-degree polynomial of the Laplacian, where the polynomial coefficients are learned. The major difference is the order of random walk versus non-linearity in their model compared to ours. The study compares models for node classification accuracy, combining convolution and random walks for state-of-the-art results. BID8 defines graph convolutions as a K-degree polynomial of the Laplacian, where the polynomial coefficients are learned. Our model calculates k q k g( A k ), using a GCN module, which differs from DCNN proposed by BID2. Our model utilizes GCN layers that multiply by A k at every layer, making DCNN a special case when GCN module contains only one layer. We propose a meta-model that can run arbitrary Graph Convolution models on the output of random walks. The study introduces a model, Network of GCNs, that combines graph convolutions and random walks for node classification. The model operates on the adjacency matrix, learning information from near or distant neighbors. It effectively handles adversarial perturbations by shifting weights towards instances consuming higher powers of the adjacency matrix. Future work includes extending the method to larger graph datasets and implementing a stochastic approach. Algorithm 4, SAGE Model BID10, utilizes mean-pool aggregation and operates in full-batch. It requires normalization of Z and returns a network using SAGE with mean-pooling aggregation. The model differs from a vanilla GCN model in adjacency normalization and skip connections. Future work includes implementing a stochastic approach for edge sampling. In SAGE Model BID10, features are concatenated with diffused features using adjacency multiplication. Node-wise L2 feature normalization is applied, similar to layernorm transformation. The model allows for different aggregations like max-pooling or LSTM, deviating from GCN. Model selection is crucial due to small labeled nodes to avoid overfitting. Hyperparameters like r, K, classification network choice, and random seed significantly impact test accuracy. The study explores the impact of hyperparameters r, K, classification network choice, and the use of 0 on N-GCN and N-SAGE models. Results show that unmodified random walks improve performance by gathering information from nearby and distant nodes. Tables display N-GCN results on Citeseer dataset, showcasing the performance with different settings of K and r. Table 5: N-GCN results on Citeseer dataset with 0 enabled show varying performance with different hyperparameters. Table 6 displays N-GCN fc results with 0 disabled, while Table 7 shows N-GCN fc results with 0 enabled. Table 9 presents N-SAGE results on Citeseer dataset with 0 enabled. Table 10: N-SAGE fc results on Citeseer dataset, with 0 disabled, show performance of SAGE models with different hyperparameters. Table 11: N-SAGE fc results on Citeseer dataset, with 0 enabled. Table 12: N-GCN a results on Cora dataset, with 0 disabled, display performance of GCN models. Table 13: N-GCN results on Cora dataset. Table 14: N-GCN fc results on Cora dataset, with 0 disabled, show performance of GCN models with different hyperparameters. Table 15: N-GCN fc results on Cora dataset, with 0 enabled. Table 16: N-SAGE a results on Cora dataset, with 0 disabled, display performance of SAGE models. Table 17: N-SAGE a results on Cora dataset, with 0 enabled. Table 17: N-SAGE results on Cora dataset, with 0 enabled, show performance of SAGE models. The results indicate varying performance based on different hyperparameters. Table 21: N-GCN results on Pubmed dataset, with 0 enabled, show performance of GCN models. Table 22: N-GCN fc results on Pubmed dataset, with 0 disabled, display results for ensemble of GCN models. Table 23: N-GCN fc results on Pubmed dataset, with 0 enabled, show performance of GCN models. Table 25: N-SAGE results on Pubmed dataset, with 0 enabled. Table 25: N-SAGE results on Pubmed dataset, with 0 enabled, display performance metrics for N-SAGE models. Table 26: N-SAGE fc results on Pubmed dataset, with 0 disabled, show results for ensemble of SAGE models. Table 27: N-SAGE fc results on Pubmed dataset, with 0 enabled, present additional performance metrics."
}