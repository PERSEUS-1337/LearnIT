{
    "title": "B1lmSeHKwB",
    "content": "Zero-Shot Learning (ZSL) involves classifying unseen classes without labeled training images by using side information like semantic attributes. The key step is bridging visual to semantic space through nonlinear embedding. A novel approach proposed in this paper is to treat ZSL as a fully connected neural network with cross-entropy loss, embedding visual space into semantic space. Soft-labeling is used during training to introduce unseen visual information based on semantic similarities between seen and unseen classes. The proposed model utilizes soft-labeling based on semantic similarities for zero-shot learning. It achieves state-of-the-art performance on benchmark datasets and outperforms supervised classifiers in fine-grained classification. Humans can recognize new classes due to their ability to transfer prior knowledge. Zero-shot learning aims to mimic human ability to transfer prior knowledge to recognize new objects, reducing the need for labeling. It involves using semantic information for unseen classes and leveraging trained classifiers on seen classes to predict unseen classes. Generalized Zero-shot Learning (GZSL) addresses the limitation of traditional Zero-shot Learning by allowing for classification between all classes, seen and unseen, during inference. To bridge the gap between visual and semantic spaces, various methods utilize embedding techniques and semantic similarity between classes. Various methods in Generalized Zero-shot Learning utilize embedding techniques and semantic similarity between classes to bridge the gap between visual and semantic spaces. Embedding based models follow different directions such as mapping visual space to semantic space, mapping semantic space to visual space, and finding a latent space to map both visual and semantic space into a joint embedding space. The lack of training samples for unseen classes in embedding based models introduces bias. One approach to address the lack of training samples for unseen classes in Generalized Zero-shot Learning is augmenting the loss function with unsupervised regularization or using Generative Adversarial Networks (GAN) to generate synthetic samples. However, training generative models can be challenging and scalability issues arise due to the need for a large number of samples and augmented data. Recent state-of-the-art GZSL methods employ a complex mixture of experts approach. Recent state-of-the-art GZSL methods like CRnet and COSMO employ a complex mixture of experts approach with multiple modules and hyperparameters. However, these methods are susceptible to errors, such as weak clustering in CRnet leading to bad results. In contrast, the proposed contribution is a simple fully connected neural network architecture. The proposed contribution is a simple fully connected neural network architecture with unified cross-entropy loss and soft-labeling. Soft-labeling allows training data from seen classes to also train unseen classes by using attribute similarity information. This approach, called Soft-labeled ZSL (SZSL), eliminates the need for unsupervised regularization and enables a simple MLP network to tackle the GZSL problem efficiently. Soft-labeled ZSL (SZSL) is a simple and efficient approach that achieves state-of-the-art performance in Generalized-ZSL setting on ZSL benchmark datasets. It uses a fully connected neural network architecture with unified cross-entropy loss and soft-labeling to train both seen and unseen classes using attribute similarity information. The proposed methodology involves mapping visual space to semantic space, computing similarity scores between attributes and input, and using Softmax for class probability computation. Visual features are extracted using a pre-trained ResNet-101 on ImageNet without fine-tuning the CNN. Our proposed model for zero-shot learning does not fine-tune the pre-trained ResNet-101 on ImageNet for visual features. It utilizes soft labeling based on semantic attribute similarity to represent relationships between seen and unseen classes. The cross entropy loss used for training only penalizes seen classes, and the similarity information between seen and unseen attributes is not utilized. In zero-shot learning, soft labeling enriches training data by assigning probabilities to unseen classes based on semantic attribute similarity. This approach acts as a regularizer and allows for more constraint on weights during training. The unseen distribution is calculated by finding the dot-product of seen class attributes with all unseen class attributes. Soft labeling in zero-shot learning enriches training data by assigning probabilities to unseen classes based on semantic attribute similarity. The distribution of unseen classes is controlled by a temperature parameter, with higher temperatures resulting in a flatter distribution and lower temperatures creating peaks on nearest unseen classes. Soft labeling introduces unseen visual features into the network without generating fake data. The proposed approach in zero-shot learning uses soft labeling to assign probabilities to unseen classes based on semantic attribute similarity. This method avoids the need to create fake samples and train difficult generative models. The temperature Softmax formula incorporates temperature parameter, total probability for unseen distribution, and soft labels for unseen classes. The proposed method is a multi-class probabilistic classifier using Softmax activation function and cross-entropy loss for training. Inspired by Hinton et al. (2015), it considers both soft targets and true labels to improve performance. The goal is to learn the nonlinear mapping through network weights W. The proposed method aims to learn the nonlinear mapping through network weights W by utilizing a multi-class probabilistic classifier with Softmax activation function and cross-entropy loss. The regularization factors \u03bb and \u03b3 are obtained through hyperparameter tuning, and the loss function L(x i ) combines cross-entropy loss over soft labels (L soft ) and cross-entropy loss over hard labels (L hard ). The soft-loss term penalizes both seen and unseen classes, while the hard-loss term alone is insufficient in zero-shot learning scenarios. The soft-loss term in the proposed method penalizes and controls probabilities within seen and unseen classes, with a trade-off coefficient q balancing cross-entropy losses. It enables the classifier to learn unseen classes by being exposed only to samples from seen classes. Our proposed SZSL method utilizes a weighted cross entropy regularizer on unseen classes, leveraging semantic similarity information between attributes. It outperforms state-of-the-art methods on GZSL settings across five benchmark datasets. The main objective of Generalized Zero-Shot Learning (GZSL) is to improve both seen and unseen samples accuracy simultaneously by imposing a trade-off between the two metrics. The standard evaluation metric for GZSL is the harmonic average of seen and unseen accuracies, chosen to prevent bias towards seen classes. To evaluate Generalized Zero-Shot Learning (GZSL), a popular experimental framework called Proposed Split (PS) is used to split classes into seen and unseen classes for comparison of GZSL/ZSL methods. The visual features of each image sample are extracted by a pre-trained ResNet-101 on ImageNet, with a dimension of 2048. Keras with TensorFlow back-end is utilized to implement the model, using proposed unseen classes for evaluation. The study utilized a TensorFlow back-end to implement the model, tuning hyperparameters through cross-validation. Results were averaged over 5 trials for consistency. Experiments were conducted on a machine with 56 vCPU cores and 2 NVIDIA-Tesla P100 GPUs. The effectiveness of the SZSL model in GZSL setting was demonstrated by comparing it with state-of-the-art models. The code is available in the supplementary material. Our proposed SZSL model outperforms state-of-the-art GZSL models on various benchmark datasets, including AwA2 and aPY. Despite its simplicity compared to CRnet and COSMO, our model achieves similar or better accuracies. The use of soft labeling in SZSL allows for a flexible trade-off between seen and unseen accuracies, leading to higher harmonic accuracy A_H. The use of soft labeling in SZSL allows for a flexible trade-off between seen and unseen accuracies, leading to higher harmonic accuracy A_H. By increasing total unseen probability q, A_U increases and A_S decreases as expected, resulting in a trade-off phenomenon depicted in Figure 2 for all datasets. In a GZSL problem, semantic similarity-based soft labeling and the trade-off knob, q, play a crucial role. Different datasets like AwA and aPY are coarse-grained, while CUB and SUN are fine-grained with varying accuracies based on q. The harmonic average curve shows consistent behavior with an optimal q value. Using the AwA dataset as an example, the concept of finding closest unseen classes to a seen class like squirrel is illustrated, highlighting the importance of shared attributes between classes. In a GZSL problem, semantic similarity-based soft labeling and the trade-off knob, q, are crucial. Different datasets like AwA and aPY are coarse-grained, while CUB and SUN are fine-grained with varying accuracies based on q. The harmonic average curve shows consistent behavior with an optimal q value. The importance of shared attributes between classes is illustrated using the example of finding closest unseen classes to a seen class like squirrel. Regularization techniques like entropy-based Deep Calibration Network (DCN) are used to train the network to classify unseen classes accurately by minimizing uncertainty. Utilizing similarity-based soft-labeling implicitly regularizes the model in a supervised fashion, improving performance significantly compared to DCN. The effect of \u03c4 on the assigned unseen distribution is shown in Figure 3 for the AwA dataset. Small \u03c4 concentrates q on the nearest unseen class, while large \u03c4 spreads q over all unseen classes, providing less helpful information to the classifier. The impact of \u03c4 on the assigned distribution for unseen classes is shown in Figure 3 .a when seen class is squirrel in AwA dataset. Unseen distribution with \u03c4 = 0.2 represents similarities between seen class (squirrel) and similar unseen classes (rat, bat, bobcat). \u03c4 = 0.01 focuses mostly on the nearest unseen class, rat, ignoring other similarities. \u03c4 = 10 flattens the unseen distribution, resulting in high uncertainty and not contributing helpful information to learning. The GZSL classifier utilizes visual-to-semantic mapping and cross-entropy loss to simultaneously learn seen and unseen classes through soft labels based on semantic attributes. This approach, combined with a novel regularization technique, achieves state-of-the-art performance on five ZSL benchmark datasets while maintaining a simple and efficient model design."
}