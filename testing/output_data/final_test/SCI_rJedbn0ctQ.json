{
    "title": "rJedbn0ctQ",
    "content": "We propose a training-free approach for sentence representation using the Gram-Schmidt Process to build an orthogonal basis of word and context. The method combines pre-trained word embeddings efficiently, outperforming existing zero-training alternatives in NLP tasks. The proposed model for sentence representation using the Gram-Schmidt Process outperforms existing zero-training alternatives in NLP tasks. Word embeddings characterize semantic similarity between words, but understanding text requires robust embeddings for longer pieces like sentences and paragraphs. Existing approaches to sentence embeddings can be categorized into parameterized and non-parameterized methods. The models for sentence embeddings include SkipThought, Sent2Vec, Quick thoughts, \u00e0 la carte, InferSent, Universal Sentence Encoder, and GRAN. These models utilize various techniques such as encoder-decoder, unsupervised learning, NLI dataset, transformer, and Paraphrase Database for training and optimization. The BID models, including BID0, BID19, BID23, and BID28, use different approaches to generate sentence embeddings without the need for further training. BID0 introduces a parameter-free method called SIF, while BID19 combines power mean word embeddings. BID23 utilizes a multi-task learning framework, and BID28 generates paraphrastic sentence representations using updated word embeddings. These methods offer fast inference speed and high-quality sentence representations. Our work aims to advance the frontier of sentence embeddings with a novel algorithm, Geometric Embedding (GEM), based on the geometric structure of word embedding space. Each word in a sentence contributes a new semantic meaning to the subspace spanned by word vectors, with the intensity of this meaning indicated by the length of projection in that direction. In this paper, a new approach is proposed to generate sentence embeddings using a QR factorization of the word embedding matrix A. The sliding-window QR factorization method is introduced to capture word context and significance. Additionally, a step similar to BID0 is taken to remove top principal vectors for unbiased sentence similarity comparison. This method ensures disparate background components for each sentence's orthogonal basis. The paper introduces a new method for generating sentence embeddings using a QR factorization of the word embedding matrix. A sentence-specific principal vector removal technique is proposed, resulting in improved performance on 11 NLP tasks. The algorithm outperforms non-parameterized methods and some parameterized approaches, with significant performance boosts on benchmark datasets. The model's running time is also competitive with existing models. The algorithm described in the text is a method for generating sentence embeddings using QR factorization of word embedding matrices. It involves a process similar to the Gram-Schmidt Process, where word vectors are decomposed into orthogonal components. This technique, known as QR factorization, can be used to factorize an embedding matrix into orthogonal and triangular matrices. The algorithm generates sentence embeddings using QR factorization of word embedding matrices, decomposing word vectors into orthogonal components. This technique calculates the novel semantic meaning of a word with respect to its neighborhood, not just preceding words. The contextual window matrix of a word is defined within its m-neighborhood window in a sentence. The QR factorization of the contextual window matrix computes the new orthogonal basis vector for each word. The algorithm generates sentence embeddings using QR factorization of word embedding matrices to calculate the novel semantic meaning of words within their neighborhood. The orthogonal basis vector q i represents the unique semantic meaning brought by word w i, quantified by its novelty, significance to context, and corpus-wise uniqueness. A word is deemed more important if its novel orthogonal basis vector q i is a significant component in v wi, quantified by a novelty score. The significance of a word is related to its alignment with the context. Singular Value Decomposition is used to identify principal directions in the contextual window matrix. The columns of U represent semantic meanings from the context, with their singular values denoting importance. The SVD of a word's contextual window matrix determines its importance based on novel semantic meaning alignment. The significance of a word is determined by its alignment with the context, quantified as \u03c3(S i ). The importance of a word in its context is defined as \u03b1 s, which represents the distance between the word and the context hyper-plane normalized by the context size. Similar to IDF, a word commonly present in the corpus has low uniqueness. The solution involves computing principal directions of the corpus and measuring their alignment with an orthogonal basis vector q i to assign a corpus-wise uniqueness to each word. The method involves computing principal vectors of a sentence embedding matrix to assign uniqueness scores to words based on alignment with a basis vector. Instead of forming the sentence embedding matrix directly, an intermediate coarse-grained matrix is obtained before computing the top principal vectors. The method computes top principal vectors of sentence embeddings to assign uniqueness scores to words based on alignment with a basis vector. Different principal vectors are selected for each sentence to account for varying alignments with the corpus. Sentence vectors are computed using a weighted sum of word embeddings based on novelty, significance, and corpus-wise uniqueness scores. The method proposes sentence-dependent principal component removal (SDR) to enhance performance on semantic similarity tasks by reranking top principal vectors based on correlation with each sentence. Ablation experiments demonstrate that SDR yields better results. The method proposes sentence-dependent principal component removal (SDR) to enhance performance on semantic similarity tasks by reranking top principal vectors based on correlation with each sentence. The algorithm is summarized in Algorithm 1 with an illustration in FIG1. Evaluation is done on the STS Benchmark BID2 dataset, aiming to predict similarity scores between sentence pairs. Two versions of the model are reported, one using GloVe word vectors (GEM + GloVe) and the other using word vectors concatenated from LexVec, fastText, and PSL BID29 (GEM + L.F.P). The final similarity score is computed using an inner product of the Form matrix S. The curr_chunk discusses the evaluation of a non-parameterized model on a CQA subtask, reporting higher scores compared to other models. Hyperparameters are chosen through a search on the dev set, with results showing superior performance on the test set. The model outperforms both non-parameterized and parameterized models, including SIF, GRAN, InferSent, and Sent2Vec. The model outperforms most parameterized models, ranking second to Reddit + SNLI. It requires no external data or training and is evaluated on the SemEval CQA subtask B, showing superior performance. The model is evaluated on re-ranking related questions based on similarity to the original question. Results show it outperforms benchmark models from the 2017 competition without requiring training. The model, GEM, is tested on nine supervised tasks, including classification tasks like movie reviews, sentiment analysis, question classification, and paraphrase identification. The sentence embeddings are fixed, and only task-specific neural structures are learned. GEM outperforms non-parameterized models like SIF and BOW on GloVe, and compares favorably with parameterized models like InferSent and SkipThought-LN. The model GEM outperforms non-parameterized models like SIF and BOW on GloVe and compares favorably with parameterized models like InferSent and SkipThought-LN. GEM's competitive performance on sentiment tasks shows that exploiting the geometric structures of two sentence subspaces is beneficial. Sensitivity tests on hyper-parameters and ablation study were conducted to analyze GEM's performance. The ablation study conducted on the GEM model shows that adding GEM weights and using sentence-dependent principal component removal methods significantly improve performance. The sensitivity study evaluates the impact of various hyper-parameters on the model's performance. The study evaluates the impact of hyper-parameters on the model's performance, including the number of principal components to remove and the power of the singular value in coarse sentence embedding. The model shows robustness with respect to hyper-parameters and inference speed is compared with benchmark models SkipThought and InferSent, showing that the model is faster even without GPU acceleration. Our method for generating sentence embeddings is based on the geometric structure of word embeddings, outperforming SkipThought and InferSent in terms of speed. Future work includes incorporating subwords and exploring different geometric structures in sentences. The novelty score, significance score, and uniqueness score are higher for words with rare appearances, bringing in new semantic meaning to sentences. Theorem 1 explains the probability of a word emitted from a sentence in a dynamic process, with the MLE for sentence embedding when Z is large. The log likelihood of a word emitted from a sentence is given by a specific formula involving the sentence embedding and vocabulary."
}