{
    "title": "H1q-TM-AW",
    "content": "Domain adaptation involves learning an accurate model in a target domain with limited labeled data by leveraging labeled data from a source domain. Domain adversarial training aims to find a common representation for both domains but faces limitations such as weak constraints with high-capacity feature extraction and performance trade-offs between source and target domains. This paper addresses these issues by considering the cluster assumption that decision boundaries should not cross high-density data regions. The proposed models, VADA and DIRT-T, aim to improve domain adaptation by penalizing the violation of the cluster assumption. Empirical results show significant performance enhancements on digit, traffic sign, and Wi-Fi recognition benchmarks. Deep neural networks have achieved impressive results in various machine learning tasks but often require large amounts of labeled data. In domain adaptation, the challenge arises when labeled data for the target domain is scarce, but abundant for a related source domain. The dissimilarity between the source and target data distributions leads to covariate shift, impacting the model's performance. This problem is addressed through domain adaptation techniques. Unsupervised domain adaptation involves using fully-labeled source samples and completely-unlabeled target samples, without guaranteeing a classifier with low generalization error in both domains. To address this, BID9 proposed constraining the classifier to rely on domain-invariant features by minimizing the divergence between source and target domain features through domain adversarial training. BID9 proposed domain adversarial training for unsupervised domain adaptation, but faced issues with weak domain-invariance constraints and target performance. BID24 introduced asymmetric tri-training as a solution. This paper explores the cluster assumption, seeking decision boundaries that avoid high-density regions. The paper proposes two novel models: Virtual Adversarial Domain Adaptation (VADA) and Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) to push decision boundaries away from high-density regions. VADA improves domain adversarial training in conservative domain adaptation, while DIRT-T addresses the mismatch between source and target optimal classifiers in non-conservative domain adaptation. DIRT-T allows transitioning from a joint classifier to a better target domain classifier, demonstrating the advantage of natural gradients in refinement steps. Results show improved performance in various domain adaptation tasks, outperforming previous methods and setting new state-of-the-art benchmarks. Several methods have been proposed for domain adaptation, including re-weighting source samples, projecting distributions into a feature space for matching, and modifying tri-training for nonconservative domain adaptation. These methods are based on theoretical analysis by BID1. Theoretical analysis by BID1 on domain adaptation focuses on the relationship between hypothesis spaces in source and target domains, emphasizing the cluster assumption that decision boundaries should not cross high-density regions. The cluster assumption has been utilized in semi-supervised learning algorithms, leading to successful deep learning approaches. This assumption has also been applied to domain adaptation, with research showing its effectiveness in solving complex, high-dimensional problems using deep neural networks. Our work considers the cluster assumption in non-conservative domain adaptation, highlighting the limitations of domain adversarial training when the feature extraction function has high capacity. The classifier h can be decomposed into an embedding function f and embedding classifier g. Domain adversarial training may not be sufficient for domain adaptation, as shown by BID8's work on self-ensembling for domain adaptation. Domain adversarial training minimizes the cross-entropy objective L y with a domain discriminator D, aiming to minimize the objective DISPLAYFORM1 using a weighting factor \u03bb d. The feature extractor f is encouraged to have a small Jensen-Shannon divergence between f(X s) and f(X t). However, achieving small source generalization error and feature divergence does not guarantee high accuracy on the target task TAB6, especially when f has infinite capacity and the source-target supports are disjoint. In this paper, the cluster assumption is applied to domain adaptation to achieve better adaptation. The assumption states that input distribution X contains clusters where points in the same cluster come from the same class. By minimizing conditional entropy with respect to the target distribution, the classifier is forced to be confident on unlabeled target data, improving decision boundaries. The classifier's decision boundaries are driven away from the target data by minimizing conditional entropy with respect to the target distribution. To ensure reliable estimation, the locally-Lipschitz constraint is incorporated via virtual adversarial training and an additional term in the objective function enforces classifier consistency within the norm-ball neighborhood of each sample. The Virtual Adversarial Domain Adaptation (VADA) model combines conditional entropy minimization, domain adversarial training, and semi-supervised training objectives. Empirically, hyperparameters (\u03bb d , \u03bb s , \u03bb t ) are easy to choose and work well across multiple tasks. VADA aligns with domain adaptation theory and modulating \u03bb t allows for a trade-off between target-side cluster assumption violation and source-side generalization. VADA reduces target-side cluster assumption violation by setting \u03bb t > 0, leading to tighter bounds on target generalization error. Empirical results show significant improvements over existing models in domain adaptation benchmarks. In non-conservative domain adaptation, the optimality gap between source and target domains is attributed to cluster assumption violation. The Decision-boundary Iterative Refinement Training (DIRT) method aims to minimize the target-side cluster assumption violation in the target domain by adjusting the classifier's decision boundaries away from data-dense regions. This approach builds upon the VADA model, which reduces target-side cluster assumption violation to improve target generalization error in domain adaptation benchmarks. Gradient descent minimizes the loss by selecting gradient steps to define a parameter space neighborhood. The neighborhood should be parameterization-invariant to ensure decision boundaries stay close to each other. The optimization step now solves for a gradient step that minimizes conditional entropy while keeping the Kullback-Leibler divergence small. The Kullback-Leibler divergence between h \u03b8 (x) and h \u03b8+\u2206\u03b8 (x) is minimized by a sequence of optimization problems approximating natural gradient steps. The model is trained to stay close to a teacher model while reducing cluster assumption violation, known as Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T). The optimization problems in DIRT-T involve the teacher model pseudo-labeling target samples with noisy labels, allowing the student model to improve decision boundaries. DIRT-T is a recursive extension of VADA, constructing a new \"source\" domain with pseudo-labels for non-conservative domain adaptation. The optimization problems in DIRT-T involve pseudo-labeling target samples with noisy labels to improve decision boundaries. Domain adversarial training is not necessary as the gap in Eq. (10) should decrease with each update of the source domain. The method can be applied to any domain adaptation tasks with a defined notion of neighborhood. Evaluation is done on various visual and non-visual domain adaptation tasks. In non-visual domain adaptation, the use of small and large CNN architectures for different experiments is discussed. The proposed method shows significant improvements compared to baseline models. Instead of gradient reversal, alternating updates to the discriminator and encoder are used. Instance normalization is explored for image preprocessing to make the classifier invariant to shifts and rescaling of pixel intensities. Instance normalization is discussed for domain adaptation, showing its effect on input images from various datasets. Hyperparameters were tuned for each task, with a focus on \u03bb values and \u03b2. Extensive tuning was not necessary for achieving state-of-the-art performance. The decision to use specific \u03bb values was based on restricted hyperparameter search. Table 1: Test set accuracy on visual domain adaptation benchmarks, including MNIST to MNIST-M and MNIST to SVHN. MNIST-M blends MNIST digits with color patches, while SVHN consists of colored street house numbers. Adaptation from MNIST to SVHN is challenging without instance normalization. Both VADA and DIRT-T achieve state-of-the-art performance in all settings. The SYN DIGITS dataset consists of 500000 images generated from Windows fonts with varying text, positioning, orientation, background, stroke color, and blur. SYN SIGNS \u2192 GTSRB involves adapting from synthetic images to real images with 43 classes. In SYN SIGNS \u2192 GTSRB, adaptation involves transitioning from synthetic images to real images with 43 classes. STL \u2194 CIFAR involves two 10-class image datasets with nine overlapping classes. By removing non-overlapping classes, a 9-class classification problem is achieved with state-of-the-art performance in both adaptation directions. Additionally, VADA and DIRT-T were applied to the Wi-Fi Activity Recognition Dataset for non-visual domain adaptation evaluation. The Wi-Fi Activity Recognition Dataset involves using WiFi Channel State Information (CSI) data to predict motion activity in indoor areas. Domain adaptation is necessary when training and testing data are from different rooms. VADA improves classification accuracy significantly compared to Source-Only and DANN. DIRT-T does not lead to further improvements on this dataset. Experiments suggest that VADA already achieves strong clustering in the target domain, making DIRT-T unlikely to yield performance improvement. Overall, the study achieves state-of-the-art results across all tasks, showing substantial improvement margins over previous models in several tasks. The DIRT-T method consistently outperforms VADA by incrementally pushing decision boundaries away from the target domain data, relying on the cluster assumption for its success. The study demonstrates the effectiveness of leveraging the cluster assumption in unsupervised domain adaptation with deep neural networks. Ablation analysis shows that virtual adversarial training and conditional entropy minimization are essential for achieving the best performance in most tasks. The study highlights the importance of virtual adversarial training in pushing classifier decision boundaries away from data. Ablation experiments show that removing the virtual adversarial training component negatively impacts model performance. The removal of the KL-term in SVHN \u2192 MNIST and STL \u2192 CIFAR also has a detrimental effect on the model. In experiments, deviating from previous classifier neighborhoods leads to spikes in KL-term and drops in target accuracy. VADA and DIRT-T improve adaptation from MNIST to SVHN, showing clustering in T-SNE embeddings. Domain adversarial training on various layers of a neural network enhances performance in MNIST \u2192 SVHN adaptation. Neural Network BID9 trained to adapt MNIST \u2192 SVHN. Deeper layers improve matching of distributions without compromising source accuracy. VADA and DIRT-T models for domain adaptation inspired by the cluster assumption. VADA penalizes violations of the cluster assumption, while DIRT-T is an extension of VADA. Our second model, DIRT-T, refines the VADA classifier by minimizing cluster assumption violations using natural gradients. Experiments show VADA's strong performance in domain adaptation, with DIRT-T further improving it. Future work includes applying DIRT-T to weakly supervised learning and enhancing natural gradient approximation. The models are recommended for other domain adaptation applications. In experiments, hyperparameter tuning is not crucial for achieving top performance. Hyperparameter search is limited to specific values for each task, with fixed values for certain parameters. The decision to adjust certain parameters can be based on prior beliefs or default values. Different optimization intervals are set based on the complexity of the task. Adam Optimizer with specific settings is used for optimization. VADA and DIRT-T were trained for different iterations, with VADA using exponential moving average. Gradient reversal was replaced with alternating minimization to stabilize domain adversarial training. The choice reflects a difference in optimization methods for approximating the mini-max. The authors propose to further constrain the model by introducing instance normalization as an image pre-processing step for the input data. Instance normalization is invariant to channel-wide scaling and shifting of input elements, and involves computing mean and standard deviation across spatial dimensions. Instance normalization applied to input data helps make the classifier invariant to channel-wide shifts and scaling of pixel intensities, reducing dH\u2206H without affecting classifier performance. It is not equivalent to gray-scaling and preserves color partially. Results are reported with and without instance normalization, denoting source and target distributions as p s (x, y) and p t (x, y). Source covariate distribution p s (x) defines random variable X s with support supp(X s ) = X s. In the context of source and target distributions p s (x, y) and p t (x, y), random variables X s and X t are defined with support supp(X s ) = X s. An embedding function f : R n \u2192 R m and an embedding classifier g : R m \u2192 C are considered, where C is the (K \u2212 1)-simplex. The analysis is simplified for the case where K = 2, and a joint distribution p(x, y) is used to calculate the generalization error of a classifier. Domain adversarial training aims to find a single classifier h for both the source and target distributions. The objective is to minimize generalization error under the distribution p(x, y) by optimizing the embedding function F and embedding classifier G. The approach assumes that good source generalization error and source-target feature matching lead to good target generalization error. However, if Xs \u2229 Xt = \u2205 and F is complex, this assumption may not hold. If Xs \u2229 Xt = \u2205 and F is complex, the assumption that good source generalization error leads to good target generalization error may not hold. Consider a set of classifiers that satisfy feature-matching constraints without worsening source generalization error. It is shown that H* is not empty by constructing an element in it. The classifier h \u2208 H* may not achieve good target generalization error. The worst-case partitioning subject to the probability mass constraint is \u03a9 = Rn \\ \u2126. The composite classifiers h = g \u2022 f and h = g \u2022 f are elements of H, achieving the maximum H\u2206H-divergence value of 2. Theoretical characterization of domain adversarial training with finite-capacity convolutional neural networks and gradient-based learning is a challenging research problem. VADA and DIRT-T were applied to the Wi-Fi Activity Recognition Dataset for non-visual domain adaptation, predicting motion activity from Wi-Fi Channel State Information data. The dataset collected CSI data stream samples for seven activities in different rooms. Room A was used as the source domain and Room B as the target domain for unsupervised domain adaptation. VADA showed improved classification accuracy compared to Source-Only and DANN, while DIRT-T did not provide further improvements. VADA outperformed Source-Only and DANN in classification accuracy on the dataset. However, DIRT-T did not lead to additional improvements. The decision boundary was successfully pushed away from data-dense regions by VADA, making further application of DIRT-T unnecessary. Visualization of t-SNE embeddings and confusion matrix confirmed the effectiveness of VADA in clustering the target domain."
}