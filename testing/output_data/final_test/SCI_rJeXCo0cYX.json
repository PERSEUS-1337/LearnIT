{
    "title": "rJeXCo0cYX",
    "content": "The BabyAI research platform aims to involve humans in training artificial agents to understand language instructions. It offers 19 levels of increasing difficulty to help agents learn a synthetic language similar to English. The platform includes a bot agent simulating a human teacher and highlights the need for more sample-efficient deep learning methods. The BabyAI research platform aims to facilitate research on grounded language learning by using a simulated human expert. This research is important for customizing intelligent helpers to better understand human desires and needs. The BabyAI research platform uses a simulated human expert in a 2D gridworld to train agents to follow natural language instructions. It supports curriculum learning and interactive teaching to improve efficiency in human-machine teaching. The BabyAI research platform supports curriculum learning and interactive teaching to enhance human-machine teaching efficiency. It features 19 levels with increasing difficulty in environment configuration and instruction language complexity. Interactive teaching, where instruction adapts to the learner's abilities, is a key aspect. Various advanced agent training methods rely on interaction between the learner and the teacher. BabyAI provides a bot agent for generating new demonstrations and guiding the learner during interactive experiments. The main challenge in language learning with human involvement is the significant amount of data and interactions needed. The BabyAI platform is used for sample efficiency research in language learning. Deep learning methods in imitation and reinforcement learning require large amounts of data. The platform estimates the number of demonstrations needed to solve levels and explores pretraining and interactive imitation learning for improved efficiency. The BabyAI research platform contributes to improving sample efficiency in language learning by providing a simulated human-in-the-loop environment with 19 levels. It establishes baseline results and reports sample efficiency for various learning approaches, aiming to allow human-in-the-loop training. The platform supports world state manipulation and is available online for further research. BabyAI introduces 3D environments with partial observability and a systematic synthetic language definition. Unlike other simulation frameworks, BabyAI features a simulated human expert for human-in-the-loop training. The use of a 3D environment comes with a high computational cost. BabyAI uses a gridworld environment instead of 3D environments due to high computational costs. A MiniGrid environment was built for BabyAI as existing gridworld platforms were insufficient for defining a compositional language. General-purpose RL testbeds like the Arcade Learning Environment do not assume a human-in-the-loop setting, making rewards expensive to obtain. imitation learning methods are used under the assumption that all rewards would be given by a human. Imitation learning methods like behavioral cloning, Searn, DAGGER, and maximum-entropy RL are preferred due to their efficiency in learning from human input. BabyAI focuses on grounded language with a simulated human, unlike previous studies on instruction following with natural language. BabyAI uses a synthetic Baby language instead of natural language for instruction following tasks. The BabyAI platform utilizes a synthetic Baby language for instruction-following tasks, allowing for full control over semantics and easy data generation. The system includes a simulated gridworld environment and instruction-following tasks formulated in the Baby Language. A bot is available to generate successful demonstrations for all levels. Studies on sample efficiency are computationally expensive due to the need for multiple runs with varying amounts of data. In our design of the environment, we have created MiniGrid, a 2D gridworld with various colored entities like agent, balls, boxes, doors, and keys. The agent can interact with objects and receive a 7x7 field of view and a Baby Language instruction at each step. MiniGrid is fast, lightweight, and open source, allowing for quick experimentation. The environment, MiniGrid, is open source and supports integration with OpenAI Gym. A synthetic Baby Language is used to give instructions to the agent, with 2.48 \u00d7 10 19 possible instructions. The language allows for tasks like going to objects, picking up objects, opening doors, and placing objects next to each other. The BabyAI platform includes a verifier to check if an agent's actions achieve instruction goals. Instructions in the language can refer to one or multiple objects, with structural restrictions to keep them readable. The language allows for sequencing subgoals using connectors like then and after. The BabyAI platform includes a verifier to check if an agent's actions achieve instruction goals, which can involve tasks like finding keys or moving obstacles. Prior literature suggests that a curriculum can enhance learning complex tasks for neural architectures. To study how a curriculum boosts sample efficiency, 19 levels were created with varying complexities, each requiring understanding a limited subset of Baby Language. Levels consist of missions combining instructions with initial environment states, designed to be solvable by agents possessing specific competencies. The BabyAI platform includes a verifier to check if an agent's actions achieve instruction goals, which can involve tasks like finding keys or moving obstacles. The platform consists of 19 levels with varying complexities, each requiring understanding a limited subset of Baby Language. Levels consist of missions combining instructions with initial environment states, designed to be solvable by agents possessing specific competencies such as Room Navigation, Ignoring Distracting Boxes, Maze Navigation, Unblocking the Way, Unlocking Doors, Guessing to Unlock Doors, Go To Instructions, and Open Instructions. The BabyAI platform includes 19 levels with varying complexities, each requiring understanding a limited subset of Baby Language. Levels consist of missions combining instructions with initial environment states, designed to be solvable by agents possessing specific competencies such as Room Navigation, Ignoring Distracting Boxes, Maze Navigation, Unblocking the Way, Unlocking Doors, Guessing to Unlock Doors, Go To Instructions, and Open Instructions. TAB1 lists all current BabyAI levels together with the competencies required to solve them, culminating with the BossLevel, which requires mastering all competencies. The BabyAI platform includes 19 levels with varying complexities, each requiring understanding a limited subset of Baby Language. The BossLevel requires mastering all competencies, with informal definitions. The bot acts as a simulated human teacher, generating demonstrations or suggesting actions for BabyAI levels. The BabyAI learner is designed to be generic and scalable to new tasks, while the bot is engineered using task-specific knowledge. The bot in the BabyAI platform is designed to understand tasks and teach the baby learner. It has access to a tree representation of instructions and executes a stack machine to achieve subgoals. The bot can interrupt its current task to pursue a new subgoal and then resume the original task. The subgoals it implements include opening doors, picking up objects, and dropping objects. The bot in the BabyAI platform executes subgoals such as picking up objects, dropping objects, and exploring unseen parts of the environment. It keeps track of grid cells it has seen and not seen to ensure realistic access to information. Exploration involves tasks like opening doors and moving obstacles. The BabyAI platform's bot executes subgoals like picking up objects, dropping objects, and exploring unseen parts of the environment. It keeps track of grid cells seen and unseen for realistic access to information. Exploration involves tasks like opening locked doors, moving obstructing objects, and navigating to objects using a shortest path search routine. The difficulty of BabyAI levels is assessed by training a behavioral cloning baseline for each level and estimating data requirements for solving simpler levels. Code for experiments and pretrained models are available online. The model uses a GRU to encode instructions and a convolutional network with FiLM layers to process observations. An LSTM memory integrates representations from the FiLM module. FiLM is chosen over gated attention for its flexibility in rescaling feature maps. The model utilizes a GRU for encoding instructions and a convolutional network with FiLM layers for processing observations. It offers a flexible alternative for rescaling convolutional feature maps and conditionally re-normalizing them. Two versions of the model are used - Large and Small, with differences in memory units and attention mechanisms. Adam optimizer with specific hyperparameters is employed, and different backpropagation steps are used for imitation learning and reinforcement learning experiments. In experiments, Proximal Policy Optimization (PPO) algorithm was used with parallelized data collection. 4 epochs of PPO were performed with 64 rollouts of length 40. A non-zero reward was given only when the agent completed the mission, with reward magnitude calculated based on episode length. Generalized advantage estimation was used with \u03bb = 0.99. Success rate was defined as the ratio of missions completed within a certain number of steps. Running the experiments required 20-50 GPUs for two weeks. Training the Large model with imitation learning using one million demonstration episodes for each BabyAI level. Models trained for 80 epochs on single-room levels and 20 epochs on 3x3 maze levels. Maximum success rate on validation set of 512 episodes reported in TAB2. Single-room levels solved with 100.0% success rate. Longer demonstrations indicate higher difficulty. Using 1M demonstrations for simple levels like GoToRedBall is inefficient for enabling human teaching. BabyAI platform supports studies on neural agents learning with less data. Neural agents can learn with less data by computing baseline sample efficiencies for imitation learning and reinforcement learning approaches on BabyAI levels. Sample efficiency is defined as the minimum number of demonstrations or RL episodes needed to train an agent to reach a success rate of at least 99%. The study estimates sample efficiency for imitation learning by training models with varying numbers of demonstrations and finding the minimum number required to cross the 99% threshold. The study analyzed sample efficiency for imitation learning and reinforcement learning on BabyAI levels. Results showed RL was 2 to 10 times less efficient than IL. Demonstrations from RL-trained agents were sometimes easier to imitate, with fewer demonstrations needed for success. The study compared sample efficiency between imitation learning (IL) and reinforcement learning (RL) on BabyAI levels. RL was found to be 2 to 10 times less efficient than IL. Results showed that demonstrations from RL-trained agents were sometimes easier to imitate, with fewer demonstrations needed for success. Pretraining experiments were conducted to explore how curriculum learning can improve learning efficiency. The study compared sample efficiency between imitation learning (IL) and reinforcement learning (RL) on BabyAI levels. Results in Table 4 showed that using GoToLocal as a base level reduced the number of demonstrations needed for success in four cases. However, pretraining with only GoToObjMaze did not show benefits. This counter-intuitive result highlights the limitations of current deep learning methods in utilizing available curriculums. An example case study demonstrated how sample efficiency can be improved by interactively providing more informative examples to the agent based on its learning progress. An iterative algorithm was used to adaptively grow the agent's training set, starting with 1000 base demonstrations and increasing the dataset size by 1.2 at each iteration by providing bot demonstrations for failed missions. The study introduced interactive imitation learning, where the agent's training set is adaptively grown by providing bot demonstrations for failed missions. This approach showed improved sample efficiency on BabyAI levels, with some runs achieving significant reductions in training set size while maintaining performance above the 99% threshold. The platform includes 19 levels of increasing difficulty based on a decomposition of tasks into basic competencies, requiring understanding of the Baby Language. The BabyAI platform consists of 19 levels based on basic competencies in the Baby Language. Current imitation and reinforcement learning methods struggle with tasks that have a compositional structure, requiring hundreds of thousands of demonstrations for learning. Curriculum learning and interactive learning show improvements in sample efficiency, but a significant enhancement is needed for learning with human involvement. Future research aims to improve sample efficiency in language learning by developing new models and teaching methods. Approaches like Neural Module Networks and Neural Programmer-Interpreters show promise. The BabyAI platform is set to be a benchmark for sample efficiency in language learning. The research environments are built on MiniGrid, an open-source gridworld package compatible with OpenAI Gym. In MiniGrid, the world is a grid of size NxN with various objects like walls, doors, keys, balls, boxes, and goals, each with a specific color. Rewards are sparse, and the agent must complete tasks within a time step limit to receive positive rewards based on completion speed. In MiniGrid, the agent's reward is closer to 1 if it completes an episode quickly. The max_steps parameter varies for each mission based on environment size and instruction length. There are seven actions available: turn left, turn right, move forward, pick up, drop, toggle, and done. Observations are partial and egocentric, with the agent seeing a 7x7 tile square by default. The agent in MiniGrid sees a 7x7 tile square in the direction it is facing, with observations provided as a tensor of shape 7x7x3. Each tile is encoded with 3 integer values for object type, color, and door state. This compact encoding allows for faster training and space efficiency. The agent also has access to instructions decomposed into subgoals added to a stack for each environment. The bot processes subgoals independently, with each leading to more subgoals or actions. The visibility mask tracks what the bot has seen. Completed subgoals are removed from the stack, allowing the bot to move to the next one. The same subgoal can result in multiple actions over time. The bot processes subgoals independently, leading to multiple actions being taken. The Close, Drop, and Pickup subgoals are straightforward, immediately removing themselves from the stack after execution. Diagrams show how Open, GoNextTo, and Explore subgoals are handled. \"Forward cell\" refers to the grid cell the agent faces, and a path from X to Y contains blockers if objects need to be moved for navigation."
}