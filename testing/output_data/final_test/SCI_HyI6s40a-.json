{
    "title": "HyI6s40a-",
    "content": "Recent advances in adversarial Deep Learning have led to new vulnerabilities in autonomous DL systems. A novel automated countermeasure called Parallel Checkpointing Learners (PCL) is introduced to enhance the reliability of DL models by minimizing rarely observed regions in the latent feature space. PCL is an unsupervised methodology that uses complementary checkpointing modules to validate the victim model execution in parallel, without leveraging adversarial samples for training. The Parallel Checkpointing Learner (PCL) method characterizes input data geometry and high-level abstractions in DL layers, requiring adversaries to deceive all defender modules simultaneously. PCL's effectiveness against various attack scenarios like FGS, JSMA, Deepfool, and Carlini&WagnerL2 is evaluated on datasets like MNIST, CIFAR10, and ImageNet. This defense mechanism addresses security concerns in sensitive applications like intelligent transportation and healthcare, crucial for the adoption of emerging learning algorithms. In sensitive applications like intelligent transportation and healthcare, the reliability of advanced learning technologies in the face of adversarial samples is crucial. Adversarial attacks can deceive machine learning models, jeopardizing the safety of autonomous systems. This paper aims to address the vulnerability of DL models to adversarial samples and the importance of rejecting risky samples to ensure model integrity. The vulnerability of neural networks to adversarial samples is due to rarely explored sub-spaces in each feature map, caused by limited access to labeled data and inefficiency of regularization algorithms. Research shows a trade-off between model robustness and accuracy. The Parallel Checkpointing Learners (PCL) defense mechanism introduces separate defender modules to assess the reliability of a victim model's predictions by characterizing explored sub-spaces and flagging data points within rarely observed regions. Adversarial samples created by attack methods often lie within these regions. In Section 4, adversarial samples created by attack methods mostly lie within partially explored sectors. A white-box attack model is considered, where the attacker has full knowledge of the victim model. The proposed approach's security is validated on various DL benchmarks like MNIST, CIFAR10, and a subset of ImageNet data. New insights on adversarial transferability are provided, and an API is open-sourced for user convenience. The paper's explicit contribution is an automated end-to-end framework for unsupervised learning. The paper introduces an automated framework for unsupervised model assurance and defense against adversaries. It proposes parallel checkpointing learners to validate data abstractions in deep learning layers and evaluates against various attack methods. Insights on the transferability of adversarial samples between models are also provided. The paper presents a methodology using defender modules to assess data density distribution in deep learning models and detect risky samples. The defender modules aim to learn the probability density function of explored sub-spaces in intermediate DL feature maps, identifying rarely observed regions for potential attacks. The training procedure involves devising parallel checkpointing modules to minimize disentanglement of data features in a Euclidean space at specific checkpoint locations. This is achieved through iterative realignment of data abstractions to characterize the latent data space as an ensemble of lower dimensional sub-spaces. The goal is to learn the probability density function of explored regions and detect atypical samples based on a user-defined security parameter. The procedure also involves minimizing entanglement between Gaussian distributions for different classes while reducing inner-class diversity. The training procedure involves devising a parallel checkpointing module to enforce disentanglement of data features in a Euclidean space. This is done through replicating the victim neural network, inserting an L2 normalization layer, and fine-tuning the network to optimize the defender module. The normalization layer maps latent feature variables into a d-dimensional hypersphere to reduce over-fitting effects. The training procedure involves creating a checkpointing module to disentangle data features in a Euclidean space by replicating the victim neural network, adding an L2 normalization layer, and fine-tuning the network for the defender module. The parameter \u03b3 is set to 0.01, and the defender model is retrained with the same optimizer as the victim model, with a lower learning rate. The goal is to condense latent data features belonging to the same class and reduce inner-class diversity for a sharper Gaussian distribution per class. The training procedure involves creating a checkpointing module to disentangle data features in a Euclidean space by replicating the victim neural network and fine-tuning it for the defender module. Reducing inner-class diversity sharpens Gaussian distribution per class, while promoting separability. The addition of loss 3 ensures centers lie on a unit hyper-sphere, preventing divergence during training. Retraining the defender module effectively separates legitimate samples from malicious ones in an unsupervised setting. In an unsupervised setting, the High Dimensional Discriminant Analysis (HDDA) algorithm is used to learn the mean and conditional covariance of each class in a Gaussian distribution. The conditional covariance matrix contains two different eigenvalues to be determined. The learned pdf variables are then used to compute the probability of a feature point coming from a specific class. The probability of a feature point coming from a specific class is evaluated for each test sample, comparing it against a user-defined security parameter. The Security Parameter (SP) controls the hardness of defender modules, illustrated in FIG2 with different security parameter values. The proposed PCL countermeasure reduces the risk of attacks by using parallel checkpointing modules, increasing intraclass distances, and learning a separate defender module in the input space. The proposed PCL countermeasure includes learning separate defender modules in the input space and latent defenders that checkpoint intermediate data features within the DL network. Adversarial samples are detected through a two-category classification task. The PCL countermeasure involves learning separate defender modules in the input space and latent defenders that checkpoint intermediate data features within the DL network to detect adversarial samples through a two-category classification task. The conditional risk in each checkpointing module is determined by misclassification penalties, with the fundamental rule being to decide on the legitimate category if certain conditions are met based on posterior probabilities. The optimal decision criteria in Eq. FORMULA6 rely on the cost of mistakes and risk of attacks, correlated with the user-defined cut-off threshold. Under the GMM assumption, conditional probability is computed as DISPLAYFORM4. Misclassification penalty determines decision cost, with dictionary learning used to filter atypical samples. An input defender module is designed to automatically filter out atypical samples by using robust dictionary learning techniques. The module works in two main steps: dictionary learning and characterizing the typical PSNR per class after sparse recovery. Each class of data has a separate dictionary learned, with the goal of finding the matrix that best represents the data. The input defender module utilizes dictionary learning to find the matrix that best represents pixel patches from images of each class. Sparse matrix V is used to represent image patches, and LAR method is employed to solve the Lasso problem. A dictionary of size 225 is learned for each class using 150,000 training patches. During execution, OMP is used to reconstruct input data with the corresponding dictionary. The input defender module utilizes dictionary learning to find the matrix that best represents pixel patches from images of each class. Sparse matrix V is used to represent image patches, and LAR method is employed to solve the Lasso problem. A dictionary of size 225 is learned for each class using 150,000 training patches. During execution, OMP is used to reconstruct input data with the corresponding dictionary. In the execution phase, non-overlapping patches within the image are denoised by the dictionary to form the reconstructed image. PSNR is used to profile legitimate samples within each class, with a threshold set to identify malicious data points based on reconstruction quality. The input defender module uses dictionary learning to reconstruct images, while the MAX I term represents the maximum pixel value. Figure 6 shows the impact of perturbation levels on adversarial detection rates for different security parameters in the FGS attack on MNIST. The experiment demonstrates that input dictionaries aid in automated detection of adversarial samples with high perturbation levels, while the latent defender module can effectively distinguish malicious samples even with minimal perturbation. The proposed PCL methodology evaluates the impact of security parameters on system performance for various benchmarks. It considers input and latent defenders jointly to detect adversarial samples, raising an alarm if either module detects a threat. Higher security parameters increase true positive detection rates but also raise the risk of mislabeling legitimate samples as malicious. The joint decision metric is evaluated for each application and attack model. The joint decision metric for each application and attack model is evaluated by considering false positive and true positive rates. ROC curves are presented to show the impact of input defenders on the overall decision policy, with a focus on the FGS attack. The FGS attack induces more perturbation compared to other methods. PCL performance against various attacks on MNIST, CIFAR10, and ImageNet benchmarks is summarized in Table 1. JSMA attack on ImageNet is computationally expensive. Adversarial samples for this attack could not be generated using the JSMA library. Performance of PCL against FGS, JSMA, Deepfool, and Carlini&WagnerL2 attacks is shown in Table 1. Reported numbers are based on ROC analysis. The table summarizes the performance of the PCL approach against various attacks on MNIST, CIFAR10, and ImageNet benchmarks. The use of a single latent defender module effectively prevents adversarial samples generated by state-of-the-art attacks. Adversarial confusion matrices show that the proposed approach can detect and remove adversarial samples without transferring them to the checkpointing modules. Refer to Appendix B for complete ROC curves. The proposed PCL approach effectively detects and removes adversarial samples by exploring sub-spaces and analyzing data point density. Adversarial samples that remain undetected are often hard to classify even by humans due to model approximation errors. The approach is demonstrated with an example adversarial confusion matrix in the MNIST application. The PCL defense method effectively detects adversarial samples without using them for training, proving its effectiveness against various attack scenarios, including potential future adversarial algorithms. However, its resilience to adaptive attacks targeting defender modules remains to be studied comprehensively. The PCL methodology provides a generic approach to strengthen defense against potential future attacks by training parallel disjoint models with diverse objectives/parameters. Using multiple checkpoints with negative correlation can reduce false alarms and increase detection rate of adversarial samples in MNIST data classification using LeNet model with 4 layers and FGS attack. The study focuses on selecting mixing coefficients to aggregate the confidence of checkpoint defenders in rejecting incoming samples. There is a trade-off between computational complexity and system reliability, with the number of validation checkpoints impacting both. Future work includes automating the customization of checkpoint modules based on application data and real-time analysis requirements. Several research attempts have been made to design deep learning strategies that are more robust against adversarial attacks. Existing countermeasures can be categorized into supervised strategies, which incorporate noise-corrupted inputs during training, and unsupervised strategies that are tailored for specific perturbation patterns. These defense mechanisms can only partially evade adversarial samples generated by different attack scenarios. Recent research has focused on developing deep learning strategies to enhance robustness against adversarial attacks. Unsupervised approaches aim to smooth out decision boundaries by incorporating smoothness penalties or compressing neural networks. However, these methods have overlooked the importance of data density in the latent space, leading to vulnerabilities when facing different perturbations. The proposed PCL methodology is the first unsupervised countermeasure developed based on probabilistic density analysis and dictionary learning to effectively thwart adversarial samples. It does not assume any particular attack strategy or perturbation pattern, demonstrating generalizability in the face of adversarial attacks. The paper introduces a novel end-to-end methodology, Parallel Checkpointing Learners (PCL), to characterize and prevent adversarial attacks in deep learning. PCL analyzes statistical properties of neural network features using complementary dictionaries and probability density functions. It successfully detects adversarial samples with low false-positive rates when tested against various attack models and datasets like MNIST, CIFAR10, and ImageNet. An open-source API for this countermeasure is provided for community testing. The paper introduces Parallel Checkpointing Learners (PCL) to prevent adversarial attacks in deep learning. An open-source API is provided for the proposed countermeasure, inviting the community to attempt attacks against benchmarks. Table 2 shows the neural network architectures for the victim models used in each benchmark, including LeNet-3 for MNIST, CIFAR-10 architecture from BID4, and an ImageNet model inspired by AlexNet. The perturbed examples are visually evaluated to determine attack parameters. The paper introduces Parallel Checkpointing Learners (PCL) to prevent adversarial attacks in deep learning. Table 3 details the parameters used for different attack algorithms, including FGS, JSMA, and Deepfool attacks. The JSMA attack for ImageNet is computationally expensive, and adversarial samples could not be generated using the provided library. The table details parameters for different attack algorithms like FGS, JSMA, and Deepfool attacks. BID22 is characterized by iterative updates denoted by n iters. For the Carlini&WagnerL2 attack BID3, \"C\" is confidence, \"LR\" is learning rate, \"steps\" is binary search steps, and \"iterations\" is the maximum number of iterations."
}