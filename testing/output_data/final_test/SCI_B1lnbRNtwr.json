{
    "title": "B1lnbRNtwr",
    "content": "Models of code can learn distributed representations of a program's syntax and semantics to predict properties. State-of-the-art models use structured representations like trees and graphs for code, providing strong inductive bias. However, these models are primarily local due to high message-passing costs. This work introduces two new hybrid model families, Graph Sandwiches, bridging the gap between global and structured models. By introducing two new hybrid model families, Graph Sandwiches and Graph Relational Embedding Attention Transformers (GREAT), this work aims to improve code representation for tasks like variable-misuse identification. These hybrid models show a 10-15% improvement over traditional models, while also being faster and more efficient in parameter usage. Well-trained models of source code can capture complex program properties such as implicit type structure, naming conventions, and potential bugs and repairs. The text discusses the importance of learning to represent a program's latent, semantic properties based on its source code. Initial representations relied on sequential models from natural-language processing, but struggled to capture the complexity of source code. The Gated Graph Neural Network (GGNN) model was proposed to model some of this structure directly, providing a powerful bias towards semantically meaningful relations in the code. This model was shown to learn better, more generalizable representations faster than classical RNN-based sequence models. The debate on effective modeling of code continues, with graph neural networks relying on local message passing and limited iterations due to computational constraints. In contrast, Transformer models allow program-wide information flow but lack the structured bias from code knowledge. This raises the question of a fundamental dichotomy between global, unstructured and local, structured representations in code modeling. The debate on effective modeling of code continues, with graph neural networks relying on local message passing and limited iterations due to computational constraints. In contrast, Transformer models allow program-wide information flow but lack the structured bias from code knowledge. Our answer is an emphatic no to the fundamental dichotomy between global, unstructured and local, structured models. The sequence-to-pointer model of Vasic et al. (2019) is state-of-the-art for localizing and repairing bugs, but lacks access to the known semantic structure of code. By replacing the sequence encoder with a GGNN, a new graph-to-multihead-pointer model was created, resulting in a 20% improvement over the state of the art. This model efficiently combines longer-distance information with semantic structural information. The proposed model families, Graph Sandwich and GREAT, outperform prior results by 10% each, while training faster and using fewer parameters. There is increasing interest in modeling source code using machine learning, with different approaches such as n-gram models and conditional random fields for predicting program properties. In this paper, various models are discussed for embedding program properties using different techniques such as neural language models, tree-structured recurrent models, and graph-based representations. Some models incorporate run-time information of programs in addition to syntactic information. In this paper, a Transformer-based model is introduced to combine sequence-based and graph-based representations of programs, with a focus on VarMisuse repair tasks. Previous work by Fernandes et al. (2018) combined RNN and GNN architectures, while this study explores larger and more diverse hybrids for program representation learning. Our multi-headed pointer graph, graph-sandwich, and GREAT models outperform previous approaches in predicting bug and repair locations in code. We explore combining local and global information in code representation models to improve performance. Graph-sandwich Models combine local and global information in program graphs and token sequences. Nodes maintain a state vector that is updated through message passing in GGNN and RNN layers. Tokens compute attention to each other using query, key, and value, improving code representation models' performance. The combined model described in the curr_chunk utilizes both RNN and GGNN layers for updating node states in program graphs. This model alternates between GGNN-style message passing operations and sequence-based layers to improve code representation. The sandwich models combine GGNN-style message passing with sequence-based models to incorporate global information. These models aim to transition performance potential by incorporating sequential features. Unlike traditional GGNNs, they directly encode structural bias into sequence-based models. Incorporating structural bias into sequence-based models, the GREAT model changes the attention computation to include a bias term for specific attention weights between tokens. The model compares different architectures and hyperparameters, following a structure proposed by Vasic et al. (2019) with an initial token-embedding layer and a core model for code representation. The core model in our work involves using SubwordTextEncoder to create a sub-token vocabulary and embedding tokens by averaging sub-token embeddings. Different graph-based message-passing neural networks have been proposed for code, with the gated graph neural network (GGNN) being commonly used. Transformers introduce components like multi-headed attention and layer-normalization. In our work, we utilize the GGNN model with a GRU cell for updating node states. We experiment with different message-passing steps and block repetitions, ranging from 4 to 16 message passes. Additionally, we explore RNN architectures, including a bi-directional RNN, for our tasks. In our study, we compare different neural network architectures for our tasks. We found that a simpler bi-directional RNN architecture outperformed the proposed model by Vasic et al. (2019). We also explored Transformer models with varying layers and attention dimensions. Additionally, we investigated sandwich models, including 'small' sandwiches with added RNN or Transformer layers and 'large' sandwiches with multiple message-passing blocks wrapped in RNN/Transformer layers. The study compared different neural network architectures for tasks, including GGNNs with 2 layers and 512 dimensions. Training was done with batch sizes of {12.5K, 25K, 50K} tokens, and sub-tokens were embedded using 128-dimensional embeddings. Models were trained on a single Tesla P100 GPU on 25 million samples. The study focused on training neural network models using different architectures on a single Tesla P100 GPU with 25 million samples. Various edge types were used in program graphs, including data-flow and syntactic edges, to represent programs as graphs. Key decisions were made regarding the Abstract Syntax Tree when representing programs as graphs. The study explored training neural network models on a Tesla P100 GPU with 25 million samples, using different graph architectures. It questioned the effectiveness of including the Abstract Syntax Tree in graph-based models and proposed a leaves-only graph representation for code, removing the need for parent-child relations among syntax nodes. The study focused on training neural network models on a Tesla P100 GPU with 25 million samples, using various graph architectures. It introduced a leaves-only graph representation for code, which is more compressed and aligns better with sequence-based models. The study specifically looked at the variable-misuse localization-and-repair task, predicting pointers for the location of a variable use containing the wrong variable and the correct variable that should be used instead. The ETH Py150 dataset was used for this task. The ETH Py150 dataset was used for training neural network models on a Tesla P100 GPU with 25 million samples. The dataset was partitioned into train and test splits, with 2M total training and 755K test samples. Top-level function definitions were extracted from the files, with up to three samples per function to avoid bias. Buggy examples were synthetically generated by randomly replacing one variable usage with another, while bug-free examples were also included to keep the dataset balanced. To ensure dataset balance, 2M training and 755K test samples were used. Models were trained with up to 250 tokens and tested with 1,000 tokens to assess generalization. Results focused on learning curves and accuracy metrics for bug localization and repair. Models were categorized into families for performance comparison. The study analyzed the performance of different model families on a synthetic dataset, focusing on bugginess-classification accuracy and bug localization and repair accuracy. The models were trained with up to 250 tokens and tested with 1,000 tokens. Results were structured around comparisons of RNNs, Transformers, GGNNs, Sandwich hybrids, and GREAT models. The study compared the performance of various model families on a synthetic dataset, focusing on bug localization and repair accuracy. Graph-to-multihead-pointer models outperformed RNNs and sometimes Transformers, while hybrid global & structured models achieved better results faster. GGNN models took the longest to converge, with Sandwich models being more accurate and faster to converge. The study compared different model families on bug localization and repair accuracy. Graph-to-multihead-pointer models outperformed RNNs and Transformers. The proposed models achieved better results faster, especially with limited training budgets. The Transformer architecture, although slower initially, outperformed the RNN baseline model. The GREAT model gradually surpassed models with explicit message passing after around 10 hours of training. The study compared different model families on bug localization and repair accuracy, with Graph-to-multihead-pointer models outperforming RNNs and Transformers. The proposed models achieved better results faster, especially with limited training budgets. The Transformer architecture, although slower initially, outperformed the RNN baseline model. The GREAT model gradually surpassed models with explicit message passing after around 10 hours of training, achieving state-of-the-art results with less training time. The RNN and GGNN models were found to be complementary, with the combined model showing a 5% improvement in localization accuracy. The Transformer Sandwich model does not show significant performance improvement compared to the RNN Sandwich model, suggesting overlap in their ability to access long-distance information. The study explores variations in parameters, models, and metrics for code analysis. Model capacity is a potential concern when comparing different model families. The study compares different model architectures for code analysis, varying in parameters and model capacity. Increasing the number and dimensionality of layers improved performance for hybrid models, with larger architectures outperforming smaller ones. The best-performing models had 15M parameters for GGNN, and 12.5M & 10M for RNN. The study compared different model architectures for code analysis, with larger architectures having 15M parameters for GGNN, 12.5M & 10M for RNN and Transformer Sandwiches, and 7.9M for GREAT. GGNN models were trained on 'full' graphs using AST structure, while sandwich models were trained on 'leaves-only' graphs. Models not using syntax trained faster due to smaller graph representations, leading in accuracy early on. GGNN with syntax outperformed its counterpart within 48h, while sandwich models lagged behind without a cross-over observed. The sandwich models show a longer lag in performance compared to GGNN baseline within 48h. Best-performing sandwich models had more message-passing steps. Expect a reversal in performance after 10+ days of training due to distance between information propagation along tokens and semantic edges. Recent work combined RNNs and GGNNs by inserting a single RNN layer into a GNN architecture. The study compared a full Sandwich model with a model that inserted a single RNN layer into a GNN architecture. The full Sandwich model outperformed the single RNN model by using fewer parameters and relying more on RNNs for message passing. The best-performing models were tested on metrics related to bug detection and repair, showing good correlation between the two tasks. The baseline RNN model achieved a modest 44% accuracy in bug detection. The hybrid models outperform Transformers and GGNNs in bug detection, especially on long functions. The GREAT model shows promise in bug repair despite limited training. There is a concern about overfitting to synthetic data and the need to ensure usefulness for real bugs. The study collects real variable misuse bugs from code on Github by analyzing 1 million commits that modify Python files. They focus on changes to functions with up to 250 tokens, identifying 170 instances where a single variable usage is changed within a function body. The study identified 170 real variable misuse bugs in Python code on Github. They created a real-world evaluation set with 322 functions and tested various models on these bugs. The models leveraging richly structured representations of source code outperformed previous baselines, showing favorable precision/recall trade-offs. Our study introduced models that combine local and global information to learn powerful representations faster. We proposed sandwich models with different message-passing schedules and the GREAT model, which incorporates sparse graph information into a Transformer for top-notch results. This led to a significant performance improvement of over 30% on the VarMisuse bug localization and repair task."
}