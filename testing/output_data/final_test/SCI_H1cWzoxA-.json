{
    "title": "H1cWzoxA-",
    "content": "Recurrent neural networks (RNN), convolutional neural networks (CNN), and self-attention networks (SAN) are commonly used for context-aware representations. RNN captures long-range dependency but is not time-efficient. CNN focuses on local dependency but may not perform well on some tasks. SAN can model both dependencies but requires more memory with sequence length. A new model, \"bi-directional block self-attention network (Bi-BloSAN)\", splits sequences into blocks and applies intra-block SAN for local context and inter-block SAN for long-range dependency. This approach requires less memory than RNN while retaining SAN's advantages. Bi-BloSAN uses feature-level attention and forward/backward masks for efficient processing of short sequences with low memory requirements. It outperforms RNN/CNN/SAN on NLP tasks, showing a better efficiency-memory trade-off. Context dependency is crucial in NLP tasks, and different neural networks like RNN, CNN, and SAN are used for context fusion. Choosing the right network remains an open problem due to their individual strengths and weaknesses. RNN is commonly used in NLP tasks for capturing long-range dependencies, but faces issues with gradient dispersion and parallelization. LSTM addresses the vanishing gradient problem, while GRU and SRU improve efficiency. CNN is gaining popularity for NLP tasks due to its highly parallelizable computation. CNN is popular for NLP tasks due to its highly parallelizable convolution computation. Hierarchical CNNs like ByteNet and ConvS2S are proposed for capturing long-range dependencies. An attention mechanism called \"bidirectional block self-attention (Bi-BloSA)\" is introduced for fast and memory-efficient context fusion by splitting a sequence into blocks and applying intra-block and inter-block self-attention. The Bi-BloSA model captures local and global dependencies efficiently by using intra-block and inter-block self-attention. A feature fusion gate combines the outputs to produce context-aware representations of tokens. Bi-BloSAN is a RNN/CNN-free sequence encoding model that compresses the output into a vector representation using an attention mechanism. Bi-BloSAN is compared to other sequence encoding models on various NLP tasks, showing advantages in training speed, accuracy, and memory consumption. It achieves a better efficiency-memory trade-off than existing RNN/CNN/SAN models. Notations for vectors, sequences, matrices, and tensors are defined. Word embedding is highlighted as a basic processing step. Word embedding is a fundamental step in deep neural networks for sequence modeling, converting discrete tokens into real value vectors. A pre-trained token embedding is applied to the sequence, producing low-dimensional vectors. The process involves an embedding weight matrix that can be fine-tuned during training. Vanilla attention calculates alignment scores between a query vector and token embeddings in the input sequence. The attention mechanism in deep neural networks calculates alignment scores between a query vector and tokens in the input sequence. Multiplicative and additive attention are two commonly used mechanisms, differing in the choice of compatibility function. Additive attention is a type of attention mechanism that uses biases and an activation function to calculate alignment scores. It is known to have better empirical performance than multiplicative attention but is more time and memory consuming. Multi-dimensional attention, on the other hand, computes alignment scores for each feature, making it more expressive and suitable for words with varying meanings in different contexts. Multi-dimensional attention computes alignment scores for each feature in a token, indicating the importance of the feature to the context. Token-to-token self-attention explores the dependency between two tokens in a sequence, generating context-aware representations. Source-to-token self-attention evaluates the importance of each token to the overall context. Source2token self-attention BID28 BID42 BID29 evaluates token importance in a sentence for a specific task. It uses a compatibility function to compute a probability matrix P. Masked self-attention BID42 allows one-way attention between tokens by applying a mask to the alignment score matrix. In addition, W is fixed to a scalar c and tanh(\u00b7/c) is used as the activation function \u03c3(\u00b7). The procedures for calculating attention output from f(x_i, x_j) are similar to token2token self-attention. Masked self-attention incorporates forward and backward masks for bi-directional order information modeling. The outputs of forward and backward self-attentions are denoted as s=[s1, s2,...,sn]. This section introduces \"masked block self-attention (mBloSA)\" and \"bi-directional block self-attention network (Bi-BloSAN)\" as fundamental self-attention modules. The \"Bi-BloSAN\" utilizes the \"mBloSA\" for sequence encoding, incorporating intra-block self-attention, inter-block self-attention, and context fusion. The goal is to capture local context dependency within each block of the input sequence. The \"Bi-BloSAN\" model uses \"mBloSA\" for sequence encoding, incorporating intra-block self-attention and inter-block self-attention. It introduces an approach to selecting the optimal block length for maximum memory utility rate. Inter-block self-attention captures long-range dependencies among blocks, and a gate is used to merge local and global context features at the block level. The \"Bi-BloSAN\" model utilizes \"mBloSA\" for sequence encoding, incorporating intra-block self-attention and inter-block self-attention. A gate is employed at the block level to merge local and global context features, enhancing the model's ability to capture long-range dependencies among blocks. The \"Bi-BloSAN\" model utilizes \"mBloSA\" for sequence encoding, incorporating intra-block self-attention and inter-block self-attention. It consists of two fully connected layers processing token embeddings, followed by two mBloSA modules with forward and backward masks. The bi-directional attention captures temporal order information, enhancing existing SAN models. The context fusion module, \"Bi-BloSA\", transforms the input sequence to obtain sequence encoding. The \"Bi-BloSAN\" model uses \"mBloSA\" for sequence encoding with intra-block and inter-block self-attention. It includes Bi-LSTM, Bi-GRU, Bi-SRU, Multi-CNN, and Hrchy-CNN models for comparison on various NLP tasks. The \"Bi-BloSAN\" model utilizes BID8 with kernel length 5, gated linear units BID6, and a residual connection. It also incorporates a 600D Multi-head attention BID49 with 8 heads and positional encoding for temporal order information. Experimental codes are implemented in Python with Tensorflow on a Nvidia GTX 1080Ti graphic card. The experiments cover natural language inference, reading comprehension, semantic relatedness, and sentence classifications, with an analysis of time cost and memory load vs. sequence length. In the experiment, Bi-BloSAN is compared to other baselines on the Stanford Natural Language Inference (SNLI) dataset. The dataset contains 549,367/9,842/9,824 samples for training/dev/test split. Various models with different encoders are evaluated based on their performance metrics. In the experiment, different models with various encoders are evaluated on the Stanford Natural Language Inference (SNLI) dataset. The models include Bi-LSTM, Bi-GRU, Bi-SRU, Multi-CNN, Hrchy-CNN, Multi-head, DiSAN, and others. The optimization objective is cross-entropy loss with L2 regularization penalty, optimized by Adadelta. Training uses a batch size of 64 and takes 50 epochs to converge. Weight matrices are initialized by Glorot Initialization, biases by 0, and word embeddings by 300D GloVe 6B pre-trained vectors. The training set words are initialized by uniform distribution. Word embeddings are fine-tuned during training. Bi-BloSAN achieves the best test accuracy among all sentence encoding models on SNLI, outperforming other models by 2.4%, 1.5%, and 0.2%. It can even perform better than semantic tree based models. Bi-BloSAN outperforms various models including SPINN-PI encoder, Tree-based CNN encoder, and NSE encoder. It achieves the best performance among RNN/CNN/SAN baselines, surpassing Bi-LSTM, Bi-GRU, Bi-SRU, multi-CNN, Hrchy-CNN, and multi-head attention. Compared to DiSAN, Bi-BloSAN is faster and more memory efficient, being 3 \u223c 4\u00d7 faster than RNN models and as fast as CNNs and multi-head attention. Bi-BloSAN surpasses state-of-the-art models by utilizing local and global context representations, with mBloSA improving test accuracy. Source2token self-attention outperforms vanilla attention, enhancing accuracy by 3.3%. Reading comprehension aims to find answers in a passage for a given question. The study evaluates models using the Stanford Question Answering Dataset (SQuAD) to locate the sentence containing the correct answer instead of predicting the answer span. Adadelta optimizer is used for training with batch size of 32 and L2 regularization penalty. The network structure tests sequence encoding power in different models to find correct answers. The study evaluates models using the Stanford Question Answering Dataset (SQuAD) to locate the sentence containing the correct answer instead of predicting the answer span. Adadelta optimizer is used for training with batch size of 32 and L2 regularization penalty. The network structure tests sequence encoding power in different models to find correct answers. Methods for evaluating Bi-BloSAN and baselines are discussed, with Bi-BloSAN showing competitive context fusion and sequence encoding capabilities compared to other models. It achieves state-of-the-art prediction accuracy and outperforms Bi-LSTM, Bi-GRU, Bi-SRU, multi-CNN, and multi-head attention models. The goal of semantic relatedness is to predict the similarity degree between two sentences, treated as a regression problem. Sentence encodings are concatenated and used as input for a fully connected layer, followed by a softmax output layer. Evaluation is done on the SICK dataset, with 9,927 sentence pairs for training/dev/test sets. The similarity degree ranges from 1 to 5. The regression problem for semantic relatedness involves sentence pairs from the SICK dataset, with 4,500/500/4,927 instances for training/dev/test sets. The optimization objective includes KL-divergence and L2 regularization penalty, using Adadelta with a batch size of 64. Various models are evaluated, with Bi-BloSAN achieving state-of-the-art prediction quality. Bi-BloSAN is a memory and time-efficient model for sentence classification, outperforming common models like Bi-LSTM, CNNs, and multi-head attention. It is evaluated on six benchmarks for NLP tasks such as sentiment analysis and question-type classification. The benchmarks include tasks like customer reviews, opinion polarity detection, and subjectivity dataset. The curr_chunk discusses various datasets used for training in NLP tasks, including TREC 8, SST-1, and SST-2 for question-type classification and sentiment analysis. The datasets have different sentiment labels and splits for training and testing. Cross-validation is implemented on certain datasets due to the lack of provided splits. The Movie Reviews dataset is not used as SST-1/2 are extensions of it. Bi-BloSAN achieves the best prediction accuracies on CR, MPQA, and TREC datasets, and state-of-the-art performances on SUBJ, SST-1, and SST-2 datasets. It performs slightly worse than RNN models on SUBJ and SST-1 but is much more time-efficient. The training setup includes cross-entropy loss, L2 regularization penalty, Adam optimization, and adjustments for small datasets to prevent overfitting. Bi-BloSAN is slightly less accurate than RNN models on SUBJ and SST-1 but more time-efficient. On the SST-2 dataset, it performs slightly worse than DiSAN in accuracy but has higher memory utility. Bi-BloSAN converges 6\u00d7 and 2\u00d7 faster than Bi-LSTM and DiSAN respectively on the CR dataset. It is less time-efficient than CNN and multi-head attention but has better prediction quality. Trade-offs between efficiency and memory are compared for different sequence encoding models. Bi-BloSAN is faster and more memory-efficient compared to RNN models like Bi-LSTM, Bi-GRU, and BI-SRU. It outperforms multi-head attention and multi-CNN in prediction quality. The block structure of Bi-BloSAN reduces inference time significantly, making it a more efficient option. DiSAN, on the other hand, is not scalable due to its memory consumption growing explosively with sequence length. Bi-BloSAN is a memory-efficient and scalable attention network for sequence modeling. It splits sequences into blocks and uses intra-block and inter-block self-attentions to capture context dependencies. It outperforms RNN models in time efficiency and prediction quality, while consuming less memory than additive attention models like DiSAN. Bi-BloSAN utilizes intra-block and inter-block self-attentions to capture context dependencies and encode temporal order information efficiently. It outperforms RNN models in time efficiency and prediction quality, while consuming less memory than additive attention models like DiSAN. The method for determining memory consumption in mBloSA is introduced, with the optimal sequence length calculated as r = 3 \u221a 2n. A method for selecting r for datasets with normal distribution of sentence lengths is proposed for training with mini-batch SGD. The upper bound of the expectation of the maximal sentence length for each mini-batch in training with SGD is \u03c3 \u221a 2 ln B + \u00b5. The block length r is computed using a specific formula. Each sample in the Stanford Question Answering Dataset (SQuAD) consists of a passage, a question, and an answer span position. The performance of sentence embedding models is evaluated by predicting the sentence containing the correct answer in a passage. A neural net is used to process each sentence with shared parameters, followed by self-attention, to represent the sentences as vectors. The goal is to predict which sentence contains the answer to a given question. The sentence embedding models evaluate performance by predicting the answer-containing sentence in a passage. A neural net processes each sentence with shared parameters and self-attention to represent them as vectors. The output is fed into a fully connected layer to compute a scalar score for each sentence, with a softmax function generating a probability distribution for cross-entropy loss. The sentence with the highest probability is predicted as containing the answer. The regression model predicts similarity degree using a probability distribution as output. KL-divergence is used as the loss function between predicted and target distributions. Structured attention mechanisms capture structural information from input sequences, similar to self-alignment and multi-head attention. Multi-perspective context matching explores dependencies between passage and question from multiple perspectives. The self-attentive structure BID28 embeds sentences to produce matrix representations, while recursive models use self-attention over children nodes to provide input for their parent node in a semantic parsing tree. BID27 applies multi-hop attention for cross-domain sentiment analysis without RNN/CNN structures."
}