{
    "title": "BJlxm30cKm",
    "content": "The study investigates neural network learning dynamics on single classification tasks to understand forgetting events. It is found that certain examples are forgotten frequently, while others are not forgotten at all. The research suggests that a significant portion of examples can be omitted from training data without affecting generalization performance. In this paper, the study explores the phenomenon of catastrophic forgetting in neural networks during continual learning. It investigates how a shift in input distribution across tasks can lead to forgetting previously learned information. The research focuses on the learning dynamics of neural networks on single classification tasks and the impact of omitting certain examples from training data on generalization performance. The study focuses on analyzing catastrophic forgetting in neural networks during continual learning. It examines the learning dynamics by studying interactions between sequentially presented dataset examples during optimization. The goal is to identify consistently forgotten examples and unforgettable examples, hypothesizing that forgotten examples do not share commonalities with other examples from the same task. The investigation aims to analyze forgettable and unforgettable examples in neural networks during continual learning. It seeks to understand the impact of these examples on a model's decision boundary and generalization error, as well as to improve data efficiency without compromising accuracy. Additionally, the study aims to use forgetting statistics to identify important samples, outliers, and examples with noisy labels. The study focuses on analyzing forgettable and unforgettable examples in neural networks during continual learning. It aims to understand the impact of these examples on a model's decision boundary and generalization error, improve data efficiency without compromising accuracy, and identify important samples, outliers, and examples with noisy labels. Techniques such as predefined curricula of examples, self-paced learning, and meta-learning have been extensively studied in the literature. Recent research considers re-weighting examples based on the variance of their predictive distribution, but little analysis has been done on the extent of this phenomenon in proposed tasks. The study aims to empirically study this phenomenon and characterize its occurrence. The study empirically analyzes unforgettable examples in neural networks, finding stable examples across different architectures. Noisy labels and visually complex images are among the most forgotten examples. Training on a dataset with removed least forgotten examples still yields competitive performance. Curriculum learning favors learning from examples of increasing difficulty. In experiments, unforgettable examples can be safely removed without affecting generalization. Curriculum learning involves considering examples of increasing difficulty and learning directly from data to minimize task loss. Noisy examples rank higher in terms of forgetting events, related to outlier detection and removal of examples with noisy labels. Koh & Liang (2017) use influence functions to evaluate the impact of training examples on a model. The impact of training examples on a model's predictions is evaluated using robust statistics. Recent studies on deep neural networks trained with stochastic gradient descent show that generalization error is not solely dependent on hypothesis space complexity. Over-parameterized models can achieve low test error even with random labels, possibly due to implicit regularization by SGD. Empirical evidence supports this phenomenon. Our work provides empirical evidence that generalization can be maintained even when a substantial portion of training examples is removed, without restricting the complexity of the hypothesis class. This aligns with the support vector interpretation. The study focuses on example forgetting in a standard classification setting using a deep neural network trained to minimize empirical risk with stochastic gradient descent. The study focuses on example forgetting and learning events in a deep neural network trained with stochastic gradient descent. It examines the distribution of forgetting events and the classification margin. The study analyzes forgetting events and learning in a deep neural network trained with stochastic gradient descent, focusing on the classification margin and defining unforgettable and forgettable examples based on learning experiences. The study focuses on analyzing forgetting events and learning in a deep neural network trained with stochastic gradient descent. It involves recording forgetting events for each example in the dataset during training by subsampling the full sequence of events. The dataset's examples are sorted based on the number of forgetting events they undergo, with ties broken randomly. Samples that are never learnt are considered forgotten an infinite number of times for sorting purposes. In the experimental evaluation, three datasets were used: MNIST, permuted MNIST, and CIFAR-10. Various model architectures and training schemes were employed to achieve test errors comparable to the state-of-the-art. For MNIST, a network with two convolutional layers and a fully connected layer was trained using SGD with momentum and dropout, achieving 0.8% test error. For CIFAR-10, a ResNet with cutout was trained using SGD and momentum with a specific learning rate schedule, achieving a competitive 3.99% test error. The forgetting events of training examples were estimated for MNIST, permutedMNIST, and CIFAR-10 datasets across 5 random seeds. The datasets show varying numbers of unforgettable examples, with MNIST having the most and CIFAR-10 the least. This suggests a correlation between forgetting statistics and the complexity of the learning problem. The stability of the metric was tested across different random seeds, showing high correlations. Confidence bounds were devised based on experiments with 100 seeds, confirming the ranking of examples with fewer forgetting events. The possibility of forgetting by chance was also quantified. The distribution of forgetting events by chance was analyzed by comparing random update steps to true SGD steps. The histogram in FIG0 shows examples are forgotten by chance a small number of times, at most twice. The observed stability, low number of chance forgetting events, and tight confidence bounds suggest the ordering produced by the metric is not due to random causes. We investigate the presentation numbers needed for unforgettable and forgettable examples to be learnt for the first time. The misclassification margin of forgetting events is computed to validate their relevance. The Spearman rank correlation is used to analyze an example's number of forgetting events. The Spearman rank correlation shows a negative relationship between the number of forgetting events and mean misclassification margin. Unforgettable examples have clear class attributes, while forgotten examples have ambiguous characteristics. The most forgettable examples exhibit atypical traits. The study found that highly forgettable examples with atypical characteristics, especially those with noisy labels, undergo more forgetting events during training. Comparing noisy and regular examples in CIFAR-10, it was observed that noisy examples are the most forgotten and exhibit a higher degree of forgetting compared to examples with original labels. These findings support the hypothesis that forgettable examples show atypical class characteristics. The study observed that in harder tasks like CIFAR-10, a significant portion of examples are forgotten during learning, indicating potential catastrophic forgetting even within the same task distribution. An experiment was conducted with two tasks created from the CIFAR-10 training set, showing some forgetting of the second task during alternating training on both partitions. When training on two partitions from the CIFAR-10 dataset, forgetting of the second task occurs when only the first task is trained. This is surprising as both tasks come from the same distribution. By analyzing forgetting events and creating tasks based on them, it was found that examples with forgotten events suffer more severe forgetting compared to randomly split examples. Additionally, examples with no forgetting events show mild forgetting when trained on examples with forgetting events. When training on examples with forgetting events, there is mild forgetting on unforgettable examples. This suggests that forgotten examples may support those that have never been forgotten. The more an example is forgotten during training, the more useful it may be for the classification task. This aligns with previous observations on re-weighting training examples based on predictive distribution variance. The study tests the possibility of removing a subset of examples during training. In CIFAR-10, removing examples ordered by forgetting events allows for up to 35% of the dataset to be removed while maintaining comparable generalization performance. Similar results were observed on other datasets as well. Removing examples with increasing forgetting events from the dataset leads to worse generalization performance for most of the curve. Some of the most forgotten examples actually hurt performance, possibly outliers or mislabeled examples. Research is ongoing to separate these points from informative ones. Gradient descent in deep networks biases learning towards simple functions and protects against overfitting. Research suggests that stochastic gradient descent converges to solutions that maximally separate the dataset, with some data points being more relevant than others. Linear networks can learn maximum margin classifiers on separable data, supporting the idea that some points are equivalent to support vectors in SVM. Our results show that forgettable training points, similar to support vectors in SVM, have little influence on generalization performance when using SGD for learning the decision function. The number of forgetting events of an example is a relevant metric to detect such support vectors, correlating with the misclassification margin. Different datasets have varying fractions of unforgettable events, leading to performance degradation at different fractions of removed examples. The number of support vectors varies across datasets. The study investigates the correlation between forgetting events and support vectors in training a neural network. The number of support vectors varies based on dataset complexity, with fewer unforgettable examples for higher intrinsic dataset dimensions. The Spearman rank correlation shows stable ordering after 75 epochs of training. The study explores the correlation between forgetting events and support vectors in neural network training. It shows stable ordering after 75 epochs, with a good correlation achieved at 25 epochs. Different architectures are compared, with a reasonably strong agreement between unforgettable examples of different networks. In this study, a WideResNet BID36 is trained on truncated datasets using the example ordering from ResNet18. WideResNet's generalization performance is plotted with 30% of the dataset removed, showing promising avenues for computing forgetting statistics with smaller architectures. The research investigates the learning dynamics of neural networks in single classification tasks, revealing the occurrence of catastrophic forgetting even within a single task. The study explores forgetting statistics within a single task, revealing that some examples are consistently unforgettable while others are prone to being forgotten. These statistics appear to uncover intrinsic data properties rather than training idiosyncrasies. Unforgettable examples have little impact on classifier performance and can be removed without affecting generalization. Future work aims to better understand forgetting events theoretically and apply findings to other areas of supervised and reinforcement learning. The permutedMNIST dataset is harder to learn for convolutional neural networks due to the continual shift of the underlying distribution. It has 45181 unforgettable examples compared to 55012 for MNIST, with an intrinsic data set dimension of 1400 versus 290. The architecture used for the experiments includes two convolutional layers with specific filter sizes and feature maps. The architecture used for the experiments includes two convolutional layers with specific filter sizes and feature maps. The ResNet18 architecture used for CIFAR-10 is described thoroughly in BID6, and a WideResNet with a depth of 28 and a widen factor of 10 is also utilized. The convolutional architecture used in Section 6 consists of a first convolutional layer with 5 by 5 filters and 6 feature maps. The networks are trained to minimize cross-entropy loss using stochastic gradient descent with specific learning rate and momentum values. The ResNet18 and WideResNet architectures are trained using different optimization techniques and learning rates. Precision-recall diagrams show high retrieval performance at 75 epochs. The effect of adding pixel noise to the input distribution is analyzed by corrupting inputs with Gaussian noise. The study analyzes the impact of adding Gaussian noise to input data on retrieval performance. Noise is added after standard normalization, with varying standard deviations. Increasing noise decreases unforgettable examples and shifts distribution towards second mode. Pixel noise is applied to 20% of training data, resulting in changes in forgetting distribution. The study compares forgetting distribution under pixel noise and label noise. Forgetting events may occur by chance, even with random gradients. To estimate the effect of chance, a classifier with randomized update steps is used. The study compares forgetting distribution under pixel noise and label noise by computing forgetting events of the clone classifier on the training set. Results show a chance forgetting rate of at most twice, with confidence intervals established through multiple seed computations. The distribution of forgetting events in CIFAR-100 is shown in FIG0. The distribution of forgetting events in CIFAR-100 is closer to noisy CIFAR-10 than to the original datasets. CIFAR-100 is the hardest to classify among the datasets, with the highest percentage of forgetting events. Each CIFAR-100 class has 10 times fewer examples than CIFAR-10 or MNIST, making each image more useful for learning. The distribution of forgetting in CIFAR-100 is closer to noisy CIFAR-10 than to the original datasets. CIFAR-100 contains images that appear multiple times in the training set under different labels, as shown in the 36 most forgotten examples. In CIFAR-100, images can appear under different labels, leading to forgetting during training. Removal experiments on CIFAR-100 show that examples can be removed without affecting generalization performance."
}