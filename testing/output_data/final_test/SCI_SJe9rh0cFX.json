{
    "title": "SJe9rh0cFX",
    "content": "Compression is essential for deploying large neural networks on resource-constrained platforms. Quantization reduces the number of bits needed to represent weights by limiting the distinct values. This paper explores the representation power of quantized neural networks, proving the universal approximability of quantized ReLU networks. It also provides upper bounds on the number of weights and memory size based on approximation error and bit-width of weights. Results show that the number of weights required by a quantized network to achieve a certain error bound is much lower than that of an unquantized network. The complexity bounds of quantized neural networks are much lower than those of unquantized networks, supporting the success of quantization techniques. Deep neural networks have shown state-of-the-art performance in various tasks, but their large sizes pose challenges for edge intelligence on resource-constrained devices. Efforts have been made to reduce the computational and memory requirements for edge deployment. Efforts have been made to reduce memory consumption of neural networks by reducing weights and decreasing bit-width. Techniques include pruning, weight sparsifying, structured sparsity learning, low rank approximation, and quantization. Quantization can be linear or nonlinear, but its effectiveness is still empirically proven. Quantization is empirically proven to compress neural network architectures, but its theoretical foundation is still lacking. Important questions remain unanswered, such as why binarized networks work well in some cases and how quantization affects network expressive power. This paper provides theoretical insights focusing on ReLU networks and complexity bounds for quantized networks. The paper focuses on the construction of quantized networks to approximate target functions with minimal error. It replaces basic units in unquantized networks with quantized sub-networks to reduce memory size. The challenge lies in minimizing residual errors that propagate throughout the network. The paper proposes solutions to minimize residual errors in quantized networks by choosing proper weight values, constructing efficient sub-networks, and balancing complexity. It compares results with unquantized networks and proves the effectiveness of extremely quantized ReLU networks. The paper demonstrates that extremely quantized ReLU networks with two distinct weight values can accurately represent a wide range of functions. It provides upper bounds on the number of weights and memory size based on the desired approximation error. The study shows that quantized networks require fewer weights compared to unquantized networks to achieve the same error bound, explaining the success of many quantization schemes in practice. Additionally, a theoretical complexity bound is used to estimate an optimal bit-width for the networks. The paper discusses how theoretical complexity bounds can help estimate the optimal bit-width for cost-effective tasks. It reviews related works, presents models and assumptions, proves universal approximability, and analyzes optimal bit-width for quantized neural networks. Nonlinear quantization reduces bit-width for weight storage, using lossless binary coding. Pruned networks like AlexNet, LENET, and VGG-16 can be quantized without loss of accuracy. Even compact networks like SqueezeNet can be quantized to 8-bit while maintaining accuracy. State-of-the-art compact networks can be quantized to 8-bit while preserving accuracy. Some works achieve little accuracy loss on ImageNet classification even with binary or ternary weights. However, reducing the precision of activations may lead to loss of the universal approximation property. The limit of quantization is still unknown, with VGG-16 being quantized to 3-bit while maintaining accuracy. Training of quantized neural networks has shown great empirical success. The training of quantized neural networks has been analyzed theoretically, focusing on network capacity and universal approximability of ReLU networks.ReLU networks are defined as feedforward neural networks with the ReLU activation function. The conclusions on ReLU networks can be extended to networks using piecewise linear activation functions like leaky ReLU and ReLU-6. The training of quantized neural networks has been analyzed theoretically, focusing on network capacity and universal approximability of ReLU networks. Linear quantization assumes distinct weight values are uniformly spaced, while nonlinear quantization allows weight values to take any values needed. Storing each weight requires log(\u03bb) 2 bits. Sparse structures overhead can be minimized using techniques like compressed sparse row (CSR) for nonlinear quantization. When constructing a network to approximate a target function f, two scenarios are considered for deriving bounds in the Sobolev space. The function space F d,n consists of locally integrable functions with weak derivatives up to order n, where the Lipschitz constant is assumed to be no greater than 1 for simplicity. If the constant is larger but bounded, the proof flow remains the same. The design choices for constructing a network to approximate a target function involve two scenarios: function-dependent and function-independent structures. The goal is to achieve a tight upper bound by transforming unquantized networks to quantized ones. The complexity of this transformation depends on the distribution of discrete weights needed. The design choices for constructing a network to approximate a target function involve function-dependent and function-independent structures. The complexity of this transformation depends on the distribution of discrete weights needed. A constructive approach is needed to bound the approximation analytically. The approximation of basic functions by subnetworks, sub-networks approximating weights, and general functions are discussed in different sections. The design parameter determining the approximation error bound is denoted as r, with detailed proof and sub-network construction included in the appendix. The design parameter determining the approximation error bound is denoted as r. A ReLU sub-network with two input units can implement a function \u00d7 : R 2 \u2192 R with certain properties. The depth is O(r), the width is constant, and the number of weights is O(r). The error bound is | \u00d7 (x, y) \u2212 xy| \u2264 6 \u00b7 2^(-2(r+1)). Equation (2) provides the error bound of \u00d7. A sub-network can be constructed to compute the absolute value of x. The approximation error bound is determined by the design parameter t. A ReLU sub-network can approximate a connection with weight w using \u03bb \u2265 2 distinct weights, with an error bounded by 2^(-t). The depth is O(\u03bbt). The text discusses representing integral multiples of 2^(-t) from 2^(-t) to 1-2^(-t) using weights in a neural network. It explains constructing sub-networks with specific weights and connections to achieve this representation. The process involves utilizing a numeral system with a given radix and binary expansion on concatenated weights. The resolution is scaled by multiplying by 2^(-1) in the last layer. The text discusses constructing sub-networks with specific weights to represent integral multiples of 2^(-t) using a neural network. By utilizing a numeral system with a given radix and binary expansion, the sub-network can be activated based on the input x. The efficiency of the sub-network is solely determined by the weights and works for any input x, with the resolution scaled by multiplying by 2^(-1) in the last layer. The efficiency of weight approximation in constructing sub-networks is crucial for overall complexity. Compared to a specific weight selection, our approximation significantly reduces the number of weights. Theorem 1 states that for any function in a given set, a ReLU network with fixed structure can approximate it with any error, with the depth and storage requirements bounded by logarithmic functions of the weights. The proof and network details are provided in Appendix A.2. The Taylor polynomial of order 0 at x = m N can be used even when only a first-order weak derivative exists. We approximate f 2 with a ReLU network denoted as f with bounded error. Nonlinear quantization is discussed, which is more general than linear quantization. For linear quantization, a different subnetwork for weight approximation with width t and depth t log \u03bb +1 is used. Theorem 2 states that for any f \u2208 F d,n, a ReLU network with fixed structure can approximate f with any error, with depth O(log(1/ )) and number of weights O(log(1/ )). The network complexity can be reduced by setting the network topology according to a specific target function, known as function-dependent structure. An upper bound for function-dependent structure is provided when d = 1 and n = 1, which is better than that of a fixed structure. Approximating f(x) with special properties to match quantized networks, piecewise linear interpolation, and \"cached\" functions BID16 are used to reduce complexity. Interpolation at a coarser scale followed by detailed filling helps satisfy error bounds with fewer weights. The complexity of the network can be reduced by using interpolation at a coarser scale and filling in details to minimize error. By assigning a \"cached\" function to intervals based on specific functions, the number of weights can be reduced. However, applying this approach to quantized ReLU networks is challenging due to constraints on weight selection. To address this, the function is transformed to maintain approximation error and Lipschitz constant, allowing for interpolation and cached function methods to approximate the network. The text discusses the use of quantized ReLU networks to approximate functions. Proposition 4 states that for any function f, there exists a function f(x) that can be approximated using interpolation and cached function approach. Theorem 3 shows that a ReLU network can approximate f with any error, with details on network complexity and storage requirements. The text introduces the optimal bit-width problem for neural networks and discusses quantization techniques for ReLU networks. It presents results on network complexity, storage requirements, and the comparison with competitive approaches. The advancement of lossless quantization in neural networks is explored on popular reference networks without changing the network topology. Different bit-widths between two and six are evaluated, with recent work showing that a bit-width of four achieves the best cost-accuracy trade-off. The design space of topology and bit-width combinations remains largely unexplored, leading to sub-optimal results. Efforts using reinforcement learning to optimize network hyper-parameters show promise for future improvements. The current design space for optimizing network hyper-parameters is limited to a single variable per layer. Estimating an optimal bit-width for a target task without training could be a future research direction. The memory bound expression derived in the paper helps determine the optimal \u03bb for the most compact network. The memory bound can be simplified as DISPLAYFORM0, where \u03b8 1 is a constant determined by n and d. An optimal \u03bb that minimizes M (\u03bb) can be found. The paper proves the existence of a global minimum in a specific range and determines \u03bb opt based on log 3n2 d. The optimal bit-width is evaluated quantitatively, showing a range between one and four for input sizes. The theoretical bounds derived for fully connected networks align closely with empirical results, indicating the potential of the approach. The complexity bound for optimal bit-width in deep neural networks is discussed, showing a slow increase with input dimension. Further investigation into optimal bit-width configurations is deferred to future work. In this section, the bound of nonlinear quantization with a function-independent structure is discussed, comparing it with the upper bound from recent work on unquantized ReLU networks. The increase in the number of weights needed for a quantized ReLU network reflects the loss of expressive power due to weight quantization, which decreases as the factor \u03bb log 1 \u03bb\u22121 increases. Comparisons with lower bounds provide a better understanding of the error bounds in weight quantization. The comparison between the upper bound of nonlinear quantization with a function-independent structure and the lower bound from previous work on unquantized ReLU networks shows the tightness of the error bounds. The upper bound also provides insight into the overhead induced by quantization, indicating that the number of weights needed by a quantized ReLU network is no more than O(log 5 (1/ )) times that needed by an unquantized ReLU network. The overhead introduced by weight quantization in neural networks is lower than the lower bound for unquantized ReLU networks. This explains the success of network compression and acceleration through quantization. The benefits of quantization in terms of memory and computation efficiency are significant, leading to its continued growth in usage, especially on resource-limited platforms. Future work includes establishing a tight lower bound for quantized neural networks to improve resource estimation and optimal bit-width determination. Further investigation into trends associated with optimal bit-width in neural networks is suggested. Hardware designers could benefit from early design exploration without extensive training. Research on allowing different bit-widths in various layers may lead to better efficiency and support the emerging trend of hybrid quantization. Implementation of g(x) can be achieved through a ReLU sub-network, enabling the construction of weights for f r s (x) with additional layers. The network is constructed with additional layers and a weight of 12, using a \"pre-scale\" method to reduce the width to a constant. The one-layer sub-network implements g(x) and scales the input by 4. Units are copied to compensate for scaling, resulting in 22rx - \u03a3i=1r 22(r-i)g\u2022i(x). This is scaled by 2-2r in the later layers to get frs(x), achieving a constant width. Theorem 1 states that for any function f in F d,n, a ReLU network with fixed structure can approximate f with any error, given \u03bb distinct weights. The proof involves approximating f using a Taylor polynomial, then approximating it further with a ReLU network. The network's complexity is discussed using a partition of unity. The function f can be approximated using a Taylor polynomial and a ReLU network with fixed structure, given \u03bb distinct weights. The approximation error is bounded by Equation (13), with steps involving the use of P m and \u03c8 m. The complexity of the network is discussed using a partition of unity. The function f can be approximated using a Taylor polynomial and a ReLU network with fixed structure. The approximation error is bounded by Equation (13) with steps involving P m and \u03c8 m. The complexity of the network is discussed using a partition of unity. f 2 is a linear combination of terms of f m,n (x) with an approximation defined as f m,n (x). The error bound of the approximation to f 2 (x) is determined using Equation FORMULA0. The final approximation error bound for the ReLU network implementing f(x) is determined by choosing appropriate weights \u03b2 m,n. The network construction involves computing f m,n(x) using a sub-network and then obtaining f(x) as a weighted sum of all f m,n(x) outputs. The needed weights \u03b2 m,n can be implemented by choosing suitable parameters. The network construction involves computing f m,n(x) using a sub-network and then obtaining f(x) as a weighted sum of all f m,n(x) outputs. The weights \u03b2 m,n can be implemented by choosing suitable parameters. The complexity of the network is analyzed, with the weight construction blocks having the highest order of number of weights. The proof of Theorem 2 shows that for any function f \u2208 F d,n, a ReLU network with fixed structure can approximate f with any error \u2208 (0, 1) using O(log(1/\u03bb)) depth, O(log(1/\u03bb) + DISPLAYFORM24) weights, and O(log(\u03bb) log(1/\u03bb) + log2(1/\u03bb)) bits to store the network. Linearly quantized networks have a minimal resolution of 1/\u03bb, with weight approximation network having width t and depth t log(\u03bb) + 1. Theorem 3 states that for any function f \u2208 F 1,1, a ReLU network with function-dependent structure can approximate f with any error \u2208 (0, 1) using specific depths, weights, and bits for storage. The proof involves transforming f, applying interpolation and cached function methods, and utilizing weight construction. The approximation network in Proposition 3 utilizes a ReLU network with function-dependent structure to approximate any function f \u2208 F 1,1 with error \u2208 (0, 1). The network consists of uniform linear interpolation function f T, sum of selected cached functions f *, and a filtering function \u03a6(x). The complexity is determined by the number of weights in the connections to B f * 1 and inside B m."
}