{
    "title": "ry018WZAZ",
    "content": "Deep learning combined with active learning can drastically reduce the amount of labeled training data needed for named entity recognition tasks. A lightweight CNN-CNN-LSTM model achieves nearly state-of-the-art performance while being computationally efficient. Incremental active learning during training allows for matching state-of-the-art performance with just 25% of the original data. Deep learning combined with active learning can significantly reduce labeled training data for named entity recognition tasks. A lightweight CNN-CNN-LSTM model achieves state-of-the-art performance with only 25% of the original data. The advantages of deep learning diminish with small datasets, highlighting the need to reduce training data requirements for broader use. Active learning offers a cost-effective approach for selecting samples to label in named entity recognition tasks, especially when obtaining annotated data is expensive. Unlike supervised learning, active learning allows the algorithm to choose which examples to label, aiming to select a more informative set of examples. The challenge lies in determining what constitutes as informative and how the active learner can recognize this based on existing knowledge. In this work, practical active learning algorithms on lightweight deep neural network architectures for the NER task are investigated. Training with active learning proceeds in multiple rounds, but traditional schemes are expensive for deep learning. Incremental training with each batch of new labels is carried out instead of retraining from scratch after each round. In this work, a new CNN-CNN-LSTM architecture for NER is proposed, reducing computational requirements for active learning. The model handles out-of-vocabulary words gracefully and trains faster than other deep models. A simple uncertainty-based heuristic is introduced for active learning with sequence tagging. Our model for active learning with sequence tagging selects sentences based on the lowest length-normalized log probability of the current prediction. Experiments on Onto-Notes 5.0 datasets show comparable results to Bayesian active learning by disagreement method, with faster computation. Achieving state-of-the-art performance with significantly fewer samples, our approach matches 99% of the F1 score of the best deep models trained in a supervised fashion using only a fraction of the data. The use of DNNs for NER, pioneered by BID7, has evolved with various architectures proposed in subsequent papers. In recent works, improvements to neural network architectures for sequence tagging have been proposed. Various papers have introduced enhancements such as replacing CNN encoders with bidirectional LSTM encoders, adding hierarchy with additional LSTM and CNN encoders, and using LSTM decoders instead of CRF decoders for faster training. While classic algorithms for active learning do not generalize well to DNNs, which are currently state-of-the-art for NER, LSTM decoders have shown superior performance in handling large numbers of entity types. Active learning applications for DNNs are limited by current theoretical analysis, leading to heuristic selection criteria. Various methods, such as margin-based selection and combining multiple criteria, have been proposed for NLP tasks. While DNNs have shown impressive results, active learning approaches for these models are still underexplored. BID45 is the first to study active learning for image classification with CNNs, using uncertainty-based sampling methods. Prior to this work, deep active learning for sequence tagging tasks, which often have structured output space and variable-length input, has not been studied extensively. Most active learning methods require frequent model retraining as new labeled examples are acquired. Efficient retraining is crucial while maintaining performance comparable to state-of-the-art DNNs. DNN architectures for NER can be broken down into three components: character-level encoder, word-level encoder, and tag decoder. The proposed lightweight neural network architecture for NER, named CNN-CNN-LSTM, utilizes CNNs for computational efficiency. Input sentences are formatted with special tokens and grouped into buckets for batch computation. Character representation follows a similar procedure. The proposed neural network architecture for NER, called CNN-CNN-LSTM, uses CNNs for efficiency. The formatted sentence is denoted as {x ij}, with character-level features extracted using CNNs. LSTM slightly outperforms CNN but with higher computational cost. ReLU nonlinearities and dropout are applied between CNN layers, with max-pooling for fixed-length word representation. The curr_chunk discusses the process of representing words using character-level features and latent word embeddings. It mentions the initialization of word embeddings with word2vec training and the use of a special token for unseen words during training. The architecture includes CNNs for word-level input features and LSTM RNNs for word-level encoding. The curr_chunk discusses the comparison between LSTM and CNN word-level encoders for F1 score improvement. It also explains the use of a tag decoder with Chain CRF or LSTM RNN for NER tasks. The time complexity for training is O(nT^2) where T is the number of entity types. The curr_chunk explains the functioning of the decoder in a locally normalized model, which speeds up training compared to using CRFs. It also mentions the use of LSTM decoders for tag decoding, showing comparable performance to chain CRF decoders. The use of RNNs for tag decoders has been explored, showing state-of-the-art performance compared to CRFs. Active learning is used to strategically choose sentences for annotation to improve NER labeling data. The learning process involves multiple rounds where the model is updated after receiving annotations. The cost of annotating a sentence is proportional to the number of words in the sentence. The cost of annotating a sentence is proportional to the number of words in the sentence. Uncertainty sampling strategy ranks unlabeled examples based on model's uncertainty in predicting labels. Three ranking methods can be easily implemented in deep neural approaches to NER. The Maximum Normalized Log-Probability (MNLP) method addresses the issue of disproportionately selecting longer sentences in the labeling process. Another approach, Bayesian Active Learning by Disagreement (BALD), considers sampling based on uncertainty measures. This method relates to dropout and deep Gaussian processes. In this paper, the authors propose a method to measure model uncertainty using dropout in deep Gaussian processes. They use multiple dropout masks to calculate uncertainty on each word and suggest sampling examples where the model is uncertain to improve efficiency. The authors propose a method to measure model uncertainty using dropout in deep Gaussian processes. They suggest sampling examples where the model is uncertain to improve efficiency and guard against collecting redundant information. They explore techniques to maximize representativeness of a labeled set through submodular optimization. The authors introduce a method to measure model uncertainty using dropout in deep Gaussian processes. They aim to select a set of examples with high uncertainty scores to improve efficiency. The approach utilizes submodular optimization to maximize the representativeness of the labeled set. However, in experiments, this method does not outperform uncertainty-based heuristics. In named entity recognition, vanilla stochastic gradient descent is commonly used as it outperforms more sophisticated methods at convergence. The step size is set at 0.001 and batch size at 128. LSTM tag decoder shows comparable performance to CRF tag decoder, and outperforms CRF decoder when used with CNN encoder. Training speed is measured by computing the time spent for one iteration of training on the dataset with eight K80 GPUs. Comparison with other models is shown in TAB5. On the CoNLL-2003 English dataset, CNN-CNN-LSTM and CNN-CNN-CRF have comparable training speeds. However, on the OntoNotes 5.0 English dataset with 18 entity types, CNN-CNN-LSTM is twice as fast as CNN-CNN-CRF due to the quadratic time complexity of computing the partition function for CRF. CNN-CNN-LSTM is also 44% faster than CNN-LSTM-LSTM on OntoNotes. Compared to the CNN-LSTM-CRF model, CNN-CNN-LSTM provides a four times speedup in training speed and achieves high performance measured by F1 score. State-of-the-art models trained on OntoNotes-5.0 English and Chinese data achieve high F1 scores. Empirical comparison of selection algorithms shows active learning algorithms outperform random baseline. MNLP and BALD slightly outperform others. Active learning algorithms, including MNLP and BALD, outperform traditional LC in early rounds. MNLP is more computationally efficient than BALD. These algorithms achieve 99% performance of the best deep model with only a fraction of the training data. Additionally, a small percentage of training data was enough for deep active learning algorithms to surpass the performance of shallow models. The experiment was repeated multiple times to confirm the trend. In an experiment on active learning algorithms, different training datasets were created from OntoNotes datasets. A CNN-CNN-LSTM model trained on half-data achieved 85.10 F1, outperforming models trained on biased datasets. Genre coverage in training data was found to be important. The algorithm chose more newswire sentences for biased datasets compared to unbiased half-data. The algorithm selected more newswire sentences for biased datasets compared to unbiased half-data. It also undersampled newswire sentences for nw-only-data and increased the proportion of broadcast news and telephone conversation genres. Despite not providing genre information, the algorithm automatically detected underexplored genres. The use of a simple greedy decoding method, such as beam search with size 1, was found to work well in practice. Changing the beam size of the decoder had minimal impact on the model's performance. The model's performance is minimally affected by changes in beam size during decoding. Beam search with size 2 slightly outperforms greedy decoding, but increasing the beam size further does not improve results. The LSTM decoder, despite not always selecting the most probable tag sequence, can still outperform the CRF due to its ability to model long-range dependencies. Experimental results on OntoNotes-5.0 English show competitive performance between MNLP, LC, and BALD in active learning experiments. In active learning experiments, MNLP and BALD outperform LC in early rounds of labeled data acquisition. Techniques are explored to select a representative set of samples to avoid oversampling outliers and collecting redundant information. The problem of maximizing representativeness is expressed as a submodular optimization problem. In active learning experiments, MNLP and BALD outperform LC in early rounds of labeled data acquisition by selecting a representative set of samples. The problem of maximizing representativeness is expressed as a submodular optimization problem, with a utility function defined for labeling new samples. The set is defined to maximize similarity gain between labeled and unlabeled data under a knapsack constraint. Representation-based sampling can benefit from uncertainty-based sampling by re-weighting samples based on model uncertainty. Representation-based sampling can benefit from uncertainty-based sampling by re-weighting samples based on model uncertainty. The optimization problem can be computationally challenging, even with state-of-the-art submodular optimization algorithms. To improve efficiency, the set of unlabeled examples is restricted to top samples from uncertainty sampling within a budget. Representation-based sampling can benefit from uncertainty-based sampling by re-weighting samples based on model uncertainty. The greedy algorithm BID33 has an (1 \u2212 1/e)-approximation guarantee for the knapsack constraint. However, the algorithm requires calculating the utility function O(l 2 n) times, where l is the number of samples and n is the number of unlabeled samples. Alternatively, lazy evaluation can decrease the computation complexity to O(ln) BID26, but requires an additional hyperparameter. The two-pass streaming algorithm of BID2, with complexity O(ln) 3, is adopted for the knapsack constraint. In the first pass, the maximum function value of a single element normalized by its weight is calculated to estimate the optimal value. In the second pass, O( 1 log K) buckets are created. Representation-based sampling can benefit from uncertainty-based sampling by re-weighting samples based on model uncertainty. The two-pass streaming algorithm of BID2 for the knapsack constraint involves creating O(1 log K) buckets in the second pass and updating them greedily. The algorithm provides a guarantee of a (1\u2212\u03b5)(1\u2212\u03b4)2-approximation for the problem. In practical label acquisition, the budget per round is usually larger than the longest sentence length in the unlabeled set, making \u03b4 negligible. Experimental results showed \u03b4 to be around 0.01."
}