{
    "title": "SkMuPjRcKQ",
    "content": "Probabilistic Neural Networks incorporate various sources of stochasticity, such as input noise and parameter uncertainties. This paper introduces a feed-forward propagation method that estimates the mean and variance for each neuron considering these sources of uncertainty. Unlike standard neural networks that only use point estimates, this approach propagates uncertainty as well. The paper presents new approximations for uncertainty propagation in networks with softmax and max-pooling layers, as well as leaky ReLU activations. The paper discusses the importance of probabilistic treatment in Neural Networks, particularly focusing on the use of dropout for improved generalization. It highlights the challenges of accurately computing expectations and the sensitivity of NN outputs to perturbations. The text discusses the impact of input uncertainties on neural network outputs, emphasizing the need to estimate output uncertainty. It also addresses the issue of classifier overconfidence and the importance of considering uncertainties in predicting output values. Bayesian learning is suggested as a way to address overfitting in neural networks. Bayesian learning addresses overfitting in neural networks by treating parameters as random variables with uncertainty propagated to predictions. Neural networks are viewed as directed probabilistic graphical models, with all units considered random in a conditional Bayesian network structure. The conditional distribution p(X k | X k\u22121 ) factorizes as p( DISPLAYFORM1 are activations. We do not consider Bayesian learning and the weights w are assumed to be non-random. The posterior distribution of each layer k > 0, given the observations x 0 , recurrently expresses as DISPLAYFORM2. Standard NNs with injected noises give rise to Bayesian networks. The conditional distribution of a belief network can be recovered from a known distribution. For a stochastic binary unit BID32, the conditional probability can be calculated using the Heaviside step function and noise distribution. Approximating the posterior distribution with a factorized distribution is common in recognition problems. The expectation is often difficult to compute due to a large number of modes in the posterior. In recognition problems, factorized approximations are commonly used for the posterior distribution. The distributions for hidden states and final predictions are concentrated around specific values. The shape of distributions can vary based on unit type, such as Bernoulli for binary and Gaussian for real-valued activations. Mean and variance are sufficient statistics for approximating distributions, allowing for flexibility in the exact shape of distributions. The optimal factorized approximation to the posterior is given by the marginals. The optimal factorized approximation to the posterior distribution is computed layer-by-layer, matching moments of exponential family distributions. This method involves propagating the factorized approximation with marginalization updates until the approximate posterior output is obtained. This approach is related to assumed density filtering and involves projecting the true posterior onto a family of distributions for inference. The activation in a deep network is a linear combination of inputs from the previous layer, approximated by a uni-modal distribution with mean and variance. The covariance matrix is approximated by its diagonal due to factorization assumptions. Nonlinear mappings involve a scalar random variable with statistics and independent noise. A scalar random variable with statistics and independent noise is used to approximate the expectation and variance of a function in deep networks. For binary variables with Heaviside nonlinearities, the distribution is described by one parameter, and the propagation rule depends on the variance for further layers. An example includes a Heaviside nonlinearity model with logistic noise, where the mean of the output is calculated based on the statistics of the noisy activations. The approximate inference in networks with binary and continuous variables can be represented as a feed-forward moment propagation. Standard neural networks simplify this method by assuming zero variance in activations and propagating only the means through non-linearities. Injected noise models result in smoothed versions of functions like the sigmoid function. The noisy Heaviside function recovers the standard sigmoid function, viewing standard NNs as a simpler form of factorized inference in the Bayesian NN model. The main technical contribution is propagation rules for argmax, softmax, and max mappings, showing that softmax is the expectation of argmax with injected noise. The standard NN with softmax layer can be seen as an approximation of argmax layer with injected noise. A new approximation for the argmax posterior probability considers uncertainty of activations, enabling propagation through argmax and softmax layers. The curr_chunk discusses approximations for activations, including argmax and softmax layers, leaky ReLU, and the softmax function in neural networks. It also explores latent variable representations and the indicator of noisy argmax. The proposed approximations are easy to compute and continuously differentiable, making them suitable for NNs. The curr_chunk discusses approximations for the noisy argmax indicator and the standard Gumbel distribution in neural networks. It explains the AP1 and AP2 approximations for latent models, focusing on computing expectations over latent noises. The approximation for the expectation of argmax indicator without injected noise is derived, and the injected noise case is addressed by increasing the variance of each component. The curr_chunk discusses approximations for the distribution of U in neural networks. It approximates U with a logistic distribution to match means and variances, allowing for the evaluation of necessary probabilities. The approximation of the distribution of U in neural networks involves decomposing variables and introducing new variables Uk. The probability P(U \u2265 \u2212Z|Z) is approximated using a logistic distribution. To achieve linear complexity, the approximation is simplified by approximating \u03c3. The approximation of the distribution of U in neural networks involves decomposing variables and introducing new variables Uk. The moments of the maximum Y = max k X k can be computed from the cdf of Y. The approximation reweighs summands differently if \u03c3 y differs from \u03c3 a and can be computed in linear time. The text discusses approximations for the mean and variance of two hierarchically related variables in neural networks, based on the probabilities of argmax values. The approximations are derived using entropy calculations and coefficients from a Taylor expansion. These approximations are useful for modeling popular layers like leaky ReLU and maxOut. The text proposes practical approximations for the mean and variance of max(X1, X2) using cheap computations and correct output variance ratios for small noises. It suggests using logistic cdf for the mean approximation and discusses the challenges of numerically approximating the cdf for large values. The text proposes practical approximations for the variance of max(X1, X2) using logistic cdf and a sigmoid-shaped function, with a simplified approximation for the variance shown in a figure and evaluated in experiments. The text proposes practical approximations for the variance of max(X1, X2) using logistic cdf and a sigmoid-shaped function, with detailed evaluation in \u00a7 B.1. The accuracy of the proposed approximation is compared to standard propagation, showing better test likelihoods with simple calibration. Implementation details, training protocols, datasets, and networks are provided in \u00a7 C. The running time of AP2 is 2\u00d7 more for a forward pass and 2-3\u00d7 more for a forward-backward pass than AP1. Multiple layers are considered, with experiments on approximating the real posterior of neurons with noise in network input and dropout. The LeNet5 model by LeCun et al. is studied using the proposed approximations for estimating ground truth statistics of neurons. The error measures for means and standard deviations are calculated, with results shown in Table 1. The LeNet5 model by LeCun et al. is analyzed using approximations to estimate ground truth statistics of neurons. AP2 is found to be more accurate than AP1, but both methods are affected by the factorization assumption. The variance computed by AP2 provides a good estimate, and the estimated categorical distribution obtained by propagating the variance through softmax aligns closely with the MC estimate. The ALL-CNN network by BID28 trained with standard dropout on CIFAR-10 is also examined, showing that GT noise std \u03c3 * varies significantly across layers, with AP2 providing a useful estimate. The AP2 method provides a useful estimate for the variance of neurons across layers. By calibrating the standard deviation in the last layer with the average factor estimated on the training set, significant improvements in test likelihoods are achieved. This method, denoted as AP2 calibrated, normalizes the output using dataset statistics to improve initialization and training conditions. The AP2 method provides a useful estimate for the variance of neurons across layers, improving test likelihoods by calibrating the standard deviation in the last layer. The method normalizes the output using dataset statistics for better initialization and training conditions. In an experiment with a fully trained LeNet5 (MNIST) network, the accuracy of mean and variance statistics approximations for each layer is shown, with observations on the growth of MC std \u03c3 * from input to output, accuracy drops at linear layers, and improvements in KL divergence with AP2. The MC class posterior is enhanced with AP2, improving accuracy in All-CNN (CIFAR-10) with dropout. The experiment approximates dropout analytically, comparing standard dropout with AP2 dropout. The MC std \u03c3 * grows with depth, causing a drop in accuracy in convolutional and pooling layers. The experiment compares standard dropout with AP2 dropout, showing that AP2 improves accuracy in All-CNN (CIFAR-10). Analytic dropout is efficient as a regularizer, non-stochastic, and speeds up training compared to standard dropout. AP2 achieves the best test likelihood and significantly improves results for the network. In comparison to standard dropout, Gaussian dropout BID29 performed similarly or slightly worse, while Variational dropout BID15 did not improve over the nodropout baseline. A new uncertainty propagation method for probabilistic neural networks was described, allowing for transparent interpretation of standard propagation in NNs. Results for All-CNN on CIFAR-10 test set show negative log likelihood (NLL) and accuracy. Different test-time methods were evaluated, with \"AP2 calibrated\" approximating dropout well. Analytic dropout achieved the best likelihood and accuracy, improving accuracy compared to standard propagation for estimating dataset statistics. New approximations were proposed for handling max, argmax, softmax, and log-softmax layers using latent variable models. The study focused on improving test likelihoods by calibrating a cheap method, addressing the factorization assumption weakness in approximation. The use of argmax and softmax within the network, such as in capsules or multiple hypothesis models, was highlighted. The developed technique has potential applications in generative learning, semi-supervised learning, and Bayesian model estimation. The study aimed to enhance test likelihoods by refining a cost-effective approach, overcoming the factorization assumption weakness in approximation. It involved utilizing argmax and softmax in networks like capsules or multiple hypothesis models. The technique developed has potential uses in generative learning, semi-supervised learning, and Bayesian model estimation. The expected value of the maximum Y = max k X k is computed through total expectation, approximating conditional densities and choosing thresholds to satisfy proportionality. The study focused on refining a cost-effective approach to enhance test likelihoods by overcoming the factorization assumption weakness in approximation. It involved utilizing argmax and softmax in networks like capsules or multiple hypothesis models. The technique developed has potential uses in generative learning, semi-supervised learning, and Bayesian model estimation. The mean and approximation formulas for logistic and normal distributions were derived, along with the joint conditional density and its marginal density. The approximation \u03bc k was proven to be an upper bound, with detailed mathematical proofs provided. The study derived mean and approximation formulas for logistic and normal distributions, proving \u03bc k as an upper bound. Variational Bayesian learning techniques were utilized to enhance test likelihoods by overcoming factorization assumption weaknesses. The approximations for expected maximum values and variance were provided, along with detailed mathematical proofs. In variational Bayesian learning, the expectation of log p(y|x, \u03b8) is computed with Jensen's inequality. The log-sum-exp operation, known as the smooth maximum, is handled using Gumbel random variables. The last layer for classification problems involves the log of softmax. In variational Bayesian learning, the expectation of log p(y|x, \u03b8) is computed with Jensen's inequality. The log-sum-exp operation, known as the smooth maximum, is handled using Gumbel random variables. The last layer for classification problems involves the log of softmax. For approximating the expectation of log softmax(X), increasing the variances of all inputs Xk by \u03c3^2S/2 and applying the approximation for the maximum can be done. This leads to the proposition that for X_i with statistics (\u00b5_i, \u03c3^2_i), the expected softmax values can be expressed using certain formulas. Additionally, for p(y=1|x) = 1/(1 + e^(-x)), the log likelihood can be expressed as -log(1 + e^(-x)), and an analogue of a previous proposition can be made using a standard Logistic random variable Z. To approximate E[log(1 + e^X)], increasing the variance of X by \u03c3^2S and applying the existing approximation for ReLU can be done. The simplified approximation of Leaky ReLU is evaluated without using the normal cdf function. The input variance is fixed at 1, and the results are plotted based on the input mean \u00b5. The approximation of mean, variance, output distribution, and derivatives are reasonable, despite some deviation from the ground truth model. The proposed approximation for softmax is evaluated using independent inputs with normal distributions. The experiment involves estimating the output class distribution and evaluating the KL divergence for quadratic and linear time approximations. The evaluation includes testing with scaled variances and is repeated 1000 times with different samples. The experiment involves evaluating the proposed softmax approximation using normal distributions for independent inputs. It includes estimating output class distribution, evaluating KL divergence, and checking the Jacobian approximation. The baseline AP1 cannot estimate the gradient in \u03c3, leading to intervals around analytic estimates due to variance. As input variance decreases, the ground truth gradient estimate degrades, approaching zero on average. The experiment involves evaluating the proposed softmax approximation using normal distributions for independent inputs. Unlike softmax, the range of \u00b5 is not important, allowing U to be fixed at 1. The expected argmax indicator value is approximated, showing accurate modeling. Gradient computation with MC methods is challenging due to the non-differentiable argmax indicator, requiring the use of the score function estimator. The score function estimator BID4, also known as the REINFORCE method, requires a large number of samples for reliable results. Gradient estimation for \u00b5, \u03c3 shows high variance with baseline MC estimates. Analytic methods for small variances have unmeasured accuracy due to degradation of ground truth estimates. Variance reduction techniques could improve accuracy, with implementation in the PyTorch framework for reproducibility. The implementation is modular, with different propagation methods for deterministic and stochastic layers. AP2 propagation involves drawing samples for stochasticity and computing Monte Carlo estimates. The feed-forward propagation with AP2 is slower than AP1 or sample methods. The feed-forward propagation with AP2 is slower than AP1 or sample methods, taking about 3 times longer. The relative times for forward-backward computation in our implementation are: standard training 1, BN 1.5, inference=AP2 3, inference=AP2-norm=AP2 6. We used MNIST and CIFAR10 datasets, splitting them into training, validation, and test sets. The validation set is for model selection and monitoring, while the test set was used for stability tests. Optimization was done with a batch size of 32 and SGD optimizer with Nesterov Momentum 0.9. The optimization process used a batch size of 32, SGD optimizer with Nesterov Momentum 0.9, and a learning rate lr \u00b7 \u03b3 k. The initial learning rate was determined through numerical search optimizing training loss in 5 epochs. Linear and convolutional layer parameters were initialized using pytorch defaults. Standard minor data augmentation was applied during training. The LeNet5 architecture BID16 and the All-CNN network BID28 were used for training on CIFAR-10 dataset with data augmentation. Normalization was applied after each convolutional and fully connected layer. The All-CNN network had convolutional layers with specific parameters for kernel size, stride, and depth, with the final layers ending with activation. The final layers of the network are modified in the ConvPool-CNN-C model with Leaky ReLU activation. The model on CIFAR-10 dataset uses stride-1 convolutions followed by 2x2 max pooling. Analyzing the normalization method BID27, the network's initialization methods are compared, showing improved behavior with statistical arguments in BID9. The text discusses the importance of initialization and normalization in efficient learning for neural networks. It highlights the use of statistical estimates for normalization, the impact of initialization on learning rate and network trainability, and the benefits of tracking normalization during training. The comparison between batch normalization and normalization through propagation of dataset statistics is also mentioned. The text discusses the impact of normalization on network performance, emphasizing the benefits of using normalization during training. It also mentions the regularization effect of batch normalization and the accuracy drop observed after max pooling in a ConvPool-CNN-C network."
}