{
    "title": "rJfJiR5ooX",
    "content": "Dense word vectors in NLP tasks lack interpretability. Previous methods focused on sparse embeddings or post-processing. This study shows conventional embeddings from Singular Value Decomposition are also interpretable. A novel eigenvector analysis method reveals semantic groups in word vector dimensions, aiding in understanding words for NLP tasks. Word embeddings, such as word2vec and GloVe, play a crucial role in NLP tasks but lack interpretability. The dimensions of word vectors are difficult to understand, making it challenging to interpret their meaning. A new method using eigenvector analysis helps reveal semantic groups in word vector dimensions, aiding in understanding words for NLP tasks. In this study, the meaning of high and low values in word embeddings is explored using eigenvector analysis. The analysis compares the eigenvectors of word embeddings obtained with truncated SVD of the PPMI matrix with the row and column space analysis of SGNS. The study aims to understand the dimensions of word vectors for NLP tasks. The study explores the meaning of high and low values in word embeddings through eigenvector analysis, comparing SVD and SGNS matrices. It suggests that word vector dimensions can represent semantic features and introduces novel analysis methods inspired by Random Matrix Theory. Previous works have shown similar results in semantic grouping, proposing various algorithms for training interpretable word vectors. In our work, we focus on the implications of word embeddings without additional constraints or post-processing steps. We define the Positive Pointwise Mutual Information (PPMI) matrix as M PPMI and the set of unique words as vocabulary V. The Pointwise Mutual Information (PPMI) matrix is denoted as M PPMI, with unique words in the vocabulary V. Word embedding matrices are created from SVD and SGNS as W SVD and W SGNS. The k-th largest eigenvalue and corresponding eigenvector of M PPMI are denoted as \u03bb k and u k \u2208 R |V|, and the k-th column of W SGNS as v k \u2208 R |V|. Co-occurrence matrix M represents co-occurrence counts of words in the corpus, with Pointwise Mutual Information (PMI) transforming the matrix by measuring the log ratio between joint probabilities of words. The Positive Pointwise Mutual Information (PPMI) measure is used to handle never observed pairs by mapping negative values to 0. Truncated SVD, equivalent to PCA, factorizes the PPMI matrix and truncates to d dimensions. The word embedding matrix is taken as W = U_d, instead of the more \"standard\" eigenvalue weighting W = U_d \u00b7 S. This approach is discussed further in Section 6.2. Unlike PPMI and SVD, word2vec does not provide exact solutions. The word2vec Skip-Gram model, proposed by BID1, trains two word embedding matrices W and C with a neural network to maximize dot product between similar word-context pairs and minimize dot product between wrong pairs. The Softmax function is used for multi-class scenarios, but the normalization constant for exponentials of all context words is computationally expensive. Skip Gram with Negative Sampling (SGNS) simplifies the objective using negative sampling. Analysis includes distributions of eigenvectors, Inverse Participation Ratios (IPR) for quantifying significant elements, and measuring structural sparsity. The empirical distribution of eigenvector elements u k is compared with a Normal distribution to measure normality, showing genuine correlation between stocks. The Inverse Participation Ratio (IPR) quantifies the ratio of significant elements in the eigenvector u k. The Inverse Participation Ratio (IPR) measures the ratio of significant participants in the eigenvector u k, indicating the sparsity of the vector. Dividing the reciprocal 1/I k with |V| yields the sparsity of the vector u k. By visualizing the top eigenvector elements, we can map indices to words in the vocabulary V to analyze semantic coherence. The dataset size was reduced from 66GB to 25GB by removing noisy non-alphanumerics. The vocabulary size is 346K with words having at least 100 occurrences. SGNS and SVD models were trained with a context window size of 2 and embedding dimension of 500. Eigenvectors corresponding to larger eigenvalues deviate from a Gaussian distribution. The larger eigenvalues like u 1 and u 2 show deviation from a Gaussian distribution, with u 1 having non-zero negative values suggesting a common bias affecting all \"words\". Eigenvectors of W SVD have higher IPR values compared to W SGNS. The eigenvectors of W SVD have significantly higher IPR values compared to W SGNS, indicating sparser vectors. The mean sparsity for W SVD eigenvectors was 27.5%, while for W SGNS column vectors it was around 36%. This difference in structural sparsity led to a deeper analysis of the eigenvectors of W SVD. The top participants of the word vector for \"airport\" in decreasing order include baseball-related words like \"buehrle\", \"rbis\", and \"astros\". The largest eigenvector, u 1, explains bias and includes strong transition words for dramatic effects. The 121st vector, u 121, shows a focused semantic grouping related to baseball with high IPR values, indicating sparsity. The text discusses the sparsity of word vectors with high IPR values, showing that sparser eigenvectors capture distinct features like foreign names or baseball topics. A comparison between W SVD and W SGNS columns reveals semantic coherence in W SVD and randomness in W SGNS. Specific word vectors like \"airport\" are analyzed for salient dimensions, with u 53 grouping location-related words and u 337 grouping airline companies. In Section 5.1 and 5.2, eigenvectors show genuine correlation and structure in the column space. Section 6.1 demonstrates that semantically coherent words form significant groups in each eigenvector. High values in W columns indicate relevance to semantic groups, such as u 121 representing a baseball-related group. The dimensions of word vectors can be interpreted as relevant semantic features. The dimensions of word vectors can be viewed as semantic features, similar to latent topics in Topic Modeling. The structural sparsity discovered with the IPR is confirmed by contrasting the vectors from SVD and SGNS. The eigenvalue weighting scales each dimension accordingly. Weighting with eigenvalues scales each feature column, incorporating a prior that may not always improve performance in NLP tasks. Analyzing eigenvectors of word embeddings reveals semantically coherent groups, making word vectors interpretable as feature vectors composed of semantic groups. The results show that word embeddings can be interpreted as feature vectors composed of semantic groups, aiding in error analysis and creating efficient task-specific embeddings. Future work will focus on practical applications of interpretability."
}