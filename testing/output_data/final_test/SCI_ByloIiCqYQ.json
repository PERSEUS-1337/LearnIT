{
    "title": "ByloIiCqYQ",
    "content": "Due to the increasing threat of software vulnerabilities, detecting vulnerabilities in binary code is crucial in the software and computer security industries. Traditional methods rely on manual selection of features by experts, but this paper introduces the Maximal Divergence Sequential Auto-Encoder using deep learning to improve binary vulnerability detection. Extensive experiments show promising results compared to baseline methods. Our proposed methods outperform baselines in all performance measures of interest for software vulnerability identification. Software vulnerabilities are flaws in software that allow malicious acts. The severity of the threat from software vulnerabilities has increased over the years despite efforts from the security community. The severity of software vulnerabilities has increased over the years, leading to significant damages to companies and individuals. Examples include threats to popular browser plugins like Adobe Flash Player and Oracle Java, as well as open-source software like Heartbleed and ShellShock. Software vulnerability detection can be categorized into source code and binary code detection, with source code detection being widely studied in previous works. Most previous works in source code vulnerability detection have relied on handcrafted features chosen by domain experts. Recent studies have explored the use of automatic features in SVD, with some employing Recurrent Neural Networks to transform code tokens into vectorial features. Binary code vulnerability detection is more challenging due to the loss of syntactic and semantic information during compilation. The ability to detect vulnerabilities in binary code without access to source code is crucial for computer security. Various techniques like fuzzing and symbolic execution have been proposed for this purpose. However, there is a lack of research on using automatically extracted features for binary code analysis. There is a lack of research on using automatically extracted features for binary code vulnerability detection. Unlike malware detection, binary code vulnerability detection aims to identify flaws in binary code. One challenge is the difficulty in distinguishing between vulnerable and non-vulnerable binaries. Another constraint is the scarcity of labeled binaries for research purposes. In binary code vulnerability detection, there is a lack of large public binary datasets. Most source code datasets are not compilable, making it challenging to collect and label source code. Recent advances in deep learning are leveraged to automatically extract features for vulnerability detection from binary code sequences. The Maximal Divergence Sequential Auto-Encoder (MDSAE) uses Variational Auto-Encoders (VAE) to represent binary code for vulnerability detection. It encourages representations of vulnerable and nonvulnerable binaries to be maximally different while preserving crucial information. Two learnable Gaussian priors are used for each class to separate representations based on divergence measures like Wasserstein distance or Kullback-Leibler divergence. The Maximal Divergence Sequential Auto-Encoder (MDSAE) utilizes Variational Auto-Encoders (VAE) to distinguish between vulnerable and non-vulnerable binary representations. It introduces two Gaussian priors for each class to separate representations based on divergence measures like Kullback-Leibler divergence. The MDSAE can generate data representations for classifiers like Support Vector Machines or Random Forest, or be integrated with a shallow feedforward neural network for simultaneous training of data representation generation and classification. Key contributions include creating a labeled dataset for binary code vulnerability detection using the NDSS18 dataset. The researchers utilized the NDSS18 dataset to extract vulnerable and non-vulnerable functions, developing a tool to detect and fix syntactical errors in source code. After preprocessing, 9,000 functions were fixed and compiled into binaries for Windows and Linux platforms. A total of 32,281 binary functions were obtained, with experiments showing superior performance of MDSAE-R and MDSAE-C variants over baselines in vulnerability detection. The study found that MDSAE-R and MDSAE-C outperformed baselines in all performance measures. MDSAE-C had higher predictive performance than MDSAE-R, confirming the hypothesis of encouraging separation in data representations for better results. The Variational Auto-Encoder (VAE) is a probabilistic auto-encoder that considers both true sample reconstruction and generalization from a latent space. Variational Auto-Encoder (VAE) is developed based on a lower bound formula to maximize log likelihood at each training example. The reparameterization trick is used to reduce variance in Monte Carlo estimation. The optimization problem involves neural networks representing the mean and covariance of the approximate Gaussian posterior. The objective function includes a reconstruction term and a regularization term to compress data. The regularization term in the VAE minimizes the latent codes z for each true example x into those sampled from the prior distribution p(z). The Kullback-Leibler (KL) divergence and L2 Wasserstein (WS) distance are used to measure the difference between two distributions. If p and q are Gaussians, both divergences can be computed in closed forms. The distance computation in machine instructions involves using the Capstone 2 binary disassembly framework to detect and extract core parts containing opcode and instruction information. Opcodes and instruction information are embedded into vectors by building vocabularies and multiplying with corresponding matrices. The process involves constructing a frequency vector of size 256 and multiplying it with the corresponding embedding matrix to encode machine instructions into a latent code that maximizes divergence between vulnerable and non-vulnerable sequences. Inspired by the Variational Auto-Encoder, a probabilistic decoder is used to decode the latent code. The proposed approach involves using a probabilistic decoder to mimic different distributions of data. The decoder aims to learn lower bounds for approximate posterior distributions, encouraging maximal divergence between vulnerable and non-vulnerable latent codes while preserving essential information. The architecture utilizes a parametric form for the decoder, leading to derived lower bounds. The optimization problem involves maximizing the difference between encodings of data in two classes by maximizing the divergence between two priors. Gaussian distributions are used for the priors and an approximate posterior. The optimization problem involves training a classifier C over the latent space either independently or simultaneously with the maximal divergence auto-encoder. The final optimization problem includes classifying data using C \u03c8 (x) to classify x as a vulnerable binary code (y = 1), with \u03b1, \u03b2 > 0 as trade-off parameters. The conditional distributions p \u03b8 (x i | h i\u22121 , z) only consider the opcode of the data. The reconstruction phase focuses on reconstructing opcodes of machine instructions in binary code. A labeled binary dataset was created for vulnerability detection by extracting and compiling functions from a source code dataset. A tool was developed to parse semantic and syntactical relationships in source code using Joern 3. Our tool utilized the compiler gcc/g++ to compile source code, captured and parsed error messages, and fixed them using Joern to understand semantic and syntactical relationships. This process was repeated until the source code was error-free. The compiled functions yielded 32,281 binary functions, with 17,977 for Windows and 14,304 for Linux. Our tool fixed tens of thousands of errors associated with specific source code to create this binary dataset. Our proposed methods MDSAE-R and MDSAE-C were compared with baselines such as RNN-R, RNN-C, and Para2Vec for learning representations and classifying vulnerable and non-vulnerable functions. Language modeling was used for unsupervised representation learning. The similarity model proposed in BID12 embeds paragraphs into a vector space using a fixed vocabulary. SeqVAE-C uses two priors to enhance separable representations, while VulDeePecker employs a BRNN for vulnerability detection in source code. The text discusses the implementation of four variants of a proposed method for analyzing machine instructions using a dynamic RNN. Data was split into training, validation, and testing sets, with specific parameters set for the model optimization. The Adam optimizer was used with a minibatch size of 64 and 100 epochs. The proposed method was implemented in Python using Tensorflow BID0. Experiments were conducted on Windows and Linux binaries, showing superior performance compared to baselines in all measures. The proposed method implemented in Python using Tensorflow BID0 showed superior performance compared to baselines in all measures on Windows and Linux binaries. Experimental results on the NDSS18 binary dataset demonstrated the effectiveness of the MDSAE-RKL and MDSAE-RWS data representations with a linear classifier. During training, the distributions of latent codes for vulnerable and non-vulnerable classes become more separated, as shown by the increasing MMD distance. The priors also become more distant, leading to compressed latent codes. Reconstruction error consistently decreases. The latent codes of vulnerable and non-vulnerable classes become more separable and distinct after training, as shown by the decreasing reconstruction error and increasing MMD distance. The priors also become more distant, leading to compressed latent codes. In this paper, the Maximal Divergence Sequential Auto-Encoder is proposed for binary vulnerability detection using deep learning representation. A labelled binary software dataset has been created to address the issue of limited labelled public binary datasets. The approach aims to make latent codes representing vulnerable and non-vulnerable binaries maximally different while preserving crucial information. Our developed tool and approach can be reused to create high-quality binary datasets. Extensive experiments were conducted to compare our methods with baselines, showing superior performance in all measures. The process of compiling the VulDeePecker dataset into binaries involves three main stages: collecting, detecting, and compiling source code. Source code is collected from VulDeePecker GitHub, containing two types of vulnerabilities in C/C++ programs. Joern's parser is used to identify function points for recognition. The parser is used to identify function points in the source code. 19,009 non-vulnerable and 12,946 vulnerable functions are detected. Identical functions are removed, resulting in 6,412 non-vulnerable and 6,592 vulnerable functions. An automatic tool based on Joern is developed to detect and fix errors in the code snippets. The automatic tool based on Joern detects and fixes errors in code snippets by preprocessing source code, reformatting it, compiling with gcc/g++, analyzing error messages with Joern's parser, and fixing functions successfully. The automatic tool based on Joern preprocesses source code, compiles with gcc/g++, and analyzes error messages with Joern's parser to fix code errors. An example of an uncompilable function is shown in FIG4, where the gcc/g++ compiler detects an error related to a missing declaration for 'unionFirst' variable at line 23. The error information is then sent to Joern's parser for analysis and solution. The 'unionFirst' variable is declared as a member of the 'CWE761 Free Pointer Not at Start of Buffer wchar t environment 34 unionType' class. If the parser cannot determine the data type of a variable, an error is logged, the function is skipped, and the tool moves to the next function. Errors are detected, fixed, and statistics are generated from the log file to prioritize upgrades. The tool progresses from fixing easier errors to more complex ones, resulting in 8,991 fixes. The automatic tool detects and fixes errors in source code, resulting in 8,991 fixed functions. It can fix up to 22 and 28 general errors for each C and C++ source code respectively. After compiling the fixed functions, there are 4,501 non-vulnerable and 4,490 vulnerable functions. The binaries are disassembled using Capstone software. The Capstone software is used to disassemble binaries into assembly code, revealing a buffer error vulnerability highlighted in the source code. The stages in the VulDeePecker dataset processing are detailed in FIG2, showing the number of vulnerable and non-vulnerable functions at each stage. This research was presented at ICLR 2019."
}