{
    "title": "rkej86VYvB",
    "content": "In reinforcement learning, ensemble algorithms like averaging and majority voting may not always be optimal due to different optimal trajectories learned by each function. A proposed Temporal Difference Weighted (TDW) algorithm adjusts weights based on temporal difference errors, improving ensemble performance by reducing weights of unfamiliar Q-functions. Experimental results on Gridworld and Atari tasks demonstrate significant performance enhancements compared to baseline algorithms. Using ensemble methods with multiple function approximators can often outperform a single function. Ensemble methods in reinforcement learning combine individually trained functions to improve test performance. While research on ensemble algorithms in reinforcement learning is not as extensive as in supervised learning, combining multiple function approximators can lead to better results. Wiering & Van Hasselt (2008) studied ensemble approaches with different agents and value-based algorithms in Gridworld. Fau\u00dfer & Schwenker (2011; 2015a) found that combining neural network approximated value functions improves performance. Weighting each agent's contribution based on accuracy is a known approach in supervised learning. Reinforcement learning agents learn from exploration trajectories, leading to different data for each agent. This is crucial in high-dimensional state-space tasks with multiple optimal trajectories. Simple averaging or majority voting may not always produce the best joint policy function. The paper proposes the temporal difference weighted (TDW) algorithm for reinforcement learning at test time. It prioritizes confident agents in action selection and reduces contributions from agents unfamiliar with the current trajectory. Weights of contributions are calculated based on accumulated TD errors, and actions are determined by weighted average or voting methods. This approach addresses the limitations of simple averaging or majority voting in producing the best joint policy function. The TDW algorithm prioritizes confident agents in action selection by calculating weights based on accumulated TD errors. It retains performance in Gridworld tasks and outperforms baseline algorithms in Atari tasks. In deep reinforcement learning, various methods like Bootstrapped DQN and Averaged-DQN leverage multiple heads or value functions to improve exploration and reduce variance in target estimation. Joint decision making with multiple agents has shown better performance in some tasks, but research has been limited to smaller tasks like Gridworld and Maze. The curr_chunk discusses the use of multiple agents in reinforcement learning, comparing methods like MMRL and module selection mechanisms. Unlike MMRL, the proposed method does not require additional components for weight calculation. The selective ensemble method is also mentioned as a joint decision-making approach. The selective ensemble method proposes eliminating agents with low confidence by measuring TD errors. This method drops outputs with TD errors exceeding a threshold, similar to a hard version of the method using softmax. Setting the threshold requires sensitive tuning due to varying TD error ranges in tasks and reward settings. The standard reinforcement learning setting involves an agent receiving a state, taking an action based on a policy function, and receiving a reward, with the return defined as a discounted cumulative reward. DQN is a deep reinforcement learning method that approximates an optimal Q-function with deep neural networks. It uses experience replay to randomly sample past state transitions. The Q-function is updated to minimize squared temporal difference errors. The target network parameter is synchronized to the main parameter in intervals. The use of experience replay in deep reinforcement learning involves randomly sampling past state transitions to compute the squared TD error. Different trained Q-functions are combined to determine the final policy using methods like Average policy and Majority Voting policy. Majority Voting policy selects actions based on the most valued action, while Average policy averages outputs to reduce prediction variance. The TDW ensemble algorithm adjusts weights based on accumulated TD errors to address complex situations in high-dimensional state-space. It considers two types of errors: prediction error at frequently visited states and error at less visited states. The TDW ensemble algorithm adjusts weights based on accumulated TD errors to address complex situations in high-dimensional state-space. It considers two types of errors: prediction error at frequently visited states and error at less visited states. \u03b4 u will be extremely large at less visited states due to insufficient propagation of TD errors. Unfamiliar states can be caused by hard exploration or suboptimal state transitions. When combining multiple agents, agents optimized at different trajectories may produce larger \u03b4 u. Uncertainty of less confident agents is measured using u i t, with \u03b1 decaying uncertainty at a previous step. The uncertainty u i is adjusted with a large \u03b1 to distinguish confident agents. Weighted ensemble methods are used based on uncertainty u i t, with weights calculated using the softmax function. Two policies, Average and MV, are considered. The TDW ensemble algorithm adjusts weights based on accumulated TD errors to address complex situations in high-dimensional state-space. The TDW Voting policy uses probabilities directly calculated by (7), increasing correlation between agents with a large decay factor \u03b1. Weighted ensemble algorithms can be extended to arbitrary methods, with experiments conducted on Gridworld and Atari tasks using Q-learning and DQN. Performance improvements from baselines were evaluated in each experiment. The algorithm was evaluated for performance improvements and the effects of the decay factor \u03b1. Two Gridworld environments were created to induce bias in learned trajectories. The state-representation is a discrete index of a table with size of 13 \u00d7 13, with four actions available. The agent starts from S and moves up, down, left, or right, with the next state remaining the same if a wall is encountered. The agent starts from state S in a Gridworld environment and receives rewards at goal states. N = 10 agents were trained with -greedy exploration. Evaluation of TDW ensemble algorithms was done after training. Four-slit Gridworld is more challenging than Two-slit Gridworld due to biased Q-functions. Evaluation results are shown in Table 1. The TDW ensemble methods outperform the Average policy and MV policy baselines in both Two-slit and Four-slit Gridworld environments. Larger decay factor \u03b1 improves performance in TDW Average policy but hinders performance in TDW Voting policy, especially in Four-slit Gridworld. The TDW algorithm was also evaluated in Atari tasks to demonstrate effectiveness in high-dimensional state-space. In high-dimensional state-space, the TDW algorithm was evaluated in Atari tasks using DQN agents trained across 6 different games. Training involved 10 agents per game with varied Q-function learning, exploration, and environments. The training lasted for 10M steps with exploration linearly decayed from 1.0 to 0.1. Evaluation included TDW Average policy, TDW Voting policy, and weighted baseline versions over 1000 episodes. The experimental results in Table 2 show that the TDW algorithms improved performance in various Atari games compared to non-weighted and weighted baselines. The globally weighted ensemble baselines performed worse, possibly due to ignoring local performance in high-dimensional state-space. The TDW algorithms with small \u03b1 outperform those with large \u03b1, indicating that a significantly large \u03b1 can reduce contributions of less confident Q-functions. Entropy analysis during a Breakout episode shows extreme low entropies when the ball is close to the pad, indicating well-learned Q-functions estimating terminal states. The large \u03b1 value in TDW algorithms reduces the influence of Q-functions that cannot predict values accurately with unseen variations of remaining blocks. In SpaceInvaders, low entropies occur when dodging beams rather than shooting invaders, as shooting requires long-term value prediction. Performance improvements on SpaceInvaders are not significantly better than weighted baselines due to this. The decay factor \u03b1 affects entropies in games, with higher values leading to more low entropy states. Games with frequent rewards like Enduro and MsPacman have more low-entropy observations. The TDW algorithm with larger \u03b1 values performs worse in MsPacman due to frequent positive rewards inducing prediction errors. Globally weighted ensemble methods outperform TDW algorithms in consistently accumulating uncertainties. The TDW algorithm accumulates temporal difference errors as uncertainties to adjust Q-function weights, improving performance in high-dimensional state-spaces. Performance evaluations in Gridworld and Atari tasks show better results compared to non-weighted and globally weighted algorithms. Challenges arise in games with frequent rewards due to accumulated prediction errors. Future work includes extending the algorithm to continuous action-space tasks. In this paper, Q-functions are discussed along with extending ensemble methods for Deep Deterministic Policy Gradients. An algorithm is proposed to measure uncertainties without reward information, which is often unavailable in real-world applications. Q-function tables are obtained on Gridworlds."
}