{
    "title": "HJeB0sC9Fm",
    "content": "We introduce a new concept of 'non-linearity' in network layers based on proximity to a linear system, determined by the non-negative rank of the activation matrix, measured through non-negative factorization. Our approach identifies high non-linearity in deep layers as a sign of memorization in neural networks. Experiments on various datasets show that our technique can be used for early stopping to prevent overfitting. The challenge of balancing bias-variance tradeoff in machine learning is addressed, questioning the assumption that deep neural networks always memorize training data despite having the capacity to do so. Neural networks, particularly those using rectified-linear units (ReLU), do not memorize training data despite their capacity to do so. The non-linearity of a ReLU layer with respect to input batches is a key factor in generalization. Networks that generalize well exhibit deep layers that are approximately linear with similar inputs, while those that memorize training data show high non-linearity. This distinction is based on the threshold at zero in ReLU networks. The non-linearity in ReLU networks is mainly due to the threshold at zero, which determines the support of the activation matrix. The non-negative rank of a matrix is indicative of the non-linearity in a ReLU activation matrix. Estimating the 'non-linearity' of a ReLU layer with NMF on a grid over the approximation rank can measure the impact on network performance and its robustness to compression. This approach is compared to other methods. Our NMF-based approach is compared to PCA and random ablations for dimensionality reduction in neural networks. Memorization is defined as learning a rule linking a sample to a label without improving performance on new data. The approach shows sensitivity to memorization in various neural network architectures across image and audio datasets. Layer-by-layer analysis reveals insights into memorization mechanisms. A proposed measure is used for early stopping to indicate memorization. The study of factors in the bias-variance tradeoff in learning models dates back several decades. Classical results in statistical learning focus on properties like VC-dimension and Rademacher complexity. However, these properties become irrelevant when considering the vast capacity of deep neural networks. Recent analyses have shown that the number of weights in a network is less important than their scalar value. Methods have been developed to explain the generalization ability of deep neural networks. Recent analyses have shown that memorizing networks contain more information in their weights, and methods have been proposed to explain generalization by examining a network's robustness to perturbations. The notion of flatness of minima on the loss surface is crucial, but reversible transformations like scaling can manipulate local flatness without affecting generalization. A new procedure applies perturbations to activations, addressing concerns about robustness to rescaling and similar transformations. Methods have been developed to account for the role of data distribution in model training. BID23 proposed using the Fisher-Rao norm to weigh the contribution of model parameters based on data geometry. Empirical studies by BID24 and BID27 explore robustness to noise, with BID1 deriving generalization bounds in terms of noise robustness. Experimental setups similar to BID24 involve compressing hidden activations and testing for robustness to noise. In experiments, robustness to NMF compression is more correlated with low memorization/high generalization than robustness to random ablations. NMF is a popular dimensionality reduction technique producing meaningful factorizations for document clustering, audio source separation, and face recognition. In deep convolutional neural networks, NMF applied to activations of an image classifier yields significant results. NMF applied to activations of an image classifier provides a decomposition into semantic parts, benefiting from transformation invariance learned by the neural network. The ReLU layer of a neural network processes inputs by sampling a sub-network that is linear with respect to the input. This can be achieved by setting columns of the weight matrix to zero based on the input's dot product, and then removing the thresholding. The ReLU layer of a neural network can be considered linear with respect to its input batch when the samples are close enough to share the same ReLU mask. By zeroing out columns based on the mask, the layer can be transformed into a linear system. The support matrix M, derived from the activation matrix A, indicates the linearity of the layer. If all rows of M are identical to a vector m, the layer is deemed completely linear with respect to its input X. The ReLU layer of a neural network is considered linear with respect to its input batch when the support matrix M is close to a unique vector m. The rectangle cover number of a matrix, rc(M), measures the linearity of the layer, with higher values indicating increasing non-linearity. This measure is important in the study of communication complexity and serves as a complexity measure for ReLU activations. The rectangle cover number rc(M) is crucial for measuring the non-linearity of a neural network's ReLU layer. Approximations and bounds have been studied for rc(M), with the non-negative rank of a matrix serving as an upper-bound. Non-negative matrix factorization can help compute the non-negative rank, providing a constraint for solving the matrix factorization problem. Non-negative matrix factorization (NMF) is used to estimate the 'linearity' of a ReLU layer by solving for U + , V + as defined above. By performing NMF on a grid over the non-negative rank k, the impact on network performance can be measured by observing the change in prediction as k varies. This approach addresses the noise in network activations and replaces feature activations with their rank k NMF approximations during the forward pass. Non-negative matrix factorization (NMF) is used to estimate the 'linearity' of a ReLU layer by flattening feature maps and reshaping them for forward propagation through the network. The goal is to cluster input samples with similar output labels while separating them from others. Deep layers are expected to be approximately linear within the same class, allowing for simpler support in classification tasks. NMF compression is applied to analyze each layer's linearity. In deep layers, networks with high memorization are less robust to NMF compression, indicating higher non-linearity. Networks with different label randomization behaviors show distinct phases of feature extraction, memorization, and clustering. The process shifts to earlier layers when labels are fully randomized. In deep layers, networks with high memorization are less robust to compression, indicating higher non-linearity. The level of induced memorization is controlled by setting a probability for label randomization. The capacity of these networks is large, with training accuracy at 1 in all cases. Batches of training data show accuracy drop as compression level increases. In experiments, accuracy drops as compression level increases. Batches are sampled stochastically and robust to batch size. Various image datasets are used for evaluations with different network architectures. In experiments, accuracy drops as compression level increases for neural networks trained with increasing levels of label randomization. Networks with fully randomized labels behave differently than those with partial or no randomization. Deep layers show less robustness to compression, indicating high non-linearity with respect to the input. In experiments, accuracy drops as compression level increases for neural networks trained with increasing levels of label randomization. Networks with p < 1 display a feed-forward trend up until layer conv3_1, indicating a generic feature extraction phase common to all. Differences between networks emerge in the phase until conv4_2, with higher memorization correlated with lower AuC. Memorization is localized to these layers, while samples of the same class cluster in conv4_3 before the final classification layer. Compression techniques are compared to detect memorization in neural networks trained with label randomization. Different methods such as NMF under the Frobenius norm, PCA, and random ablations are considered. Compression is applied sequentially to several layers, targeting the final convolutional blocks. In the study, compression techniques are applied to detect memorization in neural networks trained with label randomization. NMF compression shows sensitivity to memorization due to non-negative rank properties, while PCA is more efficient at compressing activations but less discriminative. Robustness to random ablations correlates with less memorization, with NMF showing more variance and higher computational cost. Additional results for single-class NMF on different datasets and networks are also presented. In FIG3, additional results for single-class NMF on various datasets and network architectures are shown. Results indicate that multi-class batches produce activations with higher rank compared to single-class batches, as the network separates samples of different labels. The technique is useful for predicting generalization without artificial noise, demonstrated through experiments on CIFAR-10 and a pre-trained VGG-19 network on ImageNet classes. In this study, classifiers on CIFAR-10 were evaluated over a grid of hyper-parameter values for batch size, weight decay, and optimization algorithm. NMF, PCA, and random ablations were compared based on their correlation with generalization error, with NMF showing the highest correlation. The study aimed to detect memorization during training to enable early stopping. CNNs were trained on CIFAR-10 with original labels, and test set error was recorded after training for 10K batches with a batch size of 100. After training CNNs on CIFAR-10 with original labels, the test set error was recorded every 250 batches. The non-linearity of the deepest three convolutional layers was tested using an NMF-based approach. Results comparing test loss, NMF method, and random ablations are shown in Figure 6. The test loss minima align closely with the maximum NMF area under the curve (AuC). In Figure 6 (c), the stopping times of NMF and random ablations are compared against the best test loss over 10 runs. A notion of a ReLU layer's non-linearity is introduced based on its proximity to a linear system, measured indirectly via NMF on deep activations. The approach successfully detects memorization and generalization in various neural network architectures and datasets. The impact of ablating activations in directions found by NMF and PCA is studied by forward propagating the residual. The directions found by NMF and PCA are important for capturing variance in activation matrices. Networks without induced memorization are vulnerable to ablation of these directions, unlike random ablations. The VGG-19 model, known for generalization ability, is used as an example. In this study, NMF compression is applied to the three deepest convolutional layers of a pre-trained model to analyze its generalizing ability. The denoising effect of NMF is observed on single-class batches, improving accuracy over the baseline. However, for multi-class batches, baseline accuracy is only regained with a large k value due to the critical role of non-linearity in class separation. Ablating NMF directions significantly reduces classification performance. In this study, NMF compression is used on convolutional layers to analyze generalizing ability. Ablating NMF directions reduces classification accuracy. There is a correlation between NMF AuC and test accuracy on ImageNet batches. NMF-based early stopping improves neural network accuracy in few-shot learning scenarios. Training set accuracy is shown in Figure 6. NMF compression is used for analyzing generalizing ability in convolutional layers. Early stopping based on NMF k = 1 compression improves accuracy in few-shot learning scenarios. Training set accuracy helps predict test set accuracy gradients, leading to improved accuracy by stopping at the first peak. The simple heuristic of stopping at the first peak in NMF compression improves accuracy in few-shot learning scenarios. By observing training set accuracy under NMF k = 1 compression, the gradient of test set accuracy can be predicted. Early stopping consistently improves accuracy and is comparable to a recently proposed method for few-shot learning. Additionally, NMF reconstruction is a point of interest, but interpreting the NMF error is challenging due to scale. The NMF error is challenging to interpret due to scale, with variations across networks, layers, and channels. A transition between regimes occurs around p = 0.9, affecting memorization levels and depth. Accuracy measurement reveals a phase shift at p = 0.9, where networks shift memorization to earlier layers. NMF compression incurs overhead but can improve accuracy in few-shot learning scenarios. Our implementation of the multiplicative update algorithm BID21 for NMF compression of large matrices runs efficiently with GPU acceleration. Timing results for a typical batch used for VGG-19 show that processing to convergence at k = 500 takes 197 milliseconds on average. The final runtime depends on the number of classes sampled and the granularity of the grid over k. Measurements are robust to heavy subsampling."
}