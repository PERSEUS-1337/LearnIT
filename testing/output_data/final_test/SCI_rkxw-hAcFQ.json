{
    "title": "rkxw-hAcFQ",
    "content": "We present a hierarchical framework for training sequential generative models to capture coordinated multi-agent trajectory behavior in offensive basketball gameplay. The framework utilizes intermediate variables to capture long-term coordination and high-level behavioral semantics in an interpretable way. Inspired by leveraging weak labels, we extend the approach to the spatiotemporal regime and demonstrate its effectiveness in modeling complex interactions between basketball players and generating realistic multi-agent trajectories over long time periods. The study presents a framework for training sequential generative models to capture multi-agent trajectory behavior in basketball gameplay. It utilizes weak labels and intermediate variables to model complex interactions between players and generate realistic trajectories over long time periods. The approach is validated through quantitative and qualitative evaluations, including a comparison with professional sports analysts. The framework aims to address the challenge of developing sequential generative models leveraging tracking data to capture the behavior of multiple cooperating agents in various domains. When modeling animal schooling behavior, agents can be friendly or unfriendly, with highly coordinated and non-deterministic behavior. Designing hierarchical models with intermediate variables can capture long-term coordination and high-level behavioral semantics, such as in basketball settings. Conventional approaches focus on learning interpretable intermediate variables in unsupervised ways, but struggle with complex sequential settings. The hierarchical framework presented addresses the challenge of learning complex sequential generative models using programmatic weak supervision. It utilizes labeling functions to produce weak labels for supervised learning of interpretable intermediate representations, inspired by data programming. This approach extends to the spatiotemporal regime and is compatible with existing deep generative models. Our approach focuses on generating high-quality trajectories and encoding long-term coordination between multiple agents in multi-agent tracking data. We also apply our method to modeling team offense in basketball, showcasing significant improvements over standard baselines through quantitative and qualitative validation, including a user study comparison with professional sports analysts. Incorporating stochastics into deep models can be done through explicit distribution modeling or using deep neural nets for transformation. The framework discussed can handle both approaches and is inspired by probabilistic modeling with additional structure or side information. This includes enforcing logic constraints, generative models as programs, and weak supervision via data programming. Our framework extends weak supervision via data programming to the spatiotemporal regime and is related to imitation learning, specifically in multi-agent and stochastic policies. Previous work has not focused on learning generative policies while addressing generative and multi-agent imitation learning. Experiments show highly peaked distributions in some cases, while others capture multimodal distributions by learning unimodal policies for a fixed number of experts. Learning stochastic multi-agent behavior has been addressed with significant feature engineering. Sequential generative modeling aims to learn the distribution over sequential data by factorizing the joint distribution and maximizing the log-likelihood using parameters like a recurrent neural network (RNN). However, RNNs with simple output distributions struggle to capture highly variable and structured sequential data, such as multimodal behavior. Recent work in sequential generative models addresses this issue. Recent work in sequential generative models addresses the issue by injecting stochastic latent variables into the model and optimizing using amortized variational inference. A variational RNN (VRNN) is used as the base model, which is essentially a variational autoencoder conditioned on the hidden state of an RNN. The model is trained by maximizing the evidence lower-bound (ELBO) and can be applied to sequences of trajectories of coordinating agents. In the context of sequential generative models, a hierarchical structure of macro-intents is introduced to effectively learn representations of multi-agent trajectories. This approach aims to address the difficulty in generalizing well over long time horizons and capturing coordination in the data. In the context of sequential generative models, a hierarchical structure of macro-intents is introduced to capture coordination between agents and enable long-term planning. This approach aims to learn low-dimensional representations of multi-agent trajectories that extend in time and space. In the context of sequential generative models, a hierarchical structure of macro-intents is introduced to capture coordination between agents and enable long-term planning. Macro-intents provide a compact summary of players' trajectories over time, encoding long-term intent for cohesive agent behavior. These macro-intents do not require a geometric interpretation and can be used to model various dependencies between agents. Our work is inspired by recent advances in weak supervision settings known as data programming, where labeling functions can be used to learn the underlying structure of large unlabeled datasets. These functions compute heuristics based on domain knowledge to quickly label data, such as identifying regions on a basketball court where players remain stationary to set up specific formations. This approach allows for the incorporation of macro-intents in a black box learning framework, enabling the modeling of dependencies between agents for long-term planning and cohesive behavior. Our work is inspired by data programming, where labeling functions quickly label data. We use an RNN-model to sample macro-intents and induce coordination between agent trajectories. The agent-models are learned by maximizing the VRNN objective. Our approach involves learning agent-models by maximizing the VRNN objective and independently learning the macro-intent model via supervised learning. The macro-intent g t is shared across agents, and any generative model can be used in our framework. We apply our approach to generating offensive team basketball gameplay and a synthetic Boids model dataset, presenting quantitative and qualitative experimental results. Professional sports analysts preferred basketball rollouts generated from our approach in a user study comparison. Our approach involves learning agent-models by maximizing the VRNN objective and independently learning the macro-intent model via supervised learning. The macro-intent g t is shared across agents, and any generative model can be used in our framework. We apply our approach to generating offensive team basketball gameplay and a synthetic Boids model dataset, presenting quantitative and qualitative experimental results. Professional sports analysts preferred basketball rollouts generated from our approach in a user study comparison. The user study and videos of generated rollouts can be seen in our demo video. Our qualitative results demonstrate the ability of our approach to generate high-quality rollouts under various conditions. Training data consists of trajectories of K = 5 players on the left half-court, recorded for T = 50 timesteps at 6 Hz. The offensive team has possession of the ball for the entire sequence. Players are ordered based on their relative positions, focusing on capturing the coordination and multimodality of the offensive team. The curr_chunk discusses different models used for updating defensive positions in basketball, including RNN-gauss, VRNN-single, VRNN-indep, VRNN-mixed, and VRAE-mi. These models utilize various architectures and latent variables to improve defensive strategies. The interactive demo of the hierarchical model can be accessed at http://basketball-ai.com/. The curr_chunk discusses the VRAE-mi architecture for maximizing mutual information between x \u2264T and macro-intent. It outperforms RNN-gauss in log-likelihoods on test data. A human preference study with sports analysts judges the quality of rollouts from the model compared to baselines. The preference study results show that judges preferred the model over baselines with 98% statistical significance, indicating higher quality rollouts. Domain statistics for basketball trajectories were computed, showing that trajectories from the model matched closest to ground-truth with weak supervision. See appendix D for labeling function pseudocode. The model generates macro-intents and trajectories that closely match ground-truth with weak supervision. Macro-intents induce coordination between players, showing significantly more realistic behavior than baselines. Different labeling functions like LF-window25 and LF-window50 are also assessed for quality. Our models using programmatic weak supervision match closer to the ground-truth with more informative labeling functions. A qualitative visual inspection of rollouts shows common problems in baseline rollouts, such as players moving out of bounds or in the wrong direction. These issues tend to occur at later timesteps, suggesting that the baselines do not perform well over long periods. The baselines do not perform well over long horizons due to compounding errors, while rollouts from our model are more robust. Generated macro-intents allow for interpreting individual player intent and global team strategy. Our model learns a multimodal generating distribution, resulting in a dynamic range of trajectories. Grounding macro-intents during generation allows for better control. Our hierarchical model captures distinct modes for friendly and unfriendly behavior, inducing coordination between players for realistic movement. The macro-intents encode player coordination, resulting in cohesive player behavior. The model is applied to a simplified Boids model, producing realistic schooling behavior trajectories. The Boids model BID31 generates realistic trajectories of schooling behavior for 8 agents over 50 frames. Agents exhibit friendly (c1 > 0) or unfriendly (c1 < 0) behaviors, with velocities updated at each timestep. Stochasticity is introduced by randomly updating \u03b2. The labeling function determines behavior based on average distance to closest neighbor. The hierarchical model outperforms baselines in capturing the true generating distribution of trajectories. It distinguishes between friendly and unfriendly behaviors better than other models. All models sample from a multivariate Gaussian distribution, with our model showing better performance in capturing distinct modes in the ground-truth data. The models learned to assign all weight on a single component, ignoring others, with small variance. Using a VRNN for macro-intents was considered, but an RNN sufficed in capturing the distribution of macro-intents. Latent variables in a VRNN had minimal effect on the distribution. Maximizing mutual information was not effective in learning macro-intents. The fully unsupervised VRAE-mi model ignores learned macro-intents, resulting in a lack of diversity in rollouts during generation. Future directions include exploring more complex macro-intents and adapting the method to different domains such as dialogue or music generation. The text discusses the use of macro-intents in music generation for coordinating instruments. It explores the use of recurrent neural networks (RNNs) with multiple labeling functions to improve the generation of high-quality coordinated multi-agent trajectories. RNNs with simple output distributions struggle with capturing highly variable sequential data, but recent work addresses this by incorporating stochastic latent variables into the model. Variational Autoencoders (VAE) address the issue of capturing highly variable sequential data by injecting latent variables into the joint distribution and using amortized variational inference to infer them from data. The learning objective is to maximize the evidence lower-bound (ELBO) of the log-likelihood with respect to the model parameters. The reconstruction term is approximated with Monte Carlo sampling, while the Kullback-Leibler divergence between the approximate posterior and the prior can be evaluated analytically. Variational RNNs (VRNNs) combine VAEs and RNNs by conditioning the VAE on a hidden state. They are trained by maximizing the ELBO and can model complex sequential data like speech and handwriting. VRNNs generate training and test trajectories for agents' velocity updates based on cohesion, separation, and alignment vectors. In a fully unsupervised manner, experiments were conducted to learn macro-intents by maximizing mutual information between macro-intent variables and trajectories. A VRAE-style model encodes trajectories into a latent macro-intent variable z to capture global properties of the sequence. The mutual information between z and trajectories is approximated using a discriminator, leading to a variational lower-bound. The study focuses on maximizing mutual information between macro-intent variables and trajectories using a VRAE-style model. Different approaches are explored, including training categorical and real-valued macro-intent variables. Macro-intents in basketball are defined by segmenting the court into grids. Algorithms are described for computing macro-intents based on positions in time windows or stationary positions. The study explores maximizing mutual information between macro-intent variables and trajectories using a VRAE-style model. Macro-intents in basketball are defined by segmenting the court into grids. Algorithm 2 describes LF-stationary, which computes macro-intents based on stationary positions."
}