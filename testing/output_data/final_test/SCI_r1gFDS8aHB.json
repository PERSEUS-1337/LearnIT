{
    "title": "r1gFDS8aHB",
    "content": "Recent advances in deep learning have highlighted the effectiveness of deep neural networks in extracting task-specific features. However, these features are limited to the initial task and do not capture general, task-agnostic features. Humans learn by disentangling task-agnostic features, which can be achieved by using Variational Auto-Encoders (VAEs) to capture latent variables in a generative manner. VAEs can represent latent features as a mixture of continuous and discrete variables for the latent space. By using a modified version of joint-vae, we learn disentangled features in our experiments. Feature learning is crucial in machine learning, with deep learning making significant advancements in this area. Deep neural networks excel at extracting features from raw data, but these features are often task-specific due to the use of specific loss functions tailored to the task at hand. To achieve true artificial intelligence, it is necessary to learn representations in a task-agnostic manner. Recent efforts have focused on learning disentangled representations to capture all necessary information of an entity. Recent efforts in machine learning have been towards learning disentangled representations, where a change in a single unit of the representation corresponds to a change in a single factor. In this work, JointVAE is used to explore disentangled representations for a given dataset. Various state-of-the-art variants of VAEs have been reported to extract disentangled representations, assuming latent variables follow a Gaussian distribution for simplifying sampling using reparametrization trick. In Joint-VAE Dupont (2018), a mixture of continuous and discrete latent variables is used to represent disentangled features. Continuous variables are assumed to be Gaussian distributed, while discrete variables are assumed to be multinomial distributed. Sampling from continuous variables is done using the normal reparameterization trick, while sampling from discrete variables is done using the Gumbel Softmax trick. This allows for the representation of disentangled features using both continuous and discrete variables. The Joint-VAE approach utilizes a mixture of continuous and discrete latent variables to represent disentangled features without assumptions, combining the benefits of both types of variables."
}