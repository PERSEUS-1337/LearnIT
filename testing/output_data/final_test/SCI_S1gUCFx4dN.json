{
    "title": "S1gUCFx4dN",
    "content": "In recent years, significant progress has been made in designing generative models, but current approaches struggle to capture complex global structures in data, such as spatial patterns in images. To address this, we propose incorporating programs representing global structure into generative models and using program synthesis to generate training data. Our approach outperforms state-of-the-art methods in generating and completing images with global structure. Generative models like VAEs and GANs have been successful in tasks such as image generation, completion, texture synthesis, sketch generation, and music generation. However, they struggle with capturing global structure, as seen in image completion tasks. Real-world data often contains global structures like repetitions or symmetry. Program synthesis Solar-BID17 has emerged as a promising approach to capturing patterns in data, including global structures like repetitions or symmetries. Existing approaches have synthesized programs that operate directly over raw data, limiting their ability to handle complex perceptual data. Our proposal aims to address these limitations by synthesizing programs that can represent more complex structures. Our proposal aims to address limitations in program synthesis by synthesizing programs that represent the underlying structure of high-dimensional data. Programs are decomposed into a sketch representing the skeletal structure with holes, filled by perceptual components using raw data. Incorporating programmatic structure into generative models improves completion performance compared to existing baselines. The program-synthesis (guided) generative models (PS-GM) combine neurosymbolic programs with deep generative models to improve completion quality. PS-GM outperforms baselines by incorporating programmatic structure into the completion process. PS-GM combines neurosymbolic programs with deep generative models to enhance image completion quality. The generation process involves creating a program representing the global structure, executing it to obtain a structure rendering, and using a deep generative model to complete the image while preserving the global structure. The PS-GM approach combines neurosymbolic programs with deep generative models to improve image completion quality by leveraging domain-specific program synthesis algorithms to represent global structure in training data. This allows for the capture of rich spatial structures in images. The proposed architecture combines neurosymbolic programs with deep generative models to capture rich spatial structures in images. It encodes repeating structures of 0's, 2's, and 3's in the image, and generates images with programmatic structure. The model incorporates an algorithm for training these models and evaluates the approach on synthetic data. The approach combines neurosymbolic programs with deep generative models to capture spatial structures in images, outperforming state-of-the-art models. There is a growing interest in using program synthesis for interpretability, safety, and lifelong learning in machine learning. Previous work has focused on capturing structure that deep learning models struggle with, such as repeating patterns in simple line drawings and 2D shapes. The approach combines neurosymbolic programs with deep generative models to capture spatial structures in images, outperforming state-of-the-art models. Previous work has focused on capturing structure that deep learning models struggle with, such as repeating patterns in simple line drawings and 2D shapes. BID5 captures 2D repeating patterns of simple circles and polygons by learning a simple mapping from images to symbols. Other related work includes synthesizing programs with neural components for lifelong learning and generative models that require the structure to be available for training and generating new images. The proposed architecture combines neurosymbolic programs with deep generative models to capture spatial structures in images, surpassing existing models. It focuses on generating images with diverse repeating patterns using a distribution over a space X with unknown parameters \u03b8, \u03c6. The generation pipeline is illustrated in FIG0, with a latent structure consisting of a program sketch s \u2208 S and a perceptual component c \u2208 C. The technique adapts these methods for image completion as well. The image completion pipeline involves synthesizing a program sketch representing the structure of a partial image, extrapolating it to the whole image, rendering the structure, and completing the image to resemble the original. The model executes the program to obtain the structure rendering and then completes it into an image conditioned on a latent vector. Our model samples completions based on a structure rendering and learns parameters using a variational autoencoder. Optimization is done through a lower bound on log-likelihood, integrating over program sketches and perceptual information. Our approach proposes using a single point estimate for program sketches and perceptual components instead of sampling, making the process more computationally efficient. This involves synthesizing structure using program synthesis tailored to a specific domain, such as inferring for-loop structure in images. By replacing integrals with point estimates, the model simplifies the computation, leading to a decomposition into two parts. The text discusses optimizing parameters in a VAE model by decomposing it into two parts. It mentions using a sequence-to-sequence VAE for images with for-loop structure and learning a probability distribution using another VAE. The focus is on learning the distribution over latent structure and leveraging the program for prediction. The text discusses leveraging program structure to predict x more directly by first executing the program to obtain a structure rendering. This rendering is then used as input to a neural network model, such as a VAE with convolutional layers for images. Non-probabilistic approaches are also considered to predict x from the structure rendering, including using state-of-the-art image completion models like GLCIC and CycleGAN. The text discusses modifying image completion models like GLCIC and CycleGAN to predict complete images from partial images by incorporating programmatic structure information. The program synthesis algorithm is used to generate global structures for the images, enabling better completion predictions. The text discusses training a model to predict the global structure of an image from a partial image, using program synthesis algorithms. This approach improves image completion predictions by incorporating programmatic structure information. The text discusses synthesizing a program to predict the global structure of an image from a partial image by dividing the image into a grid and considering programs that draw 2D repeating patterns of sub-images on the grid. The programs consist of pairs of sketches and perceptual components. The text discusses program synthesis to predict the global structure of an image by dividing it into a grid and using sketches with perceptual components to fill in the grid. The program outputs parameters for each sketch along with a perceptual component to create a complete image. The text discusses program synthesis for predicting the global structure of an image using sketches with perceptual components to fill in a grid. The program outputs parameters for each sketch and a perceptual component to create a complete image. The text discusses program synthesis for predicting the global structure of an image using sketches with perceptual components to fill in a grid. It defines DISPLAYFORM3 as the set of grid cells where a sketch renders a sub-image. DISPLAYFORM4 indicates whether the sketch renders a sub-image at specific grid cells. DISPLAYFORM5 defines the disjunction of boolean tensors elementwise. The program synthesis algorithm aims to optimize the number of true positives and false negatives to represent the global structure of the image accurately. The program synthesis algorithm aims to optimize the representation of the global structure of an image using sketches with perceptual components. It uses a partially greedy heuristic due to the NP-complete nature of the problem. The search space is restricted to sub-images that occur in the training image for tractability. This simplifying assumption still produces viable results in practice. The algorithm optimizes image representation using sketches with perceptual components. It selects pairs from sets S and \u0108 to increase the objective. Performance comparison with baseline models shows significant improvement in image completion tasks using a synthetic dataset based on MNIST. The program structure is a 2D repeating pattern of MNIST digits with natural noise added in each iteration. The dataset contains 10,000 training and 500 test images. Another dataset consists of 1855 images of building facades divided into a grid of cells. The approach PS-GM is evaluated on generating images with repeating patterns of objects like windows and doors. A LSTM architecture is used for the encoder and a feedforward architecture for the decoder. Programs representing training images are synthesized using Algorithm 1. A variational encoder-decoder architecture is applied to complete the structure rendering into an image, aiming to minimize reconstruction error. The VAE architecture consists of an encoder and decoder that map structured images to latent vectors and vice versa. Training is done using a typical VAE loss approach. Additionally, a Cycle-GAN model was trained to map renderings of programs to complete images. The PS-GM approach is capable of generating structured images, with examples shown in Figure 4. The study compares the performance of a VAE-based approach to a baseline VAE in generating structured images. Results show that the PS-CycleGAN images have clearer structures compared to the PS-VAE. Performance metrics such as variational lower bound and Fr\u00e9chet inception distance are used to evaluate the models. Examples of generated images are provided in Figure 4, and the results are summarized in Table 1. Inception distance and BID8 metrics show quantitative improvement of models based on the approach over baselines. Images generated using VED completion exhibit more structure compared to baseline VAE. CycleGAN captures complex structures better than SpatialGAN. Evaluation of PS-GM for image completion on synthetic and facades dataset using GLCIC, CycleGAN, and VED models. CycleGAN outperforms for the task. The text discusses the performance of a transformer model called PS-GM compared to a baseline model for image completion. PS-GM synthesizes a program to complete partial images, while the baseline model directly completes partial images. The models are trained using different completion techniques like GLCIC, CycleGAN, or VED. PS-GM outperforms the baseline model in generating images with more structure. Results show that PS-GM outperforms the baseline models in image completion, except for the VED on the facades dataset. The small size of the facades dataset is a key reason for poor performance of the baselines. PS-GM significantly outperforms baselines even on a synthetic dataset. Generative models like GLCIC show promising results in completing images. Our approach incorporates programmatic structure into deep learning models to improve generation of high-dimensional data with global structure. We have demonstrated its promise in enhancing image completion models like GLCIC. Future work includes exploring learning latent program structure during training. Future work will explore more expressive programmatic structures, including if-then-else statements. Experimental details involve sampling a random image using a 9x9 grid with 16x16 pixel cells and creating correlations between different parts of the program by sampling perceptual components. The PS-GM architecture involves generating a program P using a 3-layer LSTM encoder and a feedforward decoder. During execution, noise is introduced by sampling another MNIST image with the same label and rendering it in the same color as the original image. The architecture involves generating a program using LSTM encoder and feedforward decoder. Image compression is done with a convolutional architecture. PS-GM uses VED with CNN layers. CycleGAN model includes a discriminator and generator with transfer learning from ResNet. Baseline architecture is a vanilla VAE with similar architecture as VED. The PS-GM architecture involves using CycleGAN with the same architecture as PS-GM/GLCIC, Spatial GAN with 5 layers, and 60-dimensional global latent vectors. The completion stages include a feedforward network with three layers and a VAE with a CNN encoder and transpose CNN decoder. CycleGAN model has a discriminator with 3 convolutional layers and a generator using transfer learning from ResNet. Baseline VAE architecture is similar to the PS-GM completion step. The image completion pipeline utilized a similar architecture to the PS-GM completion step, with 4 convolutional and 6 deconvolutional layers. The input was a partial image instead of a structured image. The CycleGAN architecture was also similar to PS-GM, mapping partial images to full images. Examples of the pipeline applied to the facades dataset and a synthetic dataset are shown in Figures 6 and 7."
}