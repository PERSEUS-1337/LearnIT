{
    "title": "H1gwRx5T6Q",
    "content": "Locality sensitive hashing schemes like \\simhash provide compact representations of multisets for estimating similarity. To handle dynamically changing sets, we need a homomorphic representation for direct computation of unions and differences. Two representations for cosine similarity and progress on a third for Jaccard similarity are proposed. These hashes are used to compress the sufficient statistics of a conditional random field coreference model, impacting similarity computation during entity split and merge inferences. The \\simhash scheme is analyzed statistically to support its use in a conditional random field (CRF) hierarchical coreference model. It is shown that bias and variance decrease with the number of bits. The scheme allows for scaling the coreference algorithm significantly without compromising performance, especially with 128 or 256 bits. Angle-preserving random projections can further enhance coreference quality with fewer dimensions. The paper discusses coreference resolution in natural language data, focusing on sparse, high-dimensional feature representations. Inference involves estimating probabilities based on similarities between entities' mentions. The \\simhash scheme supports a hierarchical coreference model, showing improved performance with 128 or 256 bits and angle-preserving random projections. In order to address computational challenges with sparse, high-dimensional feature representations in coreference resolution, the proposal suggests using homomorphic compression to compute compressed representations of intermediate inference results directly from their operands. This approach aims to improve efficiency in updating feature vectors and making decisions about coreference between entities. In this paper, the proposal suggests using homomorphic compression to compute inference results directly from data operands, enabling efficient updating of feature vectors for coreference resolution. Several schemes are considered, including a homomorphic cosine-preserving hashing scheme, linear norm-preserving random projections for cosine similarity computation, and a homomorphic version of minhash for Jaccard similarity support. The paper proposes using homomorphic compression for efficient coreference resolution. Different schemes are considered, including a homomorphic version of minhash for Jaccard similarity support. The proposed simhash scheme improves probabilistic inference speed significantly without affecting model quality. Random projection representation can also enhance performance. The paper suggests using homomorphic compression for coreference resolution, including a homomorphic minhash for Jaccard similarity. Simhash improves inference speed without compromising quality. Random projection representation can provide better cosine estimates than simhash. Coreference resolution is crucial for tasks like record linkage and knowledge base construction. Coreference resolution is essential for tasks like record linkage and knowledge base construction. Machine learning can help determine if two mentions of the same entity are related by analyzing contextual features such as words in the title, topic, journal, and co-authors. Coreference resolution involves extracting contextual features like title, co-author list, venue, year, and author-name to use in a probabilistic model. These features include raw words, character-ngrams, and normalized variants with weights indicating importance. A coreference model measures similarities between mentions using functions like cosine-similarity. This type of coreference resolution is suited for similarity-based models and allows entities to grow unbounded in size, emphasizing the need for compact representations of their feature sets. Coreference resolution involves using contextual features in a probabilistic model, such as a discriminative conditional random field (CRF), to assign mentions to entities. Local search procedures like greedy-agglomerative clustering or Markov-chain Monte Carlo (MCMC) are used to find the most likely assignment. Pairwise models measure compatibility between mentions in the same cluster, but can lead to quadratic scaling issues. Entity-based models treat entities as first-class variables with their own features to avoid this problem. Entity-based models like hierarchical coreference organize mentions into latent tree structures, with mentions at the leaves and subentities as intermediate nodes. Potential functions measure compatibility between child and parent nodes in the tree, contributing to the overall score of assigning mentions to latent trees. This model won a challenge for disambiguating inventor names for the USPTO due to its accuracy and scalability. The compatibility scores in entity-based models for hierarchical coreference are parametrized cosine functions. Each mention is associated with feature variables, referred to as \"bags,\" capturing different types of features like author names and paper titles. Parent nodes in the tree also have bags determined by their children's assignments, with a parent's bag being the sum of its children's bags. The bag representations in hierarchical coreference models must be homomorphic for efficiency. Compatibility between bags is calculated using cosine distance. Potentials measure compatibility between child and parent bags, with weights and translation parameters determining coreference decisions. The potentials for each bag-type in hierarchical coreference models have parameters w, t that can be fitted to data. Sparse vector representations are typically used to represent bags, but they have a key disadvantage of dynamically changing in size as entities grow, making operations more expensive. This issue also arises in other entity-based models where features are aggregated from mentions. In the context of entity-based models, the issue of pairwise models with sparse feature vectors arises. An alternative representation called homomorphic simhash is proposed for fast cosine-similarity evaluation and dynamic updates. Other schemes supporting these properties are also discussed, including homomorphic minhash for similarity estimates. Simhash is a locality-sensitive hash function for cosine similarity, used for estimating distances between objects in a set. It involves a randomized process with vectors on a unit hypersphere to estimate cos(\u03b8). Simhash is a hash representation that uses a randomized process with vectors to estimate cosine similarity. It involves constructing a function H that produces n-bit hashes by sampling n vectors. The hash H(a) is a bit sequence where the ith bit is 1 if the sign of a \u00b7 u i is positive, otherwise 0. By comparing hashed representations H(a) and H(b), the estimate of cos(\u03b8) can be obtained by counting the number of positions where the hashes are distinct and dividing by n. Instead of sampling vectors uniformly from the unit sphere, an optimization for constructing the hash function H is to sample them from {\u22121, 1} d. This speeds up dot product computation and allows for using 1-bit feature hash functions. This approximation is useful in high-dimensional spaces for estimating cosine similarity. In high-dimensional spaces, using simhash as a representation for coreference resolution requires updating the simhash representation when entities are merged or split. Storing the actual dot product of vectors in the hash function is proposed as a solution to compute the updated parent's hash accurately. In high-dimensional spaces, simhash is used for coreference resolution. Storing the dot product in the hash allows for accurate computation of updated parent hashes. The hash function now consists of arrays instead of bit sequences, enabling efficient cosine distance calculations between vectors. The representation is homomorphic, maintaining the additive group structure. Storing dot products increases hash size but remains small compared to feature vectors, with fixed sizes. Storing dot products and signs as a bit vector allows for quick sign comparisons. In high-dimensional spaces, simhash is used for coreference resolution by storing the dot product in the hash for accurate computation of updated parent hashes. The hash function now consists of arrays for efficient cosine distance calculations between vectors. Storing dot products and signs separately allows for quick sign comparisons using bitwise operations. Statistical properties of the estimator Cn are discussed, emphasizing its use case different from duplicate detection and approximate nearest neighbor search. The estimator Cn is consistent, with additional statistical properties discussed. The Taylor series for g(x) around \u00b5 is used to bound the error term, showing the approximation's accuracy. The Taylor series for g(x) around \u00b5 is used to bound the error term, showing the approximation's accuracy. Without dropping R(x) from the Taylor approximation for g, Lagrange's remainder formula is used to bound the remainder. The method of bounded differences is applied to derive a Chernoff-like tail bound for Cn, a Lipschitz continuous function of n independent Bernoulli random variables. The statistics underlying simhash involve random projections where the Johnson-Lindenstrauss lemma applies, allowing embedding of points in Euclidean space with maintained pairwise distances. The statistics underlying simhash involve linear random projections that maintain pairwise distances within a small factor. These projections are homomorphic with respect to addition and subtraction, and preserve angles between vectors. By using a Taylor expansion, cosine similarity can be estimated directly, potentially improving the quality of the estimate. Although the bit-representation supporting fast bit-operations is lost, the estimate will be smoother. The simhash method quantizes angles into a finite number of possibilities, making the cosine estimate smoother. It can be alternated with direct cosine computation based on the situation. Minhash is a hash function for Jaccard similarity between binary vectors. Minhash is a hash function for Jaccard similarity between binary vectors, using multiple hash functions to reduce variance. It involves designing methods for union, difference, and score calculations. Unlike simhash for cosine similarity, minhash operations are non-linear due to the elementwise minimum operation. The set semantics for minhash score method are problematic due to the underlying vector of hash values. The minhash score method relies on union and difference operations to maintain set invariance. A solution is proposed in Appendix E, addressing set-semantics by augmenting the n-dimensional minhash representation with a vector of counts. However, the solution is not complete, as some corner cases are not yet handled. Despite this, the representation works well in practice. The minhash score method uses union and difference operations to maintain set invariance. A solution in Appendix E augments the minhash representation with counts to address set-semantics. The count indicates the number of child sets contributing to the hash value. When computing the union or difference of sets, the counts are adjusted accordingly. However, handling cases where the count associated with a hash value becomes zero poses a challenge in recomputing the minimum value. Instead of recomputing the minimum value from scratch, a new approach is proposed for minhash with hundreds of hash functions. By ignoring hash values associated with zero counts and using the remaining hashes to compute Jaccard, bias and variance are affected. To prevent issues with all counts being zero, periodic count refresh operations can be performed. Incorporating zero counts in Jaccard estimate to reduce bias and variance. Exploring strategies to handle zero counts during union and difference operations. Hierarchical coref involves entropy and complexity-based penalties on bag of words representations. Some representations depend on context and topics associated with entities. In this section, the study focuses on two cosine-preserving homomorphic compression schemes, simhash and random projection, in a CRF model for author coreference resolution. Theoretical analysis suggests that simhash can improve inference speed due to reduced variance and bias with increasing number of bits. However, the impact on model quality is uncertain. In a study on author coreference resolution, simhash and random projection compression schemes are compared for their impact on model quality and inference speed. Experiments are conducted to analyze simhash's effect on model quality, its improvement on inference speed, and its comparison with random projection. Initial results for homomorphic minhash are also presented to assess its performance compared to exact Jaccard. The REXA author coreference dataset is used, containing 1404 author mentions of 289 authors with ambiguous names. The training set includes mentions of the first five ambiguous names. We split the data into training and testing sets based on ambiguous names. The dataset is highly ambiguous with multiple entities for each name. We use the DBLP dataset for author mentions. Homomorphic simhash is investigated in a hierarchical coreference model using feature variables. The implementation is done using the FACTORIE toolkit. The implementation of hierarchical coreference in the FACTORIE toolkit includes simhash variables and potential functions. Parameters are estimated through hyper-parameter search on the training set. The inference algorithm used is a greedy variant of multi-try Metropolis-Hastings. The implementation details are publicly available in FACTORIE BID28 BID42. In Experiment 1, the quality of simhash representations is compared to exact representations in MCMC inference. Models with simhash representations are optimized on the REXA test set using 256, 128, 64, and 32 bit hashes. The models evaluate and score intermediate results, recording log model ratios for proposed state changes. The simhash model's accuracy is evaluated by comparing it to the exact model in MCMC inference. Varying the number of bits affects the model's ability to judge states, with agreement rates ranging from 97.8% to 88.6%. Decision boundaries show where the models disagree, with the upper-right quadrant containing true positives. Visually, a qualitative gap is observed between 64 bits and 128 bits in the data, recommending the use of at least 128 bits. The exact model achieves an F1 score of 78.6, while simhash variants achieve scores of 77.6, 75.6, 62.8, 55.6 for 256, 128, 64, 32 bits respectively. The accuracy of the 128 and 256-bit models is reasonable, with 256 being competitive with the exact model. Performance decreases significantly when using fewer bits. For detailed results, refer to Table 1 in the appendix. In Experiment 2, the simhash representation improves inference speed significantly. Despite slightly lower F1 accuracy on the REXA test set compared to the exact model, simhash shows a notable computational advantage. The simhash model achieves a significant speed improvement compared to the exact model, with a speed difference increasing with accuracy levels. The simhash model maintains a consistent sampling rate of 20,000-25,000 samples per second, while the exact model's rate drops to under 1000 samples per second as the entity size increases. The possibility of improving the speed of the exact model by reducing the number of features is addressed in Appendix C.2. In Appendix C.2, it is discussed how to improve speed and reduce the gap between simhash and coreference quality. Feature ablation may enhance performance depending on the dataset, while simhash can be applied universally. Experiment 3 compares simhash with a method using cosine on statistics for better results. In Experiment 3, the empirical study compares simhash and JL using real-valued statistics. JL outperforms simhash in terms of Spearman's rho and coreference accuracy, showing improvements with the same number of bits. Detailed results can be found in Table 3 in Appendix D. In Experiment 3, JL outperforms simhash in terms of Spearman's rho and coreference accuracy. Detailed results are in Table 3 in Appendix D. Homomorphic MinHash representation for Jaccard similarity is also investigated, with findings suggesting that 128 and 256 hash functions are sufficient for reasonable estimates. However, there is room for improvement as only 16% of hash functions have a non-zero entry after 100,000 samples. Homomorphic encryption and homomorphic compression methods are used for running models on encrypted or compressed data, respectively. The approach discussed is based on simhash, a locality-sensitive hash function, useful for large-scale streaming settings. Other hashing and sketching algorithms are also utilized in search and machine learning applications. The paper focuses on homomorphic compression schemes, including minhash, to compress sufficient statistics in Bayesian models for scalability as the number of parameters increase with the dataset. Other algorithms like feature hashing and count-min sketches have been used in scaling latent Dirichlet categorical models. The solution involves furnishing minhash values with counts, similar to a k minimum values sketch for estimating the number of distinct values in a set. Our biased estimator directly estimates Jaccard similarity using counts to bound knowledge about minimum hash values during difference operations. Unlike random projections used in machine learning for efficiency, our compression schemes are tailored for dynamic sets, unlike static objects like Gram matrices. Word embeddings also offer low-dimensional dense representations for context capture. In practice, word embeddings are too smooth for coreference resolution. Deep learning shows promise for better representations, but current research focuses on accuracy over speed and scalability. Combining deep learning with conditional random fields for hierarchical coreference remains an open problem. In the paper, various coreference resolution techniques are discussed, including noun-phrase coreference, cross-document coreference, record linking, entity linking, and author coreference. Recent work on scalable hierarchical clustering known as PERCH is mentioned, which uses Euclidean distance. The paper presents homomorphic compression schemes for representing sparse, high-dimensional features in a graphical model for coreference resolution. The paper discusses techniques for coreference resolution, including homomorphic compression schemes for representing sparse, high-dimensional features. It explores simhash and angle-preserving random projections for cosine similarity, as well as a homomorphic version of minhash for Jaccard similarity. The study found that both simhash and random projections were accurate in representing features for coreference, with random projections slightly outperforming. These representations were significantly faster than conventional sparse data structures, enabling greater scalability."
}