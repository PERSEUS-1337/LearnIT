{
    "title": "rJl6M2C5Y7",
    "content": "Effective performance of neural networks relies heavily on tuning optimization hyperparameters, particularly learning rates. Amortized Proximal Optimization (APO) aims to minimize a proximal objective with each optimization step, adapting hyperparameters to optimize this objective after a weight update. APO demonstrates global convergence to a stationary point and local second-order convergence to the global optimum for neural networks. This method incurs minimal computational overhead and can adapt various hyperparameters, such as learning rates, damping coefficients, and gradient variance exponents, during training. APO performs competitively with carefully tuned optimizers for a variety of network architectures and optimization algorithms. Tuning optimization hyperparameters, such as learning rates, is crucial for effective performance in deep learning systems. Various learning rate schedules have been proposed to achieve state-of-the-art performance on challenging datasets. Various hyperparameters, including learning rates, momentum decay factor, batch size, and damping coefficient, play a crucial role in optimization. Adapting these hyperparameters to minimize training error quickly faces obstacles related to generalization performance and short-term progress. Tuning any optimization hyperparameter is challenging due to the regularizing effect of stochastic updates and the trade-off between reducing fluctuations and long-term progress. In this paper, the focus is on optimizing hyperparameters to minimize a proximal objective in each iteration. The approach introduced is Amortized Proximal Optimization (APO), which adapts hyperparameters to achieve this goal. This method aims to balance the loss on the current batch with the average change in predictions by considering proximal objectives of a specific form. Amortized Proximal Optimization (APO) is used to tune hyperparameters of optimization algorithms like SGD, RMSprop, and K-FAC. APO includes a hyperparameter \u03bb to control update aggressiveness and can automatically adapt multiple hyperparameters with just one hand-tuned parameter. The method involves a grid search over \u03bb to achieve competitive learning rate schedules. Theoretical justification for APO is provided through strong convergence results for an oracle solving the proximal objective exactly in each iteration. The study demonstrates strong convergence results for an oracle solving the proximal objective exactly in each iteration, showing global linear and locally quadratic convergence. Amortized Proximal Optimization (APO) is evaluated on real-world tasks, yielding faster training convergence by adapting learning rates online. The solutions found by APO generalize well and view a neural network as a parameterized function. The text discusses the optimization of neural network parameters using a proximal objective function. The goal is to minimize the loss function by updating the weights and biases iteratively. The algorithm aims to minimize the proximal objective by updating the parameters in each iteration. The study shows strong convergence results for the optimization process. The text discusses optimizing neural network parameters using a proximal objective function. It proposes an algorithm to update parameters iteratively, aiming to minimize the proximal objective. The approach involves linearizing the network function to find an approximate solution. The text discusses optimizing neural network parameters using a proximal objective function, which involves linearizing the network function to find an approximate solution. Different dissimilarity settings lead to various efficient optimization algorithms. Different versions of proximal objective lead to efficient optimization algorithms like Gauss-Newton and Natural Gradient Descent. However, these methods rely on local linearization of the neural network and require careful tuning. The proposal is to directly minimize the objective in an online manner and adapt hyperparameters dynamically for better optimization. The optimization algorithm involves updating model parameters based on data, labels, online statistics, and optimization vectors. The optimization algorithm involves updating model parameters based on online statistics and optimization vectors, with hyperparameters dynamically adapted for better optimization. The meta-objective is formulated for each step using a random mini-batch for loss approximation and stability. The Amortized Proximal Optimization (APO) framework optimizes hyperparameters using a stochastic gradient-based algorithm. RMSprop is recommended as the meta-optimizer for stability. Idealized versions of APO are analyzed where an oracle minimizes the proximal objective exactly. In the context of the Amortized Proximal Optimization (APO) framework, idealized versions are explored where an oracle minimizes the proximal objective exactly in each iteration. Strong convergence results are obtained, indicating the usefulness of the proximal objective for meta-optimization in output space. The problem is viewed as optimizing over a manifold in the space of outputs on all training examples, with a focus on designing an update schedule for network outputs. The proximal objective defined as Eq. 4 uses the Euclidean distance as the dissimilarity measure, leading to the Gauss-Newton algorithm under network linearization. With an oracle, this objective results in projected gradient descent for a loss function on a data point in neural network output space. The manifold's smoothness is characterized by geodesics and bounded curvature, with the effective gradient at a point representing the neural network's stationary point. Theorem 1 proves global convergence of Eq. 14 to a stationary point in a neural network. The convergence rate is O(1/T) for the norm of the effective gradient, under smooth manifold conditions with bounded curvature. Lipschitz constants are defined for the output space, ensuring convergence, unlike the weight space where gradients are not Lipschitz continuous. The dissimilarity term is replaced with a second-order approximation. The dissimilarity term is replaced with a second-order approximation in the Proximal Newton Method in the output space, making it efficient for neural nets with a strongly convex loss function. The locally fast convergence rate of iteration is proven under certain assumptions, suggesting quadratic convergence to the unique minimum. The proximal oracle achieves second-order convergence for neural network training, motivating the use of proximal objective for meta-optimization. Traditional hyperparameter optimization methods are costly and limited to fixed values, while Hyperband can terminate poorly-performing runs early but still has limitations. Population Based Training (PBT) trains a population of networks simultaneously, terminating poorly-performing networks, replacing their weights with better-performing ones, and perturbing hyperparameters for continued training. PBT can find coarse-grained learning rate schedules but is less efficient than gradient-based meta-optimization. Gradient-based adaptation of learning rates involves unrolling optimization algorithms as computation graphs to compute gradients of hyperparameters like learning rates. Stochastic meta-descent (SMD) propagates gradients through unrolled training procedures to find optimal learning rate schedules offline. Hypergradient descent (HD) BID2 adapts hyperparameters online by taking the gradient of the learning rate with respect to the optimizer update in each iteration. Some authors have proposed learning entire optimization algorithms from a reinforcement learning perspective, where the state consists of the objective function and prior iterates, and the action is the step. Approaches that learn optimizers must be trained on a set of objective functions drawn from a distribution. The Adaptive Proximal Optimization (APO) method focuses on learning rate schedules for a single objective function, avoiding the need to train on a distribution of functions. APO is evaluated empirically on various learning tasks, with Table 1 providing details on datasets, model architectures, and base optimizers considered. The proximal objective in APO allows for flexibility in approximating the loss function. In our proximal objective, h can be any approximation to the loss function; in our experiments, we used the loss value directly. The dissimilarity term D was the squared Euclidean norm. APO was used to tune optimization hyperparameters for four base optimizers: SGD, SGD with Nesterov momentum (SGDm), RMSprop, and K-FAC. Different hyperparameters were considered for each optimizer. The text discusses the optimization methods RMSprop and K-FAC, highlighting the parameters involved in the optimization process. It also mentions the meta-optimization setup using RMSprop with specific learning rates and update frequencies. The text emphasizes the robustness of APO to the initial learning rate of the base optimizer. APO is robust to the initial learning rate of the base optimizer, with each meta-optimization step requiring similar computation as a base optimization step. By conducting meta-updates once every 10 base optimization steps, the computational overhead of using APO is only slightly higher than the original training process. Validated on the Rosenbrock function, APO tuned the learning rate of RMSprop, outperforming standard RMSprop with fixed learning rates. The learning rate adjustments by APO showed rapid progress at the start of optimization followed by gradual decreases. APO converges quickly from various starting points on the Rosenbrock surface and is robust to the initial learning rate of the base optimizer. It was also evaluated on a challenging regression problem with an ill-conditioned matrix. The adaptive learning rate of RMSprop-APO outperformed RMSprop with fixed learning rates, showing rapid progress at the beginning of optimization. The adaptive learning rate of RMSprop-APO achieved significantly lower loss values compared to RMSprop with a fixed learning rate on real-world datasets like MNIST, CIFAR-10, CIFAR-100, SVHN, and FashionMNIST. Initial learning rates were set at 0.1 for SGD-APO and SGDm-APO, and 0.0001 for RMSprop-APO, with APO showing robustness to the choice of initial learning rate. The only hyperparameter considered for APO is the value of \u03bb, selected through a grid search. For SGDm-APO and RMSprop, the best \u03bb is selected from a grid search over {0.1, 0.01, 1e-3}. A search over \u03bb is more effective than fixed learning rates. APO outperforms fixed learning rates on MNIST using a two-layer MLP with ReLU nonlinearities. APO is used to tune the global learning rate of SGD and SGD with Nesterov momentum on MNIST. The training curves of different optimization algorithms with varying learning rates and damping coefficients are compared in FIG1. APO significantly improves training loss for SGD, SGDm, and RMSprop. RMSprop-APO achieves a training loss three orders of magnitude smaller than the baseline on MNIST with a 34-layer residual network. In the study, a 34-layer residual network was trained on CIFAR-10 using different optimization algorithms and learning rate schedules. APO was found to accelerate training and improve accuracy compared to fixed learning rates and manual decay schedules for both SGD and RMSprop. In the study, APO was used to optimize RMSprop and K-FAC on a ResNet34 for CIFAR-10. APO achieved lower training loss than fixed learning rates and was competitive with manual decay schedules. The results showed that APO improved training efficiency and accuracy compared to traditional optimization methods. In experiments, a fixed learning rate of 1e-3 was used for the baseline with a decay schedule at epochs 40 and 80. APO was used with \u03bb = 1e-2, and damping was fixed at 1e-3 when not tuned. Results in FIG2 show K-FAC-APO performing well. APO was also evaluated on CIFAR-100 using ResNet34 with batch-normalization and data augmentation. SGD-APO/SGDm-APO were compared to standard SGD/SGDm with different learning rate schedules, showing improvement in training loss and test accuracy. In experiments, the learning rate is decayed by a factor of 5 at epochs 60, 120, and 180. APO achieves smaller training loss and higher test accuracy compared to other methods. APO is used to train ResNet18 on the SVHN dataset with batch normalization. Mini-batches of size 128 are used, and networks are trained for 160 epochs. RMSprop-APO achieves similar training loss to a manual schedule but higher test accuracy. Using APO to tune learning rates allows for effective training of BN networks, achieving higher test accuracy compared to manual schedules. Weight decay affects the scale of network weights, influencing the effective learning rate. Using APO for tuning learning rates enables effective training of BN networks without weight decay. SGD-APO outperformed SGD with fixed learning rates and achieved similar results as SGD with a manual schedule. APO allows for online adaptation of optimization hyperparameters, leading to faster convergence and better generalization in MLP and CNN models. The text discusses weight decay regularization and the proof of Theorem 1, introducing a lemma about gradient norms and effective gradients on a smooth manifold with bounded curvature. The proof involves constructing a point Z that satisfies certain inequalities, demonstrating the relationship between gradients and projections onto a plane. The proof involves constructing a point Z that satisfies certain inequalities, demonstrating the relationship between gradients and projections onto a plane. The Hessian \u2207 2 L(Z) is a block diagonal matrix, where each block is the Hessian of loss on a single data. The text discusses the Proximal Newton Method and the use of APO to tune optimization hyperparameters. APO can adapt RMSprop hyperparameters and per-layer learning rates, as shown in FIG6. The text discusses using APO to stabilize training and adapt per-layer learning rates in optimization. A smaller meta learning rate of 0.001 and more frequent meta-updates were found to be useful. APO was also used to train a convolutional network on the FashionMNIST dataset, showing results in FIG6. Comparisons were made between K-FAC, hand-tuned RMSprop, and RMSprop-APO, with K-FAC outperforming RMSprop-APO. In this section, additional experiments on the Rosenbrock problem are presented. APO is shown to converge quickly from different starting points on the surface and is robust to the choice of initial learning rate. APO helps in selecting an initial learning rate by quickly adapting many different rates to the same range. RMSprop-APO was used to optimize Rosenbrock with various initial learning rates, as shown in FIG0. In this section, experiments were conducted on the noisy quadratic problem using APO, demonstrating its ability to overcome the short horizon bias issue. The quadratic function optimized had evenly distributed eigenvalues in the interval [0.01, 1]. Initial learning rates spanning 5 orders of magnitude were tested, showing similar results in training loss, test accuracy, and learning rate adaptation. For each iteration, we access the noisy version of the function with gradient and function value. Different learning rate schedules are considered for SGD and SGD with APO, optimizing parameters for minimal function value after 300 iterations. Adam optimization with 10000 steps and learning rate 0.001 is used. SGD with APO achieves similar training loss as optimal SGD for noisy quadratics task, indicating no short-horizon bias. Adam is an adaptive optimization algorithm similar to RMSProp with momentum, with fixed \u03b21 and \u03b22. APO is used to tune the global learning rate \u03b7 for Adam optimization. Adam with fixed \u03b21 and \u03b22 parameters was optimized using APO to adjust the global learning rate \u03b7. Testing Adam-APO with a ResNet34 on CIFAR-10 showed improved performance compared to fixed learning rates and manual rate schedules. Population-based training (PBT) BID10 involves training a population of N neural networks simultaneously, with networks periodically evaluating their performance to optimize hyperparameters. In Population-based training (PBT), networks can clone better-performing members, copy hyperparameters, and resume training with perturbations. PBT was used to tune the learning rate for RMSprop in optimizing a ResNet34 model on CIFAR-10. A population size of 4 was found to perform better than 10, with a perturbation strategy of multiplying the learning rate by 0.8 or 1.2. Setting the probability to 0 for re-sampling hyperparameter values was critical to prevent abrupt learning rate changes. Comparing PBT with APO, it was crucial to set the probability to 0 to prevent abrupt learning rate changes. APO outperformed PBT by achieving a training loss an order of magnitude smaller and the same test accuracy much more quickly."
}