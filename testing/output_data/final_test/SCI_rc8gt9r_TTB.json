{
    "title": "rc8gt9r_TTB",
    "content": "Textual entailment data has been used for pretraining tasks in language understanding, even with pretrained models like RoBERTa. Four alternative protocols are proposed to improve the creation of pretraining data, aiming to make it easier for annotators to produce quality training examples. Despite collecting and comparing five new 9k-example training sets, no major improvements in transfer learning were observed. However, using automatically provided seed sentences for inspiration improved the quality of the resulting data. Seed sentences for inspiration improve data quality in natural language inference tasks, leading to better results in transfer learning with large neural network models like BERT. The Multi-Genre NLI Corpus (MNLI) is a significant resource for natural language inference tasks, but it has known issues with annotation artifacts. This work explores potential changes to the MNLI data collection protocol to enhance transfer learning. The study explores four changes to the MNLI data collection protocol to improve annotator ease and example quality. They compare the baseline dataset with four additional datasets to evaluate transfer learning effects on downstream language tasks in the SuperGLUE benchmark. The study introduces changes to the MNLI data collection protocol to enhance annotator ease and example quality. Annotators are asked to provide three corresponding hypothesis sentences for a given premise, categorized as entailment, contradiction, or neutral. The BASE data collection protocol closely follows MNLI, while the PARAGRAPH protocol tests the impact of providing annotators with complete paragraphs. The PARAGRAPH protocol tests the effect of supplying annotators with complete paragraphs instead of sentences as premises. Longer texts offer potential for discourse-level inferences, making the dataset more diverse and less likely to contain trivial artifacts. The EDITPREMISE and EDITOTHER protocols test pre-filling a single seed text in each text box to reduce typing and encourage minimal-pair-like examples. The CONTRAST protocol tests the impact of adding artificial constraints on annotators to encourage creativity and prevent repetitive data. Inspired by NLVR2, annotators must write valid entailments for a premise while avoiding valid entailments for a similar distractor premise. In evaluations on transfer learning with the SuperGLUE benchmark, methods like EDITOTHER and CONTRAST show improvements over a plain RoBERTa model, but only by small margins. These methods reduce annotation artifacts and maintain subjective quality without significantly increasing annotation time. The effectiveness of NLI data in pretraining was first reported for SNLI and MNLI. Several studies have shown the effectiveness of pretraining models on NLI data, including multitask pretraining and intermediate training. Transfer learning from the SocialIQA corpus has also yielded positive results. Additionally, a small body of work has explored similar concepts. The empirical landscape of supervised NLP tasks for pretraining is explored. Various NLI datasets have been created using different strategies, such as manual construction by experts and crowdworker involvement. The SNLI/MNLI approach has been effective for pretraining data collection due to its simplicity and scalability. Recent papers have explored methods like ANLI and Kaushik et al. to augment the MNLI protocol. These methods involve incentivizing crowdworkers to produce challenging sentence pairs and expanding datasets through small edits. The method introduced by al. (2019) expands datasets by making small edits to existing examples to change labels, aiming to create minimal pairs with differing labels. ANLI, similar in size to MNLI, is included in transfer evaluations. The task interface is akin to SNLI and MNLI, where human annotators provide entailment, neutral, and contradiction hypothesis sentences based on a given premise. The method introduced by al. (2019) expands datasets by making small edits to existing examples to change labels, aiming to create minimal pairs with differing labels. ANLI, similar in size to MNLI, is included in transfer evaluations. The task interface is akin to SNLI and MNLI, where human annotators provide entailment, neutral, and contradiction hypothesis sentences based on a given premise. In this baseline, annotators are asked to compose sentences for each label - neutral and contradiction. An additional method involves editing pre-filled premise sentences to meet the same requirements. The method introduced by al. (2019) involves using a similarity search method to retrieve sentences similar to a premise to improve creativity and diversity. Annotators are asked to compose sentences based on the retrieved sentences to create entailment and contradiction pairs. The method involves using a sentence-matching system to generate pairs of similar sentences for entailments or contradictions. The OpenANC corpus is used for premise sentences in MNLI, while English Wikipedia is used to avoid re-using premises. The method involves using a sentence-matching system to generate pairs of similar sentences for entailments or contradictions. Data collection for each protocol starts with a pilot of 100 items to refine task instructions and provide feedback to annotators. Regular feedback is provided throughout the annotation process to clarify ambiguities in the protocols. Annotators receive regular feedback to clarify ambiguities in protocols and discourage systematic patterns. Skip rates varied from 2.5% to 10% depending on the task. A team of 19 annotators in the US worked on the tasks, with some working on multiple protocols consecutively. This small team size introduced a modest confound. The annotators worked under multiple protocols consecutively, leading to a confound in the results. Despite this, they collected data for each protocol, split into training and validation examples, and presented randomly chosen examples in Table 1. No second-pass quality control was used in the data collection process. The annotators collected data for each protocol, presenting randomly chosen examples in Table 1. No second-pass quality control was used in the data collection process. Hypotheses are fluent, full sentences that differ from premise sentences both grammatically and stylistically. Seed sentences tend to yield longer hypotheses with no clear correlation between hypothesis-premise token overlap and label. Annotators completed each protocol. Annotators completed each of the five protocols at a similar rate, taking 3-4 minutes per prompt. The CONTRAST protocol yields fewer examples per minute due to its relatively complex nature. Table 3 shows the three words most strongly associated with each label in the datasets. Results from the interventions show a reduction in associations compared to the baseline. Our interventions in MNLI reduce associations between negation and contradiction. Longer contexts or seed sentences eliminate this strong association. Experiments compare models trained in various settings, including different training sets and datasets. Top three words associated with specific labels are shown in Table 3. The study compares models trained with different datasets, including MNLI and ANLI, using RoBERTa and XLNet. RoBERTa was state-of-the-art at the time of the experiments, while XLNet was competitive. Results are reported on validation sets for MNLI and GLUE diagnostic set. The study compares models trained on different datasets using RoBERTa and XLNet. The experiments were conducted using the jiant toolkit and various frameworks like transformers, AllenNLP, and PyTorch. Small-batch training and a maximum sequence length of 128 word pieces were used to train the models. Training durations varied for different tasks. The study evaluates models trained on different datasets using RoBERTa and XLNet. Results show that models trained on BASE data perform somewhat worse than MNLI models on validation sets, suggesting new annotations may be less reliable. The study evaluates models trained on different datasets using RoBERTa and XLNet. Results suggest that new annotations may be less reliable than those in MNLI, but this does not affect key comparisons. None of the interventions improve performance on the out-of-domain GLUE diagnostic set. The newer ANLI data yields worse performance than MNLI on out-of-domain evaluation data. Single-input versions of models trained on hypothesis-only versions of datasets show lower performance with certain interventions compared to BASE. Interventions like EDIT-PREMISE have lower hypothesis-only performance than BASE, indicating a reduction in artifacts. Transfer evaluations involve fine-tuning models on collected data and evaluating on SuperGLUE benchmark tasks. SuperGLUE benchmark tasks were selected to be difficult for BERT but relatively easy for nonexpert humans. These tasks include BoolQ, MultiRC, CommitmentBank, COPA, RTE, WSC, WiC, AX b, and AX g. SuperGLUE does not have labeled test data and does not allow for substantial ablation analyses on its test sets. No single final model's performance is evaluated on the test sets. The final model's performance is not evaluated on the test sets, and no auxiliary WSC-format data is used during training. Results show that intermediate training with different datasets leads to better transfer models compared to baseline models. NLI data is beneficial for transfer, with the ANLI training set improving performance on multiple tasks. The best result is achieved with only 9k NLI training examples. The study uses 9k NLI training examples and finds that interventions do not consistently improve transfer performance over the base MNLI data collection protocol. The study suggests that interventions in the data collection protocol, such as providing annotators with retrieved non-premise seed sentences, can offer small improvements. These interventions help reduce artifacts in generated hypotheses without increasing annotation time, potentially leading to high-quality evaluation data for teaching machines language understanding skills. Further research in teaching machines general-purpose language understanding skills could benefit from improved data collection methods and transfer learning techniques. Exploring incentive structures for crowdsourcing and task design could enhance data quality, while investigating task definitions and data collection protocols could lead to better transfer outcomes."
}