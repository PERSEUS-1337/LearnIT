{
    "title": "BygWBRNtvH",
    "content": "Deep neural networks are vulnerable to adversarial examples, a significant problem in deep learning. This paper establishes a benchmark to evaluate adversarial robustness in image classification tasks. Large-scale experiments are conducted using robustness curves as evaluation criteria, leading to important findings and insights for future research in deep learning. The existing deep learning models are vulnerable to adversarial examples, which are maliciously crafted to cause incorrect predictions. As these models are used in security-sensitive applications, the study of adversarial robustness has gained attention with numerous attack and defense methods proposed. It is essential to rigorously evaluate these methods to understand their effectiveness and drawbacks, compare their performance, and guide the development of new approaches. The research on adversarial robustness faces an \"arms race\" between attacks and defenses, with new methods constantly evading existing ones. Many techniques have been introduced to build robust models, but their effects are challenging to understand and evaluate comprehensively. Most defenses are only tested against a limited set of attacks, making it difficult to identify real progress in the field. In response to the ongoing \"arms race\" between attacks and defenses in adversarial robustness research, most defenses are only tested against a small set of attacks, limiting the evaluation of their effectiveness. Current evaluation metrics are deemed too simplistic to fully capture the performance of these methods. To address this issue, a comprehensive benchmark is proposed in this paper to provide a more thorough understanding of existing methods under various scenarios, aiming to enhance future research efforts. The study focuses on evaluating the robustness of image classifiers under different threat models, incorporating various attack and defense methods. Large-scale experiments are conducted to assess the performance of these methods against different types of attacks. The study evaluates the robustness of image classifiers against various attacks and defense methods. Findings show that defense effectiveness varies with perturbation budgets and attack iterations. Adversarially trained models remain the most robust, even against different threat models. Defenses using randomization are more resilient to black-box attacks. The evaluation experiments on adversarial robustness are conducted on a new platform developed by the researchers. The platform aims to support comprehensive evaluations that existing platforms cannot fully accommodate. Precisely defining threat models is crucial for adversarial robustness evaluations, as it specifies the adversary's goals, capabilities, and knowledge for attacks and defense strategies. In this paper, the focus is on generating untargeted and targeted adversarial examples to deceive classifiers. Adversarial examples are crafted to be indistinguishable from original inputs, with small changes made by the adversary. The study explores p norm threat models for generating adversarial examples. The study focuses on crafting adversarial examples under p norm threat models, allowing small perturbations to deceive classifiers. Two strategies are discussed to create adversarial examples with constrained perturbations, aiming to mislead the model while staying within a perturbation budget. The paper aims to provide a comprehensive evaluation of existing methods without contradicting previous results. The study evaluates existing methods for crafting adversarial examples under different threat models, considering various attack scenarios based on the adversary's knowledge of the target model. Different attack methods are proposed to approximate solutions for generating adversarial examples with optimized perturbations. The study evaluates different attack scenarios for crafting adversarial examples, including white-box, transfer-based, score-based, and decision-based black-box attacks. White-box attacks require detailed target model information, while transfer-based attacks rely on the transferability of adversarial examples. Score-based attacks query the target model for output probabilities, and decision-based attacks rely on predicted classes. White-box attacks craft adversarial examples using input gradients. Methods include FGSM, BIM, PGD, DeepFool, and C&W. These attacks are limited by the number of queries to the target model. The C&W method uses Adam for optimization but can be countered by defenses like obfuscated gradients. Adversaries can use BPDA for approximate gradients or EOT for random gradients. Transfer-based Black-box Attacks create adversarial examples against substitute models to fool black-box models. Other methods like MIM and DIM have been proposed to improve transferability. Decision-based Black-box Attacks: In this challenging setting, the model only provides discrete hard-label predictions. N ATTACK (Li et al., 2019) learns a Gaussian distribution centered around the input to generate likely adversarial examples without estimating the gradient. In the context of decision-based black-box attacks, various methods have been proposed to generate adversarial examples. These include the Boundary attack by Brendel et al. (2018a), an optimization-based method by Cheng et al. (2019), and an evolutionary attack method by Dong et al. (2019). To defend against adversarial attacks, research has focused on robust training, input transformation, randomization, model ensemble, and certified defenses. These defense techniques are not exclusive and can overlap in categories. Robust training aims to make classifiers resistant to small perturbations through adversarial training or regularization techniques. Input transformation methods include JPEG compression, bit-depth reduction, total variance minimization, autoencoder-based denoising, and projecting adversarial examples onto the data. Some defense strategies against adversarial attacks include adding randomness to classifiers, utilizing model ensembles, and employing Bayesian neural networks. These methods aim to prevent shattered or vanishing/exploding gradients that can be exploited by adaptive attacks. Ensemble strategies like random self-ensemble and promoting diversity among predictions have been proposed. Certified defenses, which are provably robust to adversarial perturbations, have been extensively studied. Recently, certified defenses have shown scalability on ImageNet. The text discusses the importance of evaluating adversarial attacks and defenses rigorously. A benchmark is established to evaluate adversarial robustness empirically, incorporating 15 attack methods and 16 defense models on two image datasets. Fair-minded evaluation metrics are adopted to show the results effectively. The evaluation metrics in the benchmark assess the accuracy of the classifier against attacks and the attack success rate. Previous methods may not fully reflect behavior, so this paper introduces robustness curves to show the classifier's resistance and the attack's effectiveness. The robustness curves evaluate the classifier's resistance and attack effectiveness. The first curve shows accuracy vs. perturbation budget, while the second curve shows accuracy vs. attack strength. These curves provide insights into the classifier's robustness and the efficiency of the attack methods. In this paper, the efficiency of attacks and the resistance of classifiers are evaluated using robustness curves. The CIFAR-10 and ImageNet datasets are used for adversarial robustness evaluation. 16 defense models are tested for fair evaluation, covering a range of scenarios. The study evaluates the efficiency of attacks and classifier resistance using robustness curves on CIFAR-10. 8 selected models cover all defense categories, including natural and adversarial training models. 15 attack methods are implemented, categorized into white-box, transfer-based, score-based, and decision-based attacks. PGD is not evaluated due to similarity with BIM, and details of attacks are outlined in Table 2. The study evaluates various attacks and defenses on a new adversarial robustness platform. White-box attacks are crafted on a substitute model, and obfuscated gradients are replaced adaptively. The experiments are conducted on CIFAR-10 and ImageNet datasets, showcasing accuracy against perturbation budgets and attack strengths of defense models. The study evaluates various attacks and defenses on a new adversarial robustness platform, showcasing accuracy against perturbation budgets and attack strengths of defense models. Experiments are conducted on CIFAR-10 using white-box attacks such as FGSM, BIM, MIM, and DeepFool. The study evaluates the accuracy of 8 models against different attacks like FGSM, BIM, MIM, and DeepFool under white-box attacks. Adversarially trained models like PGD-AT and TRADES are more robust, converging after a few iterations. Relative robustness between models can vary based on perturbation budgets and attack iterations. The study compares the accuracy of 8 models against various attacks, showing that TRADES outperforms PGD-AT in white-box attacks with small perturbation budgets but underperforms with larger budgets. The use of robustness curves is recommended to better evaluate attack and defense methods. Trade-based black-box attacks are also examined, with TRADES and PGD-AT demonstrating superior robustness. The study compares the accuracy of 8 models against various attacks, showing that TRADES outperforms PGD-AT in white-box attacks with small perturbation budgets but underperforms with larger budgets. The defenses' accuracy drops with increasing perturbation budget and attack iterations. Recent attacks like MIM and DIM do not perform better than the baseline BIM method. Score-based black-box attacks show decreasing defense accuracy with increasing perturbation budget or number of queries. N ATTACK is more effective, especially against NES and SPSA. RSE is resistant to score-based attacks, particularly NES and SPSA. The study shows that RSE is resistant to score-based attacks like NES and SPSA. Decision-based black-box attacks are also ineffective against RSE compared to other defenses due to the randomness of predictions. Experimental results on ImageNet are presented using normalized 2 distance for evaluation. The study evaluates the performance of different models against various attacks on ImageNet. FD shows superior performance and is trained using PGD-based adversarial training. Transfer-based black-box attacks using ResNet-152 as a substitute model show that MIM and DIM improve the transferability of adversarial examples over FGSM and BIM. The study evaluates the performance of different models against various attacks on ImageNet. Transfer-based black-box attacks using ResNet-152 as a substitute model show that adversarial examples generated by BIM can \"overfit\" to the substitute model, resulting in lower accuracy of black-box models. Score-based and Decision-based Attacks show that defenses based on randomization have higher accuracy, while input transformations like JPEG and Bit-Red also improve robustness over the baseline model. The study highlights key findings on the robustness of defenses against attacks, emphasizing the importance of considering varying attack parameters. PGD-based adversarial training is found to produce the most robust models, suggesting the use of robustness curves as major evaluation metrics. Adversarial training can be effective against certain threat models but may reduce natural accuracy and have high training costs. Research is focused on developing methods to maintain accuracy or reduce costs. Defenses based on randomization are resistant to certain attacks due to random predictions, making gradients unreliable for attacks. Research is needed to develop more powerful attacks that can evade randomization-based defenses. The defenses based on input transformations can slightly improve robustness and accuracy against black-box attacks. Different transfer-based attack methods show similar performance on CIFAR-10, while recent methods like MIM and DIM enhance transferability on ImageNet by avoiding overfitting issues. In this paper, a comprehensive benchmark was established to evaluate adversarial robustness of image classifiers. Large-scale experiments were conducted using two robustness curves as evaluation criteria to understand adversarial attack and defense methods better. Some key findings were drawn from the evaluation results, which could benefit future research. However, existing platforms do not fully support the comprehensive evaluations in this paper, as some attacks evaluated were not included in these platforms. Less than 10 out of the 15 attacks adopted in this study were covered by these platforms. Existing platforms do not fully support the comprehensive evaluations in this paper, as some attacks evaluated were not included. Less than 10 out of the 15 attacks adopted in this study were covered by these platforms. A new adversarial robustness platform was developed to meet the requirements, similar to DeepSec but with improvements over its flaws. Our work addresses issues with existing platforms for evaluating adversarial attacks and defenses. We focus on complete threat models, use original source codes, and adopt fair evaluation metrics. Additional details on network architectures and attack methods are provided in this section. In our experiments, we introduce untargeted attacks such as FGSM and BIM, which generate adversarial examples under different norms. These attacks can be extended to 2 attacks. We perform line and binary searches to determine accuracy vs. perturbation budget curves. MIM integrates momentum for improved performance in attacks. In white-box attacks, MIM integrates a momentum term into BIM with 20 iterations. DeepFool generates adversarial examples with a maximum of 100 iterations. C&W is an optimization-based attack method using an Adam optimizer and binary search to find a constant for generating adversarial examples. In white-box attacks, various optimization-based attack methods are used with different iterations and techniques. Ba, 2015 introduces an optimizer with binary search to find a constant for generating adversarial examples. DIM (Xie et al., 2019b) utilizes random resizing and padding of input for gradient calculation. ZOO (Chen et al., 2017) optimizes in a black-box manner through queries, estimating gradients at each coordinate. In adversarial example generation, various methods like NES, SPSA, and N ATTACK use different techniques such as full gradient estimation, Gaussian distribution sampling, and learning a distribution centered around the input. These methods set parameters like \u03c3, q, sampling variance, learning rate, and number of samples per iteration to generate adversarial examples. The decision-based black-box attacks Boundary and Evolutionary rely on heuristic search on the decision boundary, requiring an adversarial starting point. Different techniques like full gradient estimation and Gaussian distribution sampling are used in adversarial example generation. The accuracy curves of defenses on CIFAR-10 against various attacks under different norms are shown in multiple figures. White-box, transfer-based, and score-based attacks are analyzed under the \u221e norm, while untargeted and targeted attacks are examined under the 2 norm. Figures 33 to 42 display accuracy curves of defenses on CIFAR-10 against different types of attacks under the 2 norm. Figures 43 to 50 show attack success rates for various attacks on the 8 models on CIFAR-10. Additionally, targeted attacks under the \u221e norm are illustrated in Figures 51 and 52. Figures 51 to 62 depict accuracy curves and attack success rates of defenses on ImageNet against targeted attacks under different norms, including white-box, transfer-based, and score-based attacks. Additionally, Figures 63 to 66 show accuracy curves against untargeted and targeted attacks under the 2 norm. Figures 67 to 84 display accuracy curves and attack success rates of defenses on ImageNet against untargeted and targeted transfer-based, score-based, and decision-based attacks under the 2 norm."
}