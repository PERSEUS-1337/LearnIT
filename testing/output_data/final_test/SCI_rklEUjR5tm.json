{
    "title": "rklEUjR5tm",
    "content": "Derivative-free optimization (DFO) using trust region methods is commonly used for machine learning applications, particularly for optimizing parameters without known derivatives of objective functions. This work introduces a novel dynamical system called \\ThePrev---\\underline{S}tochastic \\underline{H}amiltonian \\underline{E}xploration and \\underline{E}xploitation, which combines exploration and exploitation processes to efficiently search for the minimum of a blackbox function. The \\ThePrev\\ algorithm combines exploration and exploitation to search for the minimum of a blackbox function using a time-evolving quadratic function and a fast-converging Hamiltonian system. \\TheName\\ parallelizes multiple \\ThePrev\\ threads for faster optimization, outperforming other derivative-free optimization algorithms in machine learning applications. DFO techniques like BID12, BID14, and natural evolution strategies BID41 are used for black-box function optimization when derivatives are not available. Parameter tuning for large-scale machine learning models often requires black-box optimization, where the function mapping parameter settings to model performance is unknown. DFO algorithms need to converge quickly with global/local minimum guarantee. Pioneering work has been done to ensure DFO algorithm performance, with Powell et al. proposing advancements in 1964. Trust-Region methods proposed by Powell et al. aim to surrogate DFO solutions by exploring trust regions of blackbox objective functions. These methods involve iteratively alternating between exploration and exploitation to find global/local minimum. Various algorithms using trust region for DFO surrogation have been developed, but face challenges such as high computational and storage complexity for surrogates. The computational and storage complexity for (convex) surrogates like quadratic functions and Gaussian processes is high. Fitting these surrogates through interpolation is time-consuming with high sample complexity. Trust region methods may not converge for high-dimensional nonconvex DFO. Compared to derivative-based algorithms, trust region methods face challenges in optimization. Algorithms like stochastic gradient descent and accelerated gradient methods BID3 BID44 lack theoretical convergence guarantees for DFO algorithms. Jamieson et al. BID16 established a lower bound for boolean-based comparison algorithms, showing DFO convergence at \u2126(1/ \u221a T ) rate in the best case. Novel trust region methods are being studied to address technical challenges, focusing on low-complexity Quadratic Surrogates with Limited Memory using identity Hessian matrices for computational efficiency. Our algorithm reduces memory consumption by only using the most recent sample points. However, using identity Hessian matrices for quadratic form loses information about the distribution of sample points. Fast Quadratic Exploration with Stochastic Hamiltonian Dynamical Systems aims to explore the quadratic trust region quickly, rather than taking the fastest path towards the minimum. This requires an exploration mechanism to traverse the trust region efficiently. The text discusses using stochastic Hamiltonian dynamics to explore the quadratic trust region efficiently, balancing fast-approaching the minimum with sampling the region. This approach aims to explore the trust region quickly while maintaining convergence. The method explores multiple quadratic trust regions with parallel exploration-exploitation, preserving information from multiple sample points. Inspired by recent progress in continuous-time convex minimizers, the approach accelerates through exploring the minimum from multiple trust regions using Hamiltonian dynamical systems. The new ODE and dynamical system aim to address technical challenges in derivative-free optimization by proposing a continuous-time minimizer and discrete-time algorithms for black-box optimization. Contributions include a Hamiltonian system for exploration and exploitation, algorithms SHE 2 and P-SHE 2, and experiments with real-world applications. The algorithms SHE 2 and P-SHE 2 outperform a wide range of DFO algorithms with better convergence. This work is the first to use a Hamiltonian system with coupled process for DFO algorithm design and analysis. Trust region algorithms can be categorized by the model functions used for surrogates, such as Gaussian Process or Quadratic functions. Gaussian process can fit trust regions well with confidence bounds measured using samples evaluated by the blackbox function. Recent studies have focused on improving GP-based surrogation algorithms by using kernel gradients and mini-batch techniques. Quadratic surrogation approximates trust regions through the second-order Taylor expansion of the objective function. BID39 proposed a second-order algorithm for blackbox variational inference based on quadratic surrogation, while BID30 used a Gaussian Mixture Model for policy optimization. A novel convex model generalizing the quadratic surrogation has been proposed for structured prediction. Covariance Matrix Adaptation Evolution Strategy (CMA-ES) models the energy of blackbox functions using a multivariate Gaussian distribution and can be accelerated with parallel evaluation. Nesterov's accelerated method for quadratic function minimization is also reviewed. Nesterov's accelerated method for quadratic function minimization can be viewed as an ODE, converging with theoretical consequences for convex functions. The ODE converges over quadratic loss at a specific rate with an initial status X(0). The proposed Hamiltonian system for Black-Box minimization via exploration and exploitation is introduced, derived from Nesterov's accelerated method ODE. It aims to search for the minimum of a black-box objective function in a d-dimensional vector space. Stochastic Hamiltonian Exploration and Exploitation are defined within this system. The Hamiltonian system involves exploration and exploitation processes coupled together to track dynamic processes and memorize minimum points. It utilizes second order ODEs and perturbation terms to escape unstable stationary points in a shorter time. The dynamical system treats Y (t) as the minimizer of the black-box function f (X) by approximating it with a quadratic function and using an ODE to track the minimum. The system continuously updates the quadratic function until it reaches a stationary point. The system can be interpreted as an adaptive perturbated dynamical system aiming to minimize the Euclidean distance between X(t) and Y (t) at each time t. The memory complexity of this continuous-time minimizer is O(1). The complexity of the continuous-time minimizer is O(1), utilizing a Markov process Y (t) to remember the local minimum during exploration and exploitation. Theorem 1 states that as t \u2192 \u221e, X(t) converges to a possible local minimum x *. The SHE 2 algorithm is introduced as a discrete-time approximation, with Algorithm 1 implementing it for a black-box function f (x) with step-sizes \u03b1 t and perturbation scale \u03b5. The output Y T refers to the value of Y t in the last iteration. The SHE 2 algorithm utilizes a variable Y t to remember the local minimum in each iteration without computing derivatives. It has a memory complexity of O(1) and enjoys a convergence rate of \u2126(1/ \u221a T) without additional assumptions. The algorithm behaves as a discrete-time approximation to continuous-time minimizer-coupled processes X(t) and Y(t) as \u03b1 t approaches 0. The SHE 2 algorithm uses variable Y t to remember local minimums without computing derivatives, with memory complexity of O(1) and convergence rate of \u2126(1/ \u221a T). It approximates continuous-time minimizer-coupled processes X(t) and Y(t) as \u03b1 t approaches 0, proposing a new Hamiltonian for parallel computation speedup. The SHE 2 algorithm utilizes variable Y(t) to store local minimums efficiently, with a memory complexity of O(1) and a convergence rate of \u2126(1/ \u221a T). It introduces a new Hamiltonian dynamical system with ODEs for parallel computation speedup, exploring and exploiting in parallel with multiple pairs of coupled processes. The new surrogation model Q \u03b4 measures joint distance from X i (t) to Y i (t) and Y (t) with a trade-off factor \u03b4 > 0. Y (t) is treated as the minimizer of the black-box function f (X) in a perturbated dynamical system with multiple state variables. Memory complexity is O(N) for continuous-time minimizer. The proposed P-SHE 2 algorithm is a discrete-time approximation of dynamical systems, where the sequence Y t always exploits the minimum point found by all N threads. The algorithm output Y T can be used as the minimizer of the function f(x). This algorithm can be viewed as a particle swarm optimizer with inverse-scale. P-SHE 2 algorithm is a particle swarm optimizer with inverse-scale step-size settings, aiming for faster convergence by approximating Nesterov's scheme. The research includes rigorous analysis and experiments to validate the algorithm's performance in minimizing non-convex functions. The study evaluates various optimization algorithms including GP-UCB, BOBYQA, L-BFGS, CMA-ES, and PSO. These algorithms are used to train logistic regression and support vector machine classifiers on benchmark datasets. Additionally, P-SHE 2 is utilized to optimize hyper-parameters of ResNet-50 for performance tuning on Flower 102 and MIT Indoor 67 datasets. Performance comparisons are made using 2D benchmark nonconvex functions. The study evaluates optimization algorithms like GP-UCB, BOBYQA, L-BFGS, CMA-ES, and PSO for training classifiers. P-SHE 2 optimizes hyper-parameters of ResNet-50 for Flower 102 and MIT Indoor 67 datasets. Performance comparisons are made using 2D benchmark nonconvex functions, showing P-SHE 2 (10) converges to global minimum with lower time consumption than other algorithms. The study evaluates optimization algorithms like GP-UCB, BOBYQA, L-BFGS, CMA-ES, and PSO for training classifiers. P-SHE 2 optimizes hyper-parameters of ResNet-50 for Flower 102 and MIT Indoor 67 datasets. Performance comparisons show P-SHE 2 (10) converges faster than PSO(10) and PSO(1) due to adaptive step-size settings from Nesterove's scheme. Logistic regression and SVM classifiers are trained on Iris, Breast, and Wine datasets using the mentioned algorithms. The study evaluates optimization algorithms like GP-UCB, BOBYQA, L-BFGS, CMA-ES, and PSO for training classifiers. P-SHE 2 (100) outperforms all other algorithms in terms of loss reduction and convergence performance for logistic regression and SVM classifiers on Iris, Breast, and Wine datasets. The accuracy of classifiers trained by P-SHE 2 (100) is comparable to those trained using gradient-based optimizers. P-SHE 2 outperforms other optimization algorithms in loss reduction and convergence for logistic regression and SVM classifiers on various datasets. It also shows promising results in optimizing hyper-parameters for ResNet-50 networks pre-trained on ImageNet and Place365 datasets for classification tasks. The experiments compare P-SHE 2 with various solvers and hyper-parameter tuning tools on a Xeon E5 cluster with TitanX, M40x8, and 1080Ti GPUs. P-SHE 2, PSO, and CMA-ES use 10 search threads for parallel optimization, while GP-UCB and BOBYQA use single search threads. P-SHE 2 achieves the lowest validation loss for hyper-parameter optimization of ResNet under the same settings. The experiments compared P-SHE 2 with various solvers and hyper-parameter tuning tools on a Xeon E5 cluster with TitanX, M40x8, and 1080Ti GPUs. P-SHE 2 achieved the lowest validation loss for hyper-parameter optimization of ResNet. GP-UCB achieved a validation error of 0.099854 for the Flower 102 task. The performance of the three parallel solvers is sometimes random and close to each other. Optimization algorithms using Hamiltonian dynamics for black-box function optimization. P-SHE 2 accelerates minimum search by parallelizing multiple search threads. It utilizes multiple quadratic trust regions with stochastic Hamiltonian dynamics to speed up exploration-exploitation processes without needing Hessian matrix estimation. P-SHE 2 is a optimization algorithm that accelerates minimum search by parallelizing multiple search threads and leveraging parallel black-box function evaluation. It can compete with a wide range of DFO algorithms to minimize nonconvex benchmark functions and fine-tune deep neural networks. The goal is to show convergence to a local minimum point in a dynamical system. In equations (a) and (b) of (10), the pair of processes (X 0 (t), Y 0 (t)) is coupled. X 0 (t) converges to the minimum point of f (x) as t \u2192 \u221e.Lemma 1 states for the deterministic system (10), where X 0 (t) converges to the minimum point of f (x) along its trajectory. In the given system, the process X(t) converges to the local minimum x* of the function f(x) as t approaches infinity. The addition of noise \u03b6(t) does not affect this convergence, leading to X(t) approaching x* as desired. The addition of noise \u03b6(t) does not affect the convergence of the process X(t) to the local minimum x* of the function f(x) as t approaches infinity. The process behaves close to X 0 (t) with high probability, leading to X(t) approaching x* as t \u2192 \u221e. The algorithm SHE2 converges via two steps. Lemma 1 shows that Nesterov's accelerated gradient descent helps the process X(t) catch up with the minimum point Y(t). Lemma 2 demonstrates that the noise term \u03b6(t) helps X(t) reach local minima as t approaches infinity."
}