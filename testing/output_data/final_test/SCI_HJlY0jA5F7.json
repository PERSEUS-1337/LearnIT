{
    "title": "HJlY0jA5F7",
    "content": "In this paper, an improved quantitative evaluation framework for Generative Adversarial Networks (GANs) is proposed for generating domain-specific images. The framework enhances evaluation methods by using a specialized encoder for fine-grained domain-specific representation and introducing Class-Aware Frechet Distance (CAFD) for datasets with multiple classes. Experiments show improvements over the state-of-the-art FID method. Our framework provides counter examples where FID results differ from human judgments, improving robustness in GAN evaluation. GANs excel in tasks like style transfer and super resolution by playing a game between generator and discriminator. Despite advancements in GAN architectures, a standardized evaluation framework remains a challenge. Recently proposed GAN variants lack benchmark results, leading to validation on different frameworks, making comparisons difficult. Quantitative evaluation frameworks are crucial for guiding future research on GAN models. Efforts have been made to design sample-based evaluations for GANs, measuring the distance between generated and real images. The Inception Score is commonly used but has limitations, prompting the introduction of the Frechet Inception Distance for improvement. In this work, an improved quantitative sample-based evaluating criteria is proposed to address overfitting in GAN models. The Class-Aware Frechet Distance (CAFD) is introduced to measure distribution distance, enhancing evaluation methods by using a specialized encoder for domain-specific representation. This approach aims to overcome limitations of existing evaluation metrics like Inception Score and FID by considering class information on labeled datasets. The proposed Class-Aware Frechet Distance (CAFD) measures distribution distance for each class in the feature space using a Gaussian mixture model. KL divergence is also included to detect mode dropping. Experiments show the framework's effectiveness and ability to provide counter examples where FID is inconsistent with human judgements. Model-based methods like Parzen window estimation and annealed importance sampling require density estimation or observation on the decoder's inner structure. Model-agnostic methods, such as Maximum Mean Discrepancy (MMD) and Inception Score, are popular in the GAN community for measuring distribution similarity between datasets and generated data. FID was proposed to improve Inception Score, while recent methods like Class-Aware Frechet Distance (CAFD) use Gaussian mixture models to measure distribution distance for each class in the feature space. Recently, various methods have been proposed to improve evaluation metrics for sample-based methods, including classification accuracy, precision-recall, and skill rating. While Inception Score was popular in the past, recent literature suggests it may be misleading, with the FID metric being considered far superior. The effectiveness of using the ImageNet model for Inception Score has been questioned, and comparisons between different evaluation methods have been gaining attention. The FID metric is considered superior to Inception Score, with recent experiments showing its robustness. However, there are still issues with FID, leading to inconsistent results with human judgements. An improved evaluation method is proposed to address these shortcomings, focusing on modeling the distance between real and generated samples in GAN models. Difficulties in generating domain-specific images include lack of generating ability, mode collapse, and mode dropping. A good evaluation framework should penalize mode collapse and mode dropping in GAN models. Conventional methods use an ImageNet pretrained inception model, but Inception Score is considered misleading. Mode Score, incorporating the prior distribution of ground truth labels, improves on reporting mode dropping. FID BID11 metric is also used for evaluating GAN models. The FID metric, which measures Frechet distance on the feature space using an ImageNet model, is believed to be better than Inception Score. However, FID still has two major problems. Applying the ImageNet pretrained model on non-ImageNet datasets is considered meaningless, as mentioned previously. The ImageNet model applied to map generated images to the feature space in FID can be misleading due to unmatched class labels, making CNN representations trained on ImageNet meaningless. The ImageNet model applied to map generated images to the feature space in FID can be misleading due to unmatched class labels, making CNN representations trained on ImageNet meaningless. Specifically, fine-grained features distinguishing different objects in ImageNet are not necessary for datasets like CIFAR-10 or CelebA, leading to a lack of fine-grained information in the encoded features. We propose that on datasets with multiple classes, the feature distribution is better fitted by a Gaussian mixture model. This model considers x \u223c N (\u00b5 i , C i ) with probability p i, deriving the first and second moment of the feature distribution. FID's single Gaussian assumption is oversimplified, as directly modeling the whole distribution results in a small number of degrees of freedom. FID detects mode-related problems in a small number of degrees of freedom. Using a specialized domain-specific encoder like VAE can provide more effective features for sample-based evaluation, especially in labeled datasets. In labeled datasets, a cross-entropy loss can be added for training the VAE model to improve domain-specific representations. Popular metrics like Inception Score and Mode Score measure probability distribution distance, while FID measures distance in the feature space. A class-aware metric on the feature space is proposed to combine these perspectives for datasets with multiple classes. The feature distribution is better fit with mixture Gaussian for datasets with multiple classes. The Class-Aware Frechet Distance (CAFD) is proposed to include class information by computing probability-based Frechet Distance between real data and generated samples in each class. A domain-specific VAE is trained along with cross entropy on datasets with multiple classes to use its learned representations. The expected mean and covariance matrix in each class are calculated to compute Frechet distance. The covariance matrix in each class is used to compute Frechet distance and obtain Class-Aware Frechet Distance (CAFD). This improved metric, based on a mixture Gaussian assumption, provides better evaluation of the actual distance compared to the original FID. CAFD helps in understanding the generating ability of a specific GAN model for different classes. It can identify mode dropping issues in generated samples, which FID and CAFD alone cannot address. Our proposed method incorporates KL divergence as auxiliary scores in the evaluation framework, along with the correct use of encoder and Class-Aware Frechet Distance (CAFD). It combines advantages of Inception Score, Mode Score, and FID, addressing mode dropping issues in generated samples. Experiments show that using a domain-specific encoder is crucial for accurate evaluation results. The study focused on mapping generated images onto a feature space, highlighting the importance of using domain-specific encoders for accurate evaluation results. Two proposals were made: an autoencoder and a VAE with similar network architecture. Training was done for 25 epochs with a KLD term weight of 1e-5. PCA was conducted on three feature sets encoded on CelebA dataset. The study conducted PCA on three feature sets encoded on CelebA: ImageNet inception model, proposed autoencoder, and proposed VAE. The results showed that the ImageNet model had limited representation capability in a low-dimensional subspace. VAE utilized the feature space better than the naive autoencoder. Adjustments were made to demonstrate the deficiency of the ImageNet model on CelebA images. The ImageNet inception model fails to encode fine-grained facial features, resulting in inconsistent results with human judgements. Random noise was applied to pixels and regions were exchanged to show the model's limitations in representing facial textures. FID fluctuated within a small range when adjustments were made while maintaining overall color, indicating the model's focus on general features like color and shape for object classification. The trained autoencoder and VAE were applied to compare domain-specific facial textures representation. Results showed that only representations from domain-specific encoders were effective, while the discriminator did not learn good representations for distance measurement. For datasets with images from a single class, a domain-specific encoder like a VAE should be used for acquiring representations. Our sample-based evaluation utilizes specialized representations to provide detailed information in specific domains. We compared the evaluation metric CAFD with FID on datasets with multiple classes, showing that CAFD's Gaussian mixture model fits feature distribution better. A user study demonstrated the method's improved consistency, with CAFD proving robust compared to FID in human judgement cases. Implementation details include training a VAE on the MNIST dataset with specific encoder settings. In a user study, 15 volunteers were trained to distinguish between generated samples and groundtruth images. They were asked to choose the better image sets in random pairs, with results compared to evaluation metrics like Inception Score, Mode Score, and FID. Two settings, 'easy' and 'hard', were used for experiments on MNIST dataset. In a study comparing generated images with groundtruth, volunteers were trained to choose better image sets. Results were compared to evaluation metrics like Inception Score, Mode Score, and FID. Two settings, 'easy' and 'hard', were used for experiments on MNIST dataset. Our method showed consistent improvement over baseline approaches in both settings. FID was found to be inconsistent with human judgements in some cases. Different representations were used, including a domain-specific classifier and a VAE. The study compared generated images with groundtruth using evaluation metrics like Inception Score, Mode Score, and FID. Two settings, 'easy' and 'hard', were used for experiments on the MNIST dataset. Different representations were utilized, including a domain-specific classifier and a VAE. The FID metric was found to be inconsistent with human judgements in some cases. The VAE generated images that were sampled and compared to FID, showing robustness to feature-level adjustments. The features extracted from MNIST test data were manipulated using PCA and normalization techniques, resulting in adjusted features with zero FID maintained. The study compared generated images with groundtruth using evaluation metrics like Inception Score, Mode Score, and FID. Two settings, 'easy' and 'hard', were used for experiments on the MNIST dataset. Different representations were utilized, including a domain-specific classifier and a VAE. The features extracted from MNIST test data were manipulated using PCA and normalization techniques, resulting in adjusted features with zero FID maintained. The proposed method reported changes with CAFD raising from 0 to 246.2 for VAE and 539.8 for the classifier. Reconstruction of images was done using FGSM BID8, showing that the quality of constructed images was worse than generated samples. The quality of constructed images is much worse than generated samples, suffering from mode collapse. Despite this, the VAE setting received a lower FID of 25.4 compared to 49.9 for generated samples. CAFD results show that the evaluation metric is more robust, with generated images receiving a much lower CAFD than constructed images. This demonstrates the effectiveness of the evaluation framework for Generative Adversarial Networks. The evaluation framework for Generative Adversarial Networks has been improved with an emphasis on representation and evaluation metrics. A domain-specific encoder, Class-Aware Frechet Distance, and counter examples to the FID method have been introduced. The framework proves to be more effective, especially for datasets different from ImageNet, requiring consistent class labels for training GAN models and the encoder. In this experiment, the Gaussian assumption on features was used to simplify the model for training the encoder. However, in labeled datasets with multiple classes, this assumption may be oversimplified. The Anderson-Darling test was performed to quantitatively study the normality of the data, specifically on a set of features using PCA and AD-test on the first 10 components. The test results were compared on each class and the whole training set on MNIST, using a 2-conv structure as the feature encoder. The Anderson-Darling test was used to study the normality of features in a model. Results showed that features within each class were more Gaussian compared to mixed features. This suggests that the basic assumption of the proposed framework is more reasonable than other methods. Generative Adversarial Networks have been applied to various computer vision tasks, with continuous development of better architectures and training strategies for generating domain-specific images. The Anderson-Darling test showed that features from a single class are more Gaussian compared to mixed features, supporting the proposed framework's basic assumption."
}