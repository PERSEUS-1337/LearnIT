{
    "title": "Bkx2jd4Nx7",
    "content": "We propose a unified framework using optimal transport to build unsupervised representations of individual objects or entities, incorporating both distributional estimates and vector embeddings. This approach offers rich feature representations capturing uncertainty and interpretability, demonstrated in text applications like sentence representation and entailment detection. The method shows significant advantages in empirical results and can be applied to various unsupervised or supervised problems with a co-occurrence structure. The framework utilizes Wasserstein distances and barycenters for co-occurrence structures in various data modalities, including text. It focuses on improving representation methods for natural language processing and machine learning, such as vector embeddings and neural network architectures like CNNs and RNNs. The goal is to map input entities to dense vector embeddings in a low-dimensional latent space while preserving input semantics. In a fundamentally different approach, entities are represented based on histograms of co-occurring contexts in a metric space, allowing for the optimization of moving contexts between entities. The contexts can vary from words to general entities, enhancing representation methods in natural language processing and machine learning. The proposed approach involves constructing histograms of co-occurrence structures to capture multiple semantics of entities, such as words or phrases. This method aims to address uncertainty and polysemy in natural language by using probability distributions over embeddings to capture more information compared to point-wise embeddings alone. The framework discussed involves using histograms of co-occurrence structures to capture various semantics of entities like words. It utilizes optimal transport to enhance tasks in NLP, such as word and sentence representations, hypernymy detection, and can be applied to pre-trained embeddings. This approach expands the use of optimal transport beyond document distances, offering tools like Wasserstein distances and barycenters for NLP applications. The curr_chunk discusses the efficiency of building histograms for optimal transport distances in natural language processing. It highlights the use of entropic regularization for efficient computation on GPUs and the interpretability of the obtained transport map. Previous work has focused on vector space models like Word2vec and GloVe, mapping similar words to nearby points in a latent space. Many works have suggested specializing embeddings to capture specific information for tasks. Recent approaches include representing words with Gaussian distributions or hyperbolic cones for richer information. Other methods involve elliptical distributions with a Wasserstein metric. These techniques provide more detailed semantics and uncertainties compared to traditional vector embeddings. The paper proposes associating each word with a distributional and point estimate to enable optimal transport in NLP. Previous works have explored optimal transport in tasks like document distances, clustering, and bilingual lexicon induction. The focus has been on transporting words directly, such as with the Word Mover's Distance for document comparison. Our approach considers transport over contexts to propose a representation for words, enabling the establishment of various distances between words. This framework extends to sentences using Wasserstein barycenters, offering flexibility for unsupervised representations of entities and their compositions. Optimal Transport (OT) allows comparison of probability distributions over a space G using a distance metric. It transforms distance between points to distance between distributions. The Optimal Transport distance between two distributions is found by solving a linear program with a transportation matrix T. The optimal transportation matrix T ij moves mass from point x i to y i to minimize cost. OT defines distance between probability distributions using p-Wasserstein distance. Solving the OT problem scales at least O(n 3 log(n)). Entropy regularized Wasserstein distance W \u03bb p (\u00b5, \u03bd) can efficiently solve the problem. The Wasserstein distance can be efficiently solved using Sinkhorn iterations with regularization strength \u03bb controlling the accuracy. The Wasserstein barycenter minimizes the sum of Wasserstein distances to given measures, while the regularized barycenter uses entropy regularized Wasserstein distances in the minimization problem. The Wasserstein distance W \u03bb p can be efficiently solved using iterative Bregman projections. In NLP, the probability p(w|c) of a word w occurring in a context c is estimated empirically. The distributional estimate of a word w is built from a histogram over contexts C and an embedding into space G. The curr_chunk discusses embedding contexts into a space to build a histogram based on co-occurrence matrix between words and contexts. It explores reducing the number of bins in the histogram and modifying the co-occurrence matrix for better associations. The text also mentions the need for a dense low-dimensional embedding of contexts to capture semantics effectively. Techniques like SVD or deep learning can be used for this purpose. The curr_chunk discusses embedding contexts into a low-dimensional space to capture semantics using techniques like SVD or deep neural networks. It introduces the concept of representing words with empirical distributions and point estimates in the ground space. The distributional estimate of a word is obtained by combining a histogram with the embedding of contexts. The curr_chunk discusses defining a distance between word representations using a meaningful metric in the ground space. It relates to the distributional hypothesis that words in similar contexts have similar meanings. The estimates of word representations and distributional estimates are closely tied together for effective representation. The curr_chunk discusses using the regularized Wasserstein distance to plot transportation matrices for word contexts. It highlights how words like 'ivory' adjust their movement towards related concepts like 'skin' and 'endangered'. Additionally, it explains how considering point estimates of context is crucial in determining the distance between words in distributional estimates. The curr_chunk discusses the application of regularized Wasserstein distance in representing co-occurrence structures between entities and their contexts, not limited to NLP. It emphasizes the importance of considering uncertainty in contexts to determine distances accurately. The connection with optimal transport offers a theoretical and algorithmic toolkit for NLP problems. In the next section, a framework is presented for applying sentence representation and hypernymy detection tasks. The framework focuses on context words within a symmetric window around the target word. The co-occurrence matrix reflects the associations between target and context words, with stronger associations observed in specific word pairs. The Positive Pointwise Mutual Information (PPMI) matrix is used to capture associations between target and context words based on their co-occurrence probabilities. PPMI is biased towards infrequent words and can be adjusted by smoothening context probabilities. Levy and Goldberg (2014b) have suggested this approach. The shifted PPMI (SPPMI) matrix, with a shift by log(s), enhances semantic associations from co-occurrence data. Computational challenges arise due to the vast number of possible contexts, making optimal transport between histograms complex. The use of SPPMI matrix helps to sparsify the data. The use of SPPMI matrix helps sparsify co-occurrences, but the varying cardinality of word histograms poses a challenge for the Sinkhorn algorithm. One solution is to consider a few representative contexts in the ground space and cluster them based on a meaningful metric to reduce computational complexity. To address the computational complexity in clustering word histograms, one approach is to use stochastic optimal transport techniques. By obtaining K representative contexts and calculating histograms for words with respect to these contexts, the relative portion of each context's SPPMI can be measured. This method helps in sparsifying co-occurrences and reducing computational complexity in the Sinkhorn algorithm. The method involves measuring the relative portion of a context's SPPMI used towards a word. This can be extended to different contexts like bi-grams or n-grams. The representation can be built at a low cost during NLP tasks, as co-occurrence counts are maintained. GloVe explicitly constructs the co-occurrence matrix before learning point-estimates. Our approach proposes representing sentences as probability distributions to capture uncertainty and polysemy. We believe that a sentence's meaning can be best understood through the simultaneous occurrence of its words. By using the Wasserstein barycenter of distributional estimates of words in a sentence, we can efficiently represent it as a distribution over G. This method offers a nonassociative property that enhances the representation of sentences. The barycenter operation for sentences considers the order of words and represents sentences as histograms of contexts. Using optimal transport, the distance between sentences can be defined based on the Euclidean distance between word embeddings. This approach aims to predict the similarity or dissimilarity of sentence meanings. The curr_chunk discusses the use of Word2vec and GloVe to obtain word embeddings, training on the Toronto Book Corpus. The embeddings are used for K-means clustering to find representative points. Performance is benchmarked against SIF and Bag of Words averaging using SentEval for evaluation. The curr_chunk compares the performance of different methods for sentence representation using SentEval. It shows that their approach outperforms BoW and SIF with weighted averaging on all tasks, and even beats the best variant of SIF on 3 out of 5 tasks. The hyperparameters used are the same for all tasks, and PPMI specific hyperparameters are not heavily tuned. The comparison does not include methods like Sent2vec, which are specifically trained for sentence representation tasks. The curr_chunk discusses the efficacy of their approach for representing barycenters in semantic similarity tasks without the need for additional training. It also explains the concept of hypernymy in linguistics and its relevance in detecting lexical entailment relations for NLP tasks. The curr_chunk discusses lexical entailment relations in NLP tasks, comparing different methods for entailment measures. It includes a comparison between entailment vectors, optimal transport/Wasserstein based measures, and other state-of-the-art methods. The hyperparameter \u03b1 and shift s are mentioned, along with scores from various datasets. The distributional approaches for detecting hyponymy in NLP tasks utilize unsupervised methods that leverage linguistic properties. Word embedding based methods have gained popularity recently, along with approaches using Gaussian distributions and KL-divergence for entailment measurement. These methods, especially in hypernymy detection, capture semantics and uncertainty effectively. The framework in Section 4 defines entailment as optimal transport cost between distributions, relying on a proposed model. The model proposed by BID13 BID12 interprets word embeddings to measure entailment between two vectors. The ground cost D is chosen based on this entailment operator, allowing flexibility with cost functions. The method was evaluated on 9 standard datasets, showcasing its effectiveness in NLP tasks. Our method was evaluated on 9 standard datasets using average precision AP@all as the evaluation metric. We compared our method with entailment embeddings and found significant improvements on most datasets, except for the Baroni dataset. Our method outperforms state-of-the-art methods on several datasets, matching or exceeding their performance. It is not limited to specific entailment vectors and can be used with any embedding vectors and ground cost. Improvements in ground cost or embedding vectors could further enhance performance. Our training dataset is from Wikipedia with 1.7B tokens and a vocabulary of 80,000 words, which is relatively small. We advocate for using a larger vocabulary on a larger corpus to improve results. Associating both a distributional and point estimate for each entity allows us to use optimal transport in problems with a co-occurrence structure. We utilize Python Optimal Transport for computing Wasserstein distances and barycenters on CPU, with a custom GPU implementation using PyTorch for efficiency. We implemented a batched version for barycenter computation using PyTorch, achieving a speedup of about 10x. For optimal transport computations, we use \u03bb around 0.1 and log or median normalization for stability. Clustering is done using kmcuda's efficient K-Means algorithm on GPUs. Code and pre-computed histograms will be made publicly available on GitHub soon. The detailed results of the sentence representation and hypernymy detection experiments will be available soon."
}