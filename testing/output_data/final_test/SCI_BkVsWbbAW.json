{
    "title": "BkVsWbbAW",
    "content": "Despite advances in deep learning, artificial neural networks do not learn like humans. Neural networks struggle with catastrophic forgetting, where they cannot maintain performance on learned tasks when new tasks are introduced. To address this challenge, a new architecture inspired by human memory has been developed. This model utilizes a dual memory system to mimic the hippocampus and neocortex in the brain, maintaining long-term memory through generative replay of past experiences. Experimental results demonstrate the benefits of generative replay and dual memory in continuous learning. The architecture aims to show the benefits of generative replay and dual memory in addressing catastrophic forgetting in neural networks. It mimics human memory characteristics and highlights the connection between sleep and learning. The challenge of catastrophic forgetting is addressed by storing all training data and relearning on it along with new data, enabling continuous learning. Recent works have explored various activation techniques to create sparse, less correlated feature representations in neural networks. However, these sparse encodings can be task-specific and act as heuristics to mitigate the issue of catastrophic forgetting. Natural cognitive systems demonstrate gradual systematic forgetting, where frequently encountered tasks are better retained in memory compared to rarely encountered ones. This gradual forgetting process mimics human memory characteristics and highlights the importance of continuous learning to address catastrophic forgetting in neural networks. Neuroscientific evidence suggests that humans have evolved mechanisms to separately learn new tasks and consolidate learning with previous knowledge to prevent catastrophic forgetting. This separation is achieved in the brain through the evolution of the hippocampus and neocortex. The neocortex specializes in consolidating new information with previous knowledge, while the hippocampus rapidly learns new tasks and transfers knowledge to the neocortex. Experience replay and weight consolidation are essential factors for sequential learning. Experience replay involves replaying data patterns during sleep and waking rest, while weight consolidation protects knowledge in the neocortex by consolidating neural synapses over time. Techniques like freezing neural network weights after learning tasks have been employed to achieve this. The paper proposes a dual-memory architecture to address catastrophic forgetting in sequential learning tasks. The model consists of a short-term memory (STM) and a long-term memory (LTM) to prevent interference between new and previously learned tasks. During sleep, the STM transfers samples to the LTM for consolidation. Our approach, inspired by deep generative models and experience replay, prevents catastrophic forgetting in sequential multitask learning. The model consists of short-term memory (STM) and long-term memory (LTM) to consolidate tasks. The problem setting involves learning tasks T with a model parameterized by weights \u03b8. The model easily generalizes to supervised learning tasks with training examples. Our model prevents catastrophic forgetting in sequential multitask learning by utilizing short-term memory (STM) and long-term memory (LTM) to consolidate tasks. The training algorithm can store some examples from each task, but with limited memory capacity. During testing, the model predicts labels for examples from previously seen tasks to achieve a test loss close to that of a model which learned the tasks sequentially. Experience replay is proposed as a method to prevent catastrophic forgetting in sequential multitask learning. It suggests that replaying experience to a neural network, which must be generative in nature, can help consolidate learned experiences. This approach is more efficient than storing all samples in replay memories, especially with limited memory capacity. Experience replay is a method proposed to prevent catastrophic forgetting in sequential multitask learning by replaying experience to a generative neural network. Non-generative approaches have been tested in small binary input spaces, but sampling random inputs in high-dimensional spaces does not preserve the learned mappings of neural networks. The approach involves a generative model, a feedforward network, and a memory with task IDs and encounter frequencies. The text discusses the use of Deep Generative Replay (DGR) to prevent catastrophic forgetting in sequential multitask learning. A variational autoencoder (VAE) is chosen for the generator, and a task identification system is used to identify repetitions. DGR updates a Deep Generative Model (DGM) with samples from new tasks, allocating memory capacity proportionate to the number of tasks to ensure efficient learning over time. The text discusses how Deep Generative Replay (DGR) prevents forgetting in multitask learning by allocating memory for new tasks while maintaining performance on previous tasks. DGR generates samples from previous tasks, reconstructs data using a VAE, and trains the DGM for robustness against noise and occlusion. This continual learning system aims to quickly acquire new tasks while retaining performance on old tasks. The proposed dual memory network (DGDMN) aims to address the challenge of quickly acquiring new tasks while retaining performance on previously learned tasks. It consists of a long-term memory (LTM) similar to the neocortex for storing information from past tasks, and a short-term memory (STM) like the hippocampus for learning new tasks without interference. The STM is made up of task-specific memories (STTM) that can each learn a unique task, ensuring efficient task allocation and retention. The proposed dual memory network (DGDMN) consists of a long-term memory (LTM) for storing past tasks and a short-term memory (STM) for learning new tasks. The STM is divided into task-specific memories (STTM) for efficient task allocation. The architecture consolidates tasks into the LTM during sleep, using deep generative replay. During testing, predictions are made using STTMs for known tasks or deferred to the LTM for new tasks. The proposed dual memory network (DGDMN) includes a long-term memory (LTM) for storing past tasks and a short-term memory (STM) for learning new tasks. Experiments demonstrate forgetting on sequential image classification tasks with datasets like Permnist, Digits, TDigits, Shapes, and Hindi. Various baselines for catastrophic forgetting are tested alongside the DGDMN model. In the context of sequential image classification tasks, different prevention mechanisms for catastrophic forgetting are explored, including feedforward neural networks, neural nets with dropout, pseudopattern rehearsal, elastic weight consolidation, and deep generative replay. Large networks with excessive parameters are found to adapt better to incoming tasks, masking the severity of forgetting. We evaluated different prevention mechanisms for catastrophic forgetting in sequential image classification tasks. Network architectures were chosen to share parameters among tasks to achieve joint accuracy. Results on Shapes and Hindi datasets are shown, with classification accuracy after training on each task. Various network architectures were used, including NN, PPR, EWC, DGR, LTM of DGDMN, and DropNN with dropout layers. The study evaluated prevention mechanisms for catastrophic forgetting in sequential image classification tasks using various network architectures. Results showed that NN and DropNN forget catastrophically when learning new tasks, while EWC forgets less but slows down learning. DGR and DGDMN outperform all baselines by learning tasks sequentially with the same learner network. The study evaluated prevention mechanisms for catastrophic forgetting in sequential image classification tasks using various network architectures. NN, DropNN, PPR, and EWC showed heavy forgetting on Digits, while DGR and DGDMN retained performance on all tasks by balancing new incoming samples and generated samples. The study compared prevention mechanisms for catastrophic forgetting in sequential image classification tasks. DGR and DGDMN outperformed baselines in retaining average accuracy, while NN, DropNN, PPR, and EWC struggled to learn multiple tasks simultaneously. PPR's use of experience replay did not compare against DGR and DGDMN, as it failed to accurately model the input domain. This highlights the importance of a generative replay mechanism for preserving performance. The study emphasized the importance of generative replay mechanisms for preventing catastrophic forgetting in sequential image classification tasks. Datasets with highly correlated input samples, like Digits, are crucial benchmarks for continual learning algorithms. The efficacy of algorithms like EWC, DGR, and DGDMN was demonstrated in experiments due to their ability to retain performance on tasks with high sample correlation. This contrasts with NN, DropNN, and PPR, which struggled on such tasks. In experiments on EWC, DGR, and DGDMN, it is shown that DGDMN performs similarly to DGR. EWC stagnates and cannot improve on Task 1 once learning has slowed down. DGDMN benefits from revision, learning all tasks up to Task 6 and improving accuracy through revision. DGDMN benefits significantly from revision due to gaining extra samples from the LTM. The study explores the role of dual memory architecture in learning tasks continuously. DGDMN and DGR algorithms were trained on a sequence of 40 tasks, with memory limitations leading to forgetting some tasks. DGDMN and DGR initially perform well but start to drop in accuracy after 10 tasks due to memory constraints. After 10 tasks, DGDMN retains > 40% accuracy while DGR drops below 20% due to consolidating DGM too often. DGDMN uses small STTMs to learn tasks with low error and consolidates LTM with more accurate samples, accumulating errors slower. DGDMN shows gradual forgetting, oscillating around 90% accuracy on recent tasks. The dual memory architecture of DGDMN shows superior performance compared to DGR in terms of accuracy and training time. DGDMN oscillates around 90% accuracy after 10 tasks, while DGR's accuracy drops due to frequent consolidation. The smaller and faster STTMs in DGDMN allow for quicker learning and periodic consolidation, providing a speed advantage. This design choice is critical for scalability and mirrors the natural human learning process. The importance of sleep for organisms with a developed nervous system is demonstrated in an experiment showing that without periodic sleep, learning would be short-lived and time-consuming. DGDMN shares characteristics with human memory and has intuitive hyperparameters. Visualizations of latent structures during training have been deferred due to space constraints. The hyperparameters of DGDMN have intuitive interpretations and simple heuristics for selection. A VAE is used for resilience to noise and occlusion, giving human-like object recognition abilities. The LTM model shows more robustness to noisy and occluded images compared to a NN model. The choice of generative model impacts consolidation and retention performance. The architecture is agnostic to the choice of the underlying generative model as long as it can generate reliable samples and reconstruct incoming samples accurately. Short-term memory errors slowly propagate into long-term memory, contributing to forgetting. Storing data from new tasks, consolidating it into long-term memory, and then discarding it results in higher retention compared to DGDMN. The approach in FIG8 results in higher retention compared to DGDMN in FIG3 because LTM learns from real data. However, it is not truly online as recently learnt tasks cannot be used immediately. Using a high capacity STM for true online continual learning is suggested. Previous works on multitask learning have proposed distilling individual tasks into a larger neural network to improve performance and mitigate inter-task interference. Though not using soft-labels, consolidating tasks into the LTM may lead to refinement. The model developed in this work can continuously learn on sequentially incoming tasks, avoiding catastrophic forgetting by consolidating tasks into the long-term memory (LTM). The architecture draws inspiration from complementary learning systems and experience replay in the human brain, with potential incorporation of synaptic consolidation in the dual memory system in future work. Additionally, plans include extending the architecture to learn optimal policies over time through reinforcement learning without explicit replay memories. Our model utilizes a dual memory architecture inspired by the human brain to prevent catastrophic forgetting and maintain long-term memory through generative replay of past experiences. The architecture shows parallels with the human memory system and highlights the connection between sleep and learning. Deep Generative Replay consolidates new tasks for a DGM with previously learned tasks. The Deep Generative Replay model consolidates new tasks for a DGM with previously learned tasks by computing sampling fractions, generating samples from previous tasks, and training the DGM on resulting data. It also addresses the issue of balancing new and generated samples, unlike other approaches. In this section, experiments on Shapes and Hindi datasets show similar forgetting patterns as on the Digits dataset. DGR and DGDMN were able to learn task structures sequentially, unlike other baselines. Visualizations of t-SNE on the Digits dataset suggest that learning tasks sequentially may result in a similar structure as learning them jointly. The t-SNE analysis on the Digits dataset shows that the LTM effectively separates the 10 digits in both joint and sequential embeddings. While the absolute locations of digit clusters differ, the relative locations show some similarity. Further investigation is needed to determine if the LTM discovers the same latent representation for shared task structures. Visualizations of digits from the jointly trained LTM are sharp, while those from the sequentially trained LTM are not as clear. The sequentially trained LTM produces sharp samples of recently learnt tasks but blurred samples of previously learnt tasks due to partial forgetting. Permnist involved six tasks with fixed permutations on images from the original MNIST dataset. Each task was as hard as MNIST and shared some common underlying structure. The Digits dataset contains 10 tasks for classifying digits from the MNIST dataset. The TDigits dataset is a transformed variant of MNIST with mirror images, upside-down images, and diagonal reflections, totaling 40 tasks. The Shapes dataset includes geometric shapes from the Quick, Draw! dataset. The Hindi dataset consists of 8 tasks with images from the Devanagri dataset. The Devanagri dataset BID13 contains 8 tasks for image classification of Hindi language consonants. Models were trained with RMSProp using specific parameters and batch size. Generative models (VAEs) were trained for 25 epochs. Models were chosen by training them jointly on all tasks to ensure sufficient capacity, with a preference for simpler models. Classifier models included NN, DropNN, PPR, EWC, learner for DGR, and learner for LTM in DGDMN using a neural network with three layers. The classifiers for Shapes and Hindi datasets in DGDMN utilized convolutional layers with specific kernel sizes and max-pooling layers. The models were trained with ReLU activations and a softmax activation for the last layer to minimize cross-entropy. The learners for STTMs in DGDMN were designed to be smaller for efficiency. Generative models (VAE) for DGR and LTM employed encoders and decoders with two fully-connected layers. The generators for DGR and LTM in DGDMN used encoders and decoders with fully connected layers and convolutional variants for different datasets. The size of the latent variable z varied for each dataset. DGDMN introduced two new hyperparameters: \u03ba and n ST M, which have straightforward interpretations and can be set directly without complex searches. \u03ba ensures continual incorporation of new tasks by guaranteeing them a minimum fraction of LTM samples during consolidation. By guaranteeing a minimum fraction of LTM samples during consolidation, \u03ba ensures continual incorporation of new tasks. Choosing \u03ba = 0.5K works well in practice, as observed with K = 10 and \u03ba = 0.05. Increasing nSTM controls consolidation cycle frequency, impacting learning speed and forgetting rates. The parameter for long sequences of tasks should be set at approximately 0.25 \u03ba. A maximum memory capacity of 3-6 times the number of samples in a task is recommended for replaying. Different values of the Fisher Information Matrix regularizer coefficient between 1 to 500 can work well, with 100 chosen for experiments. For experiments, the parameters for DGR and DGDMN were set to ensure enough capacity for dataset regeneration. Memory capacity was deliberately restricted for TDigits to observe long-term task learning effects. Different values of \u03ba were used, with \u03ba = 0.05 for TDigits to incorporate tasks effectively."
}