{
    "title": "rJed6j0cKX",
    "content": "The text discusses the use of Invertible Neural Networks (INNs) for determining hidden system parameters from measurements. Unlike classical neural networks, INNs focus on learning the forward process and use latent variables to capture information. This allows for the implicit learning of the corresponding inverse process. Invertible Neural Networks (INNs) learn implicitly and provide the full posterior over parameter space. They are proven to be a powerful tool for finding multi-modalities in parameter space, as demonstrated on artificial and real-world data from medicine and astrophysics. When analyzing complex physical systems, scientists use forward processes to understand how measurable quantities arise from hidden parameters. However, inferring the hidden states from measurements, known as the inverse process, is often challenging due to lost information. Inverse solvers should estimate the complete posterior of parameters to assess diverse solutions, quantify uncertainty, and identify unrecoverable parameters. This is crucial for applications in various fields. In this paper, the focus is on determining if invertible neural networks (INNs) are suitable for identifying degenerate and unrecoverable parameters in natural science applications. INNs have bijective mappings, efficient computations, and tractable Jacobians, allowing for explicit computation of posterior probabilities. Training them on the forward process x \u2192 y enables obtaining the inverse y \u2192 x without a supervised loss term, which can be problematic when y \u2192 x is ambiguous. The network uses a supervised loss only for x \u2192 y and an unsupervised loss to ensure generated x follows the prior p(x). In this paper, the focus is on determining if invertible neural networks (INNs) are suitable for identifying degenerate and unrecoverable parameters in natural science applications. INNs have bijective mappings, efficient computations, and tractable Jacobians, allowing for explicit computation of posterior probabilities. Training them on the forward process x \u2192 y enables obtaining the inverse y \u2192 x without a supervised loss term, which can be problematic when y \u2192 x is ambiguous. The network uses a supervised loss only for x \u2192 y and an unsupervised loss to ensure generated x follows the prior p(x). Additionally, the INN learns to associate hidden parameter values x with unique pairs [y, z] of measurements and latent variables, shaping the density p(z) of the latent variables as a Gaussian distribution. In this study, invertible neural networks (INNs) are explored for solving inverse problems in natural science. INNs transform the known distribution p(z) to x-space, circumventing difficulties in defining a supervised loss for direct posterior learning. The full posterior of an inverse problem can be estimated with INNs theoretically and practically on various data sets. The study explores the use of invertible neural networks (INNs) for solving inverse problems in natural science, specifically in astrophysics and medicine. The architectural restrictions of INNs do not affect the network's representational power. Combining forward and unsupervised backward training improves results on finite training sets. The approach to learning the posterior with INNs compares favorably to other methods like approximate Bayesian computation (ABC) and conditional VAEs. Modeling the conditional posterior of an inverse process is a classical statistical task that is typically solved using Bayesian methods, although exact treatment of real-world problems is often intractable. Sampling methods, such as Markov Chain Monte Carlo, are commonly used as a solution. Neural networks can approximate complex distributions efficiently for posterior inference, offering a more optimal alternative to traditional sampling methods like Markov Chain Monte Carlo. This approach combines accurate prediction of sufficient statistics with mean-field distribution learning, allowing for differentiation between data and model uncertainty. Generative modeling via learning a non-linear transformation between data distribution and a simple prior distribution has the potential to solve complex distribution problems. This approach, often formulated as a normalizing flow, gradually transforms a normal density into the desired data density. Normalizing flows, introduced by Tabak and Turner (2013), are applied to neural networks by various researchers to learn generative distributions for artificial data and standard image sets. These networks use auto-regressive flows and have shown promising results for conditional modeling as well. Flow-based networks are designed to be invertible in principle but the computation of their inverse is too costly to be practical, limiting bi-directional or cyclic training. Coupling layers are used to build true invertible networks like NICE and RealNVP architectures, which have been shown to be beneficial in generative adversarial nets and auto-encoders. Despite their simple design, these networks have been under-studied but have demonstrated memory-efficient alternatives and the lack of information reduction from input to representation does not cause overfitting. The lack of information reduction from input to representation does not cause overfitting in flow-based networks. Various architectures like RealNVPs, Flow-GAN, and Glow have shown advancements in adversarial training and image manipulations. This research inspired the extension of RealNVPs for computing posteriors in real-world inverse problems in natural and life sciences. The research field provides a model for the forward process with an intrinsic dimension smaller than the nominal dimensions. The inverse model is approximated by a tractable model using a latent random variable drawn from a standard normal distribution. This approach allows for training a full posterior probability model. The model proposes reparametrizing q(x | y) using a neural network with parameters \u03b8. It distinguishes between hidden parameters x and latent variables z. Jointly learning the inverse model g(y, z; \u03b8) and forward model f(x; \u03b8) with shared parameters avoids complications seen in other methods like cVAEs or Bayesian neural networks. The invertible network architecture enforces the relation f = g \u22121, with latent variable z having dimension K = D \u2212 m when m \u2264 M. If M + K exceeds D, input is augmented with zeros. The network expresses q(x | y) with Jacobian determinant J x using coupling layers for simple computation. The architecture consists of reversible blocks for a fully invertible neural network. The proposed network architecture consists of reversible blocks with affine coupling layers, allowing for complex transformations to be learned. The mappings within the blocks can be intricate functions of the input and output vectors, implemented using fully connected layers with leaky ReLU activations. Additional extensions can be applied to increase model capacity. In order to enhance the network's flexibility, padding with zeros is advantageous. Permutation layers are inserted between reversible blocks to shuffle input elements, increasing interaction among variables. Invertible networks optimize losses on both input and output domains simultaneously, allowing for more effective training through alternating forward and backward iterations. In order to optimize losses on both input and output domains simultaneously, invertible networks perform backward iterations in an alternating fashion, accumulating gradients from both directions before updating parameters. The loss function penalizes deviations between simulation outcomes and network predictions, enforcing that the generated latent variables follow a desired distribution and are independent from other variables. The invertible networks optimize losses on input and output domains by performing backward iterations alternately. The Jacobian determinants do not need to be explicitly known, and a theorem is proven in the appendix. Despite asymptotic sufficiency, residual dependency between variables y and z remains after training. A loss Lx is defined on the input side to speed up convergence, matching the distribution of backward predictions. Incorporating Lx does not change the optimum but aids convergence. Padding on network sides requires loss terms to prevent encoding information in extra dimensions. Squared loss is used to keep values close to zero, and noise is added in a training pass to ignore padding dimensions. Maximum Mean Discrepancy (MMD) is a kernel-based method for comparing probability distributions. It is cost-effective, stable, and useful in high-dimensional problems like GAN-based image generation. The method discussed is a kernel-based approach for comparing probability distributions, specifically focusing on the use of heavier-tailed kernels for meaningful gradients with outliers. The Inverse Multiquadratic kernel was found to yield the best results in a basic inverse problem scenario. Adjusting the relative weights of loss terms Lx, Ly, and Lz as hyperparameters is crucial due to the dependence of the Maximum Mean Discrepancy (MMD) on the kernel choice. The inclusion of all loss terms is necessary for accurate conditioning learning, with Lx aiding convergence and fidelity to the prior distribution. The method involves adjusting losses Lx, Ly, and Lz as hyperparameters to achieve balance. INNs are tested on synthetic and real-world problems in medicine and astrophysics. INNs are trained on a Gaussian mixture model for inverse problems, regressing one-hot vectors y using squared loss Ly. The INN learns accurate approximations of posteriors without mode collapse. The coupling block architecture maintains network's power. Bidirectional training is optimal, capturing conditional relationships effectively. Inverse kinematics involves a complex 2D arm movement task. The problem involves a 2D articulated arm with four degrees of freedom moving along a rail. Priors are based on a normal distribution favoring specific angles. The forward process calculates endpoint coordinates, while the inverse problem determines possible inputs for a given endpoint. INN is compared to cVAE, with IAF not improving performance. A hard example with a bi-modal posterior distribution is discussed. The study compares the performance of cVAE and INN in recovering symmetric modes in a 2D articulated arm problem. While both models perform well, cVAE tends to miss the target endpoint by a wider margin due to implicit learning of the forward process. The method is then applied to real-world problems in medicine and astronomy after demonstrating viability on synthetic data. In medical science, the functional state of biological tissue is of interest for many applications, such as detecting changes in oxygen saturation in tumors. Ground truth data cannot be obtained from living tissue, so training data is created by simulating observed spectra from a tissue model involving various parameters. Traditional methods are reliable for learning point estimates of the inverse process and are used in clinical trials. Traditional methods like BID3 are reliable for obtaining point estimates in clinical trials, but they lack the ability to express uncertainty and ambiguity crucial for accurate diagnosis. In this study, an INN, cVAE, and network using BID21 method were trained for the problem, with comparisons made to ABC method. Despite ABC's potential to produce true posterior with enough samples, inconsistencies were observed in the posteriors generated from 50,000 simulations. Learning-based methods showed promise in addressing these limitations. Learning-based methods are trained quickly on a set of 15,000 samples to estimate accuracy and shape of posterior distributions. Error measures include RMSE for point estimates and calibration error for sampling-based methods. Resimulation error is also checked by comparing simulation outcomes to conditioning values. The main parameter of interest is s O2, along with the parameter subspace of s O2, v hb, a mie. The INN method outperforms other methods in point estimate error, with its accuracy decreasing when trained without certain losses. Calibration error is a crucial metric, where INN excels over cVAE-IAF, Dropout, and ABC. Qualitative results show INN's superiority in generating parameter distributions compared to other methods. The INN method, using a sample count of 160,000, produces smooth curves in an astrophysics application. The sampled posterior of 5 parameters shows the ability to recover multiple modes and strong correlations. INN results reveal that certain parameters are unrecoverable from the data. The INN method, with a sample count of 160,000, shows smooth curves in an astrophysics application. It reveals that some parameters are unrecoverable, supported by ABC results. The blood volume fraction and scattering amplitude are strongly correlated due to light absorption in tissue. INNs can estimate the full posterior of an inverse problem theoretically and practically in various fields. The INN method offers advantages such as learning both forward and inverse processes, flexible posteriors, and computational efficiency compared to other methods like cGANs and classical variational methods. Future work will focus on analyzing different properties of the method. In future work, the plan is to analyze invertible architectures and flexible models for inverse problems. Scaling up to higher dimensions is also of interest. The method involves canceling Jacobians using the inverse function theorem, with both supervised and unsupervised losses reaching zero. The supervised and unsupervised losses reaching zero lead to the true posterior distribution being returned by the network outputs. The joint distribution of outputs is obtained by inverting the network and sampling from q*(y, z). The unsupervised reverse loss between the inverted network outputs and the prior data distribution will be 0 if the conditions are met. This justifies using the loss on the prior to speed up convergence without changing the final results. The proposed INN can approximate true posteriors well without hindrance from required coupling block architecture. Existing methods using neural networks of similar size struggle with mode collapse in Gaussian mixture toy example. Larger cGAN with 2M parameters and latent dimension of 128 needed to match INN results. The proposed cGAN has 2M parameters and a latent dimension of 128 to prevent mode collapse. An additional regularization enforces variance matching between generator outputs and training data. Another approach is using an MMD loss instead of a discriminator, which significantly improves results. Comparisons are also made with a conditional Variational Autoencoder (cVAE). The cVAE is compared to a smaller learned discriminator. The INN and cVAE have similarities in training setup, but the cVAE learns the relationship indirectly while the INN requires no reconstruction loss. The cVAE-IAF uses Inverse Autoregressive Flow between the encoder and decoder. Dropout sampling with learned error terms fails to produce multi-modal outputs. The INN fails to produce multi-modal outputs. To analyze the latent space structure, a fixed label y* is chosen, and z is sampled from a grid. Each z is used to compute x through the inverse network, colorizing points in latent space based on distance from the closest mode in x-space. The network shapes the latent space so each mode receives the expected fraction of samples. The dataset is constructed using gaussian priors x i \u223c N (0, \u03c3 i ), with \u03c3 1 = 0.25 and \u03c3 2 = \u03c3 3 = \u03c3 4 = 0.5. The INN has a clear advantage in both re-simulation error and calibration error over the test set. The posteriors generated for less challenging observations y* show the network's performance. The INN trained in Sec. 4.2 is applied pixel-wise to multispectral endoscopic footage to estimate oxygenation levels and measure uncertainty. Lower oxygenation is observed in the small intestine due to clips on connecting tissue. Uncertainty is low in crucial areas and high at edges and specularities. Star clusters form from molecular clouds in the interstellar medium, with star formation influenced by gravity, turbulence, magnetic fields, and radiation. Stellar feedback plays a regulatory role in the process. Astronomers study emission lines from chemical elements like hydrogen and oxygen to understand the impact of energy and momentum from young star clusters on the ISM. The BPT diagrams are key diagnostic tools for this analysis. The WARPFIELD 1D model is used to investigate the dynamical feedback of star clusters on their parental cloud. The WARPFIELD 1D model by Rahner et al. (2017) tracks the temporal evolution of a system until the cloud is destroyed. Radiative transfer calculations are used to create synthetic emission line maps for training a neural network. The model predicts observable quantities from simulation outputs, such as emission line ratios, cloud and cluster mass, and system age. The neural network model predicts a distribution of unobservable properties of stellar populations based on observations. The model identifies ambiguities and correlations in the system, crucial for astrophysical research. Approximating full posterior distributions could lead to breakthroughs in the field. The calibration error of different methods is plotted against confidence levels. Negative values indicate overconfidence, while positive values indicate the opposite. The use of rejection sampling is limited to obtaining samples from the posterior distribution. Two basic ways to obtain samples are through threshold and quantile methods. The threshold method involves setting an acceptance threshold and repeatedly drawing samples until a certain criteria is met. The quantile method involves selecting samples based on a specified quantile. In the quantile method, a quantile q of samples is chosen for acceptance, and N/q simulations are run. The N closest samples to a reference point form the posterior, ensuring a more predictable runtime for costly simulations. Architecture details and dataset summaries are provided in the following sections."
}