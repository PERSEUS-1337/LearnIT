{
    "title": "HJeZNLIt_4",
    "content": "Real NVP is a powerful flow-based model for density estimation, but it struggles with representing discrete structures in data distributions. To address this, a new normalizing flow architecture called Real and Discrete (RAD) has been introduced. RAD utilizes domain partitioning with locally invertible functions to incorporate both real and discrete valued latent variables. This approach maintains the benefits of normalizing flow models while allowing for the simultaneous modeling of continuous and discrete structures in data distributions. Latent generative models like variational autoencoders and generative adversarial networks are commonly used for building expressive generative models. These models assume a smooth distribution over latent variables, making it challenging to model data distributions with discrete structures. Modeling data distributions with discrete structures, such as multi-modal distributions or distributions with holes, poses challenges for latent generative models. To address this issue, mixture models or structured priors have been commonly used. In order to efficiently parametrize the model, mixture models are often formulated as discrete latent variable models. While deep mixture models offer expressivity, they also pose challenges in terms of inference and training, often requiring approximate methods like hard-EM or variational inference. This paper combines piecewise invertible functions with discrete auxiliary variables to address these challenges. The framework combines invertible functions with discrete auxiliary variables to describe a deep mixture model, allowing for both real and discrete valued units in the latent space. This approach enables capturing both continuous and discrete structures in the data distribution while maintaining exact inference, sampling, and log-likelihood evaluation. The goal is to learn a parametrized distribution on the continuous input domain by maximizing log-likelihood, overcoming the challenge of efficiently evaluating log-likelihood in expressive probabilistic models. The proposed approach extends flow-based models by transforming the problem of summation into a search problem. It involves using invertible functions to describe a deep mixture model, allowing for both real and discrete valued units in the latent space. This enables capturing both continuous and discrete structures in the data distribution while maintaining exact inference, sampling, and log-likelihood evaluation. The proposed approach extends flow-based models by using invertible functions to transform distributions in input space. It involves a generative process with piecewise invertible functions, allowing for both real and discrete stochastic structures. The proposed approach extends flow-based models using invertible functions to transform distributions in input space, allowing for both real and discrete stochastic structures. It relies on piecewise invertible functions to define a mixture model of repeated symmetrical patterns, with a gating distribution providing expressive power. The function f K is implicitly defined by f Z, capturing the structure in a sine wave in p K|Z. By factorizing p Z,K as p Z \u00b7 p K|Z, another piecewise invertible map can be applied to Z. By factorizing p Z,K as p Z \u00b7 p K|Z, a deep mixture model can be defined using piecewise invertible maps. This approach allows for a more constrained and efficient gating network, which exploits the distribution p Z in different regions more effectively. The function f K (x) introduces discontinuities, but careful boundary conditions on the gating network counteract this effect. By imposing boundary conditions on the gating network, discontinuities in f K are counteracted to keep log p X (x) continuous with respect to the parameters. A comparison on six two-dimensional toy problems with REAL NVP shows the potential expressivity gain of RAD models. Synthetic datasets are used for grid Gaussian mixture, ring Gaussian mixture, two moons, two circles, spiral, and many moons. The RAD model implementation includes piecewise linear activations in a coupling layer architecture for f Z, where conditioning variables determine parameters for piecewise linear activation. The piecewise linear activation on x 2 is used to obtain z 2 and k 2 in the gating network. A comparison is made with a REAL NVP model using affine coupling layers. Both models can solve generative modeling tasks with enough capacity. The models are studied in a low capacity regime to showcase the potential expressivity of RAD. Each model uses six coupling layers with specific neural network architectures. In comparison to REAL NVP, RAD uses more hidden units per layer and consistently achieves higher log-likelihood in various modeling tasks. The models are trained using stochastic gradient ascent with ADAM on the log-likelihood for a specific number of steps. RAD demonstrates better performance in modeling multimodal data distributions. In various modeling tasks, RAD consistently outperforms REAL NVP in log-likelihood. REAL NVP struggles with mode covering behavior, preferring to model data as one connected manifold, leading to unwanted probability mass between clusters. Flow-based models aim for Gaussianization, transforming data into a Gaussian distribution for better approximation. In Figure 7, Gaussianized variables z are shown for both models trained on the ring Gaussian mixture problem. The Gaussianization process in RAD and REAL NVP models is compared on various datasets. RAD shows better performance in handling non-linear symmetries and clustering of points in the input space. The folding process in RAD results in points that were far apart becoming neighbors in the transformed space z(5). This behavior is not observed in REAL NVP. In the case of RAD, the non-linear folding process bridges gaps in probability distribution and exploits data symmetries. RAD effectively combines different modes of distribution into a single mode, while REAL NVP struggles with this task. RAD decomposes complex structures into simpler components, unlike REAL NVP which faces difficulties in handling such structures. The approach introduces tractable evaluation and training of deep mixture models using piecewise invertible maps as a folding mechanism. This enables exact inference, generation, and evaluation of log-likelihood, improving modeling of datasets with discrete and continuous structures. RAD effectively uses foldings to bridge multiple modes of distribution into a single mode, particularly in the last layers of the transformation. REAL NVP struggles to combine multiple modes under a standard Gaussian distribution using continuous bijections. The log-likelihood in deep probabilistic models is discontinuous due to functions with discrete values and transitions between subsets. Building from a simple scalar case, a piecewise linear function is used to express the log-Jacobian determinant. The scalar case involves a piecewise linear function to express the log-Jacobian determinant. Boundary conditions at x = \u03b2 and z = \u03b1 2 \u03b2 are discussed, with the use of linear pieces to enforce conditions. The inverse is defined using the logit function. The model can be reliably learned through gradient descent methods with constraints on the values of s at the boundaries \u00b1\u03b1 2 \u03b2. The discrete variables k are only interfaced during inference with the distribution p K|Z, resulting in exact and closed form gradient signals. Inference processes of RAD and REAL NVP are plotted on various problems, and final results are compared. Comparison of Gaussianization results from trained REAL NVP and RAD models on different toy problems. REAL NVP fails in low capacity settings by leaving unpopulated areas, showing a failure in modeling the data as one manifold."
}