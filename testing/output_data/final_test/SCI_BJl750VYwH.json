{
    "title": "BJl750VYwH",
    "content": "Learning good representations of users and items is crucial for recommendation systems with implicit feedback. Matrix factorization is commonly used to derive these representations by decomposing the interaction matrix. However, existing approaches have limitations in enforcing the interaction between user and item embeddings. To address this, a novel Augmented Generalized Matrix Factorization (AGMF) approach is proposed in this paper, which incorporates historical interaction information for more effective user and item representations. Experimental results on four public datasets show that AGMF outperforms state-of-the-art methods. In the era of big data, recommender systems are crucial for handling information overload. Collaborative filtering, particularly Matrix Factorization (MF), is a popular method for predicting user preferences. A novel approach called Augmented Generalized Matrix Factorization (AGMF) enhances user and item representations by incorporating historical interaction data. AGMF outperforms state-of-the-art methods in terms of performance, convergence speed, and training loss. Matrix Factorization (MF) projects users and items into a shared latent space using latent features. Various extensions focus on modeling and learning perspectives, such as BPR-MF for implicit feedback and NeuMF for compact embeddings. DeepMF employs deep neural networks for nonlinear interactions but still faces limitations. The limitations of Matrix Factorization (MF) include the lack of explicit relationships between user and item embeddings, which are weakly enforced by fitting individual rating values. In real-world scenarios, user and item embeddings should have explicit connections based on shared high-level descriptions or properties. Enriching user embeddings with features from interacted items and vice versa can improve the model's performance. The SVD++ model enriches user embeddings with additional latent features of interacted items, but neglects to enhance item embeddings with user features. It also averages interacted item features without considering individual user preferences. Other approaches incorporate supplementary information like social relations to regularize or enrich user and item embeddings. The paper proposes a novel Augmented Generalized Matrix Factorization (AGMF) approach for learning from implicit feedback. AGMF enriches user and item embeddings using multi-hot encoding with an attention mechanism on historical items and users. This explicit relationship between embeddings improves the performance of the Generalized Matrix Factorization (GMF) model. Experimental results validate the contributions of AGMF. The MF model, GMF model, and SVD++ model are briefly introduced in the context of learning from implicit feedback data. The goal is to predict unobserved entries in the user-item interaction matrix Y to rank items, despite challenges such as misleading information and user preferences. Matrix Factorization (MF) is a latent factor model used to rank items by characterizing users and items with latent features. It estimates the user-item interaction by the inner product of latent vectors. Generalized Matrix Factorization (GMF) is a nonlinear extension of MF, using deep neural networks to learn a nonlinear interaction function for better performance. The proposed AGMF model extends Matrix Factorization (MF) by incorporating explicit ratings and implicit feedback. It leverages user-specific and item-specific latent vectors to make predictions, with a focus on enriching the user latent vector based on implicit feedback. This approach aims to improve performance by learning a nonlinear interaction function through deep neural networks. The AGMF model enhances Matrix Factorization by using multi-hot encoding for user-item interactions, improving performance compared to the GMF model. This design enriches embeddings by incorporating historical interactions, making it a simple yet advantageous approach. The AGMF model improves Matrix Factorization by incorporating multi-hot encoding for user-item interactions, capturing more historical interactions for better recommendations. It jointly learns user and item embeddings explicitly, avoiding bias towards specific items. The design of the AGMF model is detailed layer by layer, considering the target user, target item, and interacted items. The AGMF model enhances Matrix Factorization by utilizing multi-hot encoding for user-item interactions, incorporating historical interactions for improved recommendations. It projects users and items to latent feature vectors and models interactions using element-wise product, offering a more comprehensive approach compared to traditional methods. The AGMF model improves Matrix Factorization by incorporating historical interactions and utilizing latent vectors for users and items. It performs a weighted sum on the latent vectors from the pairwise interaction layer, considering the attention weights of the target user on interacted items and the target item on interacted users. This approach captures the varying contributions of historical items to the target user and vice versa. The paper introduces the AGMF model, which enhances Matrix Factorization by incorporating historical interactions and utilizing latent vectors for users and items. It defines softmax normalization for interaction scores between users and items using user and item attention models. The attention models, represented by single-layer perceptrons, show satisfactory performance while maintaining simplicity and efficiency. The AGMF model enhances Matrix Factorization by incorporating historical interactions and utilizing latent vectors for users and items. It defines softmax normalization for interaction scores using attention models. The model achieves satisfactory performance while keeping simplicity and efficiency. The predicted interaction score is calculated using a weight vector and a sigmoid activation function. The output of the AGMF model is constrained between 0 and 1, allowing for a probability explanation. The Binary Cross Entropy loss function is employed for learning from implicit feedback data. In this section, the AGMF model is evaluated through experiments on various datasets like MovieLens 1M, Yelp, Amazon Movies and Tv, and Amazon CDs and Vinyl. The importance of multi-hot encoding for generalized matrix factorization is highlighted through ablation studies. The original dataset from MovieLens is used, and four negative instances are sampled per positive instance for training epochs. The AGMF model outperforms state-of-the-art models in the experiments conducted. The AGMF model is evaluated on various datasets like MovieLens 1M, Yelp, Amazon Movies and Tv, and Amazon CDs and Vinyl. Data preprocessing involves filtering out users and items with less than 10 interactions. Explicit feedback is masked to implicit feedback. AGMF is compared with SVD++ and BPR-MF, which are state-of-the-art approaches in recommendation systems. The curr_chunk discusses different recommendation algorithms such as FISM, MLP, GMF, NeuMF-p, and ConvNCF, highlighting their unique approaches and methods. The algorithms are compared based on their performance and use of hyperparameters. The algorithms discussed include FISM, MLP, GMF, NeuMF-p, and ConvNCF, with varying hyperparameters such as learning rate, embedding size, regularization parameter, and batch size. Different models have specific training methods, with AGMF model not requiring a neural network structure. Initialization methods for weight and embedding vectors are also specified for training AGMF. The AGMF model uses Adaptive Moment Estimation (Adam) for training, with a fixed embedding size of 128. To handle large numbers of interacted users/items, the list is truncated to include only the 50 latest interactions. The leave-one-out evaluation method is used to compare AGMF with other approaches. AGMF achieves the best performance in ranking predictions compared to other algorithms, as shown in Table 2 and Table 3 for k = 5 and k = 10. The evaluation criteria used are Hit Ratio (HR) and Normalized Discount Cumulative Gain (NDCG). AGMF outperforms state-of-the-art approaches NeuMF-p and ConvNCF, achieving significantly better performance than GMF. The success of AGMF is attributed to multi-hot encoding with the attention mechanism, providing enriched information for user and item embedding. AGMF's prediction model includes supplementary latent vectors, resulting in superior performance compared to GMF. The evaluation in Table 2 and Table 3 demonstrates AGMF's significant advantage over GMF. AGMF consistently outperforms GMF in terms of evaluation metrics, converges faster, and achieves lower training loss. This is attributed to integrating historical interactions into user and item embedding, revealing the importance of multi-hot encoding for generalized recommendation systems. In this paper, a novel Augmented Generalized Matrix Factorization (AGMF) model is proposed for learning from implicit feedback data. Experimental results show that AGMF outperforms existing models, highlighting the significance of multi-hot encoding for Generalized Matrix Factorization. Future work will focus on improving user and item embedding through better user-item interaction relationships. In future work, the focus will be on investigating better user-item interaction relationships to improve recommendation performance."
}