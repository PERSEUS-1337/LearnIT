{
    "title": "H1OQukZ0-",
    "content": "We propose an efficient online hyperparameter optimization method using a joint dynamical system to evaluate gradients with respect to hyperparameters, even for non-smooth impacts like dropout probability in neural networks. This method has shown effectiveness on two distinct tasks, addressing the increasing training times and computational intensity of hyperparameter search. Recently, authors proposed online hyperparameter optimization techniques where hyperparameters are tuned alongside model parameters through short training runs. These approaches were found to be unstable and require external processes like early stopping for good performance. Modifications were made to ensure stability and efficiency by updating hyperparameters at each time step. The optimization process was extended to include dropout probability optimization. BID0 demonstrated that random search is more efficient than grid search for hyperparameter optimization, avoiding training models with poor values. BID1 improved this with quasi random search, but parameters still need to be pre-selected. BID9 used Gaussian processes to minimize the number of hyperparameter sets to try, all methods are \"black-box\" and assume no internal training knowledge. The work discusses the challenges of optimizing hyperparameters without access to the gradient of the validation loss. Previous techniques like BID2 have limitations in updating hyperparameters effectively. The proposed algorithm aims to address stability issues by incorporating a stable approach in the parameter learning process. The proposed algorithm addresses stability issues in parameter learning by treating the joint learning of parameters and hyperparameters as a dynamical system. Convergence is achieved by adjusting hyperparameters after each parameter update, simplifying the algorithm. Modifications are suggested to enhance optimization speed and robustness, with performance demonstrated on various problems. The goal is to minimize the true expected risk by minimizing empirical risk obtained from a training set, with regularization used to prevent overfitting. Holding out part of the training set helps determine the optimal regularization for improved performance on a validation set. Hyperparameter optimization aims to find the best regularization for model performance on a validation set. The focus is on regularization hyperparameters, not optimization parameters like learning rate. The process involves computing the hypergradient of the unregularized validation loss with respect to the regularization parameter \u03bb and using gradient descent to optimize it. The hypergradient is computed with respect to \u03bb and used in gradient descent. Algorithms differ in how they compute this derivative, with some proposing exact computation while others use approximations near optimal parameters to avoid high memory and time costs. The core idea is to compute approximate updates using fewer optimization steps by optimizing hyperparameters simultaneously with model parameters. BID2 proposed optimizing validation error after K steps of gradient descent with fixed hyperparameters. The K-iterate hypergradient is computed recursively by differentiating the gradient update recurrence. The hypergradient computation can be done recursively by differentiating the gradient update recurrence with respect to \u03bb. Starting from \u03b8 0 = 0, the trajectory of the second system is determined by that of \u03b8 t and does not affect the optimization process. The system may converge slowly, especially when \u03b8 0 is already at the optimal value. The behavior of y t = \u2202\u03b8t \u2202\u03bb (\u03bb 0 ) is studied, with a fixed point of y * = \u2212A \u22121 B representing the true hypergradient \u2202\u03b8 * \u2202\u03bb (\u03bb 0 ). The convergence rate of y t depends on the spectrum of I \u2212 \u03b7A, where a small \u03b7 can lead to slow convergence. Repeating the hyperparameter update process with reinitialization helps in optimization. The proposed formulation maintains a growing history of the dependency of \u03bb on \u03b8, increasing stability. BID2 with K = 1 updates hyperparameters at every gradient step. The hypergradient is estimated by y(\u03bb t ) = \u2212\u03b7 \u2202g T \u2202\u03bb (\u03b8 t , \u03bb t ). Minimizing validation loss over \u03bb is equivalent to maximizing < g V (\u03b8 t ), g T (\u03b8 t , \u03bb t ) > with a specific scaling \u03b7 for the learning rate. The hypergradient defined is proportional to a value that is generally not equal to the true hypergradient. The hypergradients are only proportional when the Hessian is a multiple of the identity, suggesting that first-order methods may fail to converge to a local optimum. Reinitializing y(\u03bb) to the last value obtained using the previous \u03bb can be beneficial, but issues may arise earlier in the optimization. Reinitializing y(\u03bb) to 0 every time favors smaller values, increasing stability. Our proposed method aims to increase stability by constraining y within a ball, affecting future updates without changing the gradient direction. The hyperparameter r is chosen to induce clipping at the beginning of optimization, resembling the BID4 method. The algorithm involves updating \u03b8 based on the gradient and adjusting \u03bb after a warmup period. The optimization process involves updating parameters \u03b8 based on the gradient and adjusting \u03bb after a warmup period. Two different regularizers are optimized: L2 penalty and dropout probability. In neural networks, the sensitivity of the loss function to the norm of weights varies depending on the type of linear layers used. Regularization techniques like L2 penalty can impact the optimization process by stabilizing the norm of weights during training, even though they may not prevent overfitting. The effective learning rate is influenced by the regularization parameter \u03bb, showing that regularization can affect optimization dynamics without necessarily providing a proper regularization effect. Dropout regularization, introduced in BID3 and further studied in BID10, prevents overfitting by preventing co-adaptation of output units in a neural network. It involves keeping or dropping output units based on a probability p, represented by a vector mask m. Dropout regularization prevents overfitting in neural networks by keeping or dropping output units based on a probability p. The training loss is computed using dropout masks, and the dependencies between the state \u03b8 and hyperparameter \u03bb are approximated with finite differences. Various methods are compared for training hyperparameters in an online way. Various methods are compared for training hyperparameters in an online way, including Unroll1-gTgV, UnrollK, and ClipR. These methods are evaluated on models of increasing complexity, starting with a toy problem and ending on a typical deep learning setup. The baseline one-shot hyperparameter optimization approach is also used for comparison. The study evaluates different online hyperparameter optimization methods, including Unroll1-gTgV, UnrollK, and ClipR, on models of increasing complexity. The goal is to simplify hyperparameter optimization by testing the sensitivity of hyper-hyperparameters like the clipping threshold in ClipR. The intrinsic stability of the algorithms is also assessed by comparing performance with and without early stopping. Detailed experiment results are available in the appendix. The study evaluates different online hyperparameter optimization methods on models of increasing complexity. The optimal \u03bb can be computed analytically. Performance on MNIST with a 4-hidden layer network is best with clipping, with or without early stopping. Small unrolls are the most stable. The study evaluates online hyperparameter optimization methods on models of increasing complexity, focusing on the performance of different methods using two metrics. Results show that UnrollK methods struggle with estimating hypergradients when the learning rate decreases, leading to a slight increase in loss. Using a clipping threshold of 2.0 results in only a slight increase in validation loss, while Clip5 converges to the correct value. The study uses a feedforward network with 4 fully connected layers trained on MNIST with regularization and a decaying learning rate schedule. The study evaluates online hyperparameter optimization methods on models of increasing complexity, focusing on the performance of different methods using two metrics. Results show that clipping outperforms other methods, especially when early stopping is not used, displaying higher stability. Clip20 performs better than a fixed \u03bb found through grid search, showing efficiency on more realistic problems. Performance on PTB with an LSTM is shown in Table 3. Finally, a language modeling task using the PTB dataset and a typical LSTM-based network architecture is considered. In a typical LSTM-based network architecture, dropout is crucial to prevent overfitting during training. A decaying learning rate with a multiplicative decay of 0.95 every 5K mini-batches is used. Hyperparameter optimization methods show good performance, with Unrolling methods being the most unstable near convergence due to under-regularization. The Unroll1 method attenuates the effect of under-regularization compared to Unroll1-gTgV by using a lower effective learning rate. Longer rollouts increase stability, while gradient clipping algorithms show low instability. Online hyperparameter optimization does not increase overfitting on the validation dataset. A convergence of the derivative indicates that the system takes time to converge to optimal parameters. During optimization, the hyperparameter value and training/validation loss variations help identify instabilities in the dynamical system's convergence to the true solution."
}