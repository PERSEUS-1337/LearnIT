{
    "title": "BJgklhAcK7",
    "content": "Gradient-based meta-learning techniques are proficient at solving few-shot learning problems but struggle in high-dimensional parameter spaces with low data. Latent embedding optimization (LEO) bypasses these limitations by learning a data-dependent latent generative representation, achieving state-of-the-art performance on miniImageNet and tieredImageNet tasks. LEO captures data uncertainty and optimizes adaptation in a low-dimensional latent space. Meta-learning is a technique that aims to quickly adapt to new information and tasks with few examples. It contrasts with traditional deep learning methods that treat each task independently, leading to data inefficiency. Humans excel at grasping new concepts with minimal examples, and meta-learning seeks to replicate this efficiency in machine learning algorithms. The proposed approach, Latent Embedding Optimization (LEO), aims to improve optimization-based meta-learning by learning a low-dimensional latent embedding of model parameters. This allows for task-specific starting points for adaptation, addressing the challenge of generalization with limited samples in a high-dimensional parameter space. The Latent Embedding Optimization (LEO) approach improves optimization-based meta-learning by learning a low-dimensional latent embedding of model parameters. This enables task-specific starting points for adaptation and better considers the joint relationship between input data. LEO achieves state-of-the-art results on miniImageNet and tieredImageNet datasets, with conditional parameter generation and optimization in latent space being critical for its success. Source code is available at https://github.com/deepmind/leo. The formulation of BID47 involves task instances sampled from a distribution, divided into training, validation, and test meta-sets with disjoint classes. Each task instance consists of training and validation sets with a subset of classes. The training set contains K samples for each class, while the validation set provides an estimate of generalization performance. Model-agnostic meta-learning (MAML) is related to the work discussed, aiming to find a single set of parameters that can be adapted to any novel task from the same distribution. Parameters are adapted to task-specific model parameters using a differentiable function. During meta-training, the parameters \u03b8 are updated through multiple sequential adaptation steps using gradient descent. The learning rate \u03b1 can be meta-learned concurrently in the Meta-SGD algorithm. MAML involves initialization with a set of model parameters, an adaptation procedure for task-specific parameters, and termination after a fixed number of optimization steps. The approach aims to minimize errors on the validation set through instance-specific parameter updates. The primary contribution of this paper is to show the decoupling of optimization-based meta-learning techniques from the high-dimensional space of model parameters by learning a stochastic latent space with an information bottleneck. The process involves differentiating through the \"inner loop\" to minimize errors of instance-specific adapted models on the validation set, referred to as the \"outer loop\" of meta-learning. The steps for Latent Embedding Optimization (LEO) are outlined, including the initialization of parameters and the encoding and decoding processes. Latent Embedding Optimization (LEO) operates in a low-dimensional latent space Z, where model parameters are generated. Unlike MAML, which works in a high-dimensional parameter space \u0398, LEO learns a generative distribution of model parameters. This allows for approximating a data-dependent conditional probability distribution over \u0398 instead of finding a single optimal \u03b8 * \u2208 \u0398. The Latent Embedding Optimization (LEO) operates in a low-dimensional latent space Z, where model parameters are generated. Unlike MAML, which works in a high-dimensional parameter space \u0398, LEO learns a generative distribution of model parameters for approximating a data-dependent conditional probability distribution over \u0398. The architecture enables MAML gradient-based adaptation steps in the learned embedding space, where inputs are encoded to produce a latent code z, decoded to parameters \u03b8 i, and adapted through differentiation of loss with respect to z. Finally, optimized codes are decoded to produce the final adapted parameters \u03b8 i. LEO incorporates model-based and optimization-based meta-learning, producing adapted parameters \u03b8 i conditioned on input data and adapted by gradient descent. The decoder acts as a generative model, mapping a latent code to model parameter distribution. Encoding ensures data-dependent initial latent code and parameters, utilizing a relation network for context-dependency. The architecture of the resulting network is shown in FIG0. The LEO procedure involves instantiating model parameters adapted to each task instance using a data-dependent latent encoding decoded to generate initial parameters. An encoding scheme leveraging a relation network maps few-shot examples to a latent vector, considering context for parameter initialization. The encoding process includes feed-forward mapping of data points and a relation network analyzing pair-wise relationships. The encoding process involves mapping few-shot examples to a latent vector using an encoder network and a relation network. Pair-wise relationships are analyzed to parameterize a class-conditional Gaussian distribution for each class. The encoder and relation network map few-shot examples to a class-dependent latent code in a Gaussian distribution. This code is concatenated to obtain the final latent code, which modulates the top layer weights of the classifier for meta-learning in latent space. The softmax classifier uses latent codes to parameterize a Gaussian distribution for sampling class-dependent parameters. The decoder function maps codes to model parameters, enabling gradient-based optimization of latent codes. The decoder converts latent codes to model parameters for gradient-based optimization. The encoder and relation net learn to provide a data-conditioned latent encoding for classifier initialization and adaptation. Meta-training involves updating encoder, relation, and decoder network parameters based on evaluation results. The decoder network parameters are optimized through meta-training by minimizing a specific objective function. This includes using a weighted KL-divergence term to encourage a disentangled embedding in the latent space. L2 regularization and orthogonality constraints are applied to the model weights to enhance expressiveness. The encoder and relation net aim to output an initialization close to the adapted code to simplify the adaptation process. The regularization term in linear encoder, relation, and decoder networks involves the correlation matrix between rows of \u03c6 d. This method can be applied to any model mapping observations to outputs using a single latent code z. The loss function is not limited to classification and can be any differentiable function computed on training and validation sets. Recent approaches to few-shot adaptation include fast weights, learning-to-learn, and meta-learning. These approaches can be categorized as metric-based, memory-based, and optimization-based methods. Some methods involve using one neural network to produce parameters for another network, with a focus on fast adaptation. Additionally, some approaches meta-learn algorithms for changing biases in deep networks conditioned on few-shot tasks. Meta-learning algorithms for fast adaptation in deep networks include BID11, BID33, MAML, and REPTILE. These approaches vary in how they update model parameters based on few-shot training samples, with some using attention kernels or gradient-based adaptation. Unlike traditional methods, these approaches do not explicitly learn a probability distribution over model parameters or utilize latent variable generative models. Approaches like MAML and BID25 use full model parameters for instance-specific adapted models, while BID52 trains a deep input representation and uses it for adaptation. However, BID52 performs adaptation in a high-dimensional parameter space. Probabilistic meta-learning methods like BID2 and BID12 learn Gaussian posteriors over model parameters for advantages in generalization. Our approach involves learning Gaussian posteriors over model parameters for advantages in generalization. We employ a generative model of parameters and adaptation in a low-dimensional latent space, aided by data-dependent initialization. This differs from other works that use deep parameter generators but lack gradient-based adaptation. Additionally, our method bears similarity to Neural Processes BID9, as both learn a mapping to and from a latent space for few-shot function estimation. The proposed approach involves training a model by optimizing a variational objective without inner loop adaptation. The evaluation focuses on few-shot regression and classification tasks, addressing questions about modeling distribution over parameters, learning from multimodal task distributions, and competitiveness on large-scale benchmarks. The evaluation uses 1D regression tasks with random sine waves or lines and Gaussian noise added to targets. The task involves few-shot regression with added Gaussian noise to targets, making it challenging due to the need to learn a distribution over models to account for uncertainty. Ambiguity arises as both sine waves and lines may fit the data, requiring a generative distribution of model parameters. A 3-layer MLP is used as the underlying model architecture, with the LEO generator producing the parameter tensor based on training inputs and noisy labels. See Appendix A for more details. Samples from a single model are shown in FIG1. In FIG1, samples from a model trained on noisy sines and lines are shown, illustrating how LEO captures uncertainty in ambiguous instances. LEO can sample different models from both families when data can be explained by both sines and lines. Scaling up to 1-shot and 5-shot classification problems using ImageNet subsets is also explored. The miniImageNet dataset is a subset of 100 classes randomly selected from the ILSVRC-12 dataset with 600 images per class. The tieredImageNet dataset is a larger subset with 608 classes grouped into 34 higher-level nodes in the ImageNet hierarchy. The dataset is split into training, validation, and test meta-sets with varying numbers of classes. This split near the root of the hierarchy creates a more challenging regime with less similar test classes. The study addresses issues with generator networks by pre-training a visual representation of data and using the generator to instantiate parameters for a linear softmax classifier. A 28-layer Wide Residual Network is trained with supervised classification using data from the training meta-set. The choice of intermediate feature representation in layer 21 is explained, and details on training, evaluation, and network architectures can be found in Appendix B. The study pre-trains a visual representation of data using a generator network and fine-tunes parameters for a linear softmax classifier. Different adaptation procedures are compared on supervised classification tasks, showing slight improvements in performance. The study evaluates various adaptation procedures on supervised classification tasks, achieving state-of-the-art performance with LEO on 1-shot and 5-shot tasks for miniImageNet and tieredImageNet datasets. LEO achieves state-of-the-art performance on 1-shot and 5-shot tasks for miniImageNet and tieredImageNet datasets, with test accuracies of 63.97% and 79.49% respectively. An ablation study was conducted to assess different components, with detailed results provided. LEO, a meta-learning approach, outperforms previous methods on various tasks by utilizing a stochastic parameter generator for latent embedding optimization and fine-tuning in parameter space. The importance of low-dimensional bottleneck and data-dependent encoding is highlighted, showing the efficacy of the latent adaptation procedure. The encoding process in LEO is crucial for performance, with stochasticity playing a role in miniImageNet but not tieredImageNet. Fine-tuning is significant for the 5-shot tieredImageNet task, showcasing the importance of data-conditional encoding and latent space adaptation. The encoding process in LEO is crucial for performance, with stochasticity playing a role in miniImageNet but not tieredImageNet. Fine-tuning is significant for the 5-shot tieredImageNet task, showcasing the importance of data-conditional encoding and latent space adaptation. The t-SNE projection in Figure 4 demonstrates substantial changes in latent codes during LEO, with encoder outputs forming a distinct cluster from adapted codes. Figure 5 shows curvature and coverage metrics for different models computed over 1000 problem instances from the test meta-set. The LEO procedure introduces qualitatively different structure in latent codes, leading to superior performance over approaches predicting parameters directly from inputs. Adapting latent codes reduces uncertainty in the few-shot regime and allows for more distinct weights for different classes. By optimizing in a lower-dimensional latent space, adapted solutions do not need to be close together in parameter space, allowing for greater control over the function. Curvature measures show that a fixed step in the latent space changes the function more drastically than in the parameter space. The latent bottleneck in the \"gen+ft\" case causes the expansion of space by the decoder, leading to larger steps in \u03b8 space compared to Meta-SGD. This supports the idea that LEO can transport models further. Latent Embedding Optimization (LEO) is a meta-learning technique that uses a parameter generative model to capture a diverse range of parameters for tasks. It achieves state-of-the-art results on challenging classification problems by learning a low-dimensional data-dependent latent embedding and performing gradient-based adaptation in this space. Future work could explore using LEO for tasks in reinforcement learning or with sequential data. The experimental setup involved 1D 5-shot noisy regression tasks with inputs sampled uniformly. Half of the tasks were sinusoids and the other half were lines with added Gaussian noise. The model used was a 3-layer MLP with rectifier nonlinearities and latent codes for parameter generation. Sampling of latent codes and parameters was done during training and evaluation. The encoder, decoder, and relation network were all 3-layer MLPs with 32 units per layer. The bottleneck embedding space size was n z = 16. Biases were not used in any layer. The last dimension of the relation network and decoder outputs are two times larger than n z and dim(\u03b8) respectively. Standard 5-way 1-shot and 5-shot classification setups were used. D tr contains 1 or 5 training examples, while D val contains 15 samples during metatraining. During meta-training, no data augmentation or feature averaging was used. The only exception was in the \"multi-view\" embedding results, where features were averaged over different representations. Feature training closely followed the pipeline of previous studies. Before meta-learning, dataset-specific feature embeddings were trained using a Wide Residual Network. During meta-training, no data augmentation or feature averaging was used. The reduction technique classified images from the meta-training set into corresponding classes. Dropout was used inside residual blocks with L2 regularization, Nesterov momentum, and SGD with a learning rate schedule. Mini-batches were of size 1024 with data augmentation similar to the inception pipeline. For evaluation accuracy, only the center crop was used and resized to 80 \u00d7 80. Table 4 shows the learning rate annealing schedules used to train feature extractors for miniImageNet and tieredImageNet. The encoder and decoder networks had a bottleneck embedding space of size 64, while the relation network had 3 layers with 128 units per layer. Biases were not used in any layer of the networks. Table 5 summarizes this information. The network and decoder outputs are larger than n z and dim(x) to parameterize Gaussian distributions. The \"Meta-SGD (our features)\" baseline used a one-layer softmax classifier. Architecture details for 5-way 1-shot miniImageNet and tieredImageNet are shown in Table 5. A meta-batch of 12 task instances is used in parallel. The relation network in the encoder has minimal computational cost and is used once per problem instance for initial model parameters. Adaptation in latent space within the LEO \"inner loop\" involves 5 steps. In the LEO \"inner loop,\" adaptation involves 5 steps in latent space followed by 5 steps in parameter space. Learning rates were meta-learned similar to Meta-SGD. Dropout was applied independently on feature embeddings in each step. Optimization was done using Adam BID19 to minimize the meta-learning objective over training instances. The optimization process in meta-learning can be challenging, especially with stochastic sampling in latent and parameter spaces. Hyperparameters were chosen through random grid search to maximize meta-validation accuracy, with the best values kept fixed for model performance evaluation. After selecting hyperparameters through random grid search, 5 independent runs were conducted with fixed best hyperparameters. Each run resulted in an average accuracy over 50000 task instances. Training of LEO took 1-2 hours for miniImageNet and 5 hours for tieredImageNet on a multi-core CPU for each of the 5 runs. Feature embeddings were cached before training LEO for an efficient meta-learning process. Training the image extractor was more compute-intensive, taking 5 hours for miniImageNet and a day for tieredImageNet using 32 GPUs. In meta-training, three stages are used: pre-training the feature embedding on the meta-training set, training LEO with early stopping on meta-validation and hyperparameter selection, and meta-learning on embeddings from both meta-train and meta-validation sets with early-stopping on meta-validation. This approach aims for good generalization by favoring models with high performance on excluded classes during feature embedding pre-training. The evaluation process for embedding pre-training involves disabling stochasticity and dropout. Parameters adapted based on L tr Ti are used for inference on a specific task. Few-shot samples are encoded and decoded to initialize the inference model's parameters, followed by adaptation steps in latent and parameter space. The final adapted model is then used for that particular problem instance."
}