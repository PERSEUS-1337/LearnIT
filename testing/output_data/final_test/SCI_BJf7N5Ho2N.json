{
    "title": "BJf7N5Ho2N",
    "content": "Modern federated networks, like wearable devices and autonomous vehicles, produce vast amounts of data daily. This data can be used to enhance user experience, but the scale and diversity of federated data pose challenges in areas like federated learning. A benchmarking framework called Leaf has been proposed to address these challenges, offering open-source datasets and evaluation tools for practical federated learning environments. Learning on data generated in federated networks presents new obstacles due to statistical challenges. Data is heterogeneous across devices with varying data generating distributions and data point quantities. Federated scenarios involve a large number of devices with differing storage, computational, and communication capacities. Variability in hardware, network connection, and power further complicates the learning process. Federated settings face communication bottlenecks due to hardware and network variability, requiring faster on-device inference. Privacy and security concerns must be balanced with statistical accuracy and efficiency in federated data operations. Experimental evaluation of proposed solutions, like federated learning, often lacks thoroughness. The three types of datasets commonly used in federated learning are: artificial partitions of MNIST, realistic but proprietary datasets, and realistic federated datasets derived from publicly available data. Meta-learning is another learning paradigm that could benefit from more realistic benchmarks, especially in federated settings where different devices can be interpreted as meta-learning tasks. In this work, the focus is on bridging the gap between artificial datasets used in meta-learning tasks and more realistic benchmarks, such as Meta-Dataset and LEAF's datasets, which allow for testing multi-task learning methods in regimes with large numbers of tasks and samples. These datasets aim to challenge current approaches in ways that real-world problems would, unlike popular benchmarks like Omniglot and miniImageNet. LEAF is a modular benchmarking framework for learning in massively distributed federated networks of remote devices. It includes open-source datasets, statistical metrics, and reference implementations, allowing for easy incorporation into experimental pipelines. LEAF is a benchmarking framework for federated networks, with core components like Reference Implementations, Metrics, and Datasets module for data preprocessing. It includes realistic federated datasets with natural keyed generation processes from networks of thousands to millions of devices. LEAF consists of three datasets: Federated Extended MNIST (FEMNIST) and Sentiment140 BID8. Pre-processing scripts are provided for each dataset, with plans to add datasets from different domains and increase machine learning tasks. Rigorous evaluation metrics are essential for assessing learning solutions in federated scenarios. LEAF introduces new metrics to evaluate learning solutions in federated scenarios, including performance at different percentiles, computing resource requirements, and weighting accuracy across devices. Considering stratified systems and accuracy metrics is crucial to avoid systematic exclusion of methods. LEAF contains reference implementations of algorithms for federated scenarios, focusing on reproducibility. It aims to expand its methods with community input. LEAF enables qualitative reproduction of results on the Shakespeare dataset for next character prediction. FedAvg method diverges with increasing local epochs, a critical setting to understand before deployment. Using FedAvg implementation, 118 devices are subsampled for rapid prototyping. Results show divergence behavior in training loss with increasing epochs. LEAF provides granular metrics for serving multiple clients simultaneously. In the study, the effect of varying the minimum number of samples per user in Sentiment140 is analyzed. The 25th percentile degrades dramatically with data-deficient users. Systems metrics are evaluated using minibatch SGD and FedAvg for FEMNIST to determine the budget needed to reach an accuracy threshold. Convergence behavior of FedAvg on a subsample of the Shakespeare dataset is also examined, showing comparable test accuracy results. The study analyzes the impact of varying the minimum number of samples per user in Sentiment140. It demonstrates the improved system profile of FedAvg in the communication vs. local computation trade-off. LEAF's modularity is showcased by incorporating its Datasets module into different experimental pipelines to validate personalization strategies in statistically heterogeneous scenarios. The study compares different SVM models for data analysis, including Global-SVM, Local-SVM, and MTL-SVM. Another pipeline evaluates the Reptile algorithm on FEM-NIST and compares it to FedAvg. Results are presented in TAB2, with experiments conducted on binary classification tasks and specific training parameters. LEAF is a benchmarking framework for learning in federated settings, supporting various learning paradigms like federated learning, metalearning, multi-task learning, and on-device learning. It aims to provide a realistic environment for evaluating new solutions in massively distributed networks of devices. Personalized strategies outperform other approaches in the experiments conducted."
}