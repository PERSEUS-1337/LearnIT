{
    "title": "SySaJ0xCZ",
    "content": "Neural networks have been successful for various tasks, but designing well-performing architectures is still done manually by experts. A new method is proposed to automatically search for CNN architectures using a hill climbing procedure and cosine annealing. This method produces competitive results with minimal resources, designing and training networks with an error rate below 6% on CIFAR-10 in just 12 hours on a single GPU. Training for one day reduces error to almost 5%. Neural networks have gained popularity for tasks like image recognition, speech recognition, and machine translation. Designing these networks by hand is time-consuming, leading to interest in automating neural architecture search. Classic optimization algorithms do not apply due to the discrete and conditional nature of the architecture search space. In an effort to automate the design of CNN architectures, evolutionary algorithms and reinforcement learning are being explored. Current methods are either costly or underperforming. A new baseline method is proposed, achieving competitive results with lower computational costs. Additionally, network morphisms are formalized and extended to improve architecture search efficiency. Neural Architecture Search by Hillclimbing (NASH) is a simple iterative approach that applies alternative network morphisms to the current network, trains resulting child networks with short optimization runs, and moves to the most promising child network. NASH finds and trains competitive architectures at a computational cost similar to training a single network. On CIFAR-10, NASH achieves CNNs with an error rate below 6% in roughly 12 hours on a single GPU, reducing to almost 5% after one day. Combining models from different stages of the algorithm can achieve an error of 4.7% within two days on a single GPU. On CIFAR-100, NASH achieves an error below 24% in one day, approaching 20%. Our method, Neural Architecture Search by Hillclimbing (NASH), achieves competitive architectures on CIFAR-100 with an error below 24% in one day, approaching 20% after two days. The approach is easy to use and extend, serving as a basis for future work. The method involves alternative network morphisms, hyperparameter optimization, and automated architecture search. The current research focus has shifted to optimizing architectures using special techniques like training reinforcement learning agents to design convolutional neural networks. These agents sequentially choose layers and parameters to generate network architectures, which are then trained and evaluated on a validation set. In a follow-up work, the RL agent learned to build cells for a neural network with a fixed global structure. Training an RL agent for architecture design is costly, requiring thousands of GPU days. BID6 proposed using network transformations/morphisms in RL to generate new pre-trained architectures efficiently. Our approach is simpler, faster, and achieves better performance compared to existing methods like BID22 and BID26, which use evolutionary algorithms to generate powerful networks iteratively. Network transformations like inserting a layer, modifying parameters, or adding skip connections serve as \"mutations\" in the evolution framework. BID22 and BID26 used significant computational resources, while BID24 starts with a large network and prunes it in the end. BID5 used hypernetworks to generate weights for a randomly sampled network architecture. Network morphism was first introduced in transfer learning to make networks deeper or wider for faster training. Network morphisms, like Net2Deeper and Net2Wider, aim to speed up training and explore network architectures. BID27 introduced additional operations for handling activation functions and altering kernel sizes, coining the term network morphism. BID6 utilized network morphisms for architecture search, focusing on simple architectures without skip connections. A network morphism maps neural networks with parameters to another network, allowing for various transformations like adding convolutional layers. The morphism \"Net2DeeperNet\" allows for adding fully-connected or convolutional layers to a neural network. It can also widen layers by increasing the number of units or channels. This morphism enables modifications to network architectures for improved training and exploration. The Net2WiderNet transformation involves increasing the width of layers in a neural network. Skip-connections by concatenation can be formulated as a network morphism. Idempotent functions can be replaced with arbitrary functions using network morphisms, allowing for incorporation of any function, especially non-linearities. The proposed algorithm involves using network morphisms to transform a small pretrained network into larger ones with potentially better performance. These new networks are considered neighbors of the initial network in the space of architectures. By applying network morphisms, different functions and non-linearities can be incorporated into the network. The proposed algorithm, Neural Architecture Search by Hill-climbing (NASH), utilizes network morphisms to initialize child networks at the same performance level as their parent. This avoids training from scratch, reducing evaluation costs. NASH can iterate this process until performance saturates on a validation set, potentially leading to improved child networks. The NASH approach utilizes network morphisms to initialize child networks at the same performance level as their parent. The function ApplyNetMorph applies network morphisms such as making the network deeper, wider, or adding skip connections. Our algorithm utilizes network morphisms to sample child networks uniformly. The child networks only need to be trained for a few epochs, requiring an optimization algorithm with good anytime performance. We use the cosine annealing strategy for training, annealing the learning rate from \u03bb start to \u03bb end after a certain number of epochs. The method can be seen as a hill-climbing or evolutionary algorithm with a population size of n. Our algorithm utilizes network morphisms for sampling child networks uniformly, resembling a simple evolutionary algorithm with a population size of n. The method is evaluated on CIFAR-10 and CIFAR-100 datasets, comparing it with other automated architecture algorithms and hand-crafted architectures. The training set is split into training and validation sets for experimentation. The algorithm uses network morphisms to sample child networks uniformly, similar to an evolutionary algorithm with a population size of n. Experiments are conducted on CIFAR-10 and CIFAR-100 datasets, comparing with other automated architecture algorithms and hand-crafted architectures. The training set is divided into training and validation sets. The experiments involve baseline tests to evaluate model selection strategies and parameter settings. In this experiment, the algorithm investigates the impact of \"weight inheritance\" on the final performance of the model. The weight inheritance serves as a strong prior on the weights, potentially hindering the new, larger model's ability to improve. The study also measures the overhead of the architecture search process by comparing the time required for generating and training a model. The study compared the time needed for retraining a model from scratch with the time for training the final model. The algorithm turned off cosine annealing during hill climbing and tried different values for \u03bb. Network morphism constraint was also turned off for initializing neighbor networks. Results of these experiments are summarized in Table 1. The hill climbing strategy was successful in identifying improvements. The hill climbing strategy in the experiments prefers larger models and is able to identify better performing models. Performance slightly decreases when models are retrained from scratch, indicating that the algorithm does not harm final performance. The overhead for searching for the architecture is roughly a factor of 3, showing that architecture search can be done in the same order of magnitude as training a single model. SGDR plays an important role in the resulting models chosen by the algorithm, with models trained with a constant learning rate performing similarly to models without any selection strategy. The experiments show that the hill climbing strategy favors larger models and can identify better performing models. Retraining models from scratch slightly decreases performance, indicating no harm to final performance. SGDR is crucial in model selection, with constant learning rate models performing similarly to those without a selection strategy. The correlation between validation and test set performance is higher with SGDR compared to a constant learning rate. The experiments compared different algorithm settings with varying parameters and error rates. Interestingly, the number of parameters decreased, indicating a preference for models without new parameters. The algorithm was also compared against wide residual networks and other automated architecture search methods. Snapshots of the best models were taken during iterations, following the idea of using SGDR for training. The proposed method in BID20 involves training snapshots on both training and validation sets to build an ensemble model. Results in TAB1 show competitive network architectures generated in 12 hours, outperforming most automated methods with additional training time. While not reaching the performance of handcrafted architectures, the ensemble models perform better, providing a simple and cost-effective solution. The ensemble models outperform most automated methods and provide a cost-effective solution. Results on CIFAR-100 show competitive performance with BID22 after one day with a single GPU. Snapshot ensemble and 5-run ensemble models can compete with handcrafted WRN. Our proposed NASH method for automated architecture search, utilizing hill climbing strategy, network morphisms, and training via SGDR, yields competitive results on CIFAR-10 and CIFAR-100 with less computational resources. The algorithm is easily extendable for further improvements in performance."
}