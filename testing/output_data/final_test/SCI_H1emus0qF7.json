{
    "title": "H1emus0qF7",
    "content": "In goal-conditioned hierarchical reinforcement learning, the choice of representation is crucial for effective task solving. A notion of sub-optimality of a representation is developed, with expressions bounding this sub-optimality. These expressions can be translated into representation learning objectives for optimization. Results on continuous-control tasks demonstrate that this approach yields better representations and hierarchical policies compared to existing methods. Hierarchical reinforcement learning relies on goal-conditioned designs where higher-level policies communicate goals to lower-levels for more complex tasks. Representation learning is crucial, mapping state observations to an abstract space for specifying desired goals. Previous works have studied two ways to choose the representation. In hierarchical reinforcement learning, representation learning is essential for mapping state observations to an abstract space for specifying goals. Previous works have explored two approaches for choosing the representation, but there are challenges with both methods. In hierarchical reinforcement learning, representation learning is crucial for mapping state observations to an abstract space for specifying goals. A study explores how unsupervised objectives can train a more concise representation than the full state, balancing compression and information loss for optimal policy expression. The study explores how unsupervised objectives can train concise representations for hierarchical policies, achieving near-optimal performance with bounded error. This is the first result showing that learned representations and temporal abstraction can achieve bounded sub-optimality against the optimal policy. The study shows that learned representations based on mutual information can lead to near-optimal performance for hierarchical RL policies. Results on continuous-control navigation tasks demonstrate the effectiveness of the representation learning objective compared to existing methods. In more general settings, a state representation function is necessary for goals BID17. A two-level hierarchical policy on M consists of a higher-level policy \u03c0 hi (g|s) that samples a high-level action g t every c steps, and a non-stationary, goal-conditioned lower-level policy \u03c0 lo (a|s, g, s, k) that translates high-level actions into low-level actions. The lower-level policy is trained using a goal-conditioned reward based on a distance function. The higher-level policy chooses a goal based on a state, mapped to a nonstationary policy. The mapping \u03a8 is crucial for subsequent analysis, as it identifies low-level behavior desired by the higher-level policy. The higher-level policy selects goals based on states, which are then translated into low-level behaviors through the mapping \u03a8. The choice of representation f impacts the mapping \u03a8, and the environment reward function R is defined only on states. The hierarchy introduced raises the question of the effectiveness of learning a higher-level policy that can only act on lower-level behaviors via \u03a8. The optimal policy on M may not be expressible by the higher-level policy \u03c0 hi due to the restrictions imposed by the mapping \u03a8. Despite this limitation, it is possible to learn a near-optimal hierarchical policy. This is achieved by comparing the optimal hierarchical policy \u03c0 * hier, which uses \u03a8, to an optimal hierarchical policy \u03c0 * agnostic to \u03a8. The latter selects low-level behaviors independently every c steps, allowing for all possible behaviors to be expressed. The full hierarchical policy resulting from this approach is denoted as \u03c0 *. The state values V \u03c0 * hier (s) are determined by the form of \u03a8, which is influenced by the choice of representation f. However, the direct relationship between f and sub-optimality is unclear. By deriving bounds, we establish a more direct link between SubOpt(\u03a8) and f. Our main result shows that defining \u03a8 as a modification of the traditional objective can translate sub-optimality to a practical representation learning objective for f. Proxy expressions are provided in this section to bound the relationship. Our main result, Claim 4, connects the sub-optimality of \u03a8 to goal-conditioned policy objectives and representation learning for the function f. The results are presented in the restricted case of c = 1 and deterministic lower-level policies, where the higher-level policy chooses a high-level action g \u2208 G at every step, translated to a low-level action a \u2208 A via \u03a8. The claims quantify how many possible low-level behaviors can be produced by \u03a8 for different choices of g. Theorem 1 bounds the sub-optimality of \u03a8 in terms of recoverable action effects, emphasizing the importance of invertibility for optimal performance. However, invertibility may not always be feasible or desirable. Theorem 1 bounds the sub-optimality of \u03a8 based on recoverable action effects, highlighting the significance of invertibility for optimal performance. However, it may not always be feasible or desirable to have an invertible \u03a8. Claim 2 connects the sub-optimality of \u03a8 to representation learning and the low-level objective's form, with a bound on sub-optimality when certain conditions are met. Equation FORMULA4 states that representation f should be chosen so dynamics in representation space are given by \u03d5(s, a). Equation 5 connects representation learning, goal-conditioned policy learning, and sub-optimality. The higher-level policy chooses a high-level action g every c steps, transformed to a lower-level behavior policy \u03c0. The auxiliary inverse goal model \u03d5(s, \u03c0) is discussed in the fully general, temporally abstracted setting. The auxiliary inverse goal model \u03d5(s, \u03c0) predicts goals that yield policies inducing similar future state distributions. The divergences between distributions are weighted by w k = 1 for k < c. The sub-optimality of \u03a8 is bounded by C, with a strong statement in Claim 4 regarding low-level objectives. The sub-optimality of \u03a8 is bounded by C, with a strong statement in Claim 4 regarding low-level objectives. If the low-level objective is defined as in Equation 9, minimizing the sub-optimality can be achieved by optimizing a representation learning objective based on Equation 8. This applies to any class of low-level policies \u03a0, including closed-loop or open-loop policies. The mathematical foundations are now in place to learn representations suitable for hierarchical RL. The translation of Equation 8 into a practical training objective for f and auxiliary \u03d5 is elaborated upon, along with the parameterization of policies \u03c0 as input to \u03d5. Training a lower-level policy to match the objective in Equation FORMULA10 allows for learning f and the lower-level policy to optimize a bound on the sub-optimality of \u03a8. The full algorithm pseudocode is provided in the Appendix (see Algorithm 1). The algorithm presented in the Appendix (Algorithm 1) utilizes separate neural networks for representation and auxiliary functions. It samples experiences from a replay buffer to construct a policy \u03c0, simplifying the structure of the function approximator for \u03d5 \u03b8. The set of candidate policies \u03a0 is defined as A c, consisting of c-step, deterministic, open-loop policies. Our proposed representation learning objective simplifies the function approximator structure for \u03d5 \u03b8. The inner objective J(\u03b8, s t , a t:t+c\u22121) is defined, with a learning objective on each s t, \u03c0 \u2261 a t:t+c\u22121. The gradient with respect to \u03b8 is calculated, and the estimation of Equation 14 is discussed using experienced s t+1:t+k and a replay buffer distribution \u03c1. The second term is approximated using a mini-batch S of states sampled from the replay buffer. Our representation learning algorithm involves maximizing mutual information between states st+k and st, guided by an energy function E\u03b8. Key differences from previous approaches include considering actions or policy in the mutual information maximization, using a different energy function based on distance, and being suitable for hierarchical reinforcement learning. Our representation learning algorithm maximizes mutual information between states st+k and st using an energy function E\u03b8. This approach is suitable for hierarchical reinforcement learning and may lead to improved performance in representation learning tasks. Different practical implementations based on our theoretical findings may yield objectives similar to Contrastive Predictive Coding (CPC), with some outperforming others. Standard RL algorithms can be used to optimize a policy \u03c0 st,g (a|s t+k , k) for each state s t , g, by maximizing the low-level reward implied by the optimization equation. Representation learning for RL involves optimizing f, \u03d5 based on Equation 8 to approximate low-level rewards. This approach utilizes sampled actions to represent \u03c0 as input to \u03d5 and interprets the hierarchy in hierarchical RL as an MDP abstraction. Previous works have viewed goal-conditioned hierarchical designs as a form of action representation. In goal-conditioned hierarchical designs, representation learning is approached in a principled manner, translating sub-optimality bounds into a practical learning objective. Theoretical work on state abstraction in RL has a long history, with formalisms introduced to categorize classic approaches such as bisimulation and homomorphism. Our work categorizes classic state abstractions like bisimulation and homomorphism based on preserved information, similar to prior approaches. Exact state abstractions have no performance loss, while approximate variants have bounded sub-optimality. Previous work focused on learning state abstractions but struggled with scalability. Our work introduces bounds for practical representation learning objectives, showing impressive results on challenging domains. Our representation learning algorithm is similar to scalable mutual information maximization objectives like CPC and MINE. We connect mutual information estimators to hierarchical RL and provide theoretical guarantees on sub-optimality of resulting representations. Our representation learning algorithm is evaluated against various baselines, including XY, VAE, E2C, E2E, and Whole obs. The learned representations are compared in a MuJoCo Ant Maze environment. Our method successfully deduces near-ideal representations in a MuJoCo Ant Maze environment, outperforming other approaches in a suite of tasks. It can approach the performance of oracle x, y representations and learn online concurrently with a hierarchical policy. The curr_chunk discusses various continuous-control MuJoCo tasks involving an ant navigating different scenarios by pushing blocks and moving through corridors. The tasks involve raw observations of the agent's coordinates, orientation, and limb positions, with some tasks also including the block's coordinates and orientation. Additionally, experiments are conducted with different raw representations to enhance difficulty. The experiments involve replacing agent coordinates with low-resolution images for representation learning methods like VAE and E2C. The importance of observation coordinates in learned representations is investigated on a block-moving task with a simulated robotic ant. The task includes moving a small red block through a corridor, with observations of ant and block coordinates. Trajectories of learned representations are shown, along with perturbations in ant and block coordinates. The method successfully learns 2D representations of raw observations, emphasizing block coordinates for a block-moving task with a simulated robotic ant. The representation is learned concurrently with a hierarchical policy, even when the behavior policy is suboptimal. Most baseline methods struggle in this setting. Our proposed method outperforms baseline methods in learning representations for a block-moving task. The learned representations focus more on block coordinates than agent coordinates, showing a principled approach to representation learning. Our approach to representation learning in hierarchical RL focuses on achieving maximum return through optimal state values. We establish a mathematical relationship between sub-optimality and representation learning, leading to practical objectives that yield impressive results on high-dimensional tasks. Special thanks to Bo Dai, Luke Metz, and the Google Brain team for their valuable insights and discussions. Our approach to representation learning in hierarchical RL focuses on maximizing return through optimal state values. We establish a mathematical relationship between sub-optimality and representation learning, leading to practical objectives with impressive results on high-dimensional tasks. Using mappings \u03d5 and \u03a8, we transform policies to high-level policies on G. By bounding the total variation divergence between state visitation frequencies of different policies, we analyze k-step state transition distributions. Expressing state visitation frequencies d * , d hier as linear operators, we denote every-c-steps \u03b3-discounted state frequencies as d c * , dc hier. The text discusses the total variation divergence between state visitation frequencies of different policies in hierarchical RL. It analyzes k-step state transition distributions and bounds the divergence using mathematical relationships. The text also considers the difference in values between policies. The text discusses the total variation divergence between state visitation frequencies of different policies in hierarchical RL. It analyzes k-step state transition distributions and bounds the divergence using mathematical relationships. The text also considers the difference in values. Experimental details for Ant Maze, Ant Push, and Ant Fall environments are provided. Target locations are selected randomly during training, and final results are evaluated on a single difficult target point. The Point Maze is a scaled-down version of the Ant Maze, with a point mass controlled by two actions. The 'Images' versions of these environments provide a low-resolution top-down view with immovable blocks, movable blocks, and chasms. The tasks in the paper involve navigation. The tasks in this paper involve navigation in environments like the Ant Block Maze and Ant Block environment, where the agent must move a small block to a target location for rewards. Target locations are randomly sampled during training. The training details for navigation tasks involve randomly sampled target locations at the end of a corridor. Differences from BID17 include using the whole observation for the lower-level policy, a Huber function for distance computation, a goal dimension of size 2, and a Gaussian with standard deviation 5 for high-level exploration. Parameterization includes a feed-forward neural network for f \u03b8 and \u03d5 \u03b8 with specific hidden layer dimensions. The network structure for \u03d5 \u03b8 is similar, with hidden layer dimensions of 400 and 300. The networks are trained using the Adam optimizer with a learning rate of 0.0001. Comparisons are made with variants of the method implemented in the style of CPC. Using a dot product instead of distance function D is found to be detrimental, but distance-based variants may perform similarly. Additional results comparing to variants of \u03b2-VAE BID10 show that the VAE approach does not perform well in complex environments due to reconstructing unimportant details. The VAE approach to representation learning may suffer in environments with high-information state observation features. However, learned representations show robustness in transferring to different tasks and environments, such as \"Reflected Ant Maze\" and \"Ant Push\". The representations can generalize successfully, even when learned with respect to a different task. In a study replicating Figure 2 results with representations learned from random higher-level policy data, a method was able to recover near-ideal representations despite a weak connection between learning and task objectives. The original HIRO formulation used oracle sub-goals based on position only, performing similarly to XY oracle in non-image tasks but worse in complex state observations like images."
}