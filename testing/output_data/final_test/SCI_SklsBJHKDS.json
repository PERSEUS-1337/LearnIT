{
    "title": "SklsBJHKDS",
    "content": "In this work, the focus is on solving data-driven optimization problems by maximizing an unknown score function using a dataset of input, score pairs. Model inversion networks (MINs) are proposed as a solution that can handle high-dimensional input spaces and leverage offline datasets for optimization. MINs can also be extended to the active setting with a simple and effective scheme for data collection. Experimental results demonstrate the effectiveness of MINs as powerful optimizers in various settings. MINs are powerful optimizers for a variety of data-driven optimization problems, including image and protein design, robot and neural net architecture design, and learning from logged feedback. These problems involve optimizing unknown reward functions using collected data without direct access to the function. Existing methods struggle when valid inputs are on a low-dimensional manifold in the input space. In this work, the goal is to develop an optimization approach that can operate on high-dimensional inputs like natural images, utilize offline static data, and minimize active data collection for solving optimization problems involving complex real-world processes. The optimization problem involves unknown function f(x) with access to dataset D={(x1, y1),...(xN, yN)}. It extends to contextual setting optimizing expected score function value across context distribution. Contextual problems with logged datasets have been studied in the context of contextual bandits. Training a proxy function f \u03b8 (x) to approximate the true score using dataset D can lead to issues when the optimizer finds inputs where f \u03b8 (x) outputs erroneously large values, especially in high-dimensional spaces like natural images. Prior work has addressed this problem using uncertainty estimation and Bayesian models for f \u03b8 (x), as well as active data collection. Instead of learning the complex function f \u03b8 (x), the proposal is to learn the inverse function mapping values y to inputs x, termed model inversion networks (MINs). MINs offer benefits like utilizing static datasets, handling high-dimensional input spaces, addressing contextual problems, and accommodating both static datasets and active data collection. Simple active data collection methods for MINs are discussed, leveraging deep generative modeling advances and scaling to high-dimensional input spaces. In this paper, the focus is on solving data-driven optimization problems using model inversion networks (MINs). MINs offer benefits like handling high-dimensional input spaces and outperforming prior methods on contextual optimization from logged data. Various algorithms such as the cross entropy method (CEM) and Bayesian optimization methods have been used in the active setting for optimization problems. MINs can replace GPs in active optimization settings, competing with Bayesian neural networks and latent variable models. They can also handle high-dimensional input spaces effectively, unlike conventional BO methods. Additionally, MINs can be applied to static settings where prior methods are not applicable. In batch contextual bandit problems, MINs outperform existing methods by combining deep generative modeling with algorithmic decisions. Deep generative modeling has been successful in modeling high-dimensional data like images, speech, and text. MINs can effectively handle high-dimensional input spaces and static settings where traditional methods fail. MINs, or model inversion networks, combine generative models with algorithmic decisions for model-based optimization. The design decisions are crucial for adapting deep generative models to optimization tasks. MINs can perform active and passive optimization in high-dimensional input spaces. The goal is to solve optimization problems where the function is unknown but a dataset of input-output tuples is available. The method is presented in a non-contextual setting, but can be extended to a contextual setting by conditioning functions on context. In the active setting, the algorithm queries f(x) to augment the dataset, while in the static setting, only an initial dataset is available. The goal is to find the best x with the highest value of f(x). One approach is to learn a proxy score function f\u03b8(x) and optimize it, but this may fail due to accuracy issues and optimization errors. The second problem arises when x is on a narrow manifold in high-dimensional space, leading to invalid x values and arbitrary outputs from f\u03b8(x). This issue is often overlooked in prior work focusing on low-dimensional domains. The brittleness of the naive approach is due to f\u03b8(x) having a high-dimensional input space, making it easy for the optimizer to find unreasonable outputs. The question is raised if a function with a small input space can implicitly understand valid x values. The approach aims to model an inverse map f \u22121 \u03b8 : Y \u2192 X to generate valid x values from a score value y. By using a stochastic map f \u22121 \u03b8 : Y \u00d7 Z \u2192 X, where z is from a prior distribution p 0 (z), the model can understand valid input values implicitly. Training the proxy inverse map f \u22121 \u03b8 under distribution p(y) involves minimizing an objective function. The MIN algorithm trains a proxy inverse map f \u22121 \u03b8 under distribution p(y) by minimizing an objective function. Different divergence measures can be used, such as Kullback-Leibler or Jensen-Shannon divergence. Adapting MINs to the contextual setting involves learning f \u22121 \u03b8 (y i , z, c i ). Choosing the right distribution p(y) can improve performance. The algorithm is based on training an inverse map and using it for inference to find the x that optimizes f (x). The MIN algorithm aims to optimize f(x) by training an inverse map and generating the best x to maximize the true score function under the dataset. It is important to select the right score y to query the inverse map for near-optimal x, especially in contextual settings where extrapolation beyond the best score is needed. Validity of generated x values is measured by the agreement between the learned inverse map and independently trained models. The optimization process involves measuring agreement between the learned inverse map and an independently trained forward model to find the best output x for a given score y. This optimization aims to extrapolate scores for x values within the valid input manifold where the forward and inverse maps agree, yielding better results than traditional methods. The optimization process involves using an inverse map that substantially constrains the search space, leading to better results than optimizing with a forward model alone. The training objective involves sampling y from the data distribution, with a focus on predicting accurate x values for high y values. Increasing weights on datapoints with larger y values could be considered, or training only on the best datapoint in extreme cases. In the optimization process, the focus is on predicting accurate x values for high y values by sampling y from the data distribution. Training on the best datapoint or increasing weights on datapoints with larger y values is considered for better results. The proposed optimal y distribution p*(y) is discussed, aiming to balance variance and bias in training. The text discusses training under a distribution other than the empirical distribution using importance sampling to reduce bias and variance. Theorem 3.1 suggests a tradeoff between being close to the optimal distribution p*(y) and reducing variance by covering the full data distribution p D. The distribution p(y) that minimizes the bound in Theorem 3.1 is identified. The distribution p(y) minimizing the bound in Theorem 3.1 is a linear function ensuring closeness between distributions p and p*. An exponential parameteric form is empirically chosen to upweight samples with higher scores and reduce weight on rare y-values. In the active setting, choosing a new query point x at each iteration is crucial for finding the best possible optimum. Thompson sampling (TS) is often used as the data-collection strategy, maintaining a posterior distribution over functions and sampling a function to query the point x that minimizes it. TS offers sub-linear Bayesian regret. Thompson sampling offers sub-linear Bayesian regret by maintaining a posterior distribution over functions and sampling a function to query the point x that minimizes it. To approximate Thompson sampling with MINs, we identify a function by the samples it generates and sample synthetic (x, y) points to implicitly define a unique function sample from the posterior. To apply Thompson sampling to MINs, the inverse map is trained with synthetically generated input-score pairs to approximate a function sampled from the posterior. This method, called \"randomized labeling,\" involves obtaining x from the inverse map, labeling it, and updating the dataset. Regret guarantees are derived under mild assumptions, making this method simple, uncertainty-free, and compatible with various function classes. In this section, the authors describe their use of MINs with deep neural network models for high-dimensional inputs. GANs are chosen for modeling the manifold of valid x, as they do not require explicit density modeling and produce realistic samples. The inverse map in MINs needs to model this manifold, making GANs a suitable choice. The authors use GANs to instantiate the inverse map in MINs, with the discriminator trained to output 1 for valid pairs and 0 otherwise. They optimize an objective similar to a cGAN and reweight the data distribution using importance sampling by discretizing the space Y into bins. In the active setting, data collection is performed using a synthetic relabelling algorithm. Two models are trained: an exploration model with synthetically generated samples and an exploitation model with real samples. The augmented dataset is generated by sampling y values from a distribution over high-scoring ys and adding positive-valued noise to promote exploration. Inputs x are sampled from the dataset or uniformly from the input domain. After training, the best possible inputs x are inferred from the trained model using the inference procedure. The goal of the empirical evaluation is to determine if MINs can successfully solve optimization problems in static and active settings, generalize to high dimensional spaces, and if reweighting the data distribution is important for effective optimization. In the data-driven model-based optimization setting, the proposed inference procedure aims to discover valid inputs x with better values than any seen in the dataset. Randomized labeling is explored for active data collection. The method is evaluated on various tasks including batch contextual bandits and high-dimensional contextual image optimization. Several non-contextual tasks involving high-dimensional image inputs are also assessed. The goal is to learn a policy from static data that predicts the correct bandit arm for each context, achieving a high overall score. MINs outperform BanditNet in contextual bandit policies trained on static datasets for classification tasks like MNIST and CIFAR 10. The data is constructed by selecting images from the MNIST/CIFAR dataset as context, a random label as input, and a binary indicator for the score. Different schemes are used for selecting random labels. The average score is reported on new contexts, representing the accuracy of the learned model on a test set. The method is compared to BanditNet on MNIST and CIFAR-10 datasets. The task involves partial feedback where only the correctness of the label is observed. Two datasets are evaluated. MINs outperform BanditNet on MNIST and CIFAR-10 datasets, showing success in contextual model-based optimization. Utilizing the inference procedure results in a 1.5% and 1.0% improvement in test accuracy. The experiment also explores MINs' optimization over high-dimensional inputs. MINs optimize over high-dimensional inputs using an image optimization task with the MNIST dataset. The goal is to produce images with thick stroke width corresponding to valid characters or character classes. MIN optimization generates faces of varying ages through model training and inference, comparing actual age with subjective user rankings. The goal of the algorithm is to optimize high-level features in images, such as maximizing the negative age of a face or the number of disconnected blobs in a digit image. The algorithm, called MINs, successfully generates images that maximize these features, outperforming other methods that directly optimize image pixels. The algorithm also benefits from a reweighting scheme and an inference procedure to achieve better results. The algorithm MINs aims to optimize high-level properties in images, like the negative age of a face. Two versions of the task are created using IMDB-Wiki faces dataset, ensuring the model cannot simply copy the youngest face. Human participants provide ground truth scores for generated faces through binary-choice questions. The algorithm MINs optimizes high-level properties in images, such as age. Results show that the model can produce faces that appear younger than any seen in the training set. Contextual image optimization experiments were also conducted with MINs. Contextual image optimization experiments were conducted with MINs, focusing on maximizing stroke width over hand-written digits using different contexts. Results show MINs achieving higher scores compared to the dataset average. Additionally, a contextual optimization experiment was done on faces from the Celeb-A dataset, optimizing for attributes like brown hair or moustache. The optimization score is determined by attributes like wavy hair, eyeglasses, smiling, and no beard. MINs successfully optimize the score while following the target context. The experiments on contextual training on Celeb-A dataset show that the model can optimize scores for observed and unobserved contexts. In the active MBO setting, MINs need to select new datapoints to improve their estimate of the optimal input. MINs are compared to prior model-based optimization methods, evaluating the exploration technique on benchmark functions. They perform comparably with Bayesian optimization methods based on Gaussian processes, reaching within \u00b10.1 units of the global minimum on standard benchmark problems. However, MINs are less efficient than GP-based methods due to training parametric neural networks with many parameters. Exact Gaussian processes and adaptive Bayesian linear regression outperform MINs in optimization precision and number of samples queried. MINs achieve comparable performance with about 4\u00d7 more samples compared to Bayesian optimization methods based on Gaussian processes. The random labeling exploration method produces better results than the greedy data collection approach for MINs, highlighting the importance of effective exploration methods. In modeling discrete values for protein design, a Gumbel-softmax GAN is used with a temperature \u03c4 = 0.75. The goal is to maximize fluorescence in proteins through score function queries from trained oracles. Comparisons are made with CbAS and other baselines like CEM and RWR. MINs, including CbAS, CEM, RWR, and GB, were evaluated for protein design optimization. Results show that MINs are comparable to the best performing method and produce samples with the highest score. This suggests that MINs can compete with model-based optimization methods, reaching comparable or better performance in protein design tasks. In this work, a novel approach towards model-based optimization (MBO) for higher-dimensional protein design tasks is proposed. MINs learn a stochastic inverse mapping from scores to inputs, enabling optimization over high dimensional values. By re-weighting the data distribution, MINs can effectively optimize from static datasets without active data collection. Experiments demonstrate MINs' capability in solving MBO tasks in various settings, including semantic score functions like age prediction in images. In this work, a method for model-based optimization (MBO) in data-driven scenarios is introduced. It is crucial for settings with expensive data collection and abundant datasets like protein, aircraft, and drug design. MINs, a family of algorithms, show promising results in MBO problems on large input spaces, scaling to high-dimensional tasks. Future work should explore the interaction between active data collection and reweighting in more detail. Future work should investigate reweighting and its consequences for MBO, bandits, and reinforcement learning. Additionally, exploring better inference procedures and different training objectives in MIN optimization is essential. The inference scheme in Section 3.2 can be seen as a deterministic relaxation of a probabilistic inference scheme. Considerations include stochastic inverse maps and probabilistic forward maps in optimization problems. The text discusses the optimization problem involving an inverse map, forward map, Shannon entropy, and KL-divergence. It focuses on maximizing the reconstructed output and the use of Gaussian random variables. The Lagrangian and the importance of specific distributions are highlighted in the context of the optimization problem. In MIN training, the bias-variance tradeoff is analyzed based on the gradient norm in two cases: with access to infinite samples of optimal distributions or with stochastic evaluations. The empirical objective for the inverse map is denoted as -1(yj). The variance of the gradient estimator is analyzed in relation to the importance sampling estimator. The variance in the gradient due to reweighting to a distribution with few observed datapoints is discussed. The text discusses bounding the range of values for the random variable \u2207 \u03b8Lp (D) with high probability using concentration bounds and Hoeffding's inequality. It also addresses the bias in the gradient caused by training on a different distribution than the optimal distribution. The text explains the bias in the gradient due to training on a different distribution than the optimal one. It introduces a randomized labeling algorithm and proves a regret bound for it using Thompson sampling. The TS algorithm queries the true function f at locations and observes true function values. It represents the true function as a distribution over possible functions parameterized by \u03b8. The T period regret over queries is given by a random variable, and Bayes risk is analyzed for stochastic selection of queries. Bayes risk is defined as the expected regret in choosing x t and observing f (x t ) over the prior distribution P (\u03b8 * ). The policy \u03c0 TS queries new datapoints with Thompson sampling. Two assumptions are made: the difference between max and min scores is bounded by 1, and the effective size of X is finite. TS queries the function value at x based on the posterior probability that x is optimal. Thompson Sampling (TS) queries x t from P(x*|D t) using parameters \u03b8 to sample an optimal input at each iteration. The regret in randomized labelling active data collection is bounded by O(\u221aT), following the analysis of TS by Russo & Van Roy (2016). Information Ratio relates TS's expected regret to its expected information gain in reducing the entropy of the posterior distribution of X*. The information ratio in Thompson Sampling is defined as the mutual information between two random variables, capturing the expected reduction in entropy of the posterior distribution of X*. If the information ratio is small, Thompson sampling can only incur large regret when it is expected to gain a lot of information about the optimal x. Russo & Van Roy (2016) bounded the expected regret in terms of the maximum amount of information any algorithm could expect to acquire, which is at most the entropy of the prior distribution of the optimal x. The proof by Russo & Van Roy (2016) shows that the information ratio in Thompson Sampling can lead to sublinear regret if queries contribute to learning about the optimal function. This is achieved by ensuring that the information ratio is lower bounded, leading to a sublinear asymptotic regret in active data collection algorithms. The randomized labeling scheme for active data collection in MINs involves sampling synthetic datapoints to uniquely describe the underlying true function. Eluder dimension is used to characterize the finite number of points needed for this purpose. The MIN algorithm uses synthetic datapoints to train an accurate inverse map for a unique function consistent with the dataset. It represents functions consistent with the data and can take random values outside of the datapoints. The regret from querying optimal points is bounded by the error and the sequence of \u03b5 t values. If a policy satisfies T t=0 \u03b5 t = O( \u221a T ), then the regret from querying optimal x t is E[Regret(T,\u03c0 TS). This lemma shows that querying approximately optimal inputs can maintain sublinear regret. The regret incurred by the MIN algorithm with randomized labeling is O( (\u0393H(X * ) + C)T ). The insight about mutual information I(x , (x t , f (x t ))) > 0 and C.2 is combined to achieve a O( \u221a T ) regret by querying optimal x t s. The curr_chunk discusses achieving sublinear regret by querying optimal x t s from a distribution that shares support with the true posterior. It mentions that bounded errors in fitting the inverse map do not significantly increase regret. Additionally, it highlights the cumulative increase in Bayesian regret due to approximations. Contextual MBO tasks on image pixels are studied in the curr_chunk. The goal is to optimize stroke width on MNIST characters based on the context provided. Results show that MINs correctly produce digits of the right class, achieving an average score higher than the dataset average. MINs are tested on their ability to complete unobserved patches of an image based on observed context, using masks to hide parts of the image. The goal is to produce valid completions while maximizing score attributes like eyeglasses, smiling, wavy hair, and no beard. Results show that MINs are effective in producing visually valid completions with high scores compared to the dataset average. MINs are evaluated on a complex semantic optimization task using a subset of attributes as context on the CelebA dataset. The model produces diverse images consistent with the context and effectively infers the score function. Results are presented in Figures 3 and 7, showcasing optimized solutions for different contexts. Additionally, non-contextual image optimization results are also discussed. MINs were evaluated on a semantic age optimization task using the IMDB-Wiki faces dataset. Comparing the MIN model to a cGAN baseline, it was found that the cGAN model tended to ignore the input score value and behaved like a regular unconditional GAN model when producing images corresponding to the smallest age. This behavior may be due to the lack of direct signal in the age attribute to guide the model. The age attribute lacks direct signal to guide the model, requiring reweighting tricks to enforce attention. Optimal solutions for the youngest face optimization task are shown in Figure 6, with MINs achieving high score values while staying within the valid digit manifold. The cGAN model ignores score values and produces images as an unconditional model. The MIN optimization ignores score values and generates images unconditionally. The optimized solutions align reasonably with the context, even for unseen contexts like brown and black hair. Experimental details and model setup are explained for tasks involving MNIST and benchmark function optimization using a fully connected GAN architecture. The code is based on an open-source implementation by Linder-Nor\u00e9n. In the open-source implementation by Linder-Nor\u00e9n, experiments were conducted using various models such as VGAN for CelebA and IMDB-Wiki faces, fully connected discriminator and generator for MNIST, and convolutional generator with Resnet18-like discriminator for CIFAR-10. Gumbel-softmax trick with a temperature \u03c4 = 0.75 was used for training the model in categorical prediction settings. Protein fluorescence maximization experiment utilized a 2-layer, 256-unit feed-forward gumbel-softmax inverse map. In the experiment, a 2-layer, 256-unit feed-forward gumbel-softmax inverse map and a 2-layer feed-forward discriminator were used. The temperature hyperparameter \u03c4 is adaptively chosen based on the 90th percentile score in the dataset. The scheme can adaptively change temperatures in the active setting by selecting a constant to determine if a bin corresponding to a value of y is small or not. The expression Ny Ny+\u03bb is converted to use densities rather than absolute counts. In the experiment, a 2-layer, 256-unit feed-forward gumbel-softmax inverse map and a 2-layer feed-forward discriminator were used. The temperature hyperparameter \u03c4 is adaptively chosen based on the 90th percentile score in the dataset. The scheme can adaptively change temperatures in the active setting by selecting a constant to determine if a bin corresponding to a value of y is small or not. The expression Ny Ny+\u03bb is converted to use densities rather than absolute counts. The inverse map is trained on continuous y values with a fixed number of bins for reweighting. Two copies of the inverse map are trained side by side in the active setting to prevent instabilities. Training is made more incremental by training both inverse maps for a fixed number of gradient steps."
}