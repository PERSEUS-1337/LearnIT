{
    "title": "rkeZO1BFDB",
    "content": "Vision-Language Navigation (VLN) involves agents navigating in unknown environments using natural language instructions. Previous research focused on the Room-to-Room (R2R) dataset with English instructions, but the goal is to expand to multiple languages. A cross-lingual R2R dataset with Chinese instructions has been created, but collecting instructions for all languages is challenging. A general cross-lingual VLN framework has been proposed to enable navigation in different languages, even without specific training data. The cross-lingual agent with a meta-learner and visually grounded alignment module shows competitive results in zero-shot learning. An adversarial domain adaption loss improves transferring ability with target language data. The study highlights the potential of building a cross-lingual agent for multilingual speakers, especially in Vision-Language Navigation tasks. Existing benchmarks for Vision-Language Navigation tasks are monolingual, hindering non-English speakers. Collecting instructions in various languages is impractical due to the vast number of languages. Language understanding and cross-modal alignment are crucial for completing the VLN task, where agents infer targets from natural language instructions. In this paper, the task of cross-lingual Vision-Language Navigation (VLN) is studied to enable agents to understand multiple languages. The approach involves training the agent with English data and using machine translation to test it on different languages, or translating all English instructions into the target language for training and testing directly. The study focuses on cross-lingual Vision-Language Navigation (VLN) where agents are trained on English data and tested using machine translation for different languages. A meta-learner is introduced to combine human-annotated instructions and their translations for better navigation, addressing the mismatch between training and inference. During training, the agent aligns source human language with target machine translation data. A visually grounded cross-lingual alignment module is proposed to align paired instructions via visual features. This alignment helps alleviate translation errors and improves knowledge transfer from source to target language. The alignment module provides a foundation for solving the circumstances where the agent has access to source language and target language instructions for training. An adversarial domain adaptation loss is introduced to address domain shifting between human-annotated and machine-translated data, improving the model's transferring ability. A cross-lingual VLN dataset (XL-R2R) is collected to facilitate navigation models for instructions in English and Chinese. The contributions include collecting the first cross-lingual VLN dataset, introducing cross-lingual vision-language navigation, and proposing a meta-learning method. The text discusses a meta-learning method for cross-lingual vision-language navigation, utilizing augmented machine-translated data. It introduces a crosslingual alignment module for knowledge transfer and an adversarial domain adaptation loss to improve navigation performance. The task involves an agent following natural language instructions to navigate in 3D indoor environments. The VLN dataset involves an agent following language instructions to reach a goal by taking sequential actions. A cross-lingual VLN agent learns multiple languages to navigate. The XL-VLN dataset extends VLN with bilingual instructions in source and target language domains. The XL-VLN dataset includes source language instructions covering the full dataset and target language instructions with varying training data percentages. The agent must navigate using instructions from either language, even in a setting with no human-annotated target language data for training. The XL-R2R dataset is created for vision-language navigation task, with trajectories in English and Chinese instructions. Chinese instructions are collected through a crowdsourcing platform and annotated by native speakers. The XL-R2R dataset includes 5,798 trajectories with 17,394 instructions in English and Chinese. Chinese instructions are shorter and have fewer long sentences compared to English. Instructions consist of sub-instructions separated by punctuation tokens, with a similar distribution across languages. English vocabulary has 1,583 words, while Chinese has 1,134 words. The XL-R2R dataset includes trajectories with instructions in English and Chinese. Chinese instructions have a higher frequency of nouns and verbs compared to English. A cross-lingual VLN framework is presented in Figure 3, with three novel modules for cross-lingual knowledge transfer. The txt2img module aligns instructions in the visual space, improving cross-lingual alignment. The adversarial domain adaptation module is designed for transferring human-annotated target language instructions. It employs a domain discriminator to distinguish human-annotated from machine translation instructions and aligns the distributions of the two domains using a sequence-to-sequence architecture. The agent encodes natural language instructions with an embedding matrix and LSTM encoder to obtain contextual word representations. The decoder LSTM updates the hidden state using image features and previous actions. An attention mechanism computes a weighted context representation based on the instruction. Machine translation is used to bridge the gap between source and target languages, providing augmented data for zero-shot or low-resource settings. The MT system translates instructions during testing, allowing for training and testing with paired human language and MT instructions. The text discusses a cross-lingual meta-learner that helps an agent decide which language representation to trust more when generating predictions. The meta-learner uses a SoftMax layer to determine the belief in the source language representation at each time step, leading to a final hidden vector that combines representations from two languages for predicting actions. The text discusses aligning two languages to images by mapping representations into a latent space for predicting actions. The loss function includes projection matrices and L2 distance to measure similarity between word and image features. The aligning mechanism aims to align human and machine translation instructions. The aligning mechanism aims to ensure consistency between human and machine translation instructions by mapping representations into a latent space. An adversarial domain adaptation loss is used to make context representations indistinguishable across domains for transfer learning. A sentence vector is computed as the mean of context vectors and forwarded to a domain discriminator through a gradient reversal layer. The gradient reversal layer is used to encourage the language encoder to be domain-agnostic by passing back gradients with opposed signs. The domain adaptation loss is minimized based on domain labels, and different models are tested for zero-shot learning and training with Chinese human annotated data. The effectiveness of the meta-learner and txt2img components for zero-shot learning is demonstrated by comparing with models trained with human-annotated Chinese instructions and machine-translated instructions. Results show a clear gap between the two training methods, with the meta-learner successfully aggregating information from both sources. The txt2img module improves vision-language alignment, aiding in generalization on unseen data. Despite not having access to target annotation data, competitive results are achieved. The proposed adversarial domain adaption module shows consistent improvement over other methods in transferring knowledge from English to Chinese, even without access to target annotation data. Results demonstrate effectiveness in both low-resource and high-resource settings, with the ability to achieve similar performance with only 40% of Chinese human data compared to 100%. The study shows that by pretraining with English data and machine-translated Chinese data, a cross-lingual VLN agent can achieve similar performance with limited Chinese training data. Four models were examined for cross-lingual VLN, each utilizing different encoder-decoder configurations for English and Chinese natural language instructions. The study explores cross-lingual VLN models trained with English and Chinese instructions, showing improved performance over monolingual models. Shared encoder-decoder designs yield competitive results with fewer parameters, indicating the potential of cross-lingual learning for navigation tasks. The meta-learner trusts more on human-annotated Chinese instructions over machine-translated English ones, as shown in Figure 6. At time step 10, the textual attention on the Chinese command makes more sense, assigning 0.25 to \"turn left\" and minimal attention to completed actions. In contrast, the attention on English is less accurate and uniform. Vision and Language Grounding in deep learning has improved computer vision and natural language processing tasks. Various benchmarks like Image and Video caption, VQA, and visual dialog have been proposed to aid research in this area. The focus is on vision-language navigation (VLN), where an agent interacts with the visual environment based on language instructions. In the context of vision-language navigation (VLN), different approaches have been proposed for the task on the R2R dataset. These include a planned-ahead module combining reinforcement learning methods, a speaker for synthesizing new instructions, and methods like Reinforced Cross-modal Matching and self-monitoring. This paper focuses on a cross-lingual perspective, aiming to build an agent that can execute instructions in different languages. Learning cross-lingual representations is essential for natural language tasks. Learning cross-lingual representations is crucial for scaling natural language tasks to all languages. Recent studies have successfully disentangled linguistic knowledge into language-common and language-specific parts. Cross-lingual image and video captioning aim to bridge vision and language for a deeper understanding. This paper addresses cross-lingual representation learning for vision-language navigation in a dynamic visual environment. In this paper, a new task called cross-lingual vision-language navigation is introduced to study cross-lingual representation learning in a dynamic visual environment. A cross-lingual R2R dataset is collected for this purpose, and a cross-lingual VLN framework is proposed to facilitate cross-lingual knowledge transfer. Future directions include incorporating recent advances in VLN and expanding the cross-lingual setting to support multiple languages. The same preprocessing procedure as previous work is followed, using a ResNet-152 pretrained model. The study introduces cross-lingual vision-language navigation and collects a cross-lingual R2R dataset. A ResNet-152 model is used for preprocessing. Image features are extracted with a 2,048-d vector. Instructions are limited to 80 words. The encoder and decoder LSTM have a hidden size of 512. The network is optimized with ADAM optimizer, with a learning rate of 0.001 and a batch size of 100. Domain adaptation loss is trained with an adaption factor. The model is run for 30,000 iterations, reporting the highest SPL iteration. For zero-shot learning, the model is trained with Chinese annotated data but no English annotated data. Results show improvement over MT data only. Transfer learning efficiently transfers knowledge between Chinese and English data, consistent with validation set results. Performance comparison on the English test set is shown in Table 3. Table 3 shows the performance comparison on the English test set for zero-shot learning and models trained with 100% target training data. The meta-learner outperforms a simple ensemble on the validation unseen set, highlighting the importance of \"learning to trust\" in cross-lingual vision-language navigation. Additionally, Table 5 demonstrates the effectiveness of adversarial domain adaptation loss in enhancing knowledge transfer between languages. The training data grows, with the vanilla model benefiting from augmented data until performance plateaus at 40% or 60% data size. Domain adaptation loss shows consistent improvement in meta-learner performance. Results on Chinese human instructions show meta-learner's efficiency in knowledge transfer between languages. Annotated instructions have fewer words and more nouns/verbs compared to machine-translated ones."
}