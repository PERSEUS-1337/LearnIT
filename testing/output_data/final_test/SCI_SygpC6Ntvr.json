{
    "title": "SygpC6Ntvr",
    "content": "Deep representation learning is widely used for visual search, recommendation, and identification. Retrieving representations from a large database is computationally challenging. Instead of compact representations, this work proposes learning high dimensional and sparse representations for efficiency in operations. The approach aims to distribute sparse embeddings uniformly across dimensions to reduce the number of operations quadratically. The proposed approach aims to learn distributed sparse embeddings by minimizing the number of floating-point operations during retrieval. Experimental results show competitive performance compared to other methods, offering a good speed-accuracy tradeoff on practical datasets. Deep neural networks are crucial for various applications, but their high-dimensional dense embeddings pose challenges for nearest neighbor search in large-scale problems. Efficient search in large-scale problems with high-dimensional embeddings is challenging. Dense features from DNNs cannot be efficiently searched through inverted index, leading to the need for approximate methods like Nearest-Neighbor Search (NNS) or Maximum Inner-Product Search (MIPS). Sacrificing accuracy for efficiency is common in high-dimensional search tasks. Inner-Product Search (MIPS) is a key area of research focusing on learning sparse high-dimensional representations for efficient retrieval. Jeong and Song (2018) introduced an end-to-end approach to learn sparse hashes, inspired by a fruit fly's olfactory circuit, showing improved retrieval speed compared to dense embeddings. This approach suggests the potential of hashing in higher dimensions without dimensionality reduction. In this work, the proposal is to learn high dimensional sparse embeddings for efficient retrieval using sparse matrix multiplication operations. Unlike lower-dimensional representations, these sparse embeddings maintain the same representational capacity as dense embeddings. The approach is based on the observation that retrieval of high-dimensional sparse embeddings with non-zero values distributed evenly can be accelerated by a factor of 1/p, where p is the fraction of non-zero values. The approach aims to distribute non-zero values evenly across dimensions to accelerate retrieval by a factor of 1/p. Sparsity alone is not enough for speedup; distribution of non-zero values is crucial. A penalty function on embedding vectors encourages even distribution, reducing floating point operations. The method is applied to large-scale metric learning for facial image embeddings, combining metric learning loss with FLOPs loss for operational efficiency. Empirical evaluation is conducted on the Megaface dataset. The proposed method aims to learn high-dimensional sparse embeddings that are significantly faster, compared to baselines, for exact retrieval of nearest neighbors. The paper discusses the expected number of FLOPs, a continuous relaxation for regularization, and compares the method's performance on a large metric learning task. In practice, high-dimensional dense embeddings from deep neural networks are expensive, leading to the use of approximate nearest neighbors (ANN) for efficient retrieval. Popular ANN approaches include Locality Sensitive Hashing (LSH), Navigable Small World Graphs (NSW), Hierarchical NSW (HNSW), Product Quantization (PQ), and Spectral Hashing. These methods aim to optimize search efficiency by utilizing different techniques such as random projections, clustering, decomposition, and binary hash computation. To speed up query times, various approaches use data structures like hashes, locality sensitive hashes, inverted file structure, trees, clustering, quantization sketches, and dimensionality reductions. Learning the ANN structure end-to-end has gained popularity, with methods like learning binary representations for the Hamming metric and using the signed output of a deep neural network. Sparse representations have been studied from various viewpoints, including exploring sparse neural networks in modeling biological neural networks, learning sparse features using deep belief networks, and exploring sparse coding with an overcomplete basis from a neurobiological viewpoint. Sparse representations have been extensively studied in neural networks, with research focusing on sparsity in auto-encoders and the use of dropout and its variants to impose sparsity. High-dimensional sparse representations have also been explored, such as in sparse deep hashing (SDH) which involves training binary hashes and continuous dense embeddings through alternate minimization steps. The curr_chunk discusses the use of a min-cost-max-flow approach and gradient descent to learn dense embeddings with sparse representations. It mentions k-sparse autoencoders, sparse-lifting approach, and the effectiveness of sparse-lifting compared to traditional techniques like LSH. It also touches on the use of Lasso regularization for imposing sparsity in neural networks. The group lasso and exclusive lasso are extensions of lasso for selecting features in groups. A proposed regularizer focuses on sparsifying embeddings rather than parameters. Existing work on Sparse Matrix Vector Product (SpMV) includes algorithms based on inverted indices, known for cache misses. Linear algebra back-ends like BLAS rely on efficient cache accesses for speedup. In the context of metric learning, various algorithms have been proposed to speed up Sparse Matrix Vector Product (SpMV) computations using specialized hardware like GPUs, FPGAs, and custom hardware. Different settings for learning embeddings have been explored, with a focus on metric learning losses such as large margin softmax loss, triplet loss, and proxy based metric loss. The effect of sparsity on the expected number of FLOPs is also studied in this section. The study focuses on the impact of sparsity on the expected number of FLOPs for retrieval. By utilizing inverted indices on embedding vectors, a speedup of up to 1/p^2 can be achieved. The analysis involves independent training samples drawn from a distribution, leading to a class of functions for computation. Sparse embeddings can lead to a quadratic speedup in retrieval tasks by utilizing inverted indices on embedding vectors. This involves looking at only the non-zero entries of the sparse query vector, resulting in a more efficient computation process. The number of FLOPs incurred scales linearly with the number of vectors in the database. The mean-FLOPs-per-row is minimized when dimensions are non-zero with equal probability. Sparse embeddings can lead to a speedup of 1/p^2 when only a fraction of entries is non-zero and evenly distributed. When comparing FLOPs reduction to speedup, it is important to consider the efficiency of processors like Intel's CPUs and Nvidia's GPUs, which are optimized for dense matrix multiplication. Sparse matrix multiplication may not benefit as much from FLOPs reduction on these processors. However, research on hardware tailored to sparse operations has shown proportional speedup to FLOPs reduction. Modeling hardware aspects such as cache can potentially improve performance. The colored cells in the sparse matrix denote non-zero entries, with arrows indicating the list structure for each column. The green cells represent accessed entries, while grey cells represent non-accessed entries. The 1 regularization induces sparsity but may not distribute non-zeros uniformly for optimal speed-up. Incorporating FLOPs directly into the loss function for optimal trade-off between search time and accuracy. Optimizing a continuous relaxation of the discontinuous FLOPs function. Estimating FLOPs based on samples to minimize loss while controlling expected FLOPs. The regularized loss function considers a parameter \u03bb for the FLOPs-accuracy tradeoff, with a continuous relaxation proposed for optimization using stochastic gradient descent. During inference, a sparse vector of a query image is obtained from the model for nearest neighbor search. During inference, a sparse vector of a query image is obtained from the model for nearest neighbor search. An efficient algorithm is used to compute the dot product of the sparse query vector with a database of sparse vectors forming a sparse matrix. The top scoring candidates are shortlisted and re-ranked using dense embeddings to ensure efficient processing. Comparing this approach to SDH (Jeong and Song, 2018), which uses binary hashes, highlights the differences in methodology. Our approach learns sparse real valued representations, contrasting with SDH's binary hashes. While SDH uses a min-cost-max-flow approach, we train using only SGD. In SDH, a shortlist of candidates is created based on hash intersections, while our approach shortlists based on top scores from sparse embeddings. We experimentally compare our continuous relaxation based FLOPs regularizer to its unrelaxed variant, showing similar performance. In the context of learning sparse real-valued representations, recent DNN analyses suggest that the output of a batch norm layer approximates a Gaussian distribution. This allows for simplifying assumptions in the activation function, which can be experimentally verified. The parameters of the model can be further simplified by assuming independence, leading to an analysis of the minimization process. In the context of learning sparse real-valued representations, recent DNN analyses suggest that the output of a batch norm layer approximates a Gaussian distribution. Simplifying assumptions in the activation function can be experimentally verified. Analyzing the minimization process involves considering population quantities instead of empirical ones. Gradient descent is used to minimize three quantities w.r.t. \u03b8. The trajectory of activation probabilities during optimization is shown in Figure 2b. In contrast to the 1-regularizer, F and F tend to sparsify the less sparse activation at a faster rate, encouraging an even distribution of non-zeros. F promotes orthogonality, and when embeddings are normalized, minimizing F is equivalent to promoting orthogonality on the absolute values of the embedding vectors. Metric learning losses minimize interclass dot product, while FLOPs regularizer minimizes pairwise dot products. The FLOPs regularizer aims to minimize pairwise dot products, leading to a tradeoff between sparsity and accuracy. This approach resembles spreading vectors to uniformly distribute embeddings on the unit sphere. Maximizing pairwise dot product reduces FLOPs, as shown in a toy example. The CDF of activations closely resembles a Gaussian random variable, and F sparsifies activations faster than the 1-regularizer. The FLOPs regularizer aims to spread out non-zero activations in all dimensions, producing balanced embeddings. Embedding dimension should be smaller than the number of training classes to avoid collapse. The proposed approach is evaluated on the Megaface dataset for face recognition, which has 85k classes for training. Having a large number of training classes helps prevent the model from overfitting to seen classes and not generalizing to unseen ones. The study evaluates face recognition on the Megaface dataset with 1 million images across 85k classes. Two network architectures, MobileFaceNet and ResNet-101, are experimented with different activation functions. The models are implemented in Tensorflow with a sparse retrieval algorithm in C++. The re-ranking step utilized 512-d dense embeddings with non-linear activation functions like ReLU and soft thresholding. The bias parameter in the layer before the activation controls sparsity. Setting a large regularization weight from the start is detrimental for training as it leads to quick sparsification. The regularization weight \u03bb is gradually increased during training to prevent dead activations and local minima. The proposed F-regularizer is compared with baselines like SDH, PCA, LSH, and PQ on 512-dimensional embeddings. Training the SDH model involves using architectures with active hash functions. The study compares the proposed F-regularizer with SDH, PCA, LSH, and PQ on 512-dimensional embeddings using various hyperparameters. Trade-off curves for MobileNet and ResNet are presented, showing the effectiveness of the F regularizer in achieving optimal performance. The study also includes details on training hyperparameters and hardware used. The study compares the proposed F-regularizer with SDH, PCA, LSH, and PQ on 512-dimensional embeddings. SDH has a poor speed-accuracy trade-off due to an increase in retrieval time with more active bits. PQ shows the best speed-up compared to LSH and PCA. The sub-optimality ratio R sub is computed over the dataset D, where R \u2265 1 and optimal R = 1 is achieved when non-zeros are evenly distributed across dimensions. The study compares the F-regularizer with other methods on 512-dimensional embeddings. F-regularizer produces more balanced non-zero distributions compared to 1-regularizer. MobileNet shows a smaller gap in recall values between 1 and F models, while ResNet exhibits a larger gap. In this paper, a novel approach to learning high dimensional embeddings is proposed to improve efficiency in retrieval tasks. The approach integrates FLOPs into the loss function as a regularizer and optimizes it for a more even distribution of non-zero activations. Experimental results show that this approach leads to a better speed-vs-accuracy trade-off compared to other baselines, making sparse embeddings around 50\u00d7 faster than dense embeddings. All images were resized to size 112 \u00d7 112 and aligned using a pre-trained aligner. The models were trained on NVIDIA Tesla V-100 GPUs using SGD with specific parameters. Batch sizes and training steps varied between architectures. Pre-training in SDH was conducted similarly to the main training process. The MobileFaceNet model was trained for 150k steps with a batch size of 256. The number of active bits k = 3 and pairwise cost p = 0.1 were set as hyper-parameters. A heuristic was used for re-ranking candidates, shortlisting based on a confidence threshold of 0.25 and top k scorers. All models were trained on 4 NVIDIA Tesla V-100 GPUs with 16G of memory, system memory was 256G, and CPU was Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz. The experiments were conducted on GPUs with 16G memory, 256G system memory, and Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz. Results showed a significant performance drop without re-ranking, with FLOPs regularizers showing a better trade-off curve. Face verification performance was evaluated using the FPR-TPR curve in addition to recall@1 metric. The FLOPs regularizer showed better performance in face verification compared to the baseline method, as demonstrated by the FPR-TPR curves. The curves with similar probability of activation showed that the FLOPs regularizer learned richer representations for the same sparsity. The FLOPs regularizer improves face verification performance by learning richer representations for the same sparsity. ResNet models show smaller gaps between sparse and dense models, indicating better representation learning. LFW dataset is easier compared to AgeDB. Experimentation on Cifar-100 dataset with triplet loss and embedding dimension of 64. Comparisons made between 1 and FLOPs regularized approaches with sparse deep hashing approach. In experiments with Cifar-100 dataset using triplet loss and embedding size d = 64, the models achieved higher precisions compared to previous work. The models used less computation than SDH with slightly lower precision. The results are reported in Table 1, showing FLOPs-per-row values without re-ranking."
}