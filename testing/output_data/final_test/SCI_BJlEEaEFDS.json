{
    "title": "BJlEEaEFDS",
    "content": "Batch Normalization (BatchNorm) is effective for improving deep neural network training but vulnerable to adversarial perturbations due to using different normalization statistics during training and inference. Robust Normalization (RobustNorm) is introduced as a solution that is resilient to adversarial attacks while retaining the benefits of BatchNorm. In computer vision tasks like image classification and semantic segmentation, deep neural networks are vulnerable to adversarial examples. Defense mechanisms against these perturbations have been a focus of recent research. Batch Normalization has been widely adopted in deep learning for stable training and higher generalization accuracy. Batch Normalization (BatchNorm) is used to eliminate internal covariate shift during training. It has been successful due to reasons like avoiding exploding activations and reducing sensitivity to initialization. However, recent studies have shown that BatchNorm can also make deep neural networks more vulnerable to adversarial attacks. Removing BatchNorm can improve robustness against perturbations but comes at the cost of sacrificing benefits like higher learning rates and faster convergence. In this paper, a new perspective on the adversarial vulnerability of the BatchNorm layer is proposed. The use of different normalization statistics during training and inference phases is identified as the cause. Experiments show that removing this aspect improves network robustness by 20%, which can be further enhanced by up to 30% with adversarial training. However, removing the tracking part leads to a drop in test accuracy on clean images. To address this issue, Robust Normalization (RobustNorm or RN) is proposed. Robust Normalization (RobustNorm or RN) is proposed as a solution to the adversarial vulnerability of the BatchNorm layer. It significantly improves the test performance of adversarially-trained DNNs and achieves comparable accuracy to BatchNorm on unperturbed datasets. The experiments conducted demonstrate better adversarial robustness performance on perturbed examples for both natural and adversarial training. The DNN minimizes a loss function with training data to generate a feature representation for classification. The adversary aims to add perturbation to create an adversarial sample that looks similar to the true image but has a different label. Adversarial attack approaches like FGSM and BIM are used to add perturbations. Adversarial attack methods like Basic Iterative Method (BIM), Carlini-Wagner attack (CW), and Projected Gradient Descent (PGD) are used to generate adversarial samples by adding perturbations to the true image. BIM extends FGSM by applying it multiple times with a smaller step size. CW minimizes an objective function to create perturbations, while PGD perturbs the image for a total number of steps with smaller step sizes and projects the adversarial example back onto the -ball of the normal image if it goes beyond. Adversarial training using PGD-based method is effective in improving the robustness of DNNs against various attacks. Different network architectures like Resnet and VGG are used with CIFAR10 and CIFAR100 datasets. Gaussian noise is also used for comparison. In this section, the BatchNorm layer is briefly explained. It is used in training with clean images and adversarial training with a PGD-based method. Different learning rates are used depending on the scenario, with networks trained for 164 epochs. Robustness evaluations are conducted with various noise levels and iterative attacks. BatchNorm layer is introduced to address internal covariate shift during training by normalizing mini-batches. It calculates mean and variance, performs activation normalization, and learns per-channel linear transformations using trainable parameters. The model tracks moving averages of mini-batch statistics during training and uses them during inference. BatchNorm layer is known for its accelerated training properties and higher clean test accuracy, but it lacks robustness to adversarial perturbations. Removing BatchNorm can improve robustness but results in loss of desirable properties like high learning rate and faster convergence. A normalization method that maintains robustness and desirable properties is needed. During training, mini-batch statistics are used to normalize activations and track mean and variance for inference. Different values for mean and variance are used during training and inference. Recent works highlight the link between distributional shift in input data and robustness to adversarial perturbations. Adversarial robustness is sensitive to changes in input data distribution. The BatchNorm layer's adversarial vulnerability is attributed to drift in input distributions caused by tracking representations during training and inference. Extensive experiments were conducted using different normalization layers and architectures, including adversarial training with PGD attack. Results are shown in Table 1. The BatchNorm layer's vulnerability to adversarial attacks is due to tracking representations during training and inference. Removing tracking can significantly increase the network's robustness, but it also reduces clean data accuracy. Using BatchNorm without tracking retains its benefits, but clean accuracy decreases compared to BatchNorm. The success of BatchNorm is not solely due to alleviation of internal covariate shift (ICS), as recent studies have shown. Recent research has shown that BatchNorm works by preventing activation explosion, allowing for training with large learning rates. Min-max rescaling, commonly used in preprocessing, helps eliminate outliers in activations. Comparing BatchNorm with RobustNorm, the latter shows better accuracy without tracking and similar accuracy with tracking. We introduce Robust Normalization (RobustNorm or RN) as a modification to BatchNorm for better convergence. By using a hyperparameter 0 < p < 1, we found that p = 0.2 generalizes well for many networks and datasets. This normalization method, RobustNorm, is more effective than BatchNorm in suppressing activations. Tracking is not used for RobustNorm, but running averages of mean and denominator are kept for comparison purposes. Robust Normalization (RobustNorm or RN) is introduced as a modification to BatchNorm for better convergence, using a hyperparameter 0 < p < 1. RobustNorm maintains running averages of mean and denominator for comparison purposes. Results show better clean accuracy of RobustNorm in natural and adversarial training scenarios, outperforming BatchNorm. It also exhibits improved performance in adversarial robustness, as demonstrated in Figure 2 with different attacks on CIFAR100 dataset. Further results on various networks and datasets can be found in Tables 4 and 5. Figure 3 illustrates the validation loss and accuracy evolution during PGD based adversarial training on Resnet20 architecture with CIFAR100 dataset. The training loss and accuracy evolution is normal, but validation metrics vary with different normalizations and random restarts. The loss landscape in Figure 9 shows different minima shapes for various normalizations. RobustNorm enhances neural network robustness compared to BatchNorm, even under increased adversarial noise as shown in Figure 6. The adversarial noise effect on CIFAR100 dataset is shown in Figure 6. Tracking in BatchNorm is highlighted as a necessary evil with benefits. RobustNorm's results can be restored by increasing batch size. Adversarial examples deceive neural networks, with BatchNorm being a vulnerability. Investigation reveals that tracking in BatchNorm causes this vulnerability. Based on the investigation into the vulnerability caused by tracking in BatchNorm, RobustNorm was proposed to enhance neural network robustness. Results of experiments on CIFAR10 and CIFAR100 datasets, including the impact of increasing adversarial noise on CIFAR100, were provided. The necessity of tracking in BatchNorm was discussed, with a call for further investigation. Additionally, recent studies have challenged the ICS hypothesis, suggesting a need for principled exploration. The results suggest exploring different normalization schemes for improved performance. RobustNorm with tracking is introduced as a new scheme, challenging the ICS hypothesis. Training loss converges similarly for all norms, but BatchNorm shows signs of overfitting. The study explores different normalization schemes for improved performance. BatchNorm shows signs of overfitting, while RobustNorm with tracking challenges the ICS hypothesis. Validation loss varies significantly across random restarts, indicating differences in generalization ability between networks with sharp vs flat minima. In the debate on generalization ability of sharp vs flat minima, more work in this direction can lead to a better understanding of how BatchNorm causes vulnerability. Experiments were conducted to understand the role of power in RobustNorm, showing the effect of changing hyperparameter p on clean and robust accuracy. Results indicate that tuning hyperparameters can improve both robustness to attacks and accuracy."
}