{
    "title": "S1e2agrFvS",
    "content": "Message-passing neural networks (MPNNs) have been successfully applied in various real-world applications. However, their aggregators have weaknesses in representing graph-structured data, such as losing structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. To address these weaknesses, a novel geometric aggregation scheme for graph neural networks is proposed, leveraging insights from classical neural networks and network geometry. The scheme is permutation-invariant and includes three modules: node embedding, structural neighborhood, and bi-level aggregation. An implementation of the scheme in graph convolutional networks, called Geom-GCN, is also presented. The Geom-GCN scheme is implemented in graph convolutional networks for transductive learning on graphs, achieving state-of-the-art performance. Message-passing neural networks like GNN, ChebNet, GG-NN, and GCN are powerful for learning on graphs by sending and aggregating \"messages\" in neighborhoods. MPNNs use permutation-invariant aggregation functions to update node feature representations. MPNNs use permutation-invariant aggregation functions to learn representations that are invariant to isomorphic graphs. However, the aggregators in MPNNs lose structural information of nodes in neighborhoods, as they treat all messages from the neighborhood as a set, resulting in a loss of information from different nodes. After aggregation in MPNNs, structural information of nodes is lost, leading to the inability to distinguish contributions from different nodes. Unlike MPNNs, CNNs use aggregators with a defined structural receiving field on grids, allowing them to distinguish each input unit and extract clues regarding topology patterns in graphs. This structural information is crucial for learning more discriminating representations for graph-structured data. MPNNs aggregate messages from nearby nodes, leading to similar representations for proximal nodes in assortative graphs. However, they may not be suitable for disassortative graphs where node homophily does not hold. MPNNs struggle to capture important features from distant nodes in graphs with high structural similarity but far apart nodes. To address this limitation, a multi-layered architecture can be used to receive messages from distant nodes. Unlike CNNs, multi-layer MPNNs find it challenging to learn good representations for disassortative graphs due to the mixing of relevant and irrelevant messages from proximal nodes. In multi-layer MPNNs, relevant information may be \"washed out\" by irrelevant messages from proximal nodes. Representations of nodes become very similar, carrying information about the entire graph. This paper addresses these weaknesses by leveraging classical neural networks' stationarity, locality, and compositionality in a continuous space, along with the concept of network geometry to bridge the gap between continuous space and graphs. The paper proposes a novel aggregation scheme for graph neural networks, called the geometric aggregation scheme. It aims to map a graph to a continuous latent space using node embedding and utilize geometric relationships in the space to build structural neighborhoods and capture long-range dependencies in the graph. The Geom-GCN scheme utilizes geometric relationships in a latent space to create structural neighborhoods for aggregation in graph neural networks. It ensures permutation invariance for graph data and extracts more structural information, allowing for feature aggregation from distant nodes. This approach is implemented in graph convolutional networks for transductive learning and node classification on graphs. The Geom-GCN scheme introduces a novel geometric aggregation method for graph neural networks, operating in both graph and latent space to address weaknesses in existing approaches. It is implemented for transductive learning and achieves state-of-the-art results on various datasets. The Geom-GCN scheme introduces a novel geometric aggregation method for graph neural networks, consisting of three modules: node embedding, structural neighborhood, and bi-level aggregation. The aggregation scheme operates in both graph and latent space, achieving state-of-the-art results on various datasets. The node embedding module in the Geom-GCN scheme maps nodes in a graph to a latent continuous space, preserving the graph's structure and properties. Various embedding methods can be used to infer the latent space. The Geom-GCN scheme involves mapping nodes in a graph to a latent space using embedding methods. A structural neighborhood is then built based on the graph and latent space, consisting of adjacent nodes in the graph and nodes with similar characteristics in the latent space. This allows for aggregation and preservation of similarities in the latent space. The relational operator \u03c4 in the latent space captures long-range dependencies in disassortative graphs by defining geometric relationships between nodes v and u. The bi-level aggregation scheme utilizes the structural neighborhood N(v) for aggregation in the latent space. The novel bi-level aggregation scheme for graph neural networks utilizes the structural neighborhood N(v) to update node features. It involves two aggregation functions operating in a neural network layer to extract structural information effectively and ensure permutation invariance for the graph. The low-level aggregation aggregates hidden features of nodes with the same geometric relationship in a neighborhood to a virtual node using a permutation-invariant function like an Lp-norm. The high-level aggregation in graph neural networks involves further aggregating features of virtual nodes using function q, which takes both node features and identities as inputs. The output is a vector obtained through a non-linear transformation with a learnable weight matrix and activation function. The proposed bi-level aggregation ensures permutation invariance for any node permutation. The bi-level aggregation, Eq. 1, guarantees invariance for any permutation of nodes in graph neural networks. It is a composite function where the low-level aggregation serves as the input for the high-level aggregation, ensuring permutation invariance. The low-level aggregation in Eq. 1 is permutation-invariant, consisting of sub-aggregations for nodes in a neighborhood with a relationship to v. The input and aggregation function are permutation-invariant, effectively modeling structural information and capturing long-range dependencies. The proposed scheme explicitly models structural information by exploiting geometric relationships between nodes in latent space and using bi-level aggregations. This approach contrasts with existing works like GAT, LGCL, GG-NN, and CCN, which use attention mechanisms or covariance architectures to learn implicit structure-like information for aggregating features. The key difference is that the proposed scheme offers an explicit and interpretable way to model structural information. Our work offers an explicit and interpretable way to model structural information in nodes' neighborhoods using geometry in a latent space. It is complementary to existing methods and can improve their performance. We address the weakness of MPNNs in capturing long-range dependencies by mapping distant nodes in a graph to a latent-space-based neighborhood of the target node for aggregations. The method relies on an appropriate embedding method to preserve similarities between distant nodes and the target node. It uses structural information to distinguish nodes in a graph-based neighborhood, capturing long-range dependencies indirectly through message propagation. JK-Nets and Kondor et al. (2018) have methods to capture long-range dependencies in graph-based neighborhoods. The method uses structural neighborhood to distinguish non-isomorphic graphs that aggregators like mean and maximum fail to differentiate. Structural neighborhood captures geometric relationships between nodes, enabling differentiation where traditional aggregators fail. In Geom-GCN, a specific implementation of geometric aggregation in graph convolutional networks is presented for transductive learning. The method utilizes different mapping functions for neighbors with varying geometric relationships to distinguish topological differences between graphs. The three key modules for implementing the general aggregation scheme are node embedding, structural neighborhood, and bi-level aggregation function. The Geom-GCN method utilizes different embedding methods like Isomap, Poincare embedding, and struc2vec to create suitable latent spaces preserving specific topology patterns in graphs. Isomap preserves distance patterns, Poincare embedding preserves hierarchies, and struc2vec preserves local structures. These methods result in three Geom-GCN variants: Geom-GCN-I, Geom-GCN-P, and Geom-GCN-S, each with distinct characteristics in the latent space. The structural neighborhood of a node in Geom-GCN includes its graph and latent space neighborhoods. The neighborhood-in-graph consists of adjacent nodes, while the neighborhood-in-latent-space includes nodes within a certain distance. The parameter \u03c1 is determined to make the average neighborhood sizes in both spaces equal. Euclidean distance is used in the Euclidean space, and geodesic distance is approximated in the hyperbolic space. The geometric operator \u03c4 represents relationships between nodes in a 2-D space. The geometric operator \u03c4 defines relationships between nodes in a 2-D space, with the relationship set R including upper left, upper right, lower left, and lower right. The operator can be designed to preserve more structural information in the neighborhood. The bi-level aggregation method involves using the same summation of normalized hidden features as GCN for low-level aggregation. The proposed Geom-GCN utilizes a bi-level aggregation method with a geometric operator \u03c4 defining relationships between nodes in a 2-D space. The features of virtual nodes are aggregated using concatenation || for all layers except the final layer, which uses mean. The overall aggregation of Geom-GCN includes a non-linear activation function ReLU and weight matrix W l estimated by backpropagation. Geom-GCN's performance is validated against Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) on transductive node-label classification tasks using nine open graph datasets. The proposed Geom-GCN utilizes a bi-level aggregation method with a geometric operator \u03c4 defining relationships between nodes in a 2-D space. It is validated using nine open graph datasets, including citation networks like Cora, Citeseer, and Pubmed, as well as the WebKB dataset with subdatasets like Cornell, Texas, and Wisconsin. The datasets consist of nodes representing papers or web pages, with edges denoting citations or hyperlinks. The curr_chunk discusses actor co-occurrence networks and Wikipedia networks, classifying nodes into categories based on keywords and web page traffic. Three Geom-GCN variants are constructed using different embedding methods. Three Geom-GCN variants are created using different embedding methods: Isomap (Geom-GCN-I), Poincare (Geom-GCN-P), and struc2vec (Geom-GCN-S). Hyper-parameter search is conducted for all models on the validation set, including parameters like number of hidden units, initial learning rate, weight decay, and dropout. The models use ReLU activation function for Geom-GCN and GCN, and ELU for GAT. The final hyper-parameter settings include dropout of p = 0.5, initial learning rate of 0.05, patience of 100 epochs, and weight decay of 5E-6 (WebKB datasets) or 5E-5 (other datasets). In GCN, the number of hidden units varies across datasets. Geom-GCN has 8 times as many hidden units as GCN due to 8 virtual nodes. GAT has different numbers of hidden units per attention head based on the dataset. Nodes are split into training, validation, and testing sets for all graph datasets. Geom-GCN achieves state-of-the-art performance, with results highlighted in Table 3 showing mean classification accuracy. The proposed Geom-GCN achieves state-of-the-art performance by aggregating \"message\" from two neighborhoods in the graph and latent space. Different embedding methods can be specified to create a suitable latent space for specific applications, leading to significant performance improvements. Ablation studies evaluate the contribution from each neighborhood by constructing new Geom-GCN variants with only one neighborhood. The Geom-GCN variants with only one neighborhood are compared to the baseline GCN to measure performance improvement. A homophily index \u03b2 is used to measure the strength of node label similarity in graphs. Assortative graphs show higher \u03b2 values than disassortative graphs. The results in Table 4 show that both graph and latent space neighborhoods benefit aggregation, with assortative graphs having larger \u03b2 values. The study compares Geom-GCN variants with one neighborhood to the baseline GCN for performance improvement. It is found that neighborhoods in latent space contribute more in disassortative graphs. Surprisingly, variants with one neighborhood outperform those with two, possibly due to the latter aggregating more irrelevant information. An attention mechanism may help address this issue in the future. The structural flexibility of Geom-GCN allows for combining different embedding spaces to create new variants. The study explores different combinations of embedding spaces for Geom-GCN variants, finding that some combinations outperform others. It suggests the need for an automated framework to determine the optimal embedding spaces. Geom-GCN has higher complexity compared to GCN, and real running time comparisons are also conducted. The study compares the running time of GCN, GAT, and Geom-GCN, with GCN being the fastest. Future work includes developing technology to improve the scalability of Geom-GCN. A visualization of feature representations from Geom-GCN-P shows spatial clustering and radial distribution of nodes. The feature representations extracted by the last layer of Geom-GCN-P on the Cora dataset are visualized in a 2-D space using t-SNE. Nodes with the same label exhibit spatial clustering, demonstrating the discriminative power of Geom-GCN. The proposed model learns the graph's hierarchy through Poincare embedding, addressing weaknesses in existing message-passing neural networks. The approach bridges a discrete graph to a continuous geometric space via graph embedding, extracting lost information and improving discriminative structures and long-range dependencies. Our approach extracts lost information from a graph by recovering discriminative structures and long-range dependencies in an embedding space. We introduced a geometric aggregation scheme with various Geom-GCN implementations, showing clear advantages over existing methods. Future work includes exploring embedding methods based on input graphs and target applications like epidemic dynamic prediction on social contact networks."
}