{
    "title": "S1gHsYFhsB",
    "content": "We propose a simple image preprocessing method for learning disentangled latent factors by leveraging features from networks pretrained on ImageNet and fine-tuning them on tasks relevant to the NeurIPS2019 disentanglement challenge. In this contribution, a VAE is trained on regionally aggregate feature maps, utilizing implicit inductive bias from models pretrained on ImageNet and fine-tuned on challenge-relevant tasks. The approach involves using pretrained CNNs to extract convolutional feature maps before VAE training for partial disentanglement, addressing issues with the extracted feature vectors. The feature extraction network used in the VAE training process has two main issues: it was trained on ImageNet, which differs from the MPI3d dataset used in the challenge, and the feature aggregation mechanism may not retain all necessary information for disentanglement. To address these issues, the network is finetuned using labels from simulation datasets MPI3d-toy and MPI3d-realistic. The method involves supervised finetuning of the feature extraction CNN, extracting feature vectors from images, and training a VAE to disentangle latent factors of variation. In this step, the feature extraction network is finetuned offline using simulation datasets MPI3d-toy and MPI3d-realistic. The goal is to adapt the network to produce aggregated feature vectors that capture latent variables well. The VGG19-BN architecture is utilized for the feature extraction network, with input images standardized using mean and variance from the ImageNet dataset. The output feature maps are fed into a feature aggregation module to reduce dimensionality before the final average pooling. The feature aggregation module reduces the feature map to a 512-dimensional vector using convolution layers with different feature maps and kernel sizes. Layerwise dropout is applied before each convolution layer, and the aggregated feature vector is 2-normalized. Linear classification layers are added for each latent factor, and the network is trained on MPI3d-toy and MPI3d-realistic datasets to learn robust features. The VGG19-BN network is trained on MPI3d-toy and MPI3d-realistic datasets with specific settings. The network achieves high classification accuracy on the validation set for different latent factors. The feature extraction network is fine-tuned to produce aggregated feature vectors after training. The VGG19-BN network is fine-tuned to produce aggregated feature vectors for a \u03b2-VAE model. Pretraining with any data is allowed by the challenge organizers, and the encoder network consists of fully-connected layers for mean and log variance. The feature vectors are standardized to match ImageNet dataset statistics. The VAE model uses 16 latent factors in the approximate posterior q and a decoder network with fully-connected layers. The network is initialized orthogonally and uses a factorized Gaussian prior on the latent variables. Optimization is done with the RAdam optimizer and the VAE is trained for 100 epochs. The VAE model is trained for 100 epochs with a cosine schedule to anneal the hyperparameter \u03b2 from 0.001 to 0.4. This approach improves performance by initially focusing on data reconstruction before emphasizing factorization of latent variables. The model achieves top rank on FactorVAE and DCI on the public leaderboard. Our best submission ranks first on FactorVAE and DCI metrics, showing a significant lead over the second-placed entry. Prior knowledge simplifies the disentanglement task, leading to improved scores. Task-specific supervision from simulation is used, demonstrating its transferability to real-world data."
}