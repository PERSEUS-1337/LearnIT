{
    "title": "Syx72jC9tm",
    "content": "Invariant and equivariant networks are used for learning various types of data, but the characterization of linear layers for graphs is not fully understood. Linear layers for (hyper-)graph data are characterized as permutation invariant and equivariant, with dimensions of $2$ and $15$ for edge-value graph data. The dimension for graph data defined on $k$-tuples of nodes is the $k$-th and $2k$-th Bell numbers. Orthogonal bases are computed, allowing for generalization to multi-graph data. The results generalize recent advancements in equivariant deep learning and show that the model can approximate any message passing neural network. Applying new linear layers in a deep neural network framework achieves comparable results to state-of-the-art and offers better expressivity than previous invariant and equivariant bases. The problem of graph learning involves finding a functional relation between input graphs and corresponding targets. Graph data points consist of nodes and values attached to hyper-edges encoded in a tensor. The order of the tensor indicates the type of data it represents, such as node-values, edge-values, or hyper-edge-values. The task involves constructing a neural network f that relates input graph data represented by a binary adjacency matrix A to targets T. The function f should be order invariant if T is a single output response, producing the same output regardless of node numbering. If T is a tensor, f should be order equivariant, meaning it commutes with node renumbering. The function f is order equivariant, meaning it commutes with node renumbering. It satisfies f(P^TAP) = P^Tf(A)P for every permutation matrix P. Invariance is defined as f(PA) = f(A), while equivariance is defined as f(PA) = Pf(A). The text discusses linear invariant and equivariant layers in neural networks, focusing on functions between different order tensors. It characterizes linear permutation invariant and equivariant operators for node-value tensors. The linear space of invariant operators is shown to be of dimension one, mainly containing the sum operator. The general equivariant tensor case is also briefly discussed. In this paper, a full characterization of permutation invariant and equivariant linear layers for general tensor input and output data is provided. The space of invariant linear layers is of dimension b(k), the k-th Bell number, while the space of equivariant linear layers is independent of the size n of the node set V. This allows the same network to be applied on graphs of different sizes. The authors provide a formula for orthogonal basis to build linear invariant or equivariant layers with maximal expressive power for graph data represented by adjacency matrices. They generalize the case of node-value data to multiple node sets to learn interactions across sets using permutation matrices. The authors introduce bases for linear invariant and equivariant layers for graph data. They show that the linear space of invariant linear layers is different from equivariant linear layers. The authors introduce orthogonal bases for linear invariant and equivariant layers for graph data, showcasing the universal approximation power of their model in approximating message-passing neural networks. In experiments, they focus on learning graph functions with single node sets and edge-value data using their linear layers. The authors demonstrate the effectiveness of their orthogonal bases for linear invariant and equivariant layers in learning graph functions, showcasing extrapolation capabilities and achieving comparable results to state-of-the-art methods on graph learning datasets. Their work combines group invariant networks and deep learning on graphs. The text discusses how neural networks can be invariant or equivariant to different symmetries of input objects, such as translation and rotational symmetries. It also mentions the generalization of convolution operators to groups of rotations and reflections, as well as the parameter sharing structure in graphs and hyper-graphs. Graph Neural Networks (GNNs) hold a state for each node in a graph and propagate these states based on the graph structure and learned functions. Various works have explored defining convolution operators on graphs, with one approach using the Laplacian operator to mimic spectral properties for generalized Fourier transformations. The Laplacian operator is used to define generalized Fourier basis on graphs, with efforts to improve filter efficiency and localization. Spectral approaches have the drawback of being graph-dependent, leading to challenges when applying the same network to different graphs. Another popular method involves learning stationary functions that update node states based on neighbors, generalizing the properties of standard convolution operators on grids. These methods, including spectral approaches, can be viewed as instances of Message Passing Neural Networks. Neural Networks. Characterizing linear invariant and equivariant layers for a single node set with edge-value data. Notation for matrices and permutation. Order invariant linear operator L is equivalent to a specific equality involving Kronecker product. Order invariant linear operator L is equivalent to the equation involving Kronecker product, showing that L is order equivariant for all permutation matrices. Finding invariant and equivariant linear layers for tensor data over one node set requires finding fixed points of permutation matrix group represented by Kronecker powers of permutation matrices. The general case for order-k tensor data A over one node set involves permutation matrices and invariant/equivariant operators. The fixed-point equations are represented by Kronecker powers of permutation matrices, showing the relationship between tensor renumbering and coordinate vectors. The text discusses the relationship between permutation matrices and invariant/equivariant linear operators for order-k tensor data. It shows how the problem of finding these operators can be reduced to solving specific equations involving Kronecker products. The fixed point equations involve an exponential number of equations with a polynomial number of unknowns, possessing a solution space of constant dimension. To find the solution, we need to solve equations formulated as QX = X for permutation matrices Q acting on tensors X in R^n. The permutation group acts on tensors X, looking for fixed points under this action by defining an equivalence relation in the index space of tensors. Equivalence classes denoted [n] / \u223c represent partitions of sets with identical values. Each class can be represented by a unique partition. For example, when n = 2, there are two equivalence classes: \u03b3 1 = {{1}, {2}} and \u03b3 2 = {{1, 2}}. Each class \u03b3 \u2208 [n] / \u223c is defined by an order-tensor B \u03b3 \u2208 R^n. The solution to equation 7 is constant on equivalence classes of the equality pattern relation \u223c, where a \u223c q(a) for a permutation q : [n] \u2192 [n]. The solution to equation 7 is constant on equivalence classes of the equality pattern relation \u223c, where a \u223c q(a) for a permutation q. Any tensor X constant on equivalence classes can be written as a linear combination of indicators of the equivalence class, forming an orthogonal basis. The curr_chunk discusses the characterization of invariant and equivariant linear layers on k-order tensor data over a single node set V. Theorem 1 focuses on purely linear layers without bias, while extending the analysis to constant layers is straightforward. Constant invariant layers are represented by constants, and constant equivariant layers on one node set are characterized by fixed-point equations. They have the same form and dimensionality as invariant layers on k-order tensors. The curr_chunk discusses the formulation of linear invariant and equivariant layers on k-order tensor data over a single node set V. The operator P A acts on nodal indices, preserving invariance as Lvec(P A) = Lvec(A). The matrix corresponding to P on R n k \u00d7d is P T \u2297k \u2297 I d. The curr_chunk explains the equivariance equation for linear invariant and equivariant layers on k-order tensor data. It defines the basis elements for the solution space of fixed-point equations and presents the formulation of linear invariant and equivariant layers with learnable parameters. The curr_chunk discusses the basis elements for linear equivariant layers and the dimension of the solution space of fixed-point equations for linear invariant and equivariant layers on k-order tensor data. It also mentions the formulation of these layers with learnable parameters. The curr_chunk discusses linear equivariant layers for tensor data, including basis elements and mixed order equivariant layers. It also mentions the renumbering operator for order equivariance. The curr_chunk explains that order equivariance is equivalent to P \u2297(k+l) vec(L) = vec(L) for all permutation matrices P, and discusses the implementation details in Tensorflow. The equivariant linear basis was efficiently implemented using basic operators for row/column/diagonal summation. Networks consisted of 1-4 equivariant linear layers with ReLU activation for equivariant functions and additional layers for invariant functions. The method was tested on synthetic datasets for equivariant and invariant graph functions, showcasing differences in expressivity. Tasks included projection onto symmetric matrices, diagonal extraction, computing maximal right singular vector, and computing the trace. In the experiment, datasets were created for tasks involving computing the trace of matrices. Different types of matrices were used for tasks (i), (ii), and (iv), while task (iii) involved matrices with specific singular values. Networks with varying hidden layers were trained using mean-squared error as loss. The experiment compared models with different linear layer bases and highlighted the performance using mean-square error metrics. The additional parameters in the model are needed for cases where permutations on rows and columns are different. The assumption of the same permutation on rows and columns leads to extra parameters related to the diagonal and transpose of the matrix. These parameters control self-edges, node features, and incoming/outgoing edges in graphs. The equivariant networks were applied to graphs of unseen sizes, showing generalization capabilities beyond the trained fixed size. The network was trained on a fixed size and showed generalization capabilities to different size graphs. Testing was done on graph classification benchmarks using real-world datasets from bioinformatics and social networks. Results compared favorably to state-of-the-art methods. The method was evaluated using the 10-fold splits of BID40 dataset. A simple architecture with 3 layers and specific features was used. Results were compared to various deep learning, graph kernel, and feature-based methods. Our method, FGSD BID33 and AWE BID15, achieved comparable results to the state-of-the-art on social networks datasets and slightly worse results on biological datasets. A generalization of our framework to data given on tuples of nodes from node sets V1, V2, ..., Vm is provided, with a reordering operator built from permutation matrices. The tensor A is defined by its entries, and the equations can be rewritten in matrix format. The number of tensors for invariant and equivariant layers is determined by the dimensions of the fixed-point equations. Orthogonal bases for these layers are listed, and the research was supported by grants. Operations are normalized to have unit max operator norm. The research, supported by grants, focuses on orthogonal bases for invariant and equivariant layers. The model is more expressive than previous ones, with a 15 element basis potentially being utilized in deeper layers. A useful combinatorial fact is proven, aiding in computing dimensions of more general spaces of linear layers. The research focuses on orthogonal bases for invariant and equivariant layers, with a 15 element basis potentially used in deeper layers. A combinatorial fact is proven to aid in computing dimensions of linear layers. The formula in proposition 3 relates to the k-th moment of a random variable counting fixed points of a permutation with uniform distribution over the permutation group. The k-th moment of a random variable counting fixed points of a permutation is related to the Bell number. Proposition 3 helps calculate dimensions of linear layer spaces for equivariant and multi-node sets. The space of invariant linear layers is of dimension dd b(k) + d, with basis elements defined in equation 9. The solution space for fixed point equations is the set V G for the matrix group G = P \u2297k \u2297 I d \u2297 I d | P \u2208 \u03a0 n. The dimension of the solution subspace for invariant linear layers can be computed using formula 12. Applying an MLP to the last d + d feature channels of Z 4 results in errors that depend on the approximation error of the MLP and previous errors. The model can represent any message passing network with arbitrary precision on compact sets, making it as powerful as any message passing neural network falling under the framework of BID9."
}