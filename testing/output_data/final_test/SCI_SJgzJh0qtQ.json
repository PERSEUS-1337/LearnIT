{
    "title": "SJgzJh0qtQ",
    "content": "Deep learning models have surpassed traditional methods in various fields like natural language processing and computer vision. This paper introduces a structured approach using Principal Component Analysis (PCA) to optimize Convolutional Neural Networks (CNNs) by determining the number of filters per layer without retraining, reducing computational costs while maximizing accuracy. The proposed technique uses Principal Component Analysis to optimize CNNs by determining the optimal number of layers and structure for a given network on a dataset. It can also adapt a pre-designed network on a new dataset, demonstrated by optimizing VGG and AlexNet networks on CIFAR-10. Deep Learning often faces computational complexity issues due to the large number of parameters and operations involved in CNN design. Networks are typically only slightly modified for new datasets, with hyperparameters adjusted during retraining. Evaluating the necessity of increased dimensionality is crucial to eliminate redundancy in network design. There is currently no fixed method for optimizing network structure. Currently, there is no fixed method for optimizing network structure in deep learning. Designers often use grid search or start with a variant of 8-64 filters per layer and double the number of filters per layer. Model compression explores ways to prune a network post-training to remove redundancy and improve efficiency. Our technique offers a more efficient approach to network pruning compared to traditional methods. It analyzes the network in a single pass, estimating the 'significant' dimensions at each layer to determine the number of filters to keep. This results in optimal networks in terms of width and depth, with minimal design time and compute power required. The paper presents a method for optimal network design that allows hardware designers to balance accuracy and computational complexity efficiently. It identifies layers that are less sensitive to pruning, enabling more aggressive pruning in those areas. This approach analyzes all network layers in a single pass, determining the optimal structure in terms of width and depth without the need for iterative searches. The analysis provides designers with a way to tune trade-offs between accuracy, speed, and power consumption for error-tolerant applications. It helps identify layers suitable for aggressive compression in network design. Various traditional approaches for model compression include pruning weights, filters, approximation techniques, quantization, and others. Aggressive reductions in model sizes are achieved through pruning individual weights using different ranking metrics. Pruning filters introduces structured sparsity in the network for hardware savings. BID13 and BID15 prune filters based on weight magnitude, resulting in good compression with minimal accuracy loss. BID3 automates threshold determination per layer using L1 norm metric, reducing computation and retraining costs. BID18 uses cosine similarity to detect redundancy in filters without retraining the network, but faces accuracy drop for CNNs. Various papers like BID9, BID8, BID2, and BID11 employ statistical techniques for weight matrix approximation, requiring multiple retraining iterations. Quantization techniques like in BID1 and BID17 leverage error resilience by reducing weights to 1 bit for network pruning. Weight sharing, as seen in BID4, is a form of vector quantization that can be adjusted for any network but requires multiple iterations. Luo et al. (2017) propose pruning based on the next layer's statistics, facing limitations similar to threshold-based methods. BID21 assigns importance scores to neurons and removes weights below a certain threshold, while BID10 learns basis filters with a new initialization technique. Our work differs from other model compression techniques by finding the optimal structure of a trained network instead of creating a new smaller model. Unlike methods like BID7, we do not guarantee the optimality of the smaller model. Additionally, our approach does not determine the optimal depth for a given trained network. Techniques like PCA can be used in conjunction with our method for dimensionality reduction. PCA is a technique used for dimensionality reduction where output features are ranked by the percentage of variance they can explain. It works by combining correlated features into a single feature to capture most of the data variance. The output of PCA shows cumulative explained variance against the number of principal components, highlighting redundancy in the feature space. The success of pruning techniques is due to redundancy in the network, as seen in the filters of AlexNet's first layer. Identifying independent filters is crucial for accuracy. Metrics like Deep Compression based metric are used to rank filters based on significance, aiming for more intuitive measures. The metric used in BID13 considers each weight independently, while the Absolute sum of gradients ranks filters based on their contribution to the total loss. Global summing of filter activation maps sums the activation maps over the validation set to identify the presence and strength of filters in the input. In an exhaustive search, filters were ranked based on their impact on accuracy, with the least significant filter being removed in each iteration. Results showed that 32 filters could be reduced to 14 without loss in accuracy, and all metrics yielded similar results. After an exhaustive search, it was found that 32 filters could be reduced to 14 without any loss in accuracy. Visualizing the process showed that remaining filters absorbed characteristics of the pruned filters, as seen in an animation. This phenomenon was observed multiple times, leading to a hypothesis. Upon retraining, it was observed that as filters were removed, they seemed to be recreated in other correlated filters without degrading accuracy. The number of significant filters removed before accuracy dropped was consistent across different metrics, indicating it is an intrinsic property of the network structure. Simple techniques were used to predict which retrained filter would absorb the pruned filter. After retraining, filters were observed to be recreated in other correlated filters without affecting accuracy. Simple techniques were used to predict which filter would absorb the pruned filter, based on pearson correlation coefficients. The experiments showed that sometimes one filter was absorbed by multiple filters, even with low correlation coefficients individually. Examples of matches and mismatches are explained in FIG2. After retraining, filters were observed to be recreated in other correlated filters without affecting accuracy. The experiments showed that sometimes one filter was absorbed by multiple filters, even with low correlation coefficients individually. Two examples of matches and mismatches are explained in FIG2. The filter that is pruned out is in black, the one closest to it according to pearson coefficient is in blue, and the two filters that changed the most after retraining are in pink. This motivated viewing the problem as a 'sub-space' of the entire filter space and using PCA for dimensionality reduction. PCA creates new uncorrelated filters to capture maximum data variation. PCA is used to create new filters that capture maximum data variance. The input to PCA is an NxM matrix where N is the number of samples and M is the number of features. The new filters are a linear combination of all input filters, with each explaining a certain percentage of variance in the data. The goal is to find the least number of filters that can explain the data well, and if less than M filters capture 99.9% of the variance, not all M filters are needed. In PCA, new filters are created to capture maximum data variance. The significant dimensions explain 99.9% of the variance in the data. The output of a convolution with each filter can be viewed as the feature value for that filter. In PCA, new filters are created to capture maximum data variance. Flattening the output feature map gives us samples for each filter, allowing us to run PCA directly. By identifying the principal filters that explain 99.9% of the variance, we can retrain the network or fine-tune it from a starting point. Training from scratch in experiments yielded the desired accuracy. In experiments, training from scratch yielded the desired accuracy, so transformations were skipped to avoid multiple iterations. However, transformations were found useful in a preliminary case discussed in Section 6. CNN classification involves non-linear transformations into higher dimension spaces, with principal filters playing a key role. If the number of principal filters stops expanding in deeper layers, those layers may not be causing an issue. Experiments on adapting deep networks like VGG-16/19 on smaller datasets show that principal dimensions tend to expand as we go deeper, but then begin to contract. Removing layers where dimensions are still expanding does not affect accuracy, providing a heuristic for determining network depth without additional analysis. There is a trade-off between dataset representation and PCA complexity. In experiments with deep networks like VGG-16/19 on smaller datasets, principal dimensions expand then contract as we go deeper. Removing expanding layers doesn't impact accuracy, offering a heuristic for determining network depth. Running PCA on the entire dataset is costly, so using randomized images from the training set, we varied the number of images for PCA analysis to stabilize results. More samples than filters are needed for stable results, especially in later layers with increased filter numbers and reduced output dimensions. In experiments with deep networks like VGG-16/19 on smaller datasets, principal dimensions expand then contract as we go deeper. Running PCA on the entire dataset is costly, so using randomized images from the training set, we varied the number of images for PCA analysis to stabilize results. The shape of the PCA curves informs us of how sensitive a layer is to compression, helping decide how aggressively we can prune a layer. This is shown in Figure 7 for training VGG16 BN on CIFAR-10 data. The PCA curves give an idea of how sharply the accuracy will degrade in a layer, guiding the pruning process. The study observed a trend in deep networks like VGG-16/19 where dimensions expand then contract as we go deeper. The hypothesis that the first 7 layers are optimal was confirmed, with negligible accuracy degradation for removal of layers 8 through 13. This trend was also seen in VGG19 on CIFAR-100 data. Figure 6 illustrates the sensitivity to pruning in VGG16 BN network trained on CIFAR-10. Layers 1-7 show increasing sensitivity, while layers 8 onwards exhibit decreased sensitivity. The results are summarized in TAB1, with details of the networks and experiments explained in subsequent sections. The VGG16 network configuration for CIFAR-10 dataset is shown as a vector, with each element representing the number of filters in a convolutional layer or 'M' for a MaxPool layer. The network has 5 modules, each ending with a maxpool layer. A toolkit available with PyTorch was used for profiling networks to determine the number of operations and parameters. The network configuration for CIFAR-10 dataset consists of 5 modules, each ending with a maxpool layer. The PCA analysis revealed the number of filters needed for each layer to explain 99.9% variance. Scratch-training a new network with this configuration resulted in a 0.31% accuracy drop. Further analysis suggests that the optimal network design should have only 7 layers. The study involved training networks from scratch with reduced layers, showing a less than 0.25% drop in accuracy with more than 7 layers. Parameters and operations decreased significantly. Figure 7 illustrates the trade-off between accuracy and computational complexity, with results for VGG16 and VGG19 on CIFAR-10 and CIFAR-100 datasets. The original VGG19 BN network for CIFAR-100 dataset has 5 modules with 16 convolutional layers. Analyzing the principal components of the activations led to a new configuration with a 0.5% drop in accuracy. The optimal depth suggests that the last module may be redundant. The original VGG19 BN network for CIFAR-100 dataset has 5 modules with 16 convolutional layers. Analyzing the principal components of the activations led to a new network design with reduced depth and improved accuracy. The optimized configuration achieved 73.03% accuracy, a 1% improvement over the original network, with reduced overfitting. This resulted in a significant reduction in computational complexity. After optimizing the VGG19 BN network for CIFAR-100, a new network configuration was identified with reduced depth and improved accuracy. The PCA analysis led to a configuration with a drop in accuracy of 1% after retraining with fewer filters. Removing the last layer and training a new network resulted in a further drop of 0.08% in accuracy but a 2.1X improvement in operations and parameters. The degradation curve with the number of filters is shown in FIG3. For VGG-19 on ImageNet, a similar analysis led to a new configuration after PCA analysis with a 0.25% drop in accuracy. The analysis focused on different network configurations with reduced depth and improved accuracy for VGG19 BN network on CIFAR-100 dataset. The accuracy dropped by around 0.25% with improvements in operations and parameters. The analysis recommends not performing ReLU in-place before analyzing activation outputs for convolutional layers. It suggests taking a large number of samples for PCA, especially in later layers with small activation map sizes. Collecting activations over many batches is crucial for having enough data for PCA analysis. Retaining 99.9% of variance is found to be effective in most cases. Retaining 99.9% variance is effective for most cases, providing a balance between accuracy and computational cost. This method is useful for designing new networks, adapting existing ones, and optimizing for faster runtimes or reduced power consumption. PCA helps in understanding dimensionality and transforming filters into orthogonal 'principal filters'. Visualization in figure 9 shows this process for the first layer of a network trained on CIFAR-10. The orthogonal transformed filters in figure 9 for the first layer of a network trained on CIFAR-10 explain 99.9% of the variance. These transformed filters can be used to initialize a smaller network instead of training from scratch, but this method did not show significant benefits in most cases. However, it was necessary when training FCN on PASCAL VOC data, where the encoder needed to be initialized with VGG-16 pretrained on ImageNet for the FCN network to learn effectively. The FCN network can be initialized with VGG-16 pretrained on ImageNet to improve learning. Fine-tuning with transformations on the weights of the encoder and next layer allows the FCN to learn effectively. Further analysis on this technique for segmentation architectures is planned for future work."
}