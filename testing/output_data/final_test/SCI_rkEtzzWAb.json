{
    "title": "rkEtzzWAb",
    "content": "Generative modeling of high dimensional data like images is a challenging problem with unclear evaluation methods. In this paper, adversarial learning, particularly with generative adversarial networks (GANs), is proposed as a framework to define more meaningful task losses for unsupervised tasks like generating visually realistic images. The relationship between GANs and structured prediction is explored within the context of statistical decision theory, highlighting connections between recent advances in structured prediction theory and the choice of divergence in GANs. The importance of selecting a divergence that aligns with the final task is emphasized through experiments, especially in scenarios like machine translation where the final task is crucial yet not clearly defined. The task of generating realistic samples, like in machine translation, is challenging due to the difficulty in defining what constitutes a good solution. By incorporating specific criteria into a task loss, such as grammatical correctness for translation, we can approximate the final task more effectively. Structured prediction and data generation become well-defined problems when formulated as the minimization of such a task loss. In object classification and machine translation, models are evaluated based on task loss, such as generalization error or BLEU score. However, defining a task loss for generative modeling to generate realistic samples is challenging. Traditionally, distribution learning uses negative-log-likelihood as the task loss, but it has limitations in high dimensions. In high-dimensional settings, log-likelihood faces challenges due to the strong Kullback-Leibler divergence, leading to optimization difficulties and poor sample quality assessment. This work explores how adversarial divergences can serve as task losses, addressing issues of KL divergence by indirectly incorporating complex criteria through parametric functions in the Generative Adversarial Network framework. Training a GAN involves minimizing the parametric adversarial divergence Div NN (p||q \u03b8) by training a generator network q \u03b8. Parametric adversarial divergences offer advantages in terms of sample complexity, computation, flexibility, and optimization ease. Structured prediction and GANs can be linked through statistical decision theory, both aiming to minimize a statistical task loss. Choosing a divergence that aligns with the final task in generative modeling is crucial. In generative modeling, the importance of choosing a good objective is highlighted, especially in high-dimensional data settings. Parametric adversarial divergences are found to be well-suited for image generation tasks where defining a perceptual loss is challenging. Experiments show the limitations of nonparametric Wasserstein on complex datasets, emphasizing the need for a parametric discriminator. Comparison experiments between maximum-likelihood and parametric adversarial divergences are conducted on high-dimensional images and specific data learning scenarios. The structured prediction framework is introduced to handle divergences in high-dimensional images and learning data with specific constraints. The goal is to learn a classifier that predicts structured outputs from inputs, facing challenges due to the exponential size of possible outputs. This framework is linked to generative modeling, and recent theoretical insights help in choosing better divergences. Additionally, parametric adversarial divergences are unified with traditional divergences for comparison in the next section. In structured prediction, classifiers use score functions to assign labels to inputs. The score function can be linear or based on a neural network. Evaluating predictions requires defining a task-dependent structured loss. In structured prediction, classifiers use score functions to assign labels to inputs, which can be linear or based on a neural network. The goal is to find a parameter \u03b8 that minimizes generalization error by minimizing a surrogate loss with nicer properties. A structured loss expresses the cost of predicting y for x when the ground truth is y, and the relation between the loss function and the final task is discussed. An example of a structured prediction task is machine translation, where the input x is a sequence of French words and the output y is a sequence of English words. In structured prediction, classifiers use score functions to assign labels to inputs, which can be linear or based on a neural network. The goal is to find a parameter \u03b8 that minimizes generalization error by minimizing a surrogate loss with nicer properties. An example of a structured prediction task is machine translation, where the input x is a sequence of French words, and the output y is a sequence of English words belonging to a dictionary D with typically |D| \u2248 10000 words. The output sequence is restricted to be shorter than T words, resulting in |Y| = |D| T, which is exponential. BLEU scores are used to define the task loss, aiming for a translation with many words in common with the ground truth. Traditional nonparametric divergences are compared with adversarial divergences using a formalism similar to BID45; BID26.\u03c8-divergences with generator function \u03c8 (f-divergences) can be written in dual form BID33 4 Div \u03c8 (p||q \u03b8 ) = sup, where \u03c8 * is the convex conjugate, allowing for various \u03c8-divergences like Kullback-Leibler, Jensen-Shannon, and Total Variation. \u03c8-divergences, including Kullback-Leibler, Jensen-Shannon, Total Variation, Chi-Squared, and Wasserstein-1 distance, measure the cost of transporting probability mass from p to q. Maximum Mean Discrepancy in Reproducing Kernel Hilbert Space has interpretations related to moment matching. Some \u03c8 require constraints like ||f||\u221e \u2264 1 for Total Variation. Properties of Divergences are discussed, with explicit and implicit models distinguished by the ability to compute q\u03b8(x). Sample complexity and computational cost are detailed in Section 3.1. The text discusses different divergence measures such as f-divergences, Wasserstein distance, Maximum Mean Discrepancy, and parametric adversarial divergences. These measures vary in terms of sample complexity, computational cost, and their ability to integrate the final loss in their base distance. The text discusses parametric adversarial divergences, which can be computed iteratively with SGD and integrate the final loss in the choice of discriminators. These divergences are associated with optimization problems where f is constrained to a parametric family, typically specified as a neural network architecture. The parametric adversarial Jensen-Shannon and Wasserstein are examples of such divergences optimized in GANs and WGANs, respectively. Parametric adversarial divergences, optimized with neural networks, offer advantages for generative modeling compared to traditional divergences. They are efficient in terms of sample complexity and computational cost, and can integrate criteria related to the final task. However, combining the KL-divergence with structured generators has limitations. Additional properties and issues are discussed in the Appendix. In Appendix A.1, parametric adversarial divergences are discussed, emphasizing their ability to provide learning signal even when nonparametric counterparts are not well-defined. The sample complexity for adversarial and traditional divergences is compared in Table 1, highlighting the advantages of using Monte-Carlo for explicit models and addressing challenges for implicit models. For implicit models, estimating f-divergences from samples can be challenging. Parametric adversarial divergences can be treated as a classification/regression problem with reasonable sample complexity. A nonparametric estimator of the Wasserstein distance can be computed efficiently using specialized algorithms. The empirical Wasserstein estimator has high sample complexity in high dimensions, making it not viable. Maximum Mean Discrepancy offers a more efficient estimator with sample complexity independent of dimensionality, but its performance can be poor with generic kernels like RBF for certain datasets. The Maximum Mean Discrepancy (MMD) performs poorly with generic kernels like RBF for MNIST and Toronto face datasets, resulting in distinguishable artifacts in generated images. The power of the MMD statistical test can decrease with increasing dimensionality, making it challenging to discriminate between high-dimensional generated and training distributions. Sample complexities provide insights on divergence comparison but should be interpreted cautiously. The empirical Wasserstein estimator has high sample complexity in high dimensions, while MMD offers a more efficient estimator with dimensionality-independent sample complexity. In structured prediction and generative modeling, optimizing for task losses and integrating final task criteria can simplify learning. Adversarial divergences offer a way to incorporate task-related criteria, unlike f-divergences. The Wasserstein distance and MMD are based on a metric and kernel that allow for expressing subjective similarity notions to specify tasks. In structured prediction and generative modeling, optimizing for task losses and integrating final task criteria can simplify learning. Adversarial divergences offer a way to incorporate task-related criteria. The metric and kernel used in Wasserstein distance and MMD express subjective similarity notions for specifying tasks. Recent research combines MMD with kernel learning, showing promising results on various image datasets. Different approaches include learning feature maps, end-to-end kernel learning, and energy distance learning. The form of the discriminator in adversarial divergences can affect the associated divergence. Different structures on the generator can yield specific divergences, such as Kullback-Leibler divergence. The relation between the discriminator and divergence is explored in interpretations and experiments in Section 6. The generative model of variational autoencoders can be seen as an infinite mixture of Gaussians, leading to blurry samples. Autoregressive models like recurrent neural networks and PixelCNNs factorize naturally, affecting the log likelihood in structured prediction. Autoregressive models like recurrent neural networks and PixelCNNs factorize naturally, training with maximum likelihood using teacher-forcing. However, there are discrepancies between training and generation due to potential error accumulation when sampling from the model. More principled approaches to sequence prediction with autoregressive models are discussed in BID22 and related references. In this section, insights are provided for designing the best adversarial divergence for the final task by establishing the relationship between structured prediction and generative adversarial networks. The theoretical results on objective choice in structured prediction are reviewed, along with their interpretation in generative modeling. The relationship between structured prediction and GANs is framed using statistical decision theory, aiming to minimize task loss through action selection in a world with possible states and actions. Generative models with Maximum Likelihood focus on available data distributions and model distributions for optimal action selection. The set of actions A is the set of possible distributions for the model, with the task loss being the negative log-likelihood in structured prediction. Criteria for good solutions are integrated into the statistical task loss, such as the generalization error in structured prediction and adversarial divergence in GANs. The hope is that minimizing the statistical task loss effectively solves the final task. The set of actions A is the set of distributions that the generator can learn, with the task loss being the adversarial divergence. The prediction function h \u03b8 is analogous to the generative model q \u03b8, and the choice of the right structured loss can be related to the discriminator family F. Criteria for good solutions are crucial in both structured prediction and data generation. In generative modeling, incorporating task-related criteria involves choosing structured losses or discriminators that reflect the criteria of interest. The structured prediction community has developed loss functions for prediction problems, such as BLEU score in machine translation, to induce desired properties on learned predictors. This process is illustrated in FIG0. In the context of image segmentation, BID35 compared structured loss functions for predicted mask properties. Adversarial divergences can induce specific properties on generated data, focusing on realistic samples over similarity to the training set. DCGAN's discriminator architecture is robust to deformations but can detect blurry samples, aligning with the goal of realistic sample generation. In structured prediction, weaker loss functions are easier to learn than stronger ones. Strong losses like the 0-1 loss lack flexibility and require exponential training examples, while weaker losses like the Hamming loss offer more flexibility in learning. Theoretical results formalize the intuition that weaker structured loss functions, like the Hamming loss, are easier to optimize in a non-parametric setting. Sample complexity for the 0-1 loss is exponential in the dimension of y, while for the Hamming loss, it is much better under certain constraints. Choosing the right structured loss, like the weaker Hamming loss, can lead to exponentially faster training. Theoretical results show that weaker losses are easier to optimize, with sample complexity being polynomial in the number of dimensions. This contrasts with strong structured loss functions, like the 0-1 loss, which make learning exponentially harder. Insights from statistical decision theory suggest that it is easier to learn with weaker divergences than with stronger ones. Comparing distributions with disjoint support can be done in weaker topologies like Wasserstein, not in stronger ones like Jensen-Shannon. Previous works analyze GANs with nonparametric views and prove sample complexities for parametric divergences. They unify parametric and nonparametric divergences, introducing the concept of strong and weak divergence. However, they do not explore the practical properties of parametric divergences. In this work, the focus is on studying the practical properties of parametric divergences and their relevance as task losses. Experiments are conducted to analyze properties such as invariance, constraint enforcement, and differences from nonparametric counterparts. The unification of structured prediction and generative modeling is also explored, offering a fresh perspective to the field. Additionally, related papers discuss unifying divergences, analyzing statistical properties, interpreting generative modeling, enhancing GANs, and critiquing maximum-likelihood as an objective for generative modeling. The first GAN paper unified traditional IPMs, viewing them as classification problems. Later papers generalized the GAN objective to any adversarial f-divergence. The MMD-GAN papers studied the effect of restricting the discriminator to be a neural network. They provided interpretations of images generated by the network using the Sinkorn-Autodiff algorithm on MNIST and CIFAR-10 datasets. Recent work has focused on improving the stability of generative adversarial networks (GANs) by introducing the adversarial Wasserstein distance and refining the objective function. Evaluating generative models has also been discussed, showing that log-likelihood may not accurately reflect visual sample quality. Additionally, comparing parametric adversarial divergence and likelihood objectives in RealNVP generators has led to better visual results with adversarial divergence. Theoretical understanding of learning in structured prediction has also been a recent focus in research. The present paper discusses the theoretical understanding of structured prediction and its link to adversarial divergence in generative modeling. It also explores the sample complexity of nonparametric Wasserstein distance in high dimensions. The Sinkhorn-AutoDiff algorithm is used to compute the entropy-regularized L2-Wasserstein distance between training and generated images. Results show success on MNIST but failure on CIFAR-10, indicating nonparametric Wasserstein's limitations in high dimensions compared to parametric Wasserstein. The study examines the robustness of parametric adversarial divergences to rotations and additive noise by analyzing the divergence between MNIST and transformed versions. Different discriminators and formulations are considered, with the Wasserstein distance showing smoother variation compared to ParametricJS, especially for rotations. The linear ParametricJS does not saturate for rotations, indicating the discriminator's architecture significantly affects the induced parametric adversarial divergence. Complex architectures like CNNs can distinguish distributions better than a linear discriminator, which may be too weak for the task. The study also involves learning high-dimensional data using a dataset of handwritten images of the digit \"8\" with elastic deformations. The study involves training a convolutional VAE and a GAN on different resolutions to generate samples of handwritten digit \"8\" with elastic deformations. The VAE fails to produce convincing samples in high dimensions, while the GAN can generate sharp and realistic samples even in 512 \u00d7 512 resolution. The GAN can generate sharp and realistic samples even in 512 \u00d7 512 resolution by learning moments easier to match than directly matching the training set. Generating high-dimensional natural images is challenging due to the intrinsic complexity of scenes, which can be hidden in low resolution. A visual hyperplane task was designed to compare VAEs and GANs quantitatively using a new dataset of concatenated MNIST images. The VAE and GAN were trained on sets of 5 images from MNIST that sum up to 25. The GAN outperformed the VAE in capturing the data distribution, with GAN samples being sharp and realistic while VAE samples were blurry. The VAE samples are blurry with gray pixel values in high-resolution, while the WGAN-GP produces sharper distributions centered around the target sum of 25. Adversarial divergences are favored over traditional divergences for generative modeling. Adversarial divergences are preferred over traditional divergences for generative modeling due to their ability to account for the final task. They can be viewed as statistical task losses and may replace hand-crafted criteria in the future. Parametric adversarial divergences offer additional advantages and properties for evaluation criteria with minimal human intervention. Parametric adversarial divergences, such as the adversarial Wasserstein and MMD-GAN divergence BID24, offer more stable and meaningful learning signals for GANs compared to traditional divergences. These divergences correlate well with sample quality and are less prone to mode collapse, making them valuable for generative modeling tasks. Neural adversarial divergences, including Wasserstein, can be stabilized by regularizing the discriminator. Maximum-likelihood requires computing density q \u03b8 (x), not feasible for implicit models like GANs. Parametric adversarial divergences can be estimated with reasonable sample complexity by sampling from the generator. f-divergences like Kullback-Leibler and Jensen-Shannon are not defined or uninformative when p is not absolutely continuous w.r.t. q \u03b8, limiting their use for learning sharp distributions. Some integral probability metrics like Wasserstein and MMD are usable for any distributions, unlike f-divergences such as Jensen-Shannon which are limited when p is not absolutely continuous w.r.t. q \u03b8. The parametric adversarial Jensen-Shannon in GANs allows for learning realistic samples, despite instability. However, their results have limitations, ignoring x dependence and being non-parametric. They only consider convex consistent surrogate losses and provide upper bounds but not lower bounds on the sample. In their analysis, the authors discuss surrogate losses and sample complexity bounds. Experimental results compare parametric adversarial divergences induced by different discriminators under the WGAN-GP BID18 formulation. The model used is a mixture of 100 Gaussians with zero-covariance, and learned prototypes are shown in figures. The learned prototypes from a mixture of 100 Gaussians are compared using different discriminators. The linear discriminator only learns the mean of the training set, the dense discriminator produces blurry prototypes, while the CNN discriminator generates clear and recognizable prototypes. Using a CNN discriminator provides better task loss for generative modeling of images."
}