{
    "title": "ry80wMW0W",
    "content": "Hierarchical reinforcement learning methods are effective for planning flexible behavior in complex domains, but discovering subtasks remains a challenge. A novel algorithm based on the MLMDP framework allows for never-before-seen tasks by representing them as a linear combination of a basis set of tasks. Non-negative matrix factorization is used to discover a minimal basis set of tasks, leading to intuitive decompositions in various domains. Hierarchical reinforcement learning methods offer faster learning in complex state spaces and better task transfer by planning at multiple levels of detail. Algorithms like HAMS, MAXQ, and the options framework enable hierarchical abstraction, allowing agents to plan at a higher level of decision-making. Hierarchical reinforcement learning methods enable agents to plan at a higher level, choosing subtasks or actions to speed up learning and task transfer. Algorithms like HAMS and MAXQ require designers to specify subtask structures, critical for efficient learning. Some work focuses on automatically discovering appropriate subtasks based on the agent's state space properties, creating subtasks to reach bottleneck states like doorways in a domain of rooms. Hierarchical reinforcement learning methods involve identifying critical access points like doorways in a domain of rooms to create subtasks for efficient learning. Techniques include passive exploration, state transition graphs, graph clustering, and betweenness analysis to define subtask states. Hierarchical reinforcement learning methods involve identifying critical access points like doorways in a domain of rooms to create subtasks for efficient learning. Recent work has focused on learning options with a single or low number of termination states, which can be computationally expensive. A novel subtask discovery algorithm based on the Multitask linearly-solvable Markov decision process (MLMDP) framework has been introduced, allowing tasks to be linearly combined to solve tasks within the basis set. Non-negative matrix factorization is used to find an appropriate basis for subtask discovery. The text discusses the use of non-negative matrix factorization for subtask discovery in hierarchical reinforcement learning, with the potential for deeper hierarchies of subtasks. The method operates in a batch off-line setting and has immediate application to probabilistic planning. It contrasts with other methods that operate in online RL settings, highlighting the challenge of achieving a deeper hierarchical architecture or immediate generalization to novel tasks. The text discusses a multitask reinforcement learning method using a library of basis tasks to perform a variety of other tasks in a fixed environment. Each task is modeled as a finite-exit LMDP, where optimal policies compose naturally due to linearity in the Bellman optimality equation. This method allows for optimal performance of tasks within the subspace spanned by the basis tasks' boundary rewards. The LMDP BID21 is defined by a three-tuple L = S, P, R, where S is a set of states, P is a transition probability distribution, and R is an expected reward function. The agent's chosen action is a transition probability distribution over next states. Control costs are associated with energy-efficient actions, and the problem is regularized by the passive transition structure. The LMDP has rewards for interior and boundary states, and can be solved by finding desirability. The LMDP is solved by finding the desirability function z(s) = e V (s)/\u03bb, where \u03bb is a temperature-like parameter. The optimal control can be computed in closed form. Despite restrictions, the LMDP is generally applicable and can be used for non-navigational tasks. Difficulty arises in translating standard MDPs into LMDPs due to constructing action-free passive dynamics P, but in many cases, it can be approximated as a Markov chain under a uniformly random policy, making the problem 'entropy regularized'. The Multitask LMDP (MLDMP) operates by learning a set of tasks with identical state space and internal rewards, but different boundary reward structures. If a new task can be expressed as a linear combination of previously learned tasks, the same weighting can be applied to derive the optimal desirability function. If not, an approximate representation can still provide a jump-start in learning. The multitask module can be stacked to form deep structures. The multitask module can be stacked to form deep hierarchies by constructing higher order MLMDPs in a feudal-like architecture. This involves augmenting the state space with terminal boundary states called subtask states, transitioning into which corresponds to accessing the next level of the hierarchy. Solving the higher layer MLMDP yields optimal actions, making some transitions more likely and desirable for the current task. The hierarchical MLMDP model involves transitions that are more desirable for the current task, with some transitions being less likely under the passive dynamic. Instantaneous rewards in the lower layer are proportional to the difference between controlled and passive dynamics. The model includes a process where the agent transitions between states, including subtask states at higher layers, with rewards defined based on the higher layer state. The hierarchical MLMDP model allows for specifying reward structures for tasks at different layers. Control is passed back to the lower layer to achieve new tasks, with details left to the lower layer. The agent transitions between subtask states, solving tasks until reaching a boundary state. This work addresses learning a suitable task basis instead of assuming it a priori. To learn a suitable task basis for new tasks, the desirability function must be representable as a linear combination of a basis matrix Z. Using PCA (SVD) to decompose Z can retain as much variance as possible. The desirability function is the exponentiated cost-to-go, requiring Z to be non-negative. Non-negative matrix factorization can uncover a low-rank representation for subtask discovery. Non-negative matrix factorization aims to find a low-rank representation by decomposing a data matrix D and weight matrix W. The choice of decomposition factor k determines the level of abstraction, with a small k leading to higher abstraction. The decomposition minimizes a cost function based on \u03b2-divergence, ensuring a unique non-negative solution due to the strict positivity of the basis Z. The \u03b2-divergence collapses to statistical distances for different values of \u03b2, such as Kullback-Leibler and Euclidean distances. The representation of tasks in the environment is defined by the basis set of tasks taken against it, not just a factorization of the domain structure. The basis set of tasks can be a small fraction of all possible tasks in the space. The representation of tasks in the environment is defined by the basis set of tasks taken against it, not just a factorization of the domain structure. An n-dimensional Z matrix containing tasks to navigate to each point individually can aid in solving tasks in a space. Desirability functions for subtasks can be overlaid onto base domains in structured domains. The proposed scheme decomposes tasks into subtasks based on domain structure, forming an approximate cover for the space. Results show intuitive decomposition in two domains with hand-picked factors. The columns in D represent the cost-to-go for discovered subtasks, while Z represents single-state tasks. The proposed scheme decomposes tasks into subtasks based on domain structure, forming an approximate cover for the space. The columns in D represent the exponentiated cost-to-go for discovered subtasks in the nested rooms and hairpin domains. Subtasks do not correspond to single states but complex distributions over preferred states. The desirability functions for each subtask are plotted over the base domain, showing that the distributed patterns collectively cover the full space regardless of the decomposition factor chosen. The proposed scheme decomposes tasks into subtasks based on domain structure, forming an approximate cover for the space. The decomposition for different values of k results in adjusted subtasks to maximize expressiveness. There is no intrinsic ordering of subtasks, as long as they collectively represent the task space Z effectively. The scheme is applicable to various tasks beyond simple navigation, illustrated by its application to the TAXI domain. The agent operates in the product space of the base domain and possible passenger locations, resulting in 125 states. A decomposition with factor k = 5 reveals subtask structure in FIG2, each representing a policy over the full state space. The desirability function for each subtask is shown in a color-map, with each column corresponding to a subtask. The first column focuses on states where the passenger is in the Taxi, indicating a general pick-up action. The second column depicts a subtask with a uniform desirability function. The second column in FIG2 represents a subtask with a uniform desirability function seeking to enter states with the passenger at location A for drop-off. The proposed scheme discovers subtasks by approximating the desirability basis matrix Z, allowing for policies to get the passenger to pick-up/drop-off locations and have the passenger in the taxi efficiently. The scheme demonstrates a recursive and multiscale nature by considering a spacial domain inspired by cities. It decomposes the city into major communities, houses, rooms, and base states in a grid. The decomposition factors lead to the discovery of subtasks corresponding to the domain's multiscale nature. The scheme automates the discovery of subtasks and transitions in a domain by choosing decomposition factors. Designers need to specify the factors at each layer, similar to defining network connectivity in neural networks. Increasing the decomposition factor reduces approximation error, with some domains showing a point of diminishing returns. The dependence of the approximation error on the decomposition factor can be represented as f(k). The scheme automates the discovery of subtasks and transitions in a domain by choosing decomposition factors. Subtasks correspond to distributed patterns of preferred states, becoming more abstract at higher layers. Higher layer states represent entire rooms, houses, and communities. The paper defines an autonomous way to uncover the contents of higher layer states and transition structures. The paper introduces a scheme for automating the discovery of subtasks and transitions in a domain by selecting decomposition factors. Subtasks represent patterns of preferred states that become more abstract at higher layers, such as entire rooms, houses, and communities. The approximation error in Eqn. FORMULA2 exhibits elbow-joint behavior, encoding the high-level domain structure. Choosing the right subtasks can enhance learning and transfer between tasks, while choosing the wrong ones can impede progress. The approach provides a measure of subtask quality by evaluating Eqn.(1), allowing for comparison of different sets of subtasks and the concept of subtask equivalence. The paper introduces a scheme for automating the discovery of subtasks and transitions in a domain by selecting decomposition factors. A formal pseudoequivalence relation is defined on the set of subtasks, uncovering complex distributions over preferred states. Weight vectors are assigned to each state, representing the state in the data matrix. The weight vector represents states in the data matrix, with a stability function measuring changes in representation under state transitions. Unstable states are identified as boundaries between subtask regions, such as doorways. A duality exists between subtasks uncovered by the scheme and other methods. In a filter-stack method, four subtasks correspond to layer one decomposition, with each subtask representing a room in the domain. An example path through the domain illustrates changing representations for different states. Desirability functions are computed for each state along the path, approximated using a linear blend of subtasks. Task weights show the change in representation for different states. The stability function identifies unstable states, like 'doorways', where representation changes starkly. The novel subtask discovery mechanism based on low rank approximation in the LMDP framework uncovers intuitive decompositions in various domains. It is dependent on the task ensemble, yielding different subtask representations. The stacking procedure for hierarchical MLMDPs allows for straightforward iteration, leading to powerful hierarchical abstractions. Analytical probing of subtask quality and equivalence is enabled, although the approach currently relies on a specific aspect. The approach relies on a discrete state space but needs to scale to high dimensional problems using state function approximation schemes and online estimation of Z. The method could be extended by allowing for nonlinear regularized composition for more complex behaviors in the hierarchy. AMS acknowledges support from the Swartz Program in Theoretical Neuroscience at Harvard University."
}