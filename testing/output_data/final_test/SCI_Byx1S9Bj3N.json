{
    "title": "Byx1S9Bj3N",
    "content": "In a high-dimensional setting, a data-enriched model is proposed for related regression tasks with common and individual parameters structured by sparsity or group sparsity. The model considers data from a fixed number of tasks and uses convex functions to characterize parameter structures. An estimator is introduced for this model, along with an iterative estimation algorithm with geometric convergence. The sample complexity and non-asymptotic bounds for estimation error are analyzed, presenting a comprehensive statistical and computational analysis of inference in the data-enriched model. In high-dimensional settings, recent advances have been made in estimating structured parameters like sparsity and low-rank in small sample problems. Estimators consider a parametric model of the response based on n samples and the true parameter of interest. The unique aspect is that the number of samples is less than the parameters, making estimation possible. In real-world problems, natural grouping among samples leads to the superposition of common and individual parameters, which has gained interest in the statistical machine learning community. In the statistical machine learning community, the Data Enrichment (DE) model involves sharing information among different groups through common and individual parameters. The model focuses on high-dimensional small sample regimes and considers a system of coupled superposition models. The DE model focuses on high-dimensional small sample regimes with structured parameters. It considers linear models and extends to non-linear models like generalized linear models. In Multi-task learning, similar models involve sharing information among different groups through common and individual parameters. In a parameter matrix decomposition framework for Multi-task Learning (MLT), different types of constraints are assumed for parameter matrices B1 and B2. The work is related to a previous proposal where regression is regularized with specific norms on rows of matrices. Parameters of B1 are more general than common parameters, enforcing shared support of certain parameters while allowing for variation between tasks. B2 induces sparse variation between parameters of different tasks, similar to individual parameters in the previous model. The DE framework is more data efficient than BID17 as it shares common parameters and only requires a large total dataset. The DE model with sparse \u03b2 g 's has applications in various domains. Limited progress has been made in understanding suitable estimators for the data enriched model. Limited progress has been made in understanding properties of suitable estimators for the data enriched model. Existing theoretical guarantees for data enrichment are limited to sparsistency under certain conditions, while our estimator works for any structure induced by arbitrary convex functions. No computational results, such as rates of convergence of optimization algorithms, have been provided. No computational results exist in the literature for optimization algorithms associated with proposed estimators. Sets are denoted by curly V, matrices by bold capital V, random variables by capital V, and vectors by small bold v letters. The data enriched model is represented by per group design matrices and output vectors. A random variable is sub-Gaussian if its moments satisfy certain conditions. The sub-Gaussian norm of a variable is defined in terms of its one-dimensional marginals. The Data Enrichment (DE) estimator \u03b2 is proposed for recovering structured parameters induced by convex functions. The estimator succeeds under the Data EnRichment Incoherence Condition (DERIC), a weaker condition compared to others in the literature. A non-asymptotic bound on parameterwise estimation error is established assuming DERIC holds. The DE estimator is introduced for recovering structured parameters under the Data EnRichment Incoherence Condition (DERIC). A non-asymptotic bound on parameterwise estimation error is established, and an efficient algorithm DICER is presented for solving DE's objective. This is the first statistical estimation guarantee for data enrichment, with a compact form of the proposed DE estimator provided. The DE estimator is used for recovering structured parameters under the Data EnRichment Incoherence Condition (DERIC). The main assumptions of Theorem 1 involve the Restricted Eigenvalue (RE) condition in high dimensional statistics literature. The RE condition holds with high probability for the design matrix X defined in (6). The RE condition holds with high probability under the Data EnRichment Incoherence Condition (DERIC) for the design matrix X. Assumptions include i.i.d. random vectors and noise, with a set I satisfying certain conditions. Using DERIC and the small ball method, the sample complexity for satisfying the RE condition is elaborated in Theorem 2. The RE condition is satisfied under the Data EnRichment Incoherence Condition (DERIC) for the design matrix X. The sample complexity required for this condition is detailed in Theorem 2, which provides a lower bound for the RE condition with a high probability. Additionally, Theorem 3 establishes a probability bound for the distribution of random vectors and noise. The general error bound for the estimator is characterized by Corollary 1, which combines Theorem 1, Theorem 2, and Theorem 3. With enough samples, the error bound simplifies for the sparse DE estimator, resulting in bounded individual errors. The proposed Data enrIChER (DICER) algorithm uses a projected block gradient descent approach to recover a sparse vector from observations. The algorithm shows exponential decrease in error with each iteration, converging to the statistical error. With sufficient samples, the algorithm's iterations follow specific update rules with high probability. The DICER algorithm uses a projected block gradient descent approach to recover a sparse vector from observations, showing exponential decrease in error with each iteration. The number of samples and iterations of the DE algorithm geometrically converges to a scaled variant of the statistical error bound with high probability. The proof for each theorem and proposition is presented in detail, with necessary results stated as lemmas. The model requires at least one group with a certain sample complexity to recover the common parameter \u03b2 * 0. The proof involves simplifying the RE condition and manipulating terms to lower bound one term and upper bound another. The proof involves simplifying the RE condition and manipulating terms to lower bound one term and upper bound another. The first term is lower bounded using Lemma 1 under the DERIC condition, while the second term is shown to satisfy the bounded difference property. The proof involves showing that t 2 has the bounded difference property. By invoking the bounded difference inequality, we can bound the expectation of the second term. With probability at least 1 \u2212 e^(-\u03c4^2/2), we have a concentration bound.lemma provides a bound on the expectation for the random vector x. The sample complexities are simplified by taking \u03be = \u03b1 6. The inner product of two vectors a and b is calculated. The union bound is used to derive an inequality with \u03c3 as the maximum value. Lemma 3 provides a concentration bound with probability at least 1 \u2212 e^(-\u03c4^2/2). The convergence properties of DICER are analyzed by upper bounding the error of each iteration. The error decreases exponentially fast to the statistical error, with deterministic bounds established for iteration errors. The constants defined in Definition 3 play a crucial role in this analysis. The error of Algorithm 1 at each iteration is bounded deterministically, with the optimization error shrinking in every iteration. By choosing specific step sizes, the estimation error of the algorithm converges geometrically to the statistical error bound. Keeping certain arguments below 1 ensures this convergence. High probability upper bounds for certain parameters are established to maintain this condition. Establish high probability upper bounds for \u03c1 g, \u03b7 g, and \u03c6 g to ensure convergence of the algorithm. A recursive relation between errors of consecutive iterations is established, leading to a bound for each iteration. Lemmas 5 and 6 provide upper bounds with high probability for certain parameters. Establish high probability upper bounds for \u03c1 g, \u03b7 g, and \u03c6 g to ensure convergence of the algorithm. Lemma 6 provides an upper bound for a certain probability. Remember that a g \u2265 1 was arbitrary, so we pick it as a g = 2 n ng 1 + c 0g to determine the sample complexity for the shared component. Any lower bound on the RHS of the equation will lead to the correct sample complexity. Establish high probability upper bounds for \u03c1 g, \u03b7 g, and \u03c6 g to ensure convergence of the algorithm. Lemma 6 provides an upper bound for a certain probability. To simplify the conditions, we assume max g\u2208[G] \\ b g \u2264 1. The sample complexity is determined by bounding terms in equation (26) for \u00b5 g = 1 agng by 1. By substituting values and considering constraints, we find that any positive lower bound of d g will satisfy the condition, with d g > 1. Establish high probability upper bounds for \u03c1 g, \u03b7 g, and \u03c6 g to ensure convergence of the algorithm. The sample complexity is determined by bounding terms in equation (26) for \u00b5 g = 1 agng by 1. Any positive lower bound of d g will satisfy the condition, with d g \u2265 1. We determine b g from previous conditions, setting b g = 1 for linear convergence. The analysis uses tail bounds for coefficients to clarify probabilities. We focus on bounding the probability P (\u03c1 \u2265 r(\u03c4 )). Establishing high probability upper bounds for \u03c1 g, \u03b7 g, and \u03c6 g is crucial for algorithm convergence. The sample complexity is determined by bounding terms in equation (26) for \u00b5 g = 1 agng by 1. The analysis focuses on bounding the probability P (\u03c1 \u2265 r(\u03c4 )). The proofs of Lemmas used in the theorems are presented, showing the weighted summation and lower bounds. Lemma 1's lower bound is a direct consequence of Lemmas 7 and 8. Lemma 7 states conditions for any unit vector u, while Lemma 8 discusses a soft indicator function. The proof involves Rademacher random variables and sub-Gaussian vectors to bound the expected supremum. Lemma 1's lower bound is proven using Rademacher random variables and sub-Gaussian vectors. The proof involves bounding the expected supremum of a function. The upper bound of the second term in the context of Lemma 1 involves sub-Gaussian random vectors and a substitution of variables."
}