{
    "title": "rytNfI1AZ",
    "content": "For efficient deployment of deep neural networks on embedded hardware, representing each weight parameter with a single bit is ideal. Error rates can increase with this requirement, but our approach improves error rates on various datasets by using 1-bit-per-weight deep convolutional neural networks. By simplifying existing methods and applying scaling factors, we achieve low error rates on CIFAR-10, CIFAR-100, and ImageNet with models requiring less than 10 MB of parameter memory. Our approach improves error rates on various datasets by using 1-bit-per-weight deep convolutional neural networks. Error rates halve previously reported values for CIFAR, and significant improvements are shown for networks that overfit by not learning batch normalization parameters. Training for 1-bit-per-weight is as fast as full-precision networks with better accuracy, achieving about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. Fast parallel computing resources, such as GPUs, have been crucial for the success of deep neural networks in computer vision tasks. However, GPUs are expensive and energy-intensive due to their use of single-precision floating point computation. There is a need to reduce reliance on high-end computing resources for embedding deep-learning methods in resource-constrained systems like robotics and IoT devices. In this paper, the authors address the challenge of reducing the resource burden of deep neural networks by exploring methods for deploying Convolutional Neural Networks (CNNs) with weights represented by a single bit each. They report a significant reduction in the gap between networks using standard precision and those using single-bit weights, leading to improved error rates. This approach has potential applications in custom hardware and has shown speedups on regular GPUs. The study explores methods for deploying CNNs with single-bit weights to reduce resource burden. Significant speedups on regular GPUs have been achieved, although not yet possible with standard libraries. The work was first presented in a workshop abstract and talk. ResNets outperform older CNNs like Alexnet and VGGNet in accuracy with fewer parameters and FLOPs. ResNets reduce parameters by using global-average-pooling layers and skip-connections for deeper networks. Despite their simplicity, ResNets still improve accuracy with more parameters. Wide Residual Networks are chosen for their efficiency in deployment on custom hardware. In this paper, Wide Residual Networks are used for better accuracy in less training time. To optimize deployment on hardware-constrained mobile devices, recent focus has been on compressing learned parameters and reducing computation precision in neural networks. Strategies include reducing weight precision during training and pruning trained neural networks. Recent focus has been on compressing learned parameters and reducing computation precision in neural networks by methods such as pruning, quantizing weights, and modifying network architectures. A study on weight-binarization methods showed promising results on convergence. One approach aims for simplicity and efficient convolution operations without multipliers. The study focused on simplifying deep CNN variants for custom hardware implementations, starting with a state-of-the-art baseline like Wide Residual Networks. The goal was to train for 1-bit-per-weight with minimal changes to the baseline, prioritizing simplicity without sacrificing accuracy. The study exceeded objectives for the full-precision baseline network. The study surpassed error rates for CIFAR-10 and CIFAR-100 using Wide ResNets with just 20 convolutional layers, achieving a significant error-rate drop by not learning per-channel scale and offset factors in batch-normalization layers. This method is most beneficial for networks that overfit, gaining around 3% in test-error rate for CIFAR-100. Our study achieved lower error rates for 1-bit-per-weight compared to full-precision weights on various image classification datasets, surpassing previous results for CIFAR-10/100. We used Wide ResNets and introduced a fixed scaling method to reduce error rate increases relative to full-precision weights, with training requiring the same number of epochs. Our innovation introduces a fixed scaling method for each convolutional layer to maintain activations and gradients with minimal change in standard deviation. We combine this with a warm-restart learning-rate method to achieve close-to-baseline results for the 1-bit case in fewer training epochs. The approach involves using 1-bit-per-weight at inference time by applying the sign function to real-valued weights during training, updating full-precision weights using SGD. Previous methods required training for many epochs or using different techniques. Our innovation introduces a fixed scaling method for each convolutional layer to maintain activations and gradients with minimal change in standard deviation. The approach involves using 1-bit-per-weight at inference time by applying the sign function to real-valued weights during training. The standard deviation of the sign of weights in a convolutional layer with kernels of size F \u00d7 F will be close to 1, assuming a mean of zero. During training, a fixed scaling method is introduced for each convolutional layer to maintain activations and gradients with minimal standard deviation change. The sign of weights is used for 1-bit-per-weight at inference time, with a layer-dependent scaling applied to the sign of weights. This scaling ensures the standard deviation of forward-propagating information is equal to that of full-precision networks. In implementation, the sign of weights in each layer is multiplied by a constant unlearned value, and during inference, a scaling layer is used to deploy weights using \u00b11. Custom hardware implementations can perform convolutions without multipliers using 0 and 1 for weight storage and \u00b11 for deployment. Scaling weights explicitly during training is crucial to prevent slow learning, requiring layer-dependent learning rates to cancel out the scaling. This approach is faster and less complex than full-precision methods. The differences in training with reduced precision involve processing convolutional weights differently for forward and backward propagation, using post-activation and identity mapping for residual connections in ResNets, and utilizing wide residual networks instead of very deep ones. The network design includes 18-layer ResNets for ImageNet and mainly 20-layer networks for other datasets, with some results for 26 layers reported. Each residual block consists of two convolutional layers with batch normalization and ReLU layers. For CIFAR-10/100, 20 layers typically produce the best results due to not learning batch-norm scale and offset parameters. The baseline ResNet design used has minor differences compared to others, detailed in Appendix A. Training methods include cross-entropy loss, minibatches of size 125, and momentum of 0.9. For CIFAR-10/100, SVHN, and MNIST, larger weight decay of 0.0005 is used to address overfitting in Wide ResNets. ImageNet32 and full ImageNet use a weight decay of 0.0001. Standard data augmentation is applied, with the addition of cutout in some experiments. The design follows a standard pre-activation ResNet structure with specific output channels for different layers based on the dataset. The final convolutional layer is followed by batch-normalization and global-average-pooling before softmax. Downsampling blocks reduce spatial dimensions by half, with downsampling used in convolutional layers where output channels increase. Average pooling is used in the residual path for downsampling. Padding and cropping are applied to datasets for training. When training on CIFAR-10/100 and SVHN, batch-norm layers do not learn scale and offset factors, initialized to 1 and 0 in all channels. Biases for convolutional layers are also not learned. Running averages are used for batch-norm layers in inference mode. When not learning batch-normalization parameters, a small benefit was found in calculating the batch-normalization moments used in inference only after training had finished. To achieve best results using this method with matconvnet, it is important to set the parameter to avoid divisions by zero to 1 \u00d7 10 \u22125. The design involves exchanging the ordering of the global average pooling layer and the final weight layer, making the final weight layer a 1 \u00d7 1 convolutional layer with as many channels as there are classes in the training set. The design involves changing the order of the global average pooling layer and the final weight layer, making the final weight layer a 1 \u00d7 1 convolutional layer with as many channels as there are classes in the training set. This architecture is similar to BID17 and BID25. The method uses a 'warm restarts' learning rate schedule that reduces the learning rate from 0.1 to 1 \u00d7 10 \u22124 according to a cosine decay, speeding up convergence. The total number of epochs is restricted to a maximum of 254 epochs. For CIFAR-10/100, experiments show that test error rates after 32 epochs are close to those after 126 or 254 epochs. Standard data augmentation includes flipping and padding images, with a minor modification of using random integers for padding. \"Cutout\" involves removing patches from training images. The method involves selecting patches from raw training images to remove, achieving state-of-the-art results on CIFAR-10/100. Larger cutout patches were used for CIFAR-100 compared to previous reports. Patches of size 18x18 were chosen for both CIFAR-10 and CIFAR-100. Cutout was not applied to other datasets, and experiments were conducted on various databases including CIFAR-10, CIFAR-100, SVHN, ImageNet32, and ILSVRC ImageNet. Uniform random integers were used for padding instead of image pixel values. RGB images datasets like CIFAR-10, CIFAR-100, SVHN, ImageNet32, and ILSVRC ImageNet were used for experiments. Wide ResNets with different configurations were utilized, with MATLAB and GPU acceleration. Wide ResNets with 160 channels in the first spatial scale were used for the full ImageNet dataset. For the full ImageNet dataset, 18-layer Wide ResNets with 160 channels in the first spatial scale were used. The networks are denoted as 18-2.5, corresponding to width 2.5\u00d7 on this dataset. Error rates for CIFAR-10/100, SVHN, ImageNet32, and full ImageNet are listed, with different augmentation techniques used. Results for single-center-crop and multi-crop testing on ImageNet are provided. Our full-precision ImageNet error rates are slightly lower than expected for a wide ResNet, possibly due to the lack of color augmentation. Comparison results from previous work on Wide ResNets and error rates on CIFAR-10, CIFAR-100, and ImageNet32 datasets are shown. The state-of-the-art for SVHN without augmentation is 1.59%, and with cutout augmentation is 1.30%. Our full-precision result for SVHN is 1.75%, close to the state-of-the-art, even though we used a smaller ResNet with fewer parameters and training epochs. Additional results on training models using the sign of weights and quantized activations are also discussed. Our baseline full-precision 10\u00d7 networks, trained with cutout, outperform deeper Wide ResNets trained with dropout. Even without cutout, our 20-10 network surpasses the CIFAR-100 error rate reported for the same network by over 2%. This improved accuracy is attributed to not learning batch-norm scale and offset parameters. Our 1-bit networks also show promising results. In comparison with training very wide 20\u00d7 ResNets on CIFAR-100, using cutout augmentation in the 10\u00d7 network reduces error rates effectively. The warm-restart method enables convergence to good solutions after 30 epochs, with further reduction in test error rates by 2% to 5% when training longer to 126 epochs. The 20-4 network can model CIFAR-10/100 training sets with over 99% accuracy, but the 1-bit version shows reduced modelling power. The 1-bit version shows reduced modelling power compared to the full-precision method. Training longer with cutout augmentation improves error rates, with peak performance reached after 126 epochs. The 1-bit method achieved error rates of 0.81%, 0.36%, and 0.27% after 1, 6, and 14 epochs respectively, while the full-precision method achieved 0.71% after 1 epoch and 0.28% after 6 epochs. The error rates on the test set and training set for 20-4 ResNets and 20-10 ResNets with and without cutout are shown. Both full-precision Wide ResNets and 1-bit-per-weight versions benefit from not learning batch-norm scale and offset parameters. The warm-restart training schedule BID18 improves accuracy in the 1-bit-per-weight case. The learning-rate schedule drops from 0.1 to 0.01 to 0.001 after 85 and 170 epochs. The learning rate decreases from 0.1 to 0.01 to 0.001 after 85 and 170 epochs. Methods lower the final error rate by around 3% absolute by not learning batch-norm parameters. Warm-restart method enables faster convergence for full-precision case but not significant in reducing error rate. For 1-bit-per-weight case, best results achieved by using warm-restart and not learning batch-norm parameters. Smaller error rate gap expected between full precision and 1-bit-per-weight networks as full precision error rate decreases. Trend shows gap in error rate tends to grow as error rate in full-precision network increases. The gap in Top-1 error rates vs full precision error rates for best performing networks on six datasets is shown in Figure 1. Our study is the first to consider more than two datasets. The error rate and gap reported by BID22 for networks using 1-bit weight method are also plotted. Better full-precision accuracy results in a smaller gap. The challenge is to derive theoretical bounds predicting the gap, which depends on various factors including the method used for 1-bit-per-weight models. The study compares the loss function between 1-bit and 32-bit weight cases, noting a higher loss for 1-bit weights. The reason for this discrepancy, whether due to precision loss or gradient mismatch, requires further investigation. The gap in error rates for different network depths on CIFAR-10/100 datasets is smaller, prompting the need for more research. The approach differs from BWN method in not calculating mean absolute weight values for each output channel. The study compares the loss function between 1-bit and 32-bit weight cases, noting a higher loss for 1-bit weights. The approach differs from BWN method in not calculating mean absolute weight values for each output channel. Our method enables faster training and requires fewer parameters. It remains to be tested how our scaling method compares in cases combining single-bit activations and 1-bit-per-weight. The design choice of not learning batch normalization parameters for CIFAR-10/100, SVHN, and MNIST datasets was made to address overfitting in Wide ResNets. Inspired by label-smoothing regularization, the final all-to-all layer of ResNets was replaced with a 1x1 convolutional layer and a batch-normalization layer to control the standard deviation of inputs to the softmax. This ensures batches flowing into the network are normalized. The batch-normalization layer in Wide ResNets ensures consistent standard deviations for batches entering the softmax layer, preventing entropy increase and lower confidence. Learning batch-norm parameters in other layers led to overfitting and higher test error rates, so it was removed. Results show benefits for both full-precision and 1-bit-per-weight networks, surpassing previous models on the same architecture. The use of 1\u00d71 convolutional projections in downsampling residual paths creates 2 layers. The method is not suitable for datasets like ImageNet32, where batch normalization parameters help reduce test error rates. Comparison with SqueezeNet BID13 shows memory savings by replacing 3x3 kernels with 1x1 kernels and deep compression techniques. Our method reduces the model size of the baseline SqueezeNet architecture by a factor of 32, with an accuracy gap compared to ResNets. Deep compression techniques in SqueezeNet were able to reduce the model size by a factor of 10, while our method achieves a reduction by a factor of 32. Our method reduces model size by a factor of 32 with a small accuracy loss compared to full-precision models. Single-bit-weight models achieve better than 33% top-1 error, requiring 8.3 MB for the model's weights. The focus is on reducing weight precision to a single bit for model compression and efficient inference. Our method focuses on reducing computational load by using quantized ReLUs or binary step functions. We enable learning of scale and offset factors in the first batch-normalization layer, eliminating the need for pre-processing. The learned offset ensures the input to the first ReLU is never negative. In our method, the first ReLU is always non-negative. Weight layers can be seen as a sequence of batch-norm, ReLU, and global average pooling. Including ReLU early in training shows differences but not at completion. Wide ResNet's first convolutional layer has a constant number of output channels. There is no need to impose this constraint. The first layer in our method always shares the same number of output channels as all blocks at the first spatial scale, increasing simplicity in network definition. We tested our method on plain all-convolutional networks without skip connections, adjusting initial weights standard deviations to improve convergence. The change to be proportional to 2 instead of \u221a 2 was the only adjustment made in Equation (1). Convergence was slower compared to ResNets, but there was only a small accuracy penalty after 126 epochs. The experiment showed that ResNets deeper than 20 layers had a significant advantage over plain all-convolutional networks. The data in Figure 7 compares residual networks with all-convolutional networks with width 4\u00d7, containing about 4.3 million learned parameters."
}