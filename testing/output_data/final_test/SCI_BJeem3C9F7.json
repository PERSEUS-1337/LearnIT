{
    "title": "BJeem3C9F7",
    "content": "Modelling 3D scenes from 2D images is a challenging task in computer vision. The pix2scene approach uses deep generative models to infer the geometric properties of a scene from images. It can predict scene structure from different viewpoints and utilizes adversarial learning to generate scene representations. A differentiable renderer is employed to train the 3D model using only images. The model's generative ability is demonstrated on custom datasets and ShapeNet, with a focus on supporting 3D spatial reasoning. The ability to infer 3D structure from 2D images is crucial in computer vision for various applications. While multiple images can aid in 3D structure inference, developing models that can do so from a single image is important. However, single image 3D recovery is a challenging task due to its under constrained nature. In computer vision, inferring 3D structure from a single image is a challenging task that requires prior knowledge and visual cues. Building a machine learning model for this task typically requires a strong bias or supervision, which may not always be available. The long term goal is to infer 3D structure from realistic scenes using unsupervised learning methods. This paper introduces a method based on adversarial learning and a unique 3D representation (surfels) to achieve this goal. The paper introduces a method based on adversarial learning and a unique 3D representation (surfels) to infer 3D structure from realistic scenes using unsupervised learning methods. The proposed approach suggests learning an implicit 3D representation that produces only the 3D geometry relevant for a specific viewpoint, as opposed to explicit representations that store all rendering-relevant information from a given 3D space. The implicit surfel-based representation adapts resolution to the viewpoint and contains the full scene in a high-dimensional latent variable. Camera-facing surfels are used to serialize the latent variable for rendering specific views, allowing for the recreation of novel surfel representations from unobserved viewpoints. The method generates surfels for newly unoccluded regions as the camera moves, minimizing the number of primitives needed. Pix2Scene is a deep generative-based model that directly models the 3D structure of a scene from images without requiring 3D groundtruth or image annotations. It is based on the Adversarially Learned Inference (ALI) approach, where the learned latent space embeds 3D information of the scene and is decoded to a view-dependent 3D surface. The resulting image is evaluated by an adversarial critic. The paper proposes a novel unsupervised method for 3D understanding from a single image using a surfel-based differentiable 3D renderer. It introduces a new implicit 3D representation based on view-space surfels and a 3D understanding evaluation benchmark called 3D-IQTT. The paper introduces an unsupervised method for 3D understanding from a single image using a surfel-based differentiable 3D renderer. It involves mental rotation and estimating camera pose to generate and reconstruct 3D objects. The approach is compared to traditional inverse graphics tasks and uses Variational Auto-Encoder (VAEs) to disentangle factors of variations from images. Unlike previous methods, this approach is fully unsupervised and implicitly generates 3D structure from single images. The paper introduces an unsupervised method for 3D understanding from a single image using a surfel-based differentiable 3D renderer. It involves mental rotation and estimating camera pose to generate and reconstruct 3D objects. Unlike previous methods, this approach is fully unsupervised and implicitly generates 3D structure from single images. The approach learns a latent representation for the underlying scene, which can later be used to render from different views and lighting conditions. Our approach is not susceptible to restrictions imposed by meshes or other scaling issues and has the potential to adapt to arbitrary scene configurations. Our implicit approach represents 3D scenes using a high-dimensional latent variable, decoded into a viewpoint-dependent representation of surface elements. This compact representation avoids the limitations of mesh-based methods and allows for efficient generation of visible scene elements. The compact representation of 3D scenes using surfels allows for efficient generation of visible scene elements. Surfels are represented by tuples containing position, surface normal vector, and reflectance values. This representation focuses on modeling structural properties such as geometry and depth in the scene. The surfel representation of 3D scenes reduces the number of surfels by projecting onto pixels in the rendered image. Position parameters are simplified to distance along a ray. The rendering process involves differentiable stages for gradient-based optimization. Forward-propagation maps surfels to pixels and computes pixel colors, while back-propagation directs gradients to projected surfels. The first stage of rendering involves mapping surfels to pixels through ray object intersection, requiring a fast rendering engine for each learning iteration. Conventional ray tracing algorithms are inefficient for our single-image rendering approach. Our generator proposes one surfel per pixel in the camera's coordinate system, allowing for efficient rendering of 128x128 surfel-based scenes in PyTorch. The implementation of the differentiable renderer can render a 128 \u00d7 128 surfel-based scene in under 1.4 ms on a mobile NVIDIA GTX 1060 GPU. The color of a surfel depends on material reflectance, position, orientation, ambient and point light source colors. Adversarial training allows the generator network to capture the target distribution by competing with a critic network. Pix2scene employs bi-directional adversarial training for modeling. Pix2scene utilizes bi-directional adversarial training to model surfel distribution from 2D images. ALI and Bi-GAN extend the GAN framework by incorporating an inference mechanism. The critic discriminates in both data and latent space, maximizing the adversarial value function over two joint distributions. The model includes an encoder network for capturing latent space distribution and a decoder network for mapping a fixed latent distribution. The decoder network in the Pix2scene model maps a fixed latent distribution to a 3D surfel representation, which is then rendered into a 2D image. This image is input to the critic for distinguishing from real image data. The decoder generates surfels depth and normal from a noise vector conditioned on the camera pose, which are then rendered into a 2D image for evaluation by the critic. The decoder in the Pix2scene model generates surfels' depth and normal from a noise vector conditioned on the camera pose. To ensure consistency, the local surface normal is estimated by considering real-world surfaces are locally planar, with normals constrained to be in the half-space of visible normal directions from the camera's viewpoint. The approach enforces consistency between predicted depth and computed normals, incentivizing realistic depth predictions. Wasserstein-GAN formalism with 1-Wasserstein distance is used for stable training dynamics. The network architectures and training hyperparameters are detailed in appendix A, with Conditional Normalization BID5 BID23 for conditioning the view point. The model uses the view point to guide the encoder, decoder, and discriminator networks. Affine parameters in Batch-Normalization layers are replaced with learned representations based on the view point. The objective includes a bi-directional reconstruction loss to ensure reconstructions stay close to inputs. The model can reconstruct 3D scenes and generate new ones, evaluated on 3D-IQTT for spatial reasoning. The model's 3D understanding capability is evaluated on 3D-IQTT, a spatial reasoning task using unlabeled training data and few labeled examples. Evaluation includes Hausdorff distance and MSE for 3D reconstruction accuracy. Datasets range from simple to complex scenes for qualitative evaluation. The percentage of correctly answered questions is the evaluation metric for 3D-IQTT. The 3D scenes for evaluation in 3D-IQTT task include simple box shapes, basic 3D shapes like box, sphere, cone, torus, teapot, and objects from the ShapeNet dataset. Each scene is rendered into a 128 \u00d7 128 \u00d7 3 image from a random camera angle. The task involves answering questions based on images, including identifying a randomly rotated version of a reference shape. The training set consists of 200k questions with scenes rotated to demonstrate 3D shape understanding. Only a few multiple-shape scenes are labeled with correct answer information. Validation and test sets each have 100k labeled questions. More details on experimental setup and evaluation can be found in the appendix. Figure 2 displays input Shape scenes data, reconstructions, depth, and normal maps. Depth map encoding indicates proximity to the camera, while normal map colors represent orientation. The curr_chunk discusses the evaluation of a model's performance on reconstructing scenes from different viewpoints, showcasing quantitative results in tables and qualitative results in figures. It also highlights the ability of the model to reconstruct unobserved views by inferring latent codes and rendering different views. The results show the Hausdorff distance and mean squared error for reconstructing scenes from various unobserved view angles. The curr_chunk discusses how the angle increase affects the reconstruction error in shape scenes. It shows qualitative results of pix2scene inferring scene extents accurately. The model is trained on ShapeNet objects and can generate correct 3D interpretations. Conditioning the generator on different target classes is also explored. The curr_chunk introduces a quantitative evaluation called 3D-IQ test task (3D-IQTT) for 3D reconstruction. Inspired by the mental rotation task in cognitive assessment, the evaluation measures the speed and accuracy of solving tasks similar to IQ tests like the Woodcock-Johnson test. The quantitative evaluation for 3D reconstruction, 3D-IQTT, is inspired by the mental rotation task in cognitive assessment. It involves using 3D objects with distractor answers, where the subject must pick the correct answer out of 3 rotated versions of the reference object. The approach aims to learn accurate embeddings of shapes and involves training pix2scene in a semi-supervised setting with labeled and unlabeled data. The latent vector encodes the 3D object and estimates the camera pose. The training for 3D reconstruction involves using 3D objects with distractor answers. The approach aims to learn accurate embeddings of shapes by training pix2scene in a semi-supervised setting. The latent vector encodes the 3D object and estimates the camera pose, with additional loss terms enforcing object consistency and maximizing distance between reference and distractors. Mutual information between object and camera pose is minimized to disentangle information sources. The mutual information is optimized using a neural network in an adversarial paradigm. The pix2scene objective includes this loss to minimize mutual information estimate. Results from 3D-IQTT test task show that Pix2scene outperforms CNN baselines in understanding 3D structure. The model infers latent 3D representations for images and selects answers based on L2 distance. The model infers latent 3D representations for images and selects answers based on L2 distance. Two different baselines were compared, one trained on labeled samples with fully-connected layers, and the other trained on contrastive loss. Results in Table 3 show 3D-IQTT performance for the method and baselines, with the baselines slightly outperforming a. The Siamese CNN's poor performance in interpreting 3D structure is attributed to its focus on pixel similarities. In contrast, pix2scene excelled by utilizing 3D object knowledge. A generative approach was proposed to learn 3D properties from single images in an unsupervised manner. The model estimates scene depth and reconstructs the input. A new IQ-task was introduced to evaluate 3D understanding capabilities, aiming to become a standard benchmark for different 3D representations. The current model requires knowledge of lighting and material properties. Future work aims to learn complex materials and texture, along with modeling lighting properties. Pix2scene consists of encoder, decoder, and critic networks. The decoder architecture is similar to DCGAN with adjustments for high-resolution images. Conditional normalization is used to condition the camera position. The model is trained for 60K iterations with a batch size of 6 and images of resolution 128 \u00d7 128 \u00d7 3. In experiments, diffuse materials with uniform reflectance are used for both input and generator sides. Camera properties include position, viewing direction, and focal length. Camera positions are randomly sampled on a sphere for 3D-IQTT task and on a spherical patch for other experiments. The camera always looks at a fixed point in the scene as its position changes. Camera properties are shared between input and generator sides. In the 3D-IQTT task, the camera pose is not assumed to be known, and the view is estimated as part of the latent representation. Experiments were conducted with single and multiple point-light sources, with random colors and positions. The lights act as spot lights with energy attenuating quadratically with distance. An ablation study showed that perfect knowledge of lights is not necessary for the model to learn 3D structure. Random lights were used in the generator for experiments. The Pixel2Scene architecture is used to learn the 3D structure of data, with random lights on the generator side affecting shading. Evaluation of 3D reconstructions is done using the Hausdorff distance as a measure of similarity. The architecture has higher capacity on decoder and critic for this challenging task, and experiments are conducted without conditioning the networks with camera pose. A statistics network estimates and minimizes mutual information between latent code dimensions. The Pixel2Scene architecture is utilized for learning 3D structure, with emphasis on scene-based and view-based information. The baseline network's architecture and contrastive loss for training are detailed. Scene reconstructions and rendering different views are demonstrated, showcasing the model's capabilities. The Pixel2Scene architecture focuses on learning 3D structure using scene-based and view-based information. FIG4 displays recovered shading, depth, and normal images from reconstructions of complex scenes like bedrooms and bunny."
}