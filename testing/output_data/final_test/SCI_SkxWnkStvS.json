{
    "title": "SkxWnkStvS",
    "content": "Neural architecture search focuses on designing an efficient search space. Xie et al. (2019a) discovered that randomly generated networks from the same distribution perform similarly, leading to the proposal of using graphons as a new search space. Graphons are scale-free probabilistic distributions that allow for NAS using fast, low-capacity models that can be scaled up when needed. An algorithm for NAS in the space of graphons has been developed and shown to outperform DenseNet and other baselines on ImageNet. Neural architecture search focuses on designing an efficient search space that balances considerations like space size and smoothness for accelerated search. A common technique is to search for small cell structures with about 10 operations each, which can be stacked to form larger networks. This approach allows cells found on one dataset to work effectively on another. However, it cannot optimize the overall network structure, which is typically divided into stages operating on different spatial resolutions. Neural architecture search focuses on designing efficient search spaces with small cell structures containing about 10 operations each. These cells can be stacked to form larger networks, divided into stages operating on different spatial resolutions. Stages contain multiple layers or multi-layer structures, with connections between cells within the same stage. DenseNet introduces connections between every pair of cells within a stage, emphasizing the difference between a stage and a cell. The paper focuses on network organization at the stage level rather than the cell level. It discusses how the stage structure can be sampled from probabilistic distributions of graphs, such as ER, WS, and BA models, leading to high-performing networks with low variance. The random graph distribution is highlighted as the main causal factor behind network performance. The paper introduces the concept of graphons as a superior search space for NAS compared to traditional random graph distributions like ER, WS, and BA models. Graphons are measurable functions defined on [0, 1] 2 \u2192 [0, 1] and provide a probabilistic distribution from which graphs can be drawn. They are considered as limit objects of Cauchy sequences of finite graphs as n \u2192 \u221e. Graphons are limit objects of Cauchy sequences of finite graphs under the cut distance metric. The Barab\u00e1si-Albert (BA) model generates random graphs with increasing numbers of nodes, converging to a graphon. The model starts with a seed graph and adds new nodes with edges based on node degrees. Different parameterizations result in the same adjacency matrix, suggesting that searching in the parameter space may revisit the same configuration. The graphon space is more efficient than the parameter space for searching. Graphons offer a unified and expressive space compared to common random graph models like WS and ER. These models only capture a small fraction of all possible graphons. Graphons allow for new possibilities such as interpolation and combining different random graph models. They are scale-free, enabling the sampling of arbitrary-sized stage-wise architectures with identical layers. This facilitates expensive NAS on small datasets using low-capacity models to build large models effectively. The study compares stage-wise graphs found by the proposed method with DenseNet and WS random graph model, showing consistent outperformance across various model capacities. The paper introduces graphon as a search space for stage-wise neural architecture, emphasizing the connection between theory and practice. The approach focuses on fair comparisons and operationalizes the theory on graphon for representation, scaling, and search of neural stage-wise graphs. In previous works, researchers explored different cell structures in neural architecture search. Some focused on single cell structures with downsampling, while others searched for two types of cells. DARTS and its variations enabled gradient-based optimization for cell structures. However, there has been less attention on NAS for stage-wise graph structures. Random graphs have been used for this purpose, but there is a need for more research in this area. The graph has received little attention, with studies showing that random graphs from Watts-Strogatz models outperform manual designs for stage-wise structures. Different approaches have been explored, such as redefining stage-wise graphs to allow multiple inputs but only one output per node, evolving connections between residual blocks for video processing, and optimizing downsampling and upsampling organization. NAS for stage structures is seen as a promising research direction with untapped potential. Weight sharing among architectures and using gradient policy to select subgraphs are also explored for accelerating search. In machine learning, graphon is utilized for hierarchical clustering and graph classification. A metric space is defined as a set with a distance function. The focus of the paper is to validate the operationalization of graphon theory, leaving weight sharing for future work. A metric space is a set with a distance function that defines the distance between points and satisfies specific properties. A complete metric space is one where every Cauchy sequence converges to a point in the set. The real numbers with the absolute difference metric form a complete metric space, while some spaces like the rational numbers under the same metric are not complete. The completion of a metric space involves adding limit points to the set. The completion of a metric space involves adding limit points. In the case of graphs, weighted undirected graphs are considered, where nodes and edges have associated weights. Every Cauchy sequence of weighted graphs converges to a graphon. The cut distance \u03b4 is defined in terms of discrete versions d and \u03b4. The cut size in a partition of graphs is defined as cut(S, T, G) = vi\u2208S,vj \u2208T \u03b1 i \u03b1 j \u03b2 ij. The metric d is a sensible measure for random graphs, with a low distance between identically distributed graphs. For directed graphs, a digraphon is defined as a 5-tuple (W 00, W 01, W 10, W 11, w) to describe edge probabilities. The adjacency matrices for representing directed acyclic graphs in neural networks need to be upper-triangular with a zero vector on the diagonal, which requires a total ordering of nodes. This is crucial for the directed edges to be properly defined. The graphon is approximated using a step function approach with a matrix B as the adjacency matrix of a weighted graph, converging to the graphon as n tends to infinity. The text discusses techniques for sampling stage-wise graphs from a graphon representation and NAS for graphon. It explains how to sample a simple graph with n nodes from a finite graph with edge weights, and how to upsample a graph to have more than n nodes using the graphon metric space. The text discusses creating weighted graphs using the k-fold blow-up procedure and fractional blow-up to handle cases where the upsampling factor is not an integer. It also mentions bounding the cut distance between graphs based on the maximum difference between edge weights. The text introduces a search algorithm using Gumbel softmax for optimizing input connections in discrete objects. It highlights the difference between the proposed upscaling methods and conventional interpolation techniques. The k-fold blow-up graph is shown to be closer to the original graph under moderate conditions. The search algorithm optimizes input connections in a stage-wise graph by enabling exploration through perturbation from \u03b2. Different cells in the same stage collaborate by pitting input combinations against each other, sampling subsets of nodes and assigning structural parameters to find the adjacency matrix. The search algorithm assigns structural parameters to input subsets to find the best adjacency matrix for nodes to collaborate. Model parameters are separate for nodes in different subsets to facilitate collaboration. Outputs from nodes in the same subset are aggregated using concatenation. The input to each node is a convex combination of outputs from all subsets. Cross-entropy loss and stochastic gradient descent are used for optimization. The input subset with the highest weight in the final adjacency matrix is selected after the search completes. The search algorithm assigns structural parameters to input subsets to find the best adjacency matrix for nodes to collaborate. The goal is to find a probabilistic distribution of random graphs, not a single graph. The last phase of the search can be viewed as a Markov chain drawing samples from a stationary distribution. The graphon is computed as the average of the adjacency matrix found in the last phase. The experiment focuses on improving and comparing with DenseNet, following its stage-wise structure. The search starts from the graphon corresponding to the WS distribution with specific parameters. The search algorithm assigns structural parameters to input subsets to find the best adjacency matrix for nodes to collaborate. The search is limited to subsets differing by 1 edge, completed on CIFAR-10 in 24 hours on 1 GPU. Four groups compare DenseNet variants, scaled to match model capacity. Parameters are adjusted for fair comparisons, with the largest graph used for DenseNet-264. The study follows standard hyperparameters and data augmentation techniques used by DenseNet for training on ImageNet. Two baseline models are introduced, one with randomly deleted edges from DenseNet's fully connected graph, and the other using random graphs similar to competitive architectures. Evaluation is done on ILSVRC-2012 validation set and ImageNet V2 test set. The study evaluates the performance of a new algorithm on the ILSVRC-2012 validation set and ImageNet V2 test set. The algorithm consistently achieves high accuracy with fewer parameters compared to other models. The performance gap between the new method and DenseNet is 0.4% on ImageNet validation and up to 0.8% on ImageNet V2. The WS-G baseline is stronger than DenseNet but weaker than the proposed technique. The study compares the performance of the new algorithm with DenseNet and WS-G baseline, showing that the proposed technique outperforms WS-G but is slightly weaker than DenseNet. Scaling techniques for graphon are found to be effective, with consistent rankings after scaling up to 64 nodes. Standard deviations are low across most methods, even with model selection on ImageNet V2. The study introduces a new approach for neural architecture search using graphon theory and cut distance metric. Results show that searching for random graph distributions is effective for organizing layers within one stage. The proposed method aims for fair comparisons rather than showcasing the best performance, emphasizing the importance of search space design in NAS. The operationalization of the graphon theory in NAS solutions is explained, highlighting its superiority over random graph models like the Erd\u0151s-R\u00e9nyi model. A technique for scaling up random graphs and finding stage-wise graphs that outperform manually designed architectures is presented. Future work involves expanding to different operators in the same stage graph. The DenseNet network architecture for CIFAR-10 and ImageNet consists of a stem network, multiple stages with transition blocks, and a global average pooling layer. Each stage includes a cell structure with two convolutions of different kernel sizes, followed by batch normalization and ReLU. This work paves the way for advanced NAS algorithms using graphon theory and the cut distance metric. During architecture search, the DenseNet framework uses a growth rate of 32 and trains for 300 epochs with a learning rate schedule. For ImageNet training, the model is trained for 90 epochs with label smoothing and a batch size of 256. The DenseNet framework uses a growth rate of 26, 26, 26, 32 for 4 stages to match Densenet-121 parameters. Learning rate is set at 0.1, decreased at epochs 30, 60, and 80. Nesterov momentum of 0.9 and weight decay of 0.00005 are used. Data augmentation includes random cropping, resizing to 224 \u00d7 224, color jitter, horizontal flip, and random changes to brightness. The search algorithm samples subsets of nodes for input while avoiding cycles. The DenseNet framework uses a growth rate of 26, 26, 26, 32 for 4 stages to match Densenet-121 parameters. Learning rate is set at 0.1, decreased at epochs 30, 60, and 80. Nesterov momentum of 0.9 and weight decay of 0.00005 are used. Data augmentation includes random cropping, resizing to 224 \u00d7 224, color jitter, horizontal flip, and random changes to brightness. The model samples subsets of nodes for input while avoiding cycles, assigning weight parameters to each subset and employing neural network operations with different parameters for each input subset. The DenseNet framework utilizes a growth rate of 26, 26, 26, 32 for 4 stages to match Densenet-121 parameters. Learning rate is set at 0.1, decreased at epochs 30, 60, and 80. Nesterov momentum of 0.9 and weight decay of 0.00005 are used. Data augmentation includes random cropping, resizing to 224 \u00d7 224, color jitter, horizontal flip, and random changes to brightness. The model samples subsets of nodes for input while avoiding cycles, assigning weight parameters to each subset and employing neural network operations with different parameters for each input subset. The current stage involves extracting a feature map using dummy nodes and applying operations to input nodes within subsets, with aggregation and competition mechanisms in place. The cut distance \u03b4 is defined for graphs with the same set of nodes, allowing for fractional overlay matrices to minimize distance. Fractional upsampling generates a new graph with additional nodes, enhancing the graph structure. Fractional upsampling involves generating a new graph with additional nodes by shifting node weights and performing a partial blow-up operation. This process ensures equal weights for all nodes, necessary for sampling probabilities. The k-way split of nodes is defined, and the cut distance between the new and original graph is analyzed, showing an upper bound of (n\u2212m)m/n(n+m) \u03b2 \u2206. Theorem 3 states that for two weighted graphs with the same set of nodes but different nodeweights, the maximum difference between any two edge weights is denoted by \u03b2 \u2206. By creating an overlay matrix and partitioning the nodes accordingly, the proof shows that the upper bound for the cut distance between the new and original graph is (n\u2212m)m/n(n+m) \u03b2 \u2206. The k-fold blow-up method is a better approximation of the original graph in terms of cut distance than 1D linear interpolation. When using the fractional blow-up method for a graph of size n + m (0 < m < n), the cut distance does not change more than O(\u03b2 \u2206) when m is 1 or n - 1. When m is 1 or n - 1, the partial blowup operation causes \u03b4 to change by O(\u03b2 \u2206 /n). However, if m is n/2, the \u03b4 between the original graph and the new graph could increase by up to \u03b2 \u2206 /6. This implies that fractional upsampling results in a graph similar to the original when only a small number of nodes is added relative to n."
}