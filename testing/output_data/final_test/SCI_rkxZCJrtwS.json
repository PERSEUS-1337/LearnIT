{
    "title": "rkxZCJrtwS",
    "content": "Over the last decade, two competing control strategies have emerged for solving complex control tasks with high efficacy. Model-based control algorithms, such as model-predictive control (MPC) and trajectory optimization, peer into the gradients of underlying system dynamics to solve tasks efficiently. However, these methods are sensitive to initializations and can get stuck in local minima. Deep reinforcement learning (DRL) explores the solution space through sampling but is computationally expensive. A hybrid method combining gradient-based methods and DRL, based on the deep deterministic policy gradients (DDPG) algorithm, is proposed in this paper. Our algorithm utilizes true gradients from a differentiable physical simulator to enhance the convergence rate of the actor and critic in DRL. It is tested on various 2D robot control tasks, including a differentiable half cheetah with hard contact constraints, showing improved performance without compromising robustness to local minima. The surge in popularity of Deep Reinforcement Learning (DRL) is due to its ability to operate on black-box simulators without the need for underlying dynamics models. However, this approach can be inefficient and overlook valuable information from dynamical gradients when available. This can limit the effectiveness of DRL in control tasks, especially when accurate models of robot dynamics are known. Robot dynamics are typically addressed using model-based methods like model-predictive control or trajectory optimization for higher sample efficiency. While model-based methods offer analytical gradients without sample-based estimation, they lack exploration and are prone to local minima. Despite advancements in differentiable simulators, control applications still rely on established methods like MPC, gradient descent, or trajectory optimization. An ideal algorithm would combine the efficiency of model-based methods with the robustness of DRL to local minima. In this paper, an actor-critic algorithm leveraging differentiable simulation combines model-based methods and DRL. True model gradients are used to enhance critic models' efficacy, optimizing critic gradients for improved convergence of both actor and critic. The algorithm co-learns critic value and gradient estimation using advantage estimation (AE) gradients, benefiting from the emergence of differentiable simulators. The paper introduces an efficient hybrid actor-critic method that enhances convergence by utilizing gradient information. It also presents a mathematical framework for fitting critic gradients and demonstrates the algorithm on various control tasks, comparing it to model-based control and DRL baselines. Simulators are used for rigid body, soft body, and fluid dynamics simulations, allowing for optimization algorithms like gradient descent. Robotic control tasks can be treated as nonlinear optimization problems using trajectory optimization and model-predictive control techniques. However, these methods can get stuck in poor local minima, especially with complex dynamics or terrain. In contrast to model-based learning, this paper focuses on using a known simulator to efficiently explore optimal control parameters. Stochastic global optimization methods, like evolutionary algorithms, explore the solution space with many samples to find global optima, but can be slow. RL incorporates sampling and exploration techniques. RL incorporates sampling, exploration, and value function learning to avoid poor local minima. The algorithm uses gradient information efficiently. The paper considers rigid body simulation as a Markov Decision Process (MDP) with state space S, action space A, transition function \u0393, and reward function R. The optimal control policy maximizes the discounted sum of rewards over a finite time horizon. The policy function \u03c0 is represented by a neural network with parameters \u03b8, predicting the next action a t. The action-value function Q \u03c0 (s t , a t ) describes the expected return at step t. The critic network Q \u03c6 (s, a) approximates this function. The focus is on differentiable rigid body simulators with fully observable states, where the state s t fully characterizes the robot's status at each time step. The hybrid algorithm in this paper improves actor policies using critic gradients, inspired by modern actor-critic algorithms like DDPG. By ensuring accurate critic gradients, the actor learning process is enhanced. The paper introduces a hybrid algorithm that enhances actor policies using critic gradients, inspired by DDPG. It includes a regularization term to prioritize accurate gradients and uses a differentiable simulation engine for backpropagation through state-action pairs. The algorithm maintains a replay buffer and records dynamical Jacobians without increasing time or space complexity. The algorithm efficiently improves actor and critic networks by re-using computed gradients during rollouts. Additional Jacobian information is used to compute gradients of the target network Q i, which serves as a regularizer in the critic loss. This computation requires a differentiable simulator and has constant time complexity. The algorithm improves actor and critic networks by re-using gradients during rollouts. Extra storage is needed for Jacobian matrices in the replay buffer, but time complexity is efficient. Simulate the agent using \u03c0(s) + N at each time step and update critic and actor parameters accordingly. Target networks are also updated. The algorithm aims to move a kinematic mass point to the origin in minimal steps by computing new states based on actions. Different sets of parameters are compared to see how they affect the training of the critic network. The critic network in the algorithm updates to fit the Q surface with sampled points from the replay buffer, while the hybrid method adds regularizers to fit tangent directions of the Q surface at sample points, boosting convergence to the ground truth. The hybrid algorithm uses regularizers to fit tangent directions of the Q surface at sample points, improving convergence to the ground truth. It is tested on robot control tasks in contact-free simulation settings. Simulation with impulse-based collision responses involves applying impulses to rigid bodies during collisions to change their velocity while conserving momentum. For systems with contacts, the Poisson collision model is used. On the other hand, simulation with constraint-based collision responses formulates contacts as constrained systems, treating them as linear complementary problems. In contrast to impulse-based collision responses, constraint-based collision responses model contacts as linear complementary problems (LCP). A 2D LCP-based rigid body simulator with hybrid control algorithm is implemented using DDPG in OpenAI Baselines. Parameter space noise is applied for exploration, and the critic network normalizes state and action inputs to achieve task-irrelevant statistical stability. Actions are scaled to fall within the range [-1, 1], and gradients are computed with normalized Q values. The curr_chunk discusses the implementation of seven 2D control tasks in a differentiable rigid body simulator, including classic control problems like CartPole and Pendulum, as well as tasks with impulse-based collision responses and LCP-based contact models. The tasks are implemented with manually computed gradient information in the simulation, and a RollingDie task is designed to demonstrate collision applicability. The curr_chunk discusses a RollingDie task involving impulse-based collision responses. The die's initial state includes position, rotation angle, velocity, and angular velocity. The action is controlling torque to impact the die's pose. The reward is based on distance from the target position. Sparse moments of contact can chaotically affect the trajectory. The die can bounce multiple times, increasing control complexity. The study focuses on optimizing control in a HalfCheetah example with LCP-based contacts. A hard contact model is used, increasing complexity and making control more challenging. The method is compared with DDPG, MPC with iLQR, and gradient descent. The study compares different optimization algorithms for control in a HalfCheetah example with LCP-based contacts. The algorithms are tested with different initializations and metrics are reported for each method. Our algorithm improves the sampling efficiency of DDPG and often discovers strategies with higher return compared to MPC and GD. It suffers less from local minima, especially for more complex tasks like HalfCheetah. The experiment is terminated based on return mean plateaus in GD, convergence in MPC, or reaching a pre-defined maximum number of timesteps in DDPG and our algorithm. Unlike other methods, MPC performs one simulation optimized online. Our algorithm improves sampling efficiency and discovers high-return strategies compared to MPC and GD. It outperforms MPC on complex tasks like HalfCheetah. DDPG and our algorithm are competitive on all presented problems, with MPC dominating on simpler tasks but struggling on more complex ones with hard contacts. DRL approaches outperform model-based approaches like GD on complex control tasks with hard contacts, showing the importance of exploration properties. Model-based approaches may succeed on simpler tasks like CartPole but struggle in dynamical environments requiring locally suboptimal motions to escape local minima. Tasks like Pendulum, CartPoleSwingUp, and MountainCar highlight the need for building momentum through local oscillations. GD fails on tasks like HalfCheetah due to its inability to handle certain configurations like toppling. DRL-based control approaches outperform model-based approaches on complex tasks with hard contacts. MPC requires long planning horizons for contact-heavy dynamics. The hybrid algorithm converges faster or to higher returns compared to DDPG. The rolling die example highlights the importance of aiming for higher return history due to initial state distribution variance. Our method outperformed MPC in terms of average return history over 16 runs by discovering a novel two-bounce strategy for challenging initial poses. Additionally, our algorithm is applicable to a wider range of reward structures compared to MPC, making it superior in certain scenarios. Our Hybrid DDPG algorithm is more robust than MPC, as closed-loop network controllers can generalize to changes in noise, initial conditions, and tasks without expensive replanning. This makes it more attractive for real-time applications on physical hardware. The algorithm uses AE gradients to co-learn critic value and gradient estimation, improving convergence of both actor and critic. Our algorithm leverages differentiable simulation to improve convergence of actor and critic in DRL. It outperforms model-based approaches in 2D control tasks and is less sensitive to local minima. Future work could explore applying the mathematical framework to enhance value functions in other DRL algorithms. Our hybrid algorithm uses \u03c6 and \u2207Q instead of L2 distance in Algorithm 1, achieving similar or better performance than pure DDPG in differentiable environments. We adjusted parameters in each environment and found the cosine angle regularizer to be more effective in some cases, while the L2 regularizer outperformed in others. Our method was also implemented with the original L2-norm loss in Soft Actor-Critic (SAC). Our method, implemented with the original L2-norm loss in Soft Actor-Critic (SAC), improves performance in three out of five examples compared to the original SAC. The results show that our modification can be applied to other actor-critic methods besides DDPG, with our hybrid algorithm performing similarly or better than pure SAC in all examples."
}