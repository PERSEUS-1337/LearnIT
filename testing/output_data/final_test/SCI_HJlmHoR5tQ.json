{
    "title": "HJlmHoR5tQ",
    "content": "Our proposed method uses generative adversarial networks to learn rewards and policies from expert examples under unknown dynamics. Empowerment-based regularization prevents overfitting to expert demonstrations, leading to more generalized behaviors and near-optimal rewards. The approach simultaneously learns empowerment, reward, and policy through variational information maximization. Evaluation on complex control tasks and transfer learning problems demonstrates the effectiveness of our method. The proposed method uses generative adversarial networks to learn rewards and policies from expert examples under unknown dynamics. It outperforms state-of-the-art inverse reinforcement learning algorithms by learning near-optimal rewards and policies matching expert behavior. Reinforcement learning is effective for solving complex decision-making tasks, but defining optimizable reward functions can be challenging for robotic applications like social-interaction skills, dexterous manipulation, and autonomous driving. Inverse reinforcement learning addresses this by learning reward functions from expert demonstrations, often considered a branch of imitation learning. The current work focuses on maximum-margin and maximum-entropy formulations in Inverse Reinforcement Learning (IRL). Maximum entropy IRL, utilizing non-linear function approximators like neural networks, is widely used but complex to design due to the need for hand engineering. In contrast, imitation learning (IL) directly learns policies from expert demonstrations, with behavior cloning (BC) being a simple solution that requires a large amount of data. To overcome BC limitations, a generative approach is proposed using adversarial networks to learn rewards and policies from expert examples under unknown dynamics. The current work introduces a generative adversarial imitation learning (GAIL) algorithm to address limitations of behavior cloning (BC) in overcoming compounding errors induced by covariate shift. GAIL utilizes a generator-discriminator framework similar to Generative Adversarial Networks (GANs) to generate expert-like trajectories. However, GAIL does not recover transferable reward functions, limiting its use cases to similar problem instances in similar environments. Reward function learning is preferred over direct imitation learning as rewards are portable functions that represent agent intention and can be re-optimized in new environments and agents. Reward learning is challenging due to multiple optimal policies and reward functions. Adversarial Inverse Reinforcement Learning (AIRL) addresses these issues by using the maximum entropy IRL method and learning disentangled reward functions. However, AIRL struggles to recover the ground truth reward when it depends on both state and action, limiting its applicability in tasks like locomotion. The paper proposes the EAIRL algorithm, which uses empowerment as a regularizer to prevent overfitting expert demonstrations and learn robust policies. Empowerment is a measure that quantifies an agent's influence on its environment, learned alongside the reward and policy from expert data using variational information maximization. The proposed EAIRL algorithm uses empowerment as a regularizer to prevent overfitting expert demonstrations and learn robust policies. It recovers near-optimal policies and transferable, disentangled reward functions. EAIRL outperforms state-of-the-art IRL methods in recovering reward functions for optimal behaviors. Policies learned through EAIRL perform comparably to GAIL and AIRL, outperforming AIRL with disentangled rewards. The proposed EAIRL algorithm utilizes empowerment as a regularizer to prevent overfitting expert demonstrations and learn robust policies. It outperforms state-of-the-art IRL methods in recovering reward functions for optimal behaviors. The algorithm generates near-optimal policies and transferable, disentangled reward functions, performing comparably to GAIL and AIRL, and outperforming AIRL with disentangled rewards. The MaxEnt-IRL framework uses an empowerment-based potential function to regularize policy updates. It models expert demonstrations as a Boltzmann distribution with a parametrized reward function. The main challenge is determining the partition function Z, which was initially computed using dynamic programming. MaxEnt-IRL aims to determine Z, initially computed using dynamic programming. Modern approaches use importance sampling to approximate Z under unknown dynamics. Adversarial Inverse Reinforcement Learning (AIRL) algorithm, a baseline method, builds on GAIL, maximum entropy IRL framework, and GAN-GCL. GAIL is a model-free adversarial learning framework where the policy learns to imitate the expert policy behavior by minimizing the JensenShannon divergence through a discriminator. AIRL extends GAIL to recover reward by imposing a structure on the discriminator, training it as a binary classifier to distinguish between expert and policy generated demonstrations. The policy is then trained to maximize the discriminative reward with free parameters in the shaping term. The reward function in AIRL has free parameters and the impact of the shaping term on the recovered reward is limited. Mutual information measures the dependency between random variables in reinforcement learning. Empowerment quantifies the internal reward based on the mutual information between actions and states. Empowerment quantifies an agent's ability to influence its future states. A scalable method has been proposed to learn empowerment by maximizing a variational lower bound, equivalent to maximizing mutual information. This method overcomes computational limitations in higher-dimensional problems. The method proposed to learn empowerment involves maximizing a variational lower bound, equivalent to maximizing mutual information. This is achieved by applying the principles of Expectation-Maximization to learn empowerment. Empowered Adversarial Inverse Reinforcement Learning algorithm is introduced to learn a robust reward function and policy from expert demonstrations. The algorithm optimizes parameters of policy \u03c0 and scalar function \u03a6 by minimizing the discrepancy between two approximations through absolute or squared error. The proposed method involves an inverse model, a reward function, an empowerment-based potential function, and a policy model trained simultaneously to recover optimal policies and generalizable reward functions. The inverse model is optimized through maximum log-likelihood supervised learning. The inverse model q \u03c6 (a|s, s ) is trained through maximum log-likelihood supervised learning using trajectories from the policy. Empowerment is estimated by minimizing the loss function w.r.t parameters \u03d5, and the reward function is learned by computing the discriminator with parameters \u03be. The reward function r \u03be (s, a) is learned with parameters \u03be, while maintaining target \u03d5 and learning \u03d5 parameters for the empowerment-based potential function. The discriminator/reward function parameters \u03be are trained via binary logistic regression to discriminate between expert and generated trajectories. The policy \u03c0 \u03b8 (a|s) is trained to maximize the discriminative reward and minimize the loss function, accounting for empowerment regularization. The overall policy training objective involves updating policy parameters \u03b8 using methods like TRPO or PPO. The training procedure involves training all function approximators simultaneously, with expert samples seen only by the discriminator. The discriminating reward function is expressed as a combination of different terms. The policy training objective can also be expressed as maximizing the product of the policy and reward functions. The policy parameters are updated using a natural gradient update rule like TRPO or PPO. After every n epochs sync \u03d5 with \u03d5, the policy update rule maximizes the discriminative reward and minimizes the loss l I. The proposed method, EAIRL, learns both reward and policy from expert demonstrations, inducing generalized behavior rather than local overfitting. The policy is regularized by the empowerment, maximizing both the discriminative reward and empowerment. Our method, EAIRL, learns reward and policy from expert demonstrations, outperforming state-of-the-art techniques like GAIL and GAN-GCL in control tasks. Comparisons show superior performance and expert-like behavior in transfer learning problems. Our method, EAIRL, outperforms GAIL and GAN-GCL in control tasks by learning near-optimal, transferable reward functions. Performance is evaluated based on mean and standard deviation of total rewards accumulated by an agent in five randomly-seeded trials. Transfer learning is tested by re-optimizing a new policy in different environments using rewards learned via IRL. During testing, a crippled ant with disabled front legs is reoptimized to move forward using a learned reward function. The agent adapts its gait to run forward when unable to move sideways. In another test case, an agent navigates a 2D point-mass through a maze with a changing central wall, requiring a different path to reach the goal. Results show near-optimal performance in transferring reward functions to unseen testing environments. In various control tasks, our method performs significantly better than others, successfully imitating expert policies in tasks such as making robots run, swim, and stand upright. EAIRL, AIRL(s, a), and GAIL show similar performance, while AIRL(s) and GAN-GCL fail to recover a policy. The importance of empowerment-regularized MaxEnt-IRL and modeling rewards as a function of both state and action is highlighted in the scalable framework BID4 BID6. Regularizing the policy update with empowerment helps reduce divergence from expert data distribution and maximize empowerment. The proposed regularization in the BID4 BID6 framework prevents premature convergence to local behavior, ensuring robust state-action based rewards learning. Empowerment quantifies the agent's control over the environment in a given state, guiding its actions for maximum influence. Experimentation emphasizes the importance of modeling reward functions as a function of both state and action in GANs framework for effective policy learning. State-only rewards do not capture action-dependent terms, leading to aggressive behavior and instability in the agent's actions. The importance of modeling reward functions as a function of both state and action in real-world applications like autonomous driving, robot locomotion, and manipulation tasks is crucial for effective policy learning. State-action reward formulation is necessary to learn action-dependent terms and recover expert-like policies, as shown in empirical results. Adversarial imitation learning fails without modeling reward/discriminator as a function of state-action. Our approach focuses on adversarial reward and policy learning from expert demonstrations by regularizing maximum-entropy inverse reinforcement learning through empowerment. The method learns empowerment through variational information maximization while training the reward and policy to imitate expert behavior and maximize agent empowerment. This regularization prevents premature convergence to local behavior, leading to a generalized policy that guides the reward-learning process to recover near-optimal rewards. Our method outperforms state-of-the-art IRL methods in imitation learning and transfer learning tasks, demonstrating successful learning of near-optimal rewards and policies. In future work, the method aims to extend learning rewards and policies from diverse human demonstrations, including sub-optimal behaviors. The derivation of presenting mutual information as a variational lower bound is discussed, emphasizing the empowerment as a maximal of mutual information. The empowerment is derived as a maximal of mutual information through a variational lower bound formulation. The optimization involves maximizing inverse model q(a|s , s) and action model w(a|s) under constraints. The optimal solution for w(a|s) is shown to be 1/Z(s) exp(u(s, a)), maximizing the lower bound. In this section, the Empowerment-regularized formulation of maximum entropy IRL is derived. The objective is to maximize the likelihood of expert demonstrations by learning a parametrized reward. An importance sampling distribution is trained to reduce variance, and the gradient is computed with respect to the reward parameters. The importance-sampler/policy is trained to maximize the likelihood further. The above gradient is used to train the importance-sampler/policy to maximize empowerment for generalization and reduce divergence from the true distribution. The discriminator is trained to minimize cross entropy loss, while the policy is trained to maximize the discriminative reward. The policy is trained to maximize the discriminative rewardr(s, a, s ) = log(D(s, a, s ) \u2212 log(1 \u2212 D(s, a, s ))) and minimize the information-theoretic loss function l I (s, a, s ). The entropy-regularization is scaled by the hyperparameter \u03bb h \u2208 R, and the policy is trained to maximize a specific equation involving rewards and probabilities. The policy is trained to maximize a specific equation involving rewards and probabilities, where the reward function is defined as r(s, a, s) = r(s, a) + \u03bbI\u03a6(s) + \u03bb\u0124(\u00b7). The figures illustrate the difference in movement between standard and crippled ants, as well as the path profiles of a 2D point-mass agent in training and testing environments. The potential function h \u03d5 (\u00b7) and \u03a6 \u03d5 (\u00b7), reward function r \u03be (\u00b7), discriminators of GAIL and GAN-GCL, policy \u03c0 \u03b8 (\u00b7), and inverse model q \u03c6 (\u00b7) are all represented by two-layer RELU networks with 32 units in each layer. The experiments used a Gaussian policy with a temperature term \u03b2 = 1. Both mean-squared and absolute error forms of l I (s, a, s ) showed similar performance in reward and policy learning. Entropy regularization weights were set to 0.1 and 0.001 for reward and policy learning, respectively, with hyperparameters \u03bb I set to 1.0 for reward learning and 0.001 for policy learning. During policy learning, the target parameters of the empowerment-based potential function were updated every 5 epochs. The batch size was set to 2000 steps for the pendulum environment and 20000 steps for other environments. Policy samples from the previous 20 iterations were used as negative data to prevent overfitting of the reward functions."
}