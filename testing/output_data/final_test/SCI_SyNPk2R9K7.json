{
    "title": "SyNPk2R9K7",
    "content": "In this paper, the authors introduce scene programs as a way to represent scenes using symbolic programs for objects, attributes, and relations. Their model can infer scene programs from hierarchical, object-based scene representations and has shown success on synthetic data and real images with compositional structures. Scene programs enable applications like visual analogy-making and scene extrapolation. Recent AI systems have made progress in detecting, segmenting, and recognizing individual objects in scenes. However, understanding high-level abstract relations among objects is less studied. While some papers have attempted holistic scene representation, capturing relationships among objects is still a challenge. Jointly discovering objects and their relations has only been explored recently. In recent AI research, progress has been made in detecting and recognizing individual objects in scenes. However, understanding high-level abstract relations among objects is still a challenge. A new approach aims to represent scenes as scene programs, capturing objects with their attributes and structural relationships. This involves inferring scene programs from complex scene images through a hierarchical bottom-up approach. Our model utilizes a hierarchical bottom-up approach to parse images into objects, group them, and generate programs describing the scene with high accuracy. It can imagine unseen objects based on structural relations and outperforms baseline methods on complex scenes. Our model uses a hierarchical approach to infer scene programs from images, handling ambiguity and allowing for high-level image editing efficiently. It generalizes to real-world images without additional supervised training and facilitates extrapolations and edits on both synthetic and photographic images. Our model infers scene programs from images hierarchically, achieving high accuracy in describing scenes. It generates realistic images capturing scene structure and object appearance. Compared to other methods like BID6 and IM2LATEX BID4, our model discovers high-level programs from images efficiently. Our model uses reinforcement learning to infer low-level drawing commands for image reproduction and learns a distribution for image sampling. Unlike prior work, our model performs program induction in 3D, inferring high-level structural patterns in object layout and color. It utilizes vision models to extract object attributes and predict object groups, then sends these representations to a sequence model to predict the program. Our program representation, similar to symmetry hierarchy, generalizes to repetitive patterns and object visual attributes. Different models like CSGNet and BID25 parse shapes using primitive commands. This paper focuses on learning high-level scene regularities through loop structures. Program synthesis models like R3NN and RobustFill perform end-to-end synthesis from examples. The BID3 model improves program synthesis by leveraging grammar and reinforcement learning, synthesizing programs based on input/output pairs. In contrast, our model focuses on describing complex correlations among objects in static scenes by combining vision and sequence models. An object parser predicts segmentation masks and attributes, a group recognizer assigns objects to groups, and a program synthesizer generates program blocks for each object group. Our model focuses on synthesizing programs from input images by predicting object groups using a Domain Specific Language (DSL) with defined primitive commands and loop structures. The DSL allows for unbounded program depth and defines program blocks to further reduce complexity. The DSL allows unbounded program depth and defines program blocks to reduce complexity. Program blocks include single objects, layered for loops of depth \u2264 3, and single-layer rotations of \u2264 4 objects. Object attributes are used as an intermediate representation between image and program space. Parsing objects involves mask and attribute prediction using Mask R-CNN and ResNet-34 to predict shape, size, material, color, and 3D coordinates. Each attribute is encoded as a one-hot vector. The overall representation of an object includes shape, size, material, color, and 3D coordinates encoded as a vector of length 18. The networks are trained with ground truth masks and attributes. A group recognizer identifies objects forming a group described by a single program block after mask prediction. The input to the model consists of the original image, mask of the input object, and mask of all objects. The input to the model consists of the original image, the mask of the input object, and the mask of all objects. These are concatenated and sent to a ResNet-152 followed by fully connected layers. The output includes a binary vector for object grouping and the category of the group. The final step involves generating program sequences for the input image using a sequence to sequence LSTM with an encoder-decoder structure and attention mechanism. The output program is predicted using a sequence-to-sequence LSTM with an attention mechanism. Program tokens and parameters are predicted at each time step, with the program synthesizer working for a single group. The group prediction is combined with program synthesis to generate the final program sequences. In practice, synthesis involves randomly choosing an object and describing its group using Algorithm 1. The process includes sampling 10 times to generate a correct program that recovers scene attributes. Experiments on synthetic scene images show quantitative comparisons with baseline methods and extensions. The model can generalize to real images with minimal hand-labeled supervision at the object level. Additionally, the method is applied to tasks like image extrapolation and visual analogy-making on both synthetic and real images, using a synthetic dataset with complex scene structures. Examples from the dataset are shown in FIG4. The dataset includes scenes with groups of objects described by program blocks, sampled from predefined primitives with symmetries and color patterns. Two synthetic datasets, REGULAR and RANDOM, are used for training and testing models, with varying scene complexities. In the RANDOM dataset, objects are randomly placed, increasing scene randomness. The evaluation results compare different models on a synthetic dataset with randomly placed objects. The models include an ablated version of the full model, two baselines, and a model using a CNN as encoder and LSTM with attention as decoder. The evaluation results compare different models on a synthetic dataset with randomly placed objects. The models include an ablated version of the full model, two baselines, and a model using a CNN as encoder and LSTM with attention as decoder. The architecture is similar to attention-based neural image captioning BID29, with the decoder predicting a token and a parameter matrix at each time step. Evaluation is done on both REGULAR and RANDOM test sets, as well as an additional test set of 100 complex images with multiple objects. Results show that the model performs well on the REGULAR test set, accurately recognizing groups and patterns in images with two or three groups. Quantitative evaluation includes program token accuracy, parameter loss, and reconstruction accuracy to assess the model's performance. Our model outperforms baseline methods in reconstructing images, achieving good performance on generalization. The deep grouping model performs better than the simple heuristic grouping method by learning from the data distribution in our program space. It excels in detecting groups among randomly placed objects, generating multiple possible programs for ambiguous input. Our hierarchical method excels in generating multiple possible programs for ambiguous input, allowing explicit specification of group category. Instead of selecting the most confident group category, we search the top 3 proposals and execute the synthesized program block to determine possible correct programs. Our model can also handle scenes with invisible objects by generating object instance masks and removing those below a certain threshold. The neural program synthesizer can generate program blocks based on partial observations of scenes, including occluded objects. It can recognize the same program pattern with different observations, as shown in examples in FIG6. The model's flexibility allows it to handle varying distances and correctly identify objects in different scenarios. Our model's program representation enables high-level structural editing of images, allowing for efficient editing through program manipulation. Various editing tasks such as spatial extrapolation, color pattern changes, and shape modifications can be achieved with just one edit in the program. This approach contrasts with traditional object representation, which requires multiple edits per image. Our method uses object attributes to connect vision and program synthesis, allowing for generalization to real images. The neural program synthesizer is independent from visual recognition, requiring only the retraining of vision systems for the entire model to work on real images. We pretrain on a synthetic dataset before fine-tuning on a small amount of real data, enabling our model to generalize well and predict accurately. Our model generalizes well with fine-tuning on real data and accurately predicts programs for test images. Image editing techniques can be applied to real images, as shown in an experiment on real image extrapolation. The program is generated from an input image, and object patches are extracted using Mask R-CNN. The program is extended by increasing the iteration number to predict future observations. A neural renderer, pix2pix BID13, is used to render new images from edited programs for real images. Newly added object masks are retrieved after executing the edited program. The edited images generated using pix2pix preserve object appearances and fix errors made by mask prediction. The average L2 distance between ground truth images and model outputs is shown in Table 3. Visual analogy making is explored where an input image is converted to a new image given other reference images. In a visual analogy making task, a program synthesis model is used as an encoder and pix2pix as a neural decoder to generate output images based on reference image pairs. The program representation involves arithmetic operations on a matrix format. Comparison with an autoencoder is shown through qualitative results in FIG9. Our model generates perceptually plausible results in the visual analogy making task, outperforming the autoencoder in preserving layout arrangements. The average L2 distance between model output and ground truth images is closer to the baseline. Scene programs serve as a structured representation of complex scenes with high-level regularities, allowing for accurate inference from 2D images in a hierarchical bottom-up manner. The model demonstrates high accuracy on synthetic datasets and generalizes well to real images, enabling applications in image editing and analogy making in computer vision tasks. The detailed method for heuristic grouping (HG) is presented in Algorithm 2, where objects in a scene are manipulated with random shifts and noise. Scene programs are represented as matrices with program commands and parameters, facilitating accurate inference from 2D images. Each program block is a matrix of size N \u00d7 14, allowing for structured representation of complex scenes. In our work, a program block is represented as a matrix of size N \u00d7 14 where N is the number of program commands. The 14 numbers are divided into four parts: program token, iteration arguments, position arguments, and color arguments. The matrix representation allows arithmetic operations in program space for scene programs in image analogy. Evaluation of output program differs in REGULAR and RANDOM settings, with rounding for integers in REGULAR and allowing small errors for continuous position arguments in RANDOM."
}