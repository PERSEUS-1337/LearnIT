{
    "title": "S1e4Q6EtDH",
    "content": "The embedding layers in deep neural networks for natural language processing can be large and impractical for limited resource settings. A novel method using Tensor Train (TT) decomposition compresses these layers with minimal impact on performance. This approach was tested on various NLP benchmarks, showing a trade-off between performance and compression ratios across different architectures like MLPs, LSTMs, and Transformers. The development of lightweight architectures in NLP research is crucial due to the cumbersome nature of large neural networks. One approach to compressing models involves implying specific structures on weight matrices, such as low-rank tensor networks. However, these methods often require additional fine-tuning to recover performance. A new parameter-efficient embedding layer called TT-embedding is introduced in this paper, which can be integrated into any model for training. The TT-embedding layer is a compressed and efficient embedding layer that can be easily integrated into any model for training. It stores smaller tensors instead of a large embedding matrix, allowing for significant model compression with minimal performance impact. The number of parameters remains relatively small and constant during training, enabling the use of larger batches or efficient training with limited resources. Experimental results show that TT-embeddings can replace standard embeddings with a compression ratio of 1-3 orders without a significant drop in performance, and sometimes even a slight improvement. The TT-embedding layer offers significant model compression without performance loss, with compression ratios of 441 on IMDB dataset and 15 on WMT 2014 En-De dataset. It also improves accuracy in binary classification tasks, such as CTR prediction in digital advertising. In recent years, research has focused on compressing and speeding up neural networks for NLP tasks. Various methods like product quantization, pruning algorithms, variational dropout, and compact encoding schemes have been proposed to reduce parameters in models. GroupReduce and WEST are efficient compression methods for embedding and softmax layers. Novikov et al. (2015) introduced reshaping weights into high-dimensional tensors using Tensor Train (TT) format for neural network compression. Lebedev et al. (2015) demonstrated compressing convolutional layers with canonical (CP) tensor decomposition. Novikov et al. (2015) introduced Tensor Train (TT) format for neural network compression, while Lebedev et al. (2015) demonstrated compressing convolutional layers with canonical tensor decomposition. Recently, Ma et al. (2019) applied Block-Term Tensor Decomposition to compress self-attention modules in the Transformer architecture. This work focuses on applying tensor machinery to compress embedding layers in NLP. In NLP models, compressing embedding layers can greatly reduce model size. Using low-rank matrix factorization, the embedding matrix can be represented as a product of two thinner matrices. This approach aims to efficiently map words into vectors by seeking the embedding matrix in a low-rank format from the start. To efficiently compress embedding layers in NLP models, low-rank matrix factorization can be used to represent the embedding matrix as a product of two thinner matrices. However, achieving significant compression ratio is challenging when the embedding dimension is much smaller than the vocabulary size. To overcome this limitation, reshaping matrices into multidimensional tensors and applying Tensor Train decomposition allows for a more compact representation with fewer parameters. The Tensor Train (TT) format enables a more compact representation of tensors, reducing the number of parameters required compared to storing the full tensor. This makes TT-decomposition appealing for problems with large tensors, offering significant parameter savings. The TT format allows for a more compact representation of tensors by reshaping and applying TT-decomposition, resulting in a TT-matrix with TT-shapes. The TT format enables a more concise representation of tensors through TT-decomposition, creating a TT-matrix with TT-shapes. TT-embedding involves trainable parameters (TT-cores) in a TT-matrix E of the underlying tensor shape, with specified TT-ranks as hyperparameters defining the compression ratio. The TT format allows for a more compact representation of tensors using TT-decomposition, creating a TT-matrix with TT-shapes. TT-embedding involves trainable parameters (TT-cores) in a TT-matrix E of the underlying tensor shape, with specified TT-ranks as hyperparameters defining the compression ratio. To compute the embedding for a word indexed i in the vocabulary, the row index i is mapped into an N-dimensional vector index (i1, ..., iN) and the components of the embedding are calculated using matrix multiplications efficiently executed in linear algebra packages like BLAS. The procedure for computing the mapping i \u2192 (i1, ..., iN) is provided in Appendix A. To construct a TT-embedding layer for a vocabulary of size I and embedding dimension J, and train a model with such a layer, factorizations of I and J into factors and a set of TT-ranks {R1, R2, ..., RN-1} need to be specified. The TT format enables compact tensor representation through TT-decomposition, creating a TT-matrix with specified TT-ranks. During training, embeddings are computed for a batch of indices using specified parameters. The order of tokens in the vocabulary is structured by TT-embedding, and the optimal order remains a future research problem. In our current experiments, we explored Tensor Ring decomposition as a more general form of TT-decomposition, which is circular permutation invariant. This could potentially improve TT-based models for NMT. Initialization of the embedding matrix E is typically done using Glorot initializer. For TT-embedding, initializing the TT-cores is more complex, but initializing each TT-core element as a normal distribution can be effective. In our experiments, we found that initializing TT-cores as a modified Glorot initializer greatly improved performance compared to a standard normal distribution. The distribution of the matrix elements approaches Gaussian with increasing TT-rank. Initializing TT-embedding layer with TT-SVD algorithm did not show better performance in practice. Hyperparameter selection is crucial for our embedding layer. The embedding layer introduces TT-shapes and TT-ranks as hyperparameters. TT-embedding does not require exact factorization of the vocabulary size. Factors should be close to each other for high compression ratio. Automated procedure for selecting values during initialization. TT-ranks directly impact compression ratio. In experiments, TT-ranks of 16 were set for small vocabularies and 64-192 for larger ones, achieving significant compression of the embedding layer with a slight impact on metrics. TT-embeddings were implemented in Python using PyTorch and tested on NLP tasks like sentiment analysis and Neural Machine Translation. The code is available at https://github.com/ttembedding/tt-embeddings. In practical applications, TT-embeddings were tested on machine translation, language modeling, and click-through rate prediction tasks. The approach was applied to various architectures like MLPs, LSTMs, and Transformers, showing wide applicability. Transformers in LM and NMT use the same weight matrix for embedding and softmax layers, reducing model size. Untying weights and tensorizing the embedding layer increases parameters without compression. In experiments, TT-decompositions were used for embedding and softmax layers, showing compression ratios as 2\u00d7TT-params. IMDB dataset with two categories and SST with five categories were utilized. Models with compressed embedding layers performed as well as or better than full models, indicating individual word embeddings may not be necessary for LSTM's expressive power. The study explored using TT-decompositions for embedding and softmax layers, achieving compression ratios of 2\u00d7TT-params. Results on IMDB and SST datasets showed that models with compressed embeddings performed as well as or better than full models, suggesting that individual word embeddings may not be essential for LSTM's expressive power. The study also hinted at the potential benefits of imposing specific low-rank structures on embedding matrices as a form of regularization for improving model generalization. The study explored using TT-decompositions for embedding and softmax layers, achieving compression ratios of 2\u00d7TT-params. Results on IMDB and SST datasets showed that models with compressed embeddings performed as well as or better than full models. In a challenging task, both embedding and softmax layers can be significantly compressed with a slight drop in BLEU score. NMT benefits more from additional capacity rather than regularization, as observed in sentiment analysis. TT-embeddings induce a 8% training iteration time overhead compared to the baseline Transformerbig. The study explored using TT-decompositions for embedding and softmax layers, achieving compression ratios of 2\u00d7TT-params. Results on IMDB and SST datasets showed that models with compressed embeddings performed as well as or better than full models. In a challenging task, both embedding and softmax layers can be significantly compressed with a slight drop in BLEU score. Compared to sentiment analysis and NMT, achieving high compression ratios for embedding and softmax layers in LM was not as successful. However, even moderate 3.8 times compression allowed for saving 100M of weights at the cost of \u223c 1.5 perplexity drop. Among other applications, the TT-embedding layer was used for CTR prediction in digital advertising. The dataset used had 39 categorical features and 45.8M samples, with binary labels indicating if a user clicked on an ad. To reduce memory usage, categorical features were mapped to integers and hashed if the vocabulary size was large. However, TT-embeddings allowed for strong compression properties, eliminating the need for hashing. Both full and hashed datasets were considered in experiments. CTR prediction in digital advertising was done using the TT-embedding layer with 39 categorical features and 45.8M samples. The neural network architecture included embedding layers with embedding size J, fully-connected layers with 1024 neurons, and ReLU activation functions. The Adam optimizer with a learning rate of 0.0005 was used. To handle large unique values in input features, a hashing procedure was employed. TT-embeddings were used to substitute the embedding layers while keeping the neural network structure unchanged. The TT-embedding layer was used in CTR prediction tasks with significant compression ratios and a slight improvement in test loss compared to baseline models. The total size of the compressed model was significantly smaller, suggesting the benefits of using TT-embedding layers in encoding categorical features. The proposed approach based on TT-decomposition effectively reduces training parameters while maintaining performance in natural language processing tasks. It can be easily integrated into deep learning frameworks, offering reduced memory requirements and increased batch size. TTembeddings impose a low-rank structure on the embedding matrix, improving generalization ability as a regularizer. Introducing non-linearity into TT-cores can enhance their expressive power. The optimal order of tokens in TT-embedding can boost performance and compression ratio. Higher-order tensor decompositions reduce parameters in neural nets, complementing traditional methods like pruning and quantization. Comparing these methods and exploring their combination for stronger compression is of interest. Tensor Ring (TR) decomposition is a generalization of TT-decomposition, where the first and last cores are 3-dimensional tensors. TR format represents a tensor X, and TR-matrix and TR-embedding layer can be defined similarly to TT. Reshaping TR decomposition involves constructing the TR-matrix from the standard embedding matrix. Performance of different methods is shown in Table 5. TR models have smaller ranks compared to TT models, impacting performance negatively and making them more computationally heavy when replacing embedding and softmax layers in NMT models."
}