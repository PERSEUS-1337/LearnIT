{
    "title": "r1fiFs09YX",
    "content": "In multi-agent scenarios, understanding other agents and making optimal decisions is crucial for high rewards. Building models for other agents and finding optimal policies can be sample-inefficient and prone to overfitting. Treating each agent as a sample from a population allows for efficient adaptation using meta-learning methods. Experiments on grid games demonstrate the effectiveness of this approach in quickly achieving high rewards in multi-agent environments. Our method quickly achieves high rewards in multi-agent scenarios by considering the influence of other agents. In the ultimatum bargaining game, players make decisions based on fairness rather than pure rationality, requiring the first player to predict the second player's reaction to propose an acceptable deal. In the ultimatum bargaining game, players need to predict the second player's reaction to make an acceptable proposal. To exploit opponents and find optimal policies, building a model for the opponent from observations is crucial. This model helps in predicting the opponent's actions and turning the task into a decision-making problem that can be solved using various RL methods. Accurate opponent modeling is essential for making informed decisions. In the ultimatum bargaining game, accurate opponent modeling is crucial for making informed decisions. Previous works propose methods to model the opponent, but it may require many observations and iterations. However, even with a precise opponent model, there is no guarantee for performance against other opponents with different types or policies. Learning a policy from scratch may be necessary when facing new opponents with unknown private types. In the ultimatum bargaining game, accurate opponent modeling is essential. Different ethnicities may have varying fairness standards, affecting opponent types. Employing meta-learning like MAML BID1 can help in policy improvement and opponent modeling separately. This approach aims to train a policy efficiently on various tasks to adapt quickly to new tasks with minimal training data. Our approach utilizes Model-Agnostic Meta-Learning (MAML) to train meta-learners for opponent modeling and policy learning separately, then jointly. The meta-learners are used to initialize the model and policy for new opponents, enabling quick adaptation with minimal interactions. The task is formalized based on stochastic games and Bayesian games, providing a general framework for multi-agent problems and incomplete information of players. The text chunk discusses a stochastic game with states, action sets, reward functions, transition functions, and policies for players. The goal is to maximize cumulative rewards. The agent in this game is player 1, and it considers two-player stochastic games with player types. The opponent, player 2, has a set of types \u0398. In a two-player stochastic game, player 2 has a set of types \u0398 with an unknown \u03b8 \u2208 \u0398. There is a prior distribution p(\u03b8) over opponents. Each type \u03b8 \u2208 \u0398 has a reward function R i (s, a 1 , a 2 , \u03b8) for players i \u2208 {1, 2}. The tasks are formalized as Bayesian stochastic games with modified reward functions R 1 , ..., R N and a prior distribution p over opponent types. Meta-learning aims to quickly train a model for new tasks using data from similar tasks. Training tasks {T } N1 i=1 are given, and N 2 new tasks {T } N1+N2 i=N1+1 are used for testing. Model-Agnostic Meta-Learning (MAML) is a meta-learning algorithm for tasks sampled from a distribution. It identifies mappings for tasks and uses a meta-learner for initialization. The update for specific tasks is based on learning rates. Before discussing Bayesian stochastic games, opponent modeling is considered to make predictions. In Model-Agnostic Meta-Learning (MAML), a meta-learning algorithm for tasks sampled from a distribution, opponent modeling is crucial for making predictions about the opponent's behavior in games. This involves predicting the opponent's goals, next actions, or positions to improve the agent's policy. The task is divided into modeling the opponent's estimated value and learning a policy based on this estimation, which can be solved using various reinforcement learning methods. Opponents are assumed to have a prior distribution for their types, allowing for more strategic decision-making. In Meta-OpponentAgent learning (MOA), opponents with a prior distribution for their types are treated as samples. Data collected from these opponents can help generalize the model and policy to new opponents. The opponent modeling and policy learning are considered as two metalearning tasks, with the former being imitation learning or supervised learning, and the latter being a RL task. Both tasks can apply MAML for meta learning. The model and policy are jointly trained via meta-learning, and when a new opponent arrives, the model and policy are initialized with the meta-model and meta-policy. The training procedure involves using a network for opponent modeling and another for policy learning, both trained via MAML. The opponent is assumed to have a fixed policy, and there is a distribution over opponent policies. The goal is to model the current opponent using data collected from playing with other opponents. The training procedure involves modeling the current opponent using data collected from playing with other opponents. A network called opponent network (OPNet) is used to learn the opponent's value function, with the goal of minimizing the distance between predicted and actual values. The training procedure involves modeling the current opponent using data collected from playing with other opponents. Data is collected for each opponent to update parameters, followed by using the learned parameters to initialize for the current opponent. An empirical Bayesian model is learned over the opponent population, making it easier to adapt to new opponents. The agent's policy learning is trained through a meta-learning process, with parameters represented by \u03c6. Various RL methods can be employed to learn the policy, such as Dueling DQN. The agent utilizes Dueling DQN BID15 as the learning method to adapt its policy. It collects data from opponents to update parameters and initializes with learned parameters for new opponents. The policy is trained through meta-learning with parameters represented by \u03c6. The agent uses Dueling DQN BID15 to adapt its policy and collects data from opponents to update parameters. It initializes with learned parameters for new opponents and trains the policy through meta-learning with parameters represented by \u03c6. The learned g \u03c6 is considered the approximated Bayesian Optimal Policy against the opponent distribution, guiding the agent to explore the potential direction when meeting a new opponent. The agent plays with each opponent using OPNet to predict the opponent, and the Dueling DQN uses the prediction as part of its input to give a policy. Both OPNet and Dueling DQN are updated jointly, focusing on multi-agent Reinforcement learning tasks and connecting them with game theory. Some works focus on solving equilibriums in specific games, such as using a self-play deep RL method for two-player zero-sum perfect-information games like Go. Other research aims to learn policies for agents using specific strategies to play against opponents. Some works address opponent modeling, with some proposing methods to infer the goals of others automatically. However, focusing on specific opponents can lead to weaknesses against others. Certain works attempt to model specific opponents while learning a robust policy from a game-theoretical perspective. Our work aims to gain information for opponent modeling and policy improvement in two-player games with specific uncertainties. We test our method on three different games: a chasing game, a blocking game, and a recommending game. In the recommending game, the opponent has a private type set with infinite elements. The game involves grid worlds where players choose one-step directions as actions. The value function for the opponent is based on goals or next positions. The Meta-Opponent method is used to train opponent models. The Meta-Opponent method trains the model for opponents during gameplay with training opponents, then uses this model to initialize the agent for a new opponent. The Meta-agent without model method uses Dueling DQN to learn directly from states via MAML without modeling opponents. The No Meta-Learning method directly trains the model and agent for new opponents to demonstrate the efficiency of meta-learning in adapting to new opponents. In the chasing game, an 8x8 grid board is used. In the chasing game, players have specific goals on an 8x8 grid board. Player 1 must chase Player 2 to its goal within 15 steps to win a reward. Player 2's goal in the chasing game is to reach specific grid locations for rewards. Four methods are tested, with MOA showing the best performance. The methods involve meta-training with 20 opponents, followed by testing with 10 new opponents. MOA outperforms MA, MO, and NM, with a reward trend that initially drops before rising during testing. The comparison of different methods in the chasing game shows that MOA outperforms MA, MO, and NM. MOA's reward trend initially drops before rising during testing, indicating adaptation to the current task. NM learns without meta-training but requires more games to improve its policy. Simply training the model of the opponent, as in MO, does not improve efficiency. MA performs the worst due to ignoring the opponent in its model. In blocking games, the goal is for player 2 to reach the top two rows while player 1 blocks them. Player 2 has 5 paths to choose from, each with a probability. Player 1 gets a reward if they block player 2. Training involves sampling 15 opponents with distributions over the paths. In blocking games, player 2 has 5 paths to choose from, with a distribution over them. Training involves sampling 15 opponents with distributions over the paths. Each method trains for 800 iterations to get meta-parameters, then tests with 10 new opponents. MOA shows quick improvement, while MA struggles, highlighting the importance of opponent modeling. In blocking games, player 2 has 5 paths to choose from, with a distribution over them. Training involves sampling 15 opponents with distributions over the paths. Each method trains for 800 iterations to get meta-parameters, then tests with 10 new opponents. MOA shows quick improvement, while MA struggles, highlighting the importance of opponent modeling. The rewards along testing process show MOA adapting to new opponents in less than 500 games, while MO and NM improve slowly. A recommending game with a 7*7 size map is similar to a business recommending goods to consumers. In recommending games, Player 1 recommends a purple object to Player 2, with rewards based on reaching the object. The reward is determined by the vertical coordinates of the goal and recommended object. MOA outperforms other methods in testing, showing sample efficiency. Random rewards add variance to the process. The results show that MOA performs well in testing, demonstrating sample efficiency by gaining prior information from meta-learning. This method can build models for opponents and find good policies, although it may be sample-inefficient. A proposed method aims to speed up learning by utilizing information from experiences with other opponents. Our method utilizes meta-learning to train opponent modeling and policy improvement simultaneously, showing sample efficiency in practical scenarios with stable opponent distributions."
}