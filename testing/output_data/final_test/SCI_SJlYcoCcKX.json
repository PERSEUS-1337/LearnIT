{
    "title": "SJlYcoCcKX",
    "content": "Although deep neural networks are powerful, deploying large models on embedded systems is challenging due to high computational costs and storage limitations. Knowledge distillation (KD) transfers model knowledge from a teacher model to a smaller student model, enabling the use of large networks on portable platforms. This paper introduces a method to improve student DNN performance by approximating the neuron manifold of a teacher network. Several novel methods for learning neuron manifold from DNN models are proposed, leading to significant performance improvements across various architectures and training data. The Neuron Manifold Transfer (NMT) method shows great improvement compared to other KD methods. In recent years, deep neural networks have gained popularity in computer vision and natural language processing. Well-trained models excel in tasks like image classification, object detection, and pattern recognition. However, larger networks come with high computational costs and memory footprints, limiting their deployment on portable devices. This hinders the utilization of vast amounts of data collected by mobile devices, causing delays in time-sensitive predictions. Recently, extensive works have been proposed to address the issue of model compression for reducing computational burden on embedded systems. In 2006, Bucilu\u01ce et al. introduced a method to train a neural network to mimic the output of a complex ensemble, leading to the concept of knowledge distillation (KD) where knowledge is transferred from a deeper network (teacher) to a shallower network (student) using softened softmax as the soft target. In knowledge distillation (KD), the teacher's knowledge is defined as softened softmax, allowing the student to mimic soft target distribution. Various works have emerged utilizing different forms of knowledge, such as regarding spatial attention maps as network knowledge. However, limitations exist, like assuming the absolute value of hidden neuron activation indicates importance only for image classification tasks. Another common assumption is that deeper networks always learn better representation, leading to methods like FitNets trying to train a thin deep network using a shallow one with more parameters. In 2017, TuSimple researchers introduced different forms of network knowledge transfer, including using softmax and attention maps. Neuron manifold transfer was also explored to reduce computational costs. Other approaches like Maximum Mean Discrepancy and cross sample similarities were used to improve performance in transferred networks. In DarkRank, network knowledge is defined as cross sample similarities. Well-defined network knowledge can greatly improve the performance of transferred networks. A perfect knowledge transfer method should transfer one neural network architecture into another while preserving generalization. Mimicking the teacher logit or features properties is not beneficial due to the complexity of large DNNs. Another perspective on DNN training is essential for effective knowledge transfer. During DNN training, the shape of neuron features, including value and distance between features, is crucial for knowledge transfer. Manifold approximation techniques, used in Topological Data Analysis, can project high-dimensional data to a lower-dimensional manifold while preserving numerical and geometric properties. This approach overcomes limitations of previous feature mapping methods and enhances class distribution matching. The paper introduces neuron manifold information to improve feature representation and spatial relations in neural networks. By compressing the teacher model, the student model becomes more reliable. The contributions include introducing neuron manifold knowledge, formalizing manifold space in feature maps, and testing the method on various tasks to enhance student network performance. The Neuron Manifold Transfer (NMT) method is tested on MNIST, CIFAR-10, and CIFAR-100 datasets, showing significant improvements in transferring performance. The Neuron Manifold Transfer (NMT) method improves student performance on MNIST, CIFAR-10, and CIFAR-100 datasets by transferring network knowledge through soft target knowledge distillation. This approach, pioneered by Geoffrey Hinton, involves mimicking the teacher network's softened outputs to achieve good performance. Soft target knowledge distillation aims to project samples into a continuous distribution, reducing intra-class variation with a noise-based regularizer. However, it may not improve performance in binary classification tasks. Network feature knowledge transfer addresses this limitation by transferring intermediate features, as seen in FitNet by Romero et al. FitNet proposed transferring a wide and shallow network to a thin and deep one, aiming to mimic the full feature maps of the teacher. However, the computational cost and differences in capacities between teacher and student can impact performance. Zagoruyko & Komodakis introduced Attention Transfer (AT) to define network knowledge as spatial attention maps, reducing computational cost by compressing a 3D tensor to a 2D spatial attention map. They also assumed the absolute value of hidden neuron activation can indicate importance. The importance of hidden neuron activation can indicate network feature knowledge. Different approaches like Maximum Mean Discrepancy and Flow of Solution Procedure provide supervision in vision tasks. Jacobian knowledge offers a full picture of DNN from a function perspective. Sobolev training uses Jacobian-based regularizer on Sobolev spaces for higher order derivatives supervision. In the context of neural network knowledge transfer, previous methods like Maximum Mean Discrepancy and Jacobian knowledge have been explored. However, the complexity of knowledge distillation in deep neural networks suggests a need for revisiting the approach. The use of Sobolev spaces and regularizers for higher order derivatives supervision has shown potential in knowledge transfer. Knowledge distillation involves training a student network (S) to mimic a teacher network (T) by minimizing a loss function that includes cross-entropy and a term encouraging similarity to the teacher's outputs. This method is used to transfer knowledge from a trained teacher network to a student network. The second term in knowledge distillation minimizes the difference between student and teacher network outputs. Teacher network layers are compared to student network layers, with a focus on spatial map activations. Manifold approximation is used to avoid dimensionality issues in CNN features. Manifold approximation is widely used in Big Data analysis to reduce dimensionality. Linear and nonlinear techniques aim to find lower dimensional manifolds in scattered input data. Determining the neuron manifold for transfer learning is crucial for performance. Most current methods are learning-based, but high computational costs require a simpler approach. Inspired by Moving Least Squares, a useful method for neuron manifold approximation is needed. Neuron manifold approximation using Moving Least Squares Projection (MLSP) with O(n) complexity requires feature points to be bounded and compact with density \u03c1. The error bound of the approximation is minimized to ||M d \u2212 m|| < k \u00b7 h m+1, where M d is the approximated manifold, m is the ground truth sub-manifold, and k is an adjustment factor. This method simplifies finding the neuron manifold for transfer learning in Big Data analysis. Neuron manifold approximation using Moving Least Squares Projection (MLSP) involves finding a local d-dimensional affine space H and projecting feature points onto it to minimize error and retrieve target points as neuron manifold features. This method simplifies finding the neuron manifold for transfer learning in Big Data analysis. The affine space H is found using an iterative procedure with randomly initialized basis vectors. A weight function \u03b8(x) ensures that features far from H have less influence. The neuron manifold projection function p is approximated by a weighted least squares vector valued polynomial function. The orthogonal projections of feature points onto H are used to formulate m(x) with a non-negative weight function \u03b8(s) and Euclidean norm. Neuron Manifold Transfer involves projecting feature maps from teacher and student networks onto lower dimensional manifolds for training. The teacher network neuron manifold is computed from the feature dimension, and student network parameters are trained accordingly. The text discusses training a student network by transferring knowledge from a pre-trained teacher network with different architectures. The teacher network includes models like ResNet-34 and VGG-19, while the student network has fewer hidden units. The process involves minimizing a specific loss function to train the student network effectively. The text discusses comparing a neural machine translation (NMT) method with other knowledge transfer methods like KD, attention transfer, HintNet, and Neuron Selectivity Transfer. Parameters for each method are specified, and modifications are made to achieve optimal results. The NMT method uses a specific manifold approximation function and lambda value for best performance. The implementation will be made publicly available if the paper is accepted. The implementation will be publicly available if the paper is accepted. A toy experiment on MNIST dataset with 10 classes is conducted, achieving high accuracy. Neuron Manifold Map is illustrated for better understanding. The Neuron Manifold Map is illustrated in FIG3, showing the first layer of Hinton-1200 normalized between 0 to 1. Moving Least Squares method is used to approximate the true manifold. Feature points are represented by white dots forming hyper-balls, with selected black dots indicating representative feature points. Experiments on CIFAR datasets show surprising results, with CIFAR-10 and CIFAR-100 datasets containing 50K training images and 10K testing images each. Training is done using ADAM optimization with a mini-batch size of 128 on a single GPU for 120 epochs. FIG4 displays the training and testing curves for CIFAR10 and CIFAR100 experiments. The Neuron Manifold Map in FIG3 shows the first layer of Hinton-1200 normalized between 0 to 1. Experiments on CIFAR datasets reveal that NMT achieves reliable classification results, focusing on inter-class relations. NMT converges faster than other methods, does not rely on soft targets, and reduces knowledge transferring time significantly. The computational cost of AT is a factor in this efficiency. The knowledge transferring time is reduced significantly with NMT compared to AT and FitNet due to lower computational cost. NMT shows great performance in training CIFAR10 with fast convergence speed. VGG training is more challenging, requiring careful selection of feature blocks for computing the neuron manifold. In training the network on CIFAR-100, system resource usage was the main focus. FitNet had the slowest matching time between teacher and student, serving as the baseline. Despite the overhead from computing neuron manifold, NMT still achieved a x6.26 speedup compared to FitNet and AT. KD was the fastest at 143 seconds per epoch but had the worst training result. FitNet and AT were not as effective as transfer methods. FitNet failed in the task of knowledge transfer. In this paper, a novel method for knowledge transfer called Neuron Manifold Transfer (NMT) is proposed. By utilizing Topological Data Analysis, DNN's feature and geometric properties are extracted. Testing on various datasets shows promising results, indicating improved feature representations for high-level vision tasks in the future. This approach aims to facilitate the design of more effective knowledge transfer methods. In future work, the NMT methods will be explored for various regression problems like super resolution and optical flow prediction to enhance knowledge transfer design."
}