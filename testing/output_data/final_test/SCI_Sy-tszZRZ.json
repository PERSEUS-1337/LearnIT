{
    "title": "Sy-tszZRZ",
    "content": "In this paper, the representational power of deep neural networks (DNN) with piecewise-linear (PWL) functions like rectifier or maxout activations is studied. The complexity of these networks is analyzed by examining the number of linear regions in the PWL function. Building upon previous work, tighter bounds on the number of linear regions for rectified and maxout networks are refined. A novel method for exact counting of linear regions is developed using a mixed-integer linear formulation. This method is used to visualize changes in the number of linear regions during DNN training. The success of deep learning algorithms in various domains has led to the design of network architectures with greater depth and PWL activation functions like ReLUs. Researchers are exploring the theoretical modeling of the representational power of DNNs by analyzing the number of linear regions in the PWL function. Visualizing changes in the number of linear regions during DNN training is crucial for understanding the network's complexity. Theoretical modeling of DNNs involves approximating any continuous function with a single hidden layer of sigmoid activation functions. Shallow networks require exponentially more neurons compared to deeper networks for modeling functions. Various activation functions like threshold, logistic, hyperbolic tangent, ReLUs, and maxouts have been used. Activation functions like sigmoid, ReLUs, and maxouts offer different modeling capabilities. ReLUs have been shown to be more expressive than similar-sized threshold networks. The complexity of neural networks belonging to the family of PWL functions can be analyzed by looking at how the network can partition the input space into linear response regions. Functions partitioning the input space into a larger number of linear regions are considered more complex. The complexity of neural networks can be analyzed by how they partition the input space into linear regions. ReLUs create exponentially more linear response regions in deep networks compared to shallow ones. Studies have provided bounds on the number of linear regions for ReLU DNNs and maxout networks, with improvements on upper and lower bounds. BID1 improves the lower bound by providing a family of ReLU DNNS with an exponential number of regions given fixed size and depth. This work aims to better understand the representational power of DNNs using PWL activation functions, focusing on feedforward neural networks with input and output variables. Each hidden layer has hidden neurons with specific activations determined by weight matrices and bias vectors. The activation functions of neurons in a DNN are obtained using a vector, with the ReLU activation function applied to hidden neurons and outputs. The DNN is treated as a piecewise linear function mapping input to output. This paper focuses on investigating the bounds of linear regions in the PWL function. Two definitions for linear regions are discussed, and an activation pattern for input vectors is considered. Activation patterns in a DNN are defined by sets of active ReLU units. Linear regions in a PWL function are sets of input vectors corresponding to activation patterns. The paper explores the relationship between linear regions and activation patterns in DNNs. In a DNN, activation patterns are sets of active ReLU units that define linear regions in a PWL function. The relationship between linear regions and activation patterns is explored in the paper. Zaslavsky (1975) shows that n hyperplanes divide a d-dimensional space into at most d s=0 n s regions. The arrangement of n hyperplanes divides a d-dimensional space into at most d s=0 n s regions in general position. This corresponds to the maximal number of regions in a single layer DNN with n ReLUs and input dimension d. Visualization in FIG1 shows how ReLUs partition the input space, with hyperplanes at different layers affecting the regions. Additional layers further partition the regions. The text discusses how activation boundaries behave like hyperplanes in hidden layers of a DNN. It presents tighter bounds on the number of linear regions for PWL functions using ReLUs, including exact counts for one-dimensional inputs. Additionally, it provides the first upper bound for multi-layer maxout networks. The text presents tighter bounds on the number of linear regions for PWL functions using ReLUs and introduces a mixed-integer linear formulation for exact counting. It also shows the exact counting of linear regions for small-sized DNNs during training, offering insights into how these regions vary throughout the process. The text introduces tighter bounds on the number of regions for deep rectifier networks with ReLUs. It presents an upper bound on the maximal number of regions, improving upon previous results. The bound is sensitive to the positioning of layers, highlighting a bottleneck effect. The text discusses the bottleneck effect in deep rectifier networks with ReLUs, showing that moving a neuron from the first layer to the second layer decreases the bound, tightening the bottleneck. Smaller widths in the initial layers create a bottleneck effect, illustrated in FIG5. The text discusses the transition from a small-to-large network to a large-to-small network in terms of width, showing that reducing the bottleneck increases the bound. Adding a layer of constant width at the end can decrease the bound when the previous layers create a bottleneck. In applications with large input dimensions, deep networks have more regions than shallow networks. In large input dimensions, deep networks have more regions than shallow networks. Comparing deep networks with L layers of width n to shallow networks with one layer of width Ln, the ratio of maximal regions approaches zero as L increases. Adding a layer of n1 + n2 ReLUs in a 2-layer network with input dimension n0 larger than both widths n1 and n2 increases the number of regions. The number of layers affecting the total size of the network plateaus at a lower value than the maximal number of regions for shallow networks. The highest bound decreases with an increase in input dimension. The proof of Theorem 1 involves defining operations on activation sets and matrices in deep networks. The proof of Theorem 1 focuses on the dimension of regions in deep networks. Zaslavsky (1975) showed that the maximal number of regions induced by hyperplanes is at most a certain value, which cannot recover to a higher dimension. The proof focuses on bounding the maximal number of regions induced by hyperplanes in deep networks. The key idea is to count regions within the row space of W. Lemmas are used to show that the number of regions induced by neurons at a certain layer is bounded. The dimension of a region can be recursively bounded in terms of the dimension of the containing region and the number of activated neurons. The proof of Theorem 1 combines Lemmas 3 and 4 to construct a recurrence that bounds the number of regions within a given region of dimension d. The dimensions of the regions are non-increasing as we move through the layers partitioning it. If a region's dimension becomes small, it cannot be further partitioned into many regions. If a region's dimension becomes small, it cannot be further partitioned into many regions. To construct a DNN with many regions, we need to keep dimensions high. The upper bound for the number of regions in a deep rectifier network with L layers and an input of dimension 1 is (n + 1) L. The lower bound is n L\u22121 (n + 1). The upper bound is tight for n 0 = 1 with enough neurons. The upper bound for the number of regions in a deep rectifier network with L layers and an input of dimension 1 is (n + 1) L. This construction aims to maximize the dimension of each region to increase the total number of regions in the network. The new construction introduces an additional region that can be replicated with other strategies. Both the lower bound from Mont\u00fafar et al. (2014) and from BID1 can be slightly improved by extending a 1-dimensional construction. Theorem 6 states that the number of linear regions in a rectifier network is lower bounded by a specific formula. The proof is provided in Appendix F. Theorem 7 shows the existence of a rectifier network with certain characteristics. The differences between these theorems and previous ones involve specific conditions and adjustments in the network size. Theorem 8 discusses the maximal number of regions in a deep neural network with maxout units, providing a formula for the bound. This is a continuation from previous theorems on rectifier networks, showing differences in conditions and network characteristics. The maximal number of regions in a deep neural network with maxout units is bounded by O((k^2n)Ln0), where x is bounded and polyhedral. This formulation can be used to count the number of linear regions in the network. The formulation for a deep neural network with maxout units involves lifting the representation to a space containing complementary neurons. Binary variables are used to denote neuron activity, with constraints mapping input to output. Theorem 9 states that a feasible solution with fixed input values yields the network output. The proof is provided in Appendix I, with further details on exact counting in Appendix J. The theory for unrestricted inputs and a mixed-integer formulation for maxout networks is discussed in Appendices K and L. These results have important implications, allowing for the analysis of trained neural networks using mixed-integer optimization solvers. Two experiments on region counting using small-sized networks with ReLU activation units on the MNIST dataset are conducted. In small-sized neural networks with ReLU activation units on the MNIST dataset, the number of linear regions classifying each digit varies widely during training. The total number of regions per training step is shown in Fig. 5(a), with error measures in Appendix M. The number of linear regions jumps orders of magnitude and varies more widely with each added layer. In the second experiment, rectifier networks with two hidden layers totaling 22 neurons were trained under the same conditions as before. The test error ranged from 5 to 6%, and the number of linear regions within 0 \u2264 x \u2264 1 was counted. The representational power of a DNN can be studied by observing the number of linear regions it represents, with improvements on upper and lower bounds. In this work, the authors improve on the upper and lower bounds for linear regions in rectified networks. They introduce a first upper bound for multi-layer maxout networks and provide insights on the impact of reducing the width of early layers on the number of regions. The dimensions of linear regions are influenced by both width and the number of activated ReLUs. This insight led to the creation of a 1-dimensional construction with the maximal number of regions by eliminating a zero-dimensional bottleneck. Shallow networks can achieve more linear regions when input dimensions exceed the number of neurons in the DNN. A mixed-integer linear formulation is used to map input space to output, showing the exact count of linear regions for small DNNs during training. The number of linear regions correctly classifying each digit in the MNIST benchmark increases with network depth. Varying the width of layers with a fixed number of neurons validates the bottleneck effect. The current results suggest new avenues for future research in understanding the design of better DNNs. Analyzing the shape of linear regions and the relationship between the number of neurons and computational resources are key areas of interest. Additionally, exploring the optimal network depths for maximizing ReLU upper bounds could provide valuable insights for designing DNNs. The research focuses on understanding the relation between the number of linear regions and accuracy in neural networks. It explores the impact of overfitting and the need for efficient algorithms for exact counting in larger networks. The theory for exact counting in the case of maxouts and unrestricted inputs is discussed, along with proofs for theorems and lemmas on linear regions. This research provides insights for designing better DNNs and maximizing ReLU upper bounds. In this section, properties of the upper bound for the number of regions of a rectifier network are presented. The bound is denoted by B(n0, n1, ..., nL), where n0 is the input dimension and n1, ..., nL are the widths of layers 1 through L of the network. The rearranged form of the bound is discussed, along with implications for the exact maximal number of regions denoted by R(n0, n1, ..., nL). Additionally, Lemma 10 is introduced as a useful tool for the section. The proposition characterizes the bound for a 2-layer network with widths n1, n2 and input dimension n0. If n0 < n1 or n0 < n2, the inequality B(n0, n1, n2) \u2264 n1+j=0 n1+n2j holds. By applying Lemma 10, the index set J becomes redundant for n0 \u2265 n1 and n0 \u2265 n2, leading to a less-or-equal sign in the inequality. The proposition and corollaries discuss the relationship between the number of regions in single-layer and two-layer networks with ReLUs. For large input dimensions, a two-layer network does not have more regions than a single-layer network with the same number of neurons. Moving a neuron from the second layer to the first layer strictly increases the bound. Moving neurons from the second layer to the first layer increases the bound in a 2-layer network. The input dimension must be large for this to hold true. However, simply moving neurons to earlier layers does not always increase the bound. In deep networks with equal widths, halving the number of layers is discussed as an extension of the previous proposition. Proposition 14 discusses the impact of halving the number of layers in a deep network with equal widths and input dimension. It suggests that making a deep network shallower allows for more regions when the input dimension is equal to the width. The inequality is met with equality when there is only one layer and strict inequality when there are two or more layers. Proposition 15 provides an upper bound for an L-layer network with equal widths n and any input dimension n0 \u2265 0. The proof involves relaxing constraints and applying Vandermonde's identity. Stirling's approximation is used for the bound on 2nn. Corollary 16 follows from Proposition 15 and Theorem 1, discussing the ratio between R(n0) and R(n). The maximal number of regions in a deep network compared to a shallow network approaches zero as the network size goes to infinity. For an L-layer rectifier network with equal widths n and input dimension n0 \u2265 n/3, the maximal number of regions is at least 2^(2/3 Ln). The lower bound for the maximal number of regions in a deep network is derived by considering the row space of the weight matrix. This bound is expressed as \u2126(2^(2/3 Ln)) and is based on the orthogonal decomposition theorem from linear algebra. The maximal number of regions in a deep rectifier network with L layers and n l rectified linear units at each layer is at most DISPLAYFORM0 . . , n l\u22121 \u2212 j l\u22121 , n l } \u2200l = 1, . . . , L}. This bound is tight when L = 1. The partitioning process at each layer sequentially divides the regions obtained from the previous layer, leading to this maximum number of regions. The number of subregions within a region in a deep rectifier network is recursively bounded by constructing a recurrence R(l, d) to determine the maximal number of regions obtained from partitioning a region. This is based on hyperplanes partitioning regions at each layer, with a maximum number of regions determined by the activation set size. The maximal number of regions in a deep rectifier network is determined by a recurrence formula based on the activation set size. The bound is tight for a single-layer network, and for a network with L layers and n neurons per layer, the maximal number of regions is L multiplied by the sum of (n + 1) over all layers. The construction strategy from Mont\u00fafar et al. (2014) aims to improve the one-dimensional construction by increasing the dimension of regions with empty activation patterns. This is achieved by shifting neurons forward and flipping the direction of specific neurons. The strategy involves constructing a linear function with a zigzag pattern using n Rectified Linear Units (ReLUs). The construction strategy involves using n ReLUs to create a linear function with a zigzag pattern that replicates in each slope a scaled copy of the function in the domain [0, 1]. This pattern aims to create n + 1 regions instead of n, achieved by having each linear piece go from zero to one or one to zero. The construction strategy involves using n ReLUs to create a linear function with a zigzag pattern. Breakpoints are turned into parameters, and sign and bias parameters are added to the function. The weights and biases are determined in each interval to form the zigzag pattern. The signs of the weights must match the directions of the ReLUs. The parameters for the linear function with a zigzag pattern include weights, biases, global bias, and breakpoints. The goal is to have linear pieces that transition smoothly from zero to one or one to zero. The pattern alternates between intervals where the function is either 1 t\u2212s x\u2212 s t\u2212s or \u2212 1 t\u2212s x+ t t\u2212s. The weights' signs determine the direction of the ReLUs. The ReLU neurons correspond to hyperplanes in one dimension, with breakpoints determining activation. Neurons point in directions that define activation conditions. Valid weights are chosen based on these directions to ensure activation in each region. The directions of the neurons determine activation conditions in each region. A system of linear equations is formed to show the existence of a solution where 0 < t1 < ... < tn < 1. Bias and weight variables can be expressed in terms of ti variables, allowing for back-substitution to find their values. The weights and biases in a rectifier network can be determined by finding values for ti variables satisfying a set of equations. Back-substitution can then be used to obtain all other weights. An example with four units is provided, showing the construction process. The maximal number of linear regions induced by such a network is lower bounded. The number of regions in a network with n 0 input units and L hidden layers is lower bounded by a 1-dimensional construction method. Each network is organized into n 0 independent networks with input dimension 1, resulting in a larger number of regions compared to previous methods. The compound network's activation patterns and regions are the product of the individual networks, ensuring a minimum number of regions for the entire network. The number of regions in a network with n0 input units and L hidden layers is lower bounded by a 1-dimensional construction method. The network can be organized into n0 independent networks with input dimension 1, resulting in a larger number of regions. Each region corresponds to a linear function, with boundaries composed of pieces contained in hyperplanes. The maximum number of regions in a layer with n maxout units is determined by an arrangement of k/2n hyperplanes. In a network with n0 input units and L hidden layers, the number of regions is lower bounded by a 1-dimensional construction method. The regions correspond to linear functions with boundaries in hyperplanes. The maximum number of regions in a layer with n maxout units is determined by k/2n hyperplanes. The maxout operator \u03c6 does not guarantee reductions in rank, unlike the ReLU operator \u03c3. Lemma 18 states that the number of regions induced by n neurons at layer l within a region S is at most DISPLAYFORM3, where dl = min{n0, n1, ..., nl}. The constraints ensure that the neural network maps input to output correctly, similar to ReLUs but with some modifications. The constraints in the neural network ensure correct input to output mapping, similar to ReLUs but with modifications. The error during training and after training for different configurations is shown in Figures 8 and 9, indicating a relation between accuracy and the linear regions' magnitude. The upper bound from Theorem 1 can be maximized by adding more layers as the number of neurons increases, while the smallest depth preserving such growth is better due to a secondary exponential effect. The upper bound from Theorem 1 can be maximized by adding more layers as the number of neurons increases, while the smallest depth preserving such growth is better due to a secondary exponential effect that shrinks the bound if the number of layers is too large for the total number of neurons."
}