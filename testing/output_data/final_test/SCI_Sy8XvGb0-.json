{
    "title": "Sy8XvGb0-",
    "content": "Deep generative neural networks are effective at modeling complex data distributions. A method is developed in this paper to condition generation without retraining the model by learning latent constraints. This allows for conditional sampling with gradient-based optimization or amortized actor functions. By combining attribute constraints with a \"realism\" constraint, realistic conditional images can be generated from an unconditional variational autoencoder. Additionally, identity-preserving transformations can be made with minimal adjustments in latent space to modify image attributes. Generative modeling of complex data like images and audio is a challenge in machine learning. While unconditional sampling is not very practical, conditional sampling, where specific attributes are specified, requires a different approach. By learning latent constraints, it is possible to modify image attributes with minimal adjustments in latent space. One can enforce user-specified constraints at training time in creative applications using deep latent-variable models like GANs and VAEs for generating varied outputs. Leveraging the structured latent space allows for creating new conditional controls for sampling and transformations. In creative applications, constraints can be enforced post-hoc on pre-trained unsupervised generative models like GANs and VAEs. This approach eliminates the need for retraining the model for each new set of constraints, making it easier for users to define custom behavior. The process involves creating an unsupervised model to reconstruct data from latent embeddings and using the latent structure as prior knowledge to impose behavioral constraints. Key contributions include using critics to predict regions in the latent space with desired attributes and high mass under the training data's marginal posterior. To generate conditional outputs, a standard VAE is pretrained for good reconstructions. An actor-critic pair is then trained using constraint-satisfaction labels to discriminate between actual data encodings and latent vectors sampled from the prior. The generator and discriminator operate on a concatenation of latent vectors and binary attribute vectors, allowing conditional mappings in latent space. Sampling is done by shifting latent samples from either the prior or data using gradient-based optimization or an amortized generator. The text discusses generating high-value samples from an unconditional model by learning a critic function in latent space and using an actor function. It focuses on the tradeoff between reconstruction and sample quality in VAEs, enforcing a \"realism\" constraint. Identity-preserving transformations are applied by adjusting latent space minimally to meet desired constraints. The text discusses using VAEs and GANs for zero-shot conditional generation, where actor-critic pairs satisfy user-specified constraints without labeled data. VAEs and GANs generate samples by passing simple distribution samples through neural networks. GANs fool a classifier while VAEs fit data using a variational approximation. Using an actor to shift prior samples to satisfy the realism constraint, we achieve more realistic samples without sacrificing sharpness. The encoder distribution q(z | x) approximates the posterior p(z | x), while the likelihood function \u03c0(x; g(z)) depends on parameters output by a decoder function g(z). The evidence lower bound (ELBO) is maximized by fitting q and g to the data. The evidence lower bound (ELBO) is often maximized by fitting q and g to the data. GANs suffer from the \"mode collapse\" problem, while VAEs exhibit a tradeoff between sharp reconstructions and sensible-looking samples. VAEs tend to produce either blurry reconstructions and plausible novel samples or sharp reconstructions but bizarre samples due to the \"holes\" problem in the decoder training process. If the decoder can reconstruct values with high accuracy, the posterior will be highly concentrated, leading to underestimation of posterior variance in samples. The marginal posterior q(z) will be highly concentrated, leading to strange-looking samples in VAEs. Tuning \u03c3 x to maximize the ELBO results in optimal \u03c3 x \u2248 0.1. Low variance in latent dimensions influences generated images, with some dimensions showing more realistic digits. In the context of VAEs, a different approach to conditional generation and identity-preserving transformation involves training an unconditional VAE with specific hyperparameters for good reconstruction. A \"realism\" critic is then trained to assess the quality of generated samples, addressing issues like mode-collapse and blurriness seen in CGANs and CVAEs. To improve sample quality, a \"realism\" critic is trained to evaluate generated samples based on specific attributes. One approach is to optimize random z vectors to satisfy both realism and attribute critics, or use an \"actor\" network to map z vectors to a latent space region meeting constraints. This helps alleviate mode-collapse in GANs. The realism constraint is defined implicitly by samples from the marginal posterior q(z) and not p(z). See sections 3, 4, 5, and 6 for more details. To close the gap between reconstruction and sample quality, a critic D is trained to differentiate between samples from q(z) and p(z). The critic loss, LD(z), is calculated using cross-entropy with labels c=1 for z \u223c q(z | x) and c=0 for z \u223c p(z). Sampling from the prior is sufficient for models with lower KL Divergence, but for larger divergences, poor sample quality results, making it hard for D to learn a tight approximation of q(z) solely from p(z) samples. To bridge the gap between reconstruction and sample quality, a critic D is trained to distinguish between samples from q(z) and p(z) using cross-entropy loss. An inner-loop of gradient-based optimization is used to move prior samples closer to q(z). This process helps in achieving a realistic constraint in conditional generation with a CGAN actor-critic pair in the latent space of a VAE. Regularization during training ensures smoother transitions in latent space, resulting in higher quality generated images. The regularized model enforces identity preservation and fights mode collapse, producing less diverse images across columns but somewhat more diverse across rows. Amortization techniques using neural networks help speed up training. Using a GAN in this scenario can lead to mode collapse challenges. Applying a GAN in latent space can help avoid mode-collapse issues by regularizing G to find diverse solutions. A regularization term is introduced to encourage nearby solutions and allow exploration. VAE utilizes only a fraction of its latent dimensions, scaling the distance penalty based on utilization. Using a CGAN with binary attribute labels can control the attributes of generated samples. Using a CGAN in latent space involves training separate critics for attributes and realism constraints. Gradient ascent is performed on a weighted combination of critic values to preserve image identity. The procedure replaces D(z) and G(z) with conditional versions and concatenates attribute information for improved performance. The procedure involves using a CGAN actor-critic pair in latent space, which is computationally inexpensive compared to training a generative model from scratch. Different sizes of CGAN pairs are tested, with the smaller one achieving slightly worse results but using significantly fewer FLOPs/iteration. The quality of conditional samples from the CGAN pair and the impact of the distance penalty are demonstrated in Figure 4. The regularized CGAN actor constrains generation to be closer to the prior sample, maintaining similarity between samples with different attributes. It fights mode-collapse and increases diversity, but without a distance penalty, samples appear more realistic with prominent attributes. This is supported by accuracy results from a separate model trained to classify attributes from images. Comparisons with invertible CGANs are also provided. CGANs for generation BID24 are discussed, with emphasis on the differences in attribute labels between experiments. Actors trained with a latent distance penalty show slightly worse accuracy but produce a greater diversity of images. A \"small model\" CGAN with fewer parameters generates images of comparable quality. The model generates local solutions with slightly less attribute accuracy but visually similar samples. Using a VAE as the base model encourages filling up the latent space for better image mapping. The prior distribution imposes a natural scale on latent variables, leading to good reconstructions of held-out data. By training a critic to predict attribute labels of data embeddings, we can transform output attributes using gradient-based optimization. By maintaining realism constraints, we can prevent image distortion during transformations. Additionally, starting from a data point, we can perform gradient descent on realism and attribute constraints jointly to generate new samples while preserving the original image structure. The text discusses how transformed images can maintain their structure by optimizing realism, attribute cost, and distance penalty without the need for retraining the network. It also mentions leveraging pre-trained models to generate exemplars scored by a user-supplied reward function, eliminating the need for labeled data. In the prior sample, notes outside the C Major scale are shown in red. Transformation by G P=CMaj,d=0 ensures all notes fall within the scale without changing note density significantly. Another transformation by G P=CMaj,d=192 increases note density beyond 192. The actor, G, and critic, D, work together to shift samples to high-value states. The critic loss is the cross-entropy from c(x), and the actor loss includes a distance penalty for diverse outputs. Audio samples can be heard at https://goo.gl/ouULt9. The text discusses using a distance penalty to promote diversity in outputs, training an LSTM VAE on melodic fragments, and constraining pitch classes and note density in generated outputs. This is supported by results in Table 2 and demonstrated in Figure 6. During training, the actor explores and exploits to find high reward modes, settling into high value states with small movements in latent space. Conditional GANs and VAEs introduce conditioning variables, with CGANs adjusting images and BID32 proposing attribute vectors for transformations. The text discusses using p(z) as a heuristic for transformations in a latent space, focusing on satisfying constraints for unconditional and conditional generation. Actors optimized for specific constraints like C Major scale and note density show high satisfaction rates with minimal impact on other constraints. This approach aims to improve the expressiveness of priors in generative models. Recent work has focused on applying more expressive prior constraints to VAEs, aiming to find an implicit distribution indistinguishable from q(z). A realism constraint is used, relying on a discriminative model to clean up results. Adversarial training is also utilized to generate latent codes in a latent space discovered by an autoencoder, with a focus on unconditional generation. The procedure involves mapping molecules to properties and using optimization in the latent space to find desired molecules. Invalid molecules are rejected using off-the-shelf software. BID11 uses a classifier and Deep Q-network for generation, while BID22 optimizes latent space to create high-quality images. The work focuses on learning an amortized generator/discriminator in the latent space for diversity. The approach involves learning an amortized generator/discriminator in the latent space for diversity in conditional generation. Different architectures can be plugged in, including autoregressive decoders or adversarial decoder costs. Autoregressive priors in VAEs show promise for this method. Conditional samples can be obtained through ancestral sampling and transformations using gradients. Samples can be obtained through ancestral sampling and transformations using gradient ascent. Active or semisupervised learning approaches can reduce sample complexity. Real-time constraint learning could enable new applications. MNIST and CelebA datasets are used for images and attributes. MNIST images are 28x28 pixels, while CelebA images are centercropped to 128x128 pixels and downscaled to 64x64 RGB pixels. Attribute labels are narrowed down to the 10 most visually salient. The text discusses reducing 40 attributes to the 10 most visually salient ones for images and collecting over 1.5 million MIDI files to extract 3 million unique melodies. Melodies are represented as sequences of 256 categorical variables. Training procedures for encoders, decoders, and classifiers are detailed. The text describes training procedures for GANs, using a gradient penalty of 10, Adam optimizer with specific parameters, and sampling techniques. Inner-loop optimization for actors involves 100 iterations of Adam with specific learning rates. The model for MNIST data includes a deep feed-forward neural network with specific layer configurations. The encoder for the CelebA data consists of 4 2D convolutional layers with specific kernel sizes and output channels. The decoder includes 3 linear layers followed by ReLU activations, producing 28x28 outputs passed through a sigmoid for image generation. The VAE decoder uses a 1024-dimension multivariate Gaussian distribution with a diagonal covariance matrix for z. It passes z through linear and transposed convolutional layers to generate the output image. The classifier for image labels is similar to the VAE encoder but ends with a sigmoid cross-entropy loss. An LSTM-based sequence VAE is used for modelling monophonic melodies. The LSTM model uses 2048 units per cell to produce 1024 outputs, with half used as \u00b5 and the softplus of the other half used as \u03c3 to parameterize a 512-dimension multivariate Gaussian distribution for z. A hierarchical decoder is used to model long melodies, with a bar-level LSTM generating individual sixteenth note events. The LSTM model uses 2048 units per cell to produce 1024 outputs, with half used as \u00b5 and the softplus of the other half used as \u03c3 to parameterize a 512-dimension multivariate Gaussian distribution for z. A deep feed-forward neural network is used for G(z) in all experiments, with 4 linear layers and 2048 outputs each, followed by ReLU activation. The transformed z is computed as (1 \u2212 gates) * z + gates * \u03b4z, aiding in training by predicting shifts in z. When conditioning on attribute labels, y, a linear layer produces 2048 outputs concatenated with z for G(z, y). The model input for D(z) is produced by passing z through a deep feed-forward neural network with 4 linear layers. When conditioning on attribute labels, y, the labels are concatenated with z to compute D(z, y). Additional CelebA faces are generated by G CGAN with different lambda values, and samples are optimized to meet realism and attribute constraints. The optimization process involves 100 steps to generate images that satisfy realism and attribute constraints. The actor in the CGAN actor-critic model maps latent vectors to images with correct attributes but different identities. Tighter posteriors indicate more use of latent dimensions, with distance regularization scaled per dimension."
}