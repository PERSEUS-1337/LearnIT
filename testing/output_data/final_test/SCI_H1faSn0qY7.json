{
    "title": "H1faSn0qY7",
    "content": "DL2 is a system for training and querying neural networks with logical constraints. It translates constraints into a differentiable loss for training or querying the network. DL2 is effective in various scenarios and datasets. The challenge of making neural networks more reliable is addressed by incorporating constraints during training and inspecting already trained networks with specific queries. These approaches are limited to certain constraints, making them difficult to apply in other settings. DL2 introduces a new method and system for training and querying neural networks with logical constraints. It allows users to enforce domain knowledge during training and interact with the network through querying. The constraint language can express rich combinations of arithmetic comparisons using negations, conjunctions, and disjunctions. DL2 translates logical constraints into non-negative loss functions with key properties to guarantee constraint satisfaction. DL2 enables training and querying neural networks with logical constraints, ensuring constraint satisfaction by minimizing a loss function. Constraints can be expressed over probabilities, allowing for new and interesting training scenarios. DL2 increases prediction accuracy of CIFAR-100 networks in semi-supervised setting by capturing constraints in classification and regression tasks. It outperforms prior work with more restricted expressiveness. For example, GalaxyGAN BID22 requires networks to respect constraints imposed by physical systems, such as ensuring the sum of input pixels equals the sum of output pixels. With DL2, constraints can be expressed without hardcoding them into the network. DL2 allows for declarative expression of constraints in network training, enabling global training with restrictions on inputs outside the training set. This approach splits global training between the optimizer and the oracle, improving network performance by providing new inputs to challenge constraints. The Lipshcitz condition requires inputs to satisfy constraints, inspired by recent works showing neural network stability. DL2 enables users to query models with an SQL-like language, as demonstrated in generating adversarial examples with ACGANs. The query aims to find an input for the generator that produces an image classified as 1 by the generator but as 7 by the network. DL2 system translates this query into a differentiable loss and optimizes it to find solutions. The system can capture various prior works, including finding responsible neurons and generating adversarial examples. DL2 introduces constraints for training neural networks, including convex sets as PGD constraints. It offers a declarative language for querying network inputs, outputs, and neurons, compiled into a differentiable loss for optimization. The system effectively trains networks with constraints not in the training set, distinguishing between adversarial example generation and adversarial training. Probabilistic Soft Logic (PSL) translates logic into continuous functions over [0, 1]. PSL is not suitable for gradient-based optimization due to potential zero gradients. DL2 can express constraints like p1 > p2, allowing for non-linear constraints in neural network training. The convexity and closed-form solution in DL2 stem from the linearity of rules in the network's output. The work of BID27 and BID5 is limited in handling non-linear constraints and constraints on distributions. Unlike DL2, no prior work supports constraints for regression tasks. The constraint language consists of quantifier-free constraints formed with conjunction, disjunction, and negation. Atomic constraints are comparisons of terms. Atomic constraints (literals) involve comparisons of terms such as variables, constants, and functions. Terms can be defined over variables, constants, and network parameters, with functions being differentiable almost everywhere. The translation of formulas into loss is defined recursively on the structure of the formula, ensuring non-negativity and differentiability almost everywhere. The loss is parametrized by tolerance for strict inequality constraints and avoids pitfalls of other approaches. The translation of atomic constraints into scalar comparisons is done using a distance function. Negations are handled by rewriting them and computing the loss of their equivalent constraint. The method for training neural networks with constraints involves rewriting negations of atomic constraints and applying De Morgan's laws for conjunctions and disjunctions. As \u03be decreases and \u03b4 approaches 0, a theorem is derived stating that if x makes the loss 0, then there is a satisfying assignment; otherwise, if x makes the loss > \u03b4, then x is not a satisfying assignment. The proof of the theorem is provided in Appendix A. Training neural networks with constraints involves formulating a maximization problem over network weights to satisfy a given constraint. This objective is bounded between 0 and 1, attaining 1 only if the network satisfies the constraint. Multiple constraints are handled by combining their objectives using a convex combination. Training is done with an empirical objective using a training set instead of an unknown distribution. Training neural networks with constraints involves formulating a maximization problem over network weights to satisfy given constraints. The training is done using a training set instead of an unknown distribution. The user specifies constraints and weights, and the optimization problem can be split into two sub-problems. Training neural networks with constraints involves formulating a maximization problem over network weights to satisfy given constraints. The optimization problem is split into two sub-problems, where an oracle solves FORMULA10 and an optimizer solves FORMULA11 by translating logical constraints into differentiable loss functions. The difficulty in optimizing the loss arises from the different magnitudes of terms, making it challenging for first-order methods. Training neural networks with constraints involves formulating a maximization problem over network weights to satisfy given constraints. The optimization problem is split into two sub-problems, where an oracle solves one part and an optimizer solves the other by translating logical constraints into differentiable loss functions. The challenge lies in the different magnitudes of terms, making it difficult for first-order methods to optimize effectively. Some constraints have closed-form solutions, such as projecting into convex sets like line segments, L2, L\u221e, or L1 balls. Projection to a convex set is generally a difficult problem, so these constraints are excluded from the optimization function and added separately. The optimization is then rewritten to incorporate functions that map random samples to convex sets, and Projected Gradient Descent (PGD) is employed to solve it efficiently. Algorithm 1 outlines the training procedure with constraints for neural networks. A mini-batch is formed from the training set, and an oracle finds a solution using a declarative language. Queries can be made to find neurons responsible for predictions, differentiate networks, and generate adversarial examples. The optimization process involves translating logical constraints into differentiable loss functions and using Projected Gradient Descent for efficient optimization. The training procedure for neural networks involves forming mini-batches from the training set and using a declarative language to find solutions. Queries can be made to identify neurons responsible for predictions, differentiate networks, and generate adversarial examples. The optimization process includes translating logical constraints into differentiable loss functions and using Projected Gradient Descent for efficient optimization. The language allows users to specify tensors, define constraints, and compute target terms in queries. Examples of queries are shown, including ones for CIFAR-10 and MNIST datasets. The DL2 queries enable declarative search for inputs satisfying constraints over networks. The optimization process involves compiling constraints into a loss and optimizing with L-BFGS-B. Unlike training, querying looks for one assignment, allowing for the use of more sophisticated optimization methods. Further optimizations are discussed in Appendix C. The DL2 system is evaluated for querying and training neural networks with logical constraints. It is implemented in PyTorch and tested on various tasks across four datasets. Different types of constraints are considered for supervised learning, including global constraints and training set constraints. Previous work does not apply to global constraints, and DL2 can handle complex training set constraints. The DL2 system in PyTorch can handle complex training set constraints, such as local robustness and global constraints. Local robustness requires output probabilities to have small KL divergence for close inputs, while global constraints ensure valid images in an input's neighborhood have high probability for the correct classification. Lipschitz conditions are also defined for the training set constraint. The Lipschitz condition has two definitions: one for the training set constraint and another for the global constraint. The C-similarity constraint imposes domain knowledge constraints for CIFAR-10 networks. Additionally, a Segment constraint requires output probabilities to be positioned accordingly between inputs. The Lipschitz condition improves constraint accuracy in DL2, with a slight decrease in prediction accuracy. Semi-supervised learning on CIFAR-100 dataset involves splitting the training set into labeled, unlabeled, and validation sets. The constraint in BID27 focuses on probabilities of groups of classes being either very high or very low. It consists of 20 groups and is used to compare different approaches using Wide Residual Network BID28 as the network architecture. The constraint restricts the probability distribution, making other methods inapplicable. Our approach outperforms existing works in predicting accuracy on the test set. In an unsupervised setting, we train a Multilayer Perceptron to predict the minimum distance from a source to every node in an unweighted graph. The logical constraint is that the minimum distance is a function with properties like the triangle inequality. Results for queries show the number of completed instances, average running time, and average running time of successful runs. Our approach outperforms existing works in predicting accuracy on the test set by training a model in an unsupervised fashion with the DL2 loss. Random graphs with 15 vertices are generated and split into training, validation, and test sets. A supervised model with mean squared error loss is also trained, but our approach achieves similar results without using any labels. DL2's loss guides the network to satisfy complex constraints, evaluated on querying tasks in TensorFlow with various image datasets and classifiers. Our approach outperforms existing works by training a model in an unsupervised fashion with the DL2 loss. Random graphs are split into training, validation, and test sets. The success of queries depends on the dataset, with some queries being successful for all datasets except GTSBR. Discriminators trained against real images or images created by a generator may explain the results. The DL2 system presented is effective in training and querying neural networks. It supports an expressive logical fragment and provides translation rules into a differentiable loss. The system handles input constraints through PGD to make training tractable and introduces a declarative language for querying networks. Experimental results show that DL2 performs well in both training and querying tasks, with successful executions terminating quickly and scaling well to large networks like ImageNet. The DL2 system is effective in training and querying neural networks, with experimental results showing its success. The proof for the if direction of Theorem 1 is given by induction on formula structure, showing how formulas are satisfied based on atomic constraints and logical operations. The proof for Lemma 1 is based on induction on formula structure, showing how formulas are satisfied by atomic constraints and logical operations. Loss is bounded for every formula \u03d5, ensuring it cannot be arbitrarily large. The proof for Lemma 1 is based on induction on formula structure, showing how formulas are satisfied by atomic constraints and logical operations. Loss is bounded for every formula \u03d5, ensuring it cannot be arbitrarily large. In the proof, inequalities are added and multiplied to derive a conclusion, with an example formula provided for illustration. The inductive proof shows that assignments with loss greater than \u03b4(\u03be) = 2\u03be do not satisfy the formula. Different methods translate logical constraints into loss, with some being non-differentiable. Probabilistic soft logic uses differentiable loss between [0, 1], but finding satisfying assignments with gradient methods can be challenging. The toy example illustrates the translation of logical constraints into loss functions. Our approach optimizes the loss compilation for L-BFGS-B by extracting box constraints and compiling the remaining constraints for easier optimization. The approach optimizes loss compilation for L-BFGS-B by extracting box constraints and passing them to the solver to find the minimum loss. This \"shifting\" excludes a dominant part of \u03d5 from the loss, making it easier to optimize. Excluding atomic constraints speeds up the solution process, especially for larger inputs like ImageNet. For supervised learning experiments, the implementation details include using a batch size of 128, Adam optimizer with a learning rate of 0.0001, and data augmentation with random cropping and horizontal flipping for CIFAR-10. Segment constraints involve embedding images in a 40-dimensional space using PCA for linear interpolation. For CIFAR-10 experiments, ResNet-18 BID9 was used, while MNIST and FASHION experiments utilized a CNN with specific layer dimensions. All methods for semi-supervised learning employed the same Wide Residual Network model with depth 28 and widening factor 10. For the experiment, a Wide Residual Network model with depth 28 and widening factor 10 was used. The neural network was optimized using Adam with a learning rate of 0.001. A weighting factor of \u03bb = 0.6 was used for DL2 loss. The encoding for the semantic loss experiment followed BID27. Rule distillation did not support the constraint, so an approximation was used. The experiment utilized Wide Residual Networks with 28 layers and a mixing factor of \u03c0 t = max(0, 1.0 \u2212 0.97 t). Other constants used were C = 1 and \u03bb = 1. The model used in the experiment was a Wide Residual Network with 28 layers and a widening factor of 10. The neural network had N*N input neurons, three hidden layers with 1000 neurons each, and an output layer of N neurons, where N is the number of vertices in the graph (15 in this case). The network used ReLU activations, dropout of 0.3 after each hidden layer, and was optimized using Adam with a learning rate of 0.0001. The experiments focused on investigating scalability and run-time behavior of DL. The experiments focused on investigating scalability and run-time behavior of DL. Experiment I studied run-time behavior in the number of variables, showing constant run-time for up to 2^13 variables and linear run-time afterwards. Experiment II examined the impact of opposing constraints. Experiment III: Scaling in the number of constraints was conducted to study the scaling of DL2 in the number of constraints. The query looked for an adversarial perturbation to a given image to be classified as a specific class. The found perturbation and resulting image were returned. The query returns the perturbation and resulting image, with constraints on rows and columns enforced. The perturbation vector values must increase from left to right."
}