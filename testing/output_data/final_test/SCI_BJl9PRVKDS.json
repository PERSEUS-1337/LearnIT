{
    "title": "BJl9PRVKDS",
    "content": "Despite their popularity and successes, deep neural networks are poorly understood theoretically and treated as 'black box' systems. A functional view of these networks provides a new lens to understand them, allowing for theoretical or experimental probing of properties such as the effect of standard initializations, the value of depth, the loss surface, and generalization origins. Generalization results from smoothness of the functional approximation and a flat initial approximation, which increases with the number of units. This explains why massively overparamaterized networks continue to generalize well. The properties of deep learning, including the importance of depth and the high generalization performance of overparameterized networks, remain mysterious. Depth is critical for success, with maximum expressivity growing exponentially. However, the value of depth may not lie solely in increasing expressivity. The success of deep learning is also attributed to its surprisingly high generalization performance. Training deep networks with excess capacity leads to improved generalization performance, even in massively overparameterized networks. Taking a functional view, focusing on shallow and deep fully connected univariate ReLU networks, reveals a Continuous Piecewise Linear approximation to the target function. Theoretical results for shallow networks are provided, showing a decrease in generalization error with overparameterization. Our approach, related to previous work, characterizes parameterization and generalization in shallow and deep networks. We use small widths and functional parameterization to measure properties like smoothness. Unlike other works, we focus on the empirical number of pieces in example tasks rather than theoretical bounds. Previous research has hinted at the importance of small function. The main contribution of this work is the functional perspective of initialization in neural networks, showing that parameters determine breakpoints and delta-slopes in the CPWL reparameterization. The distribution of delta-slopes becomes more concentrated with depth, leading to flatter approximations, while the breakpoint distribution widens, allowing deeper networks to better approximate a broader range of inputs. Depth is valued for optimization rather than expressivity. Depth in neural networks is valued for optimization rather than expressivity. Generalization in overparametrized networks is attributed to factors such as flat initialization, curvature-based parametrization, and the role of gradient descent in regularization. The impact of breakpoints and delta-slopes in neural networks helps regularize the approximating function, leading to smoother approximations and better generalization. Understanding the relationship between CPWL parameters and neural network parameters involves transforming from weights and biases to CPWL parametrizations. The CPWL parametrization in neural networks involves breakpoints and delta-slopes to improve approximation and generalization. The breakpoints congregate in areas of high curvature, while delta-slopes implement the needed curvature. As the number of pieces increases, the approximation improves, and the delta-slopes approach the true curvature of the function. The BDSO parametrization of a ReLU NN is related to but different from a traditional spline parametrization. The BDSO parametrization of a ReLU NN differs from a traditional spline parametrization by lacking the base polynomial and having two possible breakpoint orientations. Adding the base polynomial yields a ReLU ResNet parametrization. The second parametrization involves breakpoints and slopes for piecewise linear functions in neural networks. The BDSO parameterization of a ReLU NN introduces breakpoints induced by neurons, enabling easier analysis of neural nets' initialization, loss surface, and training dynamics. This approach eliminates degeneracies in parameterization and expresses the loss solely through BDSO parameters, similar to a minimum sufficient statistic in exponential family models. Recent work has also focused on analyzing function space in a similar manner. Recent work has focused on analyzing function space in deep learning, particularly studying random initializations like independent Gaussian and Uniform. These common initializations lead to flat functions that become even flatter with increasing depth. The induced distributions of function space parameters under these initializations are discussed, allowing for the derivation of marginal and conditional distributions. Corollary 1 discusses the implications of using independent Gaussian and Uniform initializations in deep learning. It shows that the breakpoint density drops quickly away from the origin for common initializations, making it more difficult to fit functions with significant curvature far from the origin. Training becomes easier when the initial breakpoint distribution matches the function curvature, as shown by training a shallow ReLU NN. During training, breakpoint distributions adjust to better match the underlying function curvature. During training, breakpoint distributions adjust to match function curvature, with a potential benefit in data-dependent initialization. Gaussian and Glorot initializations are compared for their effect on initial function smoothness, measured by roughness metric. He initialization has lower roughness tail probability, while Glorot initialization has a different formula. In He initialization, the tail probability is determined by E[\u03c1 0] = 4, while in Glorot initialization, it is different. The distribution of initial function roughness tightens around its mean as width H increases. This smoothness affects implicit regularization/generalization in deep networks. Recent works analyze random initialization in deep networks, focusing on different aspects compared to previous studies. The study focuses on the non-uniform density of breakpoints in neural networks, considering mean squared error loss and the number of possible data partitions. The research requires complex mathematical tools due to the finite width case, unlike previous studies using the Central Limit Theorem. The study explores the non-uniform density of breakpoints in neural networks for mean squared error loss and data partition possibilities. A piecewise-OLS solution is found through simulation, indicating a polynomial growth in critical points. Lonely partitions lead to infinitely many global minima parameter settings. In the overparametrized regime, with uniformly spaced breakpoints and data points, lonely partitions are highly probable. This leads to a lower bound on the total number of global minima, making optimization easier. The function space explanation for overparametrization is simpler and more transparent compared to weight space explanations. Overparameterization leads to a flatter initial function approximation, making optimization easier. Neural networks exhibit high generalization performance due to implicit regularization, independent of loss function. This regularization is a result of overparameterization, increasing flatness of initialization and non-locality of delta-slope parameters. The overparametrization regime in neural networks leads to a flatter initial function approximation, making optimization easier. The vector of delta-slopes with orientation evolves according to specific equations, ultimately resulting in a regularization effect that enhances generalization performance. The regularization effect in neural networks leads to smoother function approximation, enhancing generalization performance by introducing unexpected parameters that induce smoothness in the data gap. The smoothness-inducing terms in neural networks arise from the smoothness of the initial function and the active half space structure, due to the discrete curvature-based parameterization. Experiments confirm the theory, showing breakpoints concentrated around the origin, impacting optimization difficulty. The study found that breakpoints are concentrated around the origin, leading to flatter initial functions as depth increases. Standard initializations struggle to fit functions with significant curvature away from the origin, indicating the importance of breakpoints for modeling curvature. Breakpoints do not migrate far from their initial location, resulting in suboptimal fits. The study shows that breakpoint distributions change during training to better match ground truth curvature. By using a data-dependent initialization sampling breakpoints uniformly over the data range, it was proven that breakpoint density is causally responsible for fitting accuracy. Uniform breakpoint density rescues bad fits in cases with significant curvature, confirming the theory and suggesting a potential useful initialization strategy for high dimensions. The suboptimality of Gradient Descent raises questions about its success conditions. Empirical observations suggest that neural nets need to be overparameterized for good training performance. The theory explains that sampling many breakpoints everywhere allows fitting an arbitrary function, but many breakpoints may not add value. Testing this explanation and understanding the root causes are essential. The study focuses on comparing Gradient Descent (GD) with Dynamic Programming (DP) and Greedy Merge (GM) algorithms for fitting piecewise linear (PWL) functions using a shallow ReLU network with univariate input. The comparison is based on training loss and the number of pieces/hidden units, rather than total trainable parameters. This approach allows for a direct evaluation of the algorithms across different target function classes. The study compares Gradient Descent (GD) with Dynamic Programming (DP) and Greedy Merge (GM) algorithms for fitting piecewise linear functions using a shallow ReLU network. DP algorithm quickly reduces training error with fewer pieces, while GM algorithm requires slightly more pieces but less computational power. Variants of GD require more pieces to reduce error, with Adam outperforming BatchNorm SGD and Vanilla GD. GD is inefficient with respect to parameters, requiring significantly more for similar performance. The depth of a network affects its expressivity, with deep learning allowing for more powerful function approximation compared to a shallow network. While theoretically depth increases expressivity exponentially, recent work has challenged this notion. Recent work has challenged the notion that the expressivity of deep networks scales exponentially with depth. An experiment showed that the number of pieces in the CPWL function approximation of a deep ReLU network does not exponentially increase with depth. Depth only has a weak effect overall, indicating that more research is needed to understand its impact on the number and variability of pieces. These findings support previous research by Hanin & Rolnick (2019). In deeper layers of a ReLU network, the function approximation allows for a varying number of breakpoints, which strongly correlates with the number of units at initialization. Depth aids in optimization by enabling the creation, annihilation, and mobility of breakpoints. The CPWL function approximation develops in each layer during learning, accumulating breakpoints in areas of higher curvature in the training data. Deeper layers help optimize this process, ensuring a good fit for the target function. The deeper layers of a network aid in optimization by allowing breakpoints more mobility and the ability to create and annihilate breakpoints. Deeper units can induce multiple breakpoints, providing the network with more flexibility to optimize. Breakpoint mobility in deeper layers allows for greater movement compared to shallow layers. Experimental comparisons show variations in velocity and number of induced breakpoints across layers of a deeper network. The number of induced breakpoints varies between layers of a deeper network, with deeper layers showing more changes and higher velocity compared to shallow layers. This suggests that later layers play a significant role in optimization and breakpoint placement, even though they may not contribute as strongly to expressivity. Breakpoints induced by a layer are present in all deeper layers, making the functional approximations more complex with depth. The roughness of basis functions in deeper layers is lower than in shallow layers at initialization, but this changes as the network learns. The roughness of basis functions in deeper layers is lower at initialization but increases as the network learns. The specific CPWL initialization in shallow and deep ReLU networks influences this, with a 'spiky' initialization showing causal evidence. Testing this with standard and 'spiky' initialization methods in shallow ReLU networks reveals differences in training outcomes. In a 1D input space, a small number of training data points are needed to simulate sparsity caused by high dimensional input. Both networks fit training data well but 'spiky' initialization leads to higher generalization error. 'Spiky' features are preserved in final function approximation. For deep ReLU networks, training for 'spiky' initialization is challenging. The 'spiky' initialization has similar training performance to standard initialization but worse generalization performance. Generalization is not guaranteed by gradient descent, but by flat initializations preserved during training. 'Spiky' initializations have higher curvature preserved by gradient descent, promoting smooth approximation for better generalization. The experiment explores how smoothness depends on the number of units in the presence of large gaps in training data. The experiment investigated the impact of width and weight variance on the roughness of CPWL approximation in data gaps. Results showed that roughness decreases with width and increases with weight variance, supporting the theory. A spiky initialization led to increased roughness at convergence, suggesting that roughness in data gaps can be 'remembered' from initialization. Additionally, a higher number of pieces spread out the curvature work, resulting in smaller overall roughness. In this paper, the experiments show that smooth, flat initialization plays a significant role in implicit regularization in fully connected ReLU nets. Increasing overparameterization leads to better generalization. The value of depth in deep nets is more about learnability than expressivity, enabling better quality solutions. Smooth initial approximation encourages a smoother final solution, improving generalization. Existing initializations used in practice start with smooth initial approximations. Analyzing the loss surface for a ReLU net in function space reveals that increasing width relative to training data size leads to global minima. Implicit regularization may be due to a hidden 2nd order differential equation underlying the ReLU parameterization. New tools, architectures, and algorithms can be developed based on this functional lens. The analysis suggests that bad local minima occur when breakpoints are trapped, prompting the need for new learning algorithms to make global moves in the parameterization. Algorithms are needed to make global moves in the parameterization to avoid local minima. Experimental details include training a deep network with 4 hidden layers of width 8 on a function over the interval [-2,2]. The 'spiky' initialization was compared before and after training, with roughness measured every 50 epochs. The deep version was pre-trained for 10,000 epochs to fit the CPWL before gradient descent training. The models were trained for 20,000 epochs with a learning rate of 1e-4. Training data consisted of 20 random points in the range [-2,2], while testing data was spaced uniformly at every \u2206x = .01. The shallow model had no pre-training and trained for 20,000 epochs. Despite different learning rates, both models showed similar training loss curves and final training loss values. The final training loss values for different functions ranged from .94 to 1.02. Various functions were used such as sin(x), arctan(x), a sawtooth function, and 4 peaks function. A deep network with 6 layers and 5 hidden layers was used for training with a learning rate of 5e-5 over 25,000 epochs. The target functions tested included sin(\u03c0x), a 5-piece polynomial, a sawtooth function, arctan(x), exp(x), and 1/9 x^2. Each value in the table was the average of 5 trials. The study used a deep 6-layer network with 5 hidden layers of width 8 to train on 'smooth' and 'sharp' functions over the interval x \u2208 [\u22123, 3]. Breakpoints were calculated every 50 epochs, and their velocity was normalized to the velocity of the first layer. Different function classes were trained on a depth 1 or 4 ReLU network with 500 total units. Initial and final breakpoint distributions were compared with the underlying curvature of the ground truth function using a cubic spline fit. The study used a deep 6-layer network to train on 'smooth' and 'sharp' functions over the interval x \u2208 [\u22123, 3]. Breakpoints were calculated every 50 epochs and compared with the underlying curvature of the ground truth function using a cubic spline fit. The breakpoint densities moved over training to become more correlated with the function's curvature, especially with depth. In simple functions, a failure case emerged where there was no real change in correlation over training due to the function training almost instantaneously in the overparameterized network. The second hidden layer neurons receive input from a CPWL function and pass it through a ReLU. Breakpoints in the function are now dynamic and can change during training. Understanding breakpoints in deep networks is crucial before exploring how gradient descent optimizes them to model curvature in the data. The notion of breakpoints in a deep network is explored, where \u03b2 values determine activation. Equations (2) and (3) are derived using cubic splines. Lemma 1 discusses initialization densities, and a transformation is used to find the density of (\u03b2 i, \u00b5 i). Theorem 1(a) is then expanded for a fully connected ReLU neural net with scalar input and output. Theorem 1(a) discusses a fully connected ReLU neural net with scalar input and output, and a single hidden layer of width H. It considers weights and biases initialized randomly according to a zero-mean Gaussian or Uniform distribution. Under an independent Gaussian initialization, the Meijer G-function and modified Bessel function are used to derive the desired density function. Gradshteyn & Ryzhik (2015) is referenced for further verification. Theorem 1(b) discusses a fully connected ReLU neural net with scalar input and output, a single hidden layer of width H, and weights and biases initialized randomly. Under an independent Uniform initialization, the distribution of the parameters is analyzed using symmetric triangular distributions. The proof is completed by computing the time derivatives of the BDSO parameters and using the loss gradients of the loss with respect to the NN parameters."
}