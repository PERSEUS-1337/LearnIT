{
    "title": "SkxXg2C5FX",
    "content": "Recent literature suggests that averaged word vectors outperform many deep learning methods on semantic textual similarity tasks. A novel fuzzy bag-of-words (FBoW) representation for text is proposed, containing all words in the vocabulary with different degrees of membership based on similarities between word vectors. Max-pooled word vectors are compared using fuzzy Jaccard index rather than cosine similarity. DynaMax is introduced as an unsupervised similarity measure that dynamically extracts and max-pools features depending on the sentence pair. DynaMax is an efficient method for extracting and max-pooling features based on sentence pairs, outperforming current baselines on STS tasks. It competes with supervised word vectors for optimizing cosine similarity, addressing the challenge of determining semantic similarity between sentences. The Bag-of-Words representation remains popular but has been overshadowed by neural network representations in recent years. The Bag-of-Words representation has been overshadowed by neural network representations in recent years, with BID5 showing that averaged word vectors weighted with the SIF scheme and PCA outperform deep representations. Additionally, BID59 and BID58 demonstrated that supervised word vectors trained on paraphrases achieve state-of-the-art results for Semantic Textual Similarity tasks. This has inspired the proposal of a novel fuzzy Bag-of-Words (FBoW) representation for text. The fuzzy Bag-of-Words (FBoW) representation for text incorporates all words in the vocabulary simultaneously with varying degrees of membership based on word vector similarities. Max-pooled word vectors, a special case of FBoW, outperform averaging on standard benchmarks when trained unsupervised. The fuzzy Jaccard index is proposed as a more suitable alternative to cosine similarity for comparing these representations. Max-pooling, commonly used in NLP, has been successful in extracting features in supervised systems. To the best of our knowledge, this work is the first to explore max-pooling of pre-trained word embeddings independently and provide theoretical insights. DynaMax, an unsupervised similarity measure, dynamically extracts and max-pools features for sentence pairs, outperforming averaged word vectors on STS tasks. It even matches the performance of supervised BID58 vectors in cosine similarity, despite being unrelated to that objective. This establishes DynaMax as a strong baseline for future semantic similarity algorithms to surpass. Additionally, significance analysis is conducted. The recent literature on semantic textual similarity (STS) tends to use inappropriate parametric tests or omit significance analysis. In contrast, this study employs nonparametric approaches, such as bias-corrected and accelerated (BCa) bootstrap confidence intervals, to analyze the performance difference between two systems. The bag-of-words (BoW) model is a popular baseline in machine learning, natural language processing, and information retrieval communities. This study introduces a methodology for conducting thorough significance testing on STS benchmarks, which has not been done before. The recent literature on semantic textual similarity (STS) uses nonparametric approaches for significance analysis. The study introduces a methodology for thorough testing on STS benchmarks. The curr_chunk discusses the limitations of the bag-of-words (BoW) model and introduces a simpler set-of-words (SoW) model for sentence comparison using set similarity measures (SSMs). It highlights the issue of sets with no shared elements having a similarity of 0 and proposes a solution for better sentence comparison. The curr_chunk explains the concept of fuzzy sets, where words in a set have varying degrees of membership. By using a similarity function, singleton sets like {'cat'} can be converted into fuzzy sets with membership degrees for each word in the vocabulary. This approach generalizes classical sets and allows for more nuanced comparisons between words. The concept of fuzzy sets is explained, where words in a set have varying degrees of membership. The fuzzy union operation is used to combine fuzzy sets, with the membership degree of a word determined as the maximum among the sets. This approach allows for more nuanced comparisons between words. The membership function assigns degrees of membership to words in a fuzzy set. Words like 'cat' and 'dog' have similar membership functions due to their semantic similarity. Fuzzy set theory extends classical set theory by allowing elements to have degrees of membership. Fuzzy set similarity measures can be computed using basic tools from fuzzy set theory. A universe is a set of all possible terms in a domain. A fuzzy set is defined by a membership function. Fuzzy set similarity measures can be computed using basic tools from fuzzy set theory. The union and intersection of fuzzy sets are defined using max and min operations. Fuzzy set theory offers various choices for union and intersection operations, but only the max-min pair ensures idempotent operations like in classical set theory. The cardinality of a fuzzy set is defined as the sum of membership values. Membership functions can be expert-designed or learned from data, with text membership functions generated from word embeddings. Bag-of-words can be represented as row vectors with one-hot encodings for each term in the vocabulary. The BoW vector for a sentence is formed by counting the words in the sentence. To create a fuzzy BoW representation, word vectors are converted into membership vectors through 'word fuzzification'. This process involves converting one-hot vectors into word embeddings. The process involves converting one-hot vectors into word embeddings, computing similarities between words, and combining them into a sentence membership vector using max-pooling instead of averaging. The process involves converting one-hot vectors into word embeddings, computing similarities between words, and combining them into a sentence membership vector using max-pooling instead of averaging. In NLP, this is known as max-over-time pooling. The fuzzy bag-of-words (FBoW) is created by converting classical BoW into fuzzy BoW through 'sentence fuzzification'. Fuzzy BoW vectors can be compared using cosine similarity or fuzzy set similarity measures. The fuzzy bag-of-words (FBoW) is created by converting classical BoW into fuzzy BoW through 'sentence fuzzification'. FBoW contains similarities to all words in the vocabulary and has the same dimensionality as the original BoW. To reduce sparsity, a matrix U with fewer rows than W can be chosen, such as the top principal axes of W or centroids from clustering. Another option is to choose U = I, where I is a non-parametric solution. The fuzzy bag-of-words (FBoW) is created by converting classical BoW into fuzzy BoW through 'sentence fuzzification'. A radical non-parametric solution is to choose U = I, where I is the identity matrix. The word fuzzifier reduces to a word embedding lookup, and the sentence fuzzifier max-pools all word embeddings in the sentence. Max-pooled word vectors are a special case of fuzzy BoW and provide a stronger baseline for semantic textual similarity than averaged word vectors with cosine similarity. The fuzzy Jaccard index works for max-pooled word vectors but fails for averaged word vectors, validating the connection between fuzzy BoW representations and max-pooling operation. The fuzzy bag-of-words (FBoW) is transformed into fuzzy BoW by projecting word embeddings on a subspace of R d spanned by matrix U and max-pooling the features. To compare two sentences for semantic similarity, stacking word embeddings into matrix U is sufficient. This approach, called Dynamic Max or DynaMax, dynamically changes U depending on the sentence pair. The fuzzy set-of-words (FSoW) can be built where word counts are binary. The fuzzy bag-of-words (FBoW) is transformed into fuzzy BoW by projecting word embeddings on a subspace of R d spanned by matrix U and max-pooling the features. FSoW is implemented in Algorithm 1 and in all experiments, using dot product for membership functions. Methods that enhance bag-of-words by incorporating word similarity are relevant. The standard Vector Space Model treats all words as equally different, while BID50 proposed the 'soft cosine measure' for semantic similarity between sentences. The BoW model treats all words equally different. BID50 introduced the 'soft cosine measure' to address this issue by creating a non-orthogonal basis where cosine similarity between basis vectors corresponds to similarity between words. They compared different combinations of fuzzy BoW representation (averaged, max-pooled, or DynaMax) and similarity measures (cosine or Jaccard). Max-pooled with cosine generally outperforms other methods, except for max-pooled and DynaMax with Jaccard. In comparison to max-pooled and DynaMax with Jaccard, STS13's performance may have been impacted by the unavailability of the SMT subtask dataset. BID35 considers L1-normalised bags-of-words as probability distributions over words and introduces the Word Mover's Distance as a special case of the Earth Mover's Distance. BID65 proposes a fuzzy BoW alternative. BID65 proposed a fuzzy BoW alternative using clipped cosine similarity for word fuzzification and sum for sentence fuzzification. However, the use of dot product in our approach led to significant improvements on benchmarks. Additionally, we argue that max-pooling is a better choice for sentence fuzzification as it corresponds to fuzzy union. The soft cardinality framework proposed by BID24 BID32 BID0 generalizes classical set cardinality by considering the similarity of elements within a set. This approach differs from traditional set theory by focusing on the contribution of elements to the overall cardinality. Experimental evaluations of the proposed similarity measures were conducted on established STS tasks as part of the SemEval shared task series 2012-2016. The STS benchmarks in the SemEval shared task series 2012-2016 compare BoW-based methods to fuzzy Jaccard similarity. Max-pooled word vectors with Jaccard and DynaMax outperform other methods, showing significant improvement. The STS tasks aim to measure how well semantic similarity scores correlate with human judgements. The mean Pearson correlation between system and human scores is reported across all subtasks each year. The implementation utilizes the SentEval toolkit. Our implementation utilizes the SentEval toolkit and various publicly available word embeddings such as GloVe, fastText, and word2vec. Word frequencies were estimated from an English Wikipedia dump, and word weights were calculated using a similar approach as in previous work. Techniques used are fully unsupervised, and adjustments were made for the STS'13 SMT dataset no longer being publicly available. The experiments validated insights and derivations in Section 2, showing that max-pooled word vectors outperform averaged word vectors. Max-pooled vectors with fuzzy Jaccard similarity perform better than those with cosine similarity. Averaged vectors with fuzzy Jaccard similarity fail due to fuzzy set theory. DynaMax performs the best across all tasks. DynaMax outperforms related methods in sentence representation tasks, especially with fuzzy Jaccard similarity. Comparisons are made with classical Jaccard index and other popular methods like BoW with ELMo embeddings. Sophisticated methods do not excel in unsupervised STS tasks. Methods of computing sentence representations do not perform well on unsupervised STS tasks compared to simple Bag of Words (BoW) methods with high-quality word vectors and the appropriate similarity metric. PNMT embeddings are currently the state-of-the-art on STS tasks, while DynaMax shows equivalent performance despite being unrelated to the training objective of the vectors. Another high-performing baseline method was proposed by BID5. The DynaMax method is competitive with other approaches like PNMT embeddings and a baseline method proposed by BID5. Ablation studies show that different components of DynaMax contribute to its strong performance. Significance testing using bias-corrected and accelerated bootstrap confidence intervals is conducted to analyze the performance difference between algorithms on STS benchmarks. This is the first attempt to study statistical significance on STS benchmarks using non-parametric analysis. In this work, word embeddings are combined with classic Bag of Words (BoW) representations using fuzzy set theory. Max-pooled word vectors are shown to be a special case of FBoW, compared using the fuzzy Jaccard index instead of cosine similarity. The DynaMax algorithm, projecting word vectors onto a dynamically generated subspace before max-pooling, outperforms averaged word vectors on STS tasks. Both max-pooled vectors and DynaMax serve as strong baselines for further studies. Both max-pooled vectors and DynaMax are strong baselines for studying sentence representations and can be applied beyond NLP and word embeddings. The word fuzzification step involves obtaining membership values for a word through a similarity function between the word embedding and the universe matrix. The choice of a similarity function that yields values in R is explained, highlighting the benefits of experimenting with similarity metrics based on word meanings rather than complex sentence representations. The choice between using values in R or [0, 1] for membership values is not crucial mathematically. Fuzzy Bag of Words (BoW) unintentionally does not contain negative membership values due to max-pooling with a zero vector. To ensure a range of [0, 1], the cosine similarity is clipped to max(0, cos(w, u(j))). This is equivalent to normalizing word vectors. After comparing the performance of DynaMax and avg-cos with normalised word vectors in TAB3, it is evident that DynaMax still outperforms avg-cos significantly. However, normalisation negatively impacts both approaches and should generally be avoided. This is because normalisation makes all words equally important, which is not ideal as the length of word vectors is correlated with word importance. Additionally, in TAB4, it is shown that fuzzy versions of set similarity measures like Jaccard, OtsukaOchiai, and S\u00f8rensen-Dice coefficients have almost identical performance. The fuzzy versions of coefficients have similar performance, confirming results are not specific to the Jaccard index. Ablation studies show the dynamic universe, max-pooling operation, and fuzzy Jaccard index contribute to DynaMax-Jaccard's strong performance. FastText yielded the best results for both DynaMax and the baseline. A series of ablation studies were conducted to isolate the contribution of each component, with results presented in Table 5. The ablation study results in Table 5 show that the dynamic universe, fuzzy Jaccard index, and max-pooling operation are crucial components of DynaMax. Max-pooling outperforms other pooling operations, and the fuzzy Jaccard index surpasses cosine similarity on most benchmarks. The importance of these components is highlighted, with max-pooling being the most significant factor. The STS benchmarks aim to measure semantic similarity scores against human judgements, with detailed results provided for all 24 subtasks in Section 4. Our approach involves analyzing the results and significance of 24 STS subtasks. We compare human scores H, system scores A, and baseline system scores B using Pearson correlation coefficients. Nonparametric resampling-based approaches, like bootstrap, are used due to the non-normal distributions of human scores in STS tasks. Nonparametric resampling-based approaches, such as bootstrap, offer an alternative to parametric tests when the distribution of the test statistic is unknown. The bootstrap method involves drawing multiple samples from the actual sample to approximate the distribution of the statistic. Bias-corrected and accelerated (BCa) 95% confidence intervals are constructed using this information. This advanced method accounts for bias and skewness in the bootstrapped distributions. The study utilized bias-corrected and accelerated (BCa) 95% confidence intervals to account for bias and skewness in the bootstrapped distributions. Results showed that out of 72 experiments, the approach significantly outperformed the baseline in 77.8% and underperformed in only 1.39%. The analysis aims to provide a foundation for conducting significance testing on current and future STS benchmarks."
}