{
    "title": "HkfwpiA9KX",
    "content": "Skills learned through reinforcement learning often struggle to generalize across tasks, requiring re-training for new tasks. A framework combining formal methods with RL allows for specifying complex tasks with logical expressions and creating new skills without additional exploration. The technique is theoretically supported and tested on a grid world simulation. In robotic manipulation tasks, policies learned through reinforcement learning aim to maximize rewards but are difficult to transfer. Skill composition involves creating new skills from existing ones without additional learning. Temporal logic is used in software verification and control policy synthesis. Some approaches combine temporal logic with Q-learning for learning policies in discrete spaces. The distinction between skill composition and multi-task learning/meta-learning is made. In this work, skill composition is focused on with policies learned using automata guided reinforcement learning. The task specification language used is syntactically co-safe truncated linear temporal logic (scTLTL). Skill composition is achieved by taking the product of finite state automata (FSA). Our method for skill composition is based on graph manipulation of finite state automata (FSA), resulting in a more transparent outcome. Unlike previous approaches, we do not impose constraints on policy representation or problem class. Validation is done in simulation and on a Baxter robot. Other methods combine value functions learned with different rewards, while our approach constructs a composite policy using the Boltzmann distribution. Our method for skill composition involves representing the reward function as a linear combination of state-action features. By incorporating temporal logic, we can compose tasks of greater logical complexity with higher interpretability. The composite policy we develop is optimal in both task compositions where it maximizes the average reward of individual tasks and the (soft) maximum of the reward of constituent tasks. In entropy-regularized reinforcement learning, the goal is to maximize an objective function involving a stochastic policy \u03c0, entropy, and a temperature parameter \u03b1. The soft Q-learning algorithm optimizes this objective to find a policy. The soft Q-learning algorithm introduced by BID12 optimizes the objective function involving a stochastic policy \u03c0, entropy, and a temperature parameter \u03b1 to find a policy represented by an energy-based model. The state-action value function (Q-function) following \u03c0 is defined for a set of tasks indexed by i, each defined by an MDP M i with a different reward function. The optimal entropy-regularized Q-function Q \u03c0 i \u03b1 is derived for these tasks. Authors of BID21 provide results for a new task defined by a reward function r, and the optimal Q-function is determined based on nonnegative weights w. Corollary 1 states the optimal Q-function for a specific objective. In the low temperature limit, the maximum of the optimal entropy-regularized Q-functions approaches the standard optimal Q-function. Tasks are specified with syntactically co-safe Truncated Linear Temporal Logic (scTLTL) derived from truncated linear temporal logic (TLTL). The syntax of scTLTL includes Boolean constants, predicates over MDP states, Boolean connectives, and temporal operators. State trajectories are denoted as sequences of states from time t to t + k. The Boolean and quantitative semantics of scTLTL are defined for trajectories satisfying formulas, with robustness as a measure of satisfaction. An FSA corresponding to a scTLTL formula is defined as a tuple with automaton states, input alphabet, initial state, and final states. The FSA Augmented MDP is defined with transition probabilities and rewards, where a policy \u03c0 \u03c6 satisfies a TLTL formula \u03c6 based on robustness evaluation at time step 0. The FSA augmented MDP connects TL specifications with reinforcement learning. A discount factor helps find a policy that satisfies a formula in the least time. The policy learned has implicit knowledge of the FSA. The goal is to obtain optimal policies that satisfy combined formulas \u03c6 \u2227 and \u03c6 \u2228. In this section, a solution is provided for Problem 1, which involves skill composition by constructing a policy that satisfies the conjunction or disjunction of given specifications. This approach allows for breaking down complex tasks into manageable components and combining policies to adapt to changes in task specifications over time. The solution for Problem 1 involves constructing the FSA of \u03c61 \u2227 \u03c62 to synthesize the policy for the combined skill. The FSA of \u03c61 \u2227 \u03c62 is the product automaton of A\u03c61 and A\u03c62, with defined states and transition probabilities. An example is given with formulas \u03c61 = \u2666r \u2227 \u2666g and \u03c62 = \u2666b resulting in the formula \u03c6 = \u2666r \u2227 \u2666g \u2227 \u2666b. Theorem on automata guided skill composition is provided, stating that transitions in the product automaton exist only if corresponding transitions in the individual automata exist. Reward functions and states are defined for the FSA augmented MDPs. The reward functions and states for FSA augmented MDPs are defined. The log-sum-exp of the composite reward is an approximation of the maximum function. The optimal Q-function can be used to construct a policy in both discrete and continuous action spaces. The policy update procedure in actor-critic methods can be applied to extract a policy from the Q-function. The procedure for task compositions remains the same, with the only difference being the task type. The procedure for task compositions in FSA augmented MDPs is the same for both AN D and OR tasks, with the only difference being the termination condition. In Algorithm 1, steps 3 and 4 aim to obtain the optimal policy and Q-function using off-policy actor critic RL algorithm. The composed Q-function is constructed using Theorem 2, and a product replay buffer is created for policy extraction. Step 1 Summarized Text Chunk:\nStep 6 constructs product replay buffer for policy extraction, transforming experiences to include additional automaton states. Step 7 extracts optimal composed policy from Q-function. Evaluation done in 2 environments: 2D grid world for proof of concept and robot manipulation environment. \n\nStep 4 Summarized Text Chunk:\nSteps 6 and 7 involve constructing a replay buffer for policy extraction and extracting the optimal composed policy from the Q-function. Evaluation is conducted in two environments: a 2D grid world for proof of concept and a robot manipulation environment. In a 8x10 grid world, an agent navigates with actions [up, down, left, right, stay]. The agent follows actions with 0.8 probability or chooses randomly with 0.2 probability. Trained on tasks \u03c61 = \u2666r \u2227 \u2666g and \u03c62 = \u2666b, with regions defined by predicates. FSA shown for each task, applying tabular Q-learning on the FSA augmented MDP environment. The agent in an 8x10 grid world follows a discount factor of 0.95, learning rate of 0.1, and an episode horizon of 200 steps. The learned optimal policies are shown in FIG3, with sub-policies represented by \u03c0 \u03c6i (x, y, q \u03c6i ). The composed policy of \u03c6 \u2227 = \u03c6 1 \u2227 \u03c6 2 is able to act optimally in maximizing expected rewards. Following this policy transitions the FSA in FIG1 to satisfy desired tasks. The policy controls the joint velocities of a Baxter robot in a complex manipulation task involving three circular regions. The state space is 22-dimensional, including joint angles, positions of regions, user's hand, and robot's end-effector. State and action spaces are continuous. In a manipulation task involving a Baxter robot, the state space is 22-dimensional with continuous joint angles, positions of regions, user's hand, and robot's end-effector. Predicates are defined to constrain distances and evaluate hand presence. The experiment involves composition tasks with random resets of joint angles, FSA state, plate positions, and hand position. Each episode lasts 100 time-steps at 20 Hz, restarting if the final automaton state is reached. During training, 100 policy and Q-function updates are performed every 5 episodes of exploration. The policy can transfer to the real robot without further fine-tuning. Performance statistics are calculated after each set of policy updates. The composition method achieves higher returns with lower variance in fewer update steps compared to learning with SQL and FSA augmented MDP. Shorter episode lengths indicate faster task accomplishment. The composition method achieves faster task accomplishment with lower variance compared to learning methods. Shorter episode lengths indicate quicker completion of the task. The policy obtained from composition shows a noticeable decrease in average episode length, while learning time is significantly longer. Skill composition uses already collected experience, making policy learning much faster. The mean training time and standard deviation for different tasks are shown, with shorter training times for tasks with higher success rates. Task success is evaluated based on trajectory robustness, with one task failing despite convergence during training. A technique utilizing finite state automata composition is presented for task completion. Our technique utilizes finite state automata to perform deterministic skill composition for optimal composite policies in tasks. The method is effective in grid world simulations and real-world robotic tasks, with future work focusing on adapting it for task-space transfer with arbitrary logical combinations of specifications."
}