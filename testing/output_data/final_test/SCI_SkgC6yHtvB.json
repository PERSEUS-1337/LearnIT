{
    "title": "SkgC6yHtvB",
    "content": "In many computer vision tasks, Euclidean and spherical embeddings are commonly used for tasks like image classification and retrieval. However, hyperbolic embeddings are shown to be a better alternative in practical scenarios. High-dimensional embeddings are widely used in modern computer vision systems, with many utilizing non-linear mappings. Many modern computer vision systems use deep convolutional networks to learn non-linear mappings from images into high-dimensional spaces. The embeddings in the penultimate layer live in Euclidean space, enabling tasks like image classification, retrieval, face recognition, and one-shot learning. In this work, hyperbolic spaces with negative curvature are proposed for learning image embeddings. Hyperbolic network layers are added to computer vision networks to explore their potential benefits. The use of hyperbolic geometry in image embeddings has shown improved performance in computer vision networks. Hyperbolic spaces are motivated by their ability to embed hierarchies with low distortion, which is beneficial for tasks like image classification and person re-identification. Hyperbolic spaces are suitable for embedding hierarchies in natural language processing and computer vision tasks. Hierarchical relations between images are common, such as in image retrieval and classification tasks. Embedding a dataset with composite images into continuous space is akin to embedding a hierarchy. In image datasets with varying quality, hierarchical structures are important. Recent developments allow deep learning models to operate on hyperbolic embeddings, improving tasks like image classification and few-shot learning. Standard architectures can be easily modified to work with hyperbolic spaces. Hyperbolic space, denoted as Hn, is a n-dimensional Riemannian manifold with constant negative sectional curvature. It cannot be isometrically embedded into Euclidean space, but there are well-studied models of hyperbolic geometry. The Poincar\u00e9 ball model is commonly used in NLP works. The Poincar\u00e9 ball model is commonly used in NLP works. It utilizes a conformal factor 1\u2212 x 2 and the Euclidean metric tensor g E = I n. The geodesic distance between two points is defined in this model. The Klein model of hyperbolic space is used to define the hyperbolic average, which takes a simple form in Klein coordinates. In hyperbolic geometry, all points in the Poincar\u00e9 ball are equivalent. The models discussed use a hybrid approach, with most layers using Euclidean operators and only the final layers operating in hyperbolic geometry. The distance to the origin in these models serves as a measure of uncertainty, with more generic objects closer to the origin and more specific objects towards the boundary. This concept is useful in natural language processing tasks. Hyperbolic embeddings in natural language processing have been successful, motivated by the ability to embed hierarchies with low distortion. Research includes embedding trees in hyperbolic space and imposing hyperbolic structure on neural network activations. Few-shot learning, focusing on model generalization, has attracted attention with metric learning methods like Siamese Networks, Matching Networks, and Prototypical Networks. Person re-identification involves matching pedestrian images from different surveillance cameras using pairwise models to determine similarity scores for classification. Current models in few-shot learning include Siamese Networks, Matching Networks, and Prototypical Networks, but there is a lack of extension to hyperbolic space in these models. The work focuses on learning a mapping of pedestrian images to the Euclidean descriptor space using verification loss functions. Hyperbolic neural networks are utilized, extending typical neural network operations to hyperbolic space. The paper introduces hyperbolic versions of feedforward networks, multinomial logistic regression, and recurrent neural networks. The paper discusses hyperbolic neural networks and the use of an additional hyperparameter c to balance between hyperbolic and Euclidean geometries. The operation of averaging feature vectors in hyperbolic spaces is called the Einstein midpoint. The Einstein midpoint in hyperbolic spaces is defined in Klein coordinates, with transition formulas between Poincar\u00e9 and Klein models. Practical implementation includes clipping by norm for stability during training. In hyperbolic neural networks, parameters are mapped to hyperbolic counterparts using the exponential map. The radius for embedding in the Poincar\u00e9 disk is determined by the curvature parameter c. Gromov's \u03b4-hyperbolicity is closely related to the disk radius. The \u03b4-hyperbolicity value determines the Poincar\u00e9 disk radius for accurate embedding in metric spaces. It is defined by the Gromov product for points x, y, z \u2208 X, with a four-point condition for all points x, y, z, w \u2208 X. The \u03b4 value can be computed computationally using pairwise Gromov products. The \u03b4 value is determined by the largest coefficient in the matrix (A \u2297 A) \u2212 A, computed using pairwise Gromov products. The \u03b4-hyperbolicity of a dataset can be estimated by computing the scale-invariant metric \u03b4 rel (X) = 2\u03b4(X) diam(X), indicating how close the dataset is to a perfect hyperbolic space. The \u03b4-hyperbolicity of datasets is determined by the largest coefficient in the matrix (A \u2297 A) \u2212 A, indicating how close the dataset is to a perfect hyperbolic space. \u03b4 rel values for various datasets were computed using the Euclidean distance between features extracted with VGG16. Results show high hyperbolicity in image datasets, supporting the hypothesis. Embeddings from a hyperbolic neural network trained for the MNIST classification task show significant differences, with Omniglot images closer to the origin. Further experiments focus on few-shot classification. In further experiments, the focus shifts to few-shot classification and person re-identification tasks using datasets like Omniglot, MiniImageNet, and Caltech-UCSD Birds-200-2011. Four models are trained for one-shot and five-shot classification tasks in Euclidean and hyperbolic spaces. Re-identification results are provided for Market-1501 and DukeMTMD datasets. The hypothesis that training a hyperbolic classifier affects the distance of Poincar\u00e9 ball embedding is validated. The code is available on github. Training a hyperbolic classifier can affect the distance of the Poincar\u00e9 ball embedding of an image, serving as a measure of model confidence. A hyperbolic convolutional neural network was trained on MNIST, achieving 99% test accuracy. Evaluation on the Omniglot dataset showed the hyperbolic distance to the origin of embeddings produced by the network. Comparing distributions of maximum class probability predicted by the network with Euclidean classifiers on MNIST revealed interesting findings. The study compared the distributions of distance to the origin for MNIST and Omniglot datasets embedded into the Poincar\u00e9 ball with a hyperbolic classifier trained on MNIST. The findings showed that distances to the origin were a more statistically significant indicator of dataset dissimilarity in 3 cases. Additionally, the visualization revealed that 'unclear' images were located near the center, while easily classifiable images were closer to the boundary. The study compared distance to the origin as a significant indicator of dataset dissimilarity. Hyperbolic embeddings were tested on the Omniglot dataset for few-shot classification using the ProtoNet architecture. In ProtoNet, hyperbolic embeddings are used for few-shot learning, showing mixed results compared to Euclidean embeddings. The simplicity of the dataset may explain the lack of significant benefits. The approach is further tested on the MiniImageNet dataset, with specific class splits for training and validation sets. The training dataset has 64 classes, validation dataset has 16 classes, and the test dataset has 20 classes. The prototype network (ProtoNet) is used as a baseline model for one-shot and five-shot classifications. Results on MiniImageNet dataset show a slight gain in model accuracy compared to other models. The CUB dataset consists of 11,788 images of 200 bird species for fine-grained classification. The study utilized a split of 100 classes for training, 50 for validation, and 50 for testing, following a pre-processing step of resizing images to 64x64. Results on the CUB dataset showed superior performance of the hyperbolic version over the Euclidean counterpart. Additionally, the DukeMTMC-reID dataset contains 16,522 training images and the Market1501 dataset contains 12,936 training images. Rank1 and Mean Average Precision were reported for both datasets. More detailed experimental settings can be found in reference B. The results, reported after 300 training epochs, show that the hyperbolic version generally outperforms the baseline. The study investigates using hyperbolic spaces for image embeddings, utilizing Euclidean operations in most layers and transitioning to hyperbolic spaces at the end of the network. This approach is compatible with existing backbone networks trained in Euclidean geometry and has shown significant improvements in tasks like few-shot image classification. Learning hyperbolic embeddings can boost accuracy in few-shot image classification by conforming better to the intrinsic geometry of image manifolds. Understanding when and why hyperbolic geometry is justified is crucial, as numeric precision effects can impact model performance. Care should be taken to minimize numerical errors during learning by clipping the embeddings. The hyperbolic MLR formula for K classes is derived by embedding the output into the Poincar\u00e9 ball of dimension 64 using the exponential map. The model uses learnable parameters p k \u2208 D n c and a k \u2208 T p k D n c \\ {0}. The prototype network (ProtoNet) is used as a baseline model with 3 \u00d7 3 convolutional layers, batch normalization, ReLU nonlinearity, and 2 \u00d7 2 max-pooling layers. The number of filters in the last convolutional layer corresponds to the embedding dimension of 64. The hyperbolic model embeds the output into a Poincar\u00e9 ball of dimension 1024 using the exponential map. The learning rate starts at 10^-3 and decreases by 0.2 every 10 epochs out of a total of 200 epochs. The model is tested on one-shot and five-shot classifications with 15 query points in each batch. The last convolutional block output is embedded into the Poincar\u00e9 ball of dimension 1024. In ProtoNet, a prototype representation of a class is used, defined as the mean of the embedded support set. The initial learning rate is 10^-3, decreasing by 0.2 every 10 epochs out of 200. For Caltech-UCSD Birds, the embedding dimension is fixed at 512, with a learning rate scheduler multiplying the initial rate by 0.7 every 20 epochs out of 100. ResNet-50 architecture is used for person re-identification. For person re-identification, ResNet-50 architecture with different embedding dimensionalities (32, 64, 128) is used. The experiments include baseline and hyperbolic versions with sensitivity to learning rate schedules. Smaller c values yield better results, with c set to 10^-5. The hyperbolic setting is close to Euclidean. The hyperbolic setting for person re-identification is similar to Euclidean. Starting learning rates are set to 3 \u00b7 10 \u22124 and 6 \u00b7 10 \u22124 for different schedules, adjusting them by 0.1 after specific epochs."
}