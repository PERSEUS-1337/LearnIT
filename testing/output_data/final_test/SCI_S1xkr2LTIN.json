{
    "title": "S1xkr2LTIN",
    "content": "The paper introduces simple_rl, an open-source library for reinforcement learning experiments in Python. It aims to simplify the complex process by providing tools for creating agents, environments, and analyzing results. The core functionality includes creating agents and an MDP, running experiments, and plotting interactions. The paper introduces simple_rl, an open-source library for reinforcement learning experiments in Python. It simplifies the process by creating agents and an MDP, running experiments, and plotting interactions. Running an experiment generates a log file for result reproduction. Reinforcement learning has gained popularity for challenging tasks like playing Atari games and robotic control. TensorFlow, scipy, and numpy are used in conjunction with the library. A lightweight library is needed for quick execution and analysis of RL experiments in Python. The simple_rl library in Python focuses on simplicity for users conducting quick RL experiments. It generates a JSON file logging experimental details with minimal code required. The library is designed for easy sharing and reproduction of findings. The simple_rl library in Python generates a JSON file to track experiments and reproduce plots. It showcases the main design philosophy of simple_rl with agents and environments. The library simplifies MDP instance definition by using functions for state and reward output. The run agents on mdp function allows for multiple experiment repetitions and episodes. The simple_rl library in Python generates a JSON file to track experiments and reproduce plots. It simplifies MDP instance definition and allows for multiple experiment repetitions and episodes. The JSON file serves as a certificate for reproducing plots if the same experiment is run again. The library can also run similar experiments in the OpenAI Gym. The experiment structure is the same for GymMDP. Input the environment name, like CartPole-v1, and choose to visualize the learning process with render flag. Additional feedback can be obtained by setting the verbose flag to true for detailed tracking of experiment progress. Various ways to run experiments exist, but these examples cover the core experimental cycle. The JSON files shared in the library allow users to rerun experiments with the same results. The function reproduce from exp file(exp name) reconstructs all components to remake the plot. Providing a JSON file certifies similar results. The library focuses on agents, MDPs, and interactions, obscuring complexity. The library focuses on agents, MDPs, and interactions, simplifying plotting and experiment tracking. Plotting is tightly coupled with running experiments, with a basic plot showing time on the x-axis and cumulative reward on the y-axis for each algorithm. The experimental pipeline allows the end programmer to control the type of plot generated, with cumulative reward displayed on the y-axis for each algorithm. The core approach to RL involves forming abstractions of state."
}