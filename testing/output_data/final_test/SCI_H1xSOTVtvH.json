{
    "title": "H1xSOTVtvH",
    "content": "Producing agents that can generalize in reinforcement learning is challenging. Domain randomization is a method to address this, but it can be inefficient. Minimizing the policy's Lipschitz constant with respect to randomization parameters reduces variance in learned policies. A new method involves training the agent on one environment variation and regularizing its state representations. Experiments show this technique leads to more efficient and robust learning compared to standard domain randomization. Our technique improves learning efficiency and robustness compared to standard domain randomization, maintaining equal generalization scores. Deep Reinforcement Learning has excelled in complex tasks, but overfitting remains a challenge. Generalization is crucial for robotics and autonomous vehicles, where agents must adapt to novel real-world conditions after training in simulated environments. The focus is on generalizing between visually different environments in robotics, addressing the reality gap. Prior work includes domain adaptation and domain randomization techniques to match simulation data with real-world distribution. Domain randomization involves randomizing visual and physical properties during training episodes, leading to improved learning efficiency and robustness in Deep Reinforcement Learning. Regularization method proposed for learning policies robust to irrelevant visual changes in the environment, combining aspects of domain adaptation and domain randomization. Aim to achieve good generalization over randomization space to address suboptimal policies with high variance in performance. The paper formalizes the visual domain randomization problem and introduces an algorithm to train the agent on one environment variation while minimizing the Lipschitz constant. Experimental results show that this method is more efficient with lower variance policies compared to standard domain randomization, while maintaining equal or better returns and generalization ability. The paper also reviews related work, presents theory contributions, describes the regularization method, and compares it with standard domain randomization. In a toy gridworld problem, the visual generalization challenge in reinforcement learning is illustrated. Deep RL algorithms are prone to overfitting and memorization of training scenarios, as shown in previous studies. Evaluating generalization to novel conditions is not common practice in Deep RL, unlike in supervised learning. Domain shift can be induced by adjusting robot parameters like mass and length to create new environments. Farebrother et al. (2018) suggest using different game modes in Atari games to assess generalization. They draw from supervised learning techniques, noting that L2 regularization and dropout can aid agents in learning more adaptable features. These studies highlight the tendency of standard Deep RL algorithms to overfit to the training environment, emphasizing the need for agents that can generalize effectively. Visual domain randomization, which focuses on maintaining policy consistency across domains, has been utilized to transfer RL agents from simulation to reality without real image data. Transfer RL agents from simulation to the real world using low fidelity rendering and randomized scene properties for improved generalization. Some approaches combine domain randomization and adaptation techniques to bridge the gap between simulated and real-world trajectories. However, these methods may require a large number of real-world samples, which can be costly. Previous studies have highlighted the inefficiency of domain randomization in this context. Domain randomization has been shown to lead to suboptimal policies that vary between domains. Mehta et al. (2019) suggest training on the most informative environment variations within randomization ranges. Zakharov et al. (2019) use DeceptionNet to guide domain randomization for image classification tasks. Learning domain-invariant features is a promising approach to leverage commonalities between domains. In reinforcement learning, methods have been extended from semi-supervised learning to enforce similar predictions for original and augmented data points, reducing the need for labeled data. In reinforcement learning, various methods have been explored to encourage networks to learn similar embeddings for samples from different domains. These methods aim to transfer policies for controlling aerial vehicles to various environments and improve generalization in robot grasping tasks. Adversarial loss is used to train RL agents to learn similar policies in both simulated and target domains. However, these methods are limited to cases where both domains are known and cannot be easily applied when the target domain is within a distribution of domains. Our work proposes a regularization scheme for learning policies that are invariant to visual changes in the environment without real-world data. We differ from previous work by regularizing intermediate layers instead of network outputs. Experimental comparison shows that regularizing network outputs leads to a trade-off between agent performance and generalization. In reinforcement learning, the agent aims to maximize cumulative rewards by finding a policy that maps states to actions. A framework involves modifying environment parameters within a randomization space to visually change the environment. Standard domain randomization produces policies with varying performance. Domain randomization involves modifying environment parameters to visually change the environment, leading to high variance in learned policies. A visually randomized MDP is formalized by defining a randomizer function that maps states to new states. Despite sharing the same rewards and transitions, the agent's policy can vary between domains, adopting different policies for different randomizations. The agent may have different policies for different randomizations due to the unique optimal Q-function or imperfect function approximation. Lipschitz continuity of a policy over randomizations is introduced to compare policy differences. Lipschitz constant quantifies the variation in policies over randomizations. The Lipschitz constant quantifies the robustness of RL agents over a randomization space. A smaller Lipschitz constant means less impact on the policy from different randomization parameters. If a policy is Lipschitz continuous over randomized MDPs, small changes in the environment will have a small impact on the policy. The inequalities show that smaller Lipschitz constants result in smaller variations of the policy over the randomization space. Regularization technique minimizes Lipschitz constant of policy to produce low-variance policies over randomization space. Agent trained on one environment with policy parameterized by \u03b8. Loss minimized during training with regularization parameter \u03bb. Feature extractor f\u03b8 used in policy. Output of last hidden layer chosen as feature extractor in experiments. Our method involves using the output of the last hidden layer as a feature extractor to minimize the Lipschitz constant and learn representations of states that ignore random variations. It can be applied to various RL algorithms by adding an additional term to the learning loss. Experimental applications to both value-based and policy-based reinforcement learning algorithms are demonstrated, with implementation details available in the appendix. Experiments on a simple gridworld show the effectiveness of the technique. The agent navigates a 3x3 gridworld to reach the goal while avoiding fire. Regularized agents' policies are compared to theoretical bounds. Randomizations of the domain show more consistent behavior with our regularization method. The environment includes two optimal policies, with the agent starting at the bottom left and moving up or right to reach the goal within a time limit of 10 steps. Random variations are minimized using the output of the last hidden layer as a feature extractor. In a 3x3 gridworld, agents navigate to reach the goal while avoiding fire. Randomized agents learn different paths based on randomization parameter \u03be. Regularized agents consistently follow the same path regardless of \u03be, showing more stable behavior. This approach reduces variance in policy, crucial for complex environments. In a 3x3 gridworld, agents navigate to reach the goal while avoiding fire. Randomized agents learn different paths based on randomization parameter \u03be. Regularized agents consistently follow the same path regardless of \u03be, showing more stable behavior. This approach reduces variance in policy, crucial for complex environments. Comparing policies learned by regularized agents on different domains, the measured difference is compared to theoretical bounds. Increasing \u03bb leads to decreases in both empirical returns difference and theoretical bound. Our regularization method reduces variance in policy and improves training stability in a visual Cartpole environment. Randomization involves changing the background color, and our implementation is based on the OpenAI Gym. The DQN algorithm is used for training in this challenging visual environment. The study compares the performance of three agents trained on different randomization spaces in a visual Cartpole environment using the DQN algorithm. The agents include Normal, Randomized, and Regularized, each trained with the same hyperparameters over the same number of steps. The Regularized agent uses a regularization method to improve training stability. The study compares the performance of three agents (Normal, Randomized, and Regularized) trained on different randomization spaces in a visual Cartpole environment using the DQN algorithm. The Regularized agent shows improved training stability compared to the other agents, especially in high-dimensional problems with larger randomization spaces. Standard domain randomization scales poorly with the size of the randomization space, while the regularization method is more robust. The Regularized agent demonstrates more stable policies compared to the Randomized agent in various domains within the randomization space. Despite being trained on one domain, the Regularized agent consistently achieves high scores on other domains, while the Randomized agent's policies show high variance between domains. The Regularized agent shows stable policies across different domains in the randomization space, achieving high scores consistently. In contrast, the Randomized agent exhibits varying policies between domains, as seen in the visualization of learned representations for different background colors. The regularized agent learns a similar representation for different backgrounds, while the randomized agent clearly separates them. The regularization technique reduces the variance of the estimated value function. Our regularization technique successfully reduces the variance of the value function over the randomization domain in the CarRacing environment. Experiments with the PPO algorithm also show the applicability of the regularization method to different domains. In this experiment, randomization over the entire RGB cube is larger than for cartpole experiments. The randomized agent fails to learn a successful policy on this large randomization space, while other agents succeed. Generalization ability is compared to agents trained with different randomization and regularization methods. Agents are trained on both reference and randomized domains, showing varying scores. Our regularization method leads to successful training and generalization in visually diverse environments. A higher \u03bb value results in better generalization scores. Dropout is the only regularization scheme that improves generalization compared to the baseline. The study focused on deep reinforcement learning in visually diverse environments, proposing a method for robust, low-variance policies. Multiple experiments supported the claims using on-policy and off-policy algorithms. The proof presented supports claims for MDPs with a discrete action space and can be generalized to continuous action spaces. The lemma bounds the total variation distance of joint distributions. By marginalizing over actions, a looser bound is achieved. Aractingi et al. (2019) provided shaded errors for 95% confidence intervals. In a separate study, Aractingi et al. (2019) proposed a regularization scheme on randomized visual domains using the PPO algorithm on the VizDoom environment. They focused on regularizing the output of the policy network, while our work emphasizes regularizing the final hidden layer to separate representation and policy learning. In experiments on the visual cartpole domain, varying the regularization parameter \u03bb showed a tradeoff between generalization and policy performance. Increasing \u03bb negatively impacted training but led to more consistent results across randomization space. Changing \u03bb only affected generalization, not agent performance on the reference domain. The study contrasts with Aractingi et al. (2019) by focusing on regularizing the final hidden layer for representation and policy learning. For the visual cartpole environment, images are 84x84 pixels with RGB channels. Momentum information is included by stacking 3 frames, resulting in a state shape of 84x84x9. Agents trained with this preprocessing achieve average returns of 175 instead of the maximum score of 200. In CarRacing, each state consists of 96x96 pixels with RGB channels. In CarRacing, states consist of 96x96 pixels with RGB channels. Frame skipping is used with a skip parameter of 5, limiting episodes to 200 actions. Two frames are stacked to include momentum information, resulting in a state shape of 96x96x6. Preprocessing speeds up training but may prevent agents from reaching the maximum score. Regularized agents, trained on \u039e big, outperform randomized agents in interpolation but struggle with extrapolation outside the training domain. The regularization method does not guarantee good extrapolation performance. This result is crucial for achieving transfer to unknown target domains. The randomized agent learns different representations for different domains within the randomization space, while the regularized agent learns similar representations. The randomized agent clearly separates training domains, whereas the regularized agent shows similar representations for both domains. The regularized agent is more robust to domain shifts than the randomized agent, as shown by the t-SNE plots of their features in different background color domains. Additionally, the regularized agent trains faster than the randomized agent, as seen in the training curves. The randomized agent is slower and less stable than the regularized agent. Generalization scores show that the randomized agent specializes in specific training domains, while the regularized agent achieves consistent scores across domains due to its regularization method. In a dynamics randomization experiment, representations learned by agents are analyzed. Policies are rolled out in randomized environments using a greedy strategy. Activations of the last hidden layer are collected and visualized using t-SNE plots. The randomized agent learns differently from the regularized agent. The randomized agent learns different representations for two environments, leading to high variance in training. In contrast, the regularized agent uses the same representation for both domains, enabling faster learning and robust policies."
}