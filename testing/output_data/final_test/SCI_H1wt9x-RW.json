{
    "title": "H1wt9x-RW",
    "content": "Teachers intentionally select informative examples to teach students, but when teacher and student are neural networks, the examples given by the teacher network are often uninterpretable. Training the student and teacher iteratively can lead to interpretable teaching strategies, demonstrated through measuring similarity to intuitive strategies and human experiments. The teacher network learns to provide interpretable examples for teaching various concepts effectively. Human teachers also provide informative examples to help students learn faster and more accurately. Recent work on learning emergent communication protocols in deep-learning based agents has been successful at solving a variety of tasks. However, the protocols learned by the agents are usually uninterpretable to humans, limiting their potential for communication. The hypothesis is that the uninterpretability arises from the agents being optimized jointly. Recent work on learning emergent communication protocols in deep-learning based agents has shown success in solving tasks. However, the learned protocols are often uninterpretable to humans due to joint optimization of teacher and student networks. This raises the question of whether there is an alternative approach where the student maintains an interpretable learning strategy. The study aims to jointly optimize teaching and learning strategies, focusing on interpretability. By training the student on random examples and having the teacher select examples for the student, they achieve interpretable teaching and learning strategies. The effectiveness of these strategies is evaluated in teaching rule-based, probabilistic, boolean, and hierarchical concepts. The study focuses on optimizing teaching and learning strategies for interpretability. A rational teacher selects examples to help the student infer the correct concept, updating their prior belief. Shafto et al propose a Bayesian model for human pedagogical reasoning, where the teacher's example selection is based on the student's posterior probability of the concept. This leads to recursive equations for updating the student's posterior. In optimizing teaching strategies for interpretability, the initial distribution for the teacher is crucial. The teacher distribution is initialized uniformly over examples consistent with the concept, following Shafto et al's suggestion. The optimization process involves training the student on this initial distribution and then optimizing the teacher for the fixed student. This approach yields interpretable strategies with just one iteration. Teaching via examples is akin to communication between teacher and student, with recent work focusing on learning communication protocols in deep-learning agents. Communication protocols in deep-learning agents can be difficult to interpret despite efforts to encourage interpretability. Techniques like limiting symbol vocabulary size or introducing auxiliary tasks have been suggested, but the reasons for humans creating interpretable protocols while neural networks do not remain unclear. One possible reason is that communication protocols are typically learned through joint optimization of all agents. The machine teaching literature explores the challenge of finding a student-teacher pair that can learn concepts without resorting to contrived solutions known as \"coding tricks.\" These tricks involve the teacher and student colluding on a pre-specified protocol for encoding concepts through examples, leading to the proposal of additional constraints to prevent such tricks. The machine teaching literature aims to prevent collusion between teacher and student by imposing constraints like learning through any superset of examples, working for any concept ordering, and using incompatible hypothesis spaces. Unlike prior theoretical work, this study takes an experimental approach, evaluating interpretability criteria by modifying optimization procedures on a set of concepts and examples. In the machine teaching literature, the study evaluates interpretability criteria by modifying optimization procedures on a set of concepts and examples. The performance of the student and teacher is evaluated by a loss function that takes in the true concept and the student's output after a sequence of examples. In machine teaching, the student and teacher are modeled with deep recurrent neural networks. The optimization of S and T can be done jointly or using a best response approach, where S and T are trained iteratively in two steps. In machine teaching, deep recurrent neural networks model the student (S) and teacher (T). The optimization of S and T is done in two steps: 1. Train S on concept examples from a prior distribution. 2. Train T to select or generate examples for S. The separation of optimization into two steps aims to encourage interpretable learning and teaching strategies. In machine teaching, deep recurrent neural networks model the student (S) and teacher (T). The optimization of S and T is done in two steps: 1. Train S on concept examples from a prior distribution. 2. Train T to select or generate examples for S. T learns to pick informative examples within a rectangle, such as the corners. Various tasks were created to evaluate different types of concepts. One task involves the rectangle game where the student must infer the boundary of a rectangle from examples provided by the teacher. The student's goal is to infer the boundary of the rectangle from examples of points within the rectangle. Human teachers tend to pick opposite corners of the rectangle as an intuitive strategy, which the teacher (T) also learns to match. Concept learning is modeled as estimating the probability density of the concept, especially when dealing with multiple subtypes of a concept like a bimodal distribution. The bimodal distribution is parameterized as a mixture of two Gaussian distributions, and the goal is to learn the location of the modes. The task is to see what strategy T learns to quickly teach S which properties are relevant to a concept, using examples that vary based on size, color, shape, and border. Only one to three properties define a concept, such as red circles. T learns to teach S which properties are relevant to a concept by selecting examples with common properties required by the concept. Hierarchical concepts are tested by pruning subtrees from Imagenet, where T's goal is to teach S nodes from any level in the hierarchy using images from leaf nodes. The concept node allows generalization in the hierarchy. A concept is encoded as a four-vector representing a rectangle. The loss function measures the difference between the true concept and the model's output. Training involves generating examples and teaching the model. In joint optimization, the relationship between examples and the ground-truth concept is unclear. The examples given by T and S relate to the ground-truth rectangle and the policy for inferring the rectangle. T outputs points close to opposite corners, while S expands its estimate to fit the examples given by T. The distance between examples and corners is measured to determine closeness. Concepts are encoded as two-dimensional vectors with locations of modes. The text discusses a scenario where a system S is trained using examples generated from a mixture of two Gaussians. Another system T is then trained to teach S using two examples, learning to match an intuitive strategy better than random or joint strategies. Examples are images of size 25 x 25 x 3, and concepts are represented as ten-dimensional binary vectors. The text discusses training system S with examples from a mixture of two Gaussians. System T is then trained to teach S using two examples, learning to select examples with common required properties for the concept. System T is trained to select examples with common required properties for a concept, illustrated by examples of red shapes. Hierarchical concepts are created by pruning a subtree from Imagenet, with each node representing a concept encoded as a one-hot vector. Images are randomly selected for each leaf node, with possible examples for interior nodes being images from descendant leaves. For instance, in the hierarchy of apes, the \"lesser apes\" concept includes images of siamangs or gibbons. System T is trained to select examples for a concept by using a pretrained ResNet-50 model to embed images into vectors. It learns to pick examples from two leaf nodes such that the lowest common ancestor of the nodes is the concept node, teaching the concept effectively. System T is trained to select examples for a concept by using a pretrained ResNet-50 model to embed images into vectors. It effectively teaches concepts by selecting examples from two leaf nodes with the lowest common ancestor as the concept node. T's strategy outperforms jointly trained strategies in teaching tasks and is evaluated for interpretability by measuring its effectiveness in teaching humans. In a study, 60 subjects were recruited from Amazon Mechanical Turk and tested on concepts with different modes. They were shown examples generated by a teacher model and asked to rate the likelihood of lines belonging to the concept. Subjects who received examples from the teacher had higher accuracy (18%) compared to those with random examples (8%). The teacher group also showed a higher standard deviation in their ratings. The teacher group had significantly higher accuracy (18%) compared to the random group (8%), with a higher standard deviation. The maximum accuracy in the teacher group was 70%, while in the random group it was only 20%. The difficulty of the task lies in needing to get the entire distribution correct for a correct answer. Many subjects had the wrong hypothesis about the concept structure, believing it to be unimodal instead of bimodal. The study evaluated human learning of boolean concepts by sampling ten test concepts and recruiting 80 subjects on Amazon Mechanical Turk. Participants were shown examples and asked to classify new images. The group that received examples from the teacher performed better with a mean accuracy of 76% compared to 71% for those who received random examples. The study compared human learning with deep learning models and found that humans have limitations in jointly optimizing. By using an iterative optimization approach, they discovered more interpretable teaching protocols. This limitation is just one of many that could explain differences in learning protocols between humans and deep learning models. Humans had different priors over concepts compared to the student network, leading to limitations in learning. It is unrealistic to expect a perfect match in prior knowledge between teacher and student. Future work should explore teaching in more complex settings and communication tasks. Our work aims to bridge the gap in interpretability between machine agents and human agents, focusing on teaching through demonstrations and communication tasks."
}