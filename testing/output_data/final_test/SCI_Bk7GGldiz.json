{
    "title": "Bk7GGldiz",
    "content": "The current state-of-the-art end-to-end semantic role labeling (SRL) model is a deep neural network architecture without explicit linguistic features. However, incorporating gold syntax trees can significantly enhance SRL performance. A new model called linguistically-informed self-attention (LISA) combines multi-head self-attention with multi-task learning for dependency parsing, part-of-speech tagging, predicate detection, and SRL. The model can predict these tasks and can also benefit from high-quality syntactic parses at test time without re-training. LISA achieves significant improvements in semantic role labeling (SRL) performance on various datasets, surpassing previous state-of-the-art models. SRL extracts meaning from sentences by labeling actions and entities. Recent advancements in deep neural network architectures have shown superior SRL performance without explicit syntax modeling. Recent work suggests that incorporating syntax into neural network models for semantic role labeling (SRL) can lead to higher performance gains. While some models show small improvements with constrained inference using predicted parses, significant gains are achieved with gold-quality parses. However, current architectures utilizing syntax, such as graph convolutional neural networks, only result in marginal increases or even worse performance on out-of-domain data compared to syntax-free models. This indicates that there is still room for improvement in designing neural network architectures that maximize the benefits of incorporating syntax for SRL. Proposing linguistically-informed self-attention (LISA) model for neural network SRL, combining multi-task learning with self-attention to provide syntactic parses for predicting semantic role labels. End-to-end model predicts parts-of-speech, predicates, syntactic parse, and semantic roles in a single pass, benefiting from improved parsing models without re-training. The linguistically-informed self-attention (LISA) model combines multi-task learning with self-attention to predict semantic role labels by providing syntactic parses. The model achieves state-of-the-art accuracy in less training time on CoNLL-2005 and CoNLL-2012 datasets, outperforming syntax-free models. Our model, LISA, achieves a 13% reduction in error and outperforms state-of-the-art ensembles on various datasets. It combines multi-task learning, neural attention for predicting syntactic dependencies, and careful conditioning during training. The model utilizes word embeddings and various neural network layers to encode rich token-level representations. The model utilizes stacked convolutional, feed-forward, and multihead self-attention layers to efficiently produce contextually encoded token embeddings. It outperforms LSTM, CNN, or self-attention layers alone in terms of speed-accuracy Pareto efficiency. To predict semantic role labels, the contextually encoded tokens are projected to distinct predicate and role embeddings, and each predicted predicate is scored with the sequence's role representations using a bilinear model. One self-attention head is trained to attend to each token's syntactic parent to incorporate syntax. The model integrates syntactic information by using a gold parse oracle during training and incorporating part-of-speech and predicate information in earlier layers. It simplifies optimization by treating tagging and predicate detection as a single task and is trained end-to-end using stochastic gradient descent. The network integrates syntactic information by using a gold parse oracle during training and includes part-of-speech and predicate information in earlier layers. Token representations are input to convolutional layers with residual connections, producing contextually embedded token representations at each layer. The final CNN output is generated by convolutional layers with leaky ReLU activations and layer normalization. This representation is then fed into residual multi-head self-attention layers for long-distance context observation. This architecture allows for parallel computation of token representations at each layer. The output of convolutional layers is projected to a representation that is the same size as the output of self-attention layers. Self-attention layers are applied with layer normalization after each residual connection. The final token representations are obtained by performing a weighted sum of value vectors for each attention head. The self-attention mechanism in neural networks is trained to attend to specific tokens based on the syntactic structure of the sentence. This is achieved by encouraging one attention head to focus on each token's parent in a syntactic dependency tree. The attention weights between tokens are used as the distribution over possible heads for each token. The attention head in the neural network model emits a directed graph where each token's head is the one with the highest weight assigned by the attention. This attention head serves as an oracle for syntax, providing a dependency parse to downstream layers. The model can incorporate external parsing information at test time without re-training, allowing it to benefit from improved parsing models. Unlike typical multi-task models, this approach maintains the ability to leverage external syntactic information. Without specialized training, injecting gold dependency arcs into the attention head at test time does not significantly improve model accuracy for predicting semantic role labels. The model is limited by poor representations during early training, learning from randomly initialized attention rather than strong syntactic information. Training with the described technique improves SRL F1 by 7.0 points compared to using predicted parses. During training, the model learns to encode syntax information itself rather than relying on explicit syntax cues. To improve model performance, the gold parse is clamped when using its representation for later layers, enhancing the model's ability to leverage parse information at test time. Our approach extends teacher forcing BID66 to MTL, improving the model's ability to leverage parse information at test time. Training on predicted labels rather than gold labels has shown to enhance test-time accuracy. Sharing parameters of lower layers in the model for predicting POS tags and predicates is beneficial. Previous work often uses a pipelined approach for POS tagging, dependency parsing, and predicate detection, but our simple approach works well without advanced techniques. In a pipelined approach, a multi-task learning (MTL) method is used for POS tagging and predicate detection in SRL. Parameters are shared between layers for joint POS and predicate detection objectives. A joint label space is created by combining POS tagging and predicate detection, improving training efficiency. Localized probabilities are computed using softmax function for label prediction in the joint space. Supervision is applied at earlier layers to enhance model performance. The model predicts semantic roles for each predicate in the sequence by scoring each predicate against each token using a bilinear operation. Token representations are transformed into predicate-specific and role-specific representations before being scored using a bilinear transformation. The role label scores for each token with respect to the predicate are computed in parallel across all semantic frames in a minibatch using softmax. The model predicts semantic roles for each predicate in the sequence using a locally normalized distribution over role labels for token t in frame f. At test time, constrained decoding is performed using the Viterbi algorithm to emit valid sequences of BIO tags. The network learns parameters to model the coupling between tasks, with a penalty on syntactic attention loss. Training involves Nadam SGD with a learning rate schedule. The model predicts semantic roles using locally normalized distribution over role labels. Training involves Nadam SGD with a learning rate schedule and regularization techniques. Early approaches to SRL focused on linguistic features and constrained inference methods. Some models combined syntactic parsing and SRL, while others used neural networks for SRL. Neural network models for Semantic Role Labeling (SRL) have evolved over time. Early models incorporated syntax, but more recent syntax-free models have shown success. However, models that incorporate syntax may risk overfitting. Syntactically-informed self-attention has also been explored in SRL models. Neural network models for Semantic Role Labeling (SRL) have evolved over time, incorporating syntax. MTL is popular in NLP, with various tasks like POS tagging, chunking, parsing, and semantic relatedness being jointly trained. Different combinations of NLP tagging tasks have been investigated, including FrameNet semantics. MTL has also been applied to enhance machine translation models and semantic dependency parsing. Neural network models for Semantic Role Labeling (SRL) have evolved over time, incorporating syntax. Training on gold versus predicted labels is closely related to learning to search and scheduled sampling, with applications in NLP. More sophisticated approaches extending these techniques to Multi-Task Learning (MTL) could improve results. State-of-the-art results were achieved on the CoNLL-2005 shared task and the CoNLL-2012 English subset of OntoNotes 5.0 using pre-trained GloVe word embeddings. Models were trained for a maximum of 7 days on one TitanX GPU with early stopping on the validation set. For CoNLL-2005 and CoNLL-2012, syntactic information was utilized in SRL models. Various baseline models were compared to the best model (LISA G), including a syntax-free deep LSTM model, an ablation of a self-attention model without syntactic information, and another ablation with syntactically-informed self-attention. The models were trained with different approaches, such as using predicted attention weights instead of gold parse during training. Our models benefit from using predicted parses at test time, improving performance. Evaluating with gold syntactic parse shows room for improvement. Detailed analysis compares models using gold and predicted parses to understand where syntax provides the most benefit to SRL. The study evaluates parsing models' performance on SRL tasks, with Dozat and Manning achieving state-of-the-art results. Despite low parser UAS, SRL accuracy improves significantly with D&M parses. The SA model performs strongly without syntax access, outperforming BID25's single model but underperforming their ensemble. Our study evaluates parsing models' performance on SRL tasks, with attention increasing over models without syntax. Using dynamic versus gold parse oracles during training, our models achieve impressive scores, outperforming previous best scores. Our single models also outperform ensembles when using injected parses, showing marked improvements in F1 scores. Additionally, our SA model achieves higher precision than recall, indicating memorization of predicate words from training data. Interestingly, our SA model slightly outperforms syntax-infused models, possibly due to the LISA models' learning approach. Our study evaluates parsing models' performance on SRL tasks, achieving impressive scores by using dynamic versus gold parse oracles during training. Our best single models outperform ensembles, with the SA model slightly outperforming syntax-infused models, possibly due to the LISA models' learning approach. Our best single models outperform ensembles, with the SA model slightly outperforming syntax-infused models. LISA in its current form does not perform as well when gold predicates are given at test time. Our models outperform BID25 with D&M parses, but not BID60. Our model shows good performance even without gold predicates, outperforming ensembles and syntax-infused models. However, there are challenges with incorrect predicate predictions in certain scenarios. Further improvements could be made by focusing on modeling towards gold predicates. In the wild, LISA shows significant improvement with gold syntax trees compared to predicted syntax. Viterbi decoding has a larger impact on LISA parses than on D&M, but has the same impact on D&M and gold parses. Gold parses do not provide improvement in terms of BIO label consistency. Providing LISA with gold parses is particularly beneficial for sentences longer than 10 tokens. Incorporating gold syntax trees significantly improves performance for LISA, especially for sentences longer than 10 tokens. Correcting span boundary errors, such as Merge Spans and Split Spans, is crucial for enhancing parse accuracy. These errors are often related to prepositional phrase attachment mistakes. The text chunk discusses the breakdown of split/merge corrections by phrase type in semantic role labeling. A new multi-task neural network model, LISA, incorporating rich linguistic information out-performs the state-of-the-art on benchmark datasets. Future work includes improving parsing accuracy and adapting to more tasks. The CoNLL-2012 data split BID49 of OntoNotes 5.0 BID27 includes annotated text from various domains with gold part-of-speech, syntactic constituencies, named entities, and semantic role labels. The data is processed by converting semantic propositions and role segmentations to BIO boundary-encoded tags, using pre-trained GloVe embeddings, and converting constituency structure to dependencies. The text discusses parsing from Choi et al. FORMULA1, converting constituency structure to dependencies using ClearNLP, and evaluating SRL performance using the srl-eval.pl script provided by CoNLL-2005 shared task. The dataset contains verbal predicates with 28 distinct role label types and 105 SRL labels. The text discusses parsing and evaluating SRL performance using the CoNLL-2005 shared task script. The model is trained on WSJ sections 02-21, with Nadam algorithm for adaptive SGD. The model is trained on WSJ sections 02-21 using the Nadam algorithm for adaptive SGD. An initial learning rate of 0.04 is used with a linear increase for the first warm training steps, followed by a decay proportional to the inverse square root of the step number. The model consists of four self-attention layers with 8 attention heads each, two CNN layers with a filter size of 1024, and MLP projections of size 256. Training includes 4000 warmup steps and gradient norms are clipped to 5."
}