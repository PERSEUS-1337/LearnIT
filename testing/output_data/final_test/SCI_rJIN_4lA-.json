{
    "title": "rJIN_4lA-",
    "content": "Social dilemmas involve individuals facing a temptation to prioritize their own payoffs over total welfare. Creating AI agents that navigate these dilemmas is crucial for real-world interactions. By modifying reinforcement learning methods, agents can act in understandable, cooperative, provokable, and forgiving ways to maintain cooperation in social dilemmas. This approach does not require complex training methods, making it applicable in environments where good strategies can be developed. In social dilemmas, individuals must decide whether to prioritize personal benefits or contribute to the greater good. Creating AI agents that can navigate these dilemmas is essential for real-world interactions. By modifying reinforcement learning methods, agents can act in cooperative ways to maintain mutual cooperation. The goal is to construct artificial agents that can effectively solve complex bilateral social dilemmas, such as the Prisoner's Dilemma. The Prisoner's Dilemma (PD) suggests the strategy of tit-for-tat (TFT) for achieving high rewards in social dilemmas. TFT involves cooperating initially and then copying the partner's previous move. TFT is popular due to its ability to avoid exploitation, achieve cooperative payoffs, correct errors, and create good incentives for cooperation. Our contribution is to expand the idea of TFT to one-shot Markov social dilemmas using deep reinforcement learning. Our strategy, amTFT, aims to solve more complex Markov social dilemmas by approximating TFT properties through RL function approximation. Unlike in the PD, cooperation and defection are not simple labeled strategies in these dilemmas. amTFT is a strategy that learns two policies - a cooperative policy and a 'safe' policy (defection) - through modified self-play. It intelligently switches between these policies during a single game by calculating the gain from the partner's chosen action compared to the cooperative policy. If the total gain is below a threshold, it follows the cooperative policy; otherwise, it switches to the defection policy for a set number of turns. amTFT is a strategy that learns two policies - a cooperative policy and a 'safe' policy (defection) - through modified self-play. It switches between these policies based on the partner's gains, with a threshold determining when to switch to the defecting policy for a set number of turns. The strategy is shown to solve Markov social dilemmas and is robust to different cooperative policies, making it suitable for scaling agents beyond simple games. The definition of 'cooperative' policies aims to maximize both players' payoff in symmetric games. Human social preferences consider distribution, altruism, and context-dependent concerns. The focal point for applying amTFT in different circumstances must be chosen carefully. Once determined, the amTFT algorithm can be used by swapping out the cooperative objective function during training. The paper discusses the possibility of computing equilibria in repeated games by swapping out the cooperative objective function during training. It contrasts with existing literature by focusing on strategies within a single game rather than across iterations, and on finding a 'good' strategy for a single agent rather than equilibria. The advantage of amTFT is that it requires no additional machinery beyond standard self-play. It can help solve social dilemmas in environments where competitive agents are already constructed. In real social dilemmas, such as economic situations, a safe policy is to stop transacting with agents who defect. This differs from the usual focus on maintaining cooperation across iterations in repeated games. The literature on maintaining cooperation in repeated games differs from the focus on learning and evolution in games. While the former emphasizes threats of defection to maintain cooperation, the latter explores how environmental properties affect agents governed by learning or evolutionary dynamics. This literature provides insights but does not address the design of a single agent. The literature focuses on long-term interactions where agents need to discern a partner's type or shape the partner's adaptation. Works related to this construct 'leader' agents using reward shaping or policy gradient learning. There is a recent interest in using deep RL to construct agents for high payoffs in multi-agent environments, often in zero-sum scenarios. The literature focuses on long-term interactions in multi-agent environments, often in zero-sum scenarios. Self-play is used to construct agents that can achieve good outcomes, but naive application can lead to bad results. The repeated Prisoner's Dilemma is commonly used to study human decision-making in social dilemmas. Recent work in cognitive science explores more complex games and RL techniques. The main objective is to understand human decision-making, not to improve agent construction. A Markov game involves states, actions, and rewards for players. Players choose policies that map states to action probabilities. Value functions evaluate states based on policies. The focus is on understanding human decision-making in complex games using RL techniques. In Markov games, players use policies to make decisions based on states, actions, and rewards. Value functions and Q functions evaluate expected rewards for players based on their policies. Best response policies are strategies that maximize rewards in strategic interactions. A Nash equilibrium in a game occurs when policies for players are best responses to each other. A Markov perfect equilibrium is when these policies are perfect best responses. Cooperative Markov policies aim to cooperate from any state, forming equilibria. In a social dilemma, there are no cooperative policies that form equilibria, allowing for exploitation and higher rewards at the expense of cooperation. Policies that achieve cooperative payoffs require memory to maintain cooperation, as the current state alone does not contain past behavior information. Memory can be learned (e.g. an RNN) or designed as a summary statistic. Adding memory does not eliminate equilibria where both players cooperate. The text discusses the use of memory in maintaining cooperation in games. It explains that adding memory, such as using an RNN or a summary statistic, does not guarantee cooperation. The text introduces the concept of amTFT, a strategy that aims to incentivize cooperation by intelligently switching between policies during gameplay. The goal is to construct strategies that promote cooperation in social dilemmas. Policies in games must be exchangeable to ensure a unique distribution of rewards between players. If policies are not exchangeable, a coordination problem arises, complicating cooperation. Strategies with worse payoffs for both players are necessary for solving social dilemmas. The solution often depends on contextual factors and may involve choosing focal points in coordination games. In game theory, a game is considered \u03c0 D dominant for player 2 if it has worse payoffs for player 2. This dominance is a sufficient but not necessary condition, and it can be used to bound the payoffs of a partner during a punishment phase. The concept of compound policies is introduced, and the construction of an amTFT agent is discussed in theory. The amTFT agent starts in phase C, playing according to \u03c0 C. It switches to phase D if d > 0, choosing actions according to \u03c0 D for k periods. After k periods, it returns to phase C. The strategy guarantees cooperation if agents start in phase C and there is no noise. The amTFT agent transitions from phase C to phase D and back, ensuring cooperation without noise. Using RL methods, it constructs cooperative and defect policies through self-play and two reward schedules: selfish and cooperative. The reward schedule for each agent includes rewards from their own payoff and the other agent's rewards. amTFT differs from TFT by allowing any action during the D phase, similar to the rPD strategy. The converged policy and value function approximations are denoted as \u03c0Ci and QiCC. The learning algorithm used to compute policies is not specified in this paper. Convergence issues may arise with selfish self-play, while cooperative reward schedules follow standard RL convergence. The amTFT agent in self-play uses a memory state to approximate gains from partner actions and accumulates total payoff balance. If the balance crosses a threshold, actions are chosen according to a specific policy. The amTFT agent uses rollouts to compute a threshold and a constant to determine partner actions. Hyperparameters T and \u03b1 balance approximation error and noise, with T allowing for more error but relaxing partner constraints, and \u03b1 making defection costlier. The algorithm estimates rewards for deviations from the recommended strategy. The amTFT agent is an unbiased estimator of Q CC in games with limited impact on future payoffs. It uses rollouts to determine partner actions and has a robustness property when partner strategies are similar. Tested in grid-world and pixel-based environments, the agent shows success in Markov social dilemmas. In the Pong Player's Dilemma (PPD), players collect coins of their own color to maximize payoff, but are tempted to pick up coins of the other player's color, resulting in a penalty. Strategies must be learned from raw pixels, altering the reward structure of Atari Pong to create a dilemma where the winning move is not to play. In the Pong Player's Dilemma (PPD), strategies are explored using deep neural networks for state representation. Selfish training leads to suboptimal behavior, while Cooperative training finds policies for socially optimal outcomes. In Coins, agents learn to pick up all colors, while in PPD, selfishly trained agents compete for scores. In the Pong Player's Dilemma (PPD), selfishly trained agents compete to score, while prosocially trained agents cooperate by hitting the ball back and forth. Two Markov social dilemmas show that standard self-play leads to defecting strategies, while modified self-play results in cooperative but exploitable strategies. The results are used to construct strategies \u03c0 C and \u03c0 D. Further research is needed to improve the efficiency of learning models for QCC approximation. The Axelrod desiderata focuses on cooperation, robustness against defectors, and incentivizing cooperation. The 'Grim' strategy based on BID4 behaves like pure defection. Standard self-play results in strategy \u03c0 D. A tournament evaluates Markov social dilemma strategies, with 1000 replicates per strategy pair. Comparison is made between \u03c0 C, \u03c0 D, amTFT, and the direct adaptation of BID4. Folk theorem algorithm maintains equilibria through threat of deviation. The Grim Trigger Strategy, inspired by the folk theorem algorithm, switches to a different policy if the partner deviates from the cooperative policy. This strategy aims to avoid exploitation, cooperate with conditional cooperators, and incentivize cooperation. The safety of the strategy from exploitation is measured by the metric DISPLAYFORM0. The metric DISPLAYFORM0 measures how safe a strategy is from exploitation by a defector. It quantifies social welfare achieved in a world where everyone behaves according to strategy X and indicates how well strategies like Grim and amTFT behave against conditional cooperators. The measure DISPLAYFORM2 evaluates if strategy X incentivizes cooperation from its partner. The metrics evaluated for different strategies show that amTFT is a good strategy in a mixed environment with cooperators, tit-for-tat agents, and defectors. Retraining in the domain of Coins can be used as an additional metric. In the domain of Coins, retraining can serve as a metric for exploitability of strategies. Selfish RL agents converge to a 'grab all coins' strategy. Learners paired with cooperative teachers or amTFT learn to exploit or cooperate, respectively, with higher payoffs for both. Humans are adept at solving social dilemmas, and researchers are exploring how to imbue artificial agents with this capability. The amTFT strategy can promote cooperation and prevent exploitation in various environments, requiring only modified self-play for implementation. It is crucial to treat agents differently from other elements in the environment, as they possess beliefs, desires, and the ability to learn and optimize. Future work involves integrating concepts from inverse reinforcement learning and cognitive science to develop cooperative agents with a theory of mind. In exploring how to imbue artificial agents with the capability to solve social dilemmas, researchers focus on constructing agents with a theory of mind. The construction of agents that can interact in social dilemmas with humans will require AI designers to understand and adapt to human cooperative and moral intuitions. In social dilemmas, there are equilibria of mutual defection and conditional cooperation. Simple equilibria of constant mutual defection often have larger basins of attraction than cooperative policies in large policy spaces. Using the example of the repeated Prisoner's Dilemma, RL agents using policy gradient can converge to different equilibria. Using Adam BID25 and SGD, policies are learned from states to behavior in a policy space containing TFT, Grim Trigger, and Pavlov strategies. Each episode is a repeated PD game with policies mapping states to cooperation or not, trained using policy gradient and the REINFORCE algorithm. The REINFORCE algorithm is used to learn policies in a repeated PD game with various strategies. Cooperation is robust when it is a dominant strategy for both players, making the game no longer a social dilemma. Evolutionary game theoretic results show that cooperation can emerge in strategy spaces under certain conditions. In a repeated Prisoner's Dilemma game, even with favorable conditions, self-play fails to discover cooperation-maintaining strategies. The one deviation principle is applied to prove this, where player 1 is fixed as an amTFT agent and player 2's policy is considered based on player 1's behavior. In a repeated Prisoner's Dilemma game, the policy for player 2 is to play differently based on player 1's phase. The Principle of Optimality states that if there are no better responses, then the prescribed policy is optimal. Player 2's best response in the D phase is to play \u03c0 D 2. In the C phase, the per-period reward stream for player 2 is considered, with rewards being bounded and dependent on the discount rate. For sufficiently long games, the risk of deviation is minimized. In sufficiently long repeated games, the risk of deviation is minimized, allowing TFT to gain a foothold. In learning scenarios, this argument does not apply as the first k time steps approximate the full discounted expectation well. The highest profit from deviating with an amTFT partner is limited, and there exists a length k where deviating costs more than staying with the prescribed policy. This completes the proof by bounding the necessary length of a deviation phase. In order to compute the necessary length of a deviation phase, the amTFT agent requires access to the best response policy to \u03c0 D k C. Even if \u03c0 D -dominance is not strictly met, it can be a sufficient approximation. Training an RL agent on episodes where the partner plays \u03c0 D k C allows for approximating the best response policy. Rollouts are used to calculate the debit to the amTFT's partner at each time period, with good performance for both PPD and Coins. Learning a model of Q can also be beneficial. It is challenging to learn an accurate modelQ for all state-action pairs, not just those sampled by policies. In Coins, a modified training procedure was needed to train aQ model without policy rollouts, using the same neural network architecture as the policies. After finishing Selfish and Cooperative training, a second step of training using a fixed \u03c0 C is performed. The learner behaves according to a mixture of \u03c0 C , \u03c0 D , and random policies while the partner continues according to \u03c0 C. Q is updated via off-policy Bellman iteration. This modified procedure produced a Q function good enough to maintain cooperation. For more complex games like Coins, future work includes developing methodologies for more accurate Q approximations or combining a Q model with rollouts effectively. Coins has four actions and S is represented as a 4 \u00d7 5 \u00d7 5 binary tensor. The model uses a multi-layer convolutional neural network to approximate the policy and state-value function for a small game with higher-dimensional 2D state spaces. The model has repeated layers with 2D convolutions, batch normalization, and ReLU activation. Channel sizes decrease from k to k/2 while doubling in number. For a 5x5 board, channel sizes are 13, 26, 52, 104. The model uses a multi-layer convolutional neural network with channel sizes decreasing from k to k/2 while doubling in number. \u03c0 is computed via a linear layer with 4 outputs with softmax, while the value function is computed via a single-output linear layer. The actor and critic are updated episodically with a common learning rate, and training parameters include a learning rate of 0.001, continuation probability of .998, discount rate of 0.98, and a batch size of 32 for a total of 40,000 games. The policy is trained directly from pixels via A3C with modified rewards. Inputs are rescaled and normalized to 42x42, with a frame skip of 8. The network is a convolutional neural network with four layers and two heads for the actor and critic. The LSTM layer used in the library is found to be unnecessary. The pytorch-a3c library includes a convolutional neural network with four layers and two heads for actor and critic. Results of a tournament in two Markov social dilemmas show amTFT achieves cooperative payoffs with itself and close to defect payoff against defectors. Its partner receives higher payoff for cooperation than defection."
}