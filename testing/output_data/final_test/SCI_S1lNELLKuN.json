{
    "title": "S1lNELLKuN",
    "content": "The goal of unpaired cross-domain translation is to learn mappings between two domains by ensuring exact cycle consistency in the learned mappings. AlignFlow, a framework proposed in this work, uses a normalizing flow model to specify a single invertible mapping between the domains. It can be learned via adversarial training, maximum likelihood estimation, or a hybrid of the two methods. Theoretical consistency results for AlignFlow guarantee recovery of desirable mappings under suitable assumptions. Empirically, AlignFlow demonstrates its effectiveness. AlignFlow is a framework for cross-domain translation that uses a normalizing flow model to learn mappings between two domains. It shows significant improvements over baselines on image-to-image translation and unsupervised domain adaptation tasks. This ability to align domains has applications in various machine learning tasks. CycleGAN is a successful approach for unpaired cross-domain translation, learning conditional generative models to match marginal distributions via an adversarial objective. It aims to learn a joint distribution over two domains without paired data, showing significant improvements over baselines on image-to-image translation tasks. AlignFlow is a learning framework that aims to replace the cycle-consistency objective in CycleGAN with a single, invertible model G A\u2192B. This model draws inspiration from the literature on invertible generative models and seeks to learn approximately bijective mappings between domains. AlignFlow is a learning framework that uses normalizing flow models to represent mappings between domains, ensuring exact cycle-consistency by design. It simplifies the standard CycleGAN learning objective by learning a single, invertible mapping and provides flexibility in specifying the training objective, including adversarial training and maximum likelihood estimation. In this section, we discuss the background on generative adversarial networks (GANs) and normalizing flows. GANs specify a mapping between latent variables Z and observed variables X, while normalizing flows represent mappings between domains. The training involves maximum likelihood estimation (MLE) and adversarial training to achieve stable dynamics and regularization effects. Generative adversarial networks (GANs) involve latent variables Z and observed variables X. GANs are trained through adversarial training, where a generator plays against a critic to generate samples that confuse the critic. Different learning objectives like f-divergences, Wasserstein Distance, and maximum mean discrepancy are used for training. The generator and critic are deep neural networks optimized alternately. Adversarial training is used to train generative models with intractable likelihoods, giving excellent performance on sampling-based tasks. Normalizing flows represent a latent variable generative model with an invertible mapping between latent and observed variables. The models are harder to train due to alternating minimax optimization and suffer from issues like mode collapse. Adversarial training is used to train generative models with intractable likelihoods, giving excellent performance on sampling-based tasks. Normalizing flows involve an invertible mapping between latent and observed variables, requiring efficient evaluation of prior density and transformations for sampling. In recent literature on normalizing flows, networks like NICE and Autoregressive Flows have been proposed for efficient likelihood evaluation and sampling. Flow models can be trained via maximum likelihood estimation or likelihood-free adversarial training. The goal in unpaired cross-domain translation is to learn conditional distributions without paired data. CycleGAN addresses the issue of learning conditional distributions without paired data by introducing additional constraints. It uses a pair of conditional GANs to translate data from two domains, A and B, through cross-domain mappings G A\u2192B : A \u2192 B and G B\u2192A : B \u2192 A. The mapping G A\u2192B is learned adversarially with the help of a critic C B trained to distinguish between real and synthetic data from domain B. Similarly, a conditional GAN G B\u2192A and a critic C A for adversarial learning of the reverse mapping from B to A. Cycle consistency encourages data translated from domain A to B to be mapped back to the original datapoints in A. The cycle-consistency loss in CycleGAN is defined for translation between domains A and B, with hyperparameters controlling the strength of cycle consistent terms. It has been empirically shown to aid in learning cross-domain translations but requires a careful balance with adversarial training. Techniques like identity loss are proposed in BID0 to stabilize training and improve performance. The AlignFlow framework is introduced for learning cross-domain translations between domains A and B. The AlignFlow framework facilitates cross-domain translations between domains A and B by utilizing a graphical model with latent variables Z representing a shared feature space. The model allows for efficient learning and inference, with prior density assumptions over Z. Marginal densities over A and B are learned using unpaired data from the two domains. The AlignFlow framework enables cross-domain translations between domains A and B by utilizing a graphical model with latent variables Z. The joint distribution between variables A and Z, and B and Z, is specified to be invertible. The framework specifies cross-domain mappings as compositions of invertible mappings, allowing for increased flexibility in learning objectives and efficient inference. The AlignFlow framework enables cross-domain translations between domains A and B by utilizing a graphical model with latent variables Z. It specifies a cycle-consistent mapping B\u2192Z, shared latent space Z, and training via adversarial training and maximum likelihood estimation. The goal is to learn a mapping from B \u2192 A to align the marginal distribution over A with the target distribution p * A. The AlignFlow framework allows for cross-domain translations between domains A and B using latent variables Z. Adversarial training is used to learn a mapping from B to A, either through a critic distinguishing real and generated samples or by simply learning the mapping G Z\u2192A with a prior density p Z. This approach enables the alignment of the marginal distribution over A with the target distribution p * A. Flow models can be trained using adversarial training objectives like GAN loss or Maximum Likelihood Estimation. MLE objective maximizes the likelihood of dataset DA. Cycle consistency, as seen in CycleGAN, is crucial for learning mappings useful for downstream tasks. AlignFlow utilizes cycle consistency to encourage learning of meaningful cross-domain mappings. The overall objective combines adversarial learning and maximum likelihood objectives, with hyperparameters \u03bb A and \u03bb B reflecting the strength of MLE terms for domains A and B. The AlignFlow objective minimizes w.r.t. generator parameters G A\u2192B and maximizes w.r.t. critic parameters C A and C B. The AlignFlow objective minimizes the generator G A\u2192B and maximizes the critics C A and C B with different choices of \u03bb A and \u03bb B. Adversarial training, pure maximum likelihood training, and hybrid approaches are covered based on the values of \u03bb A and \u03bb B. The AlignFlow objective combines adversarial and maximum likelihood training for conditional and unconditional sampling. It differs from CycleGAN in model family, learning algorithm, and inference capabilities. The AlignFlow objective combines adversarial and maximum likelihood training for conditional and unconditional sampling, utilizing a single invertible mapping. In contrast, CycleGAN parameterizes two independent mappings and is restricted to conditional sampling. AlignFlow allows for training via adversarial learning, MLE, or a hybrid approach, with three parametric models involved. The optimal solutions to deep neural network architectures are analyzed within specific model families, characterizing conditions for marginal consistency of generators. AlignFlow is shown to be marginally-consistent for well-specified model families, with non-identifiable constructions provided in Appendix A.3. The AlignFlow model involves two critics and characterizes the dependence of optimal critics for a given invertible mapping. The optimal critics for the AlignFlow objective with cross-entropy GAN loss are related to true data densities for domains A and B. The result shows the relationship between optimal critics for the AlignFlow model. The optimal critic for one domain can be obtained via the optimal critic of another domain using an invertible mapping. AlignFlow is evaluated for image-to-image translation and unsupervised domain adaptation, with comparisons to CycleGAN. The AlignFlow architecture is based on invertible transformations introduced in Real-NVP. AlignFlow is evaluated on image-to-image translation datasets like Facades, Maps, and CityScapes. The evaluation includes using mean squared error (MSE) for unpaired translation models. Hybrid training of AlignFlow outperforms CycleGAN in most cases, showing significant improvements. The study shows that AlignFlow significantly outperforms CycleGAN in various cases, with a focus on unsupervised domain adaptation. The goal is to learn a classifier for a target domain using data from related source and target domains. Various algorithms aim to match distributions across domains, with CyCADA being a relevant model for this experiment. In this experiment, AlignFlow is evaluated for domain adaptation in the CyCADA framework, using MNIST, USPS, and SVHN datasets of handwritten digits. Instead of using a cycle-consistency loss term, AlignFlow is trained with only the GAN-based loss in the target direction. The study aims to assess the effectiveness of AlignFlow compared to CycleGAN in unsupervised domain adaptation. In unsupervised domain alignment, the key assumption is the existence of a mapping G A\u2192B such that the distribution of B matches that of G A\u2192B (A). CycleGAN, DiscoGAN, and DualGAN added a cycle-consistency constraint to address the under-constrained objective, while alternatives like variational autoencoders offer effective solutions without cycles or adversarial training. AlignFlow is a model that focuses on one-to-one unsupervised domain alignment by leveraging a shared latent space and exact cycle-consistency. It demonstrates the successful use of invertible models instead of the cycle-consistency objective, allowing for exact maximum likelihood training and meaningful shared latent space interpolation. AlignFlow is a learning framework that utilizes invertible generative models for unsupervised domain alignment, enabling exact maximum likelihood training and meaningful shared latent space interpolation. It builds upon previous work on volume-preserving invertible neural networks and incorporates advancements such as volume transformations and invertible convolutions. The model also leverages fast inversion using the Real-NVP model. AlignFlow is a learning framework for cross-domain translations using normalizing flow models, ensuring cycle-consistency, shared latent space, and flexible training objectives. The model shows gains in image-to-image translation and unsupervised domain adaptation, with increased inference capabilities through invertible models. Future work includes exploring alignment settings. AlignFlow is a learning framework for cross-domain translations using normalizing flow models, ensuring cycle-consistency, shared latent space, and flexible training objectives. Future work includes extensions to learning stochastic mappings and translations across multiple domains, as well as exploring the latent space from a manifold learning perspective. The model aims to aid in developing a theory for domain alignments and characterizing structure for identifiable recovery of cross-domain mappings. The optimal value for L MLE (G Z\u2192A ) is achieved at a marginally-consistent mapping G * Z\u2192A. Similarly, there exists a marginally-consistent mapping G * Z\u2192B that optimizes L MLE (G Z\u2192B). The cross-entropy GAN objective L GAN (C A , G B\u2192A) is globally minimized when p A = p * A and the critic is Bayes optimal. G * B\u2192A is marginally-consistent w.r.t. Z\u2192B and globally optimizes the AlignFlow objective. The Bayes optimal critic C * A prediction for any a \u2208 A is given by Eq 2. Proposition 1 in BID9 provides a proof. The densities p A (a) and p B (b) are related via a change of variables. The Bayes optimal critic C * B for any b \u2208 B is given by Proposition 1 in BID9. Marginal consistency and invertibility can reduce the underconstrained nature of the unpaired cross-domain translation problem but not eliminate it completely. Identifying non-identifiable model families for the MLE-only objective of AlignFlow with symmetric densities like isotropic Gaussian and Laplacian distributions. Proposition 2 states that for a symmetric prior and a function class closed under permutations, there exists an optimal solution to the AlignFlow objective. The text discusses the uniqueness of a solution for the AlignFlow objective via contradiction, considering alternate mappings and permutation matrices. The composition of permutation matrices and invertibility constraints are explored, leading to a change-of-variables formula. The text explores the failure of MLE-only training to identify the optimal mapping in the AlignFlow objective, suggesting potential issues with identifiability. Hybrid and adversarial training methods are found to be more effective for cross-domain translations. The text suggests that identifiability issues are less of a concern in achieving the objectives."
}