{
    "title": "H1gHb1rFwr",
    "content": "Recent evidence suggests that CNNs are biased towards textures, making them non-robust to adversarial perturbations. This paper proposes EVPNet, a network leveraging SIFT properties to improve CNN accuracy and robustness. EVPNet includes components like parametric DoG, truncated ReLU, and PNL for extreme value modeling. Experiments show that EVPNet achieves similar or better accuracy than traditional CNNs. Experiments show that EVPNets achieve similar or better accuracy than conventional CNNs and demonstrate much better robustness against adversarial attacks without adversarial training. Various network architectures have evolved rapidly since AlexNet's breakthrough in 2012, including VGGNet, GoogleNet, ResNet, DenseNet, and SENet. State-of-the-art CNNs face challenges in robustness, especially vulnerability to adversarial attacks. The text discusses the vulnerability of ImageNet models to adversarial attacks and various methods proposed to improve network robustness, including modifying input features, changing loss functions during training, and designing robust network architectures. Some fundamental questions remain unanswered regarding adversarial examples, such as their causes and impact on performance. Recent studies suggest that model architecture plays a more critical role in network robustness than model size. Research shows that CNNs are biased towards textures, leading to poor performance on shape-dominating objects. Human behavior differs from CNNs in distinguishing between texture-rich and shape-dominating objects. Deep convolutional features can be categorized into robust and non-robust features, with non-robust features potentially contributing to good generalization but lacking model interpretability. Disentangling these features with human priors in network design or training is an intriguing topic. Human priors, like SIFT, have been used to create robust visual features before the rise of CNNs in 2012. In 2012, Krizhevsky & Hinton introduced scale-space extrema using a DoG function, inspired by neural processing in the retina. Existing CNNs lack this operation. A new approach, EVPNet, leverages SIFT's scale-space extrema idea to enhance CNN architectures for improved accuracy and robustness. It includes parametric DoG (pDoG) for extracting extreme values, truncated ReLU (tReLU) to suppress noise, and projected components. The EVPNet architecture enhances CNNs by incorporating pDoG for extreme value extraction, tReLU for noise suppression, and PNL for feature normalization. EVPConv, a combination of pDoG and tReLU, outperforms standard convolution + ReLU in separating robust features from non-robust ones. The proposed EVPNet explicitly separates these features in deep neural networks, improving accuracy and robustness. The text discusses the proposal of extreme value preserving networks (EVPNets) to enhance deep neural networks by incorporating parametric DoG, truncated ReLU, and projected normalization layer components. These components improve accuracy and robustness against adversarial attacks, separating robust visual features effectively. Traditional robust visual feature algorithms like SIFT and SURF are based on scale-space theory, showing similarities with receptive field profiles. The text discusses the proposal of extreme value preserving networks (EVPNets) to enhance deep neural networks by incorporating parametric DoG, truncated ReLU, and projected normalization layer components. These components improve accuracy and robustness against adversarial attacks, mimicking key stages of SIFT algorithm. There are limited works addressing network architecture design for robustness against adversarial attacks. Methods include non-local denoising layers, peer sample information with graph convolution, and biologically inspired protection using non-linear saturated activation layers. These approaches aim to improve network robustness to white-box attacks. In this work, inspired by robust visual feature SIFT, innovative architecture components are systematically designed to improve model accuracy and robustness. Extreme value theory is used to measure DNN robustness by exploring input data distribution. Difference-of-Gaussian is defined for convolution operations on input images. The Scale-space Difference-of-Gaussian (DoG) method convolves input images with Gaussian kernels to produce difference-of-Gaussian images. Scale-space extrema are detected by comparing pixels to their neighbors. Adversarial attacks aim to find perturbations that deviate the classifier's output from the true label, with the Fast Gradient Sign Method (FGSM) being a simple attack method. Adversarial attacks use methods like Projected Gradient Descent (PGD) and advanced attacks like DeepFool and CW to perturb input images and deceive classifiers. Adversarial Training injects adversarial examples into training to improve classification of such examples. Adversarial training introduces adversarial examples into training to enhance model robustness. This paper proposes a novel network architecture, EVPNet, incorporating components like pDoG, tReLU, and PNL to mimic SIFT features and improve model accuracy. The extreme value preserving network (EVPNet) incorporates components like pDoG, which mimics DoG operation using learnable convolutional filters. This network architecture aims to improve model accuracy by mimicking SIFT features. Recent evidence suggests that non-symmetric DoG may outperform traditional methods. Depth-wise convolutions are applied to all feature-map channels, with shared weights to prevent \"extrema drift\" in the pDoG space. This approach focuses on producing extrema for the current scale without the need for full octave extrema like SIFT. The pDoG block introduces a new element into deep neural networks by incorporating a minus component. Local extrema are computed using maxout operations instead of 3x3 spatial grids like in SIFT. A maxout operation is used to merge two feature maps and ensure the output feature map is compatible with existing networks. The final output of this block is obtained through Truncated ReLU (tReLU). The pDoG block introduces a new element into deep neural networks by incorporating a minus component and computes local extrema using maxout operations. To suppress non-robust local extrema, a truncated ReLU (tReLU) is proposed with a learnable truncated parameter \u03b8 to truncate small extrema corresponding to noise and non-stable extrema in the pDoG space. This modification ensures a continuous version for easy training. The pDoG block introduces a truncated ReLU (tReLU) with a learnable parameter \u03b8 to suppress non-robust local extrema. This modification simplifies operations and improves robustness and accuracy for pDoG feature maps. SIFT computes gradient orientation histogram and L2 normalization for feature representation. PCA-SIFT addresses gradient pixel relationship by projecting patches into eigen-space using PCA. PNL replaces GAP in CNNs with a 1x1 conv-layer for learnable projection matrix W. The output u_i forms data matrix U for further processing. The PNL layer in CNNs replaces GAP with a 1x1 conv-layer for a learnable projection matrix W, producing a hyper-ball for robustness. This novel convolution block named EVPConv, along with EVPNet, incorporates three components for improved performance. The EVPConv block in EVPNet replaces Equation 8 with tReLU and incorporates the SE module for calibration. It can replace k \u00d7 k conv-layers in CNNs and the PNL layer can replace the GAP layer for feature abstraction. The EVPConv block in EVPNet introduces few additional parameters for shared depth-wise convolution, tReLU, and SE module. It brings about 7-20% more parameters and increases computing cost by 3-10%. The added cost is significant in practice, making training experiments 2x slower due to increased memory usage. The proposed network components and EVPNet are evaluated on CIFAR10 and SVHN datasets, with modifications made to the ResNet model for comparison. Training details include using SGD with momentum 0.9 for 160 epochs on CIFAR-10. The networks are trained with SGD using momentum 0.9 for 160 epochs. The initial learning rate is 0.1, divided by 10 at 80 and 120 epochs. For SVHN, the same network architecture as CIFAR-10 is used. Adversarial perturbations are constrained under l \u221e norm with a perturbation norm of 8 pixels. Non-targeted attack adversarial robustness is evaluated in three settings: normal training, FGSM adversarial training, and PGD adversarial training. R-FGSM is used for FGSM adversarial training to avoid gradient masking. During training, PGD attacks are used to generate adversarial examples with 7 iterations and a 2-pixel step size. Whitebox attacks like FGSM, PGD, DeepFool, and CW are evaluated, with results compared for PGD-10 and PGD-40. For blackbox attacks, VGG-16 is chosen as the source model, with FGSM used to generate adversarial examples. A thorough ablation study is conducted to demonstrate the effectiveness of each novel architecture. This part conducts a thorough ablation study on the effectiveness of novel architecture components for strong adversarial robustness. Experiments on CIFAR-10 with SE-ResNet-20 show interactions between components. Results show significant robustness improvement by adding pDoG or PNL layers, with pDoG also improving clean accuracy. The study explores the effectiveness of novel architecture components for adversarial robustness. Results show that tReLU combined with pDoG improves both clean and adversarial accuracy. Incorporating all three components leads to the best robustness with a 1.2% increase in clean accuracy. The proposed EVPNet outperforms source networks on CIFAR-10 and SVHN datasets. The study introduces the EVPNet architecture, which outperforms baseline networks on CIFAR-10 and SVHN datasets in terms of robustness against various attacks. EVPNet shows superior performance in clean model accuracy and whitebox attacks, even without adversarial training. The proposed EVPNet architecture demonstrates better robustness on shape/edge dominating object instances compared to CIFAR-10. It consistently outperforms baseline networks under all PGD iterations on both CIFAR-10 and SVHN datasets. Despite concerns about its accuracy on the strongest PGD-40 attack, EVPNet proves to be a robust network architecture. The proposed EVPNet architecture shows robustness on object instances, outperforming baseline networks on CIFAR-10 and SVHN datasets. Despite concerns about accuracy on PGD-40 attack, it proves to be a robust network architecture with remarkable results. The methodology developed may guide future studies on network robustness and architecture design. The EVPNet architecture demonstrates robustness on object instances, surpassing baseline models on CIFAR-10 and SVHN datasets. It shows lower average normalized distance compared to baseline models, highlighting the need for further investigation on robustness in latter layers. Additionally, a comparison between regular convolution + ReLU and EVPConv on ImageNet dataset shows that EVPNet significantly reduces error amplification effect. The paper introduces EVPNet, a network architecture that enhances robustness to adversarial attacks compared to conventional CNNs. EVPNet demonstrates better accuracy and robustness on ImageNet, outperforming ResNet-50. Experiments show that EVPNet variants maintain 6-10% top-1 accuracy even under adversarial attacks, while conventional CNNs drop to near zero accuracy. The EVP-ResNet variants show 6-10% top-1 accuracy under adversarial attacks, outperforming conventional CNNs. The robustness improvement is notable for MobileNet as well. The results may inspire new ways for network architecture design. Wide-ResNet is chosen as an example to extend the proposed components to other state-of-the-art architectures. In the study, WRN-22-8 outperforms ResNet-20 and ResNet-56 in clean accuracy. EVPNet replaces certain layers in WRN-22-8 and shows similar clean accuracy but significantly better performance against adversarial attacks. This demonstrates the strong robustness of EVPNet without adversarial training."
}