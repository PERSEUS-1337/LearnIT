{
    "title": "BJMuY-gRW",
    "content": "We introduce a neural network that represents sentences by composing words with Tree-LSTM using a fully differentiable natural language chart parser. The model optimizes both the composition function and the parser, eliminating the need for externally-provided parse trees. It outperforms supervised Tree-LSTM architectures on textual entailment and reverse dictionary tasks. An attention mechanism improves performance by fully exploiting the parse chart. The Tree-LSTM network utilizes an attention mechanism to fully exploit the parse chart, improving performance in natural language processing tasks. It generalizes the LSTM to tree-structured topologies, showing effectiveness in semantic relatedness and sentiment analysis. The Tree-LSTM network outperforms standard LSTM in semantic relatedness and sentiment analysis tasks but requires extra labelling of input sentences with parse trees. A proposed approach includes a fully differentiable chart parser in the model, allowing for end-to-end training for downstream tasks. The model can be trained end-to-end using stochastic gradient descent for downstream tasks. It outperforms baseline Tree-LSTM architectures on textual entailment and reverse dictionary tasks. An attention mechanism is introduced for the model, attending over all possible subspans of the source sentence via the parse chart. The work is part of a wider class of sentence embedding models guided by a tree structure. BID27, BID32, BID33, and BID20 propose models for composition using different approaches such as vectors, tensors, and recursive neural networks. They all rely on external trees or parse forests for syntactic information. BID23 also utilizes syntactic information and convolutional neural networks for composition. BID23 proposes a model using syntactic information and convolutional networks for bottom-up representation. BID3 introduces an RNN for sentence embeddings optimized for downstream tasks. BID39 shows similarities to the proposed model. In the second group, BID39 is similar to the proposed model, using reinforcement learning to learn tree structures for a neural network model. Other models in the group take different approaches, such as formalizing a dependency parser as a graphical model. All models take a sentence as input and output a sentence representation. The LSTM architecture is a popular choice for sentence encoding, using input, forget, and output gates to compute a final representation of the sentence. It deviates slightly from the vanilla LSTM by incorporating a candidate update vector. Tree-LSTMs are an extension of the LSTM architecture for tree structures, specifically designed for binary constituency trees. The representation of a node with children is computed recursively following the tree structure, with the whole sentence's representation given by the root node's h-state. A bias of 1 is added to the forget gates, similar to the LSTM implementation. Our proposed extension optimizes the Tree-LSTM by including a differentiable CYK-style chart parser in the model, eliminating the need for a parse tree structure as input. The chart stores all possible binary parse trees of a sentence efficiently, using a grammar with a single non-terminal. The Tree-LSTM utilizes a chart to efficiently represent binary parse trees of the input sentence. Each cell in the chart contains a pair of vectors (h, c) representing the Tree-LSTM RNN state at that node. The process starts at the bottom row with the leaves of the parse tree and moves up by computing nodes with the appropriate children. The Tree-LSTM RNN state is computed by combining nodes with appropriate children, starting from the bottom row with the leaves of the parse tree. Ambiguity arises in building constituents, leading to multiple compositions and candidate constituents with assigned energies. The Tree-LSTM RNN state is computed by combining nodes with appropriate children, starting from the bottom row with the leaves of the parse tree. Ambiguity arises in building constituents, leading to multiple compositions and candidate constituents with assigned energies. Each candidate is assigned an energy based on cosine similarity and weights, passed through a softmax function for normalization, and the final cell representation is calculated as a weighted sum of all candidates using the softmax output. The process is repeated for all higher rows, with the final output given by the top cell's h-state. The final output of the Tree-LSTM RNN state is determined by the top cell's h-state. Different ways of obtaining the final sentence representation are shown in FIG0. The models are implemented in Python 3.5.2 with the DyNet neural network library BID26. Test set performance is reported for the best-performing model on the development set. The code for the experiments will be available on the first author's website shortly after the article's publication date. The textual entailment model and reverse dictionary model were trained on different hardware, with the former taking three days and the latter taking five days to converge. Two additional Tree-LSTM models with fixed composition orders were also trained. The models were tested on the Stanford Natural Language Inference task BID2, which involves predicting entailment, contradiction, or neutrality between pairs of sentences. The model used 100D input embeddings initialized with GloVe vectors and fine-tuned during training. The Tree-LSTM model used parse trees from the dataset and Adam optimization algorithm with a batch size of 16. The accuracy and number of parameters for the model were compared to baselines and other sentence embedding models in the literature. The model utilized 100D input embeddings initialized with GloVe vectors and fine-tuned during training, along with parse trees from the dataset and Adam optimization algorithm. Attention mechanism was incorporated to soft-search for relevant parts of a sentence, enhancing performance in various linguistic tasks. The LSTM model was modified to return outputs for all time steps, resulting in improved vector representation. The attention mechanism was extended to the Unsupervised Tree-LSTM, where attention is over the whole chart structure instead of all words in the source sentences. Results for the attention-augmented models are reported in Table 4, showing median rank and accuracies on the reverse dictionary task. The study also evaluates the model on the reverse dictionary task of BID9, using 852k word-definition pairs. Performance is measured by the median rank and accuracy of retrieving the correct word from a list based on its definition. The model uses 500D CBOW vectors as output embeddings and the same vectors as input embeddings. The authors used 500D CBOW vectors BID25 for input embeddings, reduced to 256 dimensions with PCA. The model produces an embedding s \u2208 R 256, mapped to the output space via a trained projection matrix W \u2208 R 500\u00d7256. The training objective is to maximize cosine similarity cos(Ws, d) between the definition embedding and the output embedding d of the word being defined. The supervised Tree-LSTM model parsed definitions with Stanford CoreNLP to obtain parse trees and used stochastic gradient descent for training. The model performance was optimized by keeping the parameters constant. Results in Table 4 compare our model with baselines and BID9's \"w2v\" models. Our models have varying parameter sizes, with the Unsupervised Tree-LSTM outperforming baselines in textual entailment tasks. The Unsupervised Tree-LSTM model outperformed other sentence embedding models with more parameters by using attention over all possible subspans. In the reverse dictionary task, the supervised Tree-LSTM struggled due to unusual tokenization in the dataset. Despite slower training time, the model showed robustness to noisy data and required fewer training examples to reach convergence compared to LSTM and other baselines. The Unsupervised Tree-LSTM model surpassed other sentence embedding models in performance by utilizing attention over all subspans. It required fewer training examples to reach convergence and showed robustness to noisy data. The model's learned trees were manually inspected to compare with conventional syntax trees, revealing some deviations in verb phrase structure. The fully differentiable model presented jointly learns sentence embeddings and syntax using Tree-LSTM composition function. An attention mechanism over the parse chart improves performance for textual entailment tasks. The model is easy to train with popular deep learning toolkits like DyNet and PyTorch. The unsupervised Tree-LSTM can be enhanced by combining it with other models, particularly in the function assigning energy to alternative constituents. In future work, improving performance by using more complex functions like tracking LSTM in BID3 and techniques such as batch normalization BID11 or layer normalization BID0 could be explored. Training models on multiple tasks instead of a single one may lead to obtaining trees closer to human intuition, an important feature for intelligent agents to demonstrate BID21. Elastic weight consolidation BID19 could also help with multitask learning and be applied to the model."
}