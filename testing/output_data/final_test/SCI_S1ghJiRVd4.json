{
    "title": "S1ghJiRVd4",
    "content": "We present a weakly-supervised data augmentation approach to improve Named Entity Recognition (NER) in the biomedical domain by training a neural NER model over a small seed of fully-labeled examples, using a reference set of entity names for high precision identification, assigning weak labels to the corpus, and iteratively retraining the model for refined labels. This process significantly enhances NER performance in extracting biomedical entities from scientific literature. In this paper, augmented bootstrapping methods are explored to leverage noisy labels from unlabeled data in the biomedical domain. Named Entity Recognition (NER) is crucial for extracting important concepts like entities and their relations from text. Reliable NER is essential for identifying named entities such as proteins in the biomedical literature. The text discusses a framework for training neural NER models in the biomedical domain using partially labeled data. It involves creating an augmented training set from a small fully-labeled seed set and an unlabeled corpus set, refining labels through an iterative process. The approach aims to improve NER performance in a challenging domain where labeling is expensive. An analysis of reference-based automated approaches to labeling data for NER systems in the biomedical domain. Techniques involve using knowledge bases and linguistic features to tag entities, combining knowledge from external references with predicted labels, and refining them iteratively. The approach aims to improve NER performance by labeling data with low accuracy using heuristic-based functions. Our proposed iterative training technique aims to improve NER systems in the biomedical domain by making the learning process more robust to noisy labels. It is based on bootstrapping approaches used in previous studies for word-sense disambiguation, NER, language processing, and image classification. We introduce an augmented bootstrapping technique for neural NER models applied to biomedical literature. The study introduces an augmented bootstrapping technique for neural NER models in biomedical literature. Soft label values are refined for noisy data, and a domain-independent data annotation scheme is used to augment the bootstrapping process. The goal is to leverage external information to reduce the need for a fully-labeled dataset. The study proposes an iterative solution to improve NER by combining seed and augmented datasets, achieving high performance with fewer labels. Soft scores are used to label the corpus set instead of binary labels. The process involves training a NER model with a small seed dataset, using it to label the unlabeled corpus, and refining labels iteratively. The study proposes an iterative solution to improve NER by combining seed and augmented datasets, achieving high performance with fewer labels. Recent high-performing neural NER models use Bi-directional LSTM layers trained on character and word embeddings. Soft scores are used to label the corpus set instead of binary labels, allowing the model to reinforce weak signals and improve label quality. The approach replaces the CRF layer with a softmax layer to produce soft scores for each tag, showing effectiveness in extracting protein mentions from biomedical literature. The study evaluates different techniques for extracting protein mentions from biomedical literature using the BioCreative VI Bio-ID dataset. The dataset contains annotated figure captions from 570 articles, split into training, development, and test sets. The experiments aim to simulate data augmentation by searching for relevant bioentities in a large corpus like PubMed Central. Three main techniques are evaluated for this purpose. The study evaluates techniques for extracting protein mentions from biomedical literature using the BioCreative VI Bio-ID dataset. Three main techniques are evaluated, including using a reference set of entity names, predicting labels for unknown tokens, and refining label predictions iteratively. Experimental evaluation shows Precision, Recall, and F1 scores over the Bio-ID test set for different conditions. The NNER system trained over the full Bio-ID dataset achieves F1 of 82.99% (BiLSTM) and 83.34% (BiLSTM-CRF), simulating performance over a large amount of labeled data. Experiment 3 evaluates a baseline NNER system trained on 3% of the Bio-ID dataset combined with one true protein label per sentence for the remaining 97%, resulting in a 60% reduction in protein labels. Experiment 4 adds a CRF to the architecture, increasing the F1 score by 9 points to approximately 58%, despite a decrease in precision. In experiments 5 and 6, the iterative label refinement method is shown to significantly improve performance. Training on only 3% of the data already achieves good F1 scores for BiLSTM and BiLSTM+CRF architectures. Iteratively retraining the system further improves accuracy, resulting in higher F1 scores. The iterative label refinement method significantly improves performance, reducing the distance to the 100% trained system from 25 to 4 percentage points. The evolution of the procedure is shown in TAB1, with NNER-3% used for predictions and gradual improvements in subsequent iterations. Experiment 7 involves searching for exact mentions of protein/gene names in a large corpus for training. The text chunk discusses the poor performance of a selection technique for identifying true proteins in a large corpus. Despite using an iterative training technique, the precision and recall remain low. To reduce noise, the search procedure was refined by filtering out ambiguous protein names and those appearing in an English dictionary. The text chunk describes the improved precision and recall achieved by refining the search strategy for identifying proteins. By filtering out short protein names and using case-insensitive matching, the method achieved a precision of 90.20% and a recall of 39.35%. The augmented training dataset combined seed, reference-set, and predicted labels, resulting in a F1 score of 76.63% for BiLSTM and 77.70% for BiLSTM+CRF. This approach shows promising results with a small labeled dataset, approaching the performance of systems trained with significantly more data. Our method combines bootstrapping and weakly-labeled data augmentation to improve NER with limited labeled data in technical domains like biomedicine. Experimental evaluation shows performance equivalent to systems trained with significantly more labeled data. Future work includes exploring additional augmentation methods over challenging datasets and applying findings to improve NER performance in larger real-world scenarios using all available labeled data."
}