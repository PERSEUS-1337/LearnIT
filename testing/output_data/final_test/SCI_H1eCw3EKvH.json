{
    "title": "H1eCw3EKvH",
    "content": "Reinforcement learning (RL) is commonly used in text generation tasks like machine translation (MT) with methods such as Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, the effectiveness of RL in MT is limited as it does not always optimize expected rewards and some methods take too long to converge. RL practices in MT may only enhance performance when pre-trained parameters are already close to producing the correct translation. The findings suggest that gains in reinforcement learning for text generation may be due to changes in the distribution curve shape. RL is beneficial for Machine Translation as it optimizes non-differentiable score functions and addresses exposure bias. Various policy gradient methods like REINFORCE and Minimum Risk Training are commonly used in RL for text generation tasks. Theoretical analysis shows common approximation methods in RL may not minimize risk. Performance gains in text generation likely due to changing output distribution peakiness, not target token probability. In text generation, promoting the target token to be the mode may take a long time. Improvements are only likely when the target token is among the most probable tokens. REINFORCE and CMRT may improve over pre-trained models only in optimal conditions. Other RL practices in machine translation should be avoided, and alternative approaches are discussed to tackle errors in pre-trained models. In text generation, the probability of generating a token is based on the source sentence and model parameters. The score for generating a token is determined by the reference sentence. Training is typically done against a single reference, with rewards approximated using MonteCarlo methods or given at the end of the episode. The goal of RL is to improve text generation. RL, specifically REINFORCE, is used in text generation tasks to maximize the expected episode reward by updating model parameters based on sampled tokens from the probability distribution. This method allows for stochastic gradient ascent on the reward function and has been applied in various tasks such as machine translation and image-to-text generation. REINFORCE is commonly used in text generation tasks, including machine translation, with recent work focusing on adversarial training and RL for NMT. Minimum Risk Training (MRT) is applied ambiguously in MT, either using REINFORCE to minimize risk or Contrastive MRT (CMRT) for a different estimation method. CMRT, proposed by Och (2003) and adapted to NMT, involves sampling at each iteration. Contrastive MRT (CMRT) involves sampling tokens at each iteration and updating \u03b8 based on the gradient. Despite similarities in definitions, CMRT and REINFORCE differ in key aspects related to support and smoothness parameters. CMRT does not optimize R or E[R]. Despite its popularity, CMRT does not optimize R or E[R], making it theoretically ill-founded. The gradient of R is influenced by the contrastive effect in CMRT, which may improve convergence rates. CMRT with deduplication sums over distinct values in S, while REINFORCE sums over all values. In CMRT, the gradient of R is influenced by the contrastive effect, potentially improving convergence rates. REINFORCE, on the other hand, gives more weight to improbable tokens with \u03b1 < 1. Tuning \u03b1 is crucial for the convergence of REINFORCE in NMT. Implementing stochastic gradient ascent, REINFORCE converges to a stationary point of R. However, its convergence rate in NMT conditions is not well understood. When using RL for text generation, the peaky distributions learned by RNNs can slow down convergence rates. REINFORCE increases the probabilities of rewarding observations but may limit exploration of other tokens. Sampling unique tokens from the policy distribution can be challenging, especially with a limited number of epochs. The use of REINFORCE in text generation with RNNs can lead to peaky distributions, slowing down convergence rates. This peakiness-effect (PKE) causes the most probable tokens to initially gain probability mass, limiting exploration of other tokens. Over iterations, more rewarding tokens are eventually sampled, but training remains sample-inefficient. This is empirically assessed in \u00a75, showing slow convergence rates. The rate of convergence is empirically assessed in \u00a75, showing that updates are more likely to increase the total predicted probability of the most probable tokens. Decreasing the temperature for the prediction softmax can lead to performance gains similar to those of GANs. This effect is attributed to an underlying mechanism, which is relevant for RL use. The study suggests that the results are applicable to reinforcement learning in other tasks with discrete, high-dimensional output spaces. They used a 1-layer softmax model to predict tokens, similar to NMT decoders. The model was trained on Transformer NMT system parameters and WMT2015 data. Hyperparameters are provided in the appendix. The study experimented with two reward functions for token prediction: Simulated Reward and Constant Reward. The Simulated Reward assigned higher rewards to specific tokens, while the Constant Reward gave a constant reward of 1 to all tokens. Experiments were conducted multiple times with different settings to evaluate the effectiveness of the reward functions. The study compared two reward functions for token prediction: Simulated Reward and Constant Reward. It was found that running more repetitions rather than more steps per initialization was more effective. Sampling 10,000 pretrained distributions and performing a single REINFORCE step was sufficient. The estimated number of steps used in practice was around 1M, with similar trends observed in experiments with 50K-100K steps. The peakiness of a distribution was evaluated based on the probability of the most probable token. The study compared two reward functions for token prediction: Simulated Reward and Constant Reward. Running more repetitions was found to be more effective than more steps per initialization. Peakiness of the distribution increased with the probability of the most probable token and entropy decreased. The entropy decreased from 2.9 to 2.85 after one REINFORCE step in the pretrained model. In the Simulated Reward setting, entropy decreased from 3 to about 0.001 in 100K steps, indicating a deterministic policy. PKE was achieved in a few hundred steps. The study compared two reward functions for token prediction: Simulated Reward and Constant Reward. Peakiness of the distribution increased with the probability of the most probable token and entropy decreased. In a real-world application of REINFORCE to NMT, it is rare for REINFORCE to sample from the same conditional distribution more than a handful of times. In NMT, the reward function is defined based on expected BLEU score. Early stopping is used with a patience of 10 epochs. Results show an increase in the peakiness of conditional distributions. The study analyzed conditional distributions in NMT models, with results showing an increase in peakiness after reinforcement learning. The modes of the distributions shifted towards higher probabilities, leading to a decrease in average entropy. This suggests that the optimization procedure may not have converged to the optimal value for \u03b8. In assessing the conditions for improvement in NMT system performance, controlled simulations and NMT experiments were conducted using a specific model and setup. Results were averaged over 100 conditional distributions, caution is advised when determining the learning rate, with a LR of 0.1 yielding the best results in simulations. A higher learning rate led to faster convergence due to noise-free rewards always favoring the best option. Increasing the learning rate in NMT experiments can lead to overfitting and early stopping, preventing parameter updates. Even with a high learning rate, it may be challenging to make the desired output the mode of the distribution within a certain number of steps. After training an NMT system with expected BLEU reward, a minor improvement was observed in BLEU scores over the news2014 test set. The RL procedure influenced the ranks of target tokens, with about half not among the top three choices of the pretrained model. Comparing ranks between the reinforced and pretrained models showed differences in probability assigned to the target tokens. Results show that RL influenced the ranks of target tokens, with more tokens ranked first and less second. However, there was little consistent shift in probability mass across the top ten ranks. RL may have pushed some tokens from very low ranks to medium-low ranks, but the impact on system outputs is minimal. Simulations suggest that only initially top-ranked tokens are likely to change. A constant reward experiment also resulted in improved BLEU scores, indicating a similar pattern of improvement. The experiments with contrastive MRT showed that it does not maximize R and lacks theoretical guarantees like REINFORCE. Despite this, it is a common RL procedure in recent work. The simulations with CMRT using \u03b1 = 0.005 and k = 20 were repeated to assess its performance in these conditions. In experiments with contrastive MRT using \u03b1 = 0.005 and k = 20, CMRT's performance was evaluated. Results showed that MRT succeeds in promoting the second initially most probable token to the highest rank but struggles with tokens initially ranked third or lower. There was only a small PKE observed in MRT, likely due to the contrastive effect. The simulations allowed for sampling the same token more than once in each batch. The paper discusses how the distributions used in NMT make it difficult to promote the target token efficiently using RL. It suggests that improvements from using RL in NMT may be due to fine-tuning probable tokens in the pretrained model or unrelated effects like PKE. The paper also argues that CMRT does not optimize expected rewards and may not be the reason for observed improvements in NMT experiments. The paper discusses the challenges of using RL in NMT due to distribution issues. It suggests that PKE may play a role in disallowing learning when the reward is centered around zero. Additionally, peakiness in the reinforced model is expected to improve BLEU scores. Controlled simulations show asymptotic convergence is difficult to achieve, and gradient clipping may hinder convergence in NMT. The challenges of using RL in NMT are discussed, with PKE potentially hindering learning when rewards are near zero. Peakiness in the reinforced model is expected to boost BLEU scores, but gradient clipping may impede convergence. Per-token sampling in experiments is more exploratory than beam search, reducing PKE. Off-policy parameter updates in beam search may lead to instability in training. Adding a reference to the sample S could help address issues of never sampling target tokens, but it may lower results by destabilizing training. The standard MT scenario presents unique challenges for RL due to the action space. The standard MT scenario poses challenges for RL with high-dimensional discrete action spaces, sparse rewards, and the use of pretrained models. Training on-policy with peaky pretrained models can be problematic. Off-policy methods in RL, where observations are sampled from a different policy than the one being optimized, are beneficial for NMT. These methods allow learning from a more exploratory policy and can help address weaknesses in current RL practices for NMT. In CMRT, using \u03b1 for smoothing is a key motivation. Off-policy sampling allows for smoothing while maintaining convergence guarantees. Exploration in REINFORCE relies on stochasticity in action-selection, with more advanced methods like diversity-based and multi-goal approaches proposed for RL challenges. Adoption of these methods shows promise for future progress. Contrastive MRT is a promising method for applying RL in NLP. By optimizing \u03b8 within a specific range, the method converges to a certain value \u03b3. The gradient of the expected reward over possible samples is positive for \u03b8 \u2208 (0, \u03b3) and negative for \u03b8 \u2208 (\u03b3, 0.5]. With proper initialization and a small learning rate, Contrastive MRT will converge to \u03b3. The text discusses the use of g(\u03b8) to manipulate the value of \u03b3 independently from \u03b8. It also mentions the use of E[R] for optimization, which does not maximize R(\u03b8). Various tokenization methods were used, including BPE with 30,715 tokens. MT experiments utilized 6 layers in the encoder and decoder with 512 embedding size. Gradient clipping was applied during pre-training. Gradient clipping with size 5 was used during pre-training. Attention dropout was not utilized, but a residual dropout rate of 0.1 was applied. Sentences exceeding 50 tokens were discarded in both pretraining and training. Training was considered complete when BLEU did not improve for 10 consecutive evaluations. Learning rates of 0.01 for rmsprop in pretraining and 0.005 for adam with decay in training were used. Pretraining lasted 7 days with 4 GPUs, followed by a similar duration for training. Monte Carlo employed 20 sentence rolls per word. Graphs for constant reward setting are shown in Figures 8 and 7. The text discusses trends in Figures 8 and 7 related to the probability differences between the reinforced model and pretrained model for ranking best choices."
}