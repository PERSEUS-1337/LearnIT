{
    "title": "HJvvRoe0W",
    "content": "The folding structure of the DNA molecule, known as chromatin, is crucial for its functional properties. A convolutional neural network was developed to predict key determinants of chromatin structure using an image representation of the DNA sequence. The method can detect interactions between distant elements in the sequence, outperforming existing methods in accuracy and training time. DNA is a sequence of nucleotides {A,C,G,T} that serves as a blueprint for cellular processes. The spatial organization of DNA is achieved by integrating histone proteins to form chromatin, which defines the functional properties of local areas of the DNA. Chromatin can assume different epigenetic states based on sequential patterns in the DNA sequence, which determine the chromatin configuration and its interplay with the raw DNA. The chromatin configuration and its interplay with the raw DNA sequence are actively researched. Despite recent findings, a comprehensive understanding has not been reached. Prediction methods for chromatin states from DNA sequence are of interest, with deep neural networks showing promise in various applications, including biology. DNA's spatial configuration and interaction between distant elements are important, and a deep neural network designed for long-term interactions may improve performance. A neural network designed for long-term interactions can improve performance by considering the molecular spatial configuration of DNA. Techniques such as using larger convolution filters early in the network can help detect long-term interactions without the need for fully connected layers with a large number of parameters. The proposed approach involves using a deep network design like ResNet BID14 or Inception BID27 to prevent features from vanishing in early layers. Additionally, a novel DNA representation method maps sequences to higher-dimensional images using space-filling curves, specifically the Hilbert curve, to maintain proximity between sequence elements. Hilbert curves ensure that close sequence elements are also close in the image, aiding in mapping short-term sequential relationships in DNA. This representation method is beneficial for capturing chromatin structure and organizing DNA sequences into two-dimensional images. Results from BID10 show that arranging DNA sequences based on Hilbert curves helps map chromatin states in contiguous rectangular areas. Applying convolutional layers on these curves aids in capturing long-term interactions, such as enhancers and silencers, affecting gene activity. Watson and Crick's discovery of the double-helix model in 1953 laid the foundation for this research. DNA sequence classification is a fundamental task in bioinformatics research, with methods ranging from statistical learning to deep neural networks. Some studies use support vector machines to predict chromatin state from DNA sequence features, while others utilize random forests for feature selection. Only one study uses a CNN for this purpose. Our model architecture for predicting chromatin state differs from previous studies. We use a deeper network with residual connections and larger convolution filters, while also transforming sequence data into an image-like tensor using a space-filling curve. This approach contrasts with previous studies that kept the sequential form of input data. Our study introduces a novel method using CNNs and Hilbert curves to predict chromatin state from DNA sequences. The developed CNN outperforms existing approaches in terms of prediction performance and runtime, showcasing the benefits of transforming DNA sequences into 2D image-like arrays. The method involves transforming DNA sequences into images using CNNs and Hilbert curves. This process includes representing sequences as k-mers, converting k-mers into one-hot vectors, and creating an image-like tensor. Nucleotide motifs are emphasized for protein synthesis, with k-mers playing a crucial role in bioinformatics and q-grams in computer science. In computer science, q-grams are commonly used in text mining. DNA sequences are transformed into k-mers, with 3-mers and 4-mers being useful for predicting epigenetic states. A k value of 4 is found to yield the best performance. Word embeddings like GloVe or word2vec are used in natural language processing, with one-hot vectors being most suitable for the method described. Each element in the vector corresponds to a word, with a vector length of N. To represent k-mers in a DNA sequence, one-hot vectors of length 4k are used, with each element corresponding to a word. These vectors are then transformed into an image by assigning each vector to a pixel, creating a 3-dimensional tensor similar to those used in image classification networks. To represent k-mers in a DNA sequence, one-hot vectors of length 4k are used, transformed into an image by assigning each vector to a pixel. Space-filling curves like the Hilbert curve map 1-dimensional sequences to a 2-dimensional surface, with Hilbert curves showing the best performance for this purpose. The Hilbert curve is constructed recursively, dividing into four parts in each iteration. The Hilbert curve divides into quadrants, each further divided into sub-quadrants holding fractions of the curve. To fit all k-mers into the image, n is chosen such that 2n * 2n accommodates the number of k-mers. Cropping the unused pixels yields rectangular images. The image is cropped to remove unused parts, resulting in rectangular images. Sequences of 500 base pairs are converted to 497 4-mers, requiring a Hilbert curve of order 5. The resulting image is 32x32x256 pixels, with half filled and half empty pixels removed to create a 16x32x256 image for input into the model. Each pixel is assigned a one-hot vector, different from traditional CNNs focusing on grayscale or RGB images. In our approach, each pixel in the generated image is assigned a one-hot vector representing a k-mer. Using k = 4 results in 256 channels, reducing sparseness. A specific CNN architecture inspired by ResNet and Inception is designed for high dimensional images from DNA sequences, with layers implementing non-linear functions. The network aims to reduce sparseness in input images and enhance CNN's ability to extract relevant features from DNA space-filling curves. It includes consecutive layers [c, bn, c, bn, af, p], a Computational Block inspired by ResNet, and 3 fully-connected layers with softmax for classification. The model details are in TAB1, and code is available on Github. The Computational Block involves summing outputs of Residual Blocks and identity mapping, followed by bn and af layers. The computational block consists of 4 convolutional layers, with two in each Residual Block. The Residual Block computes a composite function of five consecutive layers and concludes with an activation function. Most layers use small filters, except for the initial layers which use large filters. Exponential Linear Units (ELU) are used as the activation function to prevent gradient vanishing. Average pooling is used over Max pooling for better prediction accuracy. The approach improved prediction accuracy by more than 2% by reducing variance in sparse images. Cross entropy was used as the loss function for testing on ten datasets from BID23, each containing DNA subsequences labeled as \"positive\" or \"negative\" based on histone protein presence. Training used 90% of data, with 5% for validation and evaluation. The network was trained using AdamOptimizer with a learning rate of 0.003, batch-size of 300, and maximum epochs of 10. The study uses early stopping to prevent overfitting in the model training process. They combine GL \u03b1 measurement and the No-Improvement-In-N-Steps method to determine when to stop training. The performance of their HCNN approach is compared to SVM and Seq-CNN models. The LSTM model uses a 4-mer profile as input, with only the 100 most frequent 4-mers included to prevent overfitting. The seq-HCNN model compares a 2D representation of DNA sequences to a sequential representation, omitting the mapping of the sequence into a 2D image. The Seq-CNN model has 1.1M parameters, while HCNN and seq-HCNN have 961K parameters, and LSTM has 455K parameters. Additional tests were conducted on splice-junction gene sequences dataset from BID18, where DNA subsequences of length 61 were classified as intron-to-exon splice-junction, exon-to-intron splice junction, or neither using 1-mers due to the small dataset size. The results show that HCNN and seq-HCNN outperformed SVM and Seq-CNN in terms of prediction accuracy on a small dataset with 1-mers. HCNN showed more consistent results over ten folds and better performance in terms of precision, recall, AP, AUC, and training time compared to seq-HCNN. The good performance of HCNN may be attributed to the conversion from DNA sequence to image or the use of the Hilbert curve. In this study, the performance of different space-filling curves was compared for prediction accuracy. The Hilbert curve showed the best accuracy in generating images for various datasets. A CNN model developed in the study outperformed existing methods for predicting epigenetic states from DNA sequence. The splice dataset showed that Seq-CNN performed best with 4-mers, while HCNN and seq-HCNN performed best with 1-mers. HCNN and seq-HCNN performed best with 1-mers for predicting chromatin state and splice-junctions in DNA subsequences. The use of Hilbert curves in the model enhances its capabilities for DNA sequence classification. Comparing Hilbert curves to other space-filling curves showed good results. The main limitation of Hilbert curves is their fixed length, resulting in empty spaces in generated images. Despite this, the 2D representation reduces training times compared to 1D sequences, likely due to optimization in CNN frameworks. Further investigation is planned to understand how the architecture components contribute to performance improvements and why Hilbert curves offer advantages in robustness and false discovery control. In this section, long-term interactions in DNA sequences are considered using four space-filling curves: reshape curve, snake curve, Hilbert curve, and diag-snake curve. Mapping sequences to images reduces distance between far elements without increasing distance between nearby elements. Different curves have varying effects on distance between far elements, assessed using a measure denoted as L C (x,y) where x, y \u2208 S, with S as the sequence and C as the curve. The Hilbert curve yields the highest values for \u0393(C) 6, indicating strong long-term interactions in DNA sequences. This curve performs best in retaining long-distance relationships compared to other space-filling curves. The Hilbert curve shows strong long-term interactions in DNA sequences, outperforming other space-filling curves. Optimization of hyperparameters, such as network architecture and learning rate, was based on maximizing accuracy. Large kernel size for the first layer and smaller kernel size for subsequent layers were chosen to capture long-term interactions effectively. To reduce parameters, residual blocks inspired by ResNet BID14 and Inception BID27 were used, along with batch normalization to prevent overfitting."
}