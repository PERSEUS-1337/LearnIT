{
    "title": "Sklsm20ctX",
    "content": "Deep learning has made significant progress in solving reinforcement learning problems with dense rewards, but struggles in sparse reward environments. A new method called competitive experience replay aims to address this by creating an exploration competition between agents to supplement sparse rewards. This approach complements existing methods like hindsight experience replay. In robotics and deep reinforcement learning, a new method called competitive experience replay enhances exploration by augmenting sparse rewards for agents. This approach improves task performance and speeds up convergence in tasks like reaching goal locations in an ant maze and manipulating objects with a robotic arm. In robotic control, developing algorithms that can learn from binary signals or unshaped reward signals is crucial in environments with sparse rewards. This helps reduce sample complexity and optimize policy in tasks where dense reward functions are not available. In goal-directed RL, the Competitive Experience Replay (CER) method addresses sample inefficiency by introducing competition between two agents to emphasize exploration. Agent A is penalized for visiting states also visited by agent B, while B is rewarded for visiting states found by A, leading to biased exploration towards task goals. The Competitive Experience Replay (CER) method generates a curriculum of exploration through competition between agents, shaping sparse rewards. Two versions of CER, independent and interact, differ in agent B's state initialization. Unlike HER, CER re-labels samples based on intra-agent behavior, allowing easy combination during training. Evaluation on various RL tasks shows effectiveness with and without HER. Reinforcement learning involves tasks like navigating an agent and manipulating objects with a robotic arm. The default reward is sparse, indicating goal completion. Ablation studies show the method's importance for high success rates and faster convergence. CER and HER are complementary methods, with CER outperforming curiosity-driven exploration when combined with HER. The text also introduces concepts for reinforcement learning with sparse rewards, Deep Deterministic Policy Gradient, and Hindsight Experience Replay. The text discusses goal-conditioned reinforcement learning in uncertain environments, where an agent aims to maximize cumulative rewards. It involves a Markov decision process with transition probabilities and reward functions. The modified reward function depends on a fixed goal for each episode, with actions chosen based on a policy. The text discusses goal-conditioned reinforcement learning in uncertain environments, where an agent aims to maximize cumulative rewards by optimizing parameters \u03b8 with respect to the expected cumulative reward. The goal is to reach a target state described by a sparse reward function, with a predefined threshold for goal completion. The policy is modeled as a conditional probability distribution over states and goals, with a discounted state visitation distribution used for optimization. The text introduces the Deep Deterministic Policy Gradient (DDPG) algorithm for continuous action spaces in goal-conditioned reinforcement learning. It maintains a deterministic target policy and a critic implemented as deep neural networks. The proposed modifications are not limited to DDPG, allowing for experimentation with other continuous control algorithms in the future. The Deep Deterministic Policy Gradient (DDPG) algorithm involves generating episodes by sampling actions from a policy with added noise. Transition tuples are stored in a replay buffer for training, where mini-batch gradient descent is performed on the loss function to update the Q-function and actor. Target values are computed using a separate target network to stabilize training. Despite advancements in deep learning for reinforcement learning, challenges persist. Despite advances in deep learning for reinforcement learning challenges, sparse rewards remain a major obstacle. BID0 introduced Hindsight Experience Replay (HER) to address this issue by treating failed rollouts as successful, leading to a relabelling strategy. HER stores each episode in the replay buffer twice, once with the original goal pursued and once with a future state as the goal. This technique aims to increase variability in rewards and improve learning efficiency. Competitive Experience Replay (CER) is introduced for policy gradient methods to address exploration challenges in goal-directed reinforcement learning with sparse rewards. Inspired by HER, CER relabels transitions to improve training efficiency and generalization to actual task goals. Our method, Competitive Experience Replay (CER), involves training a pair of agents on the same task using asymmetric reward relabelling to induce competition and encourage exploration. Each agent learns a policy (\u03c0 A and \u03c0 B) and a multi-agent critic, collecting DDPG episode rollouts in parallel for decentralized execution. The rollouts are paired and stored as multi-agent data in the replay buffer D for training. Reward re-labelling in CER creates an exploration curriculum by penalizing agent A for visiting states also visited by agent B and rewarding agent B for the same states. Each transition for agent A is penalized once, while agent B can receive additional rewards. The policy \u03c0 A is retained for evaluation after training both agents with the re-labelled rewards. Additional implementation details are available in the appendix. In CER, two variations of reward re-labelling are used: independent-CER and interact-CER. CER focuses on intra-agent behavior, while HER focuses on individual agent behavior. Both methods can be combined and are complementary. MADDPG is extended for training using CER. MADDPG, proposed by BID26, trains using CER by learning a different policy per agent and a centralized critic. The centralized critic has access to the combined states, actions, and goals of all agents. The gradient of the expected return for each agent is calculated with deterministic policies. The centralized action-value function Q estimates the expected return for each agent and is updated accordingly. Target policies with delayed updates are used in the process. The MADDPG algorithm uses target policies with delayed updates and CER to train individual agent policies and a centralized critic. The algorithm collects paired rollouts, applies re-labelling strategies, and concatenates states, actions, and goals. Performance and sample efficiency are tested in sparse reward tasks using different mazes. In the new episode, agents receive rewards based on goal achievement. The ant maze environment can be reset to any state for comparison between different training methods. Results from agents trained with HER, CER, and CER with HER are compared, along with DDPG and PPO algorithms. Adding CER to DDPG improves success rates in the maze tasks. Adding both HER and CER to DDPG improves final success rates without increasing the number of episodes. Int-CER outperforms ind-CER, showing improved learning quality and efficiency in sparse reward environments. The study evaluates the impact of adding ind-CER to HER in multi-goal sparse reward environments. Results show improved performance on challenging tasks, especially where HER alone struggles. The method addresses the issue of insufficient exploration in existing methods by using a competitor agent's behavior to determine exploratory reward criteria. The study compares the performance of CER and ICM in robotic control and maze tasks. CER consistently outperforms ICM when implemented alone, and the interaction with HER is also observed. The study shows that CER tends to improve learning speed and benefits more from the addition of CER than ICM. CER provides a curriculum for exploration, leading to more efficient utilization of visited states. Success rates of agents A and B are illustrated, along with the 'effect ratio' calculated as \u03c6 = N/M. The study highlights the strong correspondence between success rate and effect ratio in the context of CER. It is noted that CER re-labels a significant portion of rewards in the mini-batch, leading to rapid learning even with a small effect ratio. Agent A shows a more rapid increase in success rate compared to agent B, possibly due to a more targeted re-labelling strategy. Agent B's lower performance may be attributed to periodic parameter resets during early training. The study discusses how CER asymmetrically benefits agent A in training, visualizing the visitation frequency in the 'U' maze. Comparing DDPG and HER, HER helps move the state distribution towards the goal, especially early on. CER focuses the agent's visitation, avoiding getting caught along outer walls, and creates a clear mode near the goal. The study compares visitation profiles of agents A and B in the 'U' maze, showing similarities with Agent A reaching the goal more often later in training. Self-play and curriculum learning are highlighted for their benefits in training neural networks. The study discusses using program search and replay buffer to select tasks, adjusting task difficulty automatically during training. It proposes sample-based competitive experience replay for high-dimension control, integrating with Hindsight Experience Replay. Other works explore initializing based on a state not sampled from the initial state distribution, creating a backwards curriculum for continuous control tasks. Salimans & Chen (2018) suggest training policies on Pommerman and 'Montezumas Revenge' by starting episodes from different points along a demonstration. Various methods, such as backtracking models and experience replay, have been proposed to improve sample efficiency and performance in learning tasks. Prioritizing transitions in the buffer and incorporating model-free learning with experience replay have also been explored to enhance efficiency in utilizing samples. Additionally, a distributed RL system has been proposed to share experiences between parallel workers for further improvement in experience replay. In a distributed RL system, experiences are shared between parallel workers and accumulated into a central replay memory. Competitive Experience Replay encourages exploration in sparse reward settings. Future work includes exploring ways to re-label rewards based on intra-agent samples and integrating the method into more challenging environments. Future work will explore integrating the method into model-based learning approaches. Performance depends on batch size, which is manipulated to tune the strengths of Agents A and B. The batch size is controlled by changing the number of MPI workers, with each worker computing gradients for a batch size of 256. Results suggest that a sufficiently large batch size is important for optimal performance. Results suggest that a sufficiently large batch size is crucial for optimal performance, with balanced batch sizes for both agents being the optimal configuration. The use of MPI in HER BID0 code allows for parallel rollouts and gradient averaging, leading to improved performance. Having a larger batch size is found to be beneficial, with N rollouts in parallel for each agent and separate optimizers. In training agents A and B, N rollouts are done in parallel with separate optimizers. Resetting agent B's parameters early on helps agent A achieve consistent high performance by balancing HER and CER influences. L2 regularization is added, parameters are randomly initialized, and neural networks with three layers of size 256 are used for optimization. Results are averaged over 5 random seeds."
}