{
    "title": "BJGMMlGKj7",
    "content": "Computations for the softmax function in neural network models can be expensive with a large number of output classes, impacting training and inference. The Doubly Sparse Softmax (DS-Softmax) method presented in this paper aims to improve efficiency by using a two-level class hierarchy. Each expert is responsible for a subset of the output class space, reducing computation during inference. This learning-based approach does not require prior knowledge of the output class partition space and shows significant computation reductions without loss in performance. In language modeling, a two-level overlapping hierarchy can increase efficiency by allowing words to belong to more than one cluster, accommodating word homonyms. This approach can be applied to other applications besides language modeling. Our DS-Softmax method can effectively capture hierarchies, illustrated in FIG1. We use sparse gates to select top experts, allowing for back-propagation of gradients. Normalization is done after expert selection, with a lasso threshold for training. The sparsity percentage in each expert is denoted as sparsity k. The DS-Softmax method captures hierarchies using sparse gates to select top experts for back-propagation. Utilization and sparsity percentages are crucial for speedup compared to full softmax. Mitosis training scheme reduces memory requirements by inheriting sparsity from parent experts. Initialization involves setting parameters for experts and gating network, with a target performance hyper-parameter. The training algorithm involves setting parameters for experts and gating network, with a target performance hyper-parameter. The method reduces memory requirements by inheriting sparsity from parent experts and gradually breeding to a bigger one after noisy cloning. The final training objective combines various contributions and is described in Algorithm 1."
}