{
    "title": "B1xJAsA5F7",
    "content": "We approach molecule optimization as a graph-to-graph translation problem, aiming to improve molecular properties by mapping one molecular graph to another. Our model includes a junction tree encoder-decoder for diverse graph translations and an adversarial training method to align molecule distributions. By using low-dimensional latent vectors to modulate the translation process, our model outperforms previous baselines in multiple optimization tasks. The goal of drug discovery is to design molecules with desirable chemical properties using matched molecular pair analysis (MMPA) to improve target chemical properties. This paper formulates molecular optimization as graph-to-graph translation, aiming to learn to translate input molecular graphs into better graphs. The proposed translation task faces many challenges. The proposed translation task involves challenges in generating graphs as output without domain-specific linearization. The goal is to learn multimodal output distributions over graphs using a refined graph-to-graph neural architecture. Stochastic latent codes are introduced to capture diverse molecular variations. The learning problem is formulated as a variational autoencoder, inferring the posterior over latent codes from input molecular pairs. An adversarial training method is proposed to avoid invalid translations. The proposed translation task involves generating graphs using a novel adversarial training method to align distributions of generated graphs with observed targets. The model excels in discovering molecules with desired properties, outperforming state-of-the-art methods. Our model is trained to translate molecular graphs into better graphs through supervised learning, which is more sample efficient than previous methods. Unlike matched molecular pair analysis (MMPA) in drug de novo design, our approach uses neural networks to learn transformation patterns without the need for explicit rules. Our work is related to graph encoders and decoders, focusing on learning generative models of graphs and multimodal graph-to-graph mappings. It is also closely related to image-to-image translation and text style transfer methods that adversarially regularize representations for end-to-end training. Our technical contribution is a novel adversarial regularization over graphs that constrains their scaffold structures in a continuous manner. The translation model extends the junction tree variational autoencoder to an encoder-decoder architecture for learning graph-to-graph mappings, interpreting molecules as built from subgraphs chosen from a vocabulary of valid chemical substructures. This approach allows for easy enforcement of chemical validity and provides an enriched representation encoding molecules at different scales. The encoder-decoder architecture for molecule representation includes a graph message passing network for encoding molecules at different scales. It features a unified encoder for trees and graphs, with an attention mechanism in the tree decoding process. Each node in the graph has a feature vector that includes atom type, valence, and other properties, while edges connect nodes in the graph. The junction tree utilizes one-hot vectors for cluster labels and feature vectors for edges. Hidden vectors represent messages between nodes, updated iteratively via neural networks. Message updates are asynchronous to avoid bias from artificial order. Aggregated messages derive the latent vector for each vertex, capturing its local graph or tree structure. The junction tree uses one-hot vectors for cluster labels and feature vectors for edges. Hidden vectors represent messages between nodes, updated iteratively via neural networks. Message updates are asynchronous to avoid bias from artificial order. Latent vectors for each vertex are derived to capture local graph or tree structure. The message passing network is applied to derive latent vectors for each vertex. Unit BID23: Topological Prediction involves computing predictive hidden states based on node features and messages, making binary predictions on node expansion. Label Prediction predicts the label of a new child node, and constructing a molecular graph from a predicted junction tree is a non-deterministic process. The graph decoder predicts correct attachments between clusters in a junction tree by scoring candidate attachments using a graph message passing network and dot products with encoded source graph vectors. The graph decoder is trained to maximize the log-likelihood of ground truth subgraphs at all tree nodes. It learns a multimodal mapping between molecules with different properties, such as solubility or potency. The model should be able to generate diverse modifications to increase solubility for a given molecule. To enhance solubility, a new molecule X requires a model that can generate diverse outputs. The proposed approach involves augmenting the encoder-decoder model with latent vectors z to encode output distribution variability. Challenges include ensuring meaningful variations in latent codes and proper regularization to prevent invalid translations. Two techniques are proposed to address these issues. To address challenges in generating diverse outputs for a new molecule X, two techniques are proposed. The first involves deriving a latent code z from the ground truth molecule Y to reconstruct Y efficiently. The second technique involves encoding structural changes between molecules X and Y using tree and graph vectors. The approximate posterior Q(\u00b7|X, Y ) is modeled as a normal distribution to sample latent codes z T and z G. The mean and log variance are computed from \u03b4 X,Y with affine layers \u00b5(\u00b7) and \u03a3(\u00b7). The latent codes are combined with perturbed tree and graph vectors to synthesize the target molecule Y using a conditional variational autoencoder. Adversarial scaffold regularization is used to ensure valid translations. The decoder transforms latent codes to match the target domain distribution through adversarial training. The discriminator distinguishes real molecules from fake ones, while the generator aims to create indistinguishable molecules. Adversarial regularization is applied to continuous representations of decoded structures to overcome gradient propagation challenges. The adversary matches the scaffold in the tree decoding step for efficiency. The decoder predicts label distribution and expands tree structures in tree decoding. Continuous representations are computed for molecules using latent codes and teacher forcing. The discriminator is updated through adversarial regularization to match scaffold structures efficiently. The decoder expands tree structures in tree decoding by predicting label distribution. Continuous representations for molecules are computed using latent codes and teacher forcing. The discriminator is updated through adversarial regularization to match scaffold structures efficiently. The junction tree T is translated and its continuous representation h is computed. The encoder/decoder is updated by minimizing node i t. Hidden messages are computed for true samples using teacher-forcing, while for translated samples, one-hot encoding is replaced with softmax distribution over cluster labels. Messages are multiplied by a binary gate to account for the topological layout of the tree. The text discusses the use of a hard sigmoid function for gradient approximation in neural network training with dynamic computational graphs. The continuous representation of a decoded tree is derived by combining the root label distribution and inward messages. The discriminator is implemented as a feedforward network and trained using Wasserstein GAN with gradient penalty. The algorithm is detailed in Algorithm 1 and evaluated on molecular optimization tasks. Graph-to-graph translation models are evaluated on molecular optimization tasks by sampling molecular pairs with property improvement and similarity. Tanimoto similarity is used to measure molecular similarity. The methods are tested on penalized logP optimization task with different similarity constraints. Training pairs are extracted from the ZINC dataset for evaluation. The task involves improving drug likeness and biological activity of compounds. For drug likeness, molecules with QED scores between 0.7 and 0.8 need to be translated into the higher range of 0.9 to 1.0. This is challenging as only the top 6.6% of molecules fall within the target range. A training set of 88K molecule pairs with a similarity constraint of 0.4 was extracted. The test set contains 800 molecules. The third task is to enhance a molecule's biological activity against the dopamine type 2 receptor (DRD2). The model aims to translate molecules with predicted probability less than 0.05 into active compounds with probability greater than 0.5. Active compounds represent only 1.9% of the dataset. With a similarity constraint \u03b4 = 0.4, a training set of 34K molecular pairs was derived from ZINC and BID36 datasets. The test set consists of 1000 molecules. Baselines compared include MMPA, Junction Tree VAE, and VSeq2Seq. The architecture of the VSeq2Seq model is related to their autoencoder model. Another baseline model, GCPN, is a reinforcement learning based model that modifies molecules by adding or deleting atoms and bonds. Both VSeq2Seq and their models use latent codes of dimension |z| = 8, with a KL regularization weight \u03bb KL = 1/|z|. The encoder and decoder configurations for VSeq2Seq are specified for fair comparison. The study compares VSeq2Seq and their models with around 4M parameters. Translation accuracy, diversity, and novelty are quantitatively analyzed. Translation accuracy is measured by decoding with different latent codes and reporting the molecule with the highest property improvement. Successful translations are defined based on property scores falling within target ranges. Our models outperform baselines like JT-VAE and GCPN due to training on parallel data, showing the advantage of molecular translation over rule-based methods. The graph-to-graph approach performs better than VSeq2Seq, with the proposed adversarial training method providing slight improvement. VJTNN+GAN is evaluated on QED and DRD2 tasks with defined target domains. The diversity of translated molecules is measured by the Tanimoto distance, with a maximum score of 0.6 on tasks like QED and DRD2. Our methods show higher diversity than MMPA and VSeq2Seq on multiple tasks. Additionally, our model excels in discovering new molecules in the target domain, crucial for drug discovery goals. The text discusses evaluating graph-to-graph translation models for molecular optimization. By combining the variational junction tree encoder-decoder with adversarial training, better and more diverse molecules can be generated compared to baselines. The graph encoder uses neural networks for parameterization, while the tree encoder utilizes a tree GRU function for stability. The text discusses the implementation of a tree GRU function for stability in computing messages in a graph neural architecture. The attention mechanism and graph decoder are also described for scoring candidate attachments in molecular optimization models. The text describes the need to consider subtrees when scoring attachments in a graph neural network. An index is defined to mark atom positions in the junction tree, allowing retrieval of subtree messages. These messages are integrated into the network to prevent local isomorphism during adversarial training. During training, predicted label distributions q* replace ground truth input f* to enable gradient propagation from the discriminator."
}