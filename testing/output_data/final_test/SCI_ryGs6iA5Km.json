{
    "title": "ryGs6iA5Km",
    "content": "Graph Neural Networks (GNNs) are a powerful framework for learning graph representations through neighborhood aggregation. Despite their success in node and graph classification tasks, there is limited understanding of their limitations. A theoretical framework is presented to analyze the expressive power of GNNs, revealing that popular variants like Graph Convolutional Networks and GraphSAGE struggle with certain graph structures. A more expressive GNN architecture is proposed to address these limitations. Recently, there has been a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs. A simple architecture has been developed that is proven to be the most expressive among GNNs and as powerful as the Weisfeiler-Lehman graph isomorphism test. Empirical validation on various graph classification benchmarks shows that the model achieves state-of-the-art performance. The representation of an entire graph can be obtained through pooling by summing the feature vectors of all nodes in the graph. Various Graph Neural Network (GNN) variants have achieved state-of-the-art performance in tasks like node classification and graph classification. However, the design of new GNNs is mostly based on empirical intuition, with little theoretical understanding of their properties and limitations. A theoretical framework for analyzing the representational power of GNNs is presented here. Theoretical framework for analyzing the representational power of GNNs inspired by the WL graph isomorphism test, focusing on the expressive aggregation scheme of GNNs to model injective functions. Our framework analyzes the representational power of GNNs by focusing on the aggregation of feature vectors from a node's neighbors as multisets. The study explores different multiset functions and their ability to distinguish different representations. The main results show that GNNs are as powerful as the WL test in distinguishing graph structures, under certain conditions on neighbor aggregation and graph readout functions. Our study focuses on the representational power of Graph Neural Networks (GNNs) by examining different aggregation functions for feature vectors from node neighbors. We introduce the Graph Isomorphism Network (GIN) as a simple neural architecture with discriminative power equal to the Weisfeiler-Lehman (WL) test. Experimental results confirm GIN's high representational power in capturing graph structures compared to other GNN variants. Graph Neural Networks (GNNs) are powerful models that outperform other variants in terms of test set accuracy and achieve state-of-the-art performance on graph classification benchmarks. GNNs utilize the graph structure and node features to perform tasks such as node classification and graph classification. Graph Neural Networks (GNNs) utilize the graph structure and node features to learn node and graph representations. GNNs follow a neighborhood aggregation strategy to update node representations by aggregating neighbors' representations. Different architectures for aggregation have been proposed, such as max-pooling in GraphSAGE and mean pooling in Graph Convolutional Networks (GCN). In Graph Convolutional Networks (GCN), mean pooling is used for node representation. For node classification, the final iteration's node representation is used for prediction. For graph classification, the READOUT function aggregates node features to obtain the entire graph's representation. The Weisfeiler-Lehman test is an effective method for graph isomorphism. The BID36 test efficiently distinguishes a class of graphs BID2 using a 1-dimensional form of the WL test. It measures graph similarity with the WL subtree kernel, which counts node labels at different iterations. The kernel considers node labels as subtree structures rooted at the node. The framework for analyzing the expressive power of GNNs involves recursively updating each node's feature vector to capture network structure and rooted subtree structures. Node feature vectors are from a countable universe, forming multisets where the same element can appear multiple times. A multiset is a 2-tuple X = (S, m) where S is the underlying set of distinct elements, and m : S \u2192 N \u22651 gives the multiplicity of the elements. To analyze the representational power of a GNN, we examine when it maps two nodes to the same location in the embedding space based on subtree structures and features. A maximally powerful GNN maps two neighborhoods (multisets) to different representations, requiring an injective aggregation scheme. A GNN's aggregation scheme must be injective for maximum representational capacity. Popular GNN variants have non-injective aggregation schemes but can capture other graph properties. A maximally powerful GNN can distinguish different graph structures by mapping them to unique embeddings, solving the graph isomorphism problem. In analyzing the representational capacity of Graph Neural Networks (GNNs), the Weisfeiler-Lehman (WL) graph isomorphism test is a key heuristic. If a GNN maps non-isomorphic graphs to different embeddings, it can effectively distinguish between them. This implies that any aggregation-based GNN is limited by the power of the WL test in distinguishing graphs. However, Theorem 3 shows that there exist GNNs that can be as powerful as the WL test in principle. Theorem 3 states that if neighbor aggregation and graph-level readout functions in a GNN are injective, then the GNN is as powerful as the WL test in distinguishing non-isomorphic graphs. The proof of Theorem 3 is provided in the appendix. Injectiveness is crucial for preserving the distinctness of inputs in countable sets, while uncountable sets with continuous node features require further considerations. The focus is on GNNs with input node features from a countable set. The GNN can capture similarity between graph structures and generalize the WL test by embedding subtrees into a low-dimensional space. This enables GNNs to distinguish different graphs effectively. The Graph Isomorphism Network (GIN) is a simple architecture that satisfies conditions for a powerful GNN, extending the WL test to achieve maximum discriminative power. It utilizes deep multisets to model injective multiset functions for neighbor aggregation. Lemma 5 shows that sum aggregators can represent universal functions over multisets, extending from sets to multisets. This allows for the modeling of universal functions over a node and its neighbors, satisfying the injectiveness condition. Corollary 6 states that for countable X, there exists a unique function f : X \u2192 R n satisfying certain conditions. Multi-layer perceptrons (MLPs) can be used to model and learn f and \u03d5 in this context. GIN is a powerful Graph Neural Network (GNN) that updates node representations and can be used for tasks like node classification and link prediction. For graph classification tasks, a \"readout\" function is proposed to produce the embedding of the entire graph. Node representations become more refined and global with increasing iterations, with earlier features sometimes generalizing better. Utilizing information from all depths/iterations is crucial for capturing all structural information. The architecture of GIN utilizes information from all depths/iterations to capture structural information. Different aggregators like sum, mean, and max capture various aspects of the input multiset. By replacing the readout function with summing all node features from the same iterations, GIN can generalize the Weisfeiler-Lehman (WL) test. The study explores GNN variants like GCN and GraphSAGE, analyzing the impact of different aggregators on model performance. While some variants struggle with simple graphs, models with mean aggregators perform well for node classification tasks. The function f in Lemma 5 helps map distinct multisets to unique embeddings, which can be parameterized by an MLP. The study examines the use of 1-layer perceptrons in GNNs for graph learning. Lemma 7 shows that there are network neighborhoods that 1-layer perceptrons cannot distinguish, indicating limitations in their ability to capture complex graph structures. 1-layer perceptrons may struggle to distinguish different multisets and are not universal approximators of multiset functions. GNNs with 1-layer perceptrons may underfit training data and perform worse than GNNs with MLPs in graph classification. Mean and max-pooling aggregators in GNNs are permutation invariant multiset functions. The mean and max-pooling aggregators in GNNs are permutation invariant multiset functions, but they are not injective. The sum aggregator distinguishes structures by giving different values for 2 \u00b7 f(a) and 3 \u00b7 f(a), unlike the mean and max-pooling aggregators. The sum aggregator in GNNs distinguishes structures by giving different values for 2 \u00b7 f(a) and 3 \u00b7 f(a), unlike the mean and max-pooling aggregators. Max-pooling fails to distinguish graphs with nodes that have repeating features, while the sum aggregator still works in such cases. The mean aggregator in GNNs captures the distribution of elements in a multiset but not the exact multiset. It can distinguish multisets based on proportions, not the exact structure. The mean aggregator may perform well when statistical and distributional information is more important than exact structure, especially when node features are diverse and rarely repeat. The mean aggregator in GNNs is powerful when node features are diverse and rarely repeat, capturing distribution rather than exact structure. On the other hand, max-pooling is suitable for identifying representative elements or the \"skeleton\" in tasks where exact structure or distribution is less important. Max-pooling has been shown to identify the skeleton of a 3D point cloud effectively and is robust to noise and outliers. The max-pooling aggregator in GNNs is robust to noise and outliers, capturing the underlying set of a multiset. The theoretical framework can characterize the representational power of any aggregation-based GNNs, with potential for analyzing other aggregation schemes in the future. Our results provide a general framework for analyzing the expressive power of a broad class of GNNs, including the Graph Isomorphism Network (GIN) which is theoretically motivated and powerful. We compare GIN with less powerful GNN variants in terms of training and test performance. The study compares the performance of different GNN models, including GIN, on 9 graph classification benchmarks. The goal is for models to learn from network structure rather than input node features. Social network datasets have no features, while bioinformatics datasets have categorical input features. Node features for social networks are either uninformative or one-hot encodings of node degrees. The study evaluates GINs and less powerful GNN variants on 9 graph classification benchmarks. GIN-0 shows strong empirical performance, fitting training data equally well as GIN- but slightly outperforming in test accuracy. Less powerful GNN variants include architectures with mean or max-pooling aggregation and 1-layer perceptrons. The study evaluates GINs and less powerful GNN variants on 9 graph classification benchmarks. GIN-0 performs well, fitting training data equally as GIN- but slightly outperforming in test accuracy. Different GNN variants use mean or max-pooling aggregation with 1-layer perceptrons. The models undergo 10-fold cross-validation using LIB-SVM, with 5 GNN layers and 2-layer MLPs. Adam optimizer with initial learning rate 0.01 is used, with hyper-parameters tuned for each dataset. The dataset includes various hyperparameters such as hidden units, batch size, dropout ratio, and number of epochs. Due to small dataset sizes, hyperparameter selection using a validation set is unstable. Training accuracy of different GNNs is reported with fixed hyperparameters. The study compares GNNs with the WL subtree kernel and baselines. The study compares GNNs with state-of-the-art baselines for graph classification, including deep learning architectures like DCNN, PATCHY-SAN, and DGCNN, as well as Anonymous Walk Embeddings (AWL). Training accuracies are compared to validate the representational power of GNNs. In experiments comparing GNN variants, GIN and GIN-0 can fit training sets well, while mean/max pooling or 1-layer perceptrons underfit on many datasets. Training accuracy patterns align with dataset rankings. Test set classification accuracies for various GNN variants are shown in Table 1. The best-performing GNNs, including GINs, are highlighted in boldface. GINs are comparable to the best GNNs on datasets where they do not have the highest accuracy. GNN variants with MLPs have higher training accuracies than those with 1-layer perceptrons, and sum aggregators perform better than mean and max-pooling aggregators. GNNs never exceed the training accuracies of the WL subtree kernel. The WL test sets an upper bound for the representational capacity of aggregation-based GNNs, but cannot learn to combine node features effectively. The GNNs with strong expressive power can accurately capture graph structures and generalize well. GINs outperform other GNN variants on 9 datasets, achieving state-of-the-art performance, especially on social network datasets. GINs and sum-aggregation GNNs outperform other models by accurately capturing graph structures. Mean-aggregation GNNs fail to capture structures of unlabeled graphs and perform poorly even with node degrees as input features. GIN-0 slightly outperforms GIN- in terms of generalization, possibly due to its simplicity. The paper provides theoretical foundations for reasoning about GNNs' expressive power and bounds on their representational capacity. The paper discusses the design of a powerful GNN under the neighborhood aggregation framework and suggests exploring new architectures beyond neighborhood aggregation for learning with graphs. It also aims to improve the generalization properties and optimization landscape of GNNs. The proof for Lemma 2 shows that GNNs with the same node labels in iterations cannot be distinguished by the WL test. The WL test ensures that different multisets of neighboring nodes receive unique new labels during iterations in GNNs. This guarantees that the same input generates the same output in the aggregation process, leading to consistent node labels. The WL test guarantees that the same input generates the same output in the aggregation process, ensuring consistent node labels. This leads to a valid mapping between graphs, showing that graphs with the same multiset of WL neighborhood labels also have the same collection of GNN neighborhood features. The WL test ensures consistent node labels during the aggregation process, leading to a valid mapping between graphs. It shows that graphs with the same multiset of WL neighborhood labels also have the same collection of GNN neighborhood features. The neighborhood aggregation process embeds graphs into different multisets of node features with sufficient iterations. The injective functions f and \u03c6 update node representations, while the WL test applies an injective hash function g to update WL node labels. An injective function f is defined to uniquely map multisets of bounded size, ensuring permutation invariance for well-defined multisets. The function of multisets is permutation invariant, defined by \u03c6 x\u2208X f (x) = g(X). The proof involves showing that h(c, X) = h(c , X ) holds for any (c , X ) = (c, X) with c, c \u2208 X and X, X \u2282 X, if is an irrational number. This is proven by contradiction in two cases: (1) c = c but X = X, and (2) c = c. The contradiction is reached by showing that x\u2208X f (x) = x\u2208X f (x) does not hold with f (x) = N \u2212Z(x) and X = X. The proof involves showing that h(c, X) = h(c , X ) holds for any (c , X ) = (c, X) with c, c \u2208 X and X, X \u2282 X, if is an irrational number. This is proven by contradiction in two cases: (1) c = c but X = X, and (2) c = c. The contradiction is reached by showing that x\u2208X f (x) = x\u2208X f (x) does not hold with f (x) = N \u2212Z(x) and X = X. For any function g over the pairs (c, X), we can construct such \u03d5 for the desired decomposition by letting \u03d5 (1 + ) \u00b7 f (c) + x\u2208X f (x) = g(c, X). The homogeneity of ReLU is demonstrated by showing that ReLU(Wx) is either positive or 0 for all x in X1 and X2. Linearity is maintained, leading to the conclusion that ReLU(Wx) = ReLU(Wx) for all x in X."
}