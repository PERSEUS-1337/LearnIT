{
    "title": "Bkxbrn0cYX",
    "content": "Sequential learning, also known as lifelong learning, focuses on learning tasks in a sequence with access limited to the current task's data. This paper explores Selfless Sequential Learning in a scenario with fixed model capacity, emphasizing the need to consider future tasks and allocate enough capacity for them. Various regularization strategies and activation functions are studied to achieve this goal. Imposing sparsity at the representation level is more beneficial for sequential learning than parameter sparsity. A novel regularizer encourages representation sparsity through neural inhibition, resulting in few active neurons and more capacity for upcoming tasks. Our novel regularizer encourages representation sparsity through neural inhibition, leading to increased performance on diverse datasets in sequential learning. This approach addresses the challenge of avoiding catastrophic interference with previously learned tasks. In this work, the focus is on learning a sequence of tasks without access to previous or future task data, resembling how the mammalian brain learns tasks over time. The scenario involves a fixed model capacity and emphasizes privacy, scalability, and neural inhibition for representation sparsity. The focus is on learning tasks sequentially without access to past or future data, similar to how the brain learns. Artificial neural networks create dense, entangled representations sensitive to changes. Sparsity in representation reduces interference between input patterns, allowing for different responses to similar inputs. Learning a disentangled and sparse representation in neural networks can reduce interference and catastrophic forgetting, allowing for better adaptation to new tasks without compromising previous learning. In a sequential learning scenario, a sparse and decorrelated representation is preferred over parameter sparsity to reduce interference with previous tasks and prevent forgetting. A new regularizer is proposed to achieve this behavior. The proposed regularizer, Sparse coding through Local Neural Inhibition and Discounting (SLNID), penalizes active neurons locally to promote sparsity and decorrelated representation. It discounts inhibition from/to neurons with high importance to prevent interference with previously learned tasks. This regularizer, when combined with important parameters preservation methods, leads to a sparse and effective representation. Our proposed regularizer, SLNID, promotes sparse and decorrelated representations for improved lifelong learning performance. It outperforms alternatives on diverse datasets and state-of-the-art approaches in object classification challenges. In lifelong learning, the goal is to learn a sequence of tasks without forgetting previous ones. Different approaches include using a fixed model capacity and either pseudo rehearsal or identifying important parameters. The proposed regularizer, SLNID, promotes sparse and decorrelated representations for better performance in lifelong learning. Elastic Weight Consolidation, Path Integral, and Memory Aware Synapses are methods that estimate the importance of network parameters in lifelong learning tasks. They use different approaches such as Fisher information matrix approximation, network reparameterization, KL-divergence, and sensitivity analysis to measure parameter importance without supervision. Sequential learning methods like Elastic Weight Consolidation, Path Integral, and Memory Aware Synapses estimate parameter importance in lifelong learning tasks. They address issues like data distribution shift and network capacity utilization. Sparsity and representation decorrelation in sequential learning have not been widely studied. Recent approaches like learned hard attention masks with L1 regularization aim to overcome catastrophic forgetting. The regularization on accumulated hard attention masks is similar to previous approaches, but this study proposes a regularization scheme on learned representation to reduce overlap. Previous methods aimed at orthogonalizing activations, mainly for specific architectures, making integration into recent neural networks challenging. Sparsification for compression has been explored, such as using SVD decomposition to reduce parameters, but convergence to a low-rank weight matrix is not guaranteed. Other works involve iterating between pruning and retraining as a post-processing step for neural networks. In experiments, the difference between sparse penalty on representation versus weights is shown. Challenges in single model sequential learning include learning new tasks without forgetting previous ones. Importance weight methods like EWC or MAS help prevent catastrophic forgetting. Importance weight based methods like EWC or MAS introduce an importance weight for each parameter in the network to prevent catastrophic forgetting when learning new tasks. These methods penalize changes to important parameters using an L2 penalty. Additionally, a regularizer is introduced to encourage sparsity in activations for each layer. Trade-off parameters control the contribution of each term in the objective function. In the literature, sparsity is combined with the rectifier activation function to control activations and increase sparsity. Minimizing the L1 norm imposes an equal penalty on all active neurons, leading to small activation magnitudes. Learning a decorrelated representation aims to reduce overfitting by minimizing the Frobenius norm of the covariance matrix. This results in a decorrelated representation with activations close to a non-zero mean value. The objective is to merge sparse and decorrelated representation by minimizing the correlation between active neurons in a hidden layer with close to zero mean activations. This differs from minimizing the Frobenius norm of the covariance matrix and penalizes each active neuron based on the magnitude of other active neurons' activations. The objective is to merge sparse and decorrelated representation by minimizing the correlation between active neurons in a hidden layer with close to zero mean activations. To achieve this, a spatial weighting is suggested to relax the objective, where an active neuron penalizes its close neighbors more than those further away, introducing a locality in the network. The regularizer introduced in the current chunk inhibits locally active neurons in a network inspired by biological neurons, promoting sparse coding through neural inhibition. This regularizer is applied for each task in the learning sequence, preventing activation of neurons from previous tasks when new tasks have different input patterns. The regularizer introduced inhibits locally active neurons in a network to promote sparse coding. It prevents activation of neurons from previous tasks when new tasks have different input patterns. To avoid interference, a weight factor is added based on the importance of neurons determined by the sensitivity of loss to their changes. This importance weight is calculated by accumulating the absolute gradients of the loss w.r.t. neuron outputs over data points. The importance weight \u03b1i for neuron ni is determined by the sensitivity of the loss to their changes. This weight factor is calculated by accumulating the absolute gradients of the loss w.r.t. neuron outputs over data points. The regularizer inhibits locally active neurons to promote sparse coding and prevent interference from previous tasks with different input patterns. The importance weight \u03b1i for neuron ni is determined by the sensitivity of the loss to their changes, calculated by accumulating the absolute gradients of the loss w.r.t. neuron outputs over data points. The regularizer inhibits locally active neurons to promote sparse coding and prevent interference from previous tasks with different input patterns. Sparse coding through Local Neural Inhibition and Discounting (SLNID) is a method that focuses on sparsity and decorrelation of the representation in sequential learning scenarios. It compares different activation functions and regularization techniques on permuted MNIST, as well as competing techniques in sequentially learning CIFAR-100 and Tiny Imagenet classes. The SLNID regularizer can be integrated into any importance weight-based lifelong learning. Our SLNID regularizer can be integrated into lifelong learning approaches like MAS and EWC, showing improved performance. We analyze the components of our regularizer and demonstrate its effectiveness in object recognition tasks. SLNID outperforms other regularization methods, especially on the last two tasks, indicating its potential for further learning capacity. In a sequential learning scenario, regularization techniques like sparsity and decorrelation are studied to reduce interference between tasks. Activation functions inspired by biological neurons' lateral inhibition are also explored. MAS Aljundi et al. (2017) is used as the lifelong learning method. Representation-based methods include L1-Rep for sparsity promotion and Decov BID5 for decorrelating neuron activations. Dropout is not considered due to conflicting goals with the desired outcomes. Dropout is a model averaging technique that improves task performance and reduces overfitting by encouraging redundant neural representations. In a sequential learning setup using the MNIST dataset, dropout steers learning towards occupying a good portion of the network capacity, contradicting sequential learning needs. The goal is to classify MNIST digits from different permutations of input pixels, requiring the network to instantiate new neural representations for each pattern. In a sequential learning setup using the MNIST dataset, a new neural representation is created for each pattern. Different setups have been experimented with, including varying the number of neurons in hidden layers. The effect of regularization parameters on performance and sparsity is evaluated. The best parameter values are determined based on maintaining accuracy on the first task. High values of another parameter are used to minimize forgetting. Tuning these parameters can lead to better average accuracies. Results from the experiments show that representational regularizers outperform other techniques in various tasks. L1-Rep consistently performs better than L1-Param, with OrthReg also showing good performance but not as high as representational regularizers. Tuning parameters like \u03bb \u2126 can lead to improved accuracies. Refer to Appendices A and B for more details on hyperparameters and results. The experiments demonstrate that representational regularizers, such as L1-Rep, outperform other techniques like L1-Param and OrthReg. Maxout and LWTA activation functions show slightly better performance than ReLU. The proposed regularizer SLNID achieves high performance and stability, directing the learning process towards using fewer neurons. The regularizer also impacts parameter sparsity. The regularizer SLNID impacts the percentage of parameters utilized in a network with a hidden layer size of 128. Increasing \u03bb SLNID leads to fewer important parameters, indicating more neural inhibition. While too strong inhibition can hinder task performance, an improvement over No-Reg is consistently observed. The optimal lambda value appears to be one that maintains a balance between inhibition and task performance. The optimal lambda value for SLNID is one that balances inhibition and task performance. The average activation per neuron in the first layer is computed and plotted for various methods, showing SLNID's effectiveness in learning a sparse yet powerful representation. Test accuracy results for 8 tasks in object recognition are also presented. In object recognition, the SLNID method outperforms other competing methods like L1-Rep and DeCov on tasks with different categories from datasets like CIFAR-100 and Tiny ImageNet. The SLNID shows the best overall performance at the end of the sequence, with L1-Rep and DeCov also improving over the non-regularized case. Our SLNID regularizer exhibits stable and superior performance on different networks when using MAS as the importance weight preservation method. When tested on the 5 tasks permuted MNIST sequence with Elastic Weight Consolidation (EWC), SLNID showed a boost in average performance at the end of the learned sequence, with a 3.1% boost on a network with hidden layer size 128 and a 2.8% boost on a network with hidden layer size 64. SLNID regularizer improves accuracy using a 64-dimensional hidden size compared to a larger size without regularization. It consists of neural inhibition, locality relaxation, and neuron importance integration. Different variants were tested on Cifar 100 and permuted MNIST sequences. Exclusion of important neurons is not necessary when tasks have different input patterns, but important when tasks share similarities. In the Cifar 100 sequence, integration of neuron importance in the SNID and SLNID regularizers improves performance by excluding important neurons from inhibition. The locality in SLNID enhances performance, suggesting a richer representation with multiple active neurons. This contrasts with the standard task-based scenario where a new classification layer is initiated for each new task. In a more realistic scenario with gradual data distribution shifts, the Cifar 100 dataset is used. Sampling probabilities are adjusted from high to low for different classes in each training epoch. One shared classification layer is used, and importance weights and neuron importance are estimated after each training step. Six variants are considered in this experiment. In a study using the Cifar 100 dataset, six variants were considered, including SLNID, SLNI, and No-Reg. Results show SLNID improves performance significantly, even without the importance weight regularizer. There is less forgetting in scenarios without hard task boundaries compared to those with hard task boundaries. The study compared SLNID, SLNI, and No-Reg on the Cifar 100 dataset. SLNID showed significant performance improvement even without the importance weight regularizer. SLNI improved individual model performance but failed to enhance overall performance due to interference from important neurons. The proposed approach was compared with state-of-the-art sequential learning methods using 8 object recognition tasks. In this study, SLNID was tested on a pretrained network with and without dropout, showing improvements in test accuracy. Even with randomly initialized fully connected layers, SLNID outperformed the state of the art by 1.8%. The research focuses on sequential learning with a fixed capacity network for scalability and efficiency. In the context of sequential learning, sparsity is imposed at the representation level using a new regularizer inspired by lateral inhibition in the brain. The regularizer decorrelates nearby active neurons in a model that learns new tasks while preserving capacity for future tasks and avoiding forgetting previous tasks. The regularizer is applied to fully connected layers in experiments using a network with two fully connected layers trained for 10 epochs. Future work includes integrating the regularizer in convolutional layers. In experiments, a sparsity regularizer is applied to fully connected layers to preserve capacity for future tasks and prevent forgetting previous tasks. Different hyperparameters are tested for competing regularizers, with a high \u03bb\u2126 value used to minimize forgetting. The free capacity in the network is estimated based on the importance weight multiplier, with negligible importance defined as \u2126 k < 10 \u22122. The experiment involves using a sparsity regularizer with a value of \u2126ij < 10 \u22122 for the first 10 percentiles. Two variants with hidden sizes N = {256, 128} are evaluated using a base network similar to BID50. The Tiny ImageNet dataset BID48 is split into ten tasks, each with twenty categories. The tasks include Oxford Flowers, MIT Scenes, Caltech-UCSD Birds, Stanford Cars, FGVC-Aircraft, VOC Actions, and Letters. Training is done for 50 epochs with a learning rate of 10 \u22122 using SGD optimizer. In section 4.1, different regularizers and activation functions were studied on 5 permuted Mnist tasks in a network with a hidden layer of size 128. The average accuracies achieved by each method at the end of the learned sequence in a network with a hidden layer of size 64 were shown in FIG6. Maxout and LWTA performed similarly and slightly improved over ReLU. Regularizers applied to the representation were found to be more effective for sequential learning than regularizers applied directly to the parameters. L1-Rep consistently outperformed L1-Param. SLNID maintained good performance on all tasks, achieving top average test accuracies. SLNID, a representation regularizer, has shown to improve performance on all learned tasks, including in experiments with EWC on permuted Mnist and Cifar 100 sequences. The approach is not limited to MAS and has proven utility with different sequential learning methods. SLNID improves performance on learned tasks, outperforming No-Reg by over 6%. L1-Rep suffers from catastrophic forgetting by penalizing activations. SLNID weights correlation penalty based on spatial distance between neurons. Neurons importance visualized after tasks using network with hidden layer size 64. The network with hidden layer size 64 visualizes neurons importance after each task. SLNID allows more active neurons by applying inhibition locally, improving representation power. New neurons become important regardless of proximity to previous task neurons. SLNID penalizes neural correlation in the first task, limiting activation in later tasks. Neurons that are excluded from inhibition are easier to be re-used, with SLNID allowing previous neurons to be re-used for the third task while activating new neurons. Very few neurons are newly deployed for SNID, while most previous important neurons are re-adapted for the new task."
}