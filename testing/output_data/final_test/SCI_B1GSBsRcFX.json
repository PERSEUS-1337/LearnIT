{
    "title": "B1GSBsRcFX",
    "content": "Deep neural networks (DNNs) can fit random data due to inconsistency between enforced geometry and softmax cross entropy loss. A new framework, Geometrically-Regularized-Self-Validating neural Networks (GRSVNet), resolves this issue by validating geometry during training. OLE-GRSVNet produces discriminative features in orthogonal low-rank subspaces, outperforming DNNs with conventional regularization on real data. OLE-GRSVNet, unlike conventional DNNs, avoids memorizing random data or labels, focusing on learning intrinsic patterns. Previous studies have suggested that the generalization error of DNNs is limited, even with regularization techniques. Zhang et al. (2017) experiments showed that DNNs can perfectly fit training data with data-independent regularization, even with random labels. DNNs with data-independent regularization can \"memorize\" training data, raising questions about network regularization design to prevent overfitting. Data-dependent regularizations may be more effective in reducing memorization capacity. Regularizing deep neural networks with an extra geometric loss has limitations, as the softmax output may not align with the enforced geometry. This can lead to overfitting on random data or labels. Simply penalizing the softmax loss with a geometric loss is not enough; a validation loss consistent with the enforced geometry is needed for effective regularization. The training batch is split into geometry batch B g and validation batch B v. Geometric loss l g enforces desired structure on B g features. A semi-supervised algorithm uses feature geometry to predict labels for B v, creating validation loss. Total loss l = l g + \u03bbl v is minimized to enforce geometry and predict validation samples. DNNs are called Geometrically-Regularized-Self-Validating neural Networks (GRSVNets). The Orthogonal-Low-rank-Embedding-GRSVNet (OLE-GRSVNet) is a type of Geometrically-Regularized-Self-Validating neural Network (GRSVNet) that utilizes the OLE loss to produce features in orthogonal subspaces. The loss function achieves its minimum when features from different classes are orthogonal and validation features align perfectly with their corresponding subspaces. OLE-GRSVNet effectively learns intrinsic patterns by imposing data-dependent DNN regularization through the GRSVNet framework. It produces highly discriminative features with samples from the same class residing in low-rank subspaces. This approach achieves better generalization performance compared to DNNs with conventional regularizers. OLE-GRSVNet, a data-dependent DNN regularization through the GRSVNet framework, achieves better generalization performance compared to DNNs with conventional regularizers. It does not memorize training samples but learns intrinsic patterns. Various data-dependent regularizations focusing on feature geometry have been proposed for deep learning. Our proposed GRSVNet uses a validation loss based on regularized geometry for meaningful geometric interpretation of predicted label distribution. Unlike siamese network architecture, GRSVNets explicitly regularize feature geometry for better generalization performance in deep learning. Our work introduces GRSVNets, which utilize data-dependent regularization to prevent memorization of random labels or data, focusing on extracting intrinsic patterns. This approach contrasts with conventional DNNs, which may memorize random information. The core idea of GRSVNet is to self-validate geometry using a consistent validation loss. OLE-GRSVNet focuses on orthogonal low-rank subspaces for feature geometry and defines the validation loss based on principal angles. The OLE loss, proposed by Qiu & Sapiro (2015), involves learning a linear transformation to map data from the same class into a low-rank subspace. The OLE loss is a convex lower bound of the rank function on the unit ball in the operator norm. It is always nonnegative and achieves a global optimum value of 0 under certain conditions. BID8 used OLE loss as a regularization for deep learning. When combined with the softmax loss in training, the network is called \"softmax+OLE.\" However, this combination faces issues with the desired geometry of orthogonal low-rank subspaces. The OLE loss is used as a regularization for deep learning, known as \"softmax+OLE.\" It faces challenges with the desired geometry of orthogonal low-rank subspaces. The incorporation of OLE loss into the GRSVNet framework enforces orthogonal column spaces. The validation loss is defined based on the principal angles between validation features and subspaces spanned by {Zc} Kc=1. The architecture for OLE-GRSVNet involves splitting training batches into geometry and validation batches, mapping them to features, and calculating cosine similarity for classification probabilities. The validation loss is defined as cross entropy between predicted and true labels. The validation loss in OLE-GRSVNet is defined as cross entropy between predicted and true labels. The empirical loss on the training batch is always nonnegative and equals zero only under specific conditions. OLE-GRSVNet validation loss is defined as cross entropy between predicted and true labels. The features of the geometry batch belonging to different classes are orthogonal. The requirement that \u03bb > 0 is crucial in achieving the minimum. VGG-11 is used as the baseline architecture for comparing DNN performance. OLE-GRSVNet has the highest test accuracy among the compared DNNs, with more discriminative features than softmax+OLE. Features of the same class reside in a low-rank subspace, and different subspaces are orthogonal. Features of only four classes have nonzero 3D embedding. In experiments, OLE-GRSVNet outperforms other DNNs in test accuracy and features reside in low-rank subspaces. Only four classes have nonzero 3D embedding, and training data memorization is prevented without learnable patterns. In experiments, OLE-GRSVNet outperforms other DNNs in test accuracy by preventing training data memorization without learnable patterns. The model refuses to memorize random labels even with a 100-layer MLP, suggesting it generalizes well with true labeled data but not with random data. OLE-GRSVNet achieves global minimum if training batch features exhibit same structure, implying implicit data augmentation. Can it memorize random data with many parameters or is it limited by SGD algorithm? Future research focus. Operations in computational graph are basic matrix operations. The computational graph of OLE-GRSVNet involves basic matrix operations, with exceptions being the OLE loss and SVD. The OLE loss requires finding a subgradient of the nuclear norm for back-propagation. The subdifferential of the nuclear norm is explained using the SVD of a rank-s matrix A. The OLE loss in OLE-GRSVNet involves finding a stable subgradient using the SVD of a rank-s matrix A. Truncating the numerical SVD with a small threshold can speed up convergence, especially in the initial training epochs. The subgradient of the OLE loss can be easily computed with the help of Theorem 3. Thresholding the singular vectors of Z g c is necessary to ensure that the desired output U c is an orthonormal basis of the low-rank subspace span(Z g c). In the forward propagation, the singular vectors Uc are thresholded to ensure the smallest singular value is at least 1/10 of the largest singular value. Backward propagation requires knowledge of the Jacobian of SVD, which involves solving linear systems. Backward propagation of SVD was not implemented due to technical challenges with CUDA API. During back propagation, the node (U1, ..., UK) is detached from the computational graph, and the validation loss lv is only propagated through the path lv \u2192 \u0176v \u2192 Zv \u2192 \u03b8. In the forward propagation, singular vectors Uc are thresholded to ensure the smallest singular value is at least 1/10 of the largest singular value. Backward propagation requires knowledge of the Jacobian of SVD, which involves solving linear systems. Backward propagation of SVD was not implemented due to technical challenges with CUDA API. The first path modifies \u03b8 so that Zvc moves closer to Uc, while the second path moves Uc closer to Zvc. Cutting off the second path during gradient computation may slow convergence, but experiments show the training process remains stable. OLE-GRSVNet demonstrates superior generalization power when trained on true data and labels, and avoids memorizing training samples with random data or labels. The performance of networks (VGG-11,16,19) is tested on datasets like MNIST, SVHN, and CIFAR with and without data augmentation. All networks are trained from scratch using Xavier initialization and SGD with Nesterov momentum 0.9 for optimization. The optimization process uses Xavier initialization and SGD with Nesterov momentum 0.9. The batch size is 200 with a 100/100 split for geometry/validation batch in OLE-GRSVNet. The initial learning rate is 0.01, decreasing ten-fold at 50% and 75% of training epochs. Networks are trained for 100, 160 epochs for MNIST, SVHN, and 200, 300, 400 epochs for VGG-11, VGG-16, VGG-19 on CIFAR. Weight decay parameter is set to \u00b5 = 10 \u22124. OLE loss weight in \"softmax+OLE\" is 0.5 for MNIST, SVHN, VGG-11, VGG-16, and 0.25 for VGG-19 on CIFAR. The performance of networks trained on original data with real or randomly generated labels is reported in Table 1. The accuracies on testing data with real labels are shown without parentheses, while the accuracies on training data with random labels are enclosed in parentheses.\u03bb parameter is determined by cross-validation, with values set differently for MNIST, SVHN, and CIFAR datasets with VGG-11, VGG-16, and VGG-19 models. OLE-GRSVNet outperforms conventional DNNs on testing data when trained with real labels. It does not memorize training data when trained with random labels, unlike other networks. Generalization performance is better with real labels, even with increased training epochs. Different regularization techniques are combined for improved performance. By combining various regularization techniques and tuning hyperparameters, the test error of conventional DNNs can be reduced. However, these networks can still memorize training samples with random labels. A new framework, GRSVNet, is proposed for data-dependent DNN regularization, focusing on self-validation of enforced geometry for improved performance. OLE-GRSVNet, a special case of GRSVNet, produces discriminative features by ensuring samples from the same class belong to a low-rank subspace. It outperforms DNNs with different regularizations on benchmark datasets, showing better test accuracy. OLE-GRSVNet avoids memorizing and overfitting training data, reducing the memorization capacity of DNNs and extracting only learnable patterns. The explanation of why GRSVNet generalizes well on real data and avoids overfitting random data raises questions about the minimum representational capacity of the baseline DNN and the role of the learning algorithm (SGD). The focus of future work will be on understanding why conventional DNNs can memorize random data but find generalizable solutions on real data. The theorem for K = 2 is sufficient to prove the case for larger K by induction. The concatenation of matrices A and B in R N \u00d7(m+n) satisfies certain conditions for equality to hold, specifically when A * B = 0, indicating orthogonal column spaces. The necessary and sufficient conditions for this equality are discussed using symmetric positive semidefinite matrices and orthonormal eigenvectors. The validation loss l v (Y v ,\u0176 v ) is nonnegative, and l = l g + \u03bbl v is also nonnegative. l(X, Y) reaches its minimum value zero if l g (Z g ) and l v (Y v ,\u0176 v ) are zeros. l g (Z g ) = 0 if span(Z g c )\u22a5 span(Z g c ), \u2200c = c . l v (Y v ,\u0176 v ) = 0 if \u0177(x) = \u03b4 y , \u2200x \u2208 X v. If \u03bb > 0 and X v has at least one sample for each class, then rank(span(Z g c )) \u2265 1 for any c \u2208 {1, . . . , K}. The predicted probability of x belonging to class c is defined in (3)."
}