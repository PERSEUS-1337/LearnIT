{
    "title": "r1lyTjAqYX",
    "content": "In this paper, the training of RNN-based RL agents from distributed prioritized experience replay is investigated. The resulting agent, Recurrent Replay Distributed DQN, achieves significant improvements in performance on Atari-57 and DMLab-30 games, surpassing human-level performance in 52 out of 57 Atari games. This success builds on recent advancements in distributed training of RL agents. In this paper, the training of RNNs with experience replay is explored. The study reveals the impact of experience replay on parameter lag, leading to representational drift and recurrent state staleness, affecting training stability and performance, especially in distributed training settings. The study explores the effects of various approaches to RNN training with experience replay, leading to improved performance. An agent named Recurrent Replay Distributed DQN (R2D2) achieves significant advancements in Atari-57 and matches the state of the art in DMLab-30 using a single network architecture and fixed hyper-parameters within the Reinforcement Learning framework. The underlying Markov Decision Process (MDP) is defined by states, actions, transition function, and reward function. Recent advances in reinforcement learning have improved performance through distributed training architectures. Ape-X agent utilizes distributed replay and n-step return targets for learning. Ape-X utilizes n-step return targets, double Q-learning algorithm, dueling DQN network architecture, and 4-frame stacking to achieve state-of-the-art performance on Atari-57. IMPALA is a distributed reinforcement learning architecture that uses a first-in-first-out queue with a novel off-policy correction algorithm called V-trace. It stores sequences of transitions along with an initial recurrent state in the experience queue, showing strong performance in Atari-57. The new agent R2D2, similar to Ape-X, uses prioritized distributed replay and n-step double Q-learning. It incorporates an LSTM layer after the convolutional stack and stores fixed-length sequences of (s, a, r) in replay. The new agent R2D2, like Ape-X, utilizes prioritized distributed replay and n-step double Q-learning. It stores fixed-length sequences of (s, a, r) in replay, with overlapping adjacent sequences and unrolls both online and target networks on the same sequence of states for training. It uses 4-frame-stacks and the full 18-action set for Atari, and single RGB frames for observations on DMLab. Rewards are not clipped, instead using an invertible value function rescaling. Target network parameters are copied from the online network every 2500 learner steps. The R2D2 agent utilizes prioritized distributed replay and n-step double Q-learning. It uses a mixture of max and mean absolute n-step TD-errors for replay prioritization, with a more aggressive scheme compared to Ape-X. The agent also uses a slightly higher discount rate and disables the loss-of-life-as-episode-end heuristic. Training is done with a single GPU-based learner, performing approximately 5 network updates per second. To achieve good performance in a partially observed environment, an RL agent needs a state representation that encodes information about its state-action trajectory. Using an RNN, such as an LSTM, as part of the agent's state encoding is common. BID5 compared two strategies for training an LSTM from replayed experience: using a zero start state or replaying whole episode trajectories. The zero start state strategy is simple and allows independent sampling of short sequences. The appeal of using a zero start state in training an LSTM lies in its simplicity and independent sampling of short sequences. However, this approach may limit the network's ability to fully rely on its recurrent state and exploit long temporal correlations. On the other hand, replaying whole episode trajectories avoids the initial state issue but introduces practical, computational, and algorithmic challenges due to varying sequence lengths and higher variance in network updates. BID5 found little difference in empirical agent performance between the two strategies on a set of Atari games. The appeal of using a zero start state in training an LSTM lies in its simplicity and independent sampling of short sequences. However, replaying whole episode trajectories avoids the initial state issue but introduces practical, computational, and algorithmic challenges. BID5 found little difference in empirical agent performance between the two strategies on a set of Atari games. The simpler zero start state strategy was chosen over the RNN strategy due to convergence issues. To address this, two strategies for training a recurrent neural network from randomly sampled replay sequences are proposed and evaluated. The text discusses the issue of recurrent state staleness in training neural networks, particularly in the context of using replay sequences. It explores strategies such as storing the recurrent state in replay and using burn-in to address this problem. The text discusses using a burn-in period in training neural networks with replay sequences to address recurrent state staleness. The proposed agent architecture is used with replay sequences of length 80, with an optional burn-in prefix of 40 or 20 steps to mitigate negative effects on network training. The aim is to assess how different training strategies affect the Qvalues produced by the network on sampled replay sequences. The text discusses using a burn-in period in training neural networks with replay sequences to address recurrent state staleness. It aims to assess how different training strategies affect the Q-values produced by the network on sampled replay sequences. The impact of representational drift and recurrent state staleness on Q-value estimates is estimated by measuring Q-value discrepancy for the first and last states. The text discusses Q-value discrepancy for the first and last states of replay sequences in neural network training. It compares different training strategies' impact on Q-values and recurrent state staleness, showing a significant effect with the zero start state heuristic. The text discusses the impact of recurrent state staleness on neural network outputs, comparing the zero state heuristic with the stored state strategy. The burn-in strategy partially mitigates staleness in initial replayed sequences, leading to performance improvements. The burn-in strategy prevents destructive updates to RNN parameters after zero state initialization, while the stored state strategy effectively mitigates state staleness and improves Q-value consistency. Combining both methods consistently enhances empirical performance. The combination of the burn-in strategy and stored state strategy consistently improves empirical performance by reducing representation drift and recurrent state staleness. These strategies are used in the evaluation of the proposed agent in Section 4, showing robust performance gains on challenging benchmark suites for deep reinforcement learning. Additional results on distributed training effects are provided in the Appendix. The Deep Q-Networks (DQN) introduced a standard practice of using a single network architecture and hyper-parameters across Atari games. However, this standard has not been maintained beyond Atari. A comparison is made with Ape-X and IMPALA, which tune hyper-parameters separately for each benchmark. In contrast, R2D2 uses a single neural network architecture and hyper-parameters across all experiments, demonstrating greater robustness and generality in deep reinforcement learning. The heuristic of treating life losses as episode ends was disabled, and reward clipping was not applied. The study disabled life losses as episode ends and did not use reward clipping. Despite this, state-of-the-art performance was achieved in both Atari and DMLab. A detailed ablation study on these modifications is presented in the Appendix. The Atari-57 benchmark includes 57 classic Atari 2600 video games. Current state-of-the-art results are achieved by distributional reinforcement learning algorithms IQN and Rainbow for single actor, and Ape-X for multi-actor scenarios. R2D2 outperforms all single-actor agents and quadruples the previous state-of-the-art performance of Ape-X using fewer actors. It achieves the highest ever reported agent scores on many individual Atari games, 'solving' them by achieving the highest attainable score. Ape-X achieves super-human performance on 49 games, with improvements on already strong scores. R2D2 is super-human on 52 out of 57 games, with some games potentially reaching super-human performance with adjustments. DMLab-30 presents a suite of 30 challenges in a 3D game engine, requiring long-term memory for reasonable performance. The DMLab-30 challenges require long-term memory for good performance. Top-performing agents in DMLab-30 have been actor-critic algorithms trained in on-policy settings. However, a value-function-based agent, R2D2, has shown state-of-the-art performance on DMLab-30. The R2D2 agent uses the same hyper-parameters as on Atari, and is compared to the IMPALA 'experts'. The comparison of Atari-57 and DMLab-30 results shows R2D2's performance on DMLab-30. IMPALA and Ape-X also had notable scores. The re-run of IMPALA used an improved action set and was trained for a comparable number of environment frames for a fair comparison. The IMPALA agent showed substantially improved scores compared to the original in BID3 when trained for a comparable time. R2D2 outperformed the shallow IMPALA version despite using the same hyper-parameters. A modified R2D2 version for DMLab (R2D2+) with asymmetric reward clipping and a deep ResNet showed further improvements over standard R2D2. The modified R2D2 version, R2D2+, surpasses deep IMPALA in sample efficiency and asymptotic performance on DMLab-30 benchmark. The use of a recurrent neural network in R2D2 results in a significant performance boost over its predecessor, Ape-X, on Atari. The analysis focuses on the role of the LSTM network and algorithmic choices in the high performance of the R2D2 agent. The LSTM component is crucial for boosting the agent's peak performance and learning speed in the R2D2 agent, as shown in ablation results. Other design choices have mixed effects on performance. In the Appendix, ablation results for the feed-forward agent variant on the full Atari-57 suite and the impact of the life-loss-as-episode-termination heuristic are presented. The next experiment evaluates the reliance of the R2D2 agent on memory using different training strategies on Atari game MS.PACMAN and DMLab task EMSTM WATERMAZE. Agents are trained with zero and stored state strategies, and evaluated by restricting policy to a fixed history length using an LSTM. In Figure 5, the agent's performance decreases as history length is reduced from full to 0. The agent trained with zero start states shows faster performance decay compared to the one trained with stored state, indicating limitations in memory utilization. The stored state strategy in training R2D2 leads to better memory utilization and performance compared to the zero state strategy, as shown in Figure 5. Decreasing the available history length k to 0 results in a decrease in Q-values quality and greedy policy. Videos comparing behaviors learned by R2D2 and its feed-forward variant can be found at https://bit.ly/r2d2600. The empirical findings suggest that zero state initialization in previous works leads to misestimated action-values, especially in early states of replayed sequences. Without burn-in, updates through BPTT to these early time steps hinder the network's ability to recover from sub-optimal initial recurrent states. Storing context-dependent recurrent state or reserving an initial part of replayed sequences for burn-in can help exploit long-term temporal dependencies. In a distributed setting, techniques can be combined to address representational drift and recurrent state staleness. RNN training plays a crucial role in enabling better representation learning, improving performance even in fully observable domains. Scaling up RL agents through parallelization and distributed training enhances experience throughput, leading to better results on tasks like Atari-57. Improving sample efficiency and exploration are key challenges for RL agents achieving impressive results on task suites like Atari-57. R2D2 surpasses human-level performance on many games, highlighting the need for better exploration strategies. Parameter lag in distributed prioritized replay is shown in Figure 6 for different numbers of actors on DMLab levels. In this section, the effects of distributed training using a recurrent neural network are investigated. The distributed setting presents a less severe problem of representational drift compared to single-actor cases. Distributed agent training results in a smaller degree of 'parameter lag' due to the large amount of generated experience being replayed less frequently. In distributed training with a recurrent neural network, the parameter lag (mean age of network parameters during replay) is crucial. The number of actors impacts the parameter lag, with fewer actors leading to a higher lag. This can affect the severity of representational drift in RNN training. The parameter lag increases from 1500 to around 5500 updates, affecting representation drift and recurrent state staleness. The average \u2206Q rises for fewer actors in distributed training, emphasizing the need for an improved training strategy to maintain agent performance across different parameters. The 'reset' agent variant uses life losses as episode terminations, while 'roll' only prevents value function bootstrapping. In this section, additional experimental results are presented to support the empirical study. The ablation results show that different configurations can lead to improved performance in individual games. Comparisons between R2D2 and variants using the life loss signal as episode termination are made in FIG4. The variants interrupt value function bootstrapping past life loss events, with one variant resetting the LSTM state at these events ('reset') and the other only resetting at actual episode boundaries ('roll'). The text chunk discusses the comparison of R2D2 with other state-of-the-art agents on Atari-57 in terms of sample efficiency and final performance. It highlights a trend of increasing final performance being negatively correlated with sample efficiency across all algorithms. The more distributed agents show worse sample efficiency initially but improved long-term performance. R2D2 appears to achieve a qualitatively different outcome compared to other agents. R2D2 performs updates on batches of 64 \u00d7 80 observations at a rate of approximately 5 per second, resulting in a reduced replay ratio compared to Ape-X. R2D2 uses a 3-layer convolutional network followed by an LSTM with 512 hidden units, feeding into the advantage and value heads of a dueling network. R2D2 uses a 3-layer convolutional network followed by an LSTM with 512 hidden units. The LSTM receives reward and action vector inputs from the previous time step. Performance of R2D2 and R2D2+ is compared to IMPALA with improved action-set, trained on the same amount of data. R2D2+ variant has a shorter target network update frequency. The R2D2+ variant utilizes a 15-layer ResNet and 'optimistic asymmetric reward clipping' from BID3, with a shorter target network update frequency compared to R2D2."
}