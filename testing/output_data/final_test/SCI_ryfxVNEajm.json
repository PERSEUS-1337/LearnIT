{
    "title": "ryfxVNEajm",
    "content": "The Music Transformer is a tool for visualizing self-attention on polyphonic music, using it as both a descriptive tool and a generative model. It analyzes existing music to see if the self-attention structure aligns with music theory, and also examines how past notes influence future ones during music generation. The Music Transformer visualizes self-attention on polyphonic music, analyzing its impact on generated music. Relative attention is compared to regular attention, showing a consistent focus on all voices and chords. This analysis supports relative self-attention as a powerful bias for music modeling. Explore video animations and visualizations at the provided link. Attention is crucial in neural network architectures, serving as a primary or secondary mechanism. Attention is essential in neural network architectures, serving as a primary or secondary mechanism for connecting different parts of a model. It allows for visualizing the inner workings of a model and aids in tasks like machine translation and speech recognition. In creative domains like music, there is less \"groundtruth\" for what should be attended to. In language modeling, self-attention builds context by retrieving relevant information from the past to predict the future. Music Transformer, based on self-attention, is effective in modeling music and can generate sequences with repetition on multiple timescales with long-term coherence. The use of relative attention improves sample quality and allows the model to generalize beyond lengths observed during training time. This paper introduces a tool for analyzing the attention structure in these models. In this paper, a tool is introduced for visualizing self-attention on music using an interactive pianoroll with the Music Transformer model. The analysis includes examining self-attention structures in existing music to validate musical theories and understanding how past notes influence future ones during generation. Music attention is explored on JSB Chorales and Piano-e-Competition datasets, showing attention patterns related to harmonic progression, voice-leading, motifs, and gestures. The distribution and focus of heads in multihead-attention vary across temporal regions in JSB Chorales. The study compares regular attention to relative attention in generating music, focusing on different temporal regions. Relative attention shows more consistency in attending to all voices and chords, creating a musical arc. Regular attention tends to be more local, resulting in certain voices repeating notes. The JSB Chorales dataset is used for training generative models for symbolic music, representing music as a sequence of discrete tokens. The dataset contains scored choral music represented as a pianoroll with pitch and time discretized to sixteenth notes. It uses the BID4 performance encoding with a vocabulary of NOTE_ON events, NOTE_OFFs, TIME_SHIFTs, and VELOCITY bins. The Transformer BID6 is a sequence model based on self-attention with multiple heads to focus on different parts of the history. Regular attention in music generation focuses locally, while relative attention creates a musical arc by attending to all voices and chords. The attention mechanism in the Transformer model includes regular attention and relative attention, with relative attention modulating attention logits based on pairwise distances between queries and keys. The visualization tool for exploring self-attention allows users to view attention arcs connecting queries to earlier parts of the sequence, with different colors representing different heads and line thickness indicating attention weights. Animation is supported for further inspection. The tool supports animation for inspecting consistency of phenomena throughout a sequence. Different heads focus on varying timeframes, possibly due to relative attention modulating based on pairwise distance. Comparison between regular and relative attention on JSB Chorales shows relative attention aligns voices better, generating musical phrasing. The visualization tool allows for exploring music self-attention, showing how relative attention in Transformer models focuses on the entire passage compared to regular attention. The tool demonstrates how the model attends to different aspects of music based on the query, such as motifs, scales, and chord progressions. The visualization tool presented allows for exploring music self-attention in Transformer models, showing how the model attends to different aspects of music. Preliminary observations have been made, aiming to further our understanding of how these models generate music."
}