{
    "title": "r1erRoCqtX",
    "content": "Metric embeddings are useful representations of associations between entities, learned through optimizing a loss objective with stochastic gradient updates. This work proposes structured arrangements using randomized microbatches to accelerate training, showing a 3-20% improvement experimentally. Embeddings are a powerful performance knob for SGD, independent of other hyperparameters. They capture associations between entities and are used in various tasks like text document embeddings, user-video associations, and graph nodes. Associations can involve entities of the same or different types and are distilled by reweighing raw interactions. Embeddings are computed by minimizing a loss objective through stochastic gradient descent, which is more efficient than working with full gradients. Various tunable parameters and methods aim to improve quality and speed of convergence, including per-parameter tuning of the learning rate. In this work, principled schemes are introduced to control the arrangement of examples into minibatches, termed coordinated arrangements. This is separate from optimizations that alter the distribution of training examples, learning rate, or minibatch size. The baseline practice of independent arrangements is challenged in favor of coordinated arrangements, which are more likely to place corresponding associations. Coordinated arrangements in minibatches improve updates by pulling similar vectors closer and encode similarity structure effectively. Microbatches place corresponding associations together while adhering to the training examples' distribution. Locality Sensitive Hashing (LSH) maps refine microbatches for better association alignment. Locality Sensitive Hashing (LSH) maps refine microbatches to improve association alignment by leveraging available proxies of entity similarity. Efficient generators create basic and refined microbatches for LSH functions, which are then grouped into minibatches. This design allows for tuning microbatches based on the problem and training stage, enhancing the effectiveness of different arrangements in experiments on synthetic data sets. The paper discusses the benefits of basic coordination in training and the effectiveness of LSH refinements. It reports a consistent 3%-30% reduction in training compared to independent arrangements. The sections cover background on loss objectives, LSH microbatches, coordinated minibatch arrangements, experiment results, and properties of coordinated arrangements for faster convergence. The data consists of associations between focus and context entities, with \u03baij representing association strength. Embeddings are trained to minimize a loss objective, focusing on Skip Gram with Negative Sampling (SGNS) loss. Positive associations are drawn based on \u03baij, with random associations used as negative examples. The SGNS objective aims to maximize the log likelihood of positive and negative examples by using random associations as negative examples to prevent vector collapse. Optimization involves stochastic gradient updates on randomly initialized embedding vectors. The SGNS objective involves stochastic gradient updates on minibatches of examples with positive and negative instances, focusing on one-sided updates for either context or focus embedding vectors. This approach allows for more precise matching of negative examples to positive ones, improving coordination and arrangement. The SGNS objective involves stochastic gradient updates on minibatches of examples with positive and negative instances, focusing on one-sided updates for either context or focus embedding vectors. Minibatch arrangement schemes determine how examples are organized into minibatches, with a distribution over subsets of positive examples called microbatches. Subset probabilities vary between schemes, with different distributions for focus and context designations. The microbatch construction for focus updates involves drawing independent microbatches until a total of b or more positive examples are obtained. Negative examples are generated for each positive example, and training alternates between focus and context updating minibatches. The baseline method IND uses microbatches with a single positive example, while coordinated arrangements depend on designation. Basic coordinated microbatches are generated using Algorithm 3. Algorithm 3 generates basic coordinated microbatches (COO) for focus-updates, maximizing co-placement probability while respecting marginal probabilities. Basic microbatches consist of positive examples with a shared context, ensuring inclusion of related examples. Context-update microbatches also maximize co-placement of examples with a shared focus. The microbatch generator selects examples based on marginal probabilities. Positive examples are included in basic coordinated microbatches with focus and context designations. Preprocessing is done to efficiently draw random contexts. The preprocessing for efficient random context selection involves generating an index for retrieval based on a threshold value. This process is linear in the sparsity of the data and minimally impacts efficiency. The microbatch generator efficiently draws contexts and indexes entries, aiding in updating microbatches for similar entities. To improve microbatch efficiency, entities with similar embeddings are grouped using locality sensitive hashing (LSH) to refine microbatches. This reduces size and variance of gradients, enhancing training quality. To enhance microbatch efficiency, entities are grouped using LSH to refine microbatches, resulting in smaller, higher quality batches with improved precision but lower recall. LSH modules used are only a coarse proxy of target similarity, generated from weak signals or partially-trained models. Two LSH modules based on Jaccard and Angular LSH are employed to create maps for focus or context entities based on microbatch designation. The Jaccard LSH module is outlined in Algorithm 4, focusing on mapping entities based on association vectors. The Angular LSH module, outlined in Algorithm 5, uses a \"coarse\" embedding to create LSH maps based on entity angles. The cosine similarity of entities depends on the angle between their coarse embedding vectors. Multiple LSH maps can be applied to refine microbatches, decreasing size and increasing quality. The number of maps can be set statically or adaptively, with precomputed maps for efficiency. The text discusses different methods for training embeddings using minibatch arrangements, including independent arrangements, basic coordinated microbatches, and coordinated arrangements with LSH partitioned microbatches. The goal is to refine microbatches to improve quality by decreasing size and increasing efficiency. LSH with a pre-computed 3-dimensional embedding is applied to obtain microbatches of capped size. Tunable arrangements (MIX) switch between COO, COO+LSH, and IND to balance recall and precision. The switch points are hyperparameters determined through grid search. The methods aim to improve training efficiency without maximizing performance. The study presents and evaluates basic simple methods using synthetic data sets generated with the stochastic blocks model BID5. Different arrangement methods are explored by varying the number and size of blocks, focusing on simplicity and symmetry of the data. Parameters for the generative model include dimensions, number of blocks, interactions, and in-block probability. The goal is to compare arrangement methods while excluding other potential optimizations for asymmetric data. The matrix is generated by initializing associations \u03ba ij = 0 and drawing r interactions independently. A row index i is selected uniformly at random, and with probability p, a column j in the same block as i is chosen. Otherwise, a column j outside the block of i is selected. The final association \u03ba ij is the number of times the interaction (i, j) was drawn. Parameters used in experiments include n = 10^4, r = 10^7, p = 0.7, and varying block sizes B. The implementation was in Python using TensorFlow library BID0. Building on the word embedding implementation of BID16, we specified minibatches and trained embeddings with and without a bias parameter. The overall performance was significantly better with the bias parameter, leading to improved results in a two-sided manner. We used fixed and polynomially-decaying uniform learning rates with tuned parameters for different minibatch sizes and embedding dimensions. Two measures of embedding quality were used with respect to the ground truth blocks. The quality of an embedding is measured using the cosine gap and precision at k. The cosine gap compares the average cosine similarity between positive and negative examples, while precision at k focuses on the top predictions. Results for different parameters are reported. Results for d = 50, b = 64, and varying block sizes (B = 10, 20, 50, 100) are shown in FIG1 (precision) and FIG2 (cosine gap). Coordinated arrangement methods show faster convergence than the baseline IND method across all block sizes. COO+LSH methods balance microbatch size and co-placement quality, with higher recall being beneficial early in training. In early training, higher recall is beneficial, with COO dominant initially but deteriorating later for larger blocks. Jaccard COO+LSH generates large microbatches, but when used in a MIX with IND, training is faster. IND's switch point occurs in early-mid training, with gains retained. Jaccard* and angular* COO+LSH methods perform well for b = 64 and b = 256. COO outperforms them in early training for b = 4, while MIX outperforms overall. The study compares different training methods using COO, COO+LSH, and IND arrangements with varying minibatch sizes and embedding dimensions on MOVIELENS1M and AMAZON datasets. Results show coordinated methods outperform the baseline IND arrangement, with training gains of 5-30% observed. The peak quality is lower for lower embedding dimensions, justifying their use as a \"coarse\" proxy with angular LSH. The AMAZON dataset SNAP contains fine food reviews from users on food items. Review scores were preprocessed and reweighed in the MOVIELENS1M dataset. Positive examples were sampled for testing, while negative examples were random zero entries. Quality was measured using the cosine gap equation, with MIX and COO+LSH Jaccard being the best performers on both datasets. The MIX and COO+LSH Jaccard were the top performers on MOVIELENS1M and AMAZON datasets. Training gains with respect to the baseline are reported, showing a consistent reduction in training time. Coordinated arrangements help accelerate convergence by making gradient updates more effective and preserving expected similarity in fractions of epochs. Processing updates on corresponding associations of entities in the same minibatch increases the cosine similarity of their embedding vectors. The SGNS loss term for positive examples increases cosine similarity. Updating corresponding examples in the same minibatch improves similarity between focus entities. This is achieved more effectively with COO arrangements. The SGNS loss term for positive examples increases cosine similarity, especially with COO arrangements that preserve entity similarity information. Coordinated samples maximize similarity preservation, unlike IND arrangements where similarity information disperses rapidly. The text discusses the impact of coordinated and independent sampling schemes on similarity preservation in training data. It demonstrates self-similarity experimentally with stochastic block matrices by selecting small sets of positive training examples. The coordinated sampling scheme maximizes similarity preservation, unlike independent sampling where similarity information disperses rapidly. The text discusses the impact of coordinated and independent sampling schemes on similarity preservation in training data. It demonstrates self-similarity experimentally with stochastic block matrices by selecting small sets of positive training examples. Training embeddings on smaller sets of examples with coordinated sampling consistently attains faster results. In experiments with coordinated and independent sampling schemes, coordinated selection of training examples led to faster convergence in earlier epochs and higher peak quality with fewer examples per entity. The coordinated selection method was found to be a powerful performance knob for embedding computations with stochastic gradients. Future work aims to explore coordinated arrangement with other loss objectives, deeper networks, and more complex association structures. In experiments, coordinated selection of training examples led to faster convergence and higher quality with fewer examples per entity. Training gain of coordinated arrangements over baseline IND increases with minibatch size. Methods that cap microbatch size by minibatch size perform better with larger minibatches, allowing for higher recall of helpful co-placements. Gains of different methods over baseline IND arrangements are quantified in tables for precision at k = 10 and cosine gap. Results for cosine gap are reported in Tables 5 and 6 for Jaccard LSH MIX and angular* LSH MIX, showing peak quality and training efficiency. Coordinated methods consistently had training gains of 5-30%, with larger gains seen with smaller blocks and larger minibatches. Jaccard* outperformed Jaccard MIX for larger minibatch sizes. The effect of dimension on embedding quality and convergence is also discussed. The text discusses the impact of dimension on embedding quality and convergence, focusing on training with IND arrangements. Results show the cosine gap and precision for different dimensions and data sets. Training gains with Jaccard* arrangement compared to IND baseline are reported, with observations of faster convergence with higher dimensions. The text discusses the impact of dimension on embedding quality and convergence, focusing on training with IND arrangements. Results show faster convergence with higher dimensions, but higher per-example training cost. Lower dimensions are more computationally efficient for reaching a certain quality level. Training gains with Jaccard* arrangement are reported, showing reduced training amounts with MIX arrangement. Peak quality increases with dimension on recommendations and stochastic blocks data sets. The peak quality increases with dimension, with d = 3 having lower quality than d = 50. Higher dimensions are more effective for better quality. The d = 3 embeddings are used to accelerate training of d = 50 embeddings. Training gains with MIX arrangement are reported for IND baseline, showing reduced training amounts."
}