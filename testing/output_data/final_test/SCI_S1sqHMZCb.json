{
    "title": "S1sqHMZCb",
    "content": "In this work, NerveNet is proposed to model the structure of an agent as a graph for learning structured policies in continuous control. It propagates information over the agent's structure to predict actions for different parts, showing comparable performance to state-of-the-art methods on MuJoCo environments. Additionally, customized reinforcement learning environments are introduced for benchmarking structure transfer learning tasks, where NerveNet outperforms other models in policy learning and transfer capabilities. NerveNet outperforms other models in policy learning and transfer capabilities, even in a zero-shot setting. Deep reinforcement learning (RL) has gained attention for applications like playing Atari Games and Go. Advances in robotics using RL techniques have been significant, especially for agents with multiple dependent controllers like humanoid robots. Previous RL approaches use MLP to learn the agent's policy based on observations from the environment. The MLP policy predicts actions for each joint and body, aiming to discover latent relationships. In our work, we aim to exploit the body structure of an agent, relying on the fact that most robots and animals have a discrete graph structure representing joints and physical dependencies. We use a Graph Neural Network called NerveNet to propagate information between different body parts based on the graph structure, allowing for more efficient action output. NerveNet utilizes the agent's body structure to learn the correct inductive bias, reducing overfitting. It is suitable for structure transferring tasks due to shared model weights. The model is evaluated on standard RL benchmarks like OpenAI Gym, showing effectiveness in transfer learning. Our NerveNet model achieves comparable results to state-of-the-art methods on standard RL benchmarks like OpenAI Gym. We introduce customized RL environments for structure transfer tasks, demonstrating superior performance compared to competitors, including zero-shot learning capabilities. The main contribution of this paper is exploring transferable and generalized features using graph neural networks in the NerveNet model. It allows powerful transfer learning between structures, surpassing previous models, and excels in multi-task learning. The demo and code for this project can be found at http://www.cs.toronto.edu/\u02dctingwuwang/nervenet.html. The section introduces notation, constructing graphs for agents, describing NerveNet, and explaining the learning algorithm. The learning algorithm for the NerveNet model formulates locomotion control problems as an infinite-horizon discounted Markov decision process (MDP). The agent generates a stochastic policy based on the current state to maximize expected rewards. NerveNet uses a Walker-Ostrich example to fetch elements from the observation vector and compute messages between neighbors. NerveNet formulates locomotion control problems as an MDP, generating a stochastic policy to maximize rewards. The model uses tree graphs, with body and joint nodes, to represent robots and animals. The MuJoCo engine organizes agents using an XML-based kinematic tree. The model can be applied to arbitrary graphs, with the policy produced by collecting outputs from each controller. The text discusses the construction of a tree graph to represent an agent's body nodes and joints, with a focus on the CentipedeEight example. The root node provides additional information about the agent, such as the target position in the Reacher agent. The graph structure is illustrated in Fig. 1, with nodes colored differently to represent various elements of the agent. The policy is parametrized using NerveNet, a Graph Neural Network, for locomotion control problems. The text introduces the graph structure of the agent, denoting nodes and edges. It discusses node types and edge types, along with the neighborhoods of nodes. The focus is on directed graphs, with each node associated with a type corresponding to body, joint, or root. In the graph structure of the agent, nodes are connected by edges with different types to capture relationships. Node types represent varying importance, while edge types describe diverse connections for information propagation. The model incorporates two time concepts: the environment time step (\u03c4) for RL tasks and the internal propagation step (t) of NerveNet. This distinction helps in decision-making processes within the system. The agent receives observations at each time step in the environment, denoted as s \u03c4. These observations are concatenated for each node into an observation vector. The observation vector is then transformed into a fixed-size state vector through an input network. The NerveNet propagation model mimics synchronous message passing in distributed computing. The NerveNet propagation model mimics synchronous message passing in distributed computing, specifically focusing on how the state vector of each node is updated during propagation steps. Message vectors are computed for each edge, and messages from incoming neighbors are aggregated for each node. The NerveNet propagation model updates node state vectors by aggregating messages from incoming neighbors and applying an update function. This process is recurrently applied for a fixed number of time steps to obtain final state vectors for all nodes. The output model treats the standard deviation in the same way as the mean of the Gaussian distribution for each action. Predictions are made for each individual node, with a MLP taking final state vectors as input to produce the mean of the action for the corresponding actuator. Different sharing schemes for the MLP instance are available, such as nodes with similar physical structure sharing the instance. The agent generates a stochastic policy \u03c0 \u03b8 (a \u03c4 |s \u03c4 ) to interact with the environment, aiming to maximize its cumulative return. Proximal policy optimization (PPO) by BID24 is used to optimize the expected reward. NerveNet utilizes PPO to optimize expected reward by sampling trajectories and performing optimization on surrogate objective within a trust region. The algorithm clips probability ratio, adds KL-divergence penalty, and minimizes a loss function with GAE and clip value constraints. NerveNet uses PPO for optimizing expected reward by sampling trajectories and performing optimization on a surrogate objective within a trust region. The algorithm adjusts coefficients \u03b2 and \u03b1 to maintain KL-divergence constraints and balance value loss. Different options for the value network include using one GNN as both policy and value network, using separate GNNs, or using one GNN as policy network and an MLP as the value network. The output for the value GNN is a scalar instead of a vector of mean action. Reinforcement learning has seen significant success in various applications, with deep neural networks enabling agents to excel in Atari Games and games like Go. Many algorithms have been developed for training agents in continuous control problems. Hierarchical RL focuses on modeling agents' intrinsic motivation, while some approaches extend deep RL algorithms to MDPS with parameterized action spaces. Graphs have also been utilized in RL. Graphs have been used in RL problems prior to our work, with limited methods for complex multi-joint agents in physical environments. Graph Neural Networks have been generalized to graph-structured data, including convolutional neural networks. Graph Neural Networks (GNNs) utilize convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for graph analysis. GNNs allow for general graph applications and use a propagation process similar to synchronous message passing in distributed computing. In this paper, the use of GNNs, specifically GGNNs, for modeling reinforcement learning agents is discussed. Transfer and Multi-task Learning in RL, focusing on transferring policies between environments, is also highlighted. Agents in reinforcement learning are shown to be prone to overfitting, leading to poor generalization of learned policies. In model-based RL, efforts have been made to increase transferability via learning invariant visual features and meta-learning perspectives. Transfer learning methods using imitation learning and exploiting shared graph structures have also been explored. Multi-task learning has received attention, with some studies focusing on distilled policies capturing common behavior across tasks. The effectiveness of NerveNet on standard MuJoCo environments in OpenAI Gym is verified, along with investigating transfer abilities and multi-task learning capabilities. NerveNet is compared with standard MLP models like TreeNet, which removes the physical graph structure. Experiments are conducted on 8 continuous control benchmarks from Gym, including Reacher, InvertedPendulum, and HalfCheetah. In the study, NerveNet is compared with standard MLP models on various tasks including InvertedDoublePendulum, Swimmer, HalfCheetah, Hopper, Walker2d, and Ant. Grid search was used to find the best hyperparameters, and results were shown with average curves and standard deviations. MLP with the same setup as in a previous study worked best in most tasks. NerveNet performs similarly to MLP in terms of sample efficiency and convergence. TreeNet is generally worse than NerveNet, emphasizing the importance of maintaining the physical graph structure. The study benchmarks the model in structure transfer learning tasks using customized environments based on MuJoCo. Two types of tasks are investigated: size transfer and disability transfer. Size transfer involves training a model on a small agent and applying it to a larger agent, while disability transfer disables components of the original agent's model. The study explores structure transfer learning tasks using customized environments in MuJoCo. One task involves creating Centipede agents with varying lengths, from CentipedeFour to CentipedeForty. Each agent consists of repetitive torso bodies with attached legs controlled by actuators. The goal is for the agent to move quickly along the y-direction. In the study, various Centipede agents are created with different lengths, such as CentipedeFourty, due to resource limitations. The total reward for each time step is calculated based on speed, energy cost, and force feedback. Additionally, size transfer experiments are conducted with instances like \"4to06\" and \"6to10\". Disability transfer experiments involve creating CrippleCentipede agents with disabled back legs. A snake-like agent, BID10, is also designed to move quickly in the environment based on the Swimmer model in Gym. Baseline models are built for structure transfer learning to evaluate the performance of NerveNet. In structure transfer learning, NerveNet reuses weights from small-agent models for large agents with repetitive structures. For MLP based models, weights from the first hidden layer to the output layer are reused when transferring between structures. Another approach, MLP Activation Assigning (MLPAA), assigns weights from small-agent models to corresponding partial weights of large-agent models. No layers are added or removed in this process. For transfer learning, weights are reused from small-agent models to large agents with repetitive structures. The output of the large-agent model remains the same as the small-agent initially. TreeNet and MLPAA assign weights similarly for transfer learning tasks. Random policy and Centipedes environment are also included in the experiments. Training TreeNet on CentipedeFour is challenging but necessary for optimal performance. The TreeNet agent shows promising performance on CentipedeFour, despite optimization challenges. Zero-shot performance is evaluated by applying models without fine-tuning. NerveNet outperforms competitors in most scenarios, except 4toCp06. Transferring from CentipedeFour is harder than CentipedeSix due to torso connections. TreeNet performs well overall. The TreeNet agent performs well on CentipedeFour tasks but struggles with movement compared to other methods. Its high reward is mainly from standing still and gaining alive bonuses. NerveNet is the only model able to walk in the zero-shot setting and outperforms others significantly. It can provide walkable pre-trained models on new agents from CentipedeSix. Fine-tuning experiments show NerveNet's superior performance. By using the pre-trained model, NerveNet reduces the number of episodes needed to reach a solved reward level. The bottleneck of learning for the agent is learning \"how to stand\". MLPAA agents, which copy the learned policy, can bypass this process and achieve good performance. NerveNet exhibits a unique \"walk-cycle\" behavior not seen in other models, resembling the gait of insects like ants. NerveNet outperforms MLP agents in utilizing structure information and avoiding overfitting in various tasks, such as coordinating all legs in CentipedeEight. It also shows superior zero-shot performance in snakes, with starting reward values above 300, indicating effective control of new actuators. In the zero-shot setting, NerveNet outperforms MLP models by effectively coordinating with new actuators, showing good initialization and performance improvement with fine-tuning. However, training NerveNet from scratch is less sample efficient compared to MLP, indicating the optimization challenge of the model. Fine-tuning significantly enhances the sample efficiency of NerveNet. NerveNet shows good initialization and performance improvement with fine-tuning, outperforming MLP models in the zero-shot setting. Multi-task learning with NerveNet is challenging but shows potential, especially in controlling multiple agents in complex physical models like locomotion. Walker multi-task learning taskset is designed to test the model's ability in this regard. The ability of multi-task learning, specifically controlling multiple agents with one network, is evaluated for robustness in Table 3. Results show the average performance of perturbed agents compared to original performance. NerveNet's multi-task learning ability is demonstrated through various baselines, using a vanilla multi-task policy update approach. Gradients are calculated separately for each sub-task, aggregated, and applied to update the network. Additional training difficulty is addressed by linearly increasing the number of update epochs. In NerveNet, weights are shared among different agents for propagation and output. The MLP Sharing approach increases the size of weight matrices with the number of tasks. In the MLP Aggregation method, each element of the observation vector is multiplied separately by one matrix. NerveNet and TreeNet are multi-task models with shared weights among agents. NerveNet outperforms other agents in various tasks, while TreeNet lacks knowledge of agents' physical structure. Single-task MLP baselines were also trained for comparison. In comparison to MLP Sharing, NerveNet shows similar performance across different agents. MLP Sharing has limited performance on four agents and limited improvement on Walker-Hopper. MLP Aggregation and TreeNet struggle with multi-task learning, remaining at low reward levels. NerveNet demonstrates potential in vanilla optimization settings, outperforming baselines. NerveNet maintains performance in multi-task learning, unlike MLP which experiences a significant drop. NerveNet excels at learning generalized features and benefits from training different agents, while MLP methods struggle due to competition among agents. The NerveNet agent shows robustness to parameter perturbation by learning shareable features for its legs and a certain walk-cycle during training. The model's ability to learn generalized features contributes to its robustness. Performance testing on five agents from the Walker task set shows promising results. The NerveNet agent's robustness is attributed to its structure prior, facilitating overfitting. Visualizing learned representations, final state vectors are extracted and PCA is applied. In Figure 10, invariant representations are observed for each pair of legs. Trajectory density map in the feature map shows clear periodic behavior of hidden representations.Adjacent left and right legs' representations are also analyzed. The NerveNet agent's robustness is attributed to its structure, facilitating overfitting. Visualizing learned representations, final state vectors are extracted and PCA is applied. In Figure 10, invariant representations are observed for each pair of legs, demonstrating a phase shift. Several variants of NerveNet are compared based on network type for policy/value representation. NerveNet-MLP and NerveNet-2 outperform NerveNet-1, possibly due to weight sharing sensitivity in optimization methods like PPO. In this paper, a novel model called NerveNet, utilizing a Graph Neural Network to represent the agent's policy, is introduced. NerveNet achieves comparable performance to state-of-the-art methods on standard MuJoCo environments. Customized reinforcement learning environments are proposed for benchmarking structure transfer learning tasks. Policies learned show promising results. NerveNet outperforms other models in transfer learning tasks, using MLP with tanh activation for message computation. Grid search on MLP size is done. Average aggregation and GRU are used in experiments. In MuJoCo, body nodes are paired with joint nodes. Gym environments include angular velocity, twist angle, torque, and position information. The model is compact and standard for experiments. In the centipede environment, different nodes receive specific observations like angular velocity and twist angle. Grid search is conducted on hidden sizes for MLP and NerveNet, with constraints on network shapes. Hyperparameter search details can be found in TAB7. The MLP-Bind method binds weights to share structures among agents. In the centipede environment, weights are shared among agents with similar structures. Zero-shot results are normalized across different models for each transfer learning task. The worst value of results is recorded and used to set the normalization minimum value. The normalization maximum value is calculated based on the maximum value divided by a certain interval length."
}