{
    "title": "HJxrVA4FDS",
    "content": "Forming perceptual groups and individuating objects in visual scenes is crucial for visual intelligence, with connections between neurons playing a key role. Neural network architectures with different connections were evaluated on synthetic visual tasks, showing that horizontal connections aid in spatial propagation for tasks with Gestalt cues, while top-down connections help with tasks involving high-level object cues. The study explores how different neural network connections aid in forming perceptual groups and segmenting scenes into object-based representations, crucial for visual intelligence. It highlights the roles of bottom-up, horizontal, and top-down connectivity in learning to form perceptual groups flexibly. This research challenges the traditional view of feedforward and feedback processes in perceptual grouping tasks, showing how a model with various interactions can enhance visual intelligence. The visual system utilizes feedback mechanisms to refine scene segmentation in complex visual scenes with objects interposed in background clutter. Different feedback strategies are suggested to aid in grouping low-level visual features with their neighbors. The visual system utilizes feedback mechanisms to refine scene segmentation in complex visual scenes with objects interposed in background clutter. Feedback strategies involve grouping low-level visual features with their neighbors based on Gestalt laws and high-level expectations about object shape and structure. These strategies are iterative and rely on recurrent computations. The neural circuits implementing Gestalt vs. object-based strategies for perceptual grouping have been studied in visual neuroscience. Neuroscience studies suggest that perceptual grouping strategies involve horizontal and top-down neural connections. The relative contributions of these connections for grouping tasks are still unclear. This study aims to investigate the function of horizontal vs. top-down connections in perceptual grouping challenges. The study introduces a biologically-inspired deep recurrent neural network architecture for perceptual grouping challenges. They develop lesioned variants of the network to measure the contributions of feedforward and feedback processing. Additionally, they introduce the cluttered ABC challenge and pair it with the Pathfinder challenge to investigate different grouping strategies. The study investigates the ability of different neural network models to leverage object-level or Gestalt cues for grouping. Top-down connections are crucial for grouping with semantic cues, while horizontal connections support grouping without them. The recurrent model with full connections (TD+H-CNN) outperforms other models in solving tasks of varying difficulty levels. Comparing model predictions to human psychophysics data, the TD+H-CNN aligns best with human judgments on segmentation tasks, indicating a match with human visual processing strategies. Recurrent neural networks (RNNs) can approximate discrete-time dynamical systems and are used in sequence learning and computer vision tasks. Autoregressive RNNs learn statistical relationships between neighboring pixels for image generation. Convolutional RNNs incorporate convolutional kernels for object recognition and super-resolution tasks, sharing kernels across processing timesteps to achieve depth with fewer parameters than CNNs. Incorporating convolutional RNN models with feedback gated recurrent units (fGRU) enhances vision models by modeling horizontal and top-down connections between processing layers. The fGRU, more sample efficient than other models, was applied to contour detection in images. The current work explores the use of synthetic visual recognition challenges to evaluate computer vision algorithms, including datasets for instance segmentation and model tolerance to visual clutter. Recent challenges have increased task difficulty and controlled image variability. The current study introduces the \"cluttered ABC\" (cABC) challenge as a complement to the Pathfinder challenge, focusing on recognizing connected paths in images with semantic objects. This challenges network models with parametric controls for task difficulty. The \"cluttered ABC\" (cABC) challenge is introduced as a complement to the Pathfinder challenge, focusing on recognizing connected paths in images with semantic objects. The cABC task may be difficult to solve using the same strategy as Pathfinder due to differences in image features and variability. The cABC challenge complements the Pathfinder challenge by focusing on recognizing connected paths in images with semantic objects. It features English-alphabet characters that are transformed in appearance and position, making it suitable for a different grouping strategy compared to the Pathfinder challenge with smooth and flexible curves. The cABC challenge involves recognizing English-alphabet characters that are globally stable but locally degenerate, with overlapping letters forming arbitrary conjunctions. This poses a challenge for feedforward models like ResNets due to the need to learn a large number of templates. The challenges of recognizing cABC letters involve forming arbitrary conjunctions, requiring a large number of templates for discrimination. Human observers can easily solve these challenges with minimal training, indicating the capability of biological vision to rely on a single cue for perceptual grouping. The text discusses how neural networks can be affected by dataset biases and the importance of inductive biases in deep learning. The authors propose synthetic challenges with varying levels of difficulty to measure the generalization capabilities of network architectures. This method, called \"straining,\" helps dissociate architecture expressiveness from incidental factors tied to specific image domains. The difficulty of challenges like Pathfinder and cABC is controlled by factors such as the length of target curves in images and the number of possible positional arrangements between letters. In cABC, the correlation between random transformation parameters is decreased to increase difficulty. In cABC datasets, random affine transformations are independently applied to each letter, leading to varied conjunctions between overlapping letters. The variability of transformation parameters is balanced across difficulty levels to maintain consistent individual letter shape. Different configurations of a recurrent CNN architecture are used to test the role of horizontal vs. top-down connections for perceptual grouping. See Appendix A for more details on cABC generation. The model utilizes recurrent modules called fGRUs to implement top-down and/or horizontal feedback, comparing their contributions in solving tasks over a basic deep convolutional network. Additionally, representative feedforward architectures like residual networks and U-Net are tested for computer vision. The fGRU module integrates input from external drive X and internal state H. The fGRU integrates external drive X and internal state H to update the state for processing. Feedback connections spread and evolve during processing timesteps, allowing interaction between units. The final timestep's hidden state is used for category prediction. In the TD+H-CNN architecture, fGRU implements horizontal or top-down feedback using activities from preceding layers. The TD+H-CNN architecture incorporates three fGRU modules into a CNN for top-down and horizontal feedback. The fGRU utilizes spatial kernels to update its hidden state and implements feedback by interacting with activities from other fGRUs in different layers. The TD+H-CNN architecture combines Conv-pool and Transpose Conv pathways like U-Net, processing images in a bottom-up to top-down loop. It includes three fGRU modules for horizontal and top-down connections, with variants for lesioning fGRU modules. Performance of popular neural network architectures on Pathfinder and cABC is also measured. The performance of popular neural network architectures on Pathfinder and cABC challenges was measured using Residual Networks (ResNets) with different numbers of layers and a U-Net with a VGG16 encoder. The U-Net utilizes skip connections to pass activities between layers of a downsampling encoder and an upsampling decoder, effectively simulating top-down feedback for one timestep. The difficulty of the challenges was calibrated using the smallest dataset size that achieved over 90% validation accuracy, which was 60K images for Pathfinder and 45K images for cABC. These image budgets were used for all difficulty levels of the challenges. Our objective was to evaluate model sample efficiency by training models on image budgets for different difficulty levels of challenges. We used early stopping criteria during training and trained models with batches of 32 images using the Adam optimizer. Each model was trained with five random weight initializations and the best performance was reported. Our study revealed that human participants performed well on both Pathfinder and cABC challenges. The study evaluated model sample efficiency by training models on image budgets for different difficulty levels of challenges. Human participants performed well on both Pathfinder and cABC challenges, utilizing different grouping strategies for each challenge. Recurrent architectures showed complementary contributions of horizontal vs. top-down feedback, with H-CNN excelling in Pathfinder challenge and struggling in cABC challenge, while TD-CNN performed well in cABC challenge but struggled in Pathfinder challenge. Top-down interactions were found to help process object-level grouping cues. The study found that the TD+H-CNN model, with both horizontal and top-down feedback, performed significantly better than feedforward models and matched human performance in processing grouping cues at all difficulty levels. Recurrent models relied on recurrence to achieve success in the challenges tested. The study tested models on controlled variants of the cABC challenge, finding that local ambiguities and occlusion between letters require top-down feedback for grouping. Recurrent architectures outperformed strictly-feedforward models, with only ResNet-18 and U-Net successfully solving the cABC challenge. The study found that while ResNet-18 and U-Net successfully solved the cABC challenge, they struggled with difficult Pathfinder datasets. No feedforward architecture could efficiently learn both grouping strategies, indicating a trade-off between Pathfinder and cABC performance. The H+TD-CNN and human observers were the only ones to solve all levels of both challenges, prompting a large-scale psychophysics study to compare visual strategies between models and humans. Participants were recruited on Amazon Mechanical Turk to complete web-based psychophysics experiments, categorizing images in Pathfinder and cABC challenges. Each participant viewed a subset of images in a challenge, with 20 human decisions gathered for each image. Human performance was measured as a logit of average accuracy. A two-step procedure was used to measure how much human decision variance each model explained. The study estimated human inter-rater reliability and compared it to the correlation between each model's logits and human logits to calculate the explained variance of human decisions. The TD+H-CNN and H-CNN models outperformed the 50-and 152-layer ResNets in explaining human decision variance on Pathfinder images. Perceptual grouping is crucial for understanding the visual world. Neural network architectures were tested on synthetic visual tasks, revealing a dissociation between horizontal and top-down feedback connections during classification training. This study provides computational evidence for the distinct roles of these connections in perceptual grouping. Our study demonstrates the limitations of network models relying solely on feedforward processing for perceptual grouping tasks. Deep ResNets outperformed shallower ResNet-18 on the Pathfinder challenge, while the latter performed better on the cABC challenge. This adds to existing literature suggesting the importance of recurrent circuits in visual processing. The cluttered ABC challenge aims to test models and humans on making perceptual grouping judgments based on object-level semantic cues. Images in the dataset contain letters close enough to overlap, using fonts with uniform strokes to minimize local image cues. Random transformations are applied to the fonts for diversity. Random transformations, including affine transformations, geometric distortions, and pixelation, are applied to each letter in the cABC challenge to eliminate local category cues and ensure image variability. Three different linear transformations - rotation, scaling, and shearing - are used twice on each letter with varied parameters to create diverse forms. Random transformations, such as rotations, scaling, and shearing, are applied to each letter in the cABC challenge to increase image variability. These transformations are divided into \"letter-wise\" and \"common\" parameters to independently vary each letter image. The text describes the application of random transformations to increase image variability in the cABC challenge. This includes independently increasing variability at the image level while keeping letter-wise variability constant. Geometric warping and pixelation are applied to letter images to further enhance variability. The process involves pixelating the letter image into a grid, applying random translations, and sampling letter positions on an invisible circle with increasing difficulty levels. By sampling parameters from larger intervals, the relative positional variability of letters increases in harder datasets. Difficulty levels are determined by choosing different ranges for parameters such as letter rotation and displacement. Letter images are placed on sampled positions by aligning their \"center of mass\" with the coordinates in an image. Geometric warping is applied to letter images using a \"warp template\" sampled from Gaussian images. The text chunk discusses the distributions of transformation parameters used in cluttered ABC challenges, including uniform, normal, and log-normal distributions. It also mentions two control cABC challenges: luminance control and positional control. The text chunk introduces two control cABC challenges: luminance control and positional control, which provide additional cues for determining the extent of each letter in the image. The challenges involve rendering letters with specific pixel intensity values and ensuring they do not touch or overlap. The cABC challenges include luminance and positional control, offering cues for image category inference. Spatial separation minimizes clutter interference. No straining effects were observed in tested models, supporting the object-based grouping strategy. Top-down feedback is crucial when local grouping cues are absent. The cABC challenge involves providing Gestalt grouping cues through spatial and luminance segregation. Architectures without top-down feedback can solve the challenge, indicating it's not due to image features. Recurrent architectures are used to understand computations in solving segmentation challenges in the Pathfinder and cABC tasks. In the segmentation problem, models are trained on 10 thousand images from the cABC challenge and 40 thousand images from the Pathfinder challenge. The task is to predict the object tagged by a single marker per image. Different recurrent architectures are tested, including LSTM and GRU replacements for fGRU modules. The U-Net architecture is used without the readout block. Target curves in the Pathfinder challenge are analyzed. The Pathfinder challenge target curves were best segmented by H-CNN (0.79 f1 score), followed by H-CNN with GRU (0.72) and H-CNN with LSTM (0.69). The cABC challenge was best segmented by TD-CNN (0.83), followed by TD-CNN with LSTM (0.64) and TD-CNN with GRU (0.62). Horizontal architectures did not perform well in either challenge, showing a performance dissociation between horizontal and top-down connections in recurrent architectures. The method involves comparing the norm of each per-pixel feature vector in the low-level recurrent state at each timestep with its previous timestep to infer the spatial layout of activities evolving over time. Testing different recurrent modules like LSTMs and GRUs showed that they did not perform as well as the fGRU-based architecture on grouping challenges. The fGRU module, with its multiplicative and additive interactions, is crucial for learning stable transitive grouping in Pathfinder. It takes external drive and recurrent state inputs to produce an updated recurrent state. Learnable gates in the fGRU support RNN training, enabling the evolution of the recurrent state through two discrete stages. The fGRU module utilizes learnable gates to support RNN training and optimize performance through various heuristics. Parameters such as gate biases, scale parameter, and suppression/facilitation values are initialized to specific values to enhance training stability and gradient dynamics. Additionally, an extra learnable gate is incorporated for top-down connections, improving training stability further. The fGRU module introduces an extra gate for top-down connections, allowing for both additive and multiplicative combinations after each convolution step. This enables a wider range of nonlinear computations to be learned compared to traditional GRU models. The fGRU module introduces an extra gate for top-down connections, enabling a wider range of nonlinear computations. Three recurrent convolutional architectures and one feedforward variant were constructed using the fGRU module. The main recurrent network architecture, TD+H-CNN, consists of a convolutional and transpose-convolutional backbone with experiments run on NVIDIA Titan X GPUs. The fGRU module includes two convolutional layers with 7x7 kernels and 20 output channels. The feature activity is then processed by the first fGRU module with 15x15 kernels for horizontal interactions. The output goes through batch normalization, pooling, and a downsampling block with convolutional stacks increasing output channels from 20 to 128. This activity is then fed to another fGRU module in the top layer. The output of the second fGRU module is passed through an \"upsampling\" block with two transpose convolutional stacks, reducing output channels from 128 to 12 while progressively upsampling the activity. Each stack includes a 4x4 kernel transpose convolutional layer followed by ReLU activations and batch normalization. The resulting activity is then sent to the third fGRU module for further processing. The final recurrent activity from fGRU (1), H (1), is extracted in the 8th timestep and processed by the \"Readout\" block with convolutional layers and global max-pooling. The TD-CNN architecture implements top-down feedback only, while the TD+H-CNN includes horizontal feedback. The TD-CNN architecture disables horizontal connections in the fGRU, allowing only top-down feedback to drive recurrent updates in the low-level layer. The H-CNN disables fGRU (3), removing both downsampling and upsampling pathways from contributing to the final output. The BU-CNN is a purely feedforward architecture that lesions both horizontal and top-down feedback, equivalent to a deep convolutional encoder-decoder network when run for only one timestep. The BU-CNN is equivalent to a deep convolutional encoder-decoder network for one timestep. Visual categorization experiments were conducted to understand perceptual grouping abilities using stimuli similar to Eberhardt et al. Separate experiments were done for \"Pathfinder\" and \"cluttered ABC\" challenge datasets. Participants in the experiments were recruited from Amazon Mechanical Turk for the \"Pathfinder\" and \"cluttered ABC\" challenges. Each participant was presented with 90 stimuli and had a specific response time window. The stimuli were categorized as \"Same\" or \"Different\", with performance measured in terms of accuracy. The performance results of human participants in the experiment are shown in Fig. 4. Stimuli videos were generated for challenge images with different response times. Each video included a cross image before the stimulus onset. Participants were randomized into \"Same\" and \"Different\" class labels for responses. Training phase consisted of 6 trials to ensure understanding of the task before 15 experimental trials. Participants underwent a training phase with 15 trials of varying difficulty levels, responding with + or - within a time limit and receiving feedback. They then completed 90 trials without feedback, indicating \"Same\" or \"Different\" within a set response time. Difficulty levels changed every 30 trials, with rest periods in between. Instructions were given before each difficulty level block. The experiments were conducted using jsPsych and .webm video format for fast loading times. The stimuli videos were generated using challenge images converted to 256 \u00d7 256 for consistent sizing. Configuration variables for assignments were stored in a MySQL database, including finger assignment ID, difficulty ordering ID, trials list, assignment ID, and response time. Trials list stored as csv file with paths to stimulus videos. Responses filtered for reliability if rendered in less than 450 ms. Accuracy calculated across participants for different difficulty levels. The accuracy across participants for different difficulty levels was calculated using a bootstrapping procedure to determine standard error. Activity time-courses for the Pathfinder challenge and cABC challenge (Hard) were shown in Figures S10 and S11."
}