{
    "title": "S1lOTC4tDS",
    "content": "Dreamer is a reinforcement learning agent that solves long-horizon tasks purely by latent imagination. It efficiently learns behaviors by backpropagating analytic gradients of learned state values through trajectories imagined in a compact state space of a learned world model. Dreamer outperforms existing approaches in data-efficiency, computation time, and final performance on 20 challenging visual control tasks. This ability allows intelligent agents to achieve goals in complex environments without encountering the exact same situation twice. The ability to generalize to novel situations is achieved by building world models from past experience. These models represent an agent's knowledge parametrically and can predict future outcomes. Latent dynamics models abstract high-dimensional images to compact state spaces, enabling efficient prediction and learning of behaviors. Advances in deep learning and latent variable models make learning effective latent dynamics models feasible. Behaviors can be derived from these models by maximizing imagined rewards. Dreamer is an agent that learns long-horizon behaviors from images purely by latent imagination, using a novel actor critic algorithm that considers rewards beyond the planning horizon and efficiently utilizes neural network dynamics. The paper introduces Dreamer, an agent that learns long-horizon behaviors from images through latent imagination. It utilizes a novel actor critic algorithm to optimize values and actions in a learned latent space, enabling efficient policy optimization with analytic gradients. The agent observes images in 64x64x3 pixel arrays and tackles various challenging control tasks. Key contributions include learning long-horizon behaviors in imagination. Dreamer is an agent that learns long-horizon behaviors from images through latent imagination. It predicts actions and state values to overcome shortsightedness in model-based agents. Empirical performance shows Dreamer surpasses existing agents in data-efficiency, computation time, and final performance for visual control tasks. The agent Dreamer aims to maximize rewards by learning dynamics, behavior, and environment interaction. It predicts future rewards by learning latent dynamics from past experience. Dreamer outperforms existing agents in visual control tasks. Dreamer uses a dataset of past experience to predict future rewards. It incorporates any learning objective for the world model and learns action and value models from predicted latent trajectories. The latent dynamics model consists of a representation model, transition model, and a reward model. The action model is updated by propagating gradients of value estimates back through the neural network dynamics. The Dreamer model predicts future model states and rewards using a non-linear Kalman filter or HMM with real-valued states. It learns long-horizon behaviors in a compact latent space through neural network predictions of actions, states, rewards, and values. The latent dynamics define a Markov decision process that is fully observed, denoted with \u03c4 as the time index. The compact model assumes Markovian states denoted by \u03c4 as the time index. Imagined trajectories follow predictions of the transition model and a policy a \u03c4 \u223c q(a \u03c4 | s \u03c4 ). The objective is to maximize expected imagined rewards with respect to the policy. Dreamer uses an actor-critic approach to learn behaviors that consider rewards beyond the horizon by implementing an action model and a value model in the latent space of the world model. The action model predicts actions in the imagination environment, while the value model estimates state values for the action model. The action and value models are trained cooperatively using dense neural networks with parameters \u03c6 and \u03be. The action model outputs a tanh-transformed Gaussian with predicted statistics, allowing for reparameterized sampling. To learn these models, state values of imagined trajectories are estimated. Dreamer uses imagined trajectories to predict forward for an imagination horizon using actions sampled from the action model. State values are estimated in multiple ways to trade off bias and variance. Learning a state value model makes Dreamer more robust to the horizon length. The action model is trained cooperatively with the value model using dense neural networks. Dreamer uses imagined trajectories to predict forward for an imagination horizon using actions sampled from the action model. V \u03bb estimates rewards beyond k steps with the learned value model to balance bias and variance. Learning a value function in imagination enables Dreamer to solve long-horizon tasks while being robust to the imagination horizon. The objective is to output actions that result in state trajectories with high value estimates. The value model is updated to regress the targets. The value model is updated to regress the targets using analytic gradients through learned dynamics to maximize value estimates. Dreamer backpropagates through the value model, similar to analytic actor critics. Comparing to actor critic methods like A3C and PPO, Dreamer uses reparameterized sampling in neural networks to compute value predictions. The world model remains fixed while learning the action and value models. Dreamer backpropagates through the value model, extending multi-step Q-learning using learned dynamics to optimize policy. It focuses on latent dynamics models for long-term predictions and efficient imagination of trajectories. Dreamer focuses on latent dynamics models for long-term predictions and efficient imagination of trajectories. Three approaches for learning representations to use with Dreamer are image reconstruction, contrastive estimation, and reward prediction. The representation model requires transition and reward models for predicting future rewards given actions and past observations. Such representations are crucial for solving control problems, especially when the agent is exploring and the reward signal is limited. Representation learning in Dreamer involves building a world model from sequences of experience data, aiming to predict observations and rewards without overfitting. An information bottleneck framework encourages mutual information between model states and observations/rewards while penalizing information related to dataset indices. This approach enhances generalization and prevents overfitting in the representation learning process. The information bottleneck framework in representation learning, as discussed in Alemi et al. (2016) and Poole et al. (2019), focuses on pixel reconstruction and contrastive estimation. The world model used by PlaNet (Hafner et al., 2019) predicts observations and rewards from model states, incorporating a reconstruction term, reward prediction term, and KL regularizer. This model utilizes neural networks to optimize four distributions jointly for increased performance. Dreamer outperforms PlaNet and D4PG in data efficiency and performance. It reaches an average performance of 802 after 2 million environment steps, compared to PlaNet at 312 and D4PG at 786 after 1 billion steps. The model combines recurrent state space and convolutional neural networks for representation and observation models. The parameter vector is updated using reparameterization gradients. Accurately predicting pixels in visually complex environments can be challenging. Instead of reconstructing, the model predicts states using the InfoNCE bound. The state model, implemented as a CNN, optimizes the bound with reparameterization gradients. This approach efficiently extracts limited information without pixel prediction. In experiments, Dreamer is evaluated on 20 visual control tasks with various challenges like sparse rewards and 3D environments. Tasks have image observations, agent observations are 64x64x3 images, actions range from 1 to 12 dimensions, rewards are between 0 and 1, episodes contain 1000 steps, and initial states are randomized. Implementation used a single Nvidia V100 GPU and 10 CPU cores per training run with TensorFlow. Visualizations of the agent are available at https://dreamrl.github.io. Our implementation of Dreamer uses a single Nvidia V100 GPU and 10 CPU cores per training run with TensorFlow Probability. The training time is 10 hours per 10^6 environment steps without parallelization, outperforming online planning using PlaNet and D4PG. Dreamer is compared to baselines like D4PG and PlaNet, showcasing its effectiveness in various tasks. Dreamer outperforms D4PG with an average score of 802 across tasks after 2 * 10^6 environment steps. It inherits the data-efficiency of PlaNet, showing that the learned world model can generalize from small amounts of experience. Dreamer's success demonstrates that learning behaviors through latent imagination can surpass methods based on experience replay. Dreamer's ability to learn long-horizon behaviors is compared to alternatives using action models and online planning. The value model in Dreamer makes it more robust and results in high performance even for short horizons. Dreamer outperforms alternatives on 15 out of 20 tasks and ties on 3. It can be used with any dynamics model for representation learning, with choices including pixel reconstruction, contrastive estimation, and pure reward. Dreamer outperforms alternatives on various tasks by efficiently learning long-horizon behaviors for visual control through latent imagination. Reward prediction alone was not effective in solving tasks, highlighting the importance of representation learning approaches like pixel reconstruction. Further details and comparisons are provided in the paper's appendix. Various models such as E2C, RCE, World Models, PlaNet, SOLAR, I2A, Gregor, VPN, MVE, STEVE, and AlphaGo utilize latent dynamics and imagination to solve tasks efficiently. These models learn dynamics, plan ahead, and combine action predictions with state values to achieve success in various visual control tasks. Various models utilize latent dynamics and imagination to solve tasks efficiently. Planning with neural network gradients has been challenging to scale. Analytic value gradients leverage gradients of learned immediate action values to learn a policy by experience replay. SVG reduces the variance of model-free on-policy algorithms. ME-TRPO accelerates learning of a model-free agent via gradients of predicted rewards. Dreamer is an agent that learns long-horizon behaviors purely by latent imagination, outperforming previous approaches in data-efficiency, computation time, and final performance on challenging continuous control tasks. The method optimizes a parametric policy by propagating analytic gradients of multi-step values through latent neural network dynamics. Future research on learning representations may be necessary to scale latent imagination to visually more complex environments. The Dreamer agent learns long-horizon behaviors using representation learning. Trajectories are generated from states and actions, rewards and values are predicted, and value estimates are computed. The model components include convolutional encoder and decoder networks, RSSM, and dense layers. Learning updates involve training the world model, value model, and action model with batches of sequences. The Dreamer agent trains world, value, and action models with batches of sequences. The imagination horizon is set to 20, and a slow moving value network is used to compute value estimates. Environment interaction involves collecting episodes with random actions and executing predicted actions with exploration noise. Action repeat is fixed at 2 for all environments. The information bottleneck objective is defined for latent dynamics models. For the generative objective, the first term is lower bounded using the non-negativity of the KL divergence. The marginal data probability is dropped as it does not depend on the representation model. For the contrastive objective, the constant marginal probability of the data under the variational encoder is subtracted, Bayes rule is applied, and the InfoNCE mini-batch bound is used. Comparison of action selection schemes on continuous control tasks in the DeepMind Control Suite from pixel inputs. Dreamer, which learns both actions and values in imagination, is compared to learning only actions in imagination and online planning using CEM without policy learning. Baselines include D4PG, A3C, and SLAC agents. Mean scores over environment steps and standard deviation across 3 seeds are shown."
}