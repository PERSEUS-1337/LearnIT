{
    "title": "r1xZAkrFPr",
    "content": "Deep ensembles improve accuracy, uncertainty, and out-of-distribution robustness of deep learning models. Non-bootstrap ensembles with random initialization also perform well, suggesting other reasons for their success. Bayesian neural networks, theoretically motivated by Bayesian principles, do not perform as well as deep ensembles in practice, possibly due to focusing on a single mode while deep ensembles explore diverse modes in function space. Our study explores the loss landscape of neural networks and measures the similarity of functions in the space of predictions. Random initializations lead to different modes, while optimization trajectories cluster within a single mode predictions-wise. We introduce the diversity--accuracy plane concept and show that random initializations outperform popular subspace sampling methods in terms of decorrelation power. The Bayesian posterior over parameters is given by p(y n |x n , \u03b8), and computing the exact posterior distribution over \u03b8 is computationally expensive. Various approximations have been developed for Bayesian neural networks, including Laplace approximation, Markov chain Monte Carlo methods, variational Bayesian methods, and Monte-Carlo dropout. Maximum-a-posteriori (MAP) estimation is easier to perform and corresponds to a mode of the posterior. The MAP solution is a computationally efficient mode of the posterior distribution. Deep ensembles train multiple neural networks with random initialization to capture diverse solutions for quantifying uncertainty. Bayesian neural networks learn multi-modal posterior distributions, while deep ensembles, inspired by the bootstrap, capture diverse solutions for uncertainty quantification. Empirical studies show that training individual networks with random initialization is effective, with ensembles outperforming approximate Bayesian neural networks in accuracy and uncertainty quantification. Ensembles trained with random initialization work well in practice by sampling from different modes in function space, while variational Bayesian methods may fail to explore multiple modes. Recent work on loss landscapes allows investigation into this hypothesis. Recent research on loss landscapes has primarily focused on mode-connectivity and low-loss tunnels, without considering the diversity of functions from different modes. The study found that functions sampled along a single training trajectory or subspace tend to be similar in predictions, while functions from different randomly initialized trajectories are more diverse. Solution modes are connected in the loss landscape but distinct in the space of predictions. Low-loss tunnels create functions with near-identical loss values, but these functions can be very different in function space. The landscape of neural networks, known as the objective landscape, is a high-dimensional function with interesting properties. Studies have shown that the loss along a linear path from initialization to the optimum decreases smoothly. Constraints on optimization in a low-dimensional hyperplane can yield comparable results to full-space optimization. Linear paths between optima may encounter high loss areas, but continuous, low-loss paths exist. In the middle of the optima landscape, there are continuous low-loss paths connecting any pair of optima, as shown in a model by Fort & Jastrzebski (2019). These optima represent different functions in terms of predictions, indicating non-trivial connectivity. Training convolutional neural networks on CIFAR-10 dataset yields varying test accuracies: SmallCNN (64%), MediumCNN (70%), and ResNet20v1 (90%) using the Adam optimizer. We validate our results using vanilla stochastic gradient descent (SGD) in addition to the Adam optimizer for training. Batch size 128 and dropout 0.03 are used for SmallCNN and MediumCNN. Weight space and prediction space similarity results are generated with a constant learning rate of 1.6 \u00d7 10 \u22123. Consistency is observed across different architectures, datasets, and hyperparameters. Similarity between checkpoints along a trajectory is computed, showing trends in weight space and function space disagreement. In Figure 2 (b), the disagreement in function space among checkpoints is examined. Checkpoints along a trajectory show similarity in weight and function space. Functions from different initializations are diverse, as shown in Figure 3. Functions within a trajectory are more similar, while functions across trajectories are less similar. t-SNE plots are used to visualize the similarity of predictions from different checkpoints. In Figure 2 (c), different trajectories explore distant functions, while functions within a single trajectory are more similar. Subspaces are constructed based on individual trajectories to visualize the diversity of functions. Four subspace sampling methods are used to compare Bayesian neural networks and ensembles. Results on CIFAR-10 using two different architectures are presented. Two subspace sampling methods are compared: random subspace sampling and Monte Carlo dropout subspace. The diagonal Gaussian subspace method is also discussed. The text discusses two subspace sampling methods for CIFAR-10 results: random subspace sampling and Monte Carlo dropout subspace. It also introduces the diagonal Gaussian subspace method for sampling weight configurations. The text discusses sampling weight configurations from a k-dimensional normal distribution and shows that functions sampled from a subspace are more similar to each other. Diversity versus Accuracy plots illustrate that random initializations are more effective at sampling diverse and accurate solutions compared to subspace-based methods. The text discusses using SimpleCNN on CIFAR-10 to generate subspace-sampled functions from a single trajectory. The diversity score quantifies the difference between functions based on the fraction of predictions on which they differ. The approach demonstrates that functions sampled from the same subspace stay in the prediction-space neighborhood of the run around which they were constructed. The text discusses the diversity score of functions generated by SimpleCNN on CIFAR-10, quantifying differences based on prediction discrepancies. The expected fractional difference is normalized by (1 \u2212 accuracy) to account for potential random errors. The diversity reached is not as high as when functions are completely independent. The diversity of functions generated by SimpleCNN on CIFAR-10 is not as high as the theoretical optimum, even for independently initialized and optimized solutions. The radial loss landscape shows that different trajectories achieve similar loss values, but the functions themselves are different due to random initialization leading to different modes in function space. The results using MediumCNN on CIFAR-10 also demonstrate the construction of a low-loss tunnel between the origin and two independent optima. The study constructs a low-loss tunnel between different optima using a simplified procedure. Starting at a linear interpolation point, the closest point on the manifold is reached by minimizing training loss. The tunnel is confirmed to be low-loss, and a 2-dimensional cut through the loss landscape is visualized along a curved low-loss path. This is achieved by dividing the path into linear segments and computing loss and prediction similarities on a triangle. The study constructs a low-loss tunnel between different optima using a simplified procedure. Linear segments are used to visualize the loss along the manifold, showing that while accuracy remains constant, functional forms of solutions remain distinct. Subspace-based methods and ensembling may offer complementary benefits in uncertainty and accuracy. The study evaluates four variants using SmallCNN on CIFAR-10: Baseline, Subspace sampling, Ensemble, and Ensemble + Subspace sampling. Results using MediumCNN on CIFAR-10 show the benefits of subspace sampling and ensembling in terms of uncertainty and accuracy. Weight averaging techniques, such as stochastic weight averaging (SWA) and trajectory-based averaging, are proposed for better generalization in machine learning models. These methods aim to improve predictions by averaging weights over diverse solutions without significantly increasing the number of parameters. Figure S1 illustrates how these strategies can aid in generalization. Weight averaging (WA) is used in machine learning models to improve generalization by averaging weights over diverse solutions. It achieves better performance within each mode and can be combined with ensembling for further improvement. WA shows benefits in accuracy and Brier score on CIFAR-10, especially on corrupted versions, providing complementary benefits with ensembling. Ensemble size impacts accuracy and Brier score on CIFAR-10 and ImageNet datasets. Randomly initialized neural networks explore diverse modes in function space, aiding in improved performance. Subspace sampling methods like weight averaging and Monte Carlo dropout do not reach the diversity and accuracy levels of independently trained models, limiting their effectiveness for ensembling. Connectivity in the loss landscape does not imply connectivity in the space of functions. The loss landscape is visualized along original and weight-averaged directions, showing that a wider range of radius values generalize better along the weight-averaged directions. Experiments decouple the effect of random initialization and shuffling. Experiments were conducted to analyze the impact of random initialization and shuffling on the diversity of predictions. Random initialization was found to be the dominant source of randomness, while random mini-batch shuffling added more randomness at higher learning rates. Results showed that solutions obtained through subspace sampling methods had a poorer trade-off between prediction diversity and accuracy compared to independently initialized and trained optima. The study analyzed the impact of random initialization and shuffling on prediction diversity. Results showed that subspace sampling methods had a poorer trade-off between diversity and accuracy compared to independently initialized optima. The study derived functions to limit the trade-off between diversity and accuracy in different scenarios. The study analyzed the impact of random initialization and shuffling on prediction diversity. Results showed that subspace sampling methods had a poorer trade-off between diversity and accuracy compared to independently initialized optima. The study derived functions to limit the trade-off between diversity and accuracy in different scenarios. The probability of correct solutions and disagreements between reference and new solutions were also discussed. The study discussed the impact of random initialization and shuffling on prediction diversity. It derived functions to limit the trade-off between diversity and accuracy in different scenarios. The probability of correct solutions and disagreements between reference and new solutions were also analyzed. The resulting accuracy function was determined as a(p) = a * (1 \u2212 p) + p(1 \u2212 a * )/(C \u2212 1), with p(a) = (C \u2212 1)(a * \u2212 a)/(Ca * \u2212 1). The fraction of labels on which the solutions disagree is simply p."
}