{
    "title": "HJNGGmZ0Z",
    "content": "We hypothesize that end-to-end neural image captioning systems exploit 'distributional similarity' in a multimodal feature space to generate captions. By varying the input image representation, we found that image captioning models can separate structure from noisy input, cluster similar images together, and perform well even when high-dimensional representations are compressed. Our experiments confirm that image captioning systems rely on distributional similarity in a joint image-text semantic subspace to generate captions consistently. Image description generation, also known as image captioning, involves automatically creating a textual description for a given image. This task has seen significant progress with end-to-end approaches using large-scale datasets like Flickr30k and MSCOCO to train neural network systems. These systems have shown impressive performance in captioning challenges, sometimes even surpassing human performance according to automatic metrics. In this paper, the performance of end-to-end image captioning systems is still not satisfactory despite progress. The common assumption that these systems can achieve strong performance by inferring semantic information from visual representations is challenged. The systems may generate descriptions based on learned associations, such as linking the phrase \"green field\" to a green-like area in the image. Our paper provides the first empirical analysis on visual representations for image captioning. End-to-end systems exploit distributional similarity in the multimodal feature space to match images from the training set to generate captions. Previous work has hinted at this observation but not extensively explored it. In this paper, the hypothesis of distributional similarity in image captioning is investigated, focusing on the image side. Previous work has mainly concentrated on optimizing language modeling capabilities, while efforts have been made to improve image captioning by utilizing images more effectively. However, this study specifically focuses on interpretability and uses a simpler model for evaluation. The study investigates distributional similarity in image captioning, focusing on the image side. They use a basic CNN-RNN model BID17 to demonstrate how end-to-end systems utilize distributional similarity to generate captions, without fine-tuning or extensive optimization like state-of-the-art models. The study explores how end-to-end image captioning systems utilize visual-semantic subspace regardless of image representation. It introduces a sparse bag-of-objects representation to analyze image contribution and pseudo-random vectors for evaluating the systems' capabilities. The experiment in Section 4.1 shows that high dimensional image embeddings can be factorized to a lower dimensional representation without significant loss in performance. Sections 4.2 and 4.3 analyze different image representations and their transformed versions, validating claims of distributional similarity. Section 4.4 tests the IC model on an out-of-domain dataset. The IC model is tested on an out-of-domain dataset with a slightly different image distribution, showing better performance on similar distributions. Captions generated by IC models are often repeated, indicating the models exploit multimodal similarity spaces rather than actual image understanding. Our work focuses on investigating image captioning by specifically studying image representations and their effects. We implement a simple end-to-end approach using an LSTM-based language model and Exponential Linear Units as the non-linear activation. The caption generator is initialized with the projected image feature and trained to generate sentences conditioned on the image. The caption generator is trained to generate sentences conditioned on the image representation using cross-entropy loss. Inference is done with techniques like beam search or sampling, but this study focuses on language output that the models can confidently produce, using a greedy argmax approach. In this section, the study focuses on verifying the hypothesis of a 'distributional similarity' space in end-to-end image captioning systems. The experiment keeps the image captioning model constant and varies only the image representation, using pre-trained CNNs like VGG19, ResNet152, and Places365-ResNet152. The goal is to investigate the usefulness of scene-specific categories in image captioning without the network being trained to classify object-specific categories. The study explores different representations derived from pre-trained CNNs like VGG19 and ResNet152 for image captioning. It focuses on using scene-specific categories without training the network for object-specific categories. The study examines the use of pretrained CNNs like VGG19 and ResNet152 for image captioning, focusing on scene-specific categories. It investigates the impact of off-the-shelf prediction vectors on the task, utilizing predicted category posterior distributions for object and scene categories. The method utilizes averaged word representations of top-k predicted object classes from ResNet152. Softmax predictions are obtained for 1000 object categories per image, selecting objects with a posterior probability score > 5%. The 300-dimensional pre-trained word2vec BID22 representations are used to obtain the averaged vector over all retained object categories. This approach aims to represent semantic-level properties for classification tasks. Additionally, images are represented using information from object detectors that identify instances of object categories present in an image, providing a richer image representation. Ground truth region annotations for instances of 80 pre-defined categories provided with MSCOCO are used for this purpose. The method utilizes object detectors to identify instances of 80 pre-defined categories provided with MSCOCO. Annotations were done independently of image captions. YOLO BID26 is used for object detection with different representations like Bag of objects (BOO) and Binary encoding. The study investigates the importance of encoding object counts in image representations using sparse and high-level BOO representations. BOO differs from Softmax as it encodes the number of object occurrences, not class predictions. Comparisons are made between BOO representations derived from Gold annotations and YOLO detectors. Additionally, a novel experiment explores image representations in a distributional similarity space using random vectors. The study explores encoding object counts in image representations using BOO Gold-Counts and BOO Gold-Binary. Pseudo-random vectors are used to represent similar images. The dataset used is MSCOCO BID4 with 82,783 training images and 413,915 captions. Model selection is done on a 5000-image development set and results are reported on a 5000-image test set. Evaluation Metrics were used to assess system outputs on the MSCOCO validation dataset using standard metrics like BLEU, Meteor, CIDEr, and SPICE. A single hidden layer LSTM with specific hyperparameters was utilized for training vocabulary, and results were reported in Table 1 for Image Captioning on MSCOCO. Results of Image Captioning (IC) on MSCOCO are presented in Table 1, with the IC model conditioned on various image representations. Using random image embeddings performs poorly, while Softmax representations with similar object classes show similar performance. However, Places365-ResNet representations perform poorly. The results differ from previous studies where object classes were fine-tuned to correspond directly to the caption vocabulary. The degradation in performance of image representations is attributed to spurious probability distributions over object classes for similar looking images. ResNet152 outperforms VGG19 and Hybrid1365-ResNet152, while Places365-ResNet representation performs poorly. Image networks trained on object classes capture more fine-grained details, while those trained on scene classes capture more coarse-grained information. Averaged top-k word embeddings show similar performance to Softmax representation, despite being noisy. The Bag of Objects (BOO) sparse 80-dimensional annotation vector outperforms other image representations based on the CIDEr score, despite not directly corresponding to semantic information in the image or captions. The sparse representation indicates the presence of only a subset of potentially useful objects, highlighting the importance of object interactions in image captioning. The presence of multiple objects of the same kind provides extra information in image captioning. YOLOCoco outperforms YOLO9k due to training on the same dataset. Pseudo-random vectors perform as well as Gold objects, suggesting the conditioned RNN can learn a common visual-linguistic semantic subspace. In further analysis of image representations, the study explores the relevance of high-dimensional vectors and the impact of reducing dimensionality on model performance. Various methods like PCA and PPCA are experimented with to evaluate this aspect. The study explores the impact of reducing dimensionality on model performance using methods like PCA, PPCA, and ICA. Experiment results show that 80-dimensional representations retain necessary power for producing captions, indicating the model can learn from weak information effectively. In this section, the distributional similarity hypothesis is investigated by analyzing regularities in initial representations using interpretable bag-of-objects representation. Different image representations are compared based on their ability to group and distinguish semantically related images from selected categories. Up to 25 images are randomly chosen for each category or pair, represented by average image features. The Bag of Objects model clusters groups based on average image features, showing semantic similarities. Softmax models also exhibit semantic clusters, but to a lesser extent. Pool5 features result in images more similar to each other than Softmax overall. The conditioned RNN is learning a common 'visual-linguistic' semantic space, exploring differences in initial and transformed representational spaces. Image representations become more semantically coherent after transformation, with linguistic information affecting cluster types. Visual and semantic similarities are observed in initial clusters, while linguistic information from captions leads to different clusters after transformation. The captions have resulted in different clusters in Pool5, Softmax, and bag of objects representations. Pool5 clusters images based on visual appearance, while Softmax forms bigger clusters after transformation. Bag of objects clusters objects based on co-occurrence of categories like toilets and kitchens sharing sinks. In the transformed space, toilets and kitchens are clustered together due to shared sinks. End-to-end image captioning models show comparable performance to task-specific models. Initial representations lack explicit information but projected representations form clusters resembling bag-of-object clusters. End-to-end models heavily rely on datasets with similar training and test distributions. We evaluate models trained on MSCOCO on 1000 test image samples from the Flickr30k dataset, which has a different distribution and longer descriptions. Captions are generated by our model with Resnet152 representation and compared to two other state-of-the-art models: Self-Critical and Bottom Up and Top Down. The study compares models trained on MSCOCO with those on Flickr30k, noting a drop in scores attributed to linguistic mismatch. Despite typical sentence differences and longer descriptions in Flickr30k, the model struggles with generating good bigrams and unigrams. Further investigation using YOLO object detector reveals insights into object distributions in images. The study compares models trained on MSCOCO and Flickr30k, noting a drop in performance due to linguistic mismatch. Using YOLO object detector, differences in object distributions between the two datasets are observed, impacting model performance on Flickr30k. The study investigates the lack of unique captions generated by end-to-end image captioning systems, using various image representations on the MSCOCO dataset. The results show a significant portion of repeated captions across different representations, prompting further analysis with a k-nearest neighbor approach. The study explores the issue of repeated captions in image captioning systems on the MSCOCO dataset and proposes using a k-nearest neighbor approach to investigate model performance. The models do not perform text retrieval, but generate novel captions possibly by aggregating similar captions. The experiment involves generating captions for training images, finding the k-nearest training images for each test image, and computing similarity scores between generated captions. The study uses a k-nearest neighbor approach to analyze the similarity between generated captions and training images. Results show that most captions closely match those of the 5 nearest training images, with some exact matches. The projected image representation outperforms the bag of objects in capturing training set candidates. Further analysis is provided in the appendix. The study focused on how end-to-end image captioning systems match images and generate captions in a visual-semantic subspace. It was found that sparse, low-dimensional bags-of-objects representation can be used effectively for generating image captions. End-to-end models are capable of separating structure from noisy input representations. End-to-end image captioning models match images and generate captions using a joint visual-textual semantic subspace. These models rely on test sets similar to the training set for accurate caption generation. The models repeatedly generate the same captions by matching images in the joint space. This suggests that the models perform image matching rather than actual image understanding. Our findings provide insights into what end-to-end image captioning systems are actually doing, demonstrating the distributional similarity hypothesis. The Bag of Objects representation led to strong performance in image captioning despite being sparse and low-dimensional. Despite being sparse and low-dimensional, the Bag of Objects representation led to strong performance in image captioning. Each vector in the test split had only 2.86 non-zero entries on average, yet the generator RNN performed surprisingly well with minimal information. Comparing other models to this simple yet strong model, we analyzed what each representation added or subtracted. Images with the same Bag of Objects representation were selected for qualitative analysis, showing the effectiveness of this approach. The Bag of Objects model performed well in image captioning despite sparse data. Comparisons were made with other models, showing varying levels of accuracy in predicting image content. The Bag of Objects model excelled in some cases, while other representations struggled to provide accurate descriptions. The Softmax features predicted \"chainlink fence\", Places365 predicted \"kennel\", and Penultimate captured fence-like features in the image. Despite not explicitly containing the 'woman' category, Softmax features generated a caption describing a woman by predicting correlated categories like \"mask\", \"wig\", \"perfume\", and \"hairspray\". Model settings included LSTM with 128 dimensional word embeddings, 256 dimensional hidden representations, and a dropout over LSTM of 0.8. Adam was used for optimization with a fixed learning rate of 4e-4."
}