{
    "title": "BJe6BkHYDB",
    "content": "Designing accurate and efficient convolutional neural architectures for various hardware is challenging due to the complexity and diversity of hardware designs. This paper introduces HURRICANE, a hardware-aware Neural Architecture Search (NAS) approach that explores a larger search space and utilizes a multistep search scheme to generate tailored models for different hardware types. Experimental results on ImageNet demonstrate that HURRICANE achieves lower inference latency with comparable or better accuracy than existing NAS methods on three types of hardware, including a 76.63% top-1 accuracy on ImageNet with an inference latency of only 16.5 ms for DSP, representing a 3.4% accuracy improvement and a 6.35x speedup in inference. HURRICANE, a hardware-aware NAS approach, achieves higher accuracy and faster inference speed than existing methods on various hardware types. It reduces training time significantly and focuses on generating efficient CNNs for hardware execution. Existing NAS methods prioritize accuracy over hardware performance, but HURRICANE addresses both aspects effectively. Recent NAS methods are starting to consider model-inference performance on diverse processors, such as CPU, GPU, DSP, FPGA, and AI accelerators. FLOPs are not an accurate metric for predicting model-inference performance on such diverse hardware, calling for new trade-offs and designs for NAS. An experiment was conducted to measure the performance of neural network operators on Hexagon TM 685 DSP, Snapdragon 845 ARM CPU, and Movidius TM Myriad TM X Vision processors. The experiment measured the performance of neural network operators on different mobile processors: Hexagon TM 685 DSP, Snapdragon 845 ARM CPU, and Movidius TM Myriad TM X VPU. FLOPs are not a reliable metric for predicting inference latency on diverse hardware. The effectiveness of operators varies across processors, highlighting the need to choose different operators for optimal performance on each processor. Different processors require different operators for the best trade-off between model accuracy and inference latency. The computational complexity and latency of operators are influenced by the execution context, such as input feature map shapes and number of channels. Optimal operators may vary at different layers of the network, as not all layers have the same impact on latency and accuracy. Existing NAS methods tend to treat all layers equally, but some layers may have different effects on performance. Some previous works have shown differences between earlier and latter layers in CNN models. Earlier layers extract low-level features and demand more data, while latter layers capture high-level features with less computation. Exploring architecture selections in latter layers may lead to better models with limited sampling budget, while limiting latency in earlier layers is crucial for low-latency models. Leveraging layer diversity for architecture sampling in NAS is important, as there is no one-size-fits-all model for different hardware. HURRICANE is a hardware-aware method for NAS that addresses the challenge of hardware diversity by using a large candidate operators set. To reduce search and training costs, hardware-aware search space reduction is proposed at both operator and layer levels. A toolkit is developed to automatically score candidate operators in each layer on target hardware platforms, selecting a subset with low latency. Layers are split into earlier and latter groups, and a multistep search scheme is proposed using a coordinate ascent framework. The search process involves optimizing groups of layers alternately to maximize validation accuracy. This layer-level search space reduction is inspired by layer diversity. The search space reduction strategy in neural architecture search involves selecting latency-effective operators for earlier layers and allocating more sampling opportunities to latter layers. This approach results in models with low latency and high accuracy. Evaluation on ImageNet and OUIAdience-Age datasets using different hardware platforms shows that HURRICANE achieves comparable or better accuracy with significantly lower inference latency compared to state-of-the-art methods. Notably, HURRICANE reduces inference latency by 6.35\u00d7 on DSP and 1.49\u00d7 on VPU. Additionally, it reduces training time by 54.7% on ImageNet compared to Singlepath-Oneshot. Neural Architecture Search (NAS) uses reinforcement learning to find competitive architectures with low FLOPs. Recent methods adopt a layer-level hierarchical search space with a backbone structure, allowing different layer structures at different resolution blocks of a network. The goal is to search for operators for each layer to achieve competitive accuracy under given constraints. Search spaces have been built on increasingly more efficient building blocks like MnasNet. Recent advancements in Neural Architecture Search (NAS) have seen the development of more efficient building blocks such as MnasNet, ProxylessNAS, Single-path NAS, FBNet, and Singlepath-Oneshot. These structures are primarily designed for mobile CPUs, and the efficiency of manually-designed search spaces for other hardware remains unknown. While early NAS methods used FLOPs as a metric for efficiency, newer approaches are adopting direct metrics like measured latency, particularly for mobile CPUs. Additionally, there are efforts to build latency prediction models to further optimize architecture efficiency. (Cai et al., 2019) develops a latency prediction model by profiling every operator's latency and using it as a regularization loss. One-shot NAS methods like ENAS (Pham et al., 2018) utilize weight sharing to accelerate the search process. However, Singlepath-Oneshot and Fair-NAS highlight issues with coupled architecture search and weight sharing. HURRICANE aims to search for architectures that achieve maximum accuracy on a given hardware platform while meeting latency constraints. The network architecture search space is denoted as A, constructed from a backbone network with layers chosen from a set of candidate operators. The hardware aware one-shot neural architecture search in coordinate ascent framework aims to find architectures for maximum accuracy on a specific hardware platform within latency constraints. It utilizes a toolkit to score candidate operators and reduce search space at both operator and layer levels. The toolkit developed automatically scores candidate operators on different hardware platforms to reduce the search space. The top 4 or 5 operators are selected for each layer based on scores. The search space is further reduced at the layer level by choosing operators with the highest scores. The one-shot NAS method involves encoding candidate architectures into a supernet and updating weights based on training loss. Evolutionary search is used to find the winning architecture without updating weights during the search process. The one-shot NAS method uses evolutionary search to find the winning architecture with the highest validation accuracy. The new winning architecture retains the same structure in non-active layers but updates to new operators in active layers. The initial search space includes 32 candidate operators with different computation and memory complexity. The current efficient models utilize 4 basic structures: SEP with depthwise-separable convolution, MB with mobile inverted bottleneck convolution, and Choice as a basic building block in ShuffleNetV2. These structures generate different operators with varying memory and computation complexities. In ShuffleNetV2, Choice k and ChoiceX operators are introduced with smaller FLOPs but higher memory complexity. The squeeze-and-excitation network (SE) is applied with a reduction ratio of 4 and a hard version of swish ReLU 6(x+3) 6. This results in a global search space of 20 layers with 32 candidate architectures, surpassing current one-shot NAS models. The search space for operators in ShuffleNetV2 is larger than current one-shot NAS models. Hardware aware profiling is used to consider both representation capacity and real hardware performance. The scoring function approximates a weighted product of FLOPs and number of parameters. The top 4 operators with highest scores are filtered for each layer to create a specialized search space for the target platform. The candidate operator set for the last 4 layers is expanded to include a new exploring operator. Two key hyper-parameters in Algorithm 1 are the layer grouping boundary and number of iterations. The study focuses on three mobile hardware platforms for CNN deployment: DSP, CPU, and VPU. The study focuses on three mobile hardware platforms for CNN deployment: DSP (Qualcomm's Hexagon TM 685 DSP), CPU (Qualcomm's Snapdragon 845 ARM CPU), and VPU (Intel's Movidius TM Myriad TM X Vision Processing Unit). The hardware-aware search space is constructed by scoring operators on target hardware platforms and selecting the best 4 operators. Important insights from hardware profiling include optimized depthwise convolutions with kernel size k \u2264 3 on Hexagon TM 685 DSP. The study focuses on mobile hardware platforms for CNN deployment: DSP (Hexagon TM 685 DSP), CPU (Snapdragon 845 ARM CPU), and VPU (Myriad TM X VPU). The latency prediction model has high accuracy with low RMSE values for DSP, CPU, and VPU. This model can replace expensive direct hardware measurements with minimal error. HURRICANE is built on top of Singlepath-Oneshot and involves a 20-iterations evolution search for 1,000 architectures. The supernet is re-initialized randomly before each iteration. The original training set is split into two parts for validation. The training set is split into two parts: 50,000 images for validation and the rest for training. The training details include using similar settings as Singlepath-Oneshot, with exceptions in epochs and learning rate decay. Results show HURRICANE outperforms state-of-the-art models on ImageNet. HURRICANE improves accuracy by 2.59% to 4.03% on all hardware platforms compared to MobileNetV2. It leverages hardware diversity in NAS for efficiency. HURRICANE (DSP) achieves 6.35\u00d7 speedup over FBNet-iPhoneX with 3.43% accuracy improvement. HURRICANE (CPU) has 1.39% higher accuracy with 11.7ms latency reduction, and HURRICANE (VPU) has 0.83% higher accuracy with 3.1ms latency reduction. It is the only hardware-aware NAS method to achieve better accuracy with lower latency on diverse platforms. The GPU search days are crucial due to experiment environments and code implementation. Singlepath-Oneshot's primary time cost is supernet training, with HURRICANE showing a 54.7% time reduction in one-step search. It achieves better accuracy than other NAS methods and saves 30.4% time with two-step search. HURRICANE method saves 30.4% time with two-step search and demonstrates the ineffectiveness of using FLOPs as a metric for hardware aware NAS. Comparison with recent work on different tasks shows the effectiveness of HURRICANE in hardware latency constrained architecture search. The OUI-Adience-Age dataset consists of 17,000 face images split into training and testing sets. The training set is further divided for architecture retraining and search. Hyperparameters are adjusted for training and supernet convergence. Results show that the searched models outperform manually designed models like MobileNetV2. The proposed method outperforms manual designed models like MobileNetV2 in terms of accuracy and hardware efficiency. The hardware-aware search space also improves accuracy on the OUI-Adience-Age dataset. HURRICANE achieves comparable accuracy with fewer training iterations, and an additional iteration can further improve accuracy with minimal cost. HURRICANE proposes a hardware-aware search space and multistep search scheme to improve accuracy and reduce latency on various hardware platforms. The solution outperforms state-of-the-art hardware-aware NAS with lower searching costs. Future work includes supporting more diverse hardware and speeding up NAS methods. Coordinate descent algorithms are used for optimization in the proposed method. The proposed solution in the paper focuses on using Coordinate Descent (CD) or Coordinate Ascent (CA) methods to optimize layers in a neural network model. These methods are efficient and scalable in real-world scenarios. The paper emphasizes the importance of maintaining a meaningful latency constraint to achieve competitive results. If the latency constraint is too small, reducing the number of layers with skip operators can help improve performance. The paper proposes using Coordinate Descent or Coordinate Ascent methods to optimize neural network layers. It suggests adding skip operators to reduce the number of layers and adjusting the scoring function to prioritize representation capacity over latency. The layers are grouped into earlier and latter layers, split at the second resolution downsampling. A latency predictor is developed to check if an architecture exceeds latency constraints, consisting of multiple hardware-specific predictors. The paper proposes optimizing neural network layers using Coordinate Descent or Coordinate Ascent methods, adding skip operators to reduce layers, and adjusting the scoring function. A latency prediction model is built by sampling 2,000 candidate architectures and encoding them into a 84-dimension binary vector. Different regression models are used for latency prediction on diverse hardware platforms. In the paper, operations for 20 layers are searched, focusing on the stride of the first block in each repeated group."
}