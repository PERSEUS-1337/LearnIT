{
    "title": "r1GgDj0cKX",
    "content": "This paper introduces a Pruning in Training (PiT) framework for reducing network parameter size by employing sparse penalties to rank weight and filter importance. The PiT algorithms can prune networks without fine-tuning, maintaining comparable performance. (Group) Lasso-type Penalty (L-P/GL-P) and (Group) Split LBI Penalty (S-P/GS-P) are used for regularization, with a pruning strategy to assist. Experiments on MNIST, Cifar-10, and miniImageNet validate the effectiveness, with PiT saving 17.5% parameter size of LeNet-5 on MNIST while achieving 98.47% recognition accuracy. The Deep Convolutional Neural Networks (DNNs) have a trade-off between representation capability and computational cost due to the large number of parameters. Pruning parameters is essential to reduce complexity for resource-limited platforms like IoT devices. Efforts have been made to compress DNNs by learning smaller networks. Efforts have been made to compress DNNs by learning smaller networks through methods like reducing weights, distilling knowledge, and directly learning compact models. A new Pruning in Training (PiT) framework is proposed to enable pruning networks during the training process using sparsity regularizers. The PiT framework utilizes sparsity regularizers like lasso-type and split LBI penalties to train DNNs, encouraging sparsity and accelerating convergence. It iteratively computes the regularization path of layer-wise parameters, ranking them in descending order. PiT can learn sparse structures, optimize filters and connection weights, aiming for fully orthogonal weights in each layer. The orthogonal constraint can be enforced during initialization or through other regularization techniques. To efficiently train deep CNN architectures, regularization techniques like dropout, batch normalization, and weight pruning are essential to reduce computation costs and prevent overfitting. Regularization helps uncover redundant information by compressing less important filters and weights, leading to more interpretable networks. Large networks with many parameters require significant memory and computational resources, making regularization crucial during training to improve performance. Regularization techniques like dropout, batch normalization, and weight pruning are essential for training deep CNN architectures to reduce computation costs and prevent overfitting. L1 regularization enforces sparsity on weights, resulting in a memory-efficient network without sacrificing prediction performance. Group sparsity regularization can also be applied for desirable properties, such as automatically determining the optimal number of neuron groups. The (Group) Split LBI penalty is introduced for the first time to enforce structured sparsity and efficiently compute solution paths in networks. The curr_chunk discusses methods for compressing deep neural networks through weight pruning and regularization techniques. It mentions strategies like matrix decomposition, low-precision weights, and pruning methods to achieve network compression. The importance of network regularization in pruning methods is highlighted, along with the utilization of sparse properties of feature maps and weights. Previous works iteratively prune weights or neurons and fine-tune the network. The curr_chunk discusses a framework for compressing deep neural networks by ranking the importance of weights and filters using sparse regularizers. This approach differs from previous methods by solving discrete partial differential equations to optimize weights and filters. The framework discussed involves compressing deep neural networks by ranking the importance of weights and filters using sparse regularizers. This approach optimizes weights and filters through discrete partial differential equations, with a focus on pruning less important weights/filters post-training. The Residual Network structure is utilized in the framework, with potential application in other DNNs like Lenet-5. The objective is to train a sparse DNN with fewer parameters while maintaining performance. The training function includes a penalty function, loss function, and utilizes Stochastic Gradient Descent algorithm. Sparsity regularization is considered for optimizing parameters. Sparsity regularization techniques, such as Lasso-type penalty and Group-Lasso-type penalty, are used to reduce the number of connection weights in a neural network. Split LBI is employed for structural sparsity learning through variable splitting and Linearized Bregman Iteration (LBI) for computational efficiency and model selection consistency. The Split LBI algorithm introduces an augmented variable \u0393 for sparsity enforcement in neural networks. The algorithm iteratively selects variables from the weight groups to create a regularization path, starting from a null model and gradually adding more variables until overfitting. The sparse estimator W k is obtained by projecting W k onto the subset of the support set of \u0393 k, filtering out weak signals and noise. The damping factor \u03ba has low bias with a larger value but high computational cost. The step size \u03b1 is inversely scaled with \u03ba and should be small for statistical properties. The regularization parameter t k is similar to \u03bb in Lasso, balancing underfitting and overfitting. The parameter \u03bd controls the difference between W and W, with a larger value providing better model selection consistency but potentially larger estimation error. The dense estimator W can benefit from leveraging weak signals when \u03bd is greater than 0. The pruning algorithm is inspired by Fu et al. (2016b) and decomposes the dense estimator into strong signals, weak signals, and random noise. The dense estimator outperforms the sparse estimator in prediction by utilizing additional weak signals. The algorithm SGD for ResNet with Split LBI is used for this purpose. The algorithm sequentially considers all available solutions for sparse variables along the Regularization Path by decreasing regularization coefficients. It orders the parameter set \u0398 based on weight magnitude values, prunes top r% of weights, and extends pruning to multiple layers. The Split LBI method is used as an example, with the algorithm described for ResNet with Split LBI on multiple layers. The experiments are conducted on datasets MNIST, CIFAR10, and MiniImageNet with classification accuracy reported. Three pruning methods are compared: Plain (using L2 penalty), Rand (random weight/filter removal), and Ridge-Penalty (ranking weights with L2 regularization). The curr_chunk discusses the use of different penalty methods for weight and filter pruning in machine learning models, specifically focusing on Ridge-Penalty, Lasso-type penalty, Group-Lasso-type penalty, Split LBI penalty, and Group Split LBI penalty. The experiments are conducted on the MNIST dataset using the LeNet-5 architecture. No fine-tuning is performed after pruning. The experiments on weight and filter pruning in machine learning models using different penalty methods are conducted on the LeNet-5 architecture with no fine-tuning on the MNIST dataset. The models are trained and converged in 50 epochs, with results reported after saving various percentages of original parameters on each layer. The PiT algorithms are employed to prune individual layers of LeNet-5, showing successful performance on fully connected layers with minimal parameter retention. The PiT framework demonstrates successful performance in weight and filter pruning on the LeNet-5 architecture. Significant parameter reduction is achieved with minimal performance drop, showcasing the efficacy of the pruning method on convolutional layers. The PiT framework successfully prunes two layers in LeNet-5, resulting in significant parameter reduction while maintaining good performance. The conv.c5 and fc.f6 layers have the most parameters in LeNet-5. The PiT algorithms are used to prune both layers efficiently, as shown in Table 2. The framework can compress the network effectively while preserving performance. The PiT framework efficiently compresses the network while maintaining high performance. By pruning conv.c5 and fc.f6 layers, the model achieves 98.47% performance with only 17.60% parameter size of the original LeNet-5. The framework uncovers important weights and filters without fine-tuning or re-training. Best models can be downloaded online. Table 4 shows pruning multiple blocks in ResNet-18 on the Cifar-10 dataset. The PiT framework efficiently compresses the network while maintaining high performance. Applying the PiT algorithm on one residual block in ResNet-18, the pruned network with 1.57% of the original parameter size achieves similar recognition accuracies as the non-pruned network. The smallest pruned ratio of PiT still achieves high performance compared to the original network. Our PiT pruning algorithm demonstrates high performance even with the smallest pruned ratio compared to competitors. The R-P method outperforms Rand and Plain methods, but falls short of the two PiT methods due to lack of sparse constraints. Pruning multiple blocks in ResNet-18 with the most parameters shows improved performance. Pruning multiple blocks in ResNet-18 with the most parameters shows significant improvement in recognition accuracy, even with a small percentage of parameters saved. Results on miniImagenet dataset demonstrate the efficacy of PiT algorithms (GL-P and GS-P) in maintaining high accuracy levels. When no parameters are pruned, R-P outperforms PiT algorithms. However, as the ratio of pruned parameters increases, R-P's performance degrades dramatically while PiT methods (L-P/GL-P, S-P/GS-P) show slower performance decrease. Even with only 1.57% of original parameters saved, S-P/GS-P maintain high accuracy levels. These methods do not require fine-tuning and can select the most expressive weights or filters to reduce network size effectively. In experiments on three datasets, PiT algorithms can prune networks without fine-tuning. Sparse penalties are applied to at most four layers due to instability in training if applied to more layers. PiT dynamically learns weight and filter importance without the need for fine-tuning, presenting a feature selection algorithm. The PiT framework can prune networks without fine-tuning by dynamically learning weight and filter importance. Strategies like orthogonal initialization, dropout, and batch normalization help decorrelate parameters within layers. The conjecture is that DNNs may have too large a capacity for small datasets. Network pruning is essential for DNNs with large capacity learning small datasets. The challenge lies in quantifying the capacity of DNNs and dataset complexity."
}