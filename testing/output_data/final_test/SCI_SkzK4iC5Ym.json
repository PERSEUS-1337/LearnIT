{
    "title": "SkzK4iC5Ym",
    "content": "In this paper, a generalization of the BN algorithm called diminishing batch normalization (DBN) is proposed. The DBN algorithm updates BN parameters in a diminishing moving average way, accelerating the convergence of neural network training. It introduces a weighted averaging update to some trainable parameters while maintaining the overall structure of the original BN algorithm. The analysis shows that the DBN algorithm converges to a stationary point with respect to trainable parameters, which can be generalized for the original BN algorithm by setting some parameters to constant. This analysis is the first of its kind for convergence with Batch Normalization introduced, focusing on a two-layer model with arbitrary activation function. The analysis focuses on updating parameters by gradient and convergence conditions for activation functions. Sufficient and necessary conditions for stepsizes and weights are shown to ensure convergence. In numerical experiments, more complex models with ReLU activation are used, showing that DBN outperforms BN on various datasets. Deep neural networks have been successful in object detection, but training them still takes a long time. Batch normalization addresses the problem of internal covariate shift by normalizing the input of hidden layers using mini-batch statistics. The batch normalization (BN) algorithm introduces new trainable parameters per neuron to normalize the output of a layer. While it leads to improved performance, there is no theoretical guarantee for its convergence. To address this, a generalization called diminishing batch normalization (DBN) updates BN parameters in a diminishing moving average way, adjusting the output based on all past mini-batches. This helps overcome the limitations of the original BN algorithm. The diminishing batch normalization (DBN) algorithm addresses the limitations of the original BN by updating parameters in a diminishing moving average way, considering all past mini-batches. Convergence analysis with a two-layer batch-normalized neural network shows that DBN converges to a stationary point under diminishing stepsizes and standard Lipschitz conditions on loss functions. The main contribution of this paper is providing a general convergence guarantee for DBN by showing necessary conditions for stepsizes and weights to ensure parameter convergence. The algorithm converges to a stationary point under a nonconvex objective function. The paper reviews related works, presents the model and algorithm, and demonstrates the superiority of DBN over BN through numerical analysis. The introduction of Batch Normalization (BN) has been known to improve training speed in deep learning. Various methods like input whitening, decorrelation, and local response normalization have shown beneficial effects for back-propagation. Inspired by BN, new works are building upon it for further improvements. Several new techniques, such as Layer Normalization BID2, normalization propagation, weight normalization, and applying batch normalization to RNN and LSTM models, have been developed to improve upon Batch Normalization (BN) in deep learning. Despite these advancements, BN remains the most popular technique for analysis. The BN method is popular for analysis in deep learning. Our analysis explicitly considers the workings of BN, but nonconvex convergence proofs are relevant. The optimization problem for a network involves an objective function with many component functions for data records. Parameters include common parameters updated by gradients and BN parameters. The loss function in the parametric model is optimized using BN parameters introduced by the BN algorithm. Derivatives are taken with respect to \u03b8 in a deep network with 2 fully-connected layers. Each hidden layer computes y = a(Wu) with activation function a(\u00b7). BN is applied to the output of the first hidden layer, adjusting for intercept terms automatically. The computation in each layer is described to obtain the network output. The notations introduced are used in the analysis. The input data is vector X, one of DISPLAYFORM3 in the set of all BN parameters. Matrices W1, W2 are model parameters, and \u03b2, \u03b3 are introduced by BN. The objective function for the i th sample is DISPLAYFORM8 with loss function l i (\u00b7). Function f i (X i : \u03b8, \u03bb) is nonconvex. Algorithm 1 shows deviations from the standard BN algorithm, using the full gradient. The full gradient method is used in the BN algorithm, which updates parameters deterministically. The algorithm deviates from the standard BN method by using the full gradient instead of stochastic gradient. The convergence properties of both methods are speculated to be similar. The BN algorithm updates parameters using moving averages with diminishing alpha, resulting in a more general output that reflects the entire dataset distribution. Two strategies for determining alpha values are discussed, with Algorithm 1 showing better performance than the original BN algorithm in numerical experiments. The main focus is on demonstrating the convergence of Algorithm 1 in SG and non-linear activation function models. The main purpose is to show the convergence of Algorithm 1 in SG and non-linear activation function models. Assumptions include Lipschitz continuity, bounded parameters, diminishing updates, and Lipschitz continuity of loss functions. Assumptions for convergence proofs include Lipschitz continuity of loss functions, existence of a stationary point, and Lipschitz activation function conditions. These assumptions hold for standard loss functions and popular activation functions like ReLU and LeakyReLU. The lemma specifies conditions for convergence, with proofs in the Appendix. Theorem 7 states conditions for convergence, while Lemma 8 and 9 provide convergence results for non-convex functions with diminishing stepsizes. Lemma 10 further characterizes the convergence property of Algorithm 1. The main result, Theorem 11, does not directly follow from Lemma 10 due to the lack of convergence proof for {\u03b8 (m)}. Necessary conditions for Theorem 7 and Lemma 8 are h > 1, h > 2, and k \u2265 1. Computational experiments were conducted using Theano and Lasagne on Linux. The computational experiments were conducted using Theano and Lasagne on a Linux server with a Nvidia Titan-X GPU. MNIST, CIFAR-10, and Network Intrusion datasets were used to compare the performance between DBN and the original BN algorithm. Different neural network architectures were used for each dataset, with specific activation functions and layer structures. Softmax loss function and l2 regularization were applied to all models, with randomly initialized trainable parameters. The trainable parameters are randomly initialized before training using AdaGrad to update learning rates. Different choices of \u03b1 are tested, showing proper convergence for non-zero values. Small \u03b1 values in DBN still lead to convergence, but setting \u03b1 = 0 results in erratic behavior. Non-zero \u03b1 values converge at a similar rate, suggesting that BN parameters do not need to depend on the latest minibatch. The DBN algorithm outperforms the original BN algorithm on MNIST, NI, and CIFAT-10 datasets with typical deep FNN and CNN models. Different choices of \u03b1 (m) were tested, with \u03b1 (m) = 1/m and 1/m^2 performing better than the original BN. \u03b1 (m) = 0.25 was the most robust choice, consistently performing in the top 3 for all three datasets. The BN algorithm converges to similar error rates on test datasets with different choices of \u03b1 (m), except for \u03b1 (m) = 0. The BN algorithm outperforms the original BN algorithm on MNIST, NI, and CIFAT-10 datasets with deep FNN and CNN models. An extension to more than 2 layers is possible with significant notation augmentations. Proposition 12 states the existence of a constant M for any \u03b8 and fixed \u03bb. Lemma 14 discusses Cauchy series when certain conditions are met. The text discusses the convergence of a sequence {\u03bc(m)} and proves that it is a Cauchy series. It introduces the concept of \u03c3 and provides inequalities related to the sequence. The text also mentions the existence of constants and conditions for the convergence of {\u03bc(m)}. Equation 36 follows from the square function being increasing for nonnegative numbers. Equation 37 is derived using similar techniques as equations 23-25, bounding the derivatives with Lipschitz continuity. The convergence conditions for {\u03c3 (m) j } are the same as for {\u03bc}. These lemmas establish the proof of Theorem 7, where under the assumptions, |\u03bb (m) \u2212\u03bb|\u221e \u2264 am, with constants M1 and M2. Equations 38 to 47 provide a series of inequalities and bounds for various terms in the mathematical proof. The inequalities are derived from equations 30, 41, 44, and 45, with the upper bound of \u03bc defined as Am := \u03bc (m) \u2212 \u03bc (\u221e) and Bm := \u03bcj. The proof involves Cauchy series and bounding terms to establish the desired results. The second term in equation 47 is bounded by certain constants, and under the assumptions of Theorem 7, an inequality is established. This inequality is proven through four cases, leading to equation 52. Another proposition under the same assumptions yields a similar inequality with a constant M. In Algorithm 1, the update of \u03b8 is defined in a full gradient way. The proof involves equations 54, 56, and 58, leading to inequalities derived from Propositions 12 and 18. Theorem 11 is shown as a consequence of Theorem 7 and Lemmas 8, 9, and 10. Lemma 8 is proven using Lemmas 20, 21, and 22, with conditions established for equation 58. Lemma 21 provides sufficient conditions for the finiteness of equation 59 under Assumption 4. The proof involves inequalities derived from the Cauchy-Schwarz inequality and equation 60. Additionally, Lemma 14 is invoked to assert the finiteness of equation 64 and 65. The Lipschitz continuity of function f (\u03c3) = 1 \u03c3 + B is also discussed. The right-hand side of equation 75 is finite due to the parameters being in compact sets. The finiteness of the second term is shown in Lemma 21. Equations 77, 78, and 79 are finite based on certain assumptions. Lemmas 20, 21, and 22 establish Cauchy series, leading to Lemma 8. The proof follows a similar approach to Bertsekas & Tsitsiklis (2000). By Theorem 8, if \u2207f(\u03b8(m), \u03bb)2 \u2265 a for some a > 0 and integer m, then k + h > 2. This ensures the conditions for \u03b7(m) and \u03b1(m) to be satisfied. Based on Assumption 3, it is concluded that k + h > 2. The conditions for \u03b7 (m) and \u03b1 (m) to satisfy Theorem 7 are h > 1 and k \u2265 1. The conditions for Lemma 8 are h > 2 and k \u2265 1."
}