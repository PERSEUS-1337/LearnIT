{
    "title": "SygInj05Fm",
    "content": "In health, machine learning is increasingly common, yet neural network embedding learning is under-utilized for physiological signals compared to computer vision and natural language processing. The PHASE framework involves learning neural network embeddings of physiological signals, predicting outcomes, and interpreting results by estimating feature attributions in the models. The PHASE framework introduces a novel approach to estimating feature attributions in stacked models, showing that the model attributions can approximate Shapley values. Extensively tested in a cross-hospital setting, PHASE outperforms alternative embeddings and demonstrates the transferability of neural network embeddings between distinct hospitals. The PHASE framework introduces a method to learn embeddings of physiological signals for prediction tasks and feature attributions in stacked models. Tested in a cross-hospital setting, it outperforms alternative embeddings and shows transferability between hospitals. The PHASE framework utilizes computer vision and natural language processing techniques to learn embeddings of physiological signals for prediction tasks. Physiological patterns exhibit consistency and complexity similar to CV and NLP domains, making embedding learning non-trivial. This suggests that research groups spend significant time independently learning embeddings for different domains. The PHASE framework incorporates embedding learning, prediction, interpretation, and transference. Physiological signals offer unique properties for representation, similar to NLP and CV embeddings. The experimental setup details can be found in Sections 4.1 and 6.1. Physiological signals have unique properties that make them well-suited for representation learning. Privacy concerns in the health domain hinder data sharing between hospitals, but sharing models is safer and accepted. Transfer learning in this field benefits from a large community of researchers using deep learning methods for physiological signals. Physiological signals are ideal for representation learning in the health domain. However, interpreting models with learned embeddings poses challenges as traditional methods do not work effectively. Embeddings do not map to specific input features, making interpretation difficult. In healthcare, interpretability is crucial for understanding predictions and diagnoses, benefiting scientific discovery and patient care. Interpretability is crucial for understanding predictions and diagnoses in healthcare. The PHASE framework provides a methodology for mapping embedding attributions back into physiological signal attributions. It justifies the Shapley value framework and generalizes across various models. Evaluation of neural network embeddings for accurate predictions and transferability across different hospitals is discussed. Representation learning in health, particularly in medical image analysis, is gaining popularity. Examples include mammography analysis, kidney detection in ultrasound images, and diagnosing pneumonia using chest X-ray images. Transfer learning in the medical domain is also evident in various studies. Additionally, embedding learning is emerging in physiological signals analysis. Some researchers are utilizing embedding learning for physiological signals analysis, transferring neural networks as embedding functions in a partially supervised manner. This approach allows for training models on various prediction tasks, not limited to the original training task. There is potential to unify research in this area, with opportunities to transfer deep neural networks for embedding sequential physiological signals. In the realm of physiological signals analysis, researchers are exploring embedding learning techniques to unify independent research efforts. BID14 used autoencoders on blood volume pulse and skin conductance data to predict affective state, while PHASE introduced Prescience for hypoxemia predictions using operating room data and gradient boosting machines. Prescience utilizes traditional time series feature extraction methods to forecast low blood oxygen, with GBM trees identified as the best performing model. PHASE, on the other hand, employs deep learning for feature extraction and achieves better results than Prescience without clinical text features. Interpretability of models is addressed through local feature attributions, focusing on Shapley values introduced by Lloyd Shapley. Shapley values are used for local feature attributions in models like GBM trees and LSTM networks. Existing methods like Tree SHAP and Deep LIFT are not ideal for propagating attributions through multiple models. This paper introduces a method using Tree SHAP and Deep LIFT to address stacked models. In the PHASE framework, neural network embeddings are learned for physiological signals to predict outcomes across multiple hospitals. LSTM networks are used to model long term dependencies in time series physiological data. In the PHASE framework, a univariate LSTM is trained on physiological signals to predict adverse outcomes by forecasting the minimum signal value in the next five minutes. Hidden embeddings of the signals are obtained through a hidden layer, focusing on low signal values associated with adverse outcomes. The LSTM autoencoder alternative is found to be less effective. The univariate LSTM in the PHASE framework predicts adverse outcomes by forecasting the minimum signal value in the next five minutes. Univariate networks allow for flexibility in using different physiological signals and adapting to changes in data collection systems without the need to retrain entire pipelines. In this paper, the focus is on using gradient boosting machine trees in the PHASE framework for prediction. XGBoost, a popular implementation of gradient boosting machines, is utilized due to its success in predictive modeling competitions. The technique creates an ensemble of weak prediction models to perform classification/regression tasks iteratively. The authors postulate that utilizing embeddings of time series signals provides stronger features for prediction with XGBoost. The paper focuses on using XGBoost in the PHASE framework for prediction, utilizing embeddings of time series signals for stronger features. The model architecture details are in Section 6.2, and the challenge of interpreting feature attributions is addressed by combining a GBM model with a LSTM network to create a \"stacked\" model. The paper combines a LSTM network with a GBM model to create a \"stacked\" model for feature attribution. A new method for estimating Shapley values for this stacked model is introduced, addressing the challenge of interpreting feature attributions. The paper introduces adaptations to existing feature attribution methods for the stacked model (LSTMs and GBM). Deep SHAP and Independent Tree SHAP are utilized to compute attributions for single references, with low computational complexity. Independent Tree SHAP has a complexity of O(M LT), where L is the maximum number of leaves in any tree and T is the number of trees in the GBM. The paper introduces adaptations to existing feature attribution methods for the stacked model (LSTMs and GBM). By combining Deep SHAP and Independent Tree SHAP, Shapley values can be obtained for the entire stack of models. This approach allows for explanations of \"stacked\" models composed of neural networks and trees, as many embedding/prediction models can be represented as neural networks. The paper introduces adaptations to existing feature attribution methods for stacked models, combining Deep SHAP and Independent Tree SHAP to obtain Shapley values. The framework is general for models represented as neural networks. The data sets, evaluation metric, model architectures, and comparison results with alternative approaches are discussed in Section 4.2. Hospital 0/1 data was collected via the Anesthesia Information Management System. More details on model architecture and experiments can be found in the Appendix. Hospital P data, a sub-sampled version of the MIMIC dataset from PhysioNet, was collected thousands of miles away from Hospital 0/1 data. Hospital P is a public dataset with statistics available in Table 1, while Hospital 0/1 data includes static information and real-time physiological measurements. Additional details can be found in the Appendix. The Hospital P data consists of thirty-five physiological signals sampled minute by minute, with missing values imputed by the mean. The evaluation methodology involves real-time prediction tasks, with the area under the precision-recall curve used as the evaluation metric for prediction performance in binary classification. The precision-recall curve, also known as average precision (AP), is used to highlight imbalanced labels. It compares different embeddings of physiological signals and evaluates the performance in predicting adverse clinical events. The section also discusses the transfer of embedding learners between hospitals and introduces a new model stacking method for obtaining Shapley values. Key indicators of adverse outcomes in the operating room include SaO2 levels. In this section, key indicators of adverse outcomes in the operating room are discussed, including SaO2, ETCO2, and NIBPM. Forecasting these outcomes is crucial as deviations could lead to hypoxemia, hypocapnia, and hypotension. The performance gains of PHASE embeddings over other embeddings are investigated using XGB for prediction tasks. No pre-training is done for Raw and EMA embeddings, but fifteen univariate LSTM models are trained for Min h and Auto h. The study compared the performance of different LSTM models for forecasting adverse outcomes in the operating room. The Min h models consistently outperformed others in predicting hypoxemia, hypocapnia, and hypotension. Using multiple signals improved the average precision points significantly compared to using a single signal. LSTM autoencoders struggle to capture meaningful data representations compared to partially supervised LSTMs. Effective embeddings for forecasting adverse outcomes related to low signals require a close relationship between the LSTM's prediction task and downstream prediction task. The study aims to evaluate the transferability of embedding models and explore methods to adapt them to hospitals with significant domain shifts. Results show that feature embeddings from a source hospital perform better than other types of embeddings, indicating promise for future applications. The study evaluates the transferability of embedding models between hospitals with significant domain shifts. Physiological signal embeddings transfer well between hospitals with similar patient distributions, despite differences in hospital types. However, embeddings from publicly available data perform worse compared to target hospital data, indicating a large domain shift between hospitals. The study found that the distance between hospital P and OR hospitals affects the usefulness of learned embeddings for prediction. Fine-tuning models improved performance lost during transference across different hospitals. The closeness between LSTM prediction tasks and downstream tasks was beneficial for transference, suggesting specific training tasks and fine-tuning as approaches for transferring across diverse hospital datasets. In this section, the efficacy of an interpretability method for stacked models is evaluated through qualitative and quantitative approaches. Qualitative evaluations ensure human insight, while the primary goal is to verify the correctness of obtaining local feature attributions for stacked models. One qualitative evaluation shows that local feature attributions enhance the performance of anesthesiologists in predicting hypoxemia. Quantitative validation involves a standard ablation/perturbation test similar to other interpretability evaluations. The interpretability evaluations involve sorting input features by attributions and iteratively imputing them by the mean of the last two minutes. The test is conducted on the top 1000 positive samples for hypoxemia prediction, evaluating the mean predicted probability as features are imputed. Good interpretability methods show an initial steep decrease in probability as important features are imputed first. Ablation test on the top 1000 positive labels is performed by removing features based on probability predictions. In the interpretability evaluation, features are removed based on Shapley values or random ordering before predicting hypoxemia probability on the test set. Deep SHAP is extended to support a mix of model types in PHASE, comparing LSTM embeddings fed into XGB against an MLP. The interpretability method combines feature imputation and is validated in Figure 4. The interpretability method in PHASE combines Deep SHAP with Independent Tree SHAP, showing improved performance in predicting hypoxemia compared to random ordering. Imputing attributions for LSTM\u2192XGB may have a negative impact on performance initially. Visual evaluation in Appendix Section 6.5.1 confirms concordance between the two methods. PHASE introduces a new approach to machine learning with physiological signals by transferring embedding learners, potentially impacting neural network research significantly. In Section 2.2, research on neural networks for physiological signals is discussed. PHASE proposes collaboration through semi-private sharing of meaningful signals using univariate networks. Results show the importance of closeness between LSTM tasks for predictive performance and transference. Predicting the minimum of future five minutes was sufficient for good embeddings. Transferring across domains was successful when predicting hypoxemia, but representation from Hospital P was not effective. The paper discusses the challenges in utilizing representations from different hospital settings for predicting hypoxemia. Two solutions proposed are fine-tuning Min LSTM models and training specific LSTM embedding models. Additionally, a model stacking framework for neural networks and trees is introduced to obtain feature attributions. The framework generalizes to all models and was quantitatively verified. The authors plan to release code for training LSTM models, obtaining embeddings, predicting with XGB models, and model stacking feature attributions. In future work, the focus is on representation learning in health, privacy considerations for models, potential use of differential privacy, exploring higher sampling frequencies, and quantifying domain shifts in hospitals. The focus of future work includes representation learning in health, privacy considerations for models, potential use of differential privacy, exploring higher sampling frequencies, quantifying domain shifts in hospitals, and determining prediction tasks for which embeddings can be applied. Labels for hypoxemia, hypocapnia, and hypotension are defined based on specific criteria for different hospitals. The study focused on predicting hypoxemia, hypocapnia, and hypotension using different thresholds and signals. Sample sizes varied for different prediction tasks, with labels based on specific criteria for each hospital. LSTM autoencoders were trained using the same data without looking at the labels. The study trained LSTM networks with 15 features above the line without looking at labels. Models were trained for regression and classification objectives using different optimization methods. Model architectures included two hidden layers with 200 LSTM cells and dense connections. Important steps in training included imputing missing values, standardizing data, and randomizing sample ordering. Dropouts were used to prevent overfitting. To prevent overfitting, dropouts were utilized in LSTM models with a learning rate of 0.001. The models were trained using three GPUs and XGBoost for GBM trees in Python. Imputing and standardizing data were deemed unnecessary for GBM trees. A learning rate of 0.02 for hypoxemia and specific parameters were found to give good performance. In order to train the models, 72 CPUs were utilized, except for the fine-tuned LSTMs. There are a total of 60 LSTMs, with consistent architecture and hyperparameters. The signals and outputs of the LSTMs are vectors, with multiple connections simply concatenated. All LSTMs consist of two layers with 200 LSTM cells each. The models consist of two layers with 200 LSTM cells each, trained in the same way. XGB training details are also provided. Univariate predictions in FIG3 are based on a single feature for final prediction. Adjusted p-values are reported based on bootstraps of the test set with ANOVA and Tukey's HSD test. P-values less than 1e\u221214 are denoted as 0.0. Pairs involving models 10, 12, and 14 are highlighted for brevity. In a simulated setting, the Leave One Signal Out Test was conducted to predict events by excluding corresponding physiological signals. Results showed consistent outperformance of PHASEs for hypocapnia and hypotension, but poor performance for hypoxemia due to heavy reliance on SaO2. Low base rates of hypoxemia were found to be 1.06% in hospital 0 and 2.32% in hospital 1. The study found low rates of hypoxemia in hospitals 0 and 1. Further investigation revealed that Min embeddings had lower log loss on the validation set but not on the test set, indicating overfitting. The simpler EMA embeddings were favored due to the lack of signal. The models used in the transference experiment included LSTM/XGB architecture with consistent hyperparameters. LSTM layers had 200 cells each, while XGB models had multiple connections. Univariate predictions in the experiment utilized a single feature for the final prediction. In the study, low rates of hypoxemia were found in hospitals 0 and 1. Univariate predictions in the experiment utilized a single feature for the final prediction, with complex numerical values obtained. Model setup for Figure 4 showcases LSTM/XGB architecture with consistent hyperparameters. Signals and outputs of LSTMs are vectors, while multiple connections into a single XGB model are concatenated. The MLP architecture includes a single layer with 100 nodes and is trained identically to the Hypox models. LSTM\u2192MLP attributions are computed via Deep SHAP, and LSTM\u2192XGB attributions are computed via Deep SHAP combined with Independent Tree SHAP. The local feature attributions for two different model stacks are computed using Deep SHAP combined with Independent Tree SHAP. The models generally agree on their predictions, with occasional disagreements likely due to different models. Observing these trends helps understand and validate the models. True positive examples are also analyzed to ensure agreement between model stacking local feature attributions and neural network attributions. The local feature attributions for model stacks show high variability and low blood oxygen levels in true positive examples. Dips in blood oxygen minute-to-minute variability are crucial for model predictions. In contrast, true negative examples prioritize high SaO2 values near the final prediction. The MLP model exhibits consistent trends in feature attributions despite diverse samples. Randomly sampled feature attributions are presented in FIG1. In FIG1, local feature attributions for two stacked models (LSTM\u2192MLP and LSTM\u2192XGB) are shown for hypoxemia examples. The set includes true negative and true positive examples, with a focus on high variability in true positives and high SaO2 values in true negatives. The samples exhibit diverse trends in feature attributions despite looking different. The top 100 positively labeled examples from both models were used to create a set of 40 samples, from which nine samples were randomly selected."
}