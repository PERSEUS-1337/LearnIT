{
    "title": "ByxF-nAqYX",
    "content": "The paper introduces Locally Linear Unsupervised Feature Selection, a method for retaining features that capture local patterns in data. It utilizes dimensionality reduction inspired by Locally Linear Embedding to evaluate each feature's compliance with local patterns. Experimental validation on the scikit-feature benchmark suite shows its effectiveness compared to existing methods in Machine Learning. Dimensionality reduction is crucial in addressing challenges posed by the increasing dimensionality of modern datasets in both unsupervised and supervised learning approaches. The paper discusses the importance of interpretable dimensionality reduction in Machine Learning, emphasizing the need for feature selection over feature construction. In bioinformatics, identifying key genes related to a disease is a primary goal. Unsupervised learning plays a crucial role in making sense of data, while supervised learning comes later in the data exploitation process. Unsupervised feature selection methods are highlighted as essential in ML. The paper introduces Locally Linear Unsupervised Feature Selection (LLUFS) as a method to address limitations in unsupervised feature selection approaches. LLUFS aims to determine patterns in data and select relevant features to characterize these patterns, overcoming issues related to cluster reliance on initial features and irrelevant or random features. LLUFS is a 2-step process that involves building a compressed representation of data using Auto-Encoders, scoring each feature based on its contribution to the reconstruction error, and validating the approach using the scikit-feature project. The paper concludes with an empirical validation and discussion of the approach's merits and weaknesses, along with perspectives for further research. The m \u00d7 D data matrix represents samples and features. \u00b5 j and \u03c3 j are the mean and standard deviation of features. Two types of similarities, supervised SU P and unsupervised RBF, are considered. Supervised FS aims to maximize classifier accuracy by selecting feature subsets. Filter, wrapper, and embedded methods are used in supervised FS algorithms. Filter methods BID43 BID30 operate at the data pre-processing stage, agnostic to classifier algorithm. Wrappers methods BID15 BID27 aim to determine feature subset for best accuracy within specific classifier. Embedded methods BID12 BID46 learn and use hypothesis to prune/select features. This paper focuses on unsupervised and interpretable FS, considering only filter methods. Early supervised filter method based on Fisher score introduced by BID6 ranks features according to correlation. The Laplacian score, introduced by BID14, measures how well features account for sample similarity, overcoming limitations of myopic feature selection. It is related to the Fisher score but excels with RBF similarity, and is also linked to the MaxVariance FS method BID16. The Laplacian score, introduced by BID14, measures feature relevance based on sample similarity, overcoming limitations of myopic feature selection. It is related to the MaxVariance FS method BID16 and spectral clustering approach SPEC BID47, which proposes three scores \u03c6 1, \u03c6 2, and \u03c6 3 based on smooth features aligned with eigenvectors of the normalized Laplacian. The smaller \u03c6 1j, the more efficient the j-th feature is to separate clusters. Features are ranked based on scores \u03c6 1, \u03c6 2, and \u03c6 3, with a focus on separating clusters efficiently. Multi-Cluster Feature Selection (MCFS) addresses limitations by defining a score per cluster, estimating the capacity of features to separate clusters. The MCFS method defines a score per cluster by fitting eigenvectors with a regularization term to estimate the capacity of features to separate clusters. The L1 regularization term enforces sparsity in the feature vector, while the NDFS approach jointly optimizes the feature importance matrix and cluster indicator matrix with an L2 regularization. LLUFS is a method that enforces intra-cluster similarity and inter-cluster dissimilarity using an L2 regularization with orthogonal and nonnegativity constraints on a cluster indicator matrix \u03be. Inspired by Locally Linear Embedding (LLE), LLUFS aims to preserve pairwise distances among points in a low-dimensional embedding. Locally Linear Embedding (LLE) defines the local structure of data points by approximating each point as the barycenter of its nearest neighbors. It finds points in a lower-dimensional space that satisfy the same local relationships as the original points. LLE captures the local structure through a weight matrix that is invariant to transformations. The dimensionality reduction technique of LLE aims to find a set of points that maintain the local relationships expressed by the weight matrix. After capturing the local structure of the data with matrix W, it can be used to transport data from any source to target representation. The LLUFS approach aims for feature selection in a low-dimensional space, utilizing non-linear Stacked Denoising AutoEncoder neural networks (SDAE) to compress data robustly. The goal is to find a low-dimensional representation of the data, characterize matrix W, and assess initial features using linear or non-linear approaches like PCA, SVD, Isomap, or t-SNE. Z is considered the \"true\" data, while X is seen as an inflated and corrupted image of Z. The overall loss of information from the true data Z to the corrupted image X can be decomposed with respect to examples and initial features. The distortion of features is interpreted in terms of how much they are corrupted compared to the true local structure of the data. Features with lower distortion are considered more representative. However, the distortion scores depend on the latent representation produced by the auto-encoder, which may be biased due to feature redundancy. The auto-encoder may be biased due to feature redundancy, leading to a distorted latent representation. To address this, LLUFS normalizes initial features, clusters them, selects one feature per cluster, and applies the auto-encoder. Further work aims to consider feature redundancy in the auto-encoder loss. Another limitation is the distortion score's sensitivity to feature distribution. To alleviate feature redundancy in auto-encoder models, LLUFS measures the reliability of feature distortion using an empirical p-value threshold. Features are normalized, clustered, and a Sparse Denoising Autoencoder (SDAE) is trained on the filtered features to produce a compressed representation. The optimization problem is solved to find the optimal weights subject to constraints. The experimental validation aims to compare LLUFS with other unsupervised feature selection methods, focusing on predictive accuracy using a 1-nearest neighbor classifier. The impact of feature clustering and FS mechanisms is assessed, along with the robustness of LLUFS. The experimental validation compares LLUFS with other unsupervised feature selection methods, focusing on predictive accuracy using a 1-nearest neighbor classifier. The robustness of the proposed approach is investigated specifically with XOR concepts using five baseline algorithms on seven benchmark datasets. The study focuses on image and bioinformatics domains, using the Madelon XOR problem to investigate LLUFS. Features range from 500 to 10,000, classes from 2 to 11, and examples are less than 200. Continuous features are considered, normalized with three hyper-parameters for LLUFS. A 5-layer denoising auto-encoder is used for latent representation. The comparative performance of unsupervised feature selection methods is discussed. LLUFS outperforms others on datasets ALLAML and TOX171. LAP and NDFS perform well on ALLAML, while LAP and MCFS excel on TOX171. SPEC shows robust performance after selecting a sufficient number of features. RANDOM is consistently outperformed in both cases. LLUFS outperforms other algorithms on datasets PIXRAW10P and ORLRAWS10P, especially dominating NDFS on ORLRAWS10P. However, LLUFS initially struggles with capturing feature subsets compared to NDFS. RANDOM also performs decently on both datasets, ranking 2nd or 3rd for high values of d. On CARCINOM FIG4, LLUFS is initially dominated by all algorithms but MCFS for d < 20, then catches up and becomes the best algorithm for d > 60. LAP is the top performer on this dataset, indicating the relevance of the cluster structure from all features. On LUNG FIG4, LLUFS is consistently outperformed by NDFS and SPEC, performing similarly to RANDOM. The variance of predictive accuracy for RANDOM is around 0.02 for d < 5 and 5 * 10 \u22123 for d = 20. Confidence bars are omitted for readability. The Madelon problem was chosen to compare unsupervised FS methods. Most methods failed due to low signal to noise ratio, but NDFS and LLUFS performed better by learning cluster indicators and feature relevance. LLUFS excelled in highlighting data patterns and measuring feature relevance. The paper introduces a novel unsupervised feature selection approach based on finding an \"oracle\" representation of the data to assess feature quality. It aims to address issues like noise features perturbing metrics and misleading classifiers. Future research directions include considering partial feature relevance and integrating feature redundancy in the auto-encoder loss function. The Fisher score and Laplacian score are methods used for feature selection, with the Fisher score being a specific case of the Laplacian score. Madelon dataset was created for the NIPS2003 Feature Selection Challenge, containing 5 relevant features combined in XOR concepts to define classes. The Madelon dataset for the NIPS2003 Feature Selection Challenge contains classes with 2 Gaussian clusters on a hypercube's vertices. 15 \"distractor\" features are created by duplicating, combining, and perturbing the 5 relevant features, along with 480 noise features. Classifiers using all features show 50% accuracy, indicating the necessity of efficient feature selection."
}