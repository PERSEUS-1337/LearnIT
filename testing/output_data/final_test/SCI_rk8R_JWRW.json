{
    "title": "rk8R_JWRW",
    "content": "Spiking neural networks are being explored as biologically plausible models of neural computation and more efficient neural networks. Convolutional spiking neural networks have shown near state-of-the-art performance, while only one solution has been proposed for converting gated recurrent neural networks. An analog gated LSTM cell with efficient stochastic spiking neurons is designed to convert analog activation values to spike-trains. These adaptive spiking neurons use a sigmoid-like activation function and can be trained with standard backpropagation. The LSTM networks are trained on sequence prediction tasks and a working memory reinforcement learning task. Spiking neural networks, inspired by biological neurons, are being explored for computational and energy efficiency. They mimic the sparse communication of biological neurons, with successful applications in image recognition tasks. While they have shown promise, many deep learning algorithms still rely on recurrent neural networks. Many deep learning algorithms use recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) layers for image-recognition tasks. A spike-based version of LSTM has been developed for the IBM TrueNorth platform. A method to approximate LSTM for this platform involves a store-and-release mechanism for synchronization, resulting in a less biologically plausible and energy-efficient frame-based rate coding computation. A gated recurrent spiking neural network resembling an LSTM unit with a memory cell and input gate is demonstrated here. The text discusses a modified LSTM network that can be converted to a spiking neural network using adaptive stochastic spiking neurons. Input-gating is crucial for maintaining memorized information without interference, and a limited LSTM neuron model is proposed for reduced complexity. The text discusses converting LSTM neurons to spiking neural networks for tasks like speech recognition and working memory cognitive tasks. The conversion involves approximating spiking memory cell behavior through time to handle continuous signal integration. The text discusses constructing an Adapting Spiking LSTM network for tasks like speech recognition and working memory cognitive tasks. It describes Adaptive Spiking Neurons and their activation function, as well as constructing an LSTM network with a spiking memory cell and spike-driven input-gate. The Adaptive Spiking Neurons used in this paper are a variant of adapting Leaky Integrate & Fire neurons with fast adaptation to input signals. The equations describe the Adaptive Analog Neuron's behavior, including the input current, threshold, and spike times. The neuron emits spikes based on a stochastic firing condition defined by its membrane potential and activation function. The activation function of the Adaptive Analog Neuron maps the input signal to the perceived average PSC for the next neuron. The spiking activation function is normalized and fitted with a sum-of-exponentials shaped function. The parameters for the neuron are a = 148.7, b = -10.16, c = 3.256, and d = -1.08. This mapping allows the activation function to be used during training. The Adaptive Spiking LSTM is a modified version of the LSTM cell, consisting of an input gate, input and output cells, and a CEC. The ASNs are used as replacements for AANs in the trained network, with specific parameters set for all neurons. The spike height parameter, h, is crucial for binary communication across the network. The Adaptive Spiking LSTM is trained from the Adaptive Analog LSTM, aiming for a one-on-one mapping. Sigmoidal activation functions are approximated using the stochastic firing condition of the ASN to address gradient issues during training. The AAN transfer function in the Adaptive Analog LSTM cell utilizes the stochastic firing condition of the ASN, allowing for gradient calculation even below the threshold. It is used for LSTM operations like opening and closing gates, with the input gate and cell employing AAN functions. The input cell's activation value is multiplied by the input gate's activation value before entering the CEC. In the spiking version, the ASN's outgoing signal is accumulated in an intermediate neuron, enhancing the internal state for spike movement. The Adaptive Spiking LSTM merges the Constant Error Carousel (CEC) and output cell into one ASN with a non-decaying internal state. The CEC value in the Adaptive Analog LSTM corresponds to the state I of the ASN output cell in the Adaptive Spiking LSTM. By setting \u03c4 \u03b2 to a large value in the Adaptive Spiking LSTM, the CEC cell exhibits integrating behavior without a forget gate, resulting in a spiking CEC neuron that fully integrates its input. The spiking CEC neuron fully integrates its input by adding every incoming spike to a non-decaying PSC I when \u03c4 \u03b2 is set to \u221e. The integrating behavior is translated to the analog CEC of the Adaptive Analog LSTM, updating the CEC every time step by CEC(t) = CEC(t \u2212 1) + S \u03c4\u03b7. This is depicted in Figure 2 with a weight after the input gate with value 1 \u03c4\u03b7. In the spiking version, setting \u03c4 \u03b2 to \u221e for ASN out results in behavior similar to using an analog CEC that integrates with CEC(t) = CEC(t \u2212 1) + S. The error between the spiking and analog CEC increases for lower \u2206ts. The error between the spiking and analog CEC increases for lower \u2206ts due to irregular ISI and adapting behavior of the ASN. Choosing bigger time windows results in more stable responses. A customized truncated version of real-time recurrent learning (RTRL) was used to train the analog LSTMs on supervised tasks, while RL-LSTM incorporated eligibility traces for reinforcement learning tasks. The Adaptive Analog LSTM incorporates eligibility traces to improve training and Advantage Learning BID10. Regular neurons are trained with traditional backpropagation. Four classical tasks from the LSTM literature are presented that do not rely on output or forget gates, including Sequence Prediction with Long Time Lags. The ability of a CEC to maintain information over long stretches of time was demonstrated in a noise-free version of this task. The Adaptive Analog LSTM improves training with eligibility traces and Advantage Learning. It tackles tasks like Sequence Prediction with Long Time Lags, where the network must store the first element of a sequence in memory for its entire length. Twenty networks were trained on this task with p = 100, using a specific network construction method to prevent issues. In a noisy version of the task, the network still predicts the next input of the sequence. In a noisy version of the task, symbols are presented in random order, and only the final symbols can be predicted. 20 networks were trained on this task for 200k trials, with the network considered converged when the average error was less than 0.25. Additionally, a network with Adaptive Analog LSTM cells was trained on a T-Maze task in Reinforcement Learning. The T-Maze task involves a maze with a T-junction where the agent must make choices based on presented information. Rewards are given for reaching the target position and penalties for moving against the wall or in the wrong direction. The agent has 3 inputs and 4 outputs, with a larger negative reward to encourage differentiation in Q-values during the trial. The T-Maze task involves a maze with a T-junction where the agent must make choices based on presented information. Rewards are given for reaching the target position and penalties for moving against the wall or in the wrong direction. The agent has 3 inputs and 4 outputs, with a larger negative reward to encourage differentiation in Q-values during the trial. In the task, the corridor length is set to N = 20, and there are noiseless and noisy versions defined. The noisy version requires the use of input gating due to uniformly distributed random variables in the input. The network used consists of a fully connected hidden layer with 12 AAN units and 3 Adaptive Analog LSTMs. The LSTM cell's influence in the network was studied by normalizing activation functions of the AAN output cell and ASN output cell at S = 1. Training parameters were consistent with Bakker FORMULA1, with 20 networks trained for each task. Networks converged when the average total reward exceeded 3.5 in the last 100 trials. Successful networks for noise-free and noisy Sequence Prediction tasks were converted into spiking networks. Correct predictions were demonstrated before and after conversion. Average error over the last 200ms was consistently below 0.25 for successful networks. In the noisy task, successfully trained networks remained functional after conversion, with well-separated CEC values due to input noise. Results for the T-Maze task also showed success in both noise-free and noisy conditions. The Q-values of a noisy T-Maze task demonstrated correspondence between analog and spiking representations, although the CEC values of spiking LSTMs differed from their analog counterparts. The spiking CEC values of LSTMs differ from their analog counterparts, possibly due to increased network complexity. Gating in recurrent neural networks, like input gates, allows memory cells to maintain information over time, making LSTM successful in various sequence-related problems. Transferring deep neural networks to networks of spiking neurons is a key focus. To transfer deep neural networks to networks of spiking neurons, a method involves mapping spiking neuron transfer functions to analog counterparts and substituting analog neurons with spiking neurons after training. This approach was extended to gated memory units in an LSTM network, resulting in a low-firing rate asynchronous LSTM network. The requirement of a differentiable gating function, approximated using stochastic Adaptive Spiking Neurons, was a complex aspect. The activation function for the stochastic spiking neuron approximates a half-sigmoid, providing effective activation even below the resting threshold. The spiking neuron has effective activation below the resting threshold, allowing for training in that area. The resulting LSTM network is suitable for sequence prediction tasks in noise-free and noisy settings. The network can be mapped to its spiking neural network equivalent with 90% accuracy. Difficulties arise in converting analog to spiking LSTM due to differences in activation functions. An adaptive spiking LSTM network implemented in the paper explores the use of analog-valued spikes to improve efficiency. Output gates can be included by modulating synaptic strength, with the necessity of output gates in LSTM networks still being debated. Modulating each output synapse independently is less biologically plausible than for input gates. The inclusion of forget gates in the original LSTM formulation involves modulating the decaying factor of the CEC. The necessity of gates in LSTM networks is still debated, with the AuGMEnT framework not using gates for certain tasks. Research has shown that a combination of input and forget gates can outperform LSTM while reducing complexity."
}