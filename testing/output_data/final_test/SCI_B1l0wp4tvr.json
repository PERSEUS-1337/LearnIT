{
    "title": "B1l0wp4tvr",
    "content": "Analyzing deep neural networks (DNNs) using information plane (IP) theory is a popular method to understand their generalization ability. Estimating mutual information (MI) between hidden layers and input/output is challenging, especially for layers with many neurons. Existing IP methods have not been able to analyze truly deep Convolutional Neural Networks (CNNs) like VGG-16. This paper introduces an IP analysis using matrix-based R\\'enyi's entropy and tensor kernels over convolutional layers, leveraging kernel methods for representation. The new framework enables a comprehensive IP analysis of large-scale DNNs and CNNs, shedding new light on previous literature. It provides insights into the training dynamics of neural networks, utilizing Mutual Information to understand their inner workings. The Information Plane (IP) analysis of DNNs involves modeling the mutual information between input X and hidden layer T, as well as between T and output Y. This approach reveals insights into the training dynamics of neural networks, as shown in Figure 1 using a proposed estimator. The proposed estimator in Figure 1 illustrates insights into the training phases of neural networks, similar to previous observations. A novel approach for estimating mutual information in large-scale DNNs is introduced, addressing numerical instabilities associated with existing methods. The multivariate matrix-based approach is used to estimate entropy for high-dimensional multivariate data. Results show that the claim of entropy being equal to mutual information in high dimensions does not hold for the proposed estimator. The compression phase is more apparent in training data, especially for challenging datasets, and may be linked to overfitting phenomena. Early-stopping can help prevent overfitting by stopping training before the compression phase occurs. The fitting and compression phases of Deep Neural Networks (DNNs) are illustrated, with early stopping highlighted based on a patience parameter. The evolution of the Information Plane (IP) during DNN training is analyzed, showing an initial fitting phase followed by a compression phase. This concept was first proposed by Tishby & Zaslavsky (2015) and later demonstrated by Shwartz-Ziv & Tishby (2017). The compression phase in Deep Neural Networks (DNNs) during training has been a topic of debate. While some studies question its general existence, recent research supports the idea of a compression phase, with variations based on activation functions and the inclusion of L2-regularization. Some recent studies have discussed limitations of the IP framework for DNN analysis and optimization. Cheng et al. (2018) proposed an evaluation framework based on the IP, showing that MI can infer DNNs' object recognition capability. They argue that as the number of neurons in a hidden layer increases, I(T ; X) and I(Y ; T) remain approximately deterministic. Initially proposed for SAEs, empirical estimators for R\u00e9nyi's MI were used to investigate data processing inequalities. In investigating data processing inequalities in stacked autoencoders (SAEs), researchers extended empirical estimators for R\u00e9nyi's mutual information (MI) to multivariate scenarios. R\u00e9nyi's \u03b1-order entropy, a generalization of Shannon's entropy, has been widely applied in machine learning. R\u00e9nyi's \u03b1-order entropy, a generalization of Shannon's entropy, is widely used in machine learning. Giraldo et al. (2012) proposed a nonparametric framework for estimating entropy directly from data without relying on kernel density estimation. The matrix-based R\u00e9nyi's \u03b1-order entropy is calculated using a positive definite kernel matrix and eigenvalues of a matrix A. The eigenvalues of matrix A were analyzed in detail by Giraldo et al. (2012), showing its similarity to a density matrix in quantum information theory. The (Gram) matrix A is linked to an empirical covariance operator in a Reproducing Kernel Hilbert Space. Under certain conditions, Equation 2 converges to the trace of the covariance operator, capturing distribution properties with robustness to high-dimensional data. The distribution properties are robust to high-dimensional data, unlike KNN and KDE based estimators. Certain entropy estimators struggle with high dimensions and do not require binning procedures. Experiments on synthetic data illustrate the behavior of these estimators. In the limit as \u03b1 approaches 1, Equation 2 simplifies to the matrix-based von Neumann entropy resembling Shannon's definition. Giraldo et al. (2012) define joint entropy between x and y, and a multivariate extension of R\u00e9nyi's \u03b1-order entropy is proposed for estimating information in convolutional layers of DNNs. The matrix-based R\u00e9nyi's \u03b1-order entropy extends the joint-entropy calculation among C variables. Yu et al. (2018) showed how this method can analyze synergy and redundancy in convolutional layers of DNNs, but noted challenges with increasing feature maps in complex CNNs. The Hadamard products in the equation lead to numerical instability as the number of feature maps grows, affecting the modeling of VGG16. To address these limitations, a tensor-based approach is introduced for utilizing information theoretic quantities of features in convolutional layers. Our tensor-based approach utilizes entropy and mutual information in DNNs, addressing limitations in handling tensor data from convolutional layers. By using tensor kernels, we can compute the MI and represent the output of convolutional layers efficiently. The tensor kernel defined in Equation 7 simplifies the computation of entropy and mutual information in DNNs by utilizing eigenvalues of kernel matrices. It addresses limitations in handling tensor data from convolutional layers. The curr_chunk discusses the importance of choosing the kernel width parameter in methods involving RBF kernels, especially in high-dimensional data scenarios. It also mentions the potential of exploring structure-preserving kernels in future research. In this work, the optimal kernel width parameter \u03c3 is chosen based on an optimality criterion to reveal class structures in high-dimensional data. The kernel alignment loss is maximized between the kernel matrix of a layer and the label kernel matrix. An exponential moving average is used to stabilize \u03c3 values across mini batches. The approach is evaluated on the MNIST dataset with a Multi Layer Perceptron architecture. In this study, the optimal kernel width parameter is determined using an optimality criterion to reveal class structures in high-dimensional data. The kernel alignment loss is maximized between the kernel matrix of a layer and the label kernel matrix. An exponential moving average is used to stabilize the kernel width parameter values across mini batches. The experiments involve the MNIST dataset with a Multi Layer Perceptron architecture and various CNN architectures, including VGG16, on the CIFAR-10 dataset. Matrix-based R\u00e9nyi MI is computed at batch level with a moving average smoothing approach. The kernel width for each hidden layer is determined using the kernel alignment loss, with values chosen in the range of 0.1 to 10 times the mean distance between samples in a mini-batch. The number of samples is dynamically reduced during training to 50 and 100 to reduce computational complexity. The range of 0.1 to 10 times the mean distance between samples is chosen to avoid small kernel widths and cover a wide range of values. Empirical evaluation for the input kernel width is done in the range of 2-16 with consistent results. The kernel width for the label kernel matrix is set to 0.1 to avoid numerical instabilities. The IP of the MLP with ReLU activation function is studied using stochastic gradient descent and a learning rate of 0.09. MI was estimated using the MNIST dataset. The IP of the VGG16 on the CIFAR-10 dataset is also examined. The IP of the VGG16 on the CIFAR-10 dataset was studied, showing a fitting phase followed by a compression phase. The MI was estimated using test data and averaged over 2 runs. The output layer's I(Y; T) stabilizes at log 2(10) when the network achieves 100% accuracy. The entropy of Y is estimated using Equation 3, which requires computing the eigenvalues of the label kernel matrix. For an ideal case, where Ky is a rank K matrix, the entropy estimate is obtained. The IP of a CNN with ReLU activation function in hidden layers is examined, similar to a study by Noshad et al. (2019). The study analyzed the IP of the VGG16 network on the CIFAR-10 dataset, using the tanh activation function. The output layer settled at the expected value with close to 100% accuracy. This is the first time the full IP has been modeled for such a large-scale network. The MI for the training and test datasets showed similar trends to smaller networks. The study analyzed the IP of the VGG16 network on the CIFAR-10 dataset, showing trends in fitting and compression phases during training. Differences in MI values between training and test data were noted, along with challenges in computing MI for convolutional layers. The impact of early stopping on the IP was also investigated. Early stopping is a regularization technique in deep neural networks where training is halted if validation accuracy does not improve for a set number of iterations. The compression phase in the network can be affected by the early stopping procedure, potentially related to preventing overfitting. Further experiments on this topic can be found in Appendix J. A DNN is viewed as a Markov chain defining an information path that should adhere to the Data Processing Inequality. The mean difference in Mutual Information (MI) between layers in MLP and VGG16 networks shows compliance with DPI. A novel framework using R\u00e9nyi's \u03b1-order entropy for MI analysis in DNNs is proposed in this work. The proposed approach utilizes tensor-based estimate of R\u00e9nyi's \u03b1-order entropy for Mutual Information (MI) analysis in deep neural networks. It scales well to large networks, providing insights into training dynamics. The compression phase during training, linked to overfitting, is observed to occur before or at early-stopping criteria. The claim that H(X) \u2248 I(T ; X) and H(Y ) \u2248 I(T ; Y ) does not hold for the tensor-based approach. This new method aims to enhance theoretical understanding of DNNs. The experiment involves estimating entropy and mutual information in 100-dimensional data from normal distributions. 500 samples are generated from six distributions with varying variances, and entropy is estimated. Results show a decrease in entropy as variance decreases, with batch-wise approximation producing similar results to using the full dataset. In a high-dimensional setting, 500 samples are generated from normal distributions with varying variances to estimate mutual information. The results show that as the distributions overlap less, the mutual information estimates decrease, demonstrating the ability of the estimators to capture dependencies. The batch-wise approximation produces similar results to using the full dataset. The batch-wise approximation in a high-dimensional setting produces similar estimates as using the full dataset. Figures illustrate entropy and mutual information estimates on 100-dimensional normal distributions. Mutual information decreases as distributions overlap less. Equation 2 properties were analyzed in detail in a previous study. The kernel matrix A from raw data acts like a density matrix in quantum information theory. It is related to an empirical covariance operator on embeddings of probability distributions in a RKHS. The empirical covariance operator G is defined through a bilinear form. The difference between tr(G) and tr(\u011c) can be bounded. The difference between tr(G) and tr(\u011c) can be bounded under certain conditions, with probability 1-\u03b4 where C is a compact self-adjoint operator. The tensor-based approach eliminates numerical instabilities in complex neural networks, enabling training of networks with many filters. The multivariate matrix-based joint entropy struggles with a large number of channels in an image tensor. The experiment conducted involved duplicating images with noise added along the channel dimension, resulting in a kernel matrix that tends towards a diagonal matrix as the number of channels increase. This is due to vanishing Hadamard products. The experiment involved duplicating images with noise added along the channel dimension, leading to a kernel matrix that tends towards a diagonal matrix. The multivariate approach should yield the same kernel as the tensor kernel approach, but the off-diagonal elements decrease rapidly. The kernel matrices obtained using the tensor kernel approach remain almost unchanged as the number of channels increase, demonstrating consistent similarity between samples. The third row displays kernel matrices obtained using matricization-based tensor kernels, preserving structure between channels. The structured tensor kernel approach produces similar results to the tensor kernel used in the paper. For small images with centered objects like MNIST and CIFAR10, the structured tensor kernel may not capture much more information. However, for complex tensor data, exploring structure-preserving tensor kernels is an interesting avenue for future studies. The architectures utilized in the experiments are detailed in Section 5 of the main paper, with weights initialized according to specific methods based on the activation function used. The experiments used the tanh activation function with biases initialized as zeros. The MLP architecture included Batch Normalization after each hidden layer, with a fully connected layer having 256 inputs and 10 outputs followed by a softmax activation function. The IP of the MLP showed a fitting phase followed by a compression phase. The IP of the CNN with tanh activation function shows an initial increase in information transfer, followed by a decrease in one aspect while the other remains stable. The CNN can extract essential information quickly, and the optimal kernel width stabilizes rapidly in each layer. The optimal kernel width stabilizes quickly in each layer during training of a simple 3 layer perceptron with 2 ReLU neurons, demonstrating a decrease in computational complexity. A video showcasing the training process is available at the provided link. During training of a 3 layer perceptron with 2 ReLU neurons, the network approaches 100 percent accuracy after approximately 1250 iterations. The output layer shows compression, and the decision surface in the input space becomes narrower by the end of training, making it harder to classify new samples outside the training space. The compression phase during training of a 3 layer perceptron with 2 ReLU neurons is not necessarily linked to improved generalization. Mean difference in MI between layers in MLP and VGG16 networks shows compliance with the Data Processing Inequality, with lower MI differences in early layers."
}