{
    "title": "SyxPVLUYdE",
    "content": "Extending models with auxiliary latent variables increases model expressivity. Importance Weighted Autoencoders (IWAE) can be seen as expanding the variational family with auxiliary latent variables, encompassing recent developments in variational bounds. Applying these techniques to generative models, a new model similar to IWAE outperforms the Learned Accept/Reject Sampling algorithm, while being easier to implement. Deep generative models with latent variables have gained popularity due to their success in modeling various types of data such as natural images, speech, music time-series, and video. These models utilize auxiliary latent variables to construct complex marginal distributions from tractable conditional distributions. While directly optimizing the marginal likelihood is challenging, a variational lower bound like the evidence lower bound (ELBO) can be maximized instead. The tightness of variational lower bounds for latent variable models has seen recent advancements. Different bounds, such as Sequential Monte Carlo and Hamiltonian Monte Carlo, can be viewed as instances of auxiliary variable variational inference. These bounds can be justified by embedding them in the variational family as choices of auxiliary latent variables. The success of augmenting variational distributions with auxiliary latent variables motivates investigating a similar augmentation for generative models. This introduces an extra degree of learnable flexibility, allowing for the development of a latent variable model based on self-normalized importance sampling with a tractable lower bound on its log-likelihood. The model proposed in the current text chunk is based on self-normalized importance sampling and has a tractable lower bound on its log-likelihood. It is closely related to ranking NCE BID25 and outperforms other approaches in evaluation. The proposed model in this work outperforms recent approaches in BID4, despite being simpler and more computationally efficient. It involves learned probabilistic models of data with latent variables z to construct complex distributions. By optimizing a tractable lower bound on log p(x), the model achieves a tight evidence lower bound (ELBO) controlled by the accuracy of the variational distribution q(z|x). Limited expressivity in the variational family can impact the learned model negatively. The text discusses the use of variational distributions to model complex distributions, introducing the importance weighted autoencoder (IWAE) bound to tighten the variational bound. This bound converges to the log marginal as the number of samples K increases. BID6 . BID29 developed Monte Carlo Objectives (MCOs) to extend the notion of unbiased estimators in variational inference. IWAE is a special case using K-sample importance sampling. Maddison et al. investigate MCOs in sequential models based on unbiased estimators from Sequential Monte Carlo. Various unbiased estimators, like Hamiltonian Importance Sampling and Sequential Monte Carlo, can be justified as simple importance sampling on an extended state space. This approach involves defining auxiliary variables and distributions to achieve an unbiased estimator and a variational bound. Recent improvements in variational bounds involve augmenting variational families with latent variables. Recent improvements in variational bounds involve augmenting variational families with latent variables, allowing for the application of auxiliary variable variational inference tools to understand algorithm tradeoffs and derivations. This unified view reveals novel bounds and design choices. The variational family is defined by sampling candidate z i s from a proposal distribution q(z i |x) and then sampling z from an empirical distribution composed of atoms located at each z i weighted proportionally to p(x, z i )/q(z i |x). The auxiliary latent variables \u03bb are the locations of the proposal samples z 1:K and the index of the selected sample, i. The proposal involves sampling z 1:K and the index i, with w i = p(x, z i )/q(z i |x). The choice of q and r yields the IWAE bound Eq. (3) from Eq. (2). IWAE is a lower bound on the standard ELBO for q(z|x), with a gap due to D KL (q(z 1:K , i|z, x)||r(z 1:K , i|z, x)). The optimal choice of r is DISPLAYFORM2, while Eq. (5) makes an approximation that could be improved by learning a factored variational distribution. This could enhance the bound's tightness, and future work will explore this further. In future work, recent improvements in variational bounds can be seen as importance sampling on an extended state space. By explicitly choosing r, the gap between the bound and the ELBO with the marginalized variational distribution becomes clear, leading to novel choices for r. The self-normalized importance sampling (SNIS) generative process draws samples from a proposal \u03c0(x), weights them according to a potential function U(x), and then draws a sample from the empirical distribution formed by the weighted samples. The SNIS generative process is defined in Algorithm 1, with the density denoted as p SN IS(x). The density of the process p SN IS(x) can be lower bounded, with a tractable lower bound on its log-likelihood. As K \u2192 \u221e, p SN IS(x) becomes proportional to \u03c0(x) exp(U(x)). For finite K, p SN IS(x) interpolates between \u03c0(x) and \u03c0(x) exp(U(x). Training the SNIS generative model involves stochastic gradient ascent on Eq. (6) with respect to the proposal distribution \u03c0 and potential function U. The distribution \u03c0(x) and potential function U(x) are used to sample x and compute weights. Reparameterization gradients are used for continuous data, while score function gradient estimators like REINFORCE or Gumbel-Softmax are used for discrete data. Augmenting generative models with latent variables from complex samplers like Hamiltonian Monte Carlo is being explored. Equation FORMULA9 is linked to the ranking NCE loss, a common objective for training energy based models. If \u03c0(x) is considered as noise distribution pN(x) and U(x) = \u0168(x) - log pN(x), the ranking NCE loss is recovered. The ranking NCE loss is a consistent objective for any K > 1 when the true data distribution is in our model family. It provides a lower bound on the log likelihood of a well-specified model and offers a novel perspective on Contrastive Predictive Coding. The distribution \u03c0(x) and potential function U(x) are used for sampling and computing weights, with reparameterization gradients for continuous data and score function gradient estimators for discrete data. The self-normalized importance sampling distribution is used to recover the CPC bound, proving it as a lower bound on mutual information. Generative models based on SNIS were evaluated on synthetic and MNIST datasets, with comparison to the LARS model. LARS utilizes rejection sampling with a trained proposal distribution and acceptance function for generating samples efficiently. The LARS model uses rejection sampling with a trained proposal distribution and acceptance function for efficient sample generation. However, it requires estimating a normalizing constant and implementing additional tricks for successful training. In contrast, SNIS has a tractable log likelihood lower bound for any K, eliminating the need for implementation tricks and utilizing all samples for training. Comparing the performance of LARS and SNIS shows potential advantages of the latter. In a comparison between LARS and SNIS on synthetic data, both achieve similar data log-likelihood lower bounds, but SNIS does so faster. The experiment replicates a mixture of Gaussian densities model from BID4, using fixed 2-D N(0,1) proposal distributions for both methods. Batch sizes of 128 and ADAM BID19 with a learning rate of 3 \u00d7 10 \u22124 were used for model fitting. SNIS quickly converges to the solution in modeling the MNIST handwritten digit dataset with different model configurations, including a VAE with an SNIS prior and an SNIS model with a VAE proposal. Hyperparameters were chosen to match previous MNIST experiments. The SNIS model for MNIST experiments used hyperparameters to match previous studies. The VAE observation model had neural networks with two layers of 300 units and tanh activations. The SNIS potential function was parameterized by a neural network with two hidden layers of size 100 and tanh activations. The VAEs had 50-dimensional latent spaces, and SNIS's K was set to 1024. The weight of the KL term in the ELBO was linearly annealed from 0 to 1 over the first 1 \u00d7 10^5 steps, and the learning rate was dropped from 3 \u00d7 10^-4 to 1 \u00d7 10^-4 on step 1 \u00d7 10^6. The models were trained with ADAM. In the SNIS model with VAE proposal, the Straight-Through Gumbel estimator was initially used but performed worse than ignoring those gradients altogether. In the SNIS model with VAE proposal, log-likelihood lower bounds on the test set were summarized in Table 1. SNIS outperformed LARS using only 1024 samples for training and evaluation. The paper examined variational bounds improvement through auxiliary variable variational inference, exposing suboptimal choices in existing algorithms and deriving new methods like SNIS. Future work will explore unbiased gradient estimators and embedding methods like Hamiltonian Importance Sampling in generative models for better scalability. Published at ICLR 2019, this paper introduces Sampling and Annealed Importance Sampling in generative models to improve scalability with data dimension. By plugging equations into each other, the paper derives a lower bound on log-likelihood."
}