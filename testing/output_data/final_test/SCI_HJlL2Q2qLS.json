{
    "title": "HJlL2Q2qLS",
    "content": "Bayesian inference is commonly used to quantify uncertainty in a field of interest from related measurements. Generative adversarial networks (GANs) can provide an approximate distribution for Bayesian updates, addressing challenges with large discrete representations and complex prior distributions. This approach is effective in physics-based inverse problems and computer vision tasks, allowing for optimal sensor placement based on spatial uncertainty variation. Bayesian inference is a principled approach to quantify uncertainty in inverse problems, with applications in various fields. It requires an informative prior and efficient sampling from the posterior distribution. In this manuscript, a deep generative model (GANs) is proposed for Bayesian inference in inverse problems, where the goal is to infer parameters x from measurements y related through a forward model. The use of GANs addresses the ill-posedness of the inverse problem by modeling the unknown parameter and measurements as random variables. The solution to the problem involves modeling the unknown parameter and measurements as random variables to characterize uncertainty. Bayesian inference faces challenges in constructing reliable prior distributions and sampling from posterior distributions. GANs are considered to address these challenges. The main idea involves training a GAN using a sample set S and using the learned distribution as the prior in Bayesian inference. This provides a method for representing complex prior distributions and efficient sampling from the posterior distribution in terms of the latent vector z. The use of GANs in solving inverse problems is a novel approach, with machine learning-based methods like CNNs also being considered. GANs have been utilized in computer vision tasks, as well as in learning regularizers for inverse problems and compressed sensing. These approaches differ from the idea of training a GAN using a sample set and using the learned distribution as a prior in Bayesian inference. The use of GANs in solving inverse problems is a novel approach, with machine learning-based methods like CNNs also being considered. Previous approaches differ in that they solve the inverse problem as an optimization problem without quantifying uncertainty in a Bayesian framework. Adler and \u00d6ktem [2018] utilized GANs in a Bayesian setting, training the GAN to approximate the posterior distribution in a supervised fashion with paired samples. The generator learns the true distribution with infinite capacity and sufficient data. In a Bayesian framework, the posterior distribution of x can be inferred using a measurement\u0177 and setting the prior distribution to be equal to the true distribution. Sampling from the posterior distribution for x is equivalent to sampling from the posterior distribution for z and transforming the sample through the generator g, which is an efficient approach due to the smaller dimension of z. This efficient approach involves sampling from the posterior of x using a GAN trained on prior measurements of x. The algorithm allows for complex priors and efficient sampling due to the smaller dimension of z compared to x. Additionally, various estimation methods are discussed in Appendix A for estimating population parameters of the posterior. The approach involves determining the initial temperature distribution of a solid from its current temperature measurement. The inferred field is represented on a grid, and the forward operator is defined by solving the time-dependent heat conduction problem. The initial temperature is assumed to be zero everywhere except in a rectangular region, parameterized by coordinates and temperature value. The initial temperature distribution of a solid is inferred using a Wasserstein GAN with a latent space, generating noise-free and noisy versions of the measured field. The MAP estimate for the posterior distribution of the latent vector is determined, showing close agreement with the true value of the inferred field. The MAP estimates obtained using different priors show remarkable agreement with the true value, despite a high noise-to-signal ratio. Iterative image recovery with sparse measurements and uncertainty information is demonstrated, along with results from sampling the posterior distribution. The MCMC approximation shows smeared edges and corners, indicating uncertainty in the temperature field recovery. The uncertainty in recovering the initial temperature field is indicated by smeared edges and corners due to the smoothing nature of the forward operator. The point-wise standard deviation of the inferred field shows the largest uncertainty at the edges and corners. Additional examples of the inverse heat conduction problem are provided in the appendix. A problem in computer vision involves estimating the noise-free version of an image from a noisy sub-region. An iterative approach is used, selecting regions with maximum variance to make a good guess in few iterations. This falls under active learning in machine learning and is useful for expensive measurements. WGAN-GP trained on MNIST data set is used as a prior in Bayesian inference to infer the original image from a noisy version. The study involves using noise with 0.8 variance to mask regions in an image and infer the original image using a forward map. The user selects a sub-region in each iteration based on maximum variance, with results shown for different MNIST digits. The algorithm developed in the work is used to compute variance, and results from several iterations are presented. The study involves using noise with 0.8 variance to mask regions in an image and infer the original image using a forward map. The user selects a sub-region in each iteration based on maximum variance, with results shown for different MNIST digits. The estimated variance reduces with each iteration, converging to an image close to the true image in 2-3 iterations. Additional results for MNIST and CelebA dataset are provided in Appendix B, along with examples of iterative image recovery schemes. The performance of the variance-driven iterative strategy is compared to random sampling scheme. The method involves selecting a window using a variance-driven strategy to reveal an image iteration. Ground truth is indicated in the top row, with additive Gaussian noise of variance=1 used for all images."
}