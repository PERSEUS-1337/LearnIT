{
    "title": "r16u6i_Xz",
    "content": "Neural networks have shown excellent performance on classification tasks but require much data to train. To address overfitting in small-sample image classification, a new ensemble learning method called InterBoost is proposed. It involves training two base networks with complementary datasets and further training them through interaction. This process continues iteratively until a stop criterion is met. In the testing phase, the outputs of two networks are combined to obtain a final classification score. Small-sample classification remains a challenge despite advances in deep learning. The method provides detailed analysis for understanding its mechanism in image classification. Ensemble learning can help reduce variance in neural networks trained with small datasets. There is a trade-off between bias and variance in estimation errors, and ensemble learning can improve accuracy by combining diverse models trained on different datasets. Bagging is a classic strategy in ensemble learning. Ensemble learning involves two main strategies: Bagging and Boosting. Bagging trains independent classifiers on bootstrap re-samples of training data and combines them based on certain rules. Boosting starts with a classifier trained on available data and sequentially trains new member classifiers. Adaboost is an example of Boosting that works well for weak base classifiers. Ensemble methods like Adaboost work well for weak base classifiers, but can struggle with complex classifiers like large neural networks. Implicit ensemble methods in neural networks include techniques like Dropout, DropConnect, and Stochastic Depth. Snapshot Ensembling can find multiple local minima to create ensemble members, while Temporal ensembling trains on a single network for predictions. InterBoost is a new ensemble method proposed for training two base neural networks with the same structure. It re-weights the original dataset with complementary weights and trains the two networks separately. This method aims to create diverse and complementary neural networks for small-sample classification. The proposed ensemble method, InterBoost, trains two base neural networks with complementary weights on the original dataset. The networks are trained interactively to push each other in opposite directions, creating diversity. This process is repeated until a stop criterion is met. The method is evaluated on UIUC-Sports and LabelMe datasets, comparing it to other existing methods like Bagging and Adaboost. The InterBoost method involves assigning weights to data points for training two base neural networks interactively. This approach differs from traditional Boosting methods by training the networks simultaneously with complementary datasets. InterBoost method trains two base networks using complementary datasets and iteratively updates data weights based on classification probabilities. The weights are kept complementary to ensure the networks are complementary. Training and weight updates run alternately until a stop condition is met. The InterBoost method trains two base networks using complementary datasets and updates data weights based on classification probabilities. The weights are adjusted to ensure network complementarity, with training and weight updates running alternately until a stop condition is met. The interaction between the networks allows for diverse training on different datasets, resembling an \"implicit\" Adaboost approach. The InterBoost method involves training two base networks with complementary datasets and updating data weights based on classification probabilities. The weights are adjusted to ensure network complementarity, with training and weight updates running alternately until a stop condition is met. The function used to update data weights may face challenges when probabilities are similar, leading to difficulties in assigning different weights to data points in the two networks. InterBoost involves training two base networks with complementary datasets and updating data weights based on classification probabilities. The weights are adjusted to ensure network complementarity, with training and weight updates running alternately until a stop condition is met. The function used to update data weights is more sensitive to small differences between probabilities when they are both large. The training procedure of InterBoost includes training two base networks, recalculating data point weights based on prediction results, and iterating until a predefined performance is achieved or the maximum iteration number is reached. Through the interactive and iterative training process, two base neural networks are trained over various regions of the problem space. A fusion strategy combines their prediction results, and the final label is chosen based on the maximum prediction value. The unseen data's class probability is calculated using the trained networks. During the InterBoost training process, two base networks are trained with complementary datasets. The weights of data points are updated based on the prediction probabilities from the networks to ensure diversity and accuracy in classification. During the InterBoost training process, two base networks are trained with complementary datasets. The rule is sensitive to small differences in prediction probabilities to prevent premature training. The weights of data points are adjusted based on the accuracy of each network, leading to diverse training dataset pairs. This implicit ensemble approach improves the accuracy of each base network over time. The number of epochs for training is crucial to avoid overfitting. In experiments, a suitable epoch number is crucial for training to avoid overfitting. The proposed ensemble method has no limitations on the type of neural networks and can be extended for multiple networks. The focus is on small-sample image classification using datasets like LabelMe containing 8 classes of natural scene images. The LM dataset consists of 8 classes of natural scene images like coast, forest, highway, city, mountain, open country, street, and tall building, with a total of 1680 images. The UIUC-Sports dataset contains 8 classes of sports scene images with a total of 1579 images, including sports like bocce, polo, rowing, sailing, snowboarding, rock climbing, croquet, and badminton. The datasets are split into training, validation, and test datasets, with each class having a specific number of data points. The LM and UIUC datasets are divided into training, validation, and test sets. 10 data points are randomly chosen for validation, with the remaining data split equally between training and test sets. Images are resized to 256x256 and features are extracted using the VGG16 BID15 network. The final feature dimensions are 32768. A fully connected network with two layers is used, with Relu activation in the first layer. The activation functions used in the fully connected network are Relu in the first layer and Softmax in the second layer. Overfitting was observed with larger numbers of hidden units, so 32 units were chosen. Dropout was not used as it did not improve performance. The L2 norm parameter for network weights was set to 0.01. Minibatch gradient descent with RMSprop optimization and a learning rate of 0.001 was used. The classification performance of the InterBoost method was evaluated on LM and UIUC datasets compared to SVM and Softmax classifier. The study compared different classification methods including SVM, Softmax, Fully connected network, Bagging, Adaboost, and SnapShot Ensembling. The code for SnapShot was adopted from the author, while the others were implemented using the Keras framework. Different epoch numbers were tested for Adaboost, with 800 epochs showing similar performance to 500 epochs. Epoch numbers for FC, Softmax, and Bagging were set at 800. SnapShot generated a snapshot network every 800 epochs, with a total of 2 snapshots. The codes for the methods and datasets used in the experiment can be found on an anonymous webpage. Our method achieves an accuracy of 89.0% on the UIUC dataset and 86.4% on the LM dataset, outperforming Bagging. The study evaluated different classification methods, including SVM, Softmax, Fully connected network, Bagging, Adaboost, and SnapShot Ensembling. SnapShot generates a snapshot network every 800 epochs, with a total of 2 snapshots. The average accuracies of the methods are reported in TAB0, and box plots of accuracies are shown in FIG2 to evaluate robustness and stability. Our method, InterBoost, does not show superior performance compared to other baseline methods on the LM and UIUC datasets. It involves training neural networks for small-sample classification by sharing information between two base networks and training them on diverse datasets iteratively. Experimental results on UIUC-Sports and LabelMe datasets indicate that our ensemble method does not outperform other ensemble methods. Future work includes enhancing the proposed method. Future work involves improving the InterBoost method by increasing the number of networks and experimenting on different types of networks and data to evaluate its effectiveness."
}