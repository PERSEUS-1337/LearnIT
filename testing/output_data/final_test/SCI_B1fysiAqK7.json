{
    "title": "B1fysiAqK7",
    "content": "Low bit-width weights and activations are used to reduce memory and compute power in Deep Neural Networks. A probabilistic training method called PBNet is introduced for Neural Networks with binary weights and activations. Stochasticity is embraced during training to avoid gradient approximation for functions like $\\textrm{sign}(\\cdot)$, leading to a fully Binary Neural Network at test time. Ensemble predictions and uncertainty estimates are improved by sampling from the weight distribution. Stochastic versions of Batch Normalization and max pooling are introduced for operations in PBNet, which transfer to a deterministic network at test time. Two training methods for PBNet are evaluated, including one with activation distributions. Sampling binary activations is crucial for stochastic training of binary Neural Networks, as shown in experiments comparing different training methods for the PBNet. Deep Neural Networks require significant memory and computation resources, making them impractical for resource-limited environments. One approach to reduce these requirements is by decreasing the bit-width of parameters and activations in Neural Networks. Binary Neural Networks, which use a single bit for weights and activations, are a extreme example of this optimization. Binary Neural Networks offer advantages such as a 32\u00d7 reduction in memory requirements and a 58\u00d7 speedup on CPU. They are also more robust to adversarial examples. A probabilistic training method is proposed for Neural Networks with binary weights and binary activations, making them even more memory and computation efficient. The forward pass for probabilistic neural networks is obtained by constraining the input and weights to binary variables. The output of Multiply and Accumulate operations is approximated using a factorized Normal distribution. The text discusses a probabilistic training method for Binary Neural Networks, which offer advantages such as reduced memory requirements and faster processing. The method involves using stochastic versions of Max-Pooling and Batch Normalization to propagate pre-activations through a single layer. Two different training methods are explored, one involving propagating activation distributions between layers and the other sampling binary activations using a concrete relaxation method. The text discusses a probabilistic training method for Binary Neural Networks, utilizing stochastic versions of Max-Pooling and Batch Normalization. The method involves using concrete relaxation for local reparametrization in a single layer, resulting in networks named PBNet and PBNet-S. At test time, deterministic Binary Neural Networks, ensembles, or Ternary Neural Networks can be obtained. The method allows for anytime ensemble predictions and uncertainty estimates without the need for retraining. The text introduces the probabilistic setting of PBNet, focusing on the binary distribution over network weights and optimization. It also discusses the approximation of the distribution on pre-activations and mentions other operations in PBNet such as activation, sampling, pooling, and normalization. In PBNet, a binary distribution is posed over network weights, optimizing the distribution parameters instead of the weights directly. This approach deals with the discreteness of a Binary Neural Network and optimizes a bound on the actual loss through variational optimization. The Binary distribution is parameterized by \u03b8 \u2208 [\u22121, 1] and is used for both weights and activations. In PBNet, a Binary distribution is used for both weights and activations in a Neural Network. The inner product between the weights and input is distributed according to a translated and scaled Poisson binomial distribution. This distribution is the exact solution but difficult to work with in subsequent layers. In PBNet, a Normal approximation is used for the Poisson binomial distribution to simplify manipulations. The pre-activation approximation is obtained using properties of the Binary distribution and Poisson binomial distribution, similar to the Lyapunov Central Limit Theorem. This approximation allows for easier propagation through the network. In PBNet, a Normal approximation is used for the Poisson binomial distribution to simplify manipulations. The pre-activation approximation is obtained using properties of the Binary distribution and Poisson binomial distribution, similar to the Lyapunov Central Limit Theorem. This approximation allows for easier propagation through the network. Versions of Batch Normalization and Max Pooling are introduced in Section 2.2. For specifics on sampling the binary activation, see Section 2.1.1. The full forward pass for a single layer is given in detail in Algorithms 1. Various works use either deterministic or stochastic binarization functions, resulting in stochastic binary activation from random pre-activations. The activation after binarization is represented as a Binary random variable, with the Binary probability computed using the probability density above the binarization threshold. Binarization of random pre-activation considers the uncertainty about the sign of the activation, taking variance into account for correct assignment. This method differs from deterministic binarization by incorporating variance and non-zero derivatives for better representation of uncertainty. Our method considers variance for correct assignment of probability mass to -1 in binary activation. Sampling activations using Concrete distribution BID16 during training improves matching with test time input. The input to layers becomes a vector instead of a distribution, affecting pre-activation computation. The approximation to the pre-activation distribution is determined by the Lyapunov CLT, with random weight w \u223c Binary(\u03b8). Batch Normalization and pooling are important building blocks for Convolutional Neural Networks. However, for Binary Neural Networks like PBNet-S, applying Batch Normalization to binarized activations and max pooling on binary activations must be done before binarization. In PBNet, binarization is applied before sampling, so Batch Normalization and pooling operations are applied on random pre-activations. Batch Normalization (BN) is defined for random variables, with the aim of translating and scaling pre-activations to have zero mean and unit variance. Stochastic operations are used during training, but at test time, they are replaced by conventional implementations. Parameters learned in the stochastic setting are transferred to their deterministic counterparts. The aim of batch normalization is to ensure pre-activation samples have zero mean and unit variance. To achieve this in a deterministic binary neural network, the population mean and variance are subtracted and divided from each pre-activation random variable. This process ensures a standardized distribution over activations. Batch Normalization in a Binary Neural Network simplifies to an addition and sign flip of the activation at test time. Max pooling for stochastic pre-activations involves selecting one of the pre-activation values in a spatial region. The distribution of max(a1, ..., aK) is not Gaussian and does not match the input distributions. Instead, one of the input random variables is sampled in every spatial region. Max pooling for stochastic pre-activations involves selecting one of the input random variables in every spatial region based on the probability of that variable being the greatest. This can be done efficiently by sampling from the maximum of the input variables. A single sample from each normal distribution associated with each pre-activation is obtained, and the random variable with the maximum sample is kept. In stochastic pre-activations, a random variable is selected based on the probability of it being the maximum in each spatial region. Parameters for the PBNet are initialized from a uniform distribution, with a significant improvement observed in training speed by initializing based on a pre-trained Neural Network. The convolutional filters in the Neural Network are initialized with more structure to avoid slow convergence. Max pooling for random variables involves selecting the maximum sample from input distributions. Weight transfer method is used for PBNet-S to initialize parameters based on expected values of random weights. Rescaled weights may not all fall within the range of [-1, 1]. The weights in a given layer are rescaled and clipped between [-0.9, 0.9]. A deterministic binary Neural Network is obtained from the parameter distribution at test time, with three approaches: PBNET-MAP, PBNET-x ensemble, and PBNET-TERNARY ternary network. The ternary network can be seen as a sparse PBNet, with the option to set a parameter to zero based on q \u03b8. Using multiple binary neural networks in an ensemble is more efficient in computation and memory compared to full precision. Anytime ensemble predictions and uncertainty estimates are possible by sampling from the weight distribution, but this may cause a shift in batch statistics. Re-estimating batch norm statistics after weight sampling helps alleviate this issue. Binary and low precision neural networks have gained interest recently. In Binarized Neural Networks, real-valued shadow weights are binarized to obtain binary weights. Pre-activations are also binarized using the same function. The straight-through estimator is used to back-propagate through the binarization operation. Extensions like XNOR-net and ABC-nets approximate parameters and activations with binary tensors and scaling factors. ABC-nets and other methods approximate weight tensors with binary tensors and scaling factors for linear operations in forward pass. McDonnell (2018) used similar methods to binarize a wide resnet for ImageNet results close to full precision performance. Expectation Backpropagation BID22 uses central limit theorem and online expectation propagation for training binary neural networks. Other related works include BID21 using local reparametrization trick and BID1 for training Neural Networks with binary weights. The PBNet is trained with binary weights and activations, evaluated on MNIST and CIFAR-10 benchmarks, compared to BID7. Two loss functions are used: cross-entropy (CE) and binary cross entropy (BCE). For CE, no binarization in final layer, while BCE includes binarization step. The PBNet-S networks are initialized using a transfer method and a uniform initialization scheme. Models are optimized using Adam and a validation loss plateau learning rate decay scheme. The temperature for the binary concrete distribution remains static at 1.0 during training. Model parameters are optimized until convergence, and the best model is selected based on a validation set. PyTorch BID19 is used for implementation. Binarized Neural Networks follow a training procedure with a squared hinge loss and layer-specific learning. The training procedure described by BID7 includes a squared hinge loss and layer-specific learning rates determined based on the Glorot initialization method. Experimental details specific to datasets are provided in Appendix C, with results presented in TAB0. Test set accuracy is reported after binarizing the network and during training with stochastic network propagation. Ensemble usage improves accuracy, and predictions from ensemble members can estimate the certainty of the ensemble. PBNet, a full precision network (FPNet), and various PBNet variations are discussed, including PBNet-map, PBNet-Ternary, and PBNet-X ensemble. The ensemble results show that the variance is a better estimator of network certainty, improving as ensemble sizes increase. The error-coverage curve BID3 in FIG2 illustrates this, with the highest softmax score used for the Binarized Neural Network and PBNet-MAP, and the variance in prediction for ensembles. After sampling parameters of a deterministic network, batch statistics for Batch Normalization must be re-estimated. Results from using various batch sizes show even a small number of samples is sufficient. An ablation study on Batch Normalization and weight transfer for PBNet-S on CIFAR-10 was conducted. Removing batch normalization layers resulted in 79.21% test accuracy, while weight initialization with uniform scheme achieved 83.61% accuracy. Validation set accuracy during training is shown in FIG2. These results were obtained without sampling a binarized network. The PBNet-S utilizes weight transfer and stochastic Batch Normalization for improved performance. Sampling binary activations during training is crucial, as it enhances network robustness to binarization noise. While the stochastic PBNet generalizes well, a significant drop in test accuracy occurs when obtaining a binary Neural Network from it. However, this performance drop is not observed for PBNet-S, suggesting the importance of sampling binary activations during training. The method presented is a stochastic approach for training Binary Neural Networks, which has shown competitive results on standardized benchmarks. The PBNet has advantageous properties such as improved accuracy and uncertainty estimations through weight distribution and Bayesian formulation. Additionally, a Binary distribution is introduced for convenience in the paper. The Binary distribution introduced in this paper is a reparametrization of the Bernoulli distribution, with properties derived from it. The distribution has a probability mass function with parameters a and \u03b8, and mean and variance can be easily computed. During training, PBNet uses stochastic Batch Normalization, which can be transferred to conventional Batch Normalization at test time. Alternatively, Batch Normalization can be simplified to an (integer) addition and multiplication by \u00b11 after applying the sign activation function. The MNIST dataset consists of 60K training and 10K test grayscale handwritten digit images, pre-processed by subtracting global pixel mean and dividing by global pixel standard deviation. Architecture includes binary convolutional layer, fully connected layer, softmax layer, and max pooling. For the CIFAR-10 dataset, a binary neural network architecture is used with batch normalization and binarization after max pooling. The dataset consists of 50K training and 10K test RGB images divided into 10 classes. Images are pre-processed by subtracting mean and dividing by standard deviation. The architecture includes convolutional layers, max pooling, and fully connected layers with a batch size of 128 and initial learning rate of 10^-2. The architecture for the CIFAR-10 dataset includes batch normalization and binarization after max pooling. Training set is augmented with translations and horizontal flips. Results are reported in TAB0."
}