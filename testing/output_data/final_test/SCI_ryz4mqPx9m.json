{
    "title": "ryz4mqPx9m",
    "content": "Despite advances in speech recognition, current models are sensitive to noise, affecting performance. Data augmentation is commonly used to improve robustness by adding noisy examples to the training set. This paper introduces invariant-representation-learning (IRL) to ensure clean and perturbed examples map to the same representation, leading to significant reductions in character error rates on the LibriSpeech dataset. The IRL algorithm significantly reduces character error rates on clean and other test sets, as well as in out-of-domain noise settings. Ablations confirm the results are not due to shrinking activations. During training, noisy versions of examples are generated using a stochastic function, with a penalty term added to the loss function to penalize the distance between encodings of original and noisy data points. In experiments, noisy data points are represented at different layers, with the encoder output chosen as \u03c6 e. The loss function includes terms to maximize probabilities for clean and noisy examples, along with a penalty term for noise-invariant representations. The primary loss for multiclass classification is cross-entropy. Our model's softmax output, size and\u0177, is used to induce similar representations for clean and noised data by applying a penalty that penalizes the L2 distance and negative cosine similarity between the hidden representations. This joint penalty is necessary to prevent trivial manipulations that could reduce the distance between the representations. The loss function is expressed as a sum over successive representations of clean and noisy data. In experiments, IRL-C consistently shows a small improvement over previous results. In experiments, IRL-C consistently gives a small improvement over results achieved with IRL-E. The approaches differ in the penalty applied, with IRL-C concatenating representations across time steps before calculating the penalty. MUSAN noise is added to the training data with a specific signal-to-noise ratio. The text discusses model architectures, training details, and baselines for noisy speech synthesis. Models are trained with the Adam optimizer and evaluated using character error rate. Additional loss terms for IRL models include L2 loss. The text discusses training models on the LibriSpeech corpus with noisy data generated from the MUSAN dataset. Experiments show that models trained with the IRL procedure outperform baseline models, especially when faced with out-of-domain noise. Decreasing the signal-to-noise ratio affects baseline models more than IRL-trained models. The study demonstrated that models trained with the IRL algorithm outperformed baseline models on the ASR task, especially in handling out-of-domain noise. Signal-to-noise ratio had a greater impact on baseline models compared to IRL-trained models. Additionally, modifying the speaker volume did not affect the accuracy of the networks. The IRL algorithm helped improve model accuracy, robustness to noise, and convergence speed without impacting inference throughput. These findings can be applied broadly to deep networks for various supervised tasks. The study showed that the IRL algorithm improved model accuracy, robustness to noise, and convergence speed without affecting inference throughput. This can be applied to deep networks for any supervised task, with a focus on the speech setting."
}