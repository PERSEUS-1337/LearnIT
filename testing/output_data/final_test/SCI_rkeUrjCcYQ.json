{
    "title": "rkeUrjCcYQ",
    "content": "The Monge-Amp\\`ere flow is a deep generative model based on the Monge-Amp\\`ere equation in optimal transport theory. It uses a learnable potential function to guide a compressible fluid towards the target density distribution. Training involves solving an optimal control problem. This approach allows for efficient sampling and inference, and symmetry constraints can be easily imposed. The model is applied to unsupervised density estimation of the MNIST dataset and variational calculation of the two-dimensional Ising model at the critical point, incorporating insights from optimal transport and fluid dynamics. Generative modeling in deep learning research involves capturing joint probability distributions of high dimensional data to generate new samples. Variational autoencoders, generative adversarial networks, and autoregressive models have seen significant advances. Another class of generative models uses diffeomorphism to connect latent variables with a simple base. Flow-based generative models like NICE, RealNVP, and Glow use diffeomorphism to connect latent variables with a simple base distribution and data following a complex distribution. These models have tractable likelihoods and efficient exact inference due to network invertibility. However, a key concern is the tradeoff between the generative map's expressive power and training efficiency, often requiring additional constraints in the network architecture. Another challenge is imposing symmetry conditions for generating symmetry-related configurations with equal probability. The flow-based generative model aims to generate symmetry-related configurations with equal probability by drawing inspiration from optimal transport theory and dynamical systems. The model parametrizes the generative map as the gradient of a scalar potential function, formulating the generation process as a continuous-time gradient flow. This approach views the generative map as a deterministic dynamical system that evolves over time, leading to the neural network representation of the generative model. In this paper, the Monge-Amp\u00e8re flow is proposed as a new generative model for density estimation and variational calculation. The model treats probability density as a compressible fluid evolving under a learnable potential function's gradient flow. It offers tractable likelihoods and similar computational complexity for sampling and inference. The Monge-Amp\u00e8re flow allows for the construction of symmetric generative models by incorporating symmetries into the scalar potential. This framework simplifies the design of generative maps, relating probability densities in latent and physical spaces through a diffeomorphism. Solving the Monge-Amp\u00e8re equation for the convex Brenier potential u is challenging due to nonlinearity in the determinant, but offers a principled approach to designing lightweight yet powerful generative models. Solving the Brenier potential in machine learning and statistical physics is challenging due to nonlinearity in the determinant. Access to probability densities p and q is limited, making it a control problem. The computational challenge lies in the determinant of the Hessian matrix, scaling unfavorably with problem dimensionality. To address these issues, the Monge-Amp\u00e8re equation is considered in its linearized form, simplifying the transformation process. The Brenier potential is written as u(z) = ||z||^2/2 + \u03d5(z), with x \u2212 z = \u2207\u03d5(z). By expanding the logarithmic function and taking the continuous-time limit, we obtain a generative map defined by the evolution of x from time t = 0 to T. This continuous-time normalizing flow involves two ordinary differential equations describing the flow of continuous random variables and probability densities under change-of-variable transformation. The training process involves optimizing a functional to match probability densities and solve an optimal control problem for the potential function. The flow conserves total probability mass and follows a form of gradient flow. The irrotational flow transports probability masses in a generative model. The optimal transport theory motivates model architecture design for generative modeling. The optimal transport theory motivates model architecture design for generative modeling by adapting Wasserstein distances as an objective function. The potential function is parametrized using a neural network, and ODEs are integrated to transform data and their log-likelihoods. Backpropagation through ODE integration tunes the potential function for the probability density to flow to the desired distribution. The curr_chunk discusses the integration of equations under a parametrized potential function for unsupervised density estimation and variational free energy calculation. A densely connected neural network with one hidden layer is used, and the softplus activation function ensures differentiability to higher orders. The gradient and Laplacian of the potential are computed via automatic differentiation, and numerical ODE integration is employed. The integration of equations using a numerical ODE integrator is equivalent to evaluating a deep residual neural network. The network shares parameters in the depth direction, making it parameter efficient. A fourth order Runge-Kutta scheme with a fixed time step is employed for accuracy, resulting in a four-layer neural network block for ODE integration. The ODE integration procedure corresponds to layers of a neural network, resulting in samples x and their likelihoods ln p(x, T) dependent on the potential function \u03d5(x). Differentiable optimization is feasible due to small changes in input signals. The deep generative model from ODE discretization offers advantages over conventional neural networks, with a tradeoff between integration steps and potential function expressibility. Longer integration time means a deeper network, while testing allows for variable depth neural networks. The reversible ODE integrator allows for efficient inference by using larger time steps and fewer integration steps. It can integrate equations backward in time with the same complexity, returning to the starting point deterministically. The Monge-Amp\u00e8re flow is applied to unsupervised density estimation and variational calculations. Fluid dynamics are simulated to draw samples and evaluate their likelihoods using KL divergence as a measure of dissimilarity. The base distribution at time t=0 is a simple Gaussian. See Appendix C for hyperparameters used in experiments. To model the MNIST data, images are transformed into continuous variables using jittering and dequantizing procedures. The logit transformation is then applied to map the normalized input to the logit space. The Monge-Amp\u00e8re flow approach efficiently transforms data distribution to a Gaussian base distribution with lower test NLL compared to other models. It uses fewer learnable parameters and generates representative Ising configurations at different epochs. The generated samples exhibit two domains due to the Z 2 inversion symmetry in the network. Without this symmetry, the model would generate mostly black/white images. The flow maps Gaussian noises to meaningful images, minimizing the reverse KL divergence between the model and a given Boltzmann distribution. Sampling from p(x, T) involves starting from a base distribution x \u223c p(x, 0) = N(x) and evolving the samples according to equation 2. The loss function equation 7 is utilized in this process. The variational approach is applied to the Ising model, a prototypical problem in statistical physics, at the critical point to predict thermodynamics accurately. The energy function of the Ising model is represented in a continuous form, with the coupling set at the critical value. The Ising model on a periodic square lattice at the critical temperature has symmetries like spin inversion, translational, and rotational symmetry. A generative model respecting these symmetries is essential for physics applications. A generative model that respects physical symmetries is crucial for generating symmetry-related configurations with equal probability. The Monge-Amp\u00e8re flow solves this by imposing symmetry conditions on the scalar potential function. Different approaches have been explored to encode symmetry in the generating process, such as expressing the potential as an average over all elements of the symmetry group under consideration. The potential is expressed as an average over all elements of the symmetry group to ensure symmetry in generated samples. Sampling one term in the symmetric average at each step allows for evaluating the gradient and Laplacian of the potential. This approach maintains symmetry conditions in both training and data generation, leading to a 1% relative accuracy in the variational free-energy. The network generates Ising configurations directly at different training stages. The network generates Ising configurations directly at different training stages, producing domains of various shapes and a large variety of configurations respecting physical symmetries. It can estimate log-likelihoods for samples and perform inverse mapping of the energy function. This can accelerate Monte Carlo simulations by recommending updates as a generative model or performing hybrid Monte Carlo in the latent space. Comparing performances with and without translational and rotational symmetries imposed shows noticeable differences. The variational free energy is significantly higher without symmetry constraints compared to with symmetry constraints. Model distribution collapses to configurations with two horizontal domains, representing a metastable local minimum. Imposing necessary symmetry conditions is crucial for variational studies using generative models like normalizing flows. To scale to large datasets, efficient computation of the Jacobian determinant is essential. The text discusses the importance of efficient computation of the Jacobian determinant in bijective networks. It highlights the use of block structures in the Jacobian matrix to balance expressibility and efficiency. Autoregressive flows are mentioned for reducing computational cost, with forward flows used for density estimation and inverse flows for variational inference. These flows are implicitly reversible. The text discusses the efficient computation of Jacobian determinants in bijective networks using block structures. Autoregressive flows are used for density estimation and variational inference, with implicit reversibility. Continuous-time gradient flows have the same time complexity for forward and inverse transformations. The motivation for using gradient flows is to encode symmetries in the potential function. Hutchinson's trace estimator is used to simplify Jacobian computations. Our work focuses on deterministic and reversible advection flow of fluid, without stochastic forces, integrating flow equations for a finite time interval. It involves defining a dynamical system with a target terminal condition, akin to a control problem. The Neural ODE offers efficient back-propagation through blackbox ODE integrators. The Monge-Amp\u00e8re flow is used for generative modeling with a learnable potential parameterized by a neural network. Immediate improvements include adapting the neural network architecture for specific problems, exploring better integration schemes for reversible sampling, and utilizing backpropagation through the ODE. By employing backpropagation through the ODE integrator, memory consumption can be reduced and guaranteed convergence achieved in the integration process. Wasserstein distances can be used instead of KLdivergence to train the Monge-Amp\u00e8re flow, leading to a more practically useful generative model with tractable likelihood. Batch normalization layers can be considered during the integration, although their impact on the continuous gradient flow of a fluid needs further investigation. Additionally, a time-dependent potential can be used to induce a richer gradient flow of probability densities. The optimal transport flow in BID2 minimizes kinetic energy, following a pressureless flow in a time-dependent potential. The fluid moves with a constant velocity linearly interpolating between initial and final densities. A time-dependent potential corresponds to a deep generative model without shared parameters in the depth direction. The Monge-Amp\u00e8re flow has broader applications in machine learning and physics, inheriting all the benefits. The Monge-Amp\u00e8re flow has wider applications in machine learning and physics, allowing for symmetry imposition and modeling of exchangeable probabilities. It can be used for variational free energy calculation in realistic molecular systems and in conjunction with latent space hybrid Monte Carlo for efficient sampling. Monte Carlo is used for efficient sampling and gaining intuition about probability density flow under the Monge-Amp\u00e8re equation. A close form solution is worked out for a one-dimensional toy problem with a quadratic potential. The fluid parcel moves with acceleration under the quadratic potential, with those far from the origin moving faster. The width of the Gaussian distribution changes exponentially with time. The Ising model is a fundamental model in statistical mechanics with an exact solution in two dimensions. The Ising model in statistical mechanics is reformulated using continuous variables to make it amenable to the flow approach. By offsetting the coupling and decoupling the Ising interaction with continuous auxiliary variables, the model can be integrated using the fourth order Runge-Kutta method. Hyperparameters of the network and training are listed in a table. The Ising model in statistical mechanics is reformulated using continuous variables for the flow approach. Parameters include the total integration time (T = d), number of hidden neurons (h), and mini-batch size for training (B)."
}