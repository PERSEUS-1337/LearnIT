{
    "title": "rkxDoJBYPB",
    "content": "Our approach uses deep reinforcement learning to optimize neural network computation graphs in an optimizing compiler. It trains an optimizer offline and generalizes to new graphs, producing high-quality execution decisions quickly. The optimization tasks include minimizing running time and peak memory usage, outperforming classical and other learning-based methods. Deep Learning frameworks like MXNet, PyTorch, and TensorFlow represent neural network models as computation graphs. In the model parallelism setting, computation graphs can be executed using multiple devices in parallel. Nodes represent computational tasks, and edges show dependencies. Optimization involves deciding on node placement and schedule for efficient execution. Various static compilers like Glow, MLIR, TVM, and XLA are used for optimizing neural network computation graphs. In the context of model parallelism, optimization in compilers involves deciding on node placement and schedule for efficient execution on multiple devices. The goal is to minimize running time and peak memory usage while handling diverse graphs from different applications and users. Learning an optimizer that produces fast and high-quality solutions for large graphs is crucial in production settings. Learning to optimize model parallelism decisions involves generalizing to a broad set of unseen computation graphs without the need for extensive training time. Previous works have not considered this level of generalization or joint optimization of placement and scheduling. Our approach can optimize similar graphs in seconds, contrasting with previous methods that took hours per graph. Our approach involves collecting real user-defined graphs for a variety of tasks, architectures, and datasets, in contrast to previous methods that used artificially generated graphs. We use the Biased Random Key Genetic Algorithm (BRKGA) to optimize execution decisions for computation graphs, with a graph neural network policy predicting node-specific proposal distribution choices. This allows for quick optimization of similar graphs in seconds, compared to previous methods that took hours per graph. The approach involves using BRKGA to optimize execution decisions for computation graphs by learning a neural network that directs the search effort for a better solution within the same budget. The policy controls the distribution non-uniformity to allocate search effort effectively. The key idea is to optimize scheduling and placement jointly, unlike previous methods that only focused on placement decisions. The choice of BRKGA was made after evaluating various options for the best speed-vs-quality trade-off. The approach involves using BRKGA to optimize execution decisions for computation graphs by learning a neural network that directs the search effort for a better solution within the same budget. The policy controls the distribution non-uniformity to allocate search effort effectively. REGAL, \"Reinforced Genetic Algorithm Learning,\" uses a graph neural network to predict node-specific proposal distributions for BRKGA's inner loop, improving solution quality without explicit supervision. Our approach, \"Reinforced Genetic Algorithm Learning\" (REGAL), utilizes a neural network to optimize execution decisions for computation graphs, improving solution quality within the same budget. The cost model used is computationally cheap and suitable for distributed training of RL policies. The focus is on learning to optimize this cost model, which corresponds to classical NP-hard scheduling problems. Our approach, \"Reinforced Genetic Algorithm Learning\" (REGAL), focuses on optimizing a cost model using a neural network to predict proposal distributions for execution decisions in computation graphs. This approach allows for learning a more abstract policy that generalizes better to new graphs and incrementally improves proposal choices. REGAL demonstrates learning a policy for optimizing placement and scheduling in TensorFlow graphs, outperforming baseline algorithms on minimizing runtime and memory usage. Using a graph neural network, REGAL predicts mutant sampling distributions of a genetic algorithm for input graphs. The text discusses the use of learning to optimize computation graphs in the context of genetic algorithms. It compares different optimization algorithms and highlights the potential for improvement in objective value through learning. Other works using learning for optimization in different domains are also mentioned. Our work applies learning for combinatorial optimization in the context of task scheduling on large-scale TensorFlow graphs. Unlike previous works focusing on problems like Minimum Vertex Cover or Maximum Clique, we deal with a more complex objective function at the node-level. Additionally, we differ from other studies by using real-world graphs instead of synthetic ones. Bunel et al. (2017) also utilize learning to predict proposal distributions for stochastic search in the STOKE optimizer. Our work focuses on learning for combinatorial optimization in task scheduling on large-scale TensorFlow graphs, using real-world graphs instead of synthetic ones. Previous works have used handcrafted features or neural networks for similar tasks, but our approach differs by automatically learning instance representations and dealing with a more complex objective function at the node-level. Our approach focuses on optimizing task scheduling on large-scale TensorFlow graphs by combining data, model, and attribute parallelism. A graph neural network predicts beta distribution choices at each node, which are then used by BRKGA to optimize placement and scheduling decisions. The performance model, BRKGA, and learning component are explained, with a focus on finding an optimal solution in a computation graph setting. The text discusses the problem of finding an assignment of operations to devices and a schedule to minimize peak memory usage or runtime. The model does not consider certain factors but still presents challenging problems. The Biased random-key genetic algorithm (BRKGA) is used to solve these NP-hard problems. The BRKGA is a meta-heuristic framework used for solving hard combinatorial optimization problems. Chromosomes in a population are encoded as n-dimensional vectors with entries in [0, 1]. The variant includes a fitness evaluation function, integer parameters for population size, elites, and children, an elite bias, and a mutant generation distribution. The goal is to find a chromosome that maximizes the fitness function. The BRKGA is a meta-heuristic framework for solving combinatorial optimization problems. Chromosomes are encoded as n-dimensional vectors with entries in [0, 1]. Evolution steps involve sorting chromosomes by fitness, selecting parent chromosomes for crossover, and generating new chromosomes from elites, nonelites, and a distribution. The crossover procedure combines entries from parent chromosomes to produce a child chromosome. The BRKGA is a meta-heuristic framework for solving combinatorial optimization problems. Chromosomes are encoded as n-dimensional vectors with entries in [0, 1]. Evolution steps involve sorting chromosomes by fitness, selecting parent chromosomes for crossover, and generating new chromosomes from elites, nonelites, and a distribution. The crossover procedure combines entries from parent chromosomes to produce a child chromosome by independently combining entries from the parents using a mutant-sampling distribution. The BRKGA framework encodes chromosomes as n-dimensional vectors for optimization. Priorities for transfers are determined by maximum affinity, and a schedule is obtained through a topological sort based on tensor dependencies. A performance model evaluates peak memory and runtime, with fitness encoded to prioritize memory-feasible schedules. A contextual bandit policy predicts choices for nodes in a computation graph, optimizing it using BRKGA. The policy p(a|G) makes decisions at each node in a graph G, forming an action a. This is akin to a combinatorial multi-armed bandit problem. The action specifies beta distributions for optimization, with mean and variance parameters quantized for discrete choices. BRKGA is then run with these distribution choices, and the final objective value is used to compute the reward. The reward values are computed based on the final objective value achieved on a graph G with action a, divided by the objective value achieved by standard BRKGA. The reward is defined as r(a, G) = \u2212 oa(G) os (G), where a reward > \u22121 indicates a better objective value than standard BRKGA. Learning is done using REINFORCE with a scalar baseline for gradient estimate variance reduction. Multigraphs are derived from computation graphs, with nodes corresponding to ops and directed edges denoted as G = (V, E). In our setup, nodes correspond to ops and edges represent tensor dependencies. Each node and edge has attribute vectors containing features like tensor sizes. A model predicts mutant sampling distributions for BRKGA using this multigraph. Nodes have independent beta distributions for device affinities and scheduling priorities. Graph Neural Networks (GNNs) are used to learn representations for actions in reinforcement learning. Graph Neural Networks (GNNs) are utilized to learn representations for computation graphs by computing node representation vectors through iterative message passing processes. Multilayer perceptrons (MLPs) encode node and edge attributes, while messages are computed along edges to gather information from the neighborhood. Independent predictions are made for each node, with a shared MLP for output distribution parameters. Experiments involve quantizing beta distribution parameters and using a discrete action space. The baseline for the experiments involves using a separate GNN to compute node representations and aggregate them across nodes. Two tasks are considered: minimizing peak memory and minimizing running time on two devices with 16 GiB of memory each. Separate neural networks are trained for each task-dataset pair on a dataset of 372 real-world TensorFlow graphs. The dataset consists of production and research use cases, split into {train, valid, test} sets with varying percentages of graphs. The dataset is augmented with noise to create different variants per graph, representing different optimization instances. Separate datasets are created for minimizing runtime and peak memory, with different numbers of training, validation, and test graphs. Additionally, a synthetic dataset of computation graphs with a specific number of cases has been released for reproducibility. The curr_chunk discusses different methods for optimizing running time without memory constraints on a dataset generated from random graph models. It includes Graph Partitioning + Depth First Search (GP+DFS), Local Search, and Graph-As-Sequence Model (GAS) approaches. These methods aim to minimize communication across devices and improve schedules for model parallelism. The GAS approach converts the graph into a sequence using a topological sort and applies a recurrent neural network to predict node-level distributions for BRKGA. Comparisons are made with default BRKGA performance, tuned BRKGA with customized hyperparameters, and Instance-dependent Random Search (IDRS) replacing BRKGA with random search. The GAS approach converts the graph into a sequence using a topological sort and applies a recurrent neural network to predict node-level distributions for BRKGA. Comparisons are made with default BRKGA performance, tuned BRKGA with customized hyperparameters, and Instance-dependent Random Search (IDRS) replacing BRKGA with random search. Additionally, a Constraint Programming (CP) approach with the CP-SAT solver of Google ORtools is used to establish a provably global optimum for each computation graph optimization problem instance. The number of performance model evaluations allowed per graph is fixed to 5,000 in the experiments. Two approaches for training a graph neural network to predict placement and scheduling solutions directly have been explored. The GAS approach uses a recurrent neural network to predict node-level distributions for BRKGA. Training a graph neural network to predict solutions directly without BRKGA was attempted, achieving the best accuracy by predicting the solution autoregressively. RL was also used to optimize the objective value by incrementally predicting the solution and improving it with a learned local search policy. However, both approaches were found to be orders of magnitude slower than REGAL at test time and did not improve on BRKGA5K. REGAL improves on BRKGA5K by enhancing scalability and generalization. Two metrics are used for comparison: 1) Average percent improvement over BRKGA 5K, and 2) Average percent gap from the best-known solution. Results are compared to other algorithms on TensorFlow test sets and a synthetic dataset in Table 1. REGAL outperforms all baselines on TensorFlow test sets and a synthetic dataset, showing significant improvements in runtime and peak memory minimization tasks. It reduces the average percent gap from the best-known solution by about 1.8\u00d7 with respect to BRKGA 5K on TensorFlow test sets and by about 6\u00d7 and 3.3\u00d7 with respect to GP + DFS on TensorFlow Runtime and Peak Memory test sets, respectively. The learned policy successfully generalizes to unseen graphs, capturing a large fraction of the estimated room for improvement over BRKGA 5K. REGAL achieves 0.58% average runtime improvement over BRKGA 5K on XLA graphs and 3.74% average peak memory improvement. Despite training only on TensorFlow graphs, REGAL shows promising results, hinting at the potential for even bigger improvements by training directly on XLA graphs. The optimizer running times for REGAL and BRKGA 5K are 1.04 seconds and 0.89 seconds, respectively, on the TensorFlow Peak Memory test set. REGAL achieves significant improvements in runtime and peak memory optimization compared to BRKGA 5K. It outperforms BRKGA on the majority of test sets, with the highest improvement being 26.0% for runtime and 54.3% for peak memory. The worst regression is 24.0% for runtime and 17.9% for peak memory. REGAL's performance improves with more evaluations, confirming that the policy generalizes to higher evaluation limits. Even with 50,000 evaluations, BRKGA is not able to match REGAL's performance with just 5,000 evaluations. The RL agents' actions are instance dependent, with the best agent on the TF Runtime dataset having a choice of distributions. The agent's performance on the TF Runtime dataset improves with more graph message passing iterations. Utilizing a GNN with message passing leads to higher performance compared to ignoring the graph structure. REGAL successfully generalizes to new graphs by training a graph neural network policy to predict graph-conditional node-level distributions for BRKGA. REGAL's speed and generalization make it a strong choice for production compilers handling diverse graphs under time constraints. Integrating REGAL into a neural network compiler could improve placement and scheduling decisions. Enhancements like a Mixture of Experts architecture and alternative methods to BRKGA could further boost REGAL's performance. The dataset was collected by mining TensorFlow jobs in a production cluster, deduplicating graphs based on topology. No other filtering was applied, making it representative of real-world TensorFlow graphs. The dataset for a broad distribution of TensorFlow graphs was collected from a production cluster without any additional filtering. Computational costs for the graphs were simulated using an in-house simulator, ensuring a similar distribution of nodes across the train-validation-test sets. Each graph in the sets was modified by creating 99 copies and adjusting tensor sizes and operation running times. The dataset includes graphs with simulated computational costs, modified by creating copies with adjusted tensor sizes and operation running times. Graphs with no relevant cost information or room for improvement are filtered, resulting in separate datasets for runtime and peak memory. Another dataset was collected by dumping CostGraphDefs during XLA compilation and extracting control-flow-free subgraphs. Synthetic graphs were also sampled from classic random graph models. The random graph models used in the study include Barab\u00e1si & Albert (1999), WattsStrogatz (1998), and stochastic block model Holland et al. (1983). Parameters for each model are specified, such as edge probability, number of connections, and block probabilities. The graphs are converted into directed acyclic graphs by sampling a random node ordering. After setting edge directions, head and tail nodes without predecessors or successors are identified. Source and sink nodes are created, with edges connecting source to head nodes and tail nodes to sink. TensorFlow graphs have one source and sink node. Synthetic graphs from random models are shown. Each edge represents control or data dependency. Ops have probabilities to produce 0, 1, or 2 output tensors. Control dependencies exist when an op produces 0 tensors. The curr_chunk discusses the process of filling memory costs for tensors, computing time costs for operations, and filtering synthetic graphs to improve runtime. The generated synthetic graphs aim to improve runtime by 20% on average. The curr_chunk discusses improving runtime by 20% through synthetic data in CostGraphDef format. The encoding involves control dependencies as tensors of size zero, with each edge having three features. The input for the graph neural network is a directed graph with multiple edges, and each node in the computation graph is associated with node features. Alternative encodings like a bipartite graph exist as well. The curr_chunk discusses node features in a computation graph, categorized as memory-based, runtime-based, and BRKGA-based. Memory-based features include input and output tensor sizes, internal memory usage, and a one-hot indicator for the op with the greatest memory usage. Runtime-based features include predecessor and successor nodes' running times, op's running time cost, and a one-hot indicator for the op with the greatest runtime cost. BRKGA-based features involve node aggregation of chromosomes found by BRKGA for minimizing peak memory. The text discusses the normalization of node or edge features in a graph related to memory size and runtime. It also mentions fixing the placement of nodes with the highest memory or runtime to the first device to reduce symmetry. Figure 8 shows reward curves for runtime and peak memory minimization on the training set. The graph neural network policy is trained using TensorFlow on a CPU machine with multi-threading. Training takes 2-3 days, with a final average percent improvement over BRKGA5K of 7.25% on the training set for runtime minimization and 4.36% for peak memory minimization. The policy successfully generalizes to unseen graphs at test time. The computation graph is defined by a set of devices and includes ops, tensors, memory usage, and execution time. Solutions to the scheduling problem involve placements of ops on devices and defining a schedule for their execution. Transfer operations move tensors between devices. The schedule execution is modeled by assigning simulation time steps to ops on devices. The simulation time steps for each device involve managing tensors in memory, ensuring availability for operations. Valid schedules require all input tensors to be present on the device at the specified time step. Memory usage is calculated as the sum of memory used by tensors in memory. The peak memory and runtime of a schedule are determined by the maximum memory usage and execution time of operations on devices. Synchronous transfers wait for both sender and receiver to complete. The BRKGA chromosome encoding for a computation graph involves node-device affinities, node scheduling priorities, and tensor placements on devices. The execution time of synchronous transfers depends on the known bandwidth between devices. The BRKGA chromosome encoding involves node-device affinities, scheduling priorities, and tensor placements. Nodes with higher priority are scheduled first, and tensor transfer priorities determine the order of transfers across devices. Each real number in a chromosome is sampled from a Beta distribution parameterized by \u03b1 and \u03b2. In REGAL, the BRKGA chromosome consists of three parts: node placement, scheduling decisions, and tensor transfer priorities. The RL agent optimizes \u03b1 and \u03b2 values for the first two parts, while the tensor transfer priorities are fixed at \u03b1 = \u03b2 = 1. The RL agent proposes (d + 1) \u00d7 2 values for each operation in the graph, quantizing the output space for easier learning. The RL agent quantizes the output space of its actions by mapping them to Beta distributions using a specific strategy. This involves setting quantized mean and variance values for each action, which are then sampled from a Categorical distribution. The same quantization approach is used for BRKGA crossover probabilities, where a dequantized value is calculated based on a sampled integer. The dequantized crossover probability is determined by a constant k and MLPs, which are models that map input vectors to output vectors through layers of linear transformations and nonlinear activation functions. RNNs are good sequence models that contain a recurrent memory updated recursively at each step. The simplest RNN cell at step t has parameters W, b, and a nonlinearity \u03c3. LSTM models use gating to control memory access, distinguishing memory c t and output h t. LSTMs are better at modeling long-term dependencies due to carefully designed memory access control. An autoregressive model can capture structure in outputs. Node representations from GNN can be ordered and fed into an LSTM to predict outputs sequentially. The LSTM model on top of node representations did not perform well compared to the conditionally independent model. Possible reasons include unreliable node ordering, challenges with learning on long sequences, and difficulty training due to noisy signals. The computation graph and memory management under the model are illustrated in Figure 10. Additional information on baselines, such as CP SAT for multi-device memory minimization, is provided. The multi-device peak memory minimization problem is addressed using CP SAT, graph partitioning with DFS, and local search algorithms. CP SAT guarantees globally optimal solutions, graph partitioning minimizes data transfer, and local search involves changing device assignments or operation order. The hyperparameters for optimizing device assignment and op order in the current schedule are set based on grid search results on a sample of 10,000 graphs. Tuned BRKGA involves optimizing parameters through grid search, while REGAL's performance is affected by stochastic actions and randomness in BRKGA. The standard deviation of improvement statistics is below 0.1%, so error bars are omitted from the results. Table 3 displays the average running times of algorithms on the TensorFlow test set and XLA dataset, measured on an Intel Xeon E5-1650 3.60GHz machine. REGAL offers fast running time and high solution quality, slightly slower than BRKGA 5K but with significant improvement in solution quality. The added running time is mainly due to sampling beta distributions. The local search heuristic runs slowly due to inefficient implementation. REGAL can train a policy to generate actions for node placement and scheduling priorities. The best model can generate 16 actions for node placement decisions and 4 actions for node scheduling decisions. The actions determine the shape of Beta distributions for node-device affinities and scheduling priorities. The study compares different subsets of actions and finds that learning actions for both placement and scheduling leads to the best performance. In this section, node placement decisions are categorized into actions favoring device 1, device 2, or equal preference. Node scheduling decisions are divided into high and low priority actions. The average relative memory consumption of nodes with the same actions is analyzed, with memory usage normalized by the largest node's memory usage in the graph. This data is visualized in Figure 11. The average relative memory consumption of nodes influences their scheduling priorities and placement decisions. Nodes with higher memory consumption are assigned lower scheduling priorities. Nodes with the highest memory consumption often have no device preference. Nodes with lower memory consumption tend to prefer device 2, but some also prefer device 1, indicating a more complex node placement strategy. The graph neural network in the strategy is complex, with 32 state sizes for nodes and edges, 16 propagations, and mean pooling aggregation. Training involved 100000 gradient steps with mini-batches of size 4, using Adam optimizer with specific hyperparameters. The memory model had 2 devices. The best agent for runtime TF had a graph neural network with a state size of 32 for nodes and edges, 16 residual graph propagations, and specific hyperparameters for training. The optimizer used was Adam with specific beta values and learning rate. Test graphs showed rewards on the TF Runtime test set by unique graph topology. REGAL outperforms BRKGA with a reward greater than -1. Box plots display percentiles and range of points. Performance by graph architecture is shown in Figure 12, with rewards on TF Runtime test set by graph topology. Dataset is augmented for each unique topology to generate rewards distribution. Variance for a fixed topology is usually small."
}