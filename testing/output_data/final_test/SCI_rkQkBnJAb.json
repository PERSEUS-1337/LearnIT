{
    "title": "rkQkBnJAb",
    "content": "Optimal Transport GAN (OT-GAN) is a variant of generative adversarial nets that minimizes a new metric called mini-batch energy distance. This metric combines optimal transport with an energy distance in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. OT-GAN is stable when trained with large mini-batches and achieves state-of-the-art results on various image generation benchmark problems. Generative modeling is a sub-field of Machine Learning that focuses on learning models to generate images, audio, video, text, and other data for applications like image compression, speech generation, reinforcement learning planning, and representation learning. Generative models, like Generative Adversarial Nets (GANs), are crucial in artificial intelligence development as they can be trained on unlabeled data. GANs use a neural network discriminator to match the distribution of generated data with training data. Optimal Transport theory offers another method to measure this distribution distance. Optimal transport theory provides a way to measure the distribution distance between generated and training data. It offers an alternative method for defining metrics over probability distributions and training generative models. The primal formulation of optimal transport allows for closed form solutions, making it easier to define tractable training objectives. However, using primal form optimal transport with mini-batches may lead to biased gradients and inconsistency in statistical estimation. OT-GAN is introduced as a variant of generative adversarial nets that incorporates primal form optimal transport. In this paper, OT-GAN is introduced as a variant of generative adversarial nets that incorporates primal form optimal transport into its critic. A new metric called Mini-batch Energy Distance is defined, combining optimal transport with an energy distance in an adversarially learned feature space. This results in a highly discriminative metric with unbiased mini-batch gradients. The paper presents the preliminaries, theoretical contributions, and state-of-the-art results in learning generative models. The conclusion discusses the strengths, weaknesses, and future directions of the proposed method. Generative adversarial nets (GANs) involve a generator and discriminator playing a zero-sum game to produce simulated images. The discriminator distinguishes between real and generated images, while the generator aims to fool the discriminator. Training aims to find a Nash equilibrium where the generator minimizes its loss. Arjovsky et al. re-interpret GANs within the framework of optimal transport. Arjovsky et al. propose the Earth-Mover distance as a suitable objective for generative modeling in GANs. This distance measures the minimum \"mass\" needed to transform the generator distribution into the data distribution, acting as a metric where the distance is zero only when the distributions are equal. Minimizing the Earth-Mover distance provides a statistically consistent estimator of the data distribution. The Earth-Mover distance is proposed as an objective for generative modeling in GANs, providing a statistically consistent estimator of the data distribution. The dual formulation of the optimal transport problem involves maximizing over a set of 1-Lipschitz functions, which can be approximated using neural network GAN discriminators. BID11 suggests a method of bounding gradients in allowed critics, with strong empirical results supporting this interpretation of GANs. The success of GANs is supported by strong empirical results, but they can only solve the optimal transport problem approximately. The connection between GANs and optimal transport is explored further by different studies, with some focusing on different optimal transport costs and model classes. Another approach to generative modeling involves approximating the primal formulation of optimal transport using the Sinkhorn distance. The set of allowed joint distribution \u03a0 \u03b2 is restricted to distributions with entropy of at least a constant \u03b2. BID7 approximates this distance by evaluating it on mini-batches of data X, Y consisting of K data vectors x, y. The cost function c gives rise to a K \u00d7K transport cost matrix C, where C i,j = c(x i , y j ). The coupling distribution \u03b3 is replaced by a K \u00d7K matrix M of soft matchings between elements i, j, with all positive entries and rows and columns summing to one. The resulting distance, evaluated on a minibatch, can be efficiently found using the Sinkhorn algorithm on the GPU. BID7 introduces the Sinkhorn AutoDiff method for generative modeling, utilizing Equation FORMULA4. This approach offers a fully tractable mini-batch Sinkhorn distance, addressing GAN instabilities. However, the expectation of Equation 5 over mini-batches may not be a valid metric for probability distributions. BID1 suggests using the Energy Distance (Cramer Distance) for generative modeling, training the generator by minimizing this distance metric. In Cramer GAN, the generator is trained by minimizing a distance metric in a learned latent space. A new metric for generative modeling is proposed, combining insights from GANs and optimal transport. Previous work in generative modeling involves minimizing a distance between generator and data distributions, typically defined over mini-batches of images rather than individual images. The Mini-batch GAN implements a high dimensional function G(Z) to generate images from random noise Z. It introduces the Mini-batch Energy Distance, combining optimal transport and energy distance for a discriminative distance function with unbiased gradients. This new distance function generalizes the energy distance to non-Euclidean distance functions. The Mini-batch GAN introduces the Mini-batch Energy Distance, which generalizes the energy distance to non-Euclidean distance functions. The energy distance is a metric as long as the distance function satisfies certain conditions. This distance function can be viewed as a form of maximum mean discrepancy, with the MMD kernel related to the distance function. The Mini-batch GAN introduces the Minibatch Energy Distance, a valid metric between individual mini-batches, derived from the entropy-regularized Wasserstein distance. This distance function is used inside the generalized energy distance for generative modeling. The Mini-batch GAN introduces the Mini-batch Energy Distance, derived from the entropy-regularized Wasserstein distance. This distance incorporates the primal form optimal transport of the Sinkhorn distance, leading to stronger discriminative power and more stable generative modeling. The transport cost function is learned adversarially and best results are obtained using two independently sampled minibatches from each distribution. The transport cost function in generative models is crucial, with Euclidean distance performing poorly in experiments. Adversarially learning the cost function, like using cosine distance between vectors, improves discriminative ability and statistical efficiency in high dimensions. The transport cost function in generative models is optimized by using cosine distance between vectors, improving discriminative ability and statistical efficiency in high dimensions. Training the generative model and the adversarial transport cost involves alternating gradient descent, with a focus on updating the generator more frequently to prevent the cost function from becoming degenerate. This approach differs from standard GANs, where the generator controls the critic. The generator in our model has a well-defined training objective even when the critic is not updated, as long as the cost function is not degenerate. We use the Sinkhorn algorithm to compute the matching matrix M in W c, without backpropagating through it. Our training procedure is outlined in Algorithm 1 and depicted in FIG1. Algorithm 1 outlines the training procedure for generative models, allowing for the use of different optimizers. Conditional generation of images with side information is also possible. Algorithm 2 in the appendix provides details for conditional generation. The proposed method OT-GAN demonstrates improved stability and consistency compared to regular GAN on various datasets. It ensures statistical consistency for training the generator, even with a fixed transport cost. The theoretical property of never diverging is tested on a 2D mixture of 8 Gaussians in a toy example. The proposed method OT-GAN uses simple MLPs with ReLU activations to recover all 8 modes in a circular arrangement. Mode collapse is avoided by using mini-batch energy distance, ensuring the generator covers all 8 modes of the data. The training results of OT-GAN using mini-batch energy distance show consistency in mode coverage compared to the original GAN loss (DAN-S). CIFAR-10 dataset is used to evaluate OT-GAN's design decisions and visual quality of generated samples. The model is trained unsupervised and compared using the \"inception score\" metric. The generator and critic are standard convnets without batch normalization or other stabilizing additions. In this work, the effect of batch size on training stability and sample quality is investigated. Larger batch sizes lead to more stable training and higher inception scores for generated samples. The minibatch energy distance objective used depends on the chosen minibatch size, with larger minibatches covering more modes of the data distribution. Multi GPU training is utilized to reach optimal performance, with up to 8 GPUs used per experiment. Samples generated by the model are presented in Figure 4. The model trained with a batch size of 8000 outperforms other GAN models, achieving a score of 8.47 \u00b1 .12. Using optimal transport in OT-GAN is crucial, as random matching of samples reduces the Inception score to 4.64 compared to 8.47 with optimal transport. The training process achieves a score of 4.64 with this approach, compared to 8.47 with optimal transport. OT-GAN generates high-quality images on complex datasets like the dog subset of ImageNet BID20, outperforming DCGAN with an Inception score of 8.97\u00b10.09. OT-GAN outperforms DCGAN in generating high-quality images on complex datasets like the dog subset of ImageNet. It achieves the best inception score with a batch size of 2048 and introduces a new distance metric called mini-batch energy distance. OT-GAN combines optimal transport in primal form with an energy distance in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. It achieves state-of-the-art results on common benchmarks but requires large computation and memory. Future work aims to improve computational efficiency and scale up to multi-machine training for high-resolution image datasets. OT-GAN is a unique model that combines optimal transport with energy distance in an adversarial feature space. It updates the generative model more frequently than the critic, resulting in a stable transport cost function and image embedding function. Preliminary experiments show potential for unsupervised learning and other applications. OT-GAN combines optimal transport with energy distance in an adversarial feature space. The generator and critic are implemented as convolutional networks based on DCGAN with modifications. Weight normalization and data-dependent initialization are used. The generator maps latent codes to color images using upsampling and convolution operations. The critic and generator in OT-GAN do not use activation normalization techniques like batch or layer normalization. The model is trained using Adam with specific hyperparameters. The Sinkhorn algorithm in OT-GAN has two additional hyperparameters for tuning. An experiment on CIFAR-10 dataset shows the importance of learning the transport cost function adversarially. During training, OT-GAN uses a fixed distance function for the transport cost. Inception score reaches 4.93, lower than 8.47 with adversarial cost function learning. DCGAN shows mode collapse after 900 epochs on Imagenet dog data, while OT-GAN maintains sample diversity for 13000 epochs without collapse. OT-GAN maintains sample diversity for 13000 epochs without mode collapse, unlike DCGAN which shows mode collapse after 900 epochs on Imagenet dog data."
}