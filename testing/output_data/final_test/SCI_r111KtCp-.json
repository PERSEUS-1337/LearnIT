{
    "title": "r111KtCp-",
    "content": "In a carefully controlled study, we investigate how autoencoders encode and decode a simple geometric shape, the disk. The optimal solution to the training minimization problem is described, showing that the autoencoder approximates this solution during training. However, a limitation is found in the autoencoder's generalization capacity, specifically its inability to interpolate data. Various regularization schemes are explored to address this issue, providing insights into the generative capacity of neural networks. This research on simple geometric cases can offer valuable experimental setups for more complex architectures. Autoencoders are neural networks, often convolutional neural networks, that compress input data into a latent space and then reconstruct it back to the original space. The goal is to reveal important data structure and represent it in a more meaningful way. Autoencoders have various applications such as image compression and synthesis, with different variants focusing on learning useful properties of the latent space. However, little is known about the inner workings and mechanisms of autoencoders. The goal of this work is to investigate the inner workings of autoencoders, focusing on the encoding/decoding processes of a simple geometric shape, the disk. This approach allows for a detailed analysis of how the autoencoder functions in compressing and uncompressing the disk to and from a code space of dimensionality 1. The study focuses on the architecture characteristics of the network and the roles of different components, showing that the autoencoder approximates the theoretical solution of the training problem. Limitations in generalization capacity are identified when the training database is incomplete. Various regularisation schemes are analysed, with one particular scheme standing out. The study explores the architecture of autoencoders, which consist of an \"encoder\" and a \"decoder\" network to transform data into a latent space. Regularisation schemes are analyzed to improve generalization, with a focus on sparse autoencoders that aim to minimize active neurons in the network. Sparse autoencoders aim to minimize active neurons in the network by modifying the loss function or directly manipulating the code values. Different approaches include using ReLUs to encourage zeros in the code, specifying a maximum number of non-zero values, or defining a priori distribution of the code. The \"contractive\" autoencoder focuses on making the representation of the image robust to small changes in the input. Autoencoders, including denoising autoencoders and image compression, have gained attention for their generative capacity, particularly in CNNs and GANs. Despite limitations like producing low-quality images or mode collapse, research continues to explore their capabilities. This paper focuses on a careful study of autoencoders, with the main goal being to investigate their generative capacity. The autoencoder is described formally with input images denoted as x \u2208 R m\u00d7n and z \u2208 R d, where m and n represent the image's height and width. The autoencoder, consisting of encoder and decoder (E, D), aims to compress and uncompress signals into a smaller representation while minimizing information loss. It involves convolutions, filters, subsampling, biases, and non-linearities to find optimal parameters (\u0398 E, \u0398 D) by minimizing a certain function. The filters' values are referred to as weights, with encoding filters denoted as w ,i and decoding filters as w ,i. The autoencoder uses convolutions, filters, subsampling, biases, and non-linearities to compress and uncompress signals. The encoding and decoding filters are denoted as w ,i and the biases as b ,i. The architecture includes 3x3 convolutional filters and leaky ReLUs with \u03b1 = 0.2. The encoder subsamples until z is a single scalar, with a fixed spatial support and subsampling rate. The number of layers in the encoder and decoder is fixed, and the network architecture is shown in FIG0. The autoencoder architecture includes convolutional filters, subsampling, and non-linearities to compress signals. The network is designed to have a code size of d = 1 for encoding disk images. The training set consists of binary images of disks with random radii. The network can successfully encode and decode the disk images, with the resulting code space being meaningful. The autoencoder represents the disks with their radii in the latent space. The autoencoder can encode disks based on their area, with the code z representing the area. The encoding process involves integrating over the disk's area, leading to a meaningful decoding. This is supported by empirical evidence from experiments. The autoencoder encodes disks using a cascade of linear filters and convolutions. An ablation study revealed that removing biases affects the decoder's ability to spread energy in the output. The biases in the decoder help spread the energy of z in the output according to a certain function g. The decoder without biases was studied in detail, showing that the network finds the right solution. The decoder acts multiplicatively on z, affecting each layer up to the output y. The decoder without biases in the autoencoder acts multiplicatively on z, affecting each layer up to the output y. In experiments, it was found that the decoder chooses a fixed sign for the code, resulting in a linear function that is constant for each radius. This is confirmed in Figure 7 in Appendix A, where the optimization problem of the decoder is expressed. The decoding training problem of the autoencoder without biases has an optimal solution that is radially symmetric and maximizes a specific energy function. The autoencoder is limited to extrapolating only up to the largest observed radius in the training dataset. The optimal solution for the decoding training problem of the autoencoder without biases is radially symmetric and maximizes a specific energy function. The training process has achieved the optimal solution, despite the loss being non-convex. This result is compared with the actual profile learned by the network in Appendix A, showing a very close match. In this section, the study focuses on the generative capacity of autoencoders and GANs, specifically examining the network's ability to generalize simple geometric notions. The analysis involves manipulating the L2 norm of f on circles and disks to increase energy. The network's performance is evaluated by autoencoding examples of disks with varying radii, including those not observed in the database. The behavior of the autoencoder when examples are removed from the training dataset is also investigated. The study focuses on the generative capacity of autoencoders and GANs, specifically examining the network's ability to generalize simple geometric notions. The behavior of the autoencoder when examples are removed from the training dataset is investigated, showing that the network lacks the capacity to extrapolate beyond a certain radius. Removing biases from the network explains why the autoencoder fails to extrapolate when a maximum radius is imposed. The study examines the autoencoder's inability to generalize beyond the training data domain, specifically when removing data points with disk radii between 11 and 18 pixels. The network fails to reconstruct these points in the test data, suggesting limitations in handling unknown regions. Various explanations in deep learning literature are proposed for this phenomenon, such as high data manifold curvature, noisy data, or high data dimensionality. The study explores the limitations of the \"classic\" autoencoder when faced with missing data points, specifically in the context of disks. Even with the use of advanced techniques like \"iGAN\" and \"DCGAN\", the autoencoder struggles to generalize correctly in the presence of data gaps. This behavior raises concerns for applications dealing with high-dimensional data like natural images, which are likely to have similar gaps. The study demonstrates that even with advanced techniques, the autoencoder struggles to generalize when faced with missing data points, such as disks not observed in the training dataset. Different types of regularisation were tested, but the network failed to correctly autoencode the unobserved disks. This highlights a generalisation issue that may be present in more complex networks designed for natural images. The study explores regularisation techniques to improve the generalisation capacity of neural networks when faced with missing data points. Various forms of regularisation, such as imposing distribution in the latent space or encouraging sparsity in the latent space, are considered. However, certain approaches like probabilistic methods or penalisation of weights may not be effective in this context. Regularisation techniques are implemented on the network to improve generalisation capacity. This includes imposing a penalisation on filter weights and enforcing a \"locality-preservation\" property in the latent space. The training aims to minimize the data term and a regularisation term with different forms. Regularisation techniques are applied to improve generalisation capacity in the network. Different types of regularisation are tested, with type 1 not working well due to discontinuities in the data manifold. Minimising the 2 norm of encoder and decoder weights leads to an increase in code z amplitude, hindering convergence. However, regularising encoder weights proves to work well in improving results. Regularising the weights of the encoder improves results, creating a continuous manifold representing the area of the disks. This asymmetrical approach is recommended for autoencoders. The effects of different regularisation methods are consistently observed in various training runs, especially in the controlled setting of autoencoding with disks. This approach may aid in identifying the best components and regularisation schemes for processing complex input data in high-level applications. Autoencoders encode image information optimally by integrating over disks, with the code z representing the disk area. The decoder learns a function multiplied by a scalar without bias, which is then used for thresholding to ensure correct decoding. Limitations arise with missing data points in training, especially for higher-level applications with higher intrinsic dimensionality. The regularisation approach identified can overcome dimensionality issues in autoencoders by asymmetrical regularization of the encoder. Future goals include extending theoretical analyses to complex visual objects and studying decoder functions with biases included to determine autoencoders' capacity for generating complex images. The proposed framework can help identify limitations in complex networks like BID18. Future research should explore if this framework can aid in developing the right regularization scheme or architecture. Experimental results confirm theoretical derivations regarding the output of the autoencoder without biases. Experimental results confirm theoretical derivations regarding the output of the autoencoder for disks. The decoder approximates a disk of radius r with a function y(t; r) = h(r)f (t), where f is a continuous function. Numerical optimization of the energy in Equation (7) is compared using a gradient descent approach with the profile. The autoencoder optimizes energy using gradient descent without biases. A closed form solution is derived for Equation FORMULA10 using Euler-Lagrange equation. The decoder learns a function for disks with radius R=18, extending only to that maximum radius. The IGAN network of Zhu et al. fails to correctly autoencode disks beyond radius 18, as it is not tuned to data beyond that point. The IGAN network by Zhu et al. fails to accurately interpolate disks with missing radii, regardless of the code size used."
}