{
    "title": "S1ghzlHFPS",
    "content": "In a world where events involve various entities, predicting future events from past patterns becomes challenging with more event types. A factorial LSTM architecture is proposed to capture different aspects of the world state using Datalog rules. This approach aims to encode informed independence assumptions into the model for useful inductive biases. The neural Hawkes process, utilizing an LSTM to modulate event rates, shows improved generalization in synthetic and real-world scenarios through factorial designs specified by Datalog programs. Temporal sequence data in applied machine learning involves imputing missing events by fitting generative probability models. Popular models for evenly spaced sequences include hidden Markov models and LSTMs, while for irregularly spaced sequences, the Hawkes process is a good starting point. Various enhancements, including neural variants with LSTMs, have been developed. These models can be described schematically by Figure 1a, where events are assumed to be conditionally independent given the system state. The main idea of the paper is to inject structural zeros into weight matrices of neural networks to remove unnecessary connections. This sparse connectivity approach aims to explain observed data by partitioning the neural state into different node blocks capturing various aspects of the world's state. Different node blocks are used to capture different aspects of the world's state. By zeroing out parts of the weight matrix, interactions between node blocks are restricted. Parameter tying allows for generalization from frequent to rare events of the same type. The challenges of scaling up to real-world domains with millions of event types are acknowledged. Examples include modeling organizational behavior with meetings and emails or supply chains with purchases globally. In an unrestricted model, structural zeroes and parameter tying help avoid overfitting to coincidental patterns in data. Proposed architectures in graphical and causal models explicitly allow specific interactions and forbid others. Examples include Gaussian graphical models and factorial HMMs. The state in a factorial HMM is represented by node blocks encoding tuple elements. The transition matrix is fully block-diagonal. Interactions in the model are domain-specific and depend on structured event objects with participants. The text discusses using Datalog rules to model systematic dependencies in Chicago. Datalog rules can generate possible events and node blocks based on database facts, allowing for structured names and pattern-matching. The contribution is showing how to derive a neural architecture from a database using Datalog. In this paper, the focus is on real-time events where no normalizing constant is needed. Different node blocks can generate independent sequences of timestamped events, forming the observed dataset. Events are represented as k i @t i, where k i is the event type and t i is the time. The probability of an event of type k at any specific instant t is infinitesimal, and events do not occur simultaneously. A baseline model for multivariate point processes is described, where events are emitted stochastically from a distribution based on selected node blocks. The structure of the depend distribution is crucial, with s i defining intensity functions for event types. A neural Hawkes process is used to compute s i and define intensity functions \u03bb k. This inhomogeneous Poisson process generates future events stochastically, with the earliest event e i being selected as the next event. The neural Hawkes process computes intensity functions for event types, with parameters v k and functions f k. The parameters for s i+1 are obtained using matrix U, event embeddings w k, and matrix V. The structure of the depend distribution is crucial in defining intensity functions for events in an inhomogeneous Poisson process. In this paper, the advantage of introducing structural zeroes into parameters v k, U, and V is discussed. Events in the real world involve predicates and entities, which can be decomposed into structured forms like email(alice, bob) and travel(bob, chicago). The state vector h(t) can be partitioned into node blocks associated with entities, such as h mind(alice)(t) for alice and h mind(bob)(t) for bob. Different node blocks can represent various aspects of entities, like mind and body. The text discusses using Datalog, a logic programming language, to describe inductive biases on node blocks determining event intensity and updates. This approach allows querying the database to identify relevant node blocks for each event. In this section, the Datalog interface is explained by introducing keywords one step at a time. The keywords are written in bold typewriter font and color-coded for presentation. The legal node block and event types are specified using keywords like is block and is event. Different variants of event types are shown using examples like email(alice,bob) and travel(bob,chicago). To avoid repetition, facts of the same pattern can be summarized as a rule. In Datalog, facts of the same pattern can be summarized as a rule, where the head and body are separated by :- and variables are denoted by capitalized identifiers. A fact like event(email(alice,bob)) is a rule with no body, making the body vacuously true. To determine legal event types, queries can be made to the database using is event(K)? Unlike facts or rules ending with a period, queries end with a question mark. Database rules and facts about event dependencies on node blocks can be declared using the depend keyword. In Datalog, event dependencies on node blocks can be declared using the depend keyword. Queries to the database for a given event can be made using depend(k,B)? to get the set of all node blocks that instantiate the dependency. The intensity of an event is determined by rules and dependencies, with pooling operations used to extract the maximum dependency among all node blocks for each rule. The intensity of travel is determined by resorts and friends in Chicago, with only the \"best friend\" and \"signature resort\" being significant. Max-pooling is used for this purpose. Modifying depend rules with a third argument allows for semantics-preserving transformations in the Datalog program without altering the neural architecture. Rules can share parameters by specifying the third argument explicitly. Database rules and event dependencies on node blocks can be declared using the affect keyword. The affect keyword is used to declare database rules and event dependencies on node blocks. By querying the database for a given k, we can update node blocks based on affect rules. Each affect rule can be modified to share parameters by specifying a third argument. The update keyword in Datalog programming allows multiple affect rules to share parameters. By specifying how node blocks update each other, we can update a node block with an event. Parameter sharing in depend, affect, and update rules is important for regularization. When defining event types in Datalog programming, embedding vectors are automatically created and used in equations. To prevent overfitting and generalize to unseen data, event types can share embedding vectors by adding an extra argument. This allows for multiple pairs of embedding vectors to be used by an event type. In our approach, we compute final embedding vectors for event types using Datalog rules to define dimensionalities, activation functions, and multi-layer structures. Learning the parameters of the model involves locally maximizing log-likelihood given the sequence over the observation interval. Our software design allows flexibility in defining embedding vectors, but is separate from the machine learning contribution of the paper. The observation interval [0, T] involves querying the database with depend(k,B)? and affect(k,B)? for event computation and updating. Algorithm 1 in Appendix B.1 provides a detailed recipe, including a down-sampling trick for large K. Prediction of the next event's time and type is based on density functions and minimizing expected L2 loss. The prediction of the next event's time and type is based on minimizing expected L2 loss. The integrals can be estimated using samples drawn from a distribution, and a full sequence can be generated by repeatedly feeding events back into the model. The Datalog interface is used to inject inductive biases into the neural Hawkes process on various datasets, comparing it with a modified architecture called structured neural Hawkes process. The structured neural Hawkes process (structured-NHP) model is compared with the plain vanilla NHP on multiple evaluation metrics. The model is implemented in PyTorch and is designed to handle the superposition of real-time sequences, where some event types do not interact. Experimental data is drawn from a superposition of M neural Hawkes processes with four event types. Leveraging the knowledge about the superposition structure is essential for implementing a mixture model. To leverage the superposition structure, one can implement a mixture of neural Hawkes processes or transform a single neural Hawkes process into a superposition model. This involves zeroing out specific elements and setting LSTM parameters accordingly. With a Datalog interface, constructing a superposition process is made easier by writing simple rules. Learning curves were generated by training a structured-NHP and a NHP on different prefixes of the training set, showing the effectiveness of the structured model. The structured model outperforms the neural Hawkes process (NHP) at all training sizes, requiring much less data to achieve close to oracle performance. The improvement is statistically significant with a p-value < 0.01. The structured model only needs 1/16 of the training data compared to NHP to achieve a higher likelihood. This is shown by pair-permutation tests on various datasets, including the Elevator System Dataset. The dataset represents real-world domains where individuals move from one place to another. Each event type is in the form stop(C,F) where C stops at F to pick up or drop off passengers. Inductive biases express that each stop depends on and affects the associated node blocks C and F. The global node block, building, compensates for missing knowledge. The dataset represents real-world domains where individuals move from one place to another, with each event type in the form stop(C,F) where C stops at F to pick up or drop off passengers. The model compensates for missing knowledge and data, with a full Datalog specification provided in Appendix C.2. The EuroEmail Dataset models email communications between members of a European research institute, with rules expressing dependencies between events and their sender and receiver. The dataset includes rules for dependencies between events and their sender and receiver. A Datalog specification in Appendix C.2 is used for experiments. The models are evaluated through learning curves, scatterplots, and prediction tasks, showing that structured-NHP outperforms NHP in terms of data efficiency and prediction accuracy. The structured-NHP model shows higher accuracy in type prediction compared to NHP in Elevator and EuroEmail datasets. While both models perform similarly in time prediction, the structured model excels in event type prediction due to its direct relation to structured information. This highlights the importance of inductive biases in machine learning model design, with graphical models being a prime example. Our work focuses on injecting inductive biases into a neural temporal model, which is useful in various domains such as demand forecasting, personalization, event prediction, and knowledge graph modeling. Incorporating structural knowledge in the architecture design of such models has gained attention in recent years. Our work introduces a Datalog program to specify neural architecture based on a deductive database, allowing for a richer class of knowledge than previous approaches. This method differs from previous work that relied on inductive biases that were easy to specify by hand. Our work introduces a Datalog program to specify neural architecture based on a deductive database, allowing for a richer class of knowledge than previous approaches. We are the first to develop a general interface for a neural event model, with future plans to extend it to trigger assertions and retractions of facts in the database. The model architecture dynamically changes with the facts, influencing actions and expanding events. The system's learned neural state is augmented by boolean propositions, combining learning and expert knowledge. The transition function \u03a8 introduced in section 2.1 updates hidden cells based on current state vectors, following an exponential curve on intervals. Each node block is initialized at 0 and reads a beginning-of-stream event. The system is initialized with node blocks set to 0, reading a special beginning-of-stream event. The initial configuration determines hidden states and intensity functions. The bos event affects all node blocks but does not depend on any. The vanilla neural Hawkes process can be specified using the interface provided. The system is initialized with node blocks set to 0, reading a special beginning-of-stream event. The initial configuration determines hidden states and intensity functions. The bos event affects all node blocks but does not depend on any. The vanilla neural Hawkes process can be specified using the interface provided. The architecture allows users to define embedding models with multilayer structures and activation functions. The architecture allows users to define embedding models with multilayer structures and activation functions. The log-likelihood can be computed using Algorithm 1, with a down sampling trick for unbiased estimates when there are many event types. The architecture enables users to create embedding models with multilayer structures and activation functions. It provides an unbiased estimate of total intensity for event types, with a focus on reducing computational costs. Future work may involve sampling from proposal distributions to further decrease variance. In practice, Datalog queries are used to determine affected nodes and update event intensities, with results memorized for efficiency. In practice, Datalog queries are used to determine affected nodes and update event intensities, with results memorized for efficiency. Problems arise when events change the database, affecting query results and making stored memos inaccurate. Consider using a more flexible query language that updates memos automatically. Algorithm 2 can draw the next event in a sequence, with an upper bound \u03bb * constructed based on event sequences. The dataset statistics in Table 1 display information on event sequences sampled from structured processes, comprising a mixture model of neural Hawkes processes with four event types. Three datasets were created with different M values, and sequences were generated using the thinning algorithm. 2000, 100, and 100 sequences were produced for training, dev, and test sets respectively. In a simulated 5-floor building with 2 elevator cars, event sequences were generated for training, dev, and test sets. The dataset consists of stop events where cars stop at different floors. The simulation was repeated 1200 times to collect around 1200 time-stamped records per sequence. EuroEmail dataset was created using email data from a European research institute, with 986 users and 332334 email communications over 800 days. The dataset is sparse, with most users sending or receiving only a few emails. A subset of 5881 emails from the top 20 active users was used for training, dev, and test sets. Event types are defined as send(S,R) where S and R are organization members, resulting in 400 different event types. In the Elevator domain, there are various blocks, cars, and floors defined in a Datalog specification for experiments. Event types like stop(C,F) are dependent on cars, floors, and the building."
}