{
    "title": "B1g6XnCcKQ",
    "content": "Discovering objects and their attributes is crucial for autonomous agents to operate effectively in human environments. This paper presents an unsupervised approach for learning representations of objects from unlabeled monocular videos. The continuous representations are trained with a metric learning loss to group similar objects together and separate dissimilar ones. These embeddings allow robots to self-supervise in new environments. Performance is evaluated on both synthetic and real datasets. The approach presented in this paper focuses on unsupervised object understanding through metric learning on unlabeled monocular videos. The method demonstrates effectiveness on synthetic and real datasets, showcasing generalization to new objects. Object-Contrastive Networks (OCN) attract embedding nearest neighbors and repulse others to naturally form continuous object representations. This approach enables robotic manipulation tasks like pointing and grasping objects without the need for explicit positive pairs. The learning process in autonomous robotics focuses on unsupervised object understanding through metric learning on unlabeled monocular videos. Object-Contrastive Networks (OCN) attract nearest neighbors and repel others to create meaningful object representations. This autonomous training enables robots to recognize new objects and infer properties without human supervision, leading to more robust and scalable data collection. In this work, an unsupervised method is presented for learning representations that disentangle object attributes using monocular videos captured by a real robot. The system extracts objects from random frames and uses metric learning to generalize representations across objects without additional self-supervisory signals. The Object-Contrastive Network (OCN) embedding allows for reliable identification of object instances based on visual features like color and shape. Objects are also organized based on semantic or functional properties, such as cups being associated with other containers like bowls or vases. Key contributions include an unsupervised algorithm for learning object representations and the ability to contrast similar and dissimilar objects using monocular videos without explicit correspondences. Object discovery in computer vision and robotics involves identifying objects and their attributes without explicit correspondences. Various methods focus on discovering, tracking, and segmenting objects in videos using supervised or unsupervised techniques. The spatio-temporal signal in videos provides additional cues for object identification. In robotics, methods also emphasize object recognition based on 'objectness' rather than specific categories. In robotics, methods exploit depth to identify objects and their properties using convolutional deep neural networks for object detection and pixel-precise segmentations. These methods rely on supervised training and large amounts of labeled data. Self-supervised methods focus on learning representations from multi-view imagery, videos, and domain randomization. Some methods use additional signals like depth or egomotion for self-supervised learning. Unsupervised representation learning focuses on learning representations directly from data using multiple modalities and spatio-temporal coherence to enable tasks like image retrieval, transfer learning, and image denoising. Multiple architectures are explored to compare image patches and exploit temporal coherence for learning object-centric features. BID9, BID29, OCN, and BID52 utilize different approaches for metric learning and object tracking. While some rely on spatial proximity and tracking trajectories, our approach focuses on self-learned embeddings for efficient representation in robotics and real-world scenarios. Additional matching signals could enhance our simpler approach. The curr_chunk introduces a curiosity-driven approach for obtaining a reward signal from visual inputs to optimize learning objectives. It also discusses various methods for tasks such as grasping, manipulation, object detection, and semantic object classification in videos. The proposed unsupervised approach aims to simplify data collection, increase autonomy in robotics, and discover richer object representations. The curr_chunk introduces a method for learning about objects without supervision by leveraging a general objectness model and assuming the same objects are present in most frames of a video sequence. Objects are detected in two frames, embedded in a low-dimensional space, and organized by a metric learning objective. This approach differs from traditional methods by using self-supervised learning instead of human-provided similarity labels. The method introduced leverages a self-supervised approach to mine synthetic similarity labels for object detection using Faster-RCNN. Objects are detected in two stages, with object attributes discovered using OCN. Images are embedded in a low-dimensional space for metric learning. The method utilizes a self-supervised approach to generate synthetic similarity labels for object detection using Faster-RCNN. Objects are detected in two stages, with object attributes identified using OCN. Images are embedded in a low-dimensional space for metric learning, where the n-pair loss function is used to learn embeddings that distinguish between anchor-positive pairs and anchor-negative pairs in a batch. The OCN training objective involves npairs losses over all pairs of frames in an observation sequence. The architecture includes a ResNet50 with additional convolutional layers and a fully connected layer for the final embedding. The network is trained with the n-pairs metric learning loss. Different models share the same architecture but not the weights. The unsupervised model does not require supervision labels. The architecture of the model involves ResNet50 with additional layers for embedding. The model does not need supervision labels and evaluates embeddings using linear and nearest neighbor classifiers. It can differentiate object attributes and learn invariance to scene-specific properties. The network represents object-centric attributes and solves metric loss by matching (anchor, positive)-pairs and distinguishing (anchor, negative)-pairs. The approach relies on good initialization for matching objects across frames, even without ImageNet pretraining. Hypotheses driving convergence include object similarity across viewpoints, limited possible matches in a scene, low-dimensional embedding space for generalization, smoothness in learned embeddings aiding weak supervision signals, and occasional convergence. The effectiveness of OCN embeddings was evaluated using two datasets of real and synthetic objects. Real data consisted of objects arranged in table-top configurations captured from continuous camera trajectories, while synthetic data was generated from renderings of 3D objects in similar configurations. Diverse object configurations were generated using 12 categories from ModelNet BID48, covering around 8k models. The selected categories from ModelNet BID48 cover 8k models out of 12k available. Object models are split 80-20 for training and testing, with further splits for validation. Each model is manually labeled with semantic and functional properties. Scene examples show up to 20 objects with varied sizes and positions to avoid intersections. The object dataset consists of 187 unique object instances across six categories, including 'balls', 'bottles & cans', 'bowls', 'cups & mugs', 'glasses', and 'plates'. The dataset is used to generate 100K scenes for training and 50K scenes each for validation and testing. Each scene has a number of views, with random pairs of views selected for object detection. The dataset is split for training, testing, and validation, with details provided in TAB6. Automated real-world data collection was used for the dataset. The object dataset was collected using a mobile robot with an HD camera, capturing around 130 images per run. A total of 43084 images were used for training the OCN, with 15061 and 16385 images for testing and validation. The OCN is trained on two views of the same scene, ensuring the same objects are present in randomly selected frames. The OCN training involved setting time stamps apart, using npairs regularization with \u03bb = 0.002, constructing a distance matrix based on detected objects, and training on scenes with at least 5 objects in each frame. The training converged after 600k-1.2M iterations, and experiments were conducted on a synthetic dataset to evaluate the OCN embedding for object attribute disentanglement. The experiments showcased the usefulness of OCN on real robotics applications by evaluating unsupervised embeddings with attribute classifiers. Two types of classifiers, linear and nearest-neighbor, were applied on existing embeddings to measure quality. The linear classifier consisted of a single linear layer trained with various learning rates. The nearest-neighbor classifier is trained with a range of learning rates and the best model is retained for each attribute. Nearest-neighbor classification may not measure generalization like linear classification and results can vary based on the number of nearest neighbors. Baselines are compared in TAB7, including the 'Softmax' baseline which has the same architecture as OCN but is trained differently. The 'ResNet50' baseline uses unmodified ResNet50 model outputs for classification. The 'OCN supervised' baseline is the same as OCN training but with provided positive matches. Results show evaluation of unsupervised models against supervised baselines on labeled synthetic datasets. The datasets introduced in Sec. 4 have no overlap in object instances between training and evaluation sets. Unsupervised performance closely follows its supervised baseline when trained with metric learning. The cross-entropy/softmax approach performs best, establishing the error lower bound, while ResNet50 baseline sets the upper-bound results. The dataset is heavily imbalanced for attributes classification errors. Models are initialized and frozen with ImageNet-pretrained weights for the ResNet50 part of the architecture. An OCN embedding organizes objects based on visual and semantic features, allowing for matching instances across different views and time. Nearest neighbor objects can be discovered through the embedding space, enabling the identification of similar objects in a scene. An OCN embedding improves object instance detection across multiple views by finding corresponding objects with descending similarity. Attribute errors and object matching errors are differentiated, and performance is evaluated on a pointing robotic task. The OCN model is evaluated on a pointing robotic task where the robot points to an object similar to the one in front of it on a table. The experiment uses three query objects per category and is repeated three times for each combination, totaling 108 experiments. The model performs well on most categories but struggles with 'cups & mugs'. The OCN model struggles with object classes 'cups & mugs' and 'glasses', often mistaking them for 'bowls'. However, it performs better in the 'container' attribute as these categories refer to the same attribute. The model is evaluated on a grasping task where it successfully identifies and grasps objects with similar shape or color attributes. Training data did not include objects held by hand. The robot experiment involves pointing to the best match for a query object among two sets of target objects. A novel unsupervised representation learning algorithm is introduced to differentiate object attributes like color, shape, and function. The OCN model struggles with object classes 'cups & mugs' and 'glasses', but performs better in the 'container' attribute. Training data did not include objects held by hand. The OCN embedding is learned by contrasting object features from camera trajectories in indoor environments. Object attributes are disentangled to create an embedding space for robotic learning. The embedding is used in a robot experiment to grasp the object closest to a query object, demonstrating successful identification of nearest neighbors. The OCN embedding is learned by contrasting object features from camera trajectories in indoor environments. Object attributes are disentangled to create an embedding space for robotic learning. The robot successfully identifies and grasps objects based on color and shape attributes. An OCN can be efficiently trained from RGB videos obtained from a real robotic agent. The OCN embedding is trained by contrasting object features in indoor environments. Color masks are used to identify objects and associate them with semantic attributes. Models not pretrained with ImageNet still yield reasonable results, indicating the approach does not rely on good initialization. Freezing ResNet50 weights degrades results but still outperforms chance. Random weights can still produce reasonable results, as seen in prior work. The organization of real bowls based on OCN embeddings is shown in the results. A robot experiment involves pointing to the best match for a query object among two sets of target objects. The experiment includes selecting the closest match from the target object lists placed on a table. The experiment involves selecting the best match for a query object from two sets of target objects. Image snapshots show cases where attributes are matched correctly. An OCN embedding organizes objects based on visual and semantic features, with examples of query objects and their nearest neighbors listed."
}