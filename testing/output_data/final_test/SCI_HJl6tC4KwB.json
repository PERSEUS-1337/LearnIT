{
    "title": "HJl6tC4KwB",
    "content": "We propose a novel generative adversarial network, ManiGAN, for visual attributes manipulation using natural language descriptions. Our method includes a co-attention module to combine text and image information, a detail correction module to rectify mismatched attributes, and a new metric for evaluation. Extensive experiments show the effectiveness of our approach in image manipulation and generating high-quality results. Image manipulation involves changing aspects of images from low-level color or texture to high-level semantics, with applications in various fields. Recent advancements in deep learning have enabled automatic image manipulation tasks like inpainting, colorization, style transfer, and translation. However, current methods focus on specific tasks, with limited studies on more general and user-friendly manipulation using natural language descriptions. State-of-the-art methods struggle to generate high-quality images and effectively manipulate given images. The current state-of-the-art methods struggle to generate high-quality images and effectively manipulate given images, particularly on more complicated datasets like COCO. This is mainly due to the failure to precisely correlate words and visual attributes, as well as the lack of fine-grained information at the word-level. The focus is on modifying visual attributes of input images using natural language descriptions, proposing a novel generative adversarial network for this purpose. The proposed method, ManiGAN, aims to manipulate images based on natural language descriptions by introducing a co-attention module, a detail correction module (DCM), and a new metric to improve attribute generation and content reconstruction. The proposed method, ManiGAN, aims to manipulate images based on natural language descriptions by introducing a co-attention module, a detail correction module (DCM), and a new metric to improve attribute generation and content reconstruction. The method demonstrates superior performance compared to existing state-of-the-art methods in generating high-quality images accurately corresponding to text descriptions. Previous studies by Dong et al. (2017) and Nam et al. (2018) also focused on image manipulation using natural language descriptions. Our work introduces a text-adaptive discriminator for specific word-level training feedback to the generator, improving image manipulation based on text descriptions. Previous methods focused on image-to-image translation, while our approach targets image manipulation using cross-domain text descriptions. Text-to-image generation has gained attention for generating photo-realistic images using conditional GANs. The previous methods focused on generating photo-realistic images from text descriptions using conditional GANs. In contrast, the current approach aims to manipulate specific visual attributes of given images using natural language descriptions. The ControlGAN framework is adopted as the basic framework for this purpose. The current approach utilizes the ControlGAN framework to manipulate visual attributes of synthetic images. Two novel components, the co-attention module and detail correction module, are proposed for effective image manipulation. The co-attention module takes hidden features and regional image features as inputs to adjust activation values. The co-attention module adjusts activation values using regional image features and hidden features, incorporating text and image cross-domain information. This approach requires less computational resources and focuses on specific positions, unlike traditional normalization techniques. The co-attention module adjusts activation values using regional image features and hidden features, incorporating text and image cross-domain information. Implementing the co-attention module at all normalization layers fails to produce reasonable images, indicating that normalization techniques may not be suitable for tasks requiring different domain information. An ablation study evaluates the effectiveness of the functions W and b, showing that without W, some visual attributes cannot be perfectly generated, and without b, text-unrelated contents are hard to preserve. The co-attention module adjusts activation values using regional image features and hidden features to incorporate cross-domain information from text and images. The model aims to generate modified images aligned with text descriptions, but may introduce new visual attributes or mismatched contents. A detail correction module (DCM) with a generator and discriminator is proposed to rectify inappropriate attributes and reconstruct text-unrelated contents in input images. The Detail Correction Module (DCM) consists of a generator and discriminator trained alternatively to minimize objective functions. The generator takes inputs from hidden features, word features, and visual features extracted from input images. Different feature extraction methods were tested, with VGG-16 performing the best. The main module and detail correction module are trained separately, with the generator and discriminator in both modules trained alternatively to minimize generator and discriminator loss. The Detail Correction Module (DCM) consists of a generator and discriminator trained alternatively to minimize objective functions. The generator's loss function includes a regularisation term to prevent identity mapping. Various losses such as unconditional adversarial loss, conditional adversarial loss, text-image similarity, words-related visual attributes, and randomness reduction are used to fine-tune image generation. Hyperparameters control the process. The Detail Correction Module (DCM) involves hyperparameters \u03bb 1 , \u03bb 2 , \u03bb 3 , and \u03bb 4 for additional losses. The discriminator objective follows Control-GAN's approach, with conditional adversarial loss determining image realism and semantic similarity with text descriptions. Regularisation term L reg prevents identity mapping in the generated image. The model uses a regularisation term L reg to penalise large perturbations in the generated image, stopping training early at the best trade-off between new visual attributes and text-unrelated content. Training stops based on manipulative precision metric. Experiments compare the model with SISGAN and TAGAN on CUB birds and COCO datasets. The authors provide a detailed description of their network structures, datasets, and training configurations in Appendices A, B, and C. Their model generates high-quality images compared to state-of-the-art methods, evaluated using inceptions score (IS) and L1 pixel difference. The L1 pixel difference helps prevent over-training and false reporting of good reconstructions. The authors propose a new measurement metric, manipulative precision (MP), to address over-training in the model becoming an identity mapping network. MP incorporates text-image similarity and pixel difference, with the highest values on CUB and COCO datasets compared to state-of-the-art approaches. This demonstrates the method's ability to generate text-related visual attributes effectively. The model without the main module achieves the highest IS and lowest L1 pixel difference, but lacks text-image similarity. Our ManiGAN outperforms SISGAN and TAGAN on CUB and COCO datasets, producing high-quality results with accurate manipulation and semantic consistency. Our model outperforms SISGAN and TAGAN on CUB and COCO datasets by effectively manipulating images based on text descriptions. The co-attention module is shown to be effective in synthesizing objects with the same shape and pose. Our model outperforms SISGAN and TAGAN on CUB and COCO datasets by effectively manipulating images based on text descriptions. The co-attention module is shown to be effective in synthesizing objects with the same shape and pose. However, the model fails to reconstruct birds on the CUB bird dataset and struggles with manipulation on the COCO dataset. A variety of bird descriptions are provided, showcasing different attributes such as color and features. Additionally, an ablation study is conducted to validate the effectiveness of the co-attention module. The full model outperforms SISGAN and TAGAN on CUB and COCO datasets by effectively manipulating images based on text descriptions. The co-attention module is effective in synthesizing objects with the same shape and pose. The detail correction module helps in correcting inappropriate attributes and reconstructing text-unrelated contents in the images. The model corrects inappropriate attributes and reconstructs text-unrelated contents in images. Without the main module, image manipulation fails to correlate words with visual attributes, resulting in an identity mapping. The co-attention module analyzes text matching original images provided by users. The ManiGAN model introduces a co-attention module for cooperation between hidden features and image features, enabling semantic manipulation of input images using natural language descriptions. The model offers various methods for image manipulation, including concatenating image and text features without co-attention, replacing co-attention modules with concatenation, and training the DCM or main module separately. The co-attention module in the proposed method enables collaboration between hidden features and image features to reconstruct input images while keeping the synthetic result aligned with text descriptions. The detail correction module rectifies mismatched visual attributes and reconstructs text-unrelated contents in the input image. Experimental results show the effectiveness of the method in image manipulation and generating high-quality results. The ControlGAN framework is used with instance normalization in the generator network, with the co-attention module inserted before other components. The method incorporates a detail correction module before upsampling blocks and image generation networks. Evaluation is done on CUB birds and MS COCO datasets. CUB has 8,855 training images and 2,933 test images, while COCO has 82,783 training images and 40,504 validation images. The datasets are preprocessed based on Zhang et al. (2017). The detail correction module is trained separately from the main module in three stages, each with a generator and discriminator. Training is done simultaneously on three different-scale images 64\u00d764. The main module is trained for 600 epochs on the CUB dataset and 120 epochs on the COCO dataset using the Adam optimizer with specific learning rates and parameters. The detail correction module (DCM) is trained separately in three stages to balance text-related attributes and text-unrelated contents. Training durations are adjusted based on manipulative precision values to achieve a suitable balance between generation and reconstruction. The hyperparameters \u03bb 1 , \u03bb 2 , \u03bb 3 , and \u03bb 4 are set for the CUB and COCO datasets. Visual features are converted to match hidden features, spatial and channel-wise attention are used to generate new hidden features, and a co-attention module is adopted to incorporate visual features. Transformed features are then fed into residual blocks. The transformed visual features are processed through residual blocks and a convolutional layer to generate hidden features. A co-attention module is applied to enhance visual information before generating the output image. The manipulation results are tracked over epochs, showing a trade-off between generating new visual attributes and preserving text-unrelated content. As epochs increase, the generated visual attributes aligned with the text description are gradually erased, making the synthetic image more similar to the input image. The text discusses the comparison between ManiGAN, SISGAN, and TAGAN on bird datasets. Various bird descriptions are provided with color details. Additional results are shown on the CUB bird dataset. The bird descriptions include a red bird with a black beak, a blue bird with a red bill, a white bird with a yellow bill, and a white bird with a red throat."
}