{
    "title": "rkHywl-A-",
    "content": "Reinforcement learning is a powerful framework for decision making, but often requires extensive feature and reward engineering. Deep reinforcement learning eliminates the need for explicit engineering of policy or value features but still needs a manually specified reward function. Inverse reinforcement learning aims to automatically acquire rewards but struggles with large, high-dimensional problems. The proposed AIRL algorithm offers a practical and scalable solution based on adversarial reward learning, capable of recovering robust reward functions for learning policies in varying environments. Deep reinforcement learning has reduced the need for feature engineering but still requires manual reward function specification. Reward engineering remains a challenge due to difficulties in specifying rewards and potential unintended behaviors. Deep RL algorithms are sensitive to factors like reward sparsity and magnitude, affecting performance. Inverse reinforcement learning (IRL) involves inferring an expert's reward function from demonstrations, offering a potential solution to the challenge of reward engineering. However, IRL methods have been less efficient than direct methods like imitation learning, especially when using powerful function approximators such as neural networks. While IRL may be preferred in certain scenarios, it has not been shown to scale to the same complexity of tasks as direct imitation learning. Adversarial IRL methods show promise in tackling complex tasks by adapting training samples for improved learning efficiency. IRL is challenging due to ambiguity in optimal policies and rewards, with shaped rewards potentially hindering optimal behavior in varying environments. Modifications to IRL algorithms are discussed to address these issues. In this paper, adversarial inverse reinforcement learning (AIRL) is proposed as a method to learn rewards that are invariant to changing dynamics. The algorithm allows for simultaneous learning of the reward function and value function, outperforming prior IRL methods on high-dimensional tasks with unknown dynamics. Our method outperforms GAIL and other IRL methods on tasks with variable environments by effectively disentangling expert goals from environment dynamics. Inverse reinforcement learning (IRL) is a form of imitation learning that infers the expert's reward function. Previous IRL approaches include maximum margin and probabilistic methods. Our work operates under the maximum causal IRL framework. Our proposed method in the maximum causal IRL framework connects reward learning to generative model training, resembling algorithms by BID21 and BID5. Unlike GAIL, our algorithm focuses on recovering reward functions rather than the expert's policy, making it more suitable for transfer learning. Our IRL algorithm builds on the adversarial IRL framework proposed by BID5, with the discriminator corresponding to an odds ratio between the policy and exponentiated reward distribution. Previous methods have used boosting and Gaussian processes to learn nonlinear cost functions, but still suffer from the feature engineering problem. Our experiments show that direct implementation of the proposed algorithm is ineffective due to high variance. The proposed algorithm implementation is ineffective due to high variance over entire trajectories. Extending the algorithm to single state-action pairs is straightforward, but a simple form of the discriminator is vulnerable to reward ambiguity, making learning portable reward functions challenging. This limits generalization capability, as learned reward functions are not robust to environment changes and inferring agent intentions is difficult. Overcoming this issue is discussed in Section 5, focusing on achieving generalization within the standard IRL formulation. Our inverse reinforcement learning method is based on the maximum causal entropy IRL framework BID23, which involves an entropy-regularized Markov decision process (MDP) with unknown dynamics, transition distribution, initial state distribution, and reward function. The goal is to find the optimal policy that maximizes the expected entropy-regularized discounted reward. Inverse reinforcement learning (IRL) aims to infer the reward function given demonstrations from an optimal policy. The optimization problem is cast as a GAN optimization, with a trajectory-centric formulation and a learned discriminator function. The text discusses generative adversarial network guided cost learning (GAN-GCL) in the context of inverse reinforcement learning (IRL). GAN-GCL updates the reward function through the discriminator and improves the policy by updating the sampling distribution. It differs from guided cost learning (GCL) and GAIL by placing special structure on the discriminator to recover the optimal reward function. The text discusses a conversion of Eqn. 2 into the single state and action case for efficient imitation learning. However, this approach may not be robust for reward learning due to heavily entangled rewards supervising actions based on the optimal policy. In experiments, the reward may not be robust to changes in environment dynamics, leading to undesired behavior. IRL methods can struggle to learn robust reward functions due to reward shaping and the inability to differentiate between reward transformations that preserve the optimal policy. In experiments, reward functions may not be robust to changes in dynamics. Policy invariance is studied in two MDPs with the same reward but different dynamics. Changing the dynamics can break policy invariance, highlighting the challenge of learning robust reward functions. The optimal Q-function and policy with respect to reward and dynamics are denoted as Q*r,T(s,a) and \u03c0*r,T(a|s) respectively. A reward function is considered disentangled if it remains the same under different dynamics, ensuring the optimal policy is unchanged. The learned reward function should only depend on the current state to avoid unwanted reward shaping. The dynamics must satisfy a decomposability condition where functions over current and next states can be isolated. This condition can be met by adding self transitions at each state to an ergodic MDP. The exact definition and proof are included in the appendix. In the traditional IRL setup, learning a state-only reward function is crucial for recovering the true reward. However, the method presented in Section 4 does not allow for this, leading to potential shaping of learned rewards. To address this, the discriminator in Section 4 can be modified to include a shaping term, which helps mitigate unwanted shaping effects on the reward approximator. The training procedure detailed in Algorithm 1 involves alternating between training a discriminator to classify expert data and updating the policy to confuse the discriminator. By parametrizing the reward approximator as a function of the state only, rewards can be extracted independently from the environment dynamics. This approach allows for disentangled rewards and optimal value function recovery under deterministic environments. In transfer learning scenarios, AIRL aims to learn disentangled rewards that are robust to changes in environment dynamics. The algorithm produces optimal behavior even in test environments with different dynamics, while na\u00efve methods fail. Additionally, in small MDPs, the exact ground truth reward can be recovered. In small MDPs, the exact ground truth reward can be recovered. Comparing AIRL to GAIL and GAN-GCL in imitation learning and transfer setups, AIRL performs similarly to GAIL in traditional setups but outperforms it in transfer learning. It also outperforms GAN-GCL in both settings. AIRL is the only IRL algorithm known to scale to high dimensional tasks with unknown dynamics. For continuous control tasks, trust region policy optimization BID20 is used as the policy optimization algorithm. Expert demonstrations are obtained by training an expert policy on the ground truth reward, while hiding it from the IRL algorithm. The goal is to learn a reward function from demonstrations without manual reward engineering. MaxEnt IRL is first considered in a toy task with randomly generated MDPs. Additional material and code can be found at the provided website. In a toy task with randomly generated MDPs, MaxEnt IRL is used to learn reward functions. The state-only reward function recovers the ground truth reward, while the state-action reward function recovers a shaped advantage function. Transfer learning shows that the state-only reward achieves optimal performance under a new transition matrix, while the state-action reward only marginally improves performance. The learning curve for transfer learning experiments on tabular MDPs shows marginal improvement with state-action rewards compared to state-only rewards. Transfer learning experiments on continuous control tasks involve learning rewards via IRL on training environments and reoptimizing policies on test environments. Results include two IRL algorithms, GAIL policy transfer, and an oracle result with TRPO optimizing the ground truth reward function. In transfer learning experiments, the agent must adapt to changes in the environment. Only AIRL trained with state-only rewards consistently navigates to the goal in a maze with modified walls. Direct policy transfer and state-action IRL methods fail in this scenario. In a second task, a quadrupedal \"ant\" agent is trained to run forwards, but at test time, two front legs are disabled and shrunk, requiring the ant to adapt significantly. AIRL successfully learns reward functions that encourage the ant to adapt its gait when two front legs are disabled and shrunk. GAIL fails to achieve forward movement in this scenario. AIRL demonstrates the ability to learn disentangled rewards for high-dimensional environments with significant domain shifts.GAN-GCL struggles to learn rewards effectively, even in the original task, let alone transferring to a new domain. AIRL successfully learns reward functions for adapting the ant's gait when two front legs are disabled. GAIL fails to achieve forward movement in this scenario. GAN-GCL struggles to learn rewards effectively, even in the original task, let alone transferring to a new domain. The experiments compare AIRL and GAIL, showing negligible performance difference. Both methods achieve close to optimal results, contradicting the belief that IRL algorithms are less efficient. GAN-GCL is ineffective on tasks compared to AIRL and GAIL. AIRL outperforms GAIL on tasks requiring transfer and generalization by recovering disentangled rewards that effectively transfer in the presence of domain shift. This practical and scalable IRL algorithm greatly surpasses prior imitation learning and IRL methods, showing effective reward transfer under domain variation compared to brittle rewards from unmodified IRL methods and GAIL. In small MDPs, AIRL can recover ground-truth rewards up to a constant, matching the objective of solving the maximum causal entropy IRL problem. The method used is similar to BID5, showing the justification of GAN-GCL for the trajectory-centric formulation. The method involves training a separate importance sampling distribution \u00b5(\u03c4) to reduce variance in the policy trajectory distribution. This is done by using a mixture policy \u00b5(a|s) = 1/2 \u03c0(a|s) + 1/2p(a|s), where p(a|s) is a density estimate trained on demonstrations. The gradient is adjusted to adapt the importance sampler and reduce variance, while the cost learning objective is replaced with training a discriminator in AIRL. Training a discriminator to minimize cross-entropy loss between expert demonstrations and generated samples. The gradient of the discriminator objective is derived and matched with the reward function in Eqn. 5. The expression involves a mixture between dataset and policy samples. The policy objective is to maximize the reward function, with the global minimum achieved when the learned policy matches the policy under which demonstrations were collected. The discriminator output is 1/2 for all values, indicating a relationship between the reward function and the policy. The section includes proofs for Theorems 5.1 and 5.2, along with conditions on the policy. The decomposability condition in MDP dynamics allows for the separation of state-dependent and next state-dependent functions, enabling the decomposition of functions f(s) and g(s) from their sum f(s) + g(s). This condition ensures that all states in the MDP are linked with each other, facilitating the decomposition process. The decomposability condition in MDP dynamics allows for separating state-dependent and next state-dependent functions. This leads to the derivation of a(s) = c(s) + const and b(s) = d(s) + const, ensuring learning disentangled rewards. Theorem 5.1 states that if a dynamics model satisfies the decomposability condition and IRL recovers a state-only reward producing an optimal policy, then the reward is disentangled from all dynamics. Conversely, if the reward can depend on actions or states, it can change the optimal policy. In this section, it is proven that AIRL can recover the ground truth reward up to constants if the ground truth is only a function of state r(s) in deterministic environments. The optimal policy involves taking a specific action repeatedly to receive infinite positive reward, but changing the reward structure can lead to a different policy where the agent receives infinite negative reward."
}