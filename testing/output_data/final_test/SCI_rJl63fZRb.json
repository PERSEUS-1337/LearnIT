{
    "title": "rJl63fZRb",
    "content": "Neural programs are accurate policies for algorithmic tasks. Learning from demonstrations for neural networks representing computer programs is challenging due to high accuracy needs, specific data structures, and limited observability. To address this, Parametrized Hierarchical Procedures (PHPs) are proposed as a model for programs. PHPs are sequences of conditional operations that use a program counter and observations to select actions. An algorithm is developed to train PHPs from supervisor demonstrations. The algorithm developed trains Parametrized Hierarchical Procedures (PHPs) from supervisor demonstrations, allowing efficient training of multi-level PHPs. PHPs can accurately learn neural programs from limited annotated and unannotated demonstrations, showing superior generalization compared to baseline methods. This approach leverages the hierarchical structure of procedure invocations in well-designed programs for various AI applications. In this paper, the authors propose learning hierarchical neural programs from a mixture of strongly supervised and weakly supervised data using the Expectation-Gradient method and an explicit program counter. Their approach is inspired by recent work in robot learning and control, aiming to achieve superior generalization compared to baseline methods. In robot learning and control, Imitation Learning (IL) involves an agent learning behavior from supervisor demonstrations. IL approaches struggle in algorithmic domains where the target policy requires precise manipulation of inputs and data structures, like in long-hand addition. Algorithmic domains pose challenges as the agent's policy must be highly accurate to ensure correct outputs, unlike tasks in physical or simulated domains. In algorithmic domains, errors are not tolerated, and precise manipulation of data structures is required. The environment is mostly unobservable directly by the agent, making it challenging to develop accurate policies. Recent methods focus on inferring hierarchical control policies from demonstration data to address these challenges. Parametrized Hierarchical Procedures (PHPs) are introduced as a structured model of algorithmic policies inspired by the options framework and procedural programming paradigm. PHPs consist of a sequence of statements that branch conditionally based on observations to perform operations, invoke sub-procedures, or terminate. Each statement's index serves as a program counter, and conditional branching is implemented by a neural network. This model has the potential to address challenges in algorithmic domains. The PHP model, detailed in Section 4.1, addresses algorithmic challenges by maintaining a call stack and program counters for each PHP. PHPs impose a hierarchical structure on learned policies, allowing higher-level PHPs to invoke lower-level PHPs to solve sub-tasks. The call stack organizes the policy into a hierarchical structure. The PHP model introduces a hierarchical structure for learned policies, allowing higher-level PHPs to invoke lower-level PHPs to solve sub-tasks. It is easier to learn than the NPI model and shows better sample complexity in learning from supervised demonstrations. The Expectation-Gradient algorithm is proposed for efficient learning. The Expectation-Gradient algorithm efficiently trains PHPs from a mix of annotated and unannotated demonstrations. Efficient training of multi-level PHPs on NanoCraft and long-hand addition is demonstrated, achieving improved success rates. Previous works like Recursive NPI, Mixed PHP, Neural GPU, and End-to-End Memory Networks focus on learning neural programs from input-output examples, while our work considers execution traces in addition to examples for training hierarchical policies. The BID32 algorithm learns hierarchical policies from execution traces with a recursive structure for perfect generalization. Neural Program Lattices can learn from traces with limited hierarchy information, addressing complexity through selective averaging of latent variables. This method contrasts with exact gradient computation using dynamic programming for small discrete latent variables in each time step. Our approach is inspired by the structure that has small discrete latent variables in each time step, using neural networks to output programs in a discrete programming language. Various methods for automatic discovery of hierarchical structure have been studied, including action-sequence compression, identifying transitional states, learning from demonstrations, policy gradients, and value-function approximation. The paper extends the Discovery of Deep Options (DDO) algorithm by proposing an E-step that infers a call-stack of procedures and program counters in a deterministic dynamical system where the computer interacts with its environment. The environment is modeled as a Deterministic Partially Observable Markov Decision Process (DET-POMDP BID4) with state, observation, and action spaces. The agent maintains memory of past observations in partially observable environments. It has a parametrized stochastic policy in a parametric family. The policy in a parametric family induces a stochastic process where the agent's memory represents its internal state. In Imitation Learning, Behavior Cloning involves the supervisor generating demonstrations for the agent to minimize loss on its own. In imitation learning, strong supervision involves demonstrations with observable variables and memory states, allowing the agent to imitate actions and memory updates. Weak supervision only provides observable trajectories, making it challenging to maximize likelihood due to the large space of possible memory. The Expectation-Gradient algorithm addresses the challenge of maximizing likelihood in imitation learning by managing memory trajectories through a control policy. The agent's memory maintains a stack of active procedures and their program counters, selecting operations based on termination conditions and program counter increments. The PHPs leverage the call stack and program counters to allow exponentially many memory states to be expressed with a relatively small set of PHPs. Practical limitations include no support for recursive procedures in the training algorithm. The training algorithm for Parametrized Hierarchical Procedures (PHPs) does not support recursive procedures or cycles in the invocation graph. Procedures are layered in levels to restrict actions, with each level only able to invoke procedures in the level directly below it. PHPs are represented by two multi-layer perceptrons (MLPs) for operation and termination statements. The training algorithm for Parametrized Hierarchical Procedures (PHPs) involves using two multi-layer perceptrons (MLPs) for operation and termination statements. The input consists of the observation o and program counter \u03c4, with stochastic and deterministic statements generated during training and testing respectively. Weak supervision presents a challenge due to latent memory states, which is addressed using the Expectation-Gradient (EG) method. The EG method is used to address challenges in training Parametrized Hierarchical Procedures (PHPs). It involves expressing the gradient of the observable log-likelihood as the expected gradient of the full log-likelihood. The algorithm consists of E-step to find the posterior distribution of \u03b6 and G-step to calculate and apply the exact gradient of the observable log-likelihood. The training algorithm involves using two multi-layer perceptrons for operation and termination statements, with weak supervision tackled by the EG method. The stack contains program counters of PHPs, ignoring the root counter for simplicity. Stochastic operations and termination statements of procedures are denoted by \u03b7 and \u03c8. The likelihood of the policy given a demonstration is a product of terms generating the demonstration. The form of the likelihood implies that the gradient decomposes into policy-gradient terms. The likelihood of the policy given a demonstration is a product of terms generating the demonstration, decomposing into policy-gradient terms. Computing its expectation only requires marginal posterior distributions over single-step latent variables. To avoid exponential blow-up, each level of the PHP hierarchy is trained separately. To train levels in a hierarchy, we need to abstract away from elementary actions using trained levels to generate demonstrations. This can be done easily in strongly supervised demonstrations by truncating lower levels, but in weakly supervised demonstrations, an algorithm is needed to replace elementary actions with higher-level operations. To facilitate better separation from higher levels during level-wise training, an algorithm is presented to approximate separation from untrained higher levels. The \"root PHP\" used for training is augmented with an LSTM to approximate the stack memory of the i-levels. Abstraction from lower levels is achieved by rewriting weakly supervised demonstrations to show level-(i+1) operations as elementary. The algorithm approximates separation from untrained higher levels by augmenting the \"root PHP\" with an LSTM to mimic stack memory. Weakly supervised demonstrations are rewritten to show level-(i+1) operations as elementary. Three decoding algorithms are considered for level-pi`1q PHPs, with latent trajectories sampled from the posterior distribution in current experiments. The proposed method is evaluated on NanoCraft and long-hand addition tasks. The NanoCraft domain involves building a rectangular structure in a grid world by placing blocks. The state includes a 6x6 grid, agent's location, building specifications, and observations. Some blocks are pre-placed and must not be moved. The task requires moving around the grid to place blocks accurately. The NanoCraft domain involves building a rectangular structure in a grid world by placing blocks. Observations are provided to MLPs as 5-dimensional feature vectors. Top-level PHP nanocraft executes moves and builds actions. Experiment setup includes training the model on datasets of 16, 32, and 64 demonstrations with strong and weak supervision. Results from the experiments show that 32 strongly supervised demonstrations lead to perfect performance, while 16 such demonstrations combined with weakly supervised ones also achieve high success rates. The comparison between PHP and NPI models using exact gradients reveals PHP's accuracy of 1.0 compared to NPI's 0.724. The experiments show that PHP has higher accuracy than NPI, with a success rate of 0.969 compared to 0.502. The gain of PHP over NPI may be due to the exact gradients used in training the model. The state in the PHP model for addition consists of 4 tapes like a Turing Machine, each representing a number, carries, and output. The read/write heads point to the digits, and observations reveal their values in one-hot encoding. The PHP algorithm uses elementary actions to add columns of digits, write carries, and shift pointers. The PHP model for addition uses elementary actions to add columns of digits, write carries, and shift pointers. The algorithm computes the sum of the column, handles carries, and moves pointers to the next column. Experimental setup involved training the model on execution traces for inputs of length 1 to 10, with different levels of supervision provided.\u03b7 write, \u03b7 carry, and \u03b7 lshift output probability distributions over possible actions and arguments. The model was trained using elementary actions and two hyperparameters were optimized: weight on loss from strongly supervised traces and the use of \u03c4 in \u03c8. The goal was to ensure the model follows the hierarchy specified in the strongly supervised traces and avoid spurious dependencies on \u03c4 in the termination condition. Results from experiments on using \u03c4 for \u03c8 at different hierarchy levels are summarized in Table 2. Models trained with 16 traces per input length achieved 100% accuracy for input lengths of 500, but not for 1000. Empirical results show that models can generalize to length-1000 inputs with 100% accuracy with as few as 3 strongly supervised demonstrations. Early stopping of training was required when the number of strongly supervised demonstrations was less than 10. In this paper, the Parametrized Hierarchical Procedures (PHP) model was introduced for hierarchical representation of neural programs. An Expectation-Gradient algorithm was proposed for training PHPs from a mixture of strongly and weakly supervised demonstrations. The benefits of this approach were demonstrated on two benchmarks, showing how PHPs reduce sample complexity required to train policies with unstructured memory architectures. The PHP model introduced hierarchical representation of neural programs and used an Expectation-Gradient algorithm for training from strongly and weakly supervised demonstrations. Results showed that adding weakly supervised demonstrations improved performance, but caution is needed to prevent optimization process deviations. Future work will explore more complex domains and new benchmarks to challenge the community. Future work will explore the quality of solutions obtained by training from only weakly supervised demonstrations, where only observable trajectories are available, posing a challenge due to the exponential growth of possible memory trajectories. The log-likelihood gradient can be computed precisely and efficiently for maximizing log-likelihood via gradient ascent. The log-likelihood gradient can be computed precisely and efficiently using the Expectation-Gradient method. It involves two-level PHPs and stochastic operation and termination statements. The full likelihood of the policy given an annotated demonstration is calculated based on the top stack frame when an action is selected. The log-likelihood gradient can be efficiently computed using the Expectation-Gradient method, involving two-level PHPs and stochastic operation and termination statements. The likelihood of the policy from an annotated demonstration is calculated based on the top stack frame during action selection, with the gradient decomposing into a sum of gradients for expected gradient computation without needing to represent the entire posterior distribution. The log-likelihood gradient is efficiently computed using the Expectation-Gradient method, involving two-level PHPs and stochastic operation and termination statements. It calculates the likelihood of a trajectory prefix and suffix through forward and backward recursions, allowing for the calculation of marginal posteriors v and w. The forward-backward graph in two-level PHPs has efficient computation for log-likelihood gradients. It involves calculating target likelihood using recursion and marginal posteriors v and w. Level-wise training requires abstraction from lower levels and separation from higher levels. In two-level PHPs, level-wise training involves abstraction from lower levels and separation from higher levels. Weakly supervised demonstrations are rewritten to train the next-higher level i\u00b41, and decoded using latent trajectories sampled from the posterior distribution."
}