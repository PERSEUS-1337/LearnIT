{
    "title": "H1gMCsAqY7",
    "content": "The method presented allows training a single neural network with adjustable widths for instant accuracy-efficiency trade-offs at runtime. The network can dynamically adjust its width based on on-device benchmarks and resource constraints, outperforming individually trained models like MobileNet v1, MobileNet v2, ShuffleNet, and ResNet-50. The approach, called slimmable neural networks, achieves similar or better ImageNet classification accuracy and demonstrates improved performance across various applications. Recently, lightweight networks like BID51 and BID50 have been proposed for applications on mobile phones, augmented reality devices, and autonomous cars. These networks aim to achieve low computational complexities and small memory footprints for quick response times. However, existing networks are not easily re-configurable to adapt across different devices with the same response time budget due to the wide variety of Android devices available. In 2015, there were over 24,000 unique Android devices with varying runtimes for neural networks. High-end phones can achieve higher accuracy with larger models, while low-end phones sacrifice accuracy for reduced latency. Lightweight networks like BID51 and BID50 offer a width multiplier to balance latency and accuracy, but it requires training, benchmarking, and deployment of different models for each device. Maintaining an offline table for model allocation based on time and energy budgets is necessary. Dynamic neural networks have been introduced to allow selective inference paths, with controller modules in BID29 controlling the execution of other modules. While reducing depth cannot decrease memory footprint in inference on mobile devices, early-exits and selective block execution in deep residual networks are proposed to optimize performance. In this work, slimmable neural networks are introduced as a solution to trade off between accuracy and latency on the fly. These networks can switch between different model variants with varying numbers of active channels, allowing for adaptive adjustments in different layers. Slimmable networks offer several advantages compared to other solutions. A single model can be trained, benchmarked, and deployed for different conditions. By adjusting active channels on a target device, a near-optimal trade-off can be achieved. This approach is applicable to various types of convolutions, fully-connected layers, pooling layers, and different tasks like classification, detection, and image restoration. Slimmable networks can be easily deployed on mobile devices with existing runtime libraries, and switching to a new configuration incurs no additional runtime or memory cost. Switchable batch normalization is proposed as a solution to the problem of low testing accuracy in neural networks with multiple switches. It privatizes batch normalization for different switches in a slimmable network, allowing for independent accumulation of feature statistics. This approach addresses the issue of inaccurate statistics in shared Batch Normalization layers and ensures the same representation space through learnable scale and bias parameters. Switchable batch normalization introduces scale and bias parameters for different switches in a slimmable network, ensuring the same representation space. These parameters can act as conditional parameters, with the ability to be merged into moving mean and variance variables after training. Batch normalization layers have negligible size in a model. Conditional normalization, such as feature-wise transformation, is important for integrating different sources of information. BID10, BID53, BID22, and BID16 utilize early exiting prediction branches to reduce computational complexity. Conditional normalization, like batch normalization or layer normalization, is commonly used in tasks such as style transfer, image recognition, and others. When training slimmable neural networks, a naive approach resulted in low testing accuracy. The major issue was different numbers of input channels in a single layer. The naive approach in training slimmable neural networks resulted in inaccurate batch normalization statistics due to different numbers of input channels in a layer. An incremental training approach was then investigated using Mobilenet v2 on ImageNet classification task, where a base model A was trained first and then extended with extra parameters B. This approach proved to be stable in both training and testing. The incremental training approach with base model A extended with extra parameters B resulted in a slight increase in accuracy from 60.3% to 61.0%. However, individually trained MobileNet v2 0.5\u00d7 achieved 65.4% accuracy on the ImageNet validation set. The addition of new connections in the computation graph during the expansion to A+B hindered joint adaptation of weights A and B, leading to a decrease in overall performance. This motivated the development of Switchable Batch Normalization (S-BN) for a slimmable network. Switchable Batch Normalization (S-BN) is used to normalize features in deep neural networks, reducing internal covariate shift. It enables faster and stabler training, encodes conditional information, and is essential for training slimmable networks. S-BN privatizes batch normalization layers for each switch in a slimmable network, solving feature aggregation inconsistency between switches. During testing, moving averaged statistics are used, and S-BN can encode conditional information of width configuration for each switch. Switchable Batch Normalization (S-BN) allows for joint training of all switches at different widths, improving performance. It has minimal extra parameters and runtime overhead, making it efficient for deployment. Batch normalization layers are typically fused into convolution layers for efficient inference in practice. In this section, slimmable networks are evaluated on ImageNet BID5 classification, demonstrating performance with more switches and applying them to various applications. The process involves fusing batch normalization layers into convolution layers for efficient inference, switching batch normalization parameters at different widths, executing sub-networks, computing loss, updating weights, and optimizing. Finally, slimmable networks are applied to different applications, including ImageNet BID5 classification dataset with 1000 classes. The experiment involves using MobileNet v1, MobileNet v2, ShuffleNet, and ResNet-50 models with specific training settings. For ResNet-50, training is done for 100 epochs with a decrease in learning rate at 30, 60, and 90 epochs. Top-1 classification error is evaluated. The experiment involves applying slimmable networks to different applications, including ImageNet BID5 classification dataset with 1000 classes. Training is done for 100 epochs with a decrease in learning rate at 30, 60, and 90 epochs. Top-1 classification error is evaluated on the center 224 \u00d7 224 crop of images in the validation set. Results show stable error rates with switchable batch normalization and comparisons with naive training approach. Top-1 classification error for individual networks and slimmable networks with same width configurations is also presented. Slimmable networks achieve similar performance compared to individually trained networks, despite the added optimization constraints. Joint training of different switches can improve performance, especially for slim switches. The approach is applicable to various network architectures and may benefit from implicit model distillation through weight sharing and joint training. In experiments with both residual and nonresidual networks, slimmable models can be applied to various building blocks of deep neural networks. The number of switches in a slimmable network impacts accuracy and latency trade-offs. Results show that an 8-switch slimmable MobileNet v1 performs similarly to individually trained ones, demonstrating scalability. Slimmable networks are also applied to tasks like bounding-box object detection and instance segmentation. In experiments with both residual and nonresidual networks, slimmable models are applied to tasks of bounding-box object detection, instance segmentation, and keypoints detection using detection frameworks MMDetection and Detectron. Pre-trained ResNet-50 models at different widths are fine-tuned and evaluated. The lateral convolution layers in feature pyramid network BID28 are the same for different pre-trained backbone networks. For individual models, ResNet-50 with different width multipliers are trained on ImageNet and fine-tuned on each task individually. Slimmable models are trained on ImageNet using Algorithm 1, and then fine-tuned on each task. The detection head and lateral convolution layers in feature pyramid network BID28 are shared across different switches in a slimmable network. The slimmable neural networks, with the same network architecture and FLOPs across switches, outperform individually trained models, especially for slim architectures. The performance gain is attributed to implicit model distillation and richer supervision signals. Visualization of top-activated images helps understand the role of channels in different switches. The visualization of top-activated images in different switches of slimmable neural networks reveals that the same channel transitions from recognizing white to yellow colors as the network width increases. Additionally, differences in mean, variance, scale, and bias are observed between shallow and deep layers. In slimmable networks, mean, variance, scale, and bias show similarity in shallow layers but diversity in deep layers. Value discrepancy increases layer by layer, indicating slight variations in learned features. Slimmable networks allow for accuracy-efficiency trade-offs at runtime and switchable batch normalization for robust training. They perform similarly or better than individually trained models on various tasks. Potential applications include unsupervised learning and reinforcement learning, as well as network pruning and model distillation. In model distillation experiments, COCO experiments are conducted using MMDetection framework with ResNet-50. Results are consistent with official models. Keypoint detection task is done on Detectron framework with modified ResNet-50. Code and pretrained models are released for ImageNet classification and COCO detection tasks. Private parameters for Switchable Batch Normalization are introduced for each sub-network. Switchable Batch Normalization introduces private parameters for each sub-network to independently normalize features. After training, these parameters can be merged to improve performance. Ablation study results are presented in TAB8."
}