{
    "title": "r1xX42R5Fm",
    "content": "The conventional approach to solving the recommendation problem involves ranking individual document candidates by prediction scores, but this method fails to optimize the slate as a whole. To address biases caused by page layout and document interdependencies, a paradigm shift towards a direct slate generation framework is proposed. List Conditional Variational Auto-Encoders (ListCVAE) are introduced to learn the joint distribution of documents on the slate conditioned on user responses and generate full slates. List-CVAE outperforms greedy ranking methods in generating full slates based on user responses. Recommender systems in the IT industry use machine learning to generate document recommendations for users based on preferences and content. Candidate generation selects a subset of documents from a pool, and a ranking model like a neural network predicts user engagement to create a slate of top documents. The candidate generation process in recommender systems involves sorting top documents based on prediction scores to create a slate, which helps with scalability and fast inference. However, this approach has drawbacks such as not training the candidate generation and ranking models jointly, leading to biases in the ranking method due to visual presentation and context. In this paper, a paradigm shift is proposed from traditional ranking to a slate generation framework that maximizes user engagement feedback. The optimal slate consists of a list of items that the user ideally likes, such as song tracks or news articles. Positional and contextual biases in visual presentation and interactions between documents are considered. The optimal slate in slate recommendation consists of k ordered articles read by the user. The model used is agnostic to specific user response definitions and directly generates slates, considering biases learned through training. Conditional Variational Auto-Encoders (CVAEs) are applied to model document distributions in a slate based on user response. The proposed model List-CVAE is the first to offer a conditional generative framework for slate recommendation by direct generation. It can generate slates based on learned biases without needing a candidate generator at inference time. The model encodes all documents in a slate with their biases into a latent space, allowing for sampling and direct slate generation based on desired conditioning. The paper introduces an architecture using pretrained document embeddings and a downsampled k-head softmax layer within the List-CVAE model for slate generation. It discusses related work on CVAE-type models and experiments on simulated and real-world datasets. Traditional matrix factorization techniques and autoencoders have been used in recommender systems, along with Boltzmann Machines and VAE models. Several works have utilized Boltzmann Machines and variants of VAE models in Collaborative Filtering for recommender systems. These models focus on individual documents or pairs of documents in the slate and use greedy ordering at inference time. Our model, inspired by JMVAE architecture, takes whole slates as input and directly generates slates without greedy ranking. The curr_chunk discusses different methods for slate generation in Information Retrieval and Reinforcement Learning literature, highlighting the limitations of greedy ranking. It introduces a framework that predicts user engagement for document and position pairs, aiming to optimize whole page layouts. The curr_chunk discusses the formal definition of the slate recommendation problem and the use of Variational Auto-Encoders (VAEs) for training latent-variable models. It introduces the concept of marginalizing latent variables and using a variational posterior density for optimization. In a Conditional VAE (CVAE), the variational loss to minimize is defined with the conditioning vector. The model jointly distributes slates and user responses, with the input vector to the encoder being the concatenation of slates and conditions. The List-CVAE model generates optimal slates by conditioning on user responses. The input vector to the encoder is the concatenation of slates and conditions, with a learned prior distribution for the latent variable. The decoder outputs vectors mapped to real documents through dot product with embeddings, followed by a softmax operation. The model aims to generate an optimal slate by considering ideal user responses. The mapping \u03a6 transforms user response vectors into a conditioning space that encodes the user engagement metric. The decoder models a distribution that is easy to represent, with independent probabilities for each document on the slate. The marginalized posterior can be complex, and the encoder encodes the input slate into the latent space. The encoder learns the joint distribution of documents in a fixed order and encodes contextual biases into the latent variable z. The decoder reproduces the input slate distribution from z with ideal conditioning, considering biases learned during training. The latent space encodes user engagement metrics, and the decoder models independent probabilities for each document on the slate. The List-CVAE model simplifies the prior distribution of z to a fixed Gaussian distribution in R2. Generated output slates with low total responses are pushed towards the edge of the latent space while high response slates cluster towards a growing center area. Documents are embedded into a low dimensional space using a normalized embedding function \u03a8. The List-CVAE model simplifies the prior distribution of z to a fixed Gaussian distribution in R2 for generating output slates. During training, negative documents are downsampled to efficiently scale nearest neighbor search. The model is trained as a CVAE by minimizing reconstruction loss and KLdivergence term. During inference, output slates are generated by sampling z from the learned prior distribution and passing it through the decoder. The List-CVAE model simplifies the prior distribution of z to a fixed Gaussian distribution in R2 for generating output slates. During training, negative documents are downsampled to efficiently scale nearest neighbor search. The model is trained as a CVAE by minimizing reconstruction loss and KLdivergence term. Output slates are generated by sampling z from the learned prior distribution and passing it through the decoder. The decoder generates responses from the learned distribution P\u03b8(s|z, c) by taking arg max over dot-products with the full embedding matrix independently for each position i = 1, . . . , k. Evaluation does not use offline ranking metrics like NDCG or MAP. The evaluation of the List-CVAE model does not use offline ranking metrics like NDCG or MAP. Instead, it focuses on maximizing the expected number of total responses on generated slates by evaluating the expected number of clicks over the distribution of generated slates and clicks on each document. The response model is created by distilling the simulated environment using cross-entropy loss onto a neural network model. It consists of an embedding layer encoding documents into 8-dimensional embeddings, followed by hidden layers and a softmax layer predicting slate responses. The model is used to predict user responses on sampled output slates for evaluation against List-CVAE and other baseline models like Greedy MLP, Pairwise MLP, Position MLP, and Greedy LSTM models. List-CVAE, Greedy MLP, Pairwise MLP, and Position MLP are compared against auto-regressive versions of MLP and LSTM models. List-CVAE generates slates based on maximum probability, while Greedy MLP selects the top scoring documents. Pairwise MLP uses a pairwise ranking loss function. Hyperparameters \u03b1 and \u03b7 are swept along with the shared MLP model structure. Position MLP incorporates slate position as a feature during training. AR Position MLP is similar to Position MLP but considers position biases by setting the position feature to each slate position at inference time. Greedy LSTM and AR LSTM are LSTM models with fully-connected layers before and after the recurrent middle layers. Hyperparameters are tuned for the number of layers and widths. Sequences of documents are used as input during training, while single examples with sequence length 1 are used at inference time. AR LSTM selects documents sequentially based on prediction scores. During inference, documents are selected sequentially by considering the context of previous documents and their positions. Random selection is done uniformly from the training set. Trained document embeddings are used for List-CVAE and baseline models, with fixed hyperparameters for all models. Additional hyperparameter tuning is done for other baseline models. The List-CVAE model outperforms all other ranking baselines after a few training steps on a dataset from the RecSys 2015 YOOCHOOSE Challenge. The dataset consists of 9.2M user purchase sessions with temporal ordering, and the model shows promising performance with a slate size of 10. The List-CVAE model is effective for slates with temporal ordering. Slates of size 5 are created from consecutive clicked products, and user responses are based on purchases. After filtering out rarely bought products, there are 375K slates and 10,000 candidate documents. The user response distribution is shown in FIG9, where 0 represents a click without purchase and 1 denotes a purchase. A medium-scale experiment is conducted with a two-layer response model using the same hyperparameters as before. List-CVAE outperforms baseline models in a new simulation environment with 2 million documents synthesized using Gaussian noise. Negative document examples are downsampled during training, and argmax is used for inference with full dot products. During training, List-CVAE outperforms baselines by learning interactions between documents and positional biases. It achieves close to 5 purchases on average with limited complexity. Generalization to unseen optimal conditions is crucial for List-CVAE's performance. List-CVAE demonstrates strong generalization power by producing close to optimal slates even without seeing any optimal slates or slates with 4 or 5 total purchases. It surpasses greedy baselines' performance within 1000 steps, even when trained on slates with only 0, 1, or 2 total purchases. List-CVAE model can learn mechanisms from sub-optimal slates and generalize to optimal slates, moving away from conventional ranking paradigms. It is the first conditional generative modeling framework for slate recommendation, directly modeling the conditional probability distribution of documents in a slate. The List-CVAE model is a conditional generative modeling framework for slate recommendation that directly models the conditional probability distribution of documents in a slate. It outperforms greedy and auto-regressive baseline models, with good scalability using pretrained document embeddings and a downsampled k-head softmax layer. User features can be added to the conditioning for improved performance. The List-CVAE model incorporates user features into the conditioning for slate recommendation, using a set of 50 different users in the simulation engine. The response of each user on a document is considered during training, with user embeddings and responses concatenated in the conditioning. At inference, the model conditions on user-specific information for each test user. List-CVAE generates slates with higher clicks compared to baseline models, although convergence may take longer."
}