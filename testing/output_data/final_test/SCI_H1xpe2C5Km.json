{
    "title": "H1xpe2C5Km",
    "content": "In this paper, a capsule-based neural network model is proposed for semantic segmentation. The model utilizes part-whole dependencies in capsule layers to derive class label probabilities through a layer-by-layer procedure. This approach, modeled as a traceback pipeline, improves segmentation performance compared to fully convolutional network solutions. Experiments on modified MNIST and neuroimages show significant enhancements in segmentation accuracy. An effective segmentation solution should capture both semantic and location information. The fully convolutional network (FCN) BID19 and its variants BID24, BID21, BID1 are popular for this task, using an encoder-decoder architecture to generate high-level features and reconstruct target labels. FCNs utilize both rich semantics and spatial details from different layers. Originating from convolutional neural networks (CNNs) BID13, BID26. FCNs, like traditional CNNs, lack rotation invariance and explicit part-whole relationships among objects, requiring more data samples or network setups for viewpoint-invariant recognition. Capsule nets offer an alternative approach. Capsule nets, operating on a different paradigm, are built on capsules representing visual entities with activation probabilities and instantiation parameters. Coincidence filtering activates higher-level capsules to establish part-whole relationships, enabling viewpoint-invariant recognition through dynamic routing. Embedding this hierarchy into a segmentation network provides a platform for specifying contextual constraints. Our approach develops a capsule-based semantic segmentation solution, treating capsule nets as probabilistic graphical models to infer part-whole relationships. We introduce a traceback pipeline to derive class memberships for individual pixels, termed Tr-CapsNet. The model analytically derives class labels within each capsule layer, utilizing the graphical properties of capsule nets. The Tr-CapsNet approach utilizes the graphical properties of capsule nets for image segmentation and explicit class recognition simultaneously. The traceback pipeline is designed for various tasks like object localization, detection, and network interpretation. Capsule BID25 BID10 represents visual entities with probability of existence and instantiation parameters. Capsule nets, based on capsules, aim to address limitations of CNNs by using high-dimensional coincidence filtering (HDCF) to detect objects with part-whole relationships. HDCF relies on weights and coupling coefficients to connect capsules in consecutive layers for object recognition. Capsule nets use weight matrices and coupling coefficients to map object parts into clusters for whole objects, similar to CNNs. The coupling coefficients are dynamically determined during inference and ensure viewpoint invariance. Two versions of capsule nets, BID25 and BID10, follow the HDCF principle for object recognition but differ in the setup of the instantiation parameter u and the routing process for estimating coupling coefficients. In BID25, parameter u is a one-dimensional vector representing capsule presence probability. Dynamic routing updates coupling coefficients indirectly through scalar product of votes and parent outputs. Hinton et al. (2018) uses a 4x4 matrix for capsule instantiation parameters and solves routing with a modified Expectation-Maximization algorithm. Capsules in capsule nets are categorized into types like cat, bicycle, or sofa, with each layer associated with specific types. In BID25, capsule layers are associated with specific types like cat, bicycle, or sofa. Each layer has capsules of different types, with assignment probabilities between capsules. Three types of capsule layers are introduced, including the primary capsule layer for processing features from the previous convolution layer. These capsule layers are used as building blocks in Tr-CapsNet. The Tr-CapsNet model utilizes convolutional capsule layers for object classification and semantic segmentation. Capsules in the previous layer are connected to capsules in the class capsule layer, which has one capsule for each predefined class label. The model aims to leverage the viewpoint invariance and network interoperability of capsule nets for improved segmentation performance. The Tr-CapsNet model consists of three components, starting with a feature extraction module using convolution layers. It then includes a capsule & traceback module with capsule layers and a traceback pipeline for segmentation. The traceback pipeline produces class maps for segmentation purposes. The Tr-CapsNet model includes a upsampling module for restoring original resolution in image segmentation. The upsampling can be implemented through various methods like deconvolution, atrous convolution, unpooling, or bilinear interpolation. The goal of image segmentation is to compute the probability of each pixel belonging to a certain class with high accuracy. The traceback pipeline is designed to achieve this purpose by computing the probability of each capsule during the inference procedure. The Tr-CapsNet model utilizes a traceback pipeline to compute the probability of each capsule during the inference procedure. This recursive process traces class labels backward through the capsule layers, similar to Hinton et al.'s approach of interpreting images as parse trees for recognition and segmentation. The Tr-CapsNet model uses a traceback procedure to calculate the probability of each capsule during inference. It involves tracing class labels backward through the capsule layers, similar to Hinton et al.'s method of interpreting images as parse trees for recognition and segmentation. The Tr-CapsNet model utilizes a traceback procedure to calculate capsule probabilities during inference by tracing class labels backward through the capsule layers. The estimation of P(Ck |i) can be written into a recursive equation with respect to the upper layer, assuming each lower-layer capsule only takes capsules in one particular position of the higher layer as its possible parents. The Tr-CapsNet model uses a traceback procedure to estimate capsule probabilities during inference by tracing class labels backward through the capsule layers. This involves computing conditional probabilities for lower layers through a layer-by-layer backward propagation process, starting at layer L-1. The size of the last capsule layer determines the number of capsule layers to be traced along the pipeline. The Tr-CapsNet model uses a traceback procedure to estimate capsule probabilities during inference by tracing class labels backward through the capsule layers. The size of the last capsule layer, termed traceback-size, determines the number of capsule layers to be traced along the pipeline. The computation of conditional probabilities for lower layers in convolutional capsule layers may involve capsules in two or more positions as parents, requiring modifications to the calculation of P(Ck|i). The Tr-CapsNet model utilizes a traceback procedure to estimate capsule probabilities during inference by tracing class labels backward through the capsule layers. The loss function of the model includes a margin loss term for recognition and a pixel-wise softmax cross-entropy for boundary localization in semantic segmentation. The margin loss is crucial for training the model as it drives the initiation and convergence of the capsule layers. The Tr-CapsNet model uses cross-entropy loss for boundary localization in semantic segmentation and margin loss for object recognition. The ratio \u03bb1 \u03bb2 in the total loss equation is a key hyper-parameter. Model performance is evaluated on modified MNIST and Hippocampus datasets. The MNIST dataset contains handwritten digit images with added noise for stability. Ground-truth segmentations are generated using an intensity threshold. Two experiments are conducted on the noise-added MNIST dataset. The second experiment involves assessing models under various occlusion scenarios using brain images and corresponding Hippocampus masks from the ADNI website. The images are T1-weighted whole brain MRIs, aligned and cropped to focus on the right hippocampi. Two-dimensional slices are used as inputs for binary segmentations in the experiments. The experiments involve using brain images and Hippocampus masks from the ADNI website for evaluating models. The Hippocampi have complex 3D shapes with vague boundaries, making accurate delineations challenging. The masks were generated with a 3D surface fitting solution, adding noise to the ground-truth data. Various models like BID29, BID27, and BID5 are used for segmentation tasks. Recently published Hippocampus segmentation works, such as BID27 and BID5, utilized ADNI data. Different datasets, subjects, and ground-truth setups make direct comparisons challenging. Four metrics, including Pixel accuracy, mean class accuracy, and Dice ratio, were used for performance evaluations. An additional metric, number of added holes, was employed in occlusion tests to assess segmentation consistency in terms of topology preservation. The U-Net model with modified architecture and pooling schemes was used for segmentation consistency evaluation in whole objects. Different experiments were conducted with variations in convolution layers, padding, and dropout techniques to reduce overfitting. Various pooling schemes were implemented to explore the effect of receptive field sizes on feature maps. The Tr-CapsNet model utilizes different sizes of label maps in the primary capsule layer, specifically 7\u00d77, 9\u00d79, and 11\u00d711 positions. Three versions of Tr-CapsNet were trained with these variations, named Tr-CapsNet-7, Tr-CapsNet-9, and Tr-CapsNet-11. A traceback pipeline is applied between the class capsule layer and primary capsule layer in the model. In the Hippocampus experiment, Tr-CapsNet models with different label map sizes (7\u00d77, 9\u00d79, 11\u00d711) were used. Traceback-depth 1 and 2 were implemented, with varying numbers of capsule types and dimensions in the primary and convolution layers. In Tr-Capsnet, the ratio of hyper-parameters in the loss function needs to be tuned for different experiments. In the MNIST experiment, the ratio is selected from {1 \u223c 5}, while in the Hippocampus experiment, a larger range of {1 \u223c 20} is used. A large weight on recognition term is presumed to improve part-whole relationships for more accurate segmentations. Dynamic routing algorithm is adopted in all experiments, implemented in TensorFlow and trained with the Adam optimizer. In this experiment, different weight ratios between margin loss and cross-entropy were tried to evaluate the importance of the individual loss components. Results show that Tr-CapsNet outperforms the best U-Net model in all metrics for this dataset. The dimension of the primary capsule layer and weights for loss terms affect the performance of Tr-CapsNet. The dimension of the primary capsule layer and weights for loss terms were found to affect the segmentation results of Tr-CapsNet. When the primary capsule size was set to 4\u00d720 and loss weights to 15:1, the most accurate segmentation results were obtained. Reducing the dimension of the primary layer and decreasing the contribution of margin-loss worsened the model performance. Setting the feature map size to a small number in U-Nets allows for integration of pixel information from a broader range, improving class decision and boundary localization. Through validation, the best setups were identified as Tr-CapsNet-4\u00d720 and U-Net-3\u00d77. Our model, Tr-CapsNet-4\u00d720, outperforms the best U-Net model with an average Dice ratio of 87.25. Tr-CapsNet performs segmentation and recognition simultaneously, providing additional power in dealing with adversarial situations like occlusions. An occlusion test was conducted by training the model on easily confused digits to demonstrate its effectiveness. Tr-CapsNet outperforms U-Net in segmentation and recognition, achieving higher accuracies in an occlusion test with easily confused digits. The model successfully classifies pixels into digit 8 (green) and digit 0 (red), showing precise boundary localization and correct labeling for most pixels. Tr-CapsNet outperforms U-Net in segmentation and recognition, achieving higher accuracies in an occlusion test with easily confused digits. The model successfully classifies pixels into digit 8 (green) and digit 0 (red), showing precise boundary localization and correct labeling for most pixels. Tr-CapsNet generates robust segmentation results attributed to the recognition component and part-whole mechanism in the model. Dilated convolutions are used to expand receptive fields exponentially without losing resolution, and shape priors are integrated for shape-constrained segmentation results. New models like RefineNet, PSPNet, Large Kernel Matters, Clusternet, and Y-Net have integrated shape priors and loss terms into deep learning frameworks. Capsule nets, originating from the idea of capsules in object recognition, have evolved with probability components and viewpoint-invariance. Zemel's model introduced probability components and viewpoint-invariance in a fully-connected neural network. The first capsule-based semantic segmentation model was proposed by BID14, with modifications to reduce parameters and operate on large image sizes. Deconvolutional capsule layers were added to the model. The Tr-CapsNet model, unlike previous capsule-based models, focuses on explicit and interpretable label assignments based on part-whole information. It has a simpler decoder with no trainable parameters and routing needed. The model's analytical derivations make it versatile for various applications like object localization and visualization of heat maps. The Tr-CapsNet model focuses on explicit label assignments based on part-whole information, with a simpler decoder and versatile applications like object localization. Capsule nets assume one instance per entity in an image, limiting performance on multiple instances of the same class data. The limitations of capsule nets in handling multi-instance-same-class cases also restrict the capacity of the segmentation model. Remedies could involve further developing capsule nets, designing traceback solutions to bypass the multi-instance constraint, or implementing preprocessing steps like object detection before segmentation with Tr-CapsNet."
}