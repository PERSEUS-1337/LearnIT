{
    "title": "HkltHkBB37",
    "content": "The text presents a technique to record a speaker in one language and synthesize their voice in other languages they may not know. This has various applications like cross-language communication, language learning, and automatic video dubbing. The approach is referred to as multi-language speaker-conditioned speech synthesis, offering a strong baseline for this problem. The model architecture for multi-language speaker-conditioned speech synthesis uses a shared phonetic representation for all languages, allowing speech synthesis while preserving the original speaker's vocal characteristics. Fine-tuning the model's weights enables extending results to speakers not in the training dataset. The model is trained with multiple speakers to capture variations and separate speech content from speaker identity, enabling biasing the generation process to sound like a specific speaker. Our model builds upon recent developments in neural network based speech synthesis, achieving zero-shot accent transfer by grounding input from different languages to a common neural representation space. The training data consists of audio-transcript pairs, with transcripts translated into IPA equivalents and audio transformed into an intermediate representation. Each speaker in the training dataset speaks only one language, but at synthesis time, any combination of languages can be used. Our model achieves zero-shot accent transfer by generating natural sounding speech in the voice of a speaker and accent matching that of the language used. By applying different learning rates to the encoder, decoder, and speaker embedding, we improve speaker fidelity. The model can generate text in any language with a new speaker's vocal identity after fine-tuning. Experiments were conducted with models trained in bilingual and multilingual settings using various datasets. The model can generate natural sounding speech in any language with a new speaker's vocal identity after fine-tuning, achieving zero-shot accent transfer. It shows robust performance on out-of-sample speakers and uses datasets like TIMIT and DIMEx100."
}