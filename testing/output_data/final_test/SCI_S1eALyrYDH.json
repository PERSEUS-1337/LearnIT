{
    "title": "S1eALyrYDH",
    "content": "In this paper, the E2Efold deep learning model is proposed for RNA secondary structure prediction. E2Efold directly predicts the RNA base-pairing matrix and uses a constrained programming algorithm to enforce constraints. Experimental results show E2Efold outperforms previous methods, with a 29.7% improvement in F1 scores for some cases. RNA is a crucial molecule in cellular processes, consisting of nucleotides with four bases. The primary structure of RNA consists of an ordered sequence of nucleotides with four bases: Adenine (A), Guanine (G), Cytosine (C), and Uracile (U). These bases can form base-pairs to create the secondary structure, which can be represented by a binary matrix. Computational prediction of RNA secondary structure is important for understanding RNA functions and interactions with cellular components, as experimental assays are slow and expensive. Predicting RNA secondary structure is crucial in RNA research and applications like drug design. Existing methods assume energy minimization to determine secondary structure, with the search space being exponentially large. To simplify the minimization process, a nested structure assumption is often made, allowing for pairwisely factorized energy functions and dynamic programming-based approaches. Dynamic programming algorithms are commonly used for RNA structure prediction, but they are limited to nested structures, excluding pseudoknots. Pseudoknots, which make up 1.4% of base-pairs, are present in around 40% of RNAs and play a crucial role in folding into 3D structures. To predict RNA structures with pseudoknots, more computationally intensive energy-based methods are required. Energy-based methods for predicting RNA structures with pseudoknots require computationally intensive algorithms. This paper proposes a feed-forward model to learn the structure directly from data, eliminating the need for a second minimization step. Despite the advantages, designing the architecture for the RNA case remains challenging. Designing the architecture for predicting RNA structures is challenging due to constraints, limited data points, and the need for inductive biases in the network. An end-to-end deep learning approach is proposed to address these challenges. The paper presents E2Efold, an end-to-end deep learning solution for predicting RNA structures. It consists of two networks: Deep Score Network for sequence information representation and Post-Processing Network for constraint enforcement. E2Efold uses an unrolled algorithm to constrain the output space, making it easier to learn with limited data and reducing overfitting. E2Efold strikes a balance between model biases for learning and expressiveness for valid RNA structures. It outperforms state-of-the-art methods in predicting RNA secondary structures, including pseudoknots, with efficient inference time and visually close to true structures. It also shows improvements in F1 score, precision, and recall. The method is generic and can be applied to other constraint-based problems. Our method of learning unrolled algorithms to enforce constraints can be applied to various problems, such as protein folding and natural language understanding. Classical RNA folding methods rely on experimentally-measured thermodynamic parameters and struggle with pseudoknots in RNA structures. While some methods achieve linear runtime, they cannot handle pseudoknots due to the NP-completeness of predicting lowest free energy structures with pseudoknots. Pseudoknots in RNA structures pose challenges for traditional RNA folding algorithms due to their NP-completeness. Heuristic algorithms like HotKnots and Probknots attempt to predict structures with pseudoknots, but their accuracy and efficiency need improvement. Learning-based methods like ContraFold and ContextFold use known RNA structures for energy parameter estimation, resulting in higher prediction accuracies. However, these methods still rely on dynamic programming algorithms for energy minimization. CDPfold, a recent deep learning model, predicts base-pairings using convolutional neural networks but struggles with representing pseudoknotted structures and requires a DP-based post-processing step with high computational complexity. Learning with differentiable algorithms has inspired various works in neural architectures, with models applied to structured prediction problems. OptNet integrates constraints using KKT conditions but has prohibitive complexity for RNA secondary structure problems. Dependency parsing in NLP predicts the relationship between words in a sentence, similar to RNA folding. Deep learning models are used to score word dependencies. In RNA secondary structure prediction, hard constraints dictate pairing limitations. In RNA secondary structure prediction, constraints dictate nucleotide pairings and loop formations. By incorporating these constraints into deep learning models, a more accurate predictive model can be trained with less data. This approach uses an unrolled algorithm as an inductive bias to design the deep architecture, which differs from traditional deep learning architectures used in structured prediction tasks. The E2Efold deep model combines a Deep Score Network with a Post-Processing Network for RNA secondary structure prediction. The Deep Score Network outputs a symmetric matrix indicating nucleotide pairing scores, using a one-hot embedding input. The architecture includes feature maps and multi-layer perceptions. The E2Efold deep model utilizes a Deep Score Network to predict RNA secondary structures by outputting a symmetric matrix of nucleotide pairing scores. This is achieved through the use of position embedding, Transformer Encoders, and 2D Convolution layers. A post-processing step is necessary to ensure the validity of the predicted RNA structure. The E2Efold deep model uses a Deep Score Network to predict RNA secondary structures. A Post-Processing Network is introduced to enforce constraints and optimize results. The post-processing step is formulated as a constrained optimization problem and solved using a parameterized algorithm. The post-processing step in the E2Efold deep model involves formulating a constrained optimization problem to maximize the total score based on predicted scores. To simplify the optimization process, a convex relaxation is applied, allowing for a continuous solution space. Additionally, a nonlinear transformation is defined to further simplify the search space. The post-processing step in the E2Efold deep model involves formulating a constrained optimization problem to maximize the total score based on predicted scores. A transformation is applied to create a non-negative, symmetric matrix that encodes specific constraints. A penalty term is introduced to sparsify the matrix, and the problem is simplified by replacing inequality constraints with nonlinear equalities. The constrained problem is then transformed into an unconstrained problem using a Lagrange multiplier and solved using a proximal gradient algorithm. The post-processing step in the E2Efold deep model involves solving a constrained optimization problem using a proximal gradient algorithm with a multiplier \u03bb. The algorithm alternates between updating variables and applying soft threshold and gradient steps. An approximate solution is obtained after convergence. This step can be coupled with the training of U \u03b8 (x) to improve performance by designing a Post-Processing Network, PP \u03c6, and training them jointly in an end-to-end fashion. The post-processing step in the E2Efold deep model involves solving a constrained optimization problem using a proximal gradient algorithm with a multiplier \u03bb. The algorithm alternates between updating variables and applying soft threshold and gradient steps. An approximate solution is obtained after convergence. This step can be coupled with the training of U \u03b8 (x) to improve performance by designing a Post-Processing Network, PP \u03c6, and training them jointly in an end-to-end fashion. The specific computation graph of PP \u03c6 includes a recurrent cell called PPcell \u03c6, with learnable hyperparameters and a fixed number of iterations for recursive application. The Post-Processing Network PP \u03c6 in the E2Efold deep model includes a smoothed sign function and a clipping step to ensure output stays within a specific range. These modifications make the network tuning-free and differentiable, providing meaningful intermediate outputs. Combined with the deep score network, the final model undergoes end-to-end training on a dataset D containing input-output pairs. The training procedure of E2Efold for RNA secondary structure prediction involves defining a loss function to mimic the negative of F1 score, which is well-defined and differentiable. This F1 loss optimizes predictive performance by automatically balancing the abundance of negative samples over positive samples, unlike other differentiable losses such as 2 and cross-entropy losses. E2Efold for RNA secondary structure prediction uses F1 loss to automatically balance negative and positive samples, optimizing predictive performance. It pre-trains U \u03b8 with logistic regression loss and adds it as a regularization to achieve a 29.7% improvement in F1 score on RNAstralign dataset compared to state-of-the-art methods. E2Efold is compared to six methods on the RNAStralign dataset, showing superior performance in F1 score, Precision, and Recall. The dataset is divided into training, testing, and validation sets for evaluation. Mathews (2019) compared various methods on the RNAStralign dataset. E2Efold outperformed all other methods with at least a 20% improvement in F1 score. It showed higher precision than recall, possibly due to constraints in neural network training. E2Efold consistently performed well across all criteria. E2Efold consistently performs well, including on long sequences with weighted F1 scores. Testing on ArchiveII without re-training shows good results, especially on overlapping RNA types with the RNAStralign dataset. The performance of E2Efold is better than all other methods on RNAStralign. Results without domain sequences are similar to those in Table 3. In terms of inference time, LinearFold is the most efficient, CDPfold is slow, and E2Efold is fast due to its gradient-based algorithm. E2Efold is fast in designing the PostProcessing Network and has similar inference time as LinearFold on GPU. It does not exclude pseudoknots, and the F1 score is computed on sequences containing pseudoknots. RNAstructure is compared for pseudoknot prediction. Visualization of predicted RNA structures is provided in the main text and appendix. E2Efold produces RNA structures similar to ground-truths, especially for long and challenging sequences. Ablation study conducted to assess the necessity of integrating post-processing in E2Efold's performance. The post-processing step (i.e., for solving augmented Lagrangian) is applied after training Deep Score Network U \u03b8. Despite the decent performance of \"U \u03b8 + PP,\" E2Efold still has significant advantages with constraints incorporated into training. E2Efold outperforms other methods in various RNA types, such as 16S rRNA, tRNA, 5S RNA, tmRNA, and telomerase. Future improvements could involve multi-task learning with multiple models for different RNA families and an additional classifier to predict the model for input sequences. The E2Efold DL model for RNA secondary structure prediction shows superior performance in terms of quantitative criteria, running time, and visualization. Further research is needed for RNA types with fewer samples. The approach of incorporating constraints to reduce output space and sample complexity is novel compared to previous works. The idea of unrolling constrained programming and pushing gradients through post-processing can be beneficial for other structured prediction problems. Our proposed approach for RNA secondary structure prediction focuses on enforcing constraints by keeping the original algorithm structure and making hyperparameters learnable. Unlike other works that aim to learn better algorithms, we prioritize constraint incorporation. Additionally, we introduce a differentiable loss function to optimize the F1 score, especially effective when dealing with imbalanced datasets. The proposed approach for RNA secondary structure prediction focuses on enforcing constraints and making hyperparameters learnable. A differentiable loss function is introduced to optimize the F1 score, particularly useful for imbalanced datasets. The maximization step can be written as a minimization, and proximal mapping is defined as a function depending on \u03b1. The proximal gradient step is implemented using Pytorch in the E2Efold Deep Score Network with a hyper-parameter d set as 10 to control model capacity. In the final model, a hyper-parameter d is set as 10 to control model capacity. Transformer encoder layers have 2 heads, feed-forward network dimension is 2048, and dropout rate is 0.1. Position encoding uses 58 base functions to form position feature map, going through a 3-layer neural network to generate final position embedding. Pairwise concatenation is done on input X to create tensor Y. 2D convolution layers change feature map channel from 6d to d to 1, with kernel size set as 1. In the final model, a hyper-parameter d is set as 10 to control model capacity. Transformer encoder layers have 2 heads, feed-forward network dimension is 2048, and dropout rate is 0.1. Position encoding uses 58 base functions to form position feature map, going through a 3-layer neural network to generate final position embedding. Pairwise concatenation is done on input X to create tensor Y. 2D convolution layers gradually change feature map channel size and are followed by batch normalization. ReLU is used as the activation function. In the Post-Processing Network, parameters are initialized and T is set as 20. During training, the score network is pre-trained with binary crossentropy loss and Adam optimizer, using weighted loss for positive samples. The batch size is set to 20 for the Titan Xp card. For the fine-tuning process, binary cross-entropy loss was used for the score network and F1 loss for the PP network, with the final loss being the sum of these two. The batch size was set to 8 due to GPU memory limitations, with model parameters updated every 30 steps. The whole model was fine-tuned for 20 epochs, and data imbalance was addressed by up-sampling small RNA families. The training of the score network in the ablation study followed a similar process, with the unrolled number of iterations set to 0 during fine-tuning. The study used a permutation test on the Maximum Mean Discrepancy estimator to compare data distributions. Results showed that the RNAStralign training set is significantly different from ArchiveII, indicating strong generalization power on ArchiveII. E2Efold demonstrates strong generalization power on ArchiveII, outperforming other methods for long sequences. F1 scores weighted by sequence length show consistent results before and after filtering out subsequences. Weighted sampling based on family size maintains an overall F1 score of 0.83, with per-family results detailed in Table 10."
}