{
    "title": "HJluEeHKwH",
    "content": "The Cross-Entropy Method (CEM) is studied for non-convex optimization of continuous objective functions. A differentiable variant (DCEM) allows differentiation of CEM output with respect to the objective function's parameters, enabling integration into end-to-end learning pipelines. Applications include energy-based structured prediction and non-convex continuous control tasks like the cheetah and walker simulations. This approach combines model-based and model-free reinforcement learning through policy optimization. Recent work highlights how optimization procedures enhance the end-to-end machine learning pipeline. In this paper, the focus is on optimizing an unconstrained, non-convex, and continuous objective function in the machine learning setting. The goal is to make the output of the optimization procedure end-to-end learnable by using (sub-)derivatives. End-to-end learning involves defining a loss function on top of the objective function and taking gradient steps. If the objective function were convex, the gradient analysis and computation would be easier. Analyzing and computing a \"derivative\" through the non-convex arg min is challenging in theory and practice. One approach to address this issue is to approximate the arg min operation with an explicit optimization procedure, commonly done with gradient descent. This adds structure to an otherwise ill-defined problem. The Cross-Entropy Method (CEM) is proposed as an alternative to unrolling gradient descent for approximating derivatives in an unconstrained, non-convex optimization setting. CEM generates samples from the objective function and can be made differentiable using a method called DCEM, allowing it to be integrated into end-to-end learning processes. In a non-convex energy-based learning setting, DCEM is compared to unrolled gradient descent for optimizing over a SPEN. DCEM shows a more reasonable energy surface and is useful for embedding action sequences in a lower-dimensional space for control optimization. Optimization-based modeling integrates specialized operations into machine learning pipelines, using a parameterized arg min operation. This approach allows for the use of standard policy learning for model-based reinforcement learning, in addition to maximum-likelihood fitting. The method involves fine-tuning model-based components with PPO, making the control optimization process less computationally expensive. The text discusses approximating continuous and nonconvex arg min operations using the cross entropy method as an alternative to unrolling gradient descent. This method is presented as a way to differentiate through difficult optimization problems. The text discusses using the cross entropy method (DCEM) as an alternative to unrolling gradient descent for solving high-dimensional optimization problems efficiently by exploiting structural properties in the solution space. Various methods such as random feature embeddings, hand-coded embeddings, and auto-encoder-learned embeddings have been explored in the context of Bayesian optimization. DCEM offers a reasonable way to learn an embedded domain for solving larger optimization problems by directly optimizing over the latent space. The text discusses the advantages of DCEM in optimizing high-dimensional non-convex problems efficiently by learning the latent space directly. Various methods have been explored in optimization, including action space learning and policy distillation. Additionally, work has been done on learning latent state space representations. The text discusses encoding action sequences directly with DCEM, avoiding the need for expensive expert solutions. It aims to surpass expert controller performance and is end-to-end learnable. Another direction in RL and control is combining model-based and model-free methods. In this paper, the authors propose a differentiable model predictive control (MPC) approach that extends beyond cartpole and pendulum tasks to cheetah and walker tasks. They utilize neural network dynamics within the controller and backpropagate a policy loss through its components. The focus is on using the Cross-Entropy Method (CEM) for optimization in a non-convex, deterministic, and continuous objective function parameterized by \u03b8. The Cross-Entropy Method (CEM) is an iterative algorithm used to approximate solutions with samples from parametric sampling distributions. It involves hyper-parameters such as the number of candidate points sampled, elite candidates used for fitting new distributions, and the number of iterations. CEM starts with an initial distribution, generates samples, evaluates the function, and refits the distribution to top samples in each iteration. DCEM minimizes a parameterized objective function by sampling from a distribution and updating the parameters iteratively. The top-k projection of values is computed and used to update the distribution. However, the top-k operation makes it non-differentiable with respect to the parameters, requiring the use of estimators for differentiation. The top-k operation in DCEM is made differentiable using the reparameterization trick. A Limited Multi-Label Projection (LML) layer is used to project points onto the LML polytope, with a proposed temperature-scaled variant for interior projection. Equation (4) represents a convex optimization layer that can be solved accordingly. The binary entropy function is used in a convex optimization layer that can be quickly solved with a GPU-amenable method. The soft and differentiable version of the top-k operation is achieved using the reparameterization trick in DCEM. The temperature-scaled LML layer approaches the hard top-k operation. Energy-based learning for regression and classification estimates the conditional probability P(y|x) of an output y given an input x with a parameterized energy function. Predictions are made by solving an optimization problem. Linear energy functions have been well-studied and are easier to solve and analyze, while non-convex energy functions parameterized by neural networks have also been explored. Non-convex energy functions parameterized by neural networks, such as Structured Prediction Energy Networks (SPENs), are being explored for supervised learning. Gradient descent is used to approximate the energy function, with considerations for the impact of approximations on the energy surface during training. DCEM can be an alternative method to approximate the energy function. DCEM offers advantages over gradient descent in approximating energy surfaces, potentially leading to solutions closer to local minima. It is demonstrated in a synthetic example and applied to continuous control settings for learning latent control spaces. The focus is on controlling discrete-time dynamical systems with continuous state-action spaces. The focus is on controlling discrete-time dynamical systems with continuous state-action spaces by repeatedly solving a control optimization problem to find the optimal sequence of actions that optimize the cost function. The controller induces a policy that solves the optimization problem for controlling dynamical systems with continuous state-action spaces. The Cross-Entropy Method is used for solving the problem with neural network transitions. The Cross-Entropy Method (CEM) is utilized with neural network transitions to solve the control problem by sampling full action sequences and refining them. Hafner et al. (2018) uses CEM with 1000 samples in each iteration for 10 iterations with a horizon length of 12, requiring 120,000 evaluations of transition dynamics. The transition dynamics may use a deep recurrent architecture or an ensemble of models. DCEM is applied in the continuous control setting to learn a latent action space Z with a parameterized decoder mapping back to the space of optimal action sequences. The decoder maps back to the space of optimal action sequences, utilizing a latent space to capture spatial and temporal structure. This approach aims to avoid wasting computational resources on irrelevant control sequences. The decoder utilizes a latent space to capture spatial and temporal structure, avoiding wasting computation on irrelevant control sequences. By learning a latent space of candidate solutions, the optimization problem is transformed into a non-convex problem over Z. DCEM is proposed to solve the optimization problem and optimize the decoder directly for better performance. The DCEM process uses the decoder to generate samples to optimize the trajectory {x t , u t } by pushing down the cost with \u2207 \u03b8 C(\u1e91). Differentiation through transition dynamics and cost functions is necessary to compute \u2207 \u03b8 C(\u1e91) for learning an action embedding, even with ground-truth data. In non-convex cases, DCEM helps find a consistent local minima, as discussed by Antonova et al. (2019) and Wang & Ba (2019) in sect. 2.3. The DCEM process utilizes the decoder to optimize the trajectory by minimizing the cost with gradient information. It enables a differentiable policy class to be defined, allowing fine-tuning of the controller's parameters. The approach may be memory-intensive for realistic problems, but potential solutions include deleting low-influence trajectories. The implementation is done using PyTorch, and the DCEM library will be openly released along with the model-based control code. In this section, the impact of the inner optimizer on the energy surface of a SPEN is explored using a simple unidimensional regression task. The ground-truth data is generated from f(x) = x sin(x) for x \u2208 [0, 2\u03c0]. The model P(y|x) \u221d exp{\u2212E\u03b8(y|x)} is used with a single neural network E\u03b8 to make predictions \u0177 by solving an optimization problem. Gradient steps of the loss function L(\u0177, y) = ||\u0177 - y||^2 are taken to shape the energy landscape. Unrolled gradient descent and DCEM with Gaussian sampling distributions are considered, both trained to take 10 optimizer steps with specific parameters. Both unrolled gradient descent and DCEM with Gaussian sampling distributions are trained for a unidimensional regression task. The energy surface learned by CEM captures local minima around the regression target, while gradient descent quickly steps away from the final position. Discussion and limitations of the models are highlighted, with a focus on the behavior of gradient descent and DCEM in making predictions. Evaluation of final models on cheetah and walker tasks is discussed, with comparisons between CEM and DCEM performance. PPO fine-tuning is mentioned as a method to improve model-based components. In an isolated setting, it is possible to learn an embedded control space using the standard cartpole dynamical system. The learning problem focuses on only learning the embedding without complications from exploration or estimating dynamics. DCEM and algorithm 2 are used to learn a 2-dimensional latent space that maps back to the full control space. In an isolated setting, the focus is on learning an embedded control space using the standard cartpole dynamical system. DCEM and algorithm 2 are utilized to learn a 2-dimensional latent space that maps back to the full control space. For DCEM over the embedded space, 10 iterations with 100 samples in each iteration and 10 elite candidates are used. The performance of an expert CEM controller is recovered with fewer samples, showing more reasonable proposals in the embedded space compared to CEM. Additionally, it is shown that assumptions of known transition dynamics and reward can be relaxed, and a latent control space can be learned on tasks like cheetah.run and walker.walk with frame skips of 4 and 2. The study fine-tunes a learned model on tasks like cheetah.run and walker.walk using the MuJoCo physics engine. The PlaNet model with a restricted state space is used for proprioceptive-based control, maintaining deterministic hidden states and stochastic system observations. Transitions, observations, and rewards are modeled accordingly. The study fine-tunes a learned model on tasks using the MuJoCo physics engine. They use a variant of alg. 2 to learn action embeddings for control with DCEM. The DCEM controller induces a differentiable policy class for selecting actions, which is then optimized using PPO to improve episode rewards. The study implements a DCEM controller in the PyTorch PPO implementation by Kostrikov (2018) to fine-tune the policy for improved episode rewards. Results show that DCEM almost matches CEM performance with significantly fewer trajectory samples. PPO fine-tuning helps bridge the performance gap. Videos of trained models are available at the provided link. The discussion and limitations of implementing DCEM in the control setting highlight the need for further analysis and experimentation to address issues faced with model-based tasks. The study also suggests exploring different directions for efficiency and policy-based fine-tuning in model-based reinforcement learning. DCEM can be applied in various contexts beyond energy-based learning and control, showing promise for meta-learning. It can inspire the development of more powerful sampling-based optimizers that leverage gradient-based information. The scientific Python community's tools, including PyTorch and Matplotlib, were essential for this work. The temperature-scaled LML projection operation is proven to have a unique solution due to the strict convexity of the objective. The optimization process involves the sigmoid function and approaches the hard top-k function as the temperature parameter approaches zero. The decoder is influenced by the activation function used with it, with ELU performing the best. ReLU induces a biased distribution as alpha grows, unlike ELU or hyperbolic tangent. Despite reasonable initializations, hyperbolic tangent does not perform well in practice. In experiments, the decoder's performance is affected by the initial scale \u03b1 of its parameters. Ablations were considered for learning the latent action space in the cartpole task using DCEM. Different settings were tested with varying latent space dimensions and temperature parameters, showing DCEM's ability to recover expert performance on the cartpole. In the ground-truth dynamics setting, performance is measured by comparing controller costs. Using different temperature values in a two-dimensional latent space, the decoder's searchability is affected, leading to worse performance over time. In a two-dimensional latent space, updating the decoder becomes more challenging, resulting in decreased performance over time. DCEM is essential for maintaining decoder searchability in higher-dimensional spaces. A 16-dimensional latent space can be challenging for learning due to DCEM's flexibility in updating the decoder. DCEM improves performance on the cartpole task compared to CEM, showcasing its ability to recover full performance. The best validation loss is achieved with the DCEM model. DCEM reward surfaces for the cartpole task show varying initial states and decreasing temperature. The text discusses the PlaNet algorithm variant for proprioceptive control, utilizing deterministic and stochastic state models, a reward model, and an action sequence decoder. It involves initializing a dataset with random seed episodes, transitioning models, and optimizing likelihood bounds using DCEM. The PlaNet algorithm variant for proprioceptive control utilizes deterministic and stochastic state models, a reward model, and an action sequence decoder. The model is designed for multi-step training in observable environments, pushing forward information about future events. The PlaNet algorithm variant for proprioceptive control uses a GRU with 200 units for the deterministic path in the dynamics model. Training is done on batches of size 50 with trajectory sequences of length 50, starting with 5 seed episodes with random actions. Interleaving model updates with environment steps improves performance. The PlaNet algorithm variant for proprioceptive control uses CEM or DCEM optimizers with horizon length of 12 or 10 iterations. Training includes three phases to isolate DCEM additions, validated on 100 random episodes. Future work includes combining methods into a single training run and reducing sample complexity. Model initialization involves launching a single training run to obtain initial system dynamics. Phase 1 involves launching a single training run to establish initial system dynamics. The models converge to near-state-of-the-art performance on tasks, taking slightly longer than previous methods. Due to memory constraints, using policy loss for fine-tuning components is not feasible, leading to the initialization of a differentiable controller in Phase 2. In Phase 2, an embedded DCEM controller was attempted to achieve the same control cost as the full CEM controller, but it failed due to over-predicted rewards. An online data collection process was then used to update the models and prevent the controller from finding bad regions. Alternative methods were explored, such as updating the decoder to optimize control cost on samples from the replay buffer. In Phase 2, an embedded DCEM controller was attempted to achieve the same control cost as the full CEM controller, but it failed due to over-predicted rewards. An online data collection process was then used to update the models and prevent the controller from finding bad regions. The decoder can be immediately updated after planning at every step to optimize control cost on samples from the replay buffer. Hyper-parameters were kept fixed with 100 samples, 10 elites, and a temperature \u03c4 = 1. Ablation was done on the number of DCEM iterations, deleting the replay buffer, and re-initializing the model from phase 1. The best runs were used as the starting point for the next phase, achieving reasonable performance but not matching the performance of doing CEM over the full action space. These runs all use 10 DCEM iterations and keep the replay buffer from phase 1. Phase 3 involves policy optimization into the controller using Proximal Policy Optimization (PPO) without likelihood fitting, aiming to fine-tune the policy with differentiable controllers. The goal is to show that PPO can be a useful signal for updating the policy. Using standard PPO hyper-parameters, the study collected 10 episodes for each training step and tested different variations including the number of passes through episodes, fine-tuning or freezing reward, transition, and decoder, fixed variance or learning output of the controller, and learning rate of fine-tuned model-based portions. The results of the best runs were shown, and PPO-fine-tuned DCEM iterates were demonstrated for solving control optimization problems for cheetah and walker tasks."
}