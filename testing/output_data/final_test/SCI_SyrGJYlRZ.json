{
    "title": "SyrGJYlRZ",
    "content": "Hyperparameter tuning in deep learning is time-consuming. Optimizers like AdaGrad, RMSProp, and Adam adaptively tune learning rates. Momentum SGD has gained interest for potentially better results. Researchers revisit momentum SGD and find it competitive with Adam when hand-tuned. YellowFin, an automatic tuner for momentum and learning rate in SGD, outperforms Adam in convergence speed. YellowFin is an automatic tuner for momentum and learning rate in stochastic gradient descent (SGD), outperforming Adam in convergence speed by up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings. Researchers have proposed various methods for hyperparameter optimization in deep learning, from grid-search to adaptive optimizers aiming to eliminate the need for manual tuning. YellowFin is an automatic hyperparameter tuner for momentum in stochastic gradient descent, showing faster convergence than Adam. It aims to eliminate manual tuning by using Polyak's momentum SGD with hand-tuned learning rates. YELLOWFIN is an automatic hyperparameter tuner for momentum SGD, designed to handle the complex dynamics of asynchronous execution. It tunes both the learning rate and momentum on the fly, leveraging robustness insights to ensure convergence in non-convex objectives. Additionally, YELLOWFIN addresses stability concerns related to exploding gradients. The text discusses an extension to the YELLOWFIN hyperparameter tuner, using adaptive gradient clipping to stabilize training for objectives with exploding gradients. It introduces closed-loop YELLOWFIN for asynchronous training, incorporating a component to measure total momentum and control algorithmic momentum. Empirical results show YELLOWFIN converges faster than hand-tuned momentum SGD and default Adam on ResNets and LSTMs, with up to 2.69x speedup under asynchrony. YELLOWFIN is stable and achieves consistent performance, with a normalized sample standard deviation of test metrics varying from 0.05% to 0.6%. PyTorch and TensorFlow implementations have been released as drop-in replacements for any optimizer. YELLOWFIN has been implemented in various packages and its large-scale deployment in industry has provided important lessons about stability. Technical insights guiding the design of YELLOWFIN include momentum for convergence robustness to learning rate misspecification and curvature variation, particularly beneficial for deep learning. In machine learning, the objective is to minimize an objective function f(x) using gradient descent-based procedures like Polyak's momentum gradient descent update. Momentum accelerates convergence on strongly convex smooth functions, with optimal rates determined by the condition number and momentum value. The smallest momentum value ensuring uniform convergence along all directions is discussed in Section 2.2, shedding light on its implications. The dynamics of momentum on one-dimensional, nonconvex objectives are analyzed, introducing the concept of generalized curvature and its relation to the momentum operator. The derivative of f(x) can be expressed as the global minimum of f(x), with h(x) representing the generalized curvature with respect to the optimum x*. The momentum operator's spectral radius depends on momentum, showing robust convergence with respect to learning rate and curvature variations. For strongly convex quadratic objectives, the convergence rate is optimal when momentum is increased. The optimal convergence rate is achieved by increasing momentum, which widens the range of learning rates. Higher momentum values are faster and more robust for objectives with large condition numbers. Momentum is robust to curvature variation and guarantees the same convergence rate for strongly convex smooth objectives. The generalized condition number (GCN) captures variations in curvature along a scalar slice, with optimal hyperparameters ensuring a homogeneous spectral radius for momentum operators. While not guaranteeing a convergence rate, evidence supports the intuition that increasing momentum widens the range of learning rates for faster convergence. YELLOWFIN is introduced as a tuner for momentum SGD, utilizing tuning rules to handle SGD noise and optimize convergence rates. It works on a local quadratic approximation of the objective function to achieve a practically constant rate of convergence. The objective is to minimize a one-dimensional quadratic by using stochastic gradients in SGD. The model decomposes optimization on quadratics into scalar problems along principal eigenvectors of the Hessian. An exact expression for mean square error after running momentum SGD on a scalar quadratic is derived. The spectral radii of operators in YELLOWFIN are used to simplify analysis and capture convergence speed. Design decisions ensure working in the robust region for bias and variance operators. The surrogate objective in the robust region takes a specific form for optimization. YELLOWFIN uses a surrogate objective to extract a noisy tuning rule. The algorithm minimizes the expected squared distance from the optimum of a local quadratic approximation while keeping all directions in the robust region. The extremal curvatures capture curvature variation along different directions and as the landscape evolves. YELLOWFIN uses functions to measure quantities h max, h min, C, and D based on gradients. These measurement functions are designed with a negative log-probability objective in mind, aligning with typical losses in machine learning. The implementation of measurement oracles CURVATURERANGE, VARIANCE, and DISTANCE is described in Section 3.2, with a focus on the Fisher information matrix and noisy gradients. The Fisher information matrix is used to estimate curvatures in YELLOWFIN, allowing for measurements from minibatch gradients. Despite potential inaccuracies, these measurements help YELLOWFIN outperform state-of-the-art methods. Implementation details for zero-debias, slow start, and curvature range estimation are provided in Appendix E. In YELLOWFIN, the curvature of the Hessian along the gradient direction is approximated using running averages of extreme curvatures. The distance to the optimum of the local quadratic approximation is estimated by maintaining running averages of curvature and gradient norm. Neural network objectives can have large Lipschitz constants and training them involves non-stationary landscapes. The text discusses the use of adaptive gradient clipping heuristics to stabilize the training process of RNNs with hidden units, specifically addressing the 'exploding gradient' issue. The proposed method is validated on a convolutional sequence to sequence learning model for German-English translation, showing improved stability compared to traditional methods. YellowFin, with adaptive clipping, outperforms the default optimizer in Table 1, showing 0.84 higher validation BLEU4 after 120 epochs. Asynchrony is a parallelization technique that improves hardware efficiency but can increase the number of iterations. BID16 interprets asynchrony as added momentum dynamics. A closed-loop variant of YELLOWFIN is designed to automatically control algorithmic momentum, compensate for asynchrony, and accelerate convergence using a specific formula. YellowFin with adaptive clipping outperforms the default optimizer, showing higher validation BLEU4 after 120 epochs. Asynchrony is seen as added momentum dynamics in BID16. A closed-loop variant of YELLOWFIN automatically controls algorithmic momentum to compensate for asynchrony and accelerate convergence. The estimator \u03bc T is used to design a robust estimator for total momentum, and a negative feedback control loop adjusts algorithmic momentum to match the target momentum decided by YELLOWFIN. In Figure 5, momentum dynamics in an asynchronous training system are demonstrated, showing the effectiveness of closed-loop YELLOWFIN in speeding up convergence. YELLOWFIN is evaluated in both synchronous and asynchronous settings, demonstrating competitive performance with Adam without hand-tuning. Training loss and validation metrics are averaged from 3 runs with different random seeds to eliminate seed influences. Adam and momentum SGD are tuned on learning rate grids with prescribed momentum. YELLOWFIN runs without hand tuning, and full specifications are provided in the appendix for visualization purposes. In Appendix I, training losses are smoothed with a window width of 1000 for visualization. The lowest smoothed loss achieved by momentum SGD is compared to tuned Adam on ResNets, showing speedups of 1.71x and 1.87x on CIFAR10 and CIFAR100. Momentum SGD also outperforms tuned Adam, Vanilla SGD, and AdaGrad in various tasks like language modeling and parsing. YELLOWFIN, without any hand-tuning, matches hand-tuned momentum SGD for all ResNet and LSTM models, achieving better validation metrics than tuned Adam. It consistently outperforms state-of-the-art adaptive optimizers, showing 1.38x to 3.28x speedups in training losses compared to tuned Adam on various models. In this section, closed-loop YELLOWFIN is evaluated for speed and efficiency in reaching a solution. Using 16 asynchronous workers on a single machine, closed-loop YELLOWFIN achieves a 20.1x speedup compared to regular YELLOWFIN and a 2.69x speedup compared to Adam. This shows that closed-loop YELLOWFIN can accelerate by adjusting algorithmic momentum to handle asynchrony and converge in fewer iterations than Adam in asynchronous-parallel training. Many techniques have been proposed for tuning hyperparameters in optimizers, including adaptive methods like AdaGrad, RMSProp, and Adam. Existing approaches either focus on deterministic settings or analyze stochasticity with specific learning rates. YELLOWFIN is an optimization method that automatically adjusts momentum and learning rate for stochastic training of neural networks. It outperforms other adaptive optimizers in both synchronous and asynchronous settings by estimating statistics from gradients and tuning hyperparameters based on local quadratic approximations. Future work includes improving curvature estimation methods and exploring closed-loop momentum control mechanisms for faster convergence in parallel settings. Lemma 6 provides a generalized version to prove Lemma 2 in asynchronous-parallel settings. It states that the spectral radius of A t is controlled by momentum if certain conditions on eigenvalues are met. The momentum update can be expressed as a linear operator under specific conditions. Lemma 7 and Lemma 8 are proven as preparation for Lemma 4. The curvature h of a one-dimensional quadratic function f is discussed, with x t = Ex t. The proof involves the recurrence of momentum SGD and matrix form equations. The expectation of x t is x t 1, with a recurrence formula provided for quadratic functions with curvature h. The proof involves deriving the recurrence for U t and V t in matrix form for a one-dimensional quadratic function f (x) with curvature h. The proof also includes the cancellation of certain terms and the joint expression of formulas in matrix form. Additionally, Lemma 7 and 8 are used to show the expectation of x t as x t 1, leading to the proof of Lemma 4 for one-dimensional quadratics. The text discusses decomposing quadratics along eigenvector directions and applying Lemma 4 to each direction using the corresponding curvature. It presents a proof of a multiple dimensional generalized version of Lemma 5, which is a special case of Lemma 9. Lemma 9 implies that for multiple dimension quadratics, the spectral radius is equal to the eigenvalue. The text discusses the transformation of a formula using algebra and simplification techniques to find eigenvalues of symmetric matrices. It concludes that the spectral radius of a matrix can be controlled by a specific condition, leading to an analytical solution without the need for iterative solvers. In Section 3.2, estimators for learning rate and momentum tuning in YELLOWFIN are discussed. Practical implementation details for improving estimators are identified, including the use of zero-debias proposed by BID11 to accelerate exponential average adaptation. The estimated curvature in some LSTM models may decrease quickly during optimization, prompting the need to better estimate extremal curvature h max and h min. In order to estimate extremal curvature h max and h min with fast decreasing trend, zero-debias exponential average is applied on the logarithmic of h max,t and h min,t. The slow start heuristic proposed by Schaul et al. (2013) is also implemented, using a learning rate formula \u21b5 = min{\u21b5 t , t \u00b7 \u21b5 t /(10 \u00b7 w)}. Gradient clipping is mentioned as a standard tool for training objectives, with a tradeoff between adaptivity and stability. YELLOWFIN keeps running estimates of extremal gradient magnitude squares to estimate a generalized condition number. Adaptive clipping with a threshold of p h max stabilizes training on objectives with exploding gradients without compromising performance on stable models. In Section 4, the closed-loop momentum control mechanism in YELLOWFIN is discussed. It measures system dynamics and controls momentum with a negative feedback loop. Details on CIFAR10 and CIFAR100 ResNet iterations and learning rates are provided, along with PTB LSTM iterations and learning rates. In Section 4, the closed-loop momentum control mechanism in YELLOWFIN is discussed. It measures system dynamics and controls momentum with a negative feedback loop. Details on CIFAR10 and CIFAR100 ResNet iterations and learning rates are provided, along with PTB LSTM iterations and learning rates. Additionally, experimental results on training loss for CIFAR10 ResNet and CIFAR100 ResNet are demonstrated. YELLOWFIN demonstrates comparable performance to hand-tuned momentum SGD and achieves significant speedups on CIFAR10 and CIFAR100 ResNet. The adaptivity of momentum in YELLOWFIN is highlighted through experiments on TS LSTM and CIFAR100 ResNet, showing faster convergence compared to fixed momentum values. This emphasizes the importance of momentum adaptivity in YELLOWFIN. Additionally, experiments on PTB LSTM with 16 asynchronous workers using Adam are conducted following the same protocol. In experiments on PTB LSTM with 16 asynchronous workers using Adam, the smoothing parameter 1 BID11 of the first order moment estimate was swept in a grid. Tuning momentum for Adam under asynchrony showed better training loss, highlighting the importance of momentum tuning in asynchronous settings. YELLOWFIN, an adaptive tuner, does not require manual tuning and can lead to faster development iterations on model architectures compared to grid search on optimizer hyperparameters. In this section, the effectiveness of a learning rate factor on YELLOWFIN is empirically demonstrated using a 29-layer ResNext on CIFAR10 and a Tied LSTM model on the PTB dataset. The optimal learning rate factor is searched in a grid for both models, as well as for the Adam optimizer. The effectiveness of a learning rate factor on YELLOWFIN is demonstrated using ResNext and Tied LSTM models. The optimal learning rate factor is searched in a grid for both models, as well as for the Adam optimizer. With the best learning rate factor, YELLOWFIN improves validation perplexity on Tied LSTM and test accuracy on ResNext. YELLOWFIN outperforms Adam in validation metric, showing that fine-tuning the learning rate factor can effectively enhance deep learning model performance."
}