{
    "title": "BviYjfnIk",
    "content": "Camera drones offer the ability to inspect environments remotely with mobility and agility. Manual piloting is error-prone, while autopilot systems may lack flexibility. StarHopper is a touch screen interface for efficient object-centric drone navigation, combining manual and automated control for remote exploration. A lab study shows a 35.4% efficiency gain over manual piloting. StarHopper offers a 35.4% efficiency gain over manual piloting with an object-centric navigation system. Telepresence aims to go beyond replicating local experiences to enable benefits not possible when physically present, transitioning from replication to augmentation. Camera drones allow remote bodies to fly, expanding the possibilities of telepresence beyond face-to-face conversations to scenarios like shopping in a mall or attending conferences. Drones offer potential for flexible remote presence experiences, but manual control is challenging due to various factors. Autopilot techniques have been applied to relieve the burden of manual piloting. The research aims to design a camera drone control interface for efficient and flexible remote visual inspection, addressing the challenges of manual piloting and the limitations of existing autopilot systems based on predefined waypoints. The research aims to design a camera drone control interface for efficient and flexible remote visual inspection, combining manual and automatic piloting into a hybrid navigation interface. Building upon object-centric techniques, the interface allows for free exploration of a remote environment, addressing the limitations of existing drone navigation interfaces. StarHopper is a remote object-centric camera drone navigation interface designed for efficient and flexible visual inspection in indoor environments. It allows for free exploration of the environment and flexible inspection of objects from various angles. The system combines manual and automatic control through touch interactions and minimal geometric information, enabling users to specify new objects of interest easily. The system, StarHopper, is designed for efficient visual inspection in indoor environments like remote warehouse inspection and museum visits. A study showed it was 35.4% faster than manual inspection, with users preferring object-centric navigation. The research builds upon interactive drones and camera navigation techniques in computer graphics, aiming to increase efficiency in remote visual inspection tasks. Drones are used for various purposes such as flying cameras, with research focusing on automating flight paths and improving camera shot quality. Autopilot interfaces require setting flight waypoints on maps, while some work aims to reduce user input by acting autonomously based on environmental knowledge. Aerial photography interfaces leverage positional and geometrical information for efficient operation. Several aerial photography interfaces utilize positional and geometrical information of photography subjects. DJI drones with ActiveTrack can follow a subject recognized through touchscreen gestures and orbit around it based on user joystick input. Other approaches involve mapping the operator's movements and head orientation to control the drone's pose. Our design combines auto-piloting efficiency with manual controls for drone navigation. It supports object-centric navigation without needing a 3D environment map. Various interfaces have been developed for directing robots to interact with objects, such as using a laser pointer to designate objects of interest. The system extends object-aware robot control to camera drones, allowing for free exploration and unplanned movement. It includes mechanisms to work under ambiguity and does not require a 3D environment map. Numerous techniques have been developed to improve camera navigation in 3D environments, including object-centric techniques like HoverCam, StyleCam, and Navidget. These techniques allow users to smoothly move a virtual camera around objects of interest using different input methods. In computer graphics, camera manipulation techniques for physical inspection are adapted for aerial robotic systems. Guidelines for remote object-centric drone navigation are presented, emphasizing the importance of situational awareness for successful teleoperation. Users rely on static maps for flight planning, but dynamic environments require up-to-date situational awareness for effective drone inspection. The high mobility of drones is best utilized in environments with changing situations. Despite advancements in 3D reconstruction technology, it remains computationally expensive and performance is affected by environmental conditions. To adapt to a wider range of real-world applications, the navigation interface should minimize reliance on environmental knowledge. Efficient and flexible viewpoint control is essential, with automated mechanisms to simplify interaction while maintaining user control flexibility. Recent research has shown that users prefer manual control even with high automation accuracy. Control flexibility supports free exploration in remote inspection scenarios like exploring a museum. Investigating semi-autonomous interfaces allows users to benefit from automation while retaining control. Touch-based navigation commands should use simple gestures to improve efficiency and learning. Complimentary automated systems can handle complex navigation paths. When designing interactions for camera drones, physical constraints like latency, drift, and object avoidance must be considered. StarHopper is a remote drone navigation interface that follows an object-centric control paradigm. The user interface and hardware configuration are described in detail. The hardware configuration includes a regular RGB camera for an exocentric overview, real-time tracking of the drone's position, and a touch screen interface for live stream video and drone navigation. StarHopper allows users to easily determine object position and dimensions without the need for pre-built maps or expensive 3D reconstruction methods. The user can select an object of interest in the overview camera view through a drag gesture. The drone then turns to look at the selected region, and the user can further select the object in the drone camera view. A computer vision algorithm triangulates the object's position and estimates its dimensions using a bounding cylinder. Once registered, the object is set as the object-of-interest with its center marked in the drone camera view. The design includes three object-centric physical camera navigation mechanisms: 360 viewpoint widget, delayed through-thelens control, and object-centric joysticks. The 360 viewpoint widget allows users to quickly navigate and focus on an object of interest from a specified viewing angle by interacting with a semi-transparent 3D ring surrounding the object. Users can drag horizontally to rotate the arrow around the ring for the desired viewing angle and vertically to adjust the vertical height of the viewpoint. The algorithm calculates a default viewing distance based on the object's size, ensuring it fits into the central 1/3rd of the camera frame. The drone's travel time is set using a logarithmic mapping from the distance, allowing for quick approach to distant targets while maintaining smooth orbits. This speed tuning method is inspired by virtual camera control techniques. Through-the-lens camera control is a classic technique in interactive graphics, allowing users to move the camera by dragging points in the image to new positions. By leveraging the estimated 3D position of the object-of-interest in relation to the drone camera, we calculate the required drone movement to achieve the specified viewpoint. Users freeze the current frame by resting two fingers on the drone camera view and then perform a two-finger pinch-and-pan gesture to transform the frame to the desired viewpoint. The system calculates a new drone position for navigation towards the desired viewpoint. The drone camera control technique involves object-centric joysticks that remap traditional controls to focus on the object of interest. This allows for panning, zooming, and orbiting while keeping the object in view. The drone camera control technique involves object-centric joysticks for panning, zooming, and orbiting around the object of interest. Manual controls are also available for making slight adjustments to the viewpoint. The drone camera control technique includes object-centric joysticks for panning, zooming, and orbiting around the object of interest. Users can tap on thumbnails to select objects, triggering the drone to approach them using auto-pilot. Camera parameters are calibrated for triangulating object positions. The Vicon motion capture system is used for tracking the drone, but could be replaced with cheaper alternatives for precision. The technology used involves Z-axis-aligned bounding cylinders for object representation, allowing for precise camera movements. The two-view triangulation method in StarHopper utilizes image segmentation to extract foreground objects and determine their geometric centers for bounding cylinders. The technology involves Z-axis-aligned bounding cylinders for object representation. The geometric center of the bounding cylinder is approximated using rays. The radius and height of the cylinder are computed using the external calibration matrix and focal length of the drone camera. The position of the cylinder center in camera coordinates is calculated, and the width and height of the 2D bounding box of the object are used. This method slightly overestimates the size of the cylinder but works well for visual object inspection. The drone uses a ray projected from the overview camera to navigate towards the second region for triangulation. It moves to the closest position where the camera can view the object, incorporating obstacle avoidance. A Ryze Tello drone in slow mode was used for the prototype, controlled through Python with WiFi communication introducing a slight delay. The drone was controlled using a Python client based on the Ryze Tello SDK v1.3, with 4 PID controllers for velocity inputs. The user interface ran on a Windows 10 laptop with specific hardware specifications. StarHopper offers various navigation mechanisms for scene inspections, ranging from fully automated to fully manual. The automation level of a mechanism is determined using the Levels of Automation of Decision and Action Selection model. The 360 viewpoint widget allows users to approach objects from any angle with one touch gesture, but has low flexibility. Delayed through-the-lens control is less efficient and designed for smaller distances. Object-centric joysticks enable convenient camera movements for visual inspection. Object-centric joysticks provide medium efficiency and flexibility for controlling drone movements. Manual joysticks are less efficient but offer high flexibility, allowing users to move the drone freely. Higher automation levels increase efficiency but reduce flexibility in drone control. The system offers efficient and flexible navigation mechanisms, combining automatic and manual controls for object inspection tasks. A user study compared the system to a baseline for evaluating navigation mechanisms. The study compared StarHopper, a system for remote object inspection, to conventional manual joystick controls. 12 volunteers participated, with some having prior quadcopter drone piloting experience and others being frequent first-person-view (FPV) video game players. The focus was on navigation performance without the registration phase. The study involved participants with varying levels of drone piloting and FPV game experience in a simulated remote warehouse inspection task. Participants operated a camera drone to inspect target objects using different control interfaces. A wall separator was used to simulate remote operation. The study aimed to evaluate navigation performance. The study evaluated the navigation performance of StarHopper in a simulated remote warehouse inspection task. Participants practiced piloting the drone using different control interfaces and target objects were predefined in the system. The user needed to tap on the target object before performing any navigation operations. The participants needed to capture a specified side of the item with a viewpoint similar to a reference photo. ArUco fiducial markers were attached to each side of the item for quantification. The drone had to be within 800mm distance and 45\u00b0 angle to the marker for correct inspection position. If the drone remained in the correct position for 1.5 seconds, the trial was completed. Adjustments were required for the drone in all trials to meet the criteria. The participants used a drone to capture specific sides of items with ArUco fiducial markers. The drone had to be within a certain distance and angle to the marker for correct inspection. Visual feedback icons and progress bars were used to guide participants. Completion time and flight path were recorded for each trial. Participants filled out a NASA TLX worksheet and a post-study questionnaire. The study used a repeated measures design with participants capturing viewpoints of chef figurines using different interfaces and items. Each participant completed 24 trials in a 4.5 by 4.5 meters test environment, lasting about 60 minutes. Participants were seated next to the environment with their line of sight blocked to simulate remote operation. The study involved participants capturing viewpoints of chef figurines using different interfaces and items in a test environment with barriers. Each figurine had a 40mm AruCo marker on each side. The drones were operated using virtual joysticks in the StarHopper interface. Geisser correction and ANOVA tests were applied to analyze performance data, with a significant main effect of Side on Time found. Post-hoc comparisons showed differences in inspecting the front. Pairwise comparisons using t-test with Bonferroni adjustments revealed that inspecting the front side was significantly faster than the other sides, and the left or right side was faster than the back side. Participants' performance decreased as navigation route complexity increased. StarHopper showed a consistent efficiency advantage over manual control across all sides. No significant interaction between Interface and Side was found. A main effect for Item on Time was significant, but post-hoc tests did not find differences between items. Flight traces were analyzed for further insights. The flight traces were analyzed to understand performance differences. Participants preferred continuous motion with StarHopper over manual control. StarHopper was faster even for front views. Participants preferred StarHopper for remote inspection. Participants preferred using StarHopper for remote inspection over manual control, finding it more intuitive and easier to navigate. They believed virtual joysticks could enhance gestural interaction for drone navigation. Wilcoxon signed-rank tests showed StarHopper was ranked better for mental demand, physical demand, performance, and effort. The study confirmed the efficiency gain of object-centric navigation in StarHopper. The study confirmed the efficiency gain of object-centric navigation in StarHopper, enhancing remote inspection efficiency for both simple and complex routes. Users perceived lower workload and showed an overall preference for object-centric controls. Participants were given freedom to choose their preferred navigation techniques during the study. Participants in the study used a combination of object-centric navigation techniques in StarHopper, adjusting viewpoints with delayed through-the-lens control and pan/orbit movements. They hesitated to give commands to the drone during automated movements, fearing they might interrupt its actions. This behavior was unique to physical viewpoint navigation tasks. The breakdowns in automation-human communication can impede efficiency in virtual navigation tasks. Proposed solutions include using augmented reality for local drone operation and replacing the overview camera with a second camera drone to expand the exploration area. This problem has drawn strong community interest and is considered for future work. StarHopper, a drone, can function outdoors with an accurate positioning system. The study results are limited by technical and non-technical factors, including control and video streaming latency typical for small consumer drones. The primitive obstacle avoidance ability of StarHopper restricts its maximum speed, but incorporating advanced collision avoidance algorithms could enhance its potential. The drone used in the study had one fixed forward-looking camera, limiting flexibility for users. Future work will explore new design opportunities enabled by technical alternatives. The study focused on drone object-centric navigation performance in a limited lab setting with three target items. Participants were not truly remote as they could hear the drone noise, affecting navigation. Future studies will explore flexibility and efficiency in more complex environments. In future studies, the research aims to extend the exploration of touch-based object-centric navigation for camera drones in outdoor environments, such as search and rescue operations. The prototype system, StarHopper, showed notable efficiency improvements for remote visual inspection compared to manual control. The system combines automated, semiautomated, and manual control mechanisms for efficiency and flexibility. In future work, the research plans to study users' mental models when using automated camera drones for remote inspection. They also aim to expand the use of StarHopper in larger outdoor spaces with a second drone as an overview camera. The navigation techniques are inspired by classical interactive graphics, and advancements in 3D sensing and object recognition may lead to more powerful telepresence navigation techniques."
}