{
    "title": "HJJ0w--0W",
    "content": "Tensor-Train RNN (TT-RNN) is a new neural sequence architecture for multivariate forecasting in environments with nonlinear dynamics. It addresses long-term temporal dependencies, higher-order correlations, and error propagation sensitivity by learning nonlinear dynamics using higher order moments and state transition functions. The tensor-train decomposition reduces parameters while maintaining model performance, with theoretical guarantees for general sequence inputs. TT-RNN shows significant long-term prediction improvements over general RNN and LSTM architectures in various simulated nonlinear environments. Forecasting in complex systems with nonlinear dynamics, such as climate and traffic, is challenging due to long-term temporal dependencies and higher-order dynamics. Current forecasting systems struggle to accurately represent these dynamics, leading to limited long-term prediction accuracy. The key challenge is to model nonlinear dynamics accurately for stable long-term predictions. The forecasting problem involves efficiently learning a model to predict future states over a long time horizon. Common approaches include linear time series models, state space models, and deep neural networks like RNN and LSTM. While these methods perform well for short-term dynamics, they struggle to capture long-term nonlinear dynamics effectively. The text discusses the limitations of current data-driven learning methods in capturing long-term nonlinear dynamics for forecasting. It introduces Tensor-Train RNNs as a novel approach that explicitly models higher-order dynamics and interactions while being scalable. The models use tensor trains to reduce the number of parameters while preserving correlation structures. The study theoretically analyzes and experimentally validates the effectiveness of Tensor-Train RNNs in forecasting various domains. TT-RNNs encode higher-order non-Markovian dynamics and state interactions using tensor-train decomposition for efficient learning. Theoretical guarantees are provided for their representation power in nonlinear dynamics, showing improved forecasting accuracy compared to standard RNNs and LSTMs in climate and traffic environments. Nonlinear dynamics in systems are governed by differential equations, with continuous time dynamics described by differential equations and discrete time dynamics described by difference equations. Examples include the Lorenz attractor in continuous time and the Genz dynamics in discrete time. These systems exhibit higher-order correlations, long-term dependencies, and sensitivity to error propagation, making them challenging for learning. The forecasting problem aims to predict future states given a sequence of initial states. The forecasting problem involves learning a model to predict future states accurately. Common approaches in deep learning use first-order hidden-state models like recurrent neural networks (RNNs). RNNs compute outputs recursively from hidden states, considering only the most recent state transition. LSTM cells and GRUs are examples of models that improve upon RNNs by incorporating memory states. Tensor-Train RNNs (TT-RNNs) are proposed as a higher-order model to effectively learn nonlinear dynamics by explicitly modeling L-order Markov processes and polynomial interactions between hidden states and inputs. Unlike traditional RNNs, TT-RNNs consider longer historical context to improve forecasting effectiveness in environments with nonlinear dynamics. The TT-RNNs model nonlinear dynamics by considering longer historical context and using higher-order moments to approximate state transition functions. It constructs a higher-order transition tensor by modeling polynomial interactions between hidden states, defining the TT-RNN with standard RNN cell and TT-RNN with LSTM cell (TLSTM). The TT-RNN with LSTM cell, or \"TLSTM\", is a module for sequence-to-sequence framework that uses tensor-train recurrent cells in both the encoder and decoder. To address the issue of exponential growth in parameters, tensor networks are utilized to approximate the weight tensor, providing a more manageable model size for training. Tensor-train models decompose tensors into low-dimensional tensors connected in a network, reducing parameters in TT-RNN. They avoid the curse of dimensionality and offer a theoretical characterization of representation power. The representation power of tensor-train neural networks for approximating high-dimensional functions is analyzed by preserving weak differentiability and compact representation. The approximation error for TT-RNN with one hidden layer is bounded based on the regularity of the target function, input space dimension, tensor train rank, and order of the tensor. The target function describes system dynamics state transitions and is assumed to be a Sobolev function defined on the input space. The space H k \u00b5 consists of functions with bounded derivatives up to order k and are L \u00b5 -integrable. Any Sobolev function can be decomposed into eigenvalues and eigenfunctions. The target function f \u2208 H k \u00b5 can be approximated using functional tensor-train (FTT) approximation. TT-RNN implements a polynomial expansion of the state s to approximate f T T, with the approximation error bounded by TT-RNN as a one-layer hidden neural network. A weak derivative generalizes the derivative concept for functions, defined by the size of the state space, tensor-train rank, and degree of high-order polynomials. The theorem shows that smoother targets are easier to approximate and polynomial interactions are more efficient than linear ones. TT-RNN applies to various RNN types with given state transitions. Validation on synthetic and real-world datasets confirms accuracy and efficiency. The Genz \"product peak\" function is used for high-dimensional function approximation. Traffic data from Los Angeles County highway network is collected for predicting speed readings. Climate data is collected from USHCN for prediction tasks. The prediction task involves forecasting daily maximum temperatures for 15 stations over a span of approximately 124 years. After preprocessing, 6,954 sequences of length 366 were obtained. TT-RNNs were compared against 1st-order RNN and matrix RNNs using LSTM as recurrent cells for long-term forecasting. Training was done using stochastic gradient descent on sequence regression loss. TLSTM outperforms vanilla RNN and MRNN in long-term forecasting accuracy for various datasets, including traffic, climate, and Genz dynamics. TLSTM is particularly robust to long-term errors, as shown in test prediction error results. TLSTM is more robust to long-term error propagation and outperforms unfactorized models like MRNN and MLSTM. Tensor-train neural networks provide stable representations for long-term predictions, as seen in their performance on the Genz function and real-world traffic datasets. TLSTM outperforms other models like MRNN and MLSTM in long-term forecasting, showing better accuracy and capturing detailed curvatures. TT-RNNs demonstrate faster convergence and lower validation loss, indicating more efficient representation of nonlinear dynamics. Hyper-parameter analysis shows TLSTM's effectiveness in handling long-term error propagation. The TLSTM model is analyzed with hyper-parameters like tensor-train rank and lags. A random grid search is conducted to determine the best parameters, showcasing results in Table 1. The model's capacity to capture non-linear dynamics is affected by the tensor ranks, with overfitting occurring at high ranks. Changing lags also impacts the model's performance, with the best lag value varying for different forecasting horizons. TT-RNN is evaluated for long-term forecasting on chaotic dynamics like the Lorenz dynamics. TT-RNN can predict up to T = 40 steps into the future but diverges quickly beyond that due to the sensitivity to input perturbations. State-of-the-art prediction models are not stable in this setting, contrasting with classic auto-regressive models like ARMA or ARIMA. Our method explicitly models higher-order dependencies, utilizing neural networks for time series forecasting in various domains. From a modeling perspective, BID7 uses a high-order RNN to simulate a deterministic finite state machine and recognize regular grammars. BID17 proposes multiplicative RNNs that allow each hidden state to specify a different factorized hidden-to-hidden weight matrix. Our method is an efficient generalization of these works and utilizes hierarchical RNNs to model sequential data at multiple resolutions. Additionally, tensor methods have connections with neural networks, as shown in BID4 with convolutional neural networks having equivalence to hierarchical tensor structures. Our model introduces TT-RNN, a novel class of RNNs utilizing tensor networks for sequential prediction tasks in environments with nonlinear dynamics. TT-RNN provides approximation guarantees and improved forecasting accuracy for longer time horizons compared to traditional RNNs. The benefits of TT-RNN include accurate forecasting for longer time horizons in both synthetic and real-world multivariate time series data. Chaotic dynamics pose a challenge for sequential prediction models, prompting the study of robust models for such dynamics. TT-RNN's effectiveness in natural language processing and other sequential prediction settings is worth further investigation. The proposed TT-RNN model is supported by theoretical guarantees, utilizing tensor-train decompositions to preserve weak differentiability and provide a compact representation. The text discusses bounding the approximation error for TT-RNN with neural network estimation theory, focusing on the regularity of the target function, input space dimension, and tensor train rank. The target function is a Sobolev function defined on the input space, allowing for a Schmidt decomposition. This decomposition helps in truncating the target function for more efficient processing. The FTT approximation in Eqn 13 projects the target function to a subspace with finite basis, and the approximation error can be bounded using Lemma 7.1. TT-RNN implements a polynomial expansion of input states to approximate fTT, with the degree of the polynomial denoted as p. The TT-RNN structure connects with the degree of the polynomial, bounding the approximation error. A weak derivative is defined for (non)-differentiable functions, with the tensor-train rank and polynomial order affecting the required hidden units size. Models were trained using RMS-prop optimizer with a learning rate decay schedule. Hyper-parameter search range is reported in TAB3 for validation. In this work, hyper-parameter search range is reported in TAB3. A 80% \u2212 10% \u2212 10% train-validation-test split is used for all datasets, with a maximum of 1e 4 training steps. Moving average of validation loss is computed for early stopping. Genz functions are utilized for high-dimensional function approximation, with 7 different functions analyzed. Los Angeles County highway network traffic data is used in the study. The dataset from Los Angeles County highway network contains 4 months of speed readings aggregated every 5 minutes. Missing values were imputed using average values from other sensors. The dataset covers 35,136 time-series, with each sequence representing daily traffic of 288 time stamps. The dataset was up-sampled every 20 minutes, resulting in 8,784 sequences. 15 sensors were selected for joint forecasting tasks. Climate data includes daily maximum temperature from USHCN daily, spanning over 124 years and collected from 1,200 locations. The dataset analyzed covers more than 1,200 locations and 45,384 days, with a focus on 54 stations in California. The temperature readings per year were treated as sequences, with missing observations imputed using data from other stations. The dataset was augmented by rotating sequences every 7 days, resulting in 5,928 sequences. A Dickey-Fuller test showed non-stationarity in the time series data. Genz functions were used for visualization, showing dynamics and predictions from TLSTM and baselines. TLSTM performed better for \"oscillatory\", \"product peak\", and \"Gaussian\" functions. TLSTM outperforms baselines in capturing complex dynamics like \"oscillatory\" and \"Gaussian\" functions. Chaotic dynamics of Lorenz attractor are challenging due to sensitivity to perturbations. Tensor-train neural networks were evaluated for long-term forecasting on Lorenz attractor system. The dynamics of the system are generated using specific parameters. Initial conditions are randomly sampled. Different models are compared for predictions, with tensor models outperforming vanilla RNN and MRNN. TT-RNN shows slight improvement initially and more consistent predictions compared to baselines."
}