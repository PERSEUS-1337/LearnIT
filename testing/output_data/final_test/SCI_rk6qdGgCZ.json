{
    "title": "rk6qdGgCZ",
    "content": "The text proposes a modification to adaptive gradient algorithms like Adam to improve weight decay regularization. This modification decouples weight decay from optimization steps, leading to better generalization performance on image classification datasets. The text introduces a normalized variant of weight decay to improve optimization runs and proposes Adam with warm restarts (AdamWR) for strong anytime performance on CIFAR-10 and ImageNet32x32. Our source code will be available after the review process. Adaptive gradient methods like AdaGrad, RMSProp, and Adam are commonly used for training neural networks. However, recent studies have shown that stochastic gradient descent (SGD) with momentum still achieves state-of-the-art results on image classification datasets. Wilson et al. (2017) found that adaptive gradient methods do not generalize as well as SGD with momentum across various deep learning tasks. The reasons for this discrepancy include sharp local minima and inherent issues with adaptive gradient methods. In this paper, it is highlighted that the poor generalization of popular adaptive gradient methods like Adam is due to suboptimal implementation of weight decay. This issue affects other adaptive gradient methods as well. The standard way of implementing L2 regularization in Adam is found to be dysfunctional, leading to worse results compared to SGD with momentum on tasks where L2 regularization is beneficial. Contrary to common belief, L2 regularization and weight decay are not the same. Our empirical analysis of Adam suggests that optimal weight decay is influenced by the total number of batch passes/weight updates. The longer the runtime/number of batch passes, the smaller the optimal weight decay. This effect is often overlooked as hyperparameters are typically tuned for a fixed number of training epochs, leading to suboptimal performance for longer runs. Our contributions aim to address issues with weight decay in optimization algorithms like SGD and Adam. We propose decoupling weight decay from the gradient-based update, normalizing weight decay values based on the total number of batch passes, and introducing Adam with warm restarts for improved performance. AdamWR is introduced to improve anytime performance by incorporating warm restarts. The goal is to enhance Adam's weight decay to compete with SGD with momentum across various problems, eliminating the need for practitioners to switch between algorithms. The weight decay, as described by BID6, involves weights decaying exponentially with a defined rate, batch gradient, and learning rate. Additionally, modifications to the batch loss function and the inclusion of a bias term for regularization are considered. The weight decay regularization, commonly used in popular libraries like TensorFlow and PyTorch, involves modifying gradients directly to introduce L2 regularization. This method is often seen in stochastic gradient descent with momentum, where the weight decay term affects the momentum term. The weight decay regularization in popular libraries like TensorFlow and PyTorch involves modifying gradients to introduce L2 regularization, affecting the momentum term in stochastic gradient descent. The algorithm adjusts the weight decay factor and momentum term to optimize parameters. The algorithm adjusts the weight decay factor and momentum term to optimize parameters during stochastic gradient descent. The weight decay can be fixed, decay, or used for warm restarts, affecting the decay of parameters. Adjusting the hyperparameters \u03b1 and w is proposed to address non-separability in their selection. The proposed SGD variant SGDW with momentum addresses the non-separability issue in selecting \u03b1 and w by decoupling them and introducing a scaling factor \u03b7 t. This ensures that both weight decay and learning rate are scheduled appropriately, even when using L2 regularization. Using L2 regularization instead of weight decay couples regularization and learning rate in SGD with momentum. Adaptive gradient optimizers like the Adam algorithm introduce unintended behavior due to the coupling. Adam maintains a vector vt to control parameter-wise learning rates by normalizing gradients. Introducing weight decay to Adam results in an update that differs from the original weight decay. The vt vectors are responsible for both parameter-wise gradients and weights, leading to renormalization of mt. The amplitudes of weights are used to renormalize updates in Adam, affecting weight decay differently than L2 regularization. This distinction has not been previously explored in the literature. In AdamW, weight decay is applied simultaneously with gradient-based updates to improve generalization compared to Adam. A normalized weight decay factor is introduced to reduce dependence on different computational budgets. In this study, the authors propose normalized weight decay to address the shrinking effect of weight decay with smaller batch sizes. They apply warm restarts to Adam, inspired by previous work on Stochastic Gradient Descent with Warm Restarts (SGDR). Despite initial challenges with Adam's weight decay, improvements were made through fixed weight decay regularization and normalized weight decay. AdamWR is constructed by incorporating warm restarts from SGDR, where the effective learning rate changes to accelerate DNN training. Warm restarts in SGDR involve increasing the learning rate \u03b7 while using the previous solution x as a starting point, controlling the use of past information like momentum. The effective learning rate in AdamWR is controlled by a schedule multiplier that decays according to the cosine annealing BID15 for each batch iteration. The schedule multiplier is adjusted to maintain good anytime performance by updating the current epoch count and triggering restarts when necessary. The learning rate is controlled by a factor that is set to the initial learning rate and remains constant throughout the experiment. The proposed AdamWR algorithm adjusts the learning rate and weight decay parameters dynamically during training. It is based on AdamW and utilizes normalized weight decay for consistent performance across different run lengths. Similarly, SGDWR incorporates warm restarts in stochastic gradient descent. Experimental results show that incorporating Shake-Shake regularization in a 3-branch residual neural network achieves state-of-the-art results on CIFAR-10 and CIFAR-100 datasets. The network achieved new state-of-the-art results on CIFAR-10 (2.86%) and CIFAR-100 (15.85%) datasets using SGDR training with batch size 128 for 1800 epochs. The model is based on fb.resnet.torch and utilizes a 26 2x64d ResNet and a 26 2x96d ResNet. The Shake-Shake method was applied for regularization. A 2x64d ResNet was trained with cosine annealing for 100 epochs to verify the coupling of initial learning rate and weight decay factor. The learning rate was scheduled with cosine annealing for better results compared to a fixed rate. Comparisons were made between SGD vs. SGDW and Adam vs. AdamW. Weight decay and learning rate were found to be interdependent, with changing both simultaneously yielding better results than changing only one. The proposed SGDW decouples weight decay and initial learning rate, making them more separable. Changing both simultaneously can yield clear improvements, unlike changing only one which could worsen results. This approach contrasts with the sensitivity of SGD to hyperparameter settings. The results show that leaving the learning rate fixed and optimizing only the weight decay factor can still yield good results. Comparisons with Adam also show that Adam's best hyperparameter settings perform worse than SGD's best ones. The experimental results in FIG1 support the hypothesis that weight decay and learning rate hyperparameters can be decoupled, simplifying hyperparameter tuning in SGD and improving Adam's performance. AdamW outperformed the original Adam and rivaled SGD and SGDW in terms of best hyperparameter settings. The experiment compared the generalization capabilities of AdamW and Adam over 1800 epochs. Results showed AdamW often had lower training loss and test errors, especially with weight decay. The use of weight decay in Adam did not yield as good results as in AdamW, suggesting better generalization performance for AdamW. Total runtime affects optimal hyperparameters, with smaller weight decay values needed for greater total number of epochs. The normalized weight decay simplifies hyperparameter selection by showing similar optimal values for short and long runs. Experiments on CIFAR-10 and ImageNet32x32 datasets supported the square root scaling for weight decay values. The study found that using the same weight decay values for ImageNet32x32 as for CIFAR-10 without normalization led to significantly worse performance. Optimal normalized weight decay values were similar across different optimization methods. Additionally, experiments showed that using Adam without cosine annealing required much longer runs and still yielded comparable results to AdamW with fewer epochs. AdamWR greatly sped up AdamW on CIFAR-10 and ImageNet32x32, achieving a 15% relative improvement in test errors compared to Adam. The improvements closed most of the gap between Adam and SGDWR on CIFAR-10, yielding comparable performance. The dysfunctional use of L2 regularization and weight decay in adaptive gradient methods like Adam may lead to worse generalization compared to SGD with momentum. A simple fix proposed in AdamW variant showed substantially better generalization performance. Additionally, normalized weight decay and warm restarts in AdamWR variant improved hyperparameter selection and anytime performance. Further verification on a wider range of tasks is needed to integrate these findings into other methods. The study suggests integrating findings on weight decay into methods like Adam to improve performance. Results show Adam and AdamW follow similar curves until AdamW outperforms in later stages. Investigating the causes of this branching and its effects on the landscape is recommended. Designing a version of Adam with enhanced regularization effects in early training stages could be beneficial. In this paper, it is argued that the popular interpretation equating weight decay to L2 regularization is imprecise. The difference between the two leads to different effective rates of weight decay for algorithms like SGD and Adam, even with the same regularization coefficient. Decoupled weight decay results in optimizing different objective functions for algorithms like SGDW and AdamW, despite using the same weight decay factor. The original Adam algorithm with L2 regularization affects effective rates of weight. The original Adam algorithm with L2 regularization affects weight decay rates, hindering effective regularization. BID0 suggests using smaller initial weight norms for better generalization in deep networks. BID10 proposes batch-specific regularization techniques. Future work will explore adapting initial weight norms at each warm restart. The schedule multiplier for learning rate in the Adam algorithm is shown in SuppFigure 1. After 100 epochs, the learning rate reaches 0, and then restarts with a multiplier of 1. The learning rate decreases over 200 epochs before restarting again. Solutions obtained before restarts are recommended by the optimizer. Cosine annealing scheduling outperforms fixed learning rates. SuppFigure 5 shows the final test error of a 26 2x96d ResNet on CIFAR-10 after 1800 epochs using Adam without cosine annealing. Results are compared to AdamW in SuppFigure 3, indicating that AdamW with a smaller network and fewer epochs performs as well as Adam with a larger network and more epochs."
}