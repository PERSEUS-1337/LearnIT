{
    "title": "B1lMMx1CW",
    "content": "We present a personalized recommender system using neural network for recommending products based on customer's implicit feedback history. The system encodes historical behavior to predict future behavior using soft data split and introduces a convolutional layer to learn the importance of purchases over time. Offline experimental results show that neural networks with two hidden layers can capture seasonality changes and outperform other modeling techniques. Our model can be scaled to all digital categories, showing significant improvements in an online A/B test. Key enhancements to the neural network model and production pipeline are discussed. Additionally, we have open-sourced our deep learning library supporting multi-gpu model parallel training for building neural network-based recommenders. In this paper, the effectiveness of a multilayer neural network for personalized recommendations of products is explored. The approach aims to scale over different product categories, such as videos, music, and mobile apps. The focus is on improving the accuracy of the neural network-based personalized recommender, especially for products that have never been purchased before by a customer. The simplicity of the approach allows for scalability across various categories in the Amazon catalog. In this paper, the authors propose training a neural network model to predict all future purchases within a certain time frame, aiming to capture temporal trends in consumption patterns. Previous studies have focused on capturing seasonality changes using sequence modeling and combining predictor models to capture short-term user preferences using recurrent neural networks. In this paper, the authors propose combining a predictor model with an auto-encoder model using a feed forward neural network to capture short-term and static customer preferences. The model is re-trained daily to learn new popular products and changes in preferences, showing good performance in capturing seasonality changes. Previous studies have shown that improving neural network-based recommenders is important, with different methods requiring different production pipelines for various data sets. In this paper, the authors focus on improving neural network accuracy by using different training data splits and applying time decay for collaborative filtering. They introduce the use of convolutional layer BID20 to estimate the shape of the time decay function, which is a novel approach in recommender systems. In this paper, the authors focus on improving neural network accuracy by using different training data splits and applying time decay for collaborative filtering. They introduce the use of convolutional layer BID20 to estimate the shape of the time decay function, which is a novel approach in recommender systems. The local relation between adjacent songs is utilized for text feature extraction in BID37 BID18 and for extracting features from audio signal in BID31. Scaling the problem of recommending millions of products involves candidate generation and ranking, with ranking considering various features like context and impression. Another approach is learning product similarity using the DSSM method in BID8. The paper focuses on training an end-to-end neural network using only purchase events to simplify the production pipeline. The authors focus on modeling consumption patterns in digital products, using multi-GPU model parallelization to re-train large neural network models daily for fresh recommendations. They exclude purchased movies when computing offline and online metrics, and splitting training data can improve recommender accuracy metrics. The paper discusses how techniques can improve NN-based recommender accuracy metrics, feeding into recommendation technology at Amazon. It covers offline metrics for algorithm evaluation, NN model development, offline evaluation results, running NN models in production, and conclusions. Various metrics are mentioned, with RMSE being popular for explicit feedback, while implicit feedback like clicks is important in practical applications. In predicting future purchases from implicit feedback data, two metrics used are Precision at K and Product Converted Coverage (PCC) at K. Precision at K measures the accuracy of recommended items compared to actual purchases. To ensure diverse recommendations, Product Converted Coverage at K is used to capture the number of unique products recommended and purchased. Using held-out labels for evaluation may leak future purchase information, leading to inconsistent offline and online performance. Test metrics in this paper aim to reduce this gap and simulate real production environments. In order to bridge the gap between offline and online performance, historical consumption data is used for training and future purchases for testing. Data is split into past and future parts based on a pre-selected date, with offline accuracy metrics measured on the future part. Each purchase event is represented by one-hot encoding in vectors X, Y, Z for training and testing data. During training, historical purchase data is split into past (X) and future (Y) parts. The neural network captures seasonality changes by selecting dates within specific ranges. Purchase history noise is managed by setting limits on the number of purchases per customer. The same data split is applied during testing to predict future purchases (Z). During offline evaluation, the purchase data is limited to 10^5 products and 10^6 testing samples. The distribution of purchases is long-tailed, with 90% covered by less than 20% of products. This allows for a reduction in recommended products dimensionality without significant loss in precision. The recommender model aims to increase the dimensionality of recommended products by four times with minimal impact on precision, using sparse matrix multiplication for efficient model training. NN based approaches are used to predict customers' next purchases based on their purchase history. The model for L layers is defined by a function at each layer and a loss function, with weighted cross-entropy as the cost function. The parameters of the neural network model include training input and output data, output scores, number of layers, weight matrix, activation functions, dropout function, noise for robustness, sparsity parameter, and hyperparameters. These parameters are essential for optimizing the recommender model's performance. The hyper-parameters of the neural network model, such as weights, lambda, and beta, can be tuned through validation data. Different parameters like learning rate, momentum, and optimizer type were explored. Deeper networks did not show significant improvements. The models used two hidden layers, and during evaluation, XY data is fed to the NN model to generate output scores. The top K products are recommended after removing previous purchases of the customer. Recommendations are compared with data Z for accuracy calculation. The data split method used for training the model via multi-label classification is mentioned. The predictor model used for training via multi-label classification can produce personalized recommendations of popular products from past purchases. ReLU activation function is utilized to mitigate gradient vanishing in deep models. Increasing the depth of the neural network can lead to a decrease in Precision@1. Residual neural networks were explored to mitigate vanishing gradient effects in deep models, maintaining Precision@1 around 0.072 regardless of depth. However, it did not improve accuracy metrics compared to two-layer neural networks. Therefore, a neural network model with no more than 2 hidden layers was chosen for the experiment on AIV datasets. The effectiveness of the predictor model was evaluated against other models like FISM BID17 and Bayesian Personalized Ranking (BPR) BID27 on AIV datasets. The model utilizes Bayesian Personalized Ranking (BPR) loss with negative sampling and is tuned for different parameters like rank, learning rate, L2 regularizer, and order of Markov chains. LSTM and GRU are used for sequence processing, with tuning similar to LSTM. FastXML is used for extreme multi-label classification in the recommender system. Our approach in the recommender system is compared with Fast and Accurate Tree Extreme Multi-label Classifier BID25, showing our model's superior Precision and production CF's best PCC. We aim to enhance the NN model's Precision and PCC, focusing on LSTM for sequence processing. The lower accuracy of LSTM in our data is attributed to the lack of strong grammatical rules in purchase sequences. Our approach in the recommender system focuses on modeling properties by re-training the model daily and predicting popular purchases for the current week. LSTM, on the other hand, only recommends the next purchases without considering popularity. Using a time decay function can improve the recommender by increasing the importance of recent purchases. LSTM may perform better on product categories where the order of purchases is crucial, such as buying a game after purchasing a cell phone. The recommender system focuses on modeling properties by re-training daily and predicting popular purchases for the current week. To improve recent purchases' importance, a neural network with a convolutional layer is proposed to learn the time decay function's shape. Purchase dates are converted into binary encodings and processed through a convolutional layer to predict popular purchases. The recommender system utilizes a neural network with a convolutional layer to model properties and predict popular purchases based on daily re-training. The convolutional layer explores the shape of time decay using a parametric function, with a time decay parameter optimized through hyper-parameter optimization. Three models were trained on AIV datasets with different date splits, with the optimal time decay parameter found to be 40. The convolutional layer in the neural network models time decay functions for different date splits, showing a decreasing trend for October data and a spike for Christmas week, indicating the popularity of Christmas movies. The convolutional layer in the neural network models time decay functions for different date splits, showing a decreasing trend for October data and a spike for Christmas week. Time decay on input data slightly improves precision, but reduces PCC. The shape of the time decay function can be approximated by a parametrical function for efficiency. The predictor model has highest precision@K but lower PCC@K due to training data splitting method. The NN model cannot predict products purchased one month ago. The NN model cannot predict products purchased one month ago as they were not in the training data. A hybrid approach combining predictor and auto-encoder models is introduced to increase precision. The auto-encoder model captures static customer preferences and can predict products purchased at any time in the past, leading to an expected increase in PCC compared to the predictor model. The hybrid approach combines predictor and auto-encoder models to increase precision in predicting purchased products. Constant weights are used for balancing purchased and non-purchased products, along with time decay parameters and functions. The auto-encoder model captures static customer preferences and can predict past purchases, leading to an expected increase in PCC compared to the predictor model. The model combines predictor and auto-encoder models using a mixed cost function. The impact of the auto-encoder is controlled by a decay parameter, and time decay is applied to the input data. The model can be interpreted as multi-task learning, with tasks for predicting past and future purchases. Adjusting parameters allows for a predictor model or a mixture of predictor and auto-encoder models. The predictor model uses fixed parameters and time decay functions on input and output data. The first proposed approach is an Equally Weighted auto-encoder and predictor model, which predicts both past and future purchases with higher precision. The soft split model uses time decay on input data to predict past and future purchases, weighting the importance of past purchases with a function T d(). This model combines properties of the predictor model and the Equally Weighted model, resulting in higher precision and PCC. It is evaluated on the AIV dataset. The EW and soft split NN models are evaluated on the AIV dataset, showcasing scalability across digital product categories and capturing seasonality effects. The predictor model demonstrates high precision but lower PCC compared to other methods. Results in Figure 6a show the predictor model with higher precision and lower PCC, while the soft split model slightly decreases precision but doubles PCC. The EW model increases recommendation diversity by two times, while the soft split model boosts PCC but reduces precision significantly. Combining predictor and auto-encoder models in a soft way improves accuracy metrics on MovieLens datasets. The approach can be trained on all digital product categories without additional feature engineering, showcasing higher precision and PCC compared to existing baselines. The soft data split approach has higher precision and PCC compared to other methods, but lower PCC than the EW model. Production collaborative filtering shows higher PCC than other approaches. The NN based recommender has lower PCC due to dimensionality reduction in daily model re-training. The predictor approach models short-term user preferences and works well for popularity-driven categories like video. Soft split allows for information gathering from different patterns in various categories. Soft split model improves precision@K and PCC@K compared to other models. It is particularly effective for video data, where it outperforms production collaborative filtering. However, for eBooks and audio-books, the predictor model shows similar precision to production collaborative filtering. Soft split model excels in accuracy for both eBooks and Audible, indicating a similarity in consumption patterns between these categories. The soft split model outperforms the predictor model on accuracy metrics for ebooks and audiobooks datasets. It also shows higher Precision and PCC on Music and Apps data. Combining predictor and auto-encoder models leads to significant accuracy improvements. The model's simplicity allows for scalability across all digital categories, unlike other papers that focus on specific categories. Convergence properties in NN model training process are explored, showing how Precision and PCC change with epochs and number of purchases. Precision converges after 10 epochs, while PCC converges after 70 epochs. The NN model converges after 10 epochs, while PCC converges after 70 epochs. NN learns popular products quickly, leading to high precision. As NN learns less popular products, PCC increases significantly. Model accuracy depends on the number of customer purchases, with recommendations being more accurate for customers with more purchases. Deeper NN models do not significantly improve accuracy. Deeper NN models did not significantly improve accuracy, with two hidden layers being sufficient. Hyperparameter optimization focused on the number of hidden units, with ReLu activation showing higher accuracy. The model was re-trained daily to capture seasonality changes and tested against popular products purchased by customers. Evaluation was done on October and Christmas datasets. The model successfully captures temporal dynamics by recommending popular products based on future purchases. It can identify trends, such as recommending Christmas movies during the holiday season. Accuracy metrics like Precision@K and PCC@K are used to validate the performance of the NN models, with higher precision seen in Christmas recommendations. The NN model shows higher Precision@K and lower PCC@K for Christmas recommendations compared to October. There is a temporal variation in accuracy metrics, indicating adaptation to seasonality changes. Offline results are promising, but online A/B tests are needed for validation. The NN pipeline is designed to produce fresh recommendations daily and scale to all digital products. The NN pipeline, designed for daily fresh recommendations, consists of data pre-processing, model training, recommendations generation, and serving a database of recommendations. Multiple NN models are trained offline for different categories, with recommendations generated and stored in the database. Daily re-training allows for learning new popular items, producing different recommendations daily. Date splits are updated daily, feeding XY data into the NN model to produce output scores\u0176. The NN pipeline for daily fresh recommendations involves data pre-processing using Spark on a multi-host map-reduce cluster, controlling training time by managing training samples and removing noise, and adjusting model size by clipping the tail of the product distribution. Training the neural network model is the next step, with training time influenced by the number of recommended products and units in the hidden layer. Neural network model training involves using multi-GPU model parallelization to train large models efficiently. Data is split into batches for data parallel predictions using Spark on a GPU cluster. Predictions are generated by the trained NN model and displayed to customers through an online recommender service. This process is repeated daily to capture seasonality changes. Our pipeline, similar to BID5, pre-generates recommendations offline to support multiple categories without modifications. Data pre-processing, model training, and recommendation generation can be used sequentially or in parallel for multiple category recommenders. However, offline recommendations delay the propagation of recent purchases into our model. Online A/B tests of the NN-based recommender show a significant improvement in all categories. The text describes significant improvements in purchases of mobile apps, ebooks, and audio books on Amazon's digital platforms. A personalized neural network recommender system was launched for digital categories, with plans to expand to non-digital categories. Models using a split of customer purchases into past and future periods showed good results, with some models incorporating time decay for consumption events. The text discusses the success of a two-layer neural network model in improving offline metrics for digital categories. The model outperformed other approaches on public and internal datasets, leading to significant KPI improvements during online A/B tests. Additionally, a deep learning library was open-sourced to support multi-gpu model training for large models. Around 6200 products were purchased by customers during the specified period, with 90 percent of customers making a varying number of purchases. The text discusses customer purchases and product distributions, with 90 percent of customers making less than 400 purchases. The distribution of products in data X and Y have long tails, with 90 percent of purchases covered by 1000 products. Recommendations are made based on evaluation data XY, with new products recommended by removing previous purchases. Accuracy is calculated by comparing recommendations with future purchases in data Z. In data Z, 921 customers with specific purchase patterns are selected for testing. Predictor, soft split, and fastXML models' accuracy metrics are compared, showing predictor and soft split models outperform fastXML. Results are based on ratings converted to implicit feedback, providing an approximation of performance on real purchase history."
}