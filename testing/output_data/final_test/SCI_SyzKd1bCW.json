{
    "title": "SyzKd1bCW",
    "content": "Gradient-based optimization is crucial for deep learning and reinforcement learning. Even when dealing with unknown or non-differentiable mechanisms, using high-variance or biased gradient estimates can still be effective. A general framework has been introduced for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function. These estimators can be jointly trained with model parameters or policies in both discrete and continuous settings. They provide unbiased, adaptive analogs of state-of-the-art reinforcement learning methods like advantage actor-critic and are also used for training discrete latent-variable models. Gradient-based optimization, including back-propagation and the reparameterization trick, has been key to recent advances in machine learning and reinforcement learning. In reinforcement learning and fitting probabilistic models with discrete latent variables, backpropagation cannot be applied due to unknown functions and discontinuities. Recent work focuses on constructing gradient estimators for these situations, such as advantage actor-critic methods for reinforcement learning and continuous relaxations for discrete latent-variable models. In reinforcement learning and fitting latent-variable models, a method was developed to optimize parameters of a distribution to maximize an expectation. This method utilizes a neural network to create a lower-variance, unbiased gradient estimator applicable to various problems, even when no continuous relaxation is available. In this paper, the general problem of optimizing parameters in high-dimensional spaces is discussed. Gradient-based optimization is favored for providing individual parameter adjustments. Stochastic optimization is crucial for scalability but requires unbiased stochastic gradients to converge to the objective's fixed point. The score-function gradient estimator, also known as REINFORCE, is a standard method with unbiased but high variance estimations. The reparameterization trick creates a low-variance, unbiased gradient estimator for high-dimensional, continuous latent-variable models like variational autoencoders. Control variates are used to reduce the variance of stochastic estimations. Control variates are a method for reducing the variance of a stochastic estimator by using a known mean function. A new estimator is obtained by subtracting the control variate from the original estimator and adding its mean. This approach can be applied to gradient estimation for functions with unknown differentiability, using a combination of the score function estimator, reparameterization trick, and control variates. The LAX gradient estimator is introduced, which combines the score-function estimator and reparameterization estimator to reduce variance. It is unbiased for any choice of the surrogate function c \u03c6 and can have variance as low as the reparameterization estimator. The key is to optimize c \u03c6 to minimize variance in the estimator. To reduce variance in our estimator, we optimize the surrogate function c \u03c6 using stochastic gradient descent while optimizing the model parameters. By estimating gradients through empirical variance over mini-batches or using an unbiased single-sample estimator, we can directly minimize the variance of the gradient estimator. This approach contrasts with methods like Q-Prop and advantage actor-critic, which focus on minimizing squared error. Our algorithm jointly optimizes parameters to improve performance. The algorithm optimizes the surrogate function c \u03c6 and model parameters simultaneously to reduce variance in the estimator. It encourages c \u03c6 (b) to approximate f (b) with a weighting based on \u2202 \u2202\u03b8 log p(b|\u03b8). The objective balances the variance of the reparameterization estimator and the REINFORCE estimator. The LAX estimator can be adapted for discrete random variables by introducing a \"relaxed\" continuous variable z. The LAX estimator introduces a \"relaxed\" continuous variable z and uses the Gumbel-softmax trick. The RELAX estimator aims to achieve a more correlated control variate by replacing \u2202 \u2202\u03b8 log p(z|\u03b8) with \u2202 \u2202\u03b8 log p(b|\u03b8). Further refinement is done by evaluating the control variate at both z \u223c p(z|\u03b8) and z \u223c p(z|b, \u03b8). The estimator introduced uses a continuous variable z and the Gumbel-softmax trick. It aims to achieve a more correlated control variate by replacing \u2202 \u2202\u03b8 log p(z|\u03b8) with \u2202 \u2202\u03b8 log p(b|\u03b8). The distribution p(z|b, \u03b8) must be reparameterizable for this method. The variance-reduction objective allows for the use of any differentiable, parametric function as the control variate c \u03c6. In the discrete setting, if f is known and differentiable, the concrete relaxation can be used, resulting in the REBAR estimator. In reinforcement learning, the LAX estimator is applied to optimize policy distribution parameters to maximize rewards. The function R maps actions and states to rewards, and the goal is to estimate the gradient of an expectation of a black-box function. The approach involves using a generic function approximator like a neural network for c \u03c6 in training discrete variational autoencoders. The LAX estimator is used in reinforcement learning to optimize policy distribution parameters for maximizing rewards. It involves estimating the gradient of an expectation of a black-box function using a differentiable surrogate c \u03c6 that depends on both the action and state. The extension of LAX to Markov decision processes ensures unbiased estimation if the system dynamics are Markovian with respect to the state. The RL LAX estimator is extended to Markov decision processes, ensuring unbiased estimation when the system dynamics are Markovian. The baseline c \u03c6 (a t , s t ) is action-dependent, optimizing parameters \u03c6 using a single-sample estimator to avoid unstable training dynamics. The approach does not require storage of previous rollouts and is inspired by the REBAR method. The REBAR estimator relies on a scaling factor \u03b7 and temperature \u03bb, limited in optimization scope. It requires a known and differentiable function f, evaluating discrete loss at continuous inputs. In contrast, LAX and RELAX can optimize black-box functions without these constraints, making them suitable for reinforcement learning scenarios. These methods only need the ability to query the function being optimized and sample/differentiate p(b|\u03b8). In optimization settings, a dependence on \u03b8 can occur when training probabilistic models or adding a regularizer. Different methods like BID11, NVIL, VIMCO, and BID22 provide reduced variance gradient estimation in various scenarios. Our method is a single-sample estimator, unlike others that use different approaches to reduce variance. The curr_chunk discusses the development of gradient estimators for deterministic black-box functions and discrete optimization. Various methods, including introducing sampling distributions and optimizing objectives, are explored. Additionally, recent developments on action-dependent baselines for policy-gradient methods in reinforcement learning are mentioned. The curr_chunk introduces REBAR and RELAX gradient estimators for optimization problems, showcasing their effectiveness on challenging tasks like optimizing binary VAE's and reinforcement learning. The method is demonstrated using a toy example and compared with REINFORCE. The RELAX estimator outperforms REBAR in optimizing binary VAE's, showing superior performance on tasks with Bernoulli latent variables using linear or nonlinear mappings on datasets like MNIST and Omniglot. In experiments on MNIST and Omniglot BID8 datasets, a control variate improved training performance over the state-of-the-art baseline of REBAR. Validation performance and convergence speed were enhanced in linear models, but decreased in nonlinear models possibly due to overfitting. Further investigation is needed on this phenomenon. In experiments on MNIST and Omniglot BID8 datasets, a control variate improved training performance over the state-of-the-art baseline of REBAR. Validation performance and convergence speed were enhanced in linear models, but decreased in nonlinear models possibly due to overfitting. Further investigation is needed on this phenomenon. To obtain training curves, our own implementation of REBAR was created, showing slightly improved performance compared to BID30. RELAX demonstrated a notable improvement in convergence rate, with an increase in rate of convergence in linear models. Training curves for all models can be seen in FIG3 and in Appendix D. Table 2 shows the mean episodes to solve tasks using gradient estimators in reinforcement learning environments. The RELAX and LAX estimators are used for discrete and continuous actions, respectively, compared to the A2C algorithm as a baseline. Challenges arise in incorporating reward bootstrapping and variance reduction techniques into the model due to differences in control variate interpretation. Reward bootstrapping was omitted in discrete tasks but considered in more complex continuous tasks. In more complex continuous tasks, the value function is used for bootstrapping, with a control variate formula of c \u03c6 (a, s) = V (s) + \u0109(a, s). Experiments were conducted on Cart Pole, Lunar Lander, and Inverted Pendulum environments, showing improved performance and sample efficiency. The estimator produced policy gradients with reduced variance, allowing for larger learning rates while maintaining stability. The estimator achieved a 2-times speedup in convergence over the baseline in both discrete environments. A generic gradient estimator was proposed for expectations of known or black-box functions of discrete or continuous random variables, with little computational overhead. Simple extensions to reinforcement learning in both discrete and continuous-action domains were derived. Future applications could include training models with hard attention or memory indexing, or applying estimators to continuous latent-variable models with non-differentiable likelihoods. Extensions to reparameterization gradient estimators could increase the scope of distributions that can be modeled. In reinforcement learning, the method can be combined with variance-reduction techniques like generalized advantage estimation and optimization methods such as KFAC. The control variate can also be trained off-policy. The estimator is shown to be unbiased, with each term in the estimator accounted for separately. The RELAX estimator is used for low-variance control variate optimization in black-box gradient estimation. It requires reparameterized samples from specific distributions when applied to functions of discrete random variables. The LAX estimator is derived for continuous RL tasks, providing an unbiased estimate by sampling from specific distributions. The LAX estimator for continuous RL tasks is unbiased, derived using the score-function estimator. The policy parameterizes a soft-max distribution for action sampling, utilizing a reparametrization trick. The RELAX estimator is also discussed in the context of the score-function estimator. The text discusses the derivation of the LAX estimator for continuous RL tasks, utilizing a soft-max distribution for action sampling. It also introduces the RELAX estimator and compares it to the REBAR model in terms of validation scores and hyperparameters. The RELAX model adds more hyperparameters such as the depth of the neural network, weight decay, and learning rate scaling for the control variate. Various neural network models were tested with different layers and units, along with training the control variate with weight decay. In experiments with RELAX models, tuning the learning rate was crucial for convergence. The larger control variate architecture with weight decay of .001 and learning rate scaling of 1 performed best. Results were based on models achieving the highest ELBO on validation data. In one-layer linear models, the ELBO is optimized with specific weight matrices and bias vectors. In two-layer linear models, the ELBO is optimized with additional weight matrices and biases. The one-layer nonlinear model includes deterministic layers with hyperbolic-tangent nonlinearity. All models undergo an identical hyperparameter search. Both baseline A2C and RELAX models use two-layer neural networks for policy and control variate. For the tasks, two-layer neural networks with 10 units per layer were used. ReLU nonlinearity was applied, except for the linear output layer. Policy gradient was estimated with a single Monte Carlo sample. Models were trained with RMSProp BID28 optimizer and a reward discount factor of .99. Entropy regularization with a weight of .01 was used for exploration. A grid search was conducted for global learning rate and scaling factor on the learning rate. The experiments involved running Cart Pole and Lunar Lander tasks to determine the model that solves the task in the fewest episodes. The Cart Pole task is solved with an average reward >195 over 100 episodes, while Lunar Lander requires >200. The experiments used biased sampling for p(z|b, \u03b8) and neural networks with 64 hidden units. The policy network uses ELU nonlinearity in hidden layers and tanh nonlinearity in the output layer. It outputs the mean and a trainable log standard deviation. Batch size is set to 2500 for collecting rollouts and updating parameters. The value network is trained 25 times per policy update, while the control variate is a hyperparameter. Models are trained using ADAM BID6 with specific parameters. A2C has 2 hyperparameters for tuning the learning rate. The A2C case has 2 hyperparameters to tune: learning rate for the optimizer for the policy and value network. A grid search was done over the set {0.03, 0.003, 0.0003}. RELAX has 4 hyperparameters to tune: 3 learning rates for the optimizer per network, and the number of training iterations of the control variate per policy gradient update. The hyperparameter setting that yielded the shortest episode-to-completion time averaged over 5 random seeds was chosen. Inverted Pendulum experiments were run for 1,000,000 frames. BID31 pointed out a bug in the initially released code for the continuous RL experiments, which has been fixed in the publicly available code. In continuous RL experiments, a fixed batch of timesteps is used for fair comparison to the baseline. Calculating the variance loss for the control variate is complicated when the number of episodes is not fixed. Gradients for the control variate are computed outside the Tensorflow graph for practical reasons. It is recommended to use a fixed number of episodes in the batch when using this method."
}