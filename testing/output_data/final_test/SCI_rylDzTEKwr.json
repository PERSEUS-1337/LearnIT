{
    "title": "rylDzTEKwr",
    "content": "Hashing-based collaborative filtering generates binary hash codes for users and items to efficiently compute recommendations using the Hamming distance. However, a drawback is that all bits are equally weighted in the distance calculation, even though some bits may be more important based on the user's preferences. The proposed end-to-end trainable variational hashing-based collaborative filtering approach introduces the concept of self-masking, where the user hash code acts as a mask on items to encode important bits for the user's preferences. This allows for binary user-level importance weighting without storing additional weights. Experimental results show significant gains in NDCG compared to state-of-the-art baselines. An efficient implementation of self-masking incurs <4% runtime overhead compared to standard Hamming distance calculations. Recommender systems model user preferences to predict new items they may like. Hashing-based collaborative filtering allows for fast similarity searches by using binary vector representations and Hamming distance. This leads to storage reduction and increased efficiency in recommendation systems. VaHSM-CF is a novel variational deep learning approach for hashing-based collaborative filtering that addresses the issue of equal weighting of bits in the Hamming distance computation. It eliminates the need for additional storage and only slightly increases computation time. VaHSM-CF is a variational deep learning approach for hashing-based collaborative filtering that utilizes self-masking to optimize hash codes for user-level bit-weighting on items. Self-masking modifies item hash codes by applying an AND operation between item and user hash codes, focusing on bit differences that are important for the user. This technique ensures a user-level bitwise binary weighting when ranking items for a specific user. The curr_chunk introduces a new variational hashing-based collaborative filtering approach optimized for a self-masking technique that outperforms existing methods by up to 12% in NDCG across multiple datasets. The focus is on collaborative filtering with explicit feedback, aiming to rank pre-selected items based on user-specified ratings. The code for the model and an efficient implementation of the Hamming distance with self-masking are publicly available. Matrix factorization is a successful collaborative filtering method, but hashing-based collaborative filtering has been researched to reduce storage requirements and speed up computation. Users and items are represented as binary hash codes for efficient computation of user-item similarities using the Hamming distance. Two-stage approaches involve learning real-valued vectors first and then transforming them into binary hash codes while preserving user-item ratings. Hashing-based collaborative filtering aims to reduce storage requirements and speed up computation by representing users and items as binary hash codes. Two-stage approaches involve learning real-valued vectors first and then transforming them into binary hash codes while preserving user-item ratings. Zhang et al. (2016) propose Discrete Collaborative Filtering (DCF), a binary matrix factorization approach that directly learns hash codes using relaxed integer optimization, enforcing bit balancing and decorrelation constraints. Extensions of DCF focus on incorporating side-information. Recent work has focused on improving hashing-based collaborative filtering methods by addressing the issue of reduced representational power compared to real-valued vectors. Liu et al. (2019) propose Compositional Coding for Collaborative Filtering (CCCF), a hybrid approach that combines hash codes and real-valued weights to enhance model generalization. CCCF splits hash codes into blocks associated with real-valued scalars, allowing for more effective distance computation between codes. Our proposed variational hashing-based collaborative filtering with self-masking addresses the limitations of CCCF by allowing for disabling unimportant bits without the need for storing additional weights or vectors. This approach improves efficiency by avoiding individual Hamming distance computations and floating point multiplications for each block. Additionally, it reduces storage requirements by not needing to store real-valued weights for all blocks in a hash code. Hashing-based Collaborative filtering aims to learn binary user and item representations (hash codes) to indicate user-item similarity. The Hamming distance is used for fast hardware-level implementation. User preference for an item is specified by a rating, and the Hamming distance between the user and item representations is low when the rating is high. This computation is efficient due to fast hardware-level implementation. The Hamming distance computation is efficient due to fast hardware-level implementation. Integer-valued Hamming distances can be linear-time sorted using radix sort to create a ranked list based on user preference. However, assigning equal weights to all bits may not reflect user preferences accurately. It is challenging to encode preference weights without using more storage and computation time. The solution presented addresses the issue of encoding preference weights without additional storage. By using Hamming distance with self-masking, the user hash code encodes the importance of each bit directly, allowing for the disabling of unimportant bits on a user-level. This enables the model to produce user-specific item representations efficiently. The solution presented enables the model to produce user-specific item representations efficiently by using self-masking with Hamming distance. This approach allows for the encoding of preference weights without additional storage, maximizing the likelihood of observed ratings for users and items. To maximize the likelihood of observed ratings for users and items, latent vectors z u and z i are used as user and item hash codes. These vectors are conditioned on the user and item, sampled through Bernoulli trials. The log likelihood is derived by considering the log likelihood of ratings conditioned on the latent vectors. The objective is to maximize this log likelihood by multiplying and dividing with approximate posterior distributions. To maximize the log likelihood of observed ratings, latent vectors z u and z i are used as user and item hash codes. The goal is to maximize the conditional log likelihood of the rating while minimizing the KL divergence between the approximate posterior and prior distribution of the latent vectors. This involves maximizing the expected conditional log likelihood as a reconstruction term and minimizing the KL divergence as a regularizer. The computation of the approximate posterior distributions q i \u03c6 (z i |i) and q u \u03c8 (z u |u) and the conditional log likelihood of the rating p \u03b8 (R u,i |z u , z i ) are presented next. The approximate posterior distributions are modeled through neural networks as encoder functions for users and items, mapping them to hash codes. The encoder function for users involves computing the probability of each bit using a real-valued embedding, with the bit determined stochastically or deterministically. A straight-through estimator is used for backpropagation due to non-differentiable sampling. The conditional log likelihood is used for backpropagation in the model. The observed ratings are treated as ground truth with noise and discretized. The log likelihood is maximized to minimize mean squared error for training the model. Previous work also uses MSE objective, implying a normal distribution assumption. The reconstruction function is defined as the self-masking Hamming distance. The reconstruction function is defined as the self-masking Hamming distance, allowing the model to be fully differentiable and trained end-to-end using backpropagation. The model focuses on the reconstruction of observed ratings and does not explicitly utilize variational autoencoders for generative purposes. Evaluation is done on four publicly available datasets, including two movie rating datasets, Movielens 1M. The study evaluates the VaHSM-CF method and baselines using NDCG on various datasets like Movielens, Yelp, and Amazon. Baselines include hashing-based collaborative filtering and matrix factorisation with different binarisation strategies. In the context of evaluating VaHSM-CF method and baselines using NDCG on datasets like Movielens, Yelp, and Amazon, different binarisation strategies for matrix factorisation output are compared. Zhang et al. (2016) and Liu et al. (2019) use hashing-based approaches to learn user and item hash codes through binary matrix factorization. Koren et al. (2009) is referenced as a classical matrix factorization based collaborative filtering approach. The classical matrix factorization method by Koren et al. (2009) involves learning latent vectors for users and items. MF mean and MF median, based on MF, use mean or median for binary quantization. VaH-CF, a proposed method without self-masking, demonstrates the impact of neural variational hashing-based collaborative filtering. Training is done using the Adam optimizer, with a learning rate of 0.001, and a batch size of 400. The batch size for training is set at 400 and Gaussian noise is added to ratings to reduce over-fitting. The model is implemented using Tensorflow and experiments are run on Titan X GPUs. Hyperparameters for baselines are tuned with block sizes considered for CCCF. The method (VaHSM-CF) is compared against baselines using hash codes of length 32 and 64 bits. The method (VaHSM-CF) is compared against baselines using hash codes of length 32 and 64 bits. Significant improvements compared to the best existing baseline (DCF) are indicated by *. Results for hash code lengths of 32 and 64 bits are shown in Table 2, with the highest NDCG per column in bold. Our proposed VaHSM-CF outperforms hashing-based baselines by up to 12% across all datasets. VaH-CF without self-masking is second best, showing the benefit of a variational deep learning framework for collaborative filtering. Self-masking significantly improves hashing-based collaborative filtering. Top 3 baselines generally obtain similar scores. The top 3 baselines, including VaH-CF, show similar scores, indicating the challenge of improving performance without changing hash code usage. Real-valued MF outperforms hashing-based methods due to higher representational power. VaHSM-CF narrows the performance gap with MF, with NDCG difference below 0.01. Mean or median rounding leads to significant performance decrease, highlighting the importance of learning hash codes directly. Self-masking improves convergence rate, as shown in Figure 2a for the ML-1M dataset. Self-masking significantly improves convergence rate and NDCG on the ML-1M dataset. Training with self-masking reduces training time by a large margin. Figure 2a shows faster convergence with self-masking. Different sampling methods for hash codes during training and evaluation are compared in Figure 2b. Similar trends are observed on other datasets. The remaining datasets in the Appendix show similar trends. The effect of sampling strategy for hash codes during training and evaluation is investigated. Stochastic or deterministic sampling can be used, with stochastic training and deterministic evaluation performing the best. Stochastic sampling at evaluation performs significantly worse than deterministic sampling. Runtime analysis of self-masking is also discussed. Runtime analysis was conducted to investigate the additional cost of self-masking in Hamming distance calculations. The study implemented both standard Hamming distance and Hamming distance with self-masking efficiently in C on a machine with a 64-bit instruction set. The experiment involved computing distances between 100,000 users and items using both methods, with the average time taken for computation reported over 50 runs. The experiments were conducted on a single thread with all users and items loaded in RAM, compiled with the highest optimization level. The code was compiled with the highest optimization level, using all applicable optimization flags. The mean experiment time for computing distances was 8.0358s with Hamming distance and 8.3506s with self-masking. Self-masking added a 3.91% runtime overhead. The scalability of hashing-based methods to massive datasets was highlighted, showing a good trade-off between performance gains and overhead. Our proposed end-to-end trainable variational hashing-based collaborative filtering method optimizes hash codes using self-masking, a novel modification to the Hamming distance. This approach ignores user-specified bits, improving performance by up to 12% in NDCG across 4 datasets. The proposed method optimizes hash codes using self-masking, improving performance by up to 12% in NDCG across 4 datasets with minimal increase in recommendation time. Convergence plots show faster rate of convergence with self-masking."
}