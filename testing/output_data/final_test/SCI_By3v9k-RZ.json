{
    "title": "By3v9k-RZ",
    "content": "Deep neural networks (DNNs) have been successful in NLP tasks like language modeling and machine translation, but struggle with knowledge-intensive tasks like QA from large corpora. To address scalability issues, a new approach called the N-Gram Machine (NGM) uses symbolic meaning representations to efficiently encode knowledge and generate programs for answering questions. This method is applied to tasks like bAbI and shows promise for practical applications. The N-Gram Machine (NGM) is applied to bAbI tasks and a special version called \"life-long bAbI\" with stories up to 10 million sentences. NGM can solve these tasks accurately and efficiently regardless of story length. The system is trained end-to-end with REINFORCE and uses beam search to avoid high variance in gradient estimation. A stabilized auto-encoding objective and structure tweak procedure are used to handle the large search space. The paper introduces a new QA system that automatically infers schema and content structures from weak supervisions, aiming to address the limitations of existing deep QA models in integrating domain knowledge and scalability. The paper presents a QA system that utilizes structured storage of \"n-grams\" to represent semantics efficiently. It combines text auto-encoding and memory-enhanced sequence to sequence components for question answering. The system demonstrates scalability on large datasets and highlights the importance of symbolic knowledge storage for end-to-end learning. Question answering can be challenging, with the answer being a word in a sentence. An ideal system must identify references and relationships in text to answer questions accurately. Scalability is a major challenge for QA systems processing millions of documents. Question answering systems process millions of documents to provide fast responses. Paraphrasing is crucial for text understanding, but there are various ways to represent expressions. Open information extraction methods rely on corpus statistics, leading to data sparsity and brittleness in out-of-domain text. Vector space approaches embed text expressions into latent continuous spaces, allowing soft matching of semantics. However, they are hard to scale to knowledge-intensive tasks requiring inference with large amounts of data. Good representations can dramatically reduce the search space, but scalability remains a major challenge for practical end-to-end solutions. The proposed framework aims to address scalability issues in DNN text understanding models by using symbolic knowledge storage, specifically \"n-grams\". These n-grams can efficiently represent complex semantics and be indexed for computations at scale, leading to an end-to-end QA system with improved efficiency. The system consists of learnable components converting text into symbolic knowledge storage and questions into programs. A deterministic executor produces answers by executing the programs against the knowledge storage. Training discrete latent variable models on large datasets is challenging due to inference difficulties, but different techniques are used to learn N-Gram Machines. The N-Gram Machine (NGM) model structure consists of three sequence to sequence modules and an executor for executing programs against knowledge storage. To address the search problem in beam search during inference, an auto-encoding objective biases the knowledge encoder towards more interesting hypotheses, and a structural tweak procedure corrects inconsistencies among hypotheses to achieve rewards. This model can be trained end-to-end. The N-Gram Machine model structure includes three sequence to sequence modules and an executor for executing programs against knowledge storage. The knowledge storage consists of tuples with time stamps and symbols, each with a probability. The model can be trained end-to-end with reinforcement learning using the bAbI dataset as examples. The N-Gram Machine functions operate on n-grams instead of Freebase triples, using prefixes to retrieve symbols from knowledge tuples. Four functions are defined for the bAbI tasks: Hop, HopFR, Argmax, and ArgmaxFR. The knowledge storage for program execution is represented as \u0393, with knowledge tuples as (i, (\u03b31, ..., \u03b3N)). A program C is a list of expressions where each expression returns symbols by matching its arguments in knowledge storage \u0393. Executing expressions like (Hop V1 to) or (Argmax V1 to) on knowledge storage produces deterministic results with assigned probabilities. The model involves deterministic execution with assigned probabilities based on program and knowledge storage. Knowledge storage size does not affect program execution time. Three learnable components are trained to maximize answer accuracy and minimize story reconstruction loss. Code assist and structure tweak facilitate communication between knowledge encoder and programmer. Our N-Gram Machine utilizes three sequence-to-sequence BID43 neural network models. The N-Gram Machine uses three neural network models to define probability distributions over knowledge tuples and programs. These models include a knowledge encoder, a knowledge decoder, and a programmer that converts questions to programs. The knowledge encoder handles cross-sentence linguistic phenomena, the knowledge decoder enables auto-encoding training, and the programmer provides code assistance. The programmer can query knowledge storage for code assistance before generating each token, using the CopyNet BID16 architecture with copy and attention mechanisms. A key-variable memory enhances compositional semantics. Beam search is used to approximate expected rewards for executing programs on knowledge storage. The text discusses the challenges of optimizing OQA using beam search and introduces Stabilized Auto-Encoding (AE) to improve search space by adding an auto-encoding objective. This objective, trained through variational inference, aims to address instability caused by the strong coupling between encoder and decoder in the training process. To improve training stability, the decoder is augmented with an objective to predict data back from noisy partial observations, independent of \u03b8 enc. The knowledge decoder decodes from fixed hidden sequences to generate sentences, with reconstruction log-likelihoods serving as rewards for the knowledge encoder. KL divergence between language model and encoder is dropped for NGM computations. Structure Tweaking (ST) is a procedure used to retrospectively correct inconsistencies in knowledge tuples encoded by the knowledge encoder. It adjusts the encoder to cooperate with an uninformed programmer by performing an extra beam search during training with code assist turned off. This can help propose tweaked knowledge tuples when execution failures occur. Structure Tweaking (ST) is a procedure used to correct inconsistencies in knowledge tuples encoded by the knowledge encoder. It involves performing an extra beam search during training without code assist to propose tweaked knowledge tuples when execution failures occur. The procedure refines the search space of the knowledge storage, allowing the knowledge encoder to learn to generate tuples consistently in the future. The training objective function for Structure Tweaking (ST) involves parameters \u03b8 = [\u03b8 enc , \u03b8 dec , \u03b8 prog] and is augmented with experience replay for stability. Gradients are calculated with respect to each set of parameters to maximize the total expected reward for valid knowledge stores. The training objective function for Structure Tweaking (ST) involves optimizing parameters \u03b8 = [\u03b8 enc , \u03b8 dec , \u03b8 prog] with experience replay for stability. The model uses a coordinate ascent approach to optimize components alternately with REINFORCE. The N-Gram Machine (NGM) is applied to solve text reasoning tasks in the bAbI dataset, demonstrating the model's ability to build knowledge storage and generate accurate programs. The scalability advantage of NGMs is shown by handling longer stories up to 10 million sentences. The Seq2Seq components are implemented as one-layer recurrent neural networks with Gated Recurrent Unit BID11. During decoding, the neural networks use a beam size of 2 for per sentence knowledge tuple, 5 for knowledge store sample, and 30 for program. NGM outperforms MemN2N on all extractive question answering tasks in the bAbI dataset. Autoencoding is crucial for learning, but not sufficient alone to achieve high rewards. Multiple discrete latent structures are utilized, such as knowledge tuples. Structure tweaking is an effective way to refine the search space in QA tasks, improving performance. Auto-encoding training helps in finding useful hypotheses with a small knowledge encoder beam size. Sampled knowledge storages learned with different objectives and procedures are listed in TAB3. After structure tweaking, knowledge storages learned with auto-encoding are more informative. The tweaking procedure helps N-Gram Machine deal with linguistic phenomena like singular/plural and synonyms. Scalability advantage is demonstrated through experiments on question answering data with up to 10 million sentences. Answering time and quality of MemN2N BID42 and NGM are compared at different scales. Comparing MemN2N and NGM in terms of answering time and scalability. MemN2N scales poorly with inference time increasing linearly as story length increases, while NGM's answering time remains unaffected by story length. The difference in neural network architectures - NGM uses recurrent networks while MemN2N uses feed-forward networks - causes a crossover around a story length of 1000. Additionally, both models are applied to solve three life-long bAbI tasks to compare answer quality at scale. The performance of MemN2N and NGM is compared in terms of scalability and answering quality. MemN2N's performance decreases drastically with story length, while NGM's answering quality remains consistent. NGM's scalability advantages are attributed to its machine nature, allowing for robust program execution on stories of various lengths. The auto-encoding part of the model is similar to a text summarization model proposed by BID27. Our approach involves using a less restricted hidden space with CopyNet, stabilizing the decoder through experience replay, and using the pre-trained decoder's log-likelihood to guide the encoder training. The question answering component is similar to the Neural Symbolic Machine (NSM) but extends it by learning to generate storage end-to-end without relying on predefined knowledge bases or schema. The text discusses the limitations of existing text understanding technologies due to the inability to convert arbitrary text to graph structures for answering questions. Various knowledge bases suffer from incompleteness compared to large text corpora. The schema problem arises from traditional text extraction approaches requiring a fixed target schema, leading to difficulties in accurately answering questions. The text discusses the challenges of defining complex concepts like marriage in a schema for question answering. It suggests automatically inducing schemas from text corpora to improve question answering systems. The text discusses the challenges of integrating various components in question answering systems, such as schema definition, relation extraction, entity resolution, and semantic parsing. Existing systems rely on human annotation and supervised training for each component separately, leading to consistency issues. A more desirable solution would involve minimal human annotations and end-to-end training to optimize the quality of question answering. Recent advancements in applying deep neural networks to text understanding have shown significant progress. By embedding text expressions into a latent continuous space, complex inference and reasoning can be achieved without the need for manual schema definition. This approach simplifies system design for question answering systems and enables end-to-end training to optimize quality. However, scalability remains a challenge as analyzing all text in a corpus after receiving a question leads to at least O(n) complexity. Approaches like DrQA, which rely on a search subroutine, sacrifice the benefits of end-to-end training and are limited by text size. The human memory capacity is estimated to be between 10 to 100 terabytes, similar to a commercial search engine. The use of n-grams for knowledge storage is influenced by the Adaptive Resonance Theory, which models episodic memory as sets of symbols. The model structure resembles the complementary learning theory, suggesting that intelligent agents have two learning systems. In theory BID26 BID34 BID22, intelligent agents have two learning systems - one for structured knowledge representations and one for specific experiences. The model uses stories as episodic memories and DNN models for slow learning. The training procedure resembles hippocampal memory replay, and the structure tweak procedure is critical for success, similar to reconstructive memory theory BID35 BID7. The curr_chunk discusses the generative memory theory BID35 BID7, which suggests that individuals fill gaps in episodic memory with personal knowledge. It highlights how information is encoded without understanding its use, leading to inconsistencies later corrected through hypothesis formation. These memory \"tweaks\" contribute to training the knowledge encoder. The system presented combines text auto-encoding for symbolic representations and experience replay. The curr_chunk introduces a text auto-encoding component for symbolic representations and a memory-enhanced sequence-to-sequence component for answering questions. It demonstrates good scaling properties and robust inference on long stories. The system showcases end-to-end learning and scalability through symbolic knowledge storage. Further research is needed to explore more complex representations like Abstract Meaning Representations. Sandra journeyed through the hallway, bedroom, and garden. The question asked about Sandra's location. Various animals were mentioned with their colors. The question about Greg's color was posed."
}