{
    "title": "r1eX1yrKwB",
    "content": "In this paper, the Distribution Matching Prototypical Network (DMPN) is proposed for Unsupervised Domain Adaptation (UDA). It models deep features as Gaussian mixture distributions to measure the discrepancy between source and target domains. DMPN introduces two new domain discrepancy losses with probabilistic interpretations to learn discriminative and domain invariant features. DMPN is trained to learn discriminative and domain invariant features by minimizing classification loss on labeled source data and domain discrepancy losses. It outperforms state-of-the-art approaches in Digits Image transfer task and achieves 81.4% mean accuracy on VisDA 2017 dataset. The approach is robust to hyper-parameter changes and aims to leverage rich labeled data from related domains for Unsupervised Domain Adaptation. Domain adaptation transfers knowledge from a label-rich source domain to a label-scarce target domain. Traditional methods aim to learn domain-invariant features by minimizing distance metrics like Maximum Mean Discrepancy and correlation distance. Recent studies show that deep neural networks can learn more transferable features for domain adaptation. Recent studies have shown that deep neural networks can learn more transferable features for domain adaptation. However, current methods do not explicitly model the feature distributions of the source and target data to measure the discrepancy. Inspired by recent works, the proposed method models feature distributions as Gaussian mixture distributions to measure the discrepancy between domains. The DMPN method involves training a deep network on source domain data to generate features following a Gaussian mixture distribution. Pseudo labels are assigned to unlabeled target data, and the network is fine-tuned to minimize cross-entropy loss and domain discrepancy losses. Two new domain discrepancy losses, GCMM and PDM, are proposed to learn discriminative and domain invariant features. Extensive experiments were conducted on image transfer tasks. Our approach outperforms state-of-the-art methods in Digits Image transfer tasks and synthetic-to-real image transfer tasks. The proposed method, experiment results, and analysis are presented in Sections 3, 4, and 5 respectively. Traditional domain adaptation methods focus on feature matching and instance re-weighting to reduce domain discrepancy. In the era of deep learning, studies have shown that deep neural networks can learn more transferable features for domain adaptation. Domain adaptation layers have been embedded in the pipeline of deep feature learning to learn concurrently from the source domain supervision and some specially designed domain discrepancy losses. Recent works have added a domain discriminator to improve domain adaptation methods. Incorporating domain discriminators into deep feature learning pipelines enhances domain adaptation methods by generating domain invariant features. Explicitly modeling feature distributions allows for easier measurement of domain discrepancy and the proposal of new discrepancy losses. The Prototypical network (PN) was initially introduced for few-shot learning and has been shown to be equivalent to performing mixture density estimation on deep features. Recent studies have demonstrated the importance of modeling deep feature distributions for effective domain adaptation. In recent works by Wan et al. (2018) and Yang et al. (2018), modeling deep feature distributions as Gaussian mixture distributions has shown to improve classification performance. While Wan et al. (2018) and Yang et al. (2018) focus on classification in a single domain, Pan et al. (2019) applies prototypical networks for domain adaptation, achieving state-of-the-art results by minimizing domain discrepancies at both class-level and sample-level. Unlike Pan et al. (2019), our work explicitly models deep feature distributions as Gaussian mixture for domain adaptation. In our work, we model the deep feature distribution as a Gaussian mixture distribution for Unsupervised Domain Adaptation (UDA). The goal is to transfer knowledge from labeled source domain to unlabeled target domain using Gaussian component means as prototypes for each class. The posterior distribution of a class y given the embedded feature f can be expressed as in Eqn. 1. In UDA, the deep feature distribution is modeled as a Gaussian mixture distribution. The classification loss is computed using cross-entropy, and a log likelihood regularization term is defined. The final loss function for training a network with Gaussian mixture feature distribution includes a non-negative weighting coefficient. To match the deep feature distributions between the source and target data, Gaussian component means are matched. Pseudo labels are assigned to target samples using a network trained on labeled source data. In UDA, Gaussian component means are used to match deep feature distributions between the source and target data. Pseudo labels are assigned to target samples based on the labeled source data. The Gaussian component means help reduce domain discrepancy by ensuring that the source and target features follow the same Gaussian mixture distribution. The colors represent different classes, with dotted ellipses showing Gaussian mixture distribution of source embedded features. GCMM loss minimizes the domain discrepancy by bringing Gaussian component means closer between the source and pseudo labeled target data. PDM loss matches the pseudo target feature distribution to the source Gaussian mixture distribution, reducing domain difference. Matching the embedded target feature distribution on pseudo labeled target data further reduces domain gap. The PDM loss minimizes domain discrepancy by matching the embedded target feature distribution with the source Gaussian mixture distribution, reducing domain gap. It helps to reduce domain discrepancy by enforcing the network to learn an embedding function that produces similar feature distributions between source and target data. The DMPN model aims to reduce distribution discrepancy between the source and target domains by minimizing GCMM and PDM loss functions. These two losses complement each other to align feature distributions. The training objective involves learning discriminative features from labeled source data and matching embedded feature distributions between domains. The model is trained by pre-training with labeled data and further optimization using mini-batch gradient descent. The mini-batch gradient descent algorithm is used to optimize the network on labeled source data. Pseudo labels are generated for unlabeled target data using learned source distribution parameters. Filtering and weighting techniques are applied to improve the self-labeling process. Our method proposes to weight the contribution of each sample based on predicted probability for discrepancy loss. During inference, the learned embedding function is applied to target data to calculate class probabilities and make predictions. This approach is a form of Supervised Domain Adaptation (SDA) where pseudo labeled target data is used in training to generalize the problem. Our method in Supervised Domain Adaptation (SDA) addresses the problem of noisy labeled target data. By minimizing a convex combination of empirical source and target error, we can bound the target error. The noise ratio of the target labeling function is denoted as \u03c1, and the target error can be bounded accordingly. In Supervised Domain Adaptation (SDA), the target error is bounded by minimizing domain discrepancy losses and improving classifier accuracy for target data to reduce noise ratio \u03c1. Empirical verification shows \u03c1 decreases as accuracy improves. The MNIST dataset has 70k images, while the USPS dataset has 9.3k images. The SVHN dataset contains 100k real-world house number images from Google street view. Adaptation is done between MNIST, USPS, and SVHN datasets using a standard evaluation protocol. Transfer is done in three directions: M \u2192 U, U \u2192 M, and S \u2192 M. Adaptation and evaluation are done using specific sample sizes from each dataset. The CNN architecture used is a modified version of LeCun et al. (1998) with a diagonal structure and a prior probability of p(c) = 1/C. The network is pre-trained on labeled source data with a prior probability of p(c) = 1/C. Trade-off parameters are set to \u03d5=0.1, \u03b1=1, and \u03b2=0.1. The embedding dimension d is set to 10/512 for image transfer tasks. DMPN is implemented using Pytorch with specific training parameters like ADAM optimizer, weight decay, momentum, and mini-batch size. The network is trained for 350 epochs with a specific learning rate schedule. Weighted PDM loss is applied for Digits Image transfer tasks to address labeling errors. Weighted PDM loss is applied to address labeling errors in the synthetic-to-real image transfer task, using target examples with a predicted probability over 0.8 for training. Evaluation metrics include classification accuracy for Digits Image transfer tasks and average per class accuracy for synthetic-to-real image transfer. A comparison with Source-only and Tzeng et al. (2017) approaches will be provided. Different methods for domain adaptation include JAN, MCD, CDAN+E, S-En+Mini-aug, TPN, DMPN, and Train-on-target. JAN aligns network activation distributions across domains, MCD uses task-specific decision boundaries, CDAN+E adds a conditional adversarial classifier, S-En+Mini-aug modifies temporal ensembling, TPN applies PN for UDA, and DMPN introduces GCMM and PDM losses. Train-on-target is an oracle method. Our proposed method, DMPN, outperforms all existing methods in the Digits Image transfer tasks. It has improved accuracy for M \u2192 U, U \u2192 M, and S \u2192 M by 2.6%, 0.7%, and 3.8% respectively compared to the second best method. We added batch normalization layers to the original CNN architectures for the S \u2192 M task due to convergence reasons. Additionally, we re-ran experiments on other methods by adding batch normalization layers for fair comparison. For ADDA, adding batch normalization improved accuracy from 76.0% to 83.6%, an increase of 7.6%. Our method, TPN, achieved an accuracy of 96.8% on the task, making it difficult for other methods to surpass even with batch normalization layers. In the Synthetic-to-real image transfer task, our method increased the state-of-the-art single model mean accuracy by 1.0%. TPN gen reduces domain discrepancy by minimizing RKHS distances among prototypes of the source-only, target-only, and source-target data. In DMPN GCMM, we minimize the L2 distance between Gaussian component means of the source and target data, simplifying the calculation of our proposed GCMM loss. The DMPN GCMM loss simplifies distance calculation in a Linear RKHS space, leading to a 3.0%, 1.1%, 6.3%, and 6.6% gain in accuracy. Combining GCMM and PDM losses in Table 1 shows compatibility and improved accuracy. DMPN GCMM outperforms other domain adaptation methods, with decreasing losses and increasing accuracy during training. The paper introduces the Distribution Matching Prototypical Network (DMPN) for Unsupervised Domain Adaptation (UDA), which explicitly models and matches the deep feature distribution of the source and target data. The experiment results demonstrate the robustness of the method against hyper-parameter changes and confidence threshold variations. Our work introduces new domain discrepancy losses for Unsupervised Domain Adaptation (UDA) by modeling the deep feature distributions of source and target data as Gaussian mixture distributions. Experiment results on VisDA 2017 dataset show robustness against hyper-parameter changes. Additionally, on the OfficeHome dataset, our method achieves the best accuracy results in all transfer tasks after training the network for 100 epochs. In this experiment, the network is trained for 100 epochs with a learning rate initially set to 1e-5 and decayed by 0.1 at epoch 60 and 80. The parameter \u03b3 is set to 10, and p changes from 0 to 1 during the training process."
}