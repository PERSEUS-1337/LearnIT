{
    "title": "HJlMkTNYvH",
    "content": "Many large text collections exhibit graph structures, such as co-author networks, citation networks, and named-entity-cooccurrence networks. Social networks can also be extracted from email corpora, tweets, or social media for visualization purposes. Incorporating text and graph data for visualizing semantic information and network relationships. Introducing a novel algorithm for positioning documents and graph nodes in a two-dimensional landscape. Our approach effectively captures the semantics of large document collections better than other visualizations based on content or network information. Data from social media, chat apps, and emails exhibit duality as text and graph. This duality is also seen in datasets like bibliometrics with co-author and citation networks. When analyzing these datasets, the focus is usually on either content or graph structure. When analyzing datasets, the focus is often on either content or graph structure. Data-driven journalism and computational forensics deal with large, unstructured datasets to uncover patterns and evidence of criminal activity. Auditing firms and law enforcement sift through vast amounts of data involving communication networks and documents. MODiR is a scalable multi-objective dimensionality reduction algorithm that can generate an overview of entire text datasets with network information in a single interactive visualization. It aims to support users in quickly gaining insights into large and heterogeneous data, especially in cases where manual investigation is impractical. The goal is to combine network layouts with dimensionality reduction of high-dimensional semantic embedding spaces to provide an intuitive, two-dimensional representation of both the graph and text in a single visualization. This aims to improve exploration of a corpus by users unfamiliar with the domain and terminology. MODiR introduces a novel approach to transforming high-dimensional data into two dimensions by optimizing multiple constraints simultaneously. It utilizes a hypergraph to minimize computational complexity and demonstrates effectiveness with real-world datasets across different domains. MODiR introduces a novel approach to transforming high-dimensional data into two dimensions by optimizing multiple constraints simultaneously. It bridges the gap between text and network visualization by jointly reducing the dimensionality of input data. The integrated view of datasets exhibiting duality is shown to be superior to approaches focusing solely on text or network information for visualization. Other tools like LINE and VOSviewer also combine graph structure with textual elements but lack dimensionality reduction and a focus on visualization. In our work, we project textual data into a latent space for document visualization, allowing users to quickly gain insights into topics and trends. Tiara extracts topics and time-sensitive keywords to depict evolving subjects over time. Chen et al. developed an algorithm to reduce dense visualizations by selecting representative documents. Fortuna et al. (2005) and Friedl et al. (2014) use different approaches for visualizing documents, with the former generating heatmaps with salient phrases and the latter drawing clear lines between regions. Le & Mikolov (2014) propose a map analogy for visualizing document contents in a high dimensional semantic space. Cartograph (Sen et al., 2017) offers a visually similar approach with pre-rendered information at different resolutions. The text discusses the use of a tiling server with map technology for interactive document landscape interactions. It mentions the use of node-link graphs for visualizing networks and newer approaches like ForceAtlas2 for better representation of structures. The focus is on visualizing text data with graph information, but the model can work with any type of data. MODiR can work with various types of data by projecting it into a high-dimensional vector space for semantic similarity interpretation. Text can be represented as bag-of-words vectors or embeddings that preserve semantic meaning. Different methods, such as neural architectures and structural neighborhood analysis, are used to learn representations for nodes in a network. Joint representations of network structure and document contents have been attempted but showed no improvement over traditional models. The goal of dimensionality reduction is to represent high-dimensional data in a low-dimensional space while preserving the characteristics of the original data. Linear models like Principle Component Analysis (PCA) are commonly used for this purpose, as they efficiently reduce input spaces and improve downstream task performance. These methods are often used for feature extraction and visualization in two dimensions. Nonlinear dimensionality reduction techniques like Sammon mapping and Stochastic Neighbour Embeddings (SNE) aim to preserve data characteristics in two dimensions better than PCA. SNE models inter-point distances as probability distributions, with t-distributed SNE showing competitive results in visualizing datasets. FltSNE is an optimization of tSNE that addresses its nondeterministic nature. Recently, FltSNE was proposed as an optimization of tSNE, reducing computational complexity. Other newer dimensionality reduction algorithms like LargeVis and UMAP scale almost linearly by using efficient nearest neighborhood approximations and spectral embeddings. Visualizations of complex datasets are limited to two or three dimensions for easier understanding. Multiple entities are integrated into a joint visualization called a landscape, consisting of a base-layer with all documents depicted as dots forming the document landscape, and a graph layer with nodes and connections. The MODiR algorithm integrates multiple objectives to optimize the layout of data in different layers. It uses high-dimensional vectors for documents and entities, with links as constraints. A hypergraph is created based on network information from the document corpus. The MODiR algorithm integrates multiple objectives to optimize data layout in different layers using high-dimensional vectors for documents and entities. A hypergraph is created based on network information from the document corpus, with entities positioned at the center of their respective documents. MODiR uses a projection matrix R 2\u00d7n based on multiple objectives \u03d5 {1,2,3} weighted by parameters \u03b8 {1,2,3} to balance graph and document landscape effects. Objectives include positioning similar documents near each other, separating unrelated ones, and attracting documents based on co-occurrence in hyperedges. Objective (1) in MODiR aims to position similar documents near each other on the document landscape using document embeddings inspired by word2vec. This approach allows for efficient usage of context words and retains additional information when visualizing only part of the data. In MODiR, the context of a document is defined by its k nearest neighbors in the embedding space. The objective is to position similar documents close together and dissimilar documents apart. Negative examples are introduced to prevent all documents from being projected onto the same point. Connected entities are placed near each other on the two-dimensional canvas. The document embedding in MODiR aims to position related documents close together and dissimilar ones apart by using hyperedges to connect entities and attract related documents through entities, reducing computational complexity for dense graphs. The algorithm for document embedding in MODiR uses a heuristic with a subset of documents from the context to calculate positions of entities and documents on the landscape. Hypergraph H X is constructed with document contexts and relevant pairwise distances are stored in an adjacency matrix for computational efficiency. Negative neighbourhoods are randomly sampled and masked during training for more efficient processing. Repetitive computations are moved to pre-processing to optimize the algorithm. The algorithm for document embedding in MODiR optimizes computations by using Hierarchical Navigable Small World graphs (HNSW) for efficient nearest neighbour search. Pre-processing complexity is reduced to O(n log n) and each iteration complexity to O(kln). Gradient descent updates the projection matrix W to minimize error \u03a6. Selecting appropriate hyperparameters like k, l, s, and \u03b8 is crucial. Setting l = k yields the best results, ensuring each similar document has one dissimilar document for comparison. Hyperparameters k and s are dynamically set based on user-defined perplexity, inspired by tSNE. Our approach optimizes computations using HNSW graphs for efficient nearest neighbor search and dynamically sets parameters based on user-defined perplexity. The focus is on exploring business communication data, with evaluation on research publications and co-authorship networks. Results of dimensionality reduction will be compared qualitatively to baselines and quantitatively in experiments. Our approach involves optimizing computations using HNSW graphs for efficient nearest neighbor search and dynamically setting parameters based on user-defined perplexity. We focus on exploring business communication data, evaluating research publications and co-authorship networks. Experimental Setup includes using the Semantic Scholar Open Corpus (S2) and a subset of 24 hand-picked authors for in-depth comparisons. Baselines such as tSNE and PCA are used for traditional dimensionality reduction, with a two-fold quantitative evaluation. The authors provide parameter settings for tSNE and MODiR, with details on perplexity, neighborhood size, negative context size, and objective weights. The speed of convergence depends on the learning rate and dataset size, with early stopping as an option. MODiR generally converges after 10 to 200 iterations with a fixed learning rate, but a higher rate may be needed for larger datasets. For larger and more connected data, it is recommended to start with a higher learning rate in the first epoch and then gradually reduce it. Stochastic neighbor embeddings can effectively capture intrinsic structures in two-dimensional representations. The evaluation involves comparing k-means++ clustering in high-dimensional and two-dimensional spaces. The number of clusters is set to the number of research communities, and the percentage of papers for each community per cluster is calculated. Results from clustering using tSNE, PCA, MODiR, and original high dimensional embeddings show that even original embeddings struggle to accurately cluster due to topical overlap of communities. There is a significant difference between AM and S2, likely because S2 is larger and contains more recent papers. PCA layout options are discussed for readability and document positioning within the network layout. The document landscape is laid out with nodes positioned at the center of associated documents. tSNE is applied on papers and authors together, aiming to produce a good document landscape and network layer layout. Noack's normalized AtEdge-length metric is used to measure space utilization in graph layouts. Results are presented in Table 1. The AtEdge metric is used to measure graph density, with results shown in Table 1. PCA outperforms tSNE in producing more densely clustered layouts, while MODiR performs even better due to an optimized network layout. Qualitative evaluation using the Semantic Scholar dataset visually compares different baselines and MODiR in Figure 1. In Figure 1, document-focused baselines and MODiR are compared using a weighted co-authorship network. The graph shows active collaboration, distinct research communities, and spatial separation of researchers. The ML researchers are separated from the NLP community, which appears overcrowded. In Figure 1, the network layout shows distinct research communities and spatial separation of researchers. Papers are positioned based on research areas, with some scattered across the landscape. Semantic nuances between papers are lost, and the overlap between ML and NLP communities is not noticeable. The collaboration network layout in Figure 1e shows a good organization, with coherent research interests between authors. However, there are issues with readability, such as overlapping edges and colliding research groups. Authors are represented as virtual documents to improve the network layout, reducing overlap and mitigating the problem with collapsing research groups. The semantic overlap between ML and NLP is captured well. The collaboration network layout in Figure 1e is well organized, with coherent research interests between authors. The semantic overlap of ML and NLP is nicely captured, distinguishing the three research communities. The document landscape shows distinct clusters of papers from different fields, with a slight overlap due to semantic similarities. The visualization reveals that ML and NLP communities are closely related to each other, both using machine learning, more so than to the DB community. However, authorship information is only available through interaction, not in the static visualizations. The document embeddings of articles reflect semantic similarities, but when combined with information from the co-authorship network, they become more meaningful in a joint visualization. Three principles for balancing visualizations were identified, leading to formal objectives used in a gradient descend algorithm to generate landscapes with embedded texts positioned based on semantic similarity. MODiR is a novel multi-objective dimensionality reduction algorithm that optimizes document and network layout to create insightful visualizations. It outperforms state-of-the-art algorithms like tSNE and provides balanced results across various metrics. An initial prototype for intuitive and interactive exploration of multiple datasets has been implemented. Semantic Scholar and AM are large corpora with over 45 million articles, covering various scientific fields such as computer science, neuroscience, and biomedical research. Unlike DBLP, they include not only bibliographic metadata but also abstracts for most articles. The S2 dataset was reduced to 24 hand-picked authors for in-depth comparisons. This reduction helped remove unrelated articles and studies, creating a more focused dataset for analysis. The characteristics of networks vary greatly based on the type of corpus. Email corpora have a larger number of documents attributed to fewer nodes with high variance. Academic corpora show a relatively low number of documents per author. The news corpus has one entity linked to all others and documents. Different hyperparameter settings for MODiR were explored, with context sizes being the most important parameters. Small numbers for k, l, s generally perform better. In experiments, small numbers for k, l, s perform better. For datasets with a few thousand samples, k should usually be below l. Negative context is best with l = 20 for all sizes. Set \u03b8 1 = \u03b8 2 = 1.0 for all experiments. Graph context is set to s = 10. Objective weight can be adjusted between 0.8 \u2264 \u03b8 3 \u2264 1.2. The choice of s is more influential than \u03b8 3 in the entity network."
}