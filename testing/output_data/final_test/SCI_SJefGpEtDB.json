{
    "title": "SJefGpEtDB",
    "content": "This paper proposes two approaches to replace mixed-precision arithmetic with half-precision arithmetic during a large portion of training. The first approach achieves slightly slower accuracy ratios than the state-of-the-art by using half-precision for over 99% of training. The second approach maintains the same accuracy as the state-of-the-art by dynamically switching between half- and mixed-precision arithmetic, using half-precision for more than 94% of the training process. The use of Deep Neural Networks (DNNs), particularly Convolutional Neural Networks (CNNs), is widespread in various fields such as computer vision, speech recognition, and language translation. Despite their remarkable pattern detection capabilities, training DNNs requires exposing a large amount of samples multiple times, leading to high training costs in terms of resources. To reduce these costs, approaches using simpler data representation formats have been proposed. To reduce the high training costs of Deep Neural Networks (DNNs), approaches using data representation formats simpler than Floating Point 32-bit (FP32) have been proposed. These methods combine half-precision and single-precision compute during training to maintain model accuracy while reducing compute and memory costs. Mixed-precision (MP) arithmetic accelerates linear algebra operations by accumulating half-precision input operands to generate 32-bit outputs, effectively reducing resource usage. In this paper, new training methodologies are proposed to use half-precision for a large part of the training process, improving compute and memory bandwidth requirements. Two approaches are suggested: one statically assigns Brain Floating Point 16-bit (BF16) or FP32 format to model parameters, while the other dynamically switches between BF16 and MP during training. These approaches eliminate the need for mixed-precision arithmetic, delivering the same performance as half-precision operations. This paper demonstrates the extensive use of half-precision during DNN training without the need for mixed-precision arithmetic. Mixed-precision training, which combines FP16 and FP32 datatypes, has been explored previously. However, this new approach allows for the use of half-precision throughout the training process, maintaining model accuracy. The FP32 representation of network weights and biases is maintained during training, requiring additional computations to convert FP32 values to FP16 without range issues. Nvidia Tesla V100 GPUs utilize tensor cores for mixed-precision computing, allowing for the multiplication of FP16 parameters and storage of results in FP32. The mixed-precision Fused Multiply-Add (FMA) instruction combines FP16 and FP32, with FMA instructions constituting a significant portion of the training workload for CNN models. The recent approach proposes mixed-precision arithmetic combining BF16 and FP32, which simplifies the conversion from full to half precision. This approach also processes WU and BN layers with FP32. Mixed-precision FMA instructions offer benefits in terms of memory bandwidth and register storage compared to FP32 FMAs. There is potential for further improvement with an entirely BF16 FMA. There is potential for further improvement with an entirely BF16 FMA, which offers benefits in memory bandwidth and register storage compared to FP32 FMAs. BF16 FMA instructions could significantly increase FMA instructions per cycle ratio and SIMD vectorization capacity. This paper describes techniques to fully utilize half-precision arithmetic for training DNNs. This paper presents techniques to fully utilize half-precision arithmetic for training DNNs, focusing on the use of BF16 FMA instructions to improve memory bandwidth and register storage. The analysis includes three CNN models: AlexNet, Inception V2, and ResNet-50, showing a significant portion of floating-point instructions in the training workload. FMA instructions are a significant part of training workloads for CNNs like AlexNet, Inception V2, and ResNet-50, with a focus on half-precision arithmetic for performance improvement. Previous research highlights the importance of 32-bit arithmetic in Weight Updates and Batch Normalization layers. Experimental results show that the number of instructions for these layers is minimal compared to FP instructions. Reducing the cost of FMA instructions has the potential for significant performance gains. Reducing FMA instruction costs can greatly improve performance for AlexNet, Inception, and ResNet-50. Using half-precision BF16 for training can double SIMD vectorization throughput and reduce memory bandwidth requirements. Using half-precision BF16 for training can improve performance for AlexNet, Inception, and ResNet-50 by reducing FMA instruction costs and memory bandwidth requirements. While not suitable for all CNNs, it shows promising results for Inception V2 model, achieving similar accuracy as FP32. Some CNNs may still require FP32 arithmetic during training, as shown in the comparison of training techniques for ResNet-50. The BF16 approach (referred as BF16 in Figure 2b) performs FMA instructions in BF16 except for WU and BN, displaying lower accuracy than MP and FP32. A methodology is described in Section 5 to dynamically switch between MP and BF16 for the same accuracy as MP while relying on BF16 FMAs. Algorithm 1 shows a high-level pseudo-code for this proposal. The Exponential Moving Average (EMA) of the training loss is computed using BF16 FMAs, with a threshold parameter for reduction. If the reduction is significant, BF16 arithmetic is used for a certain number of batches before switching back to MP arithmetic. The experiments were conducted on Intel Xeon Platinum 8160 processors with optimized libraries for numerical kernels. The experiments were conducted on Intel Xeon Platinum 8160 processors with optimized libraries for numerical kernels. To define and run the experiments, the pyCaffe python interface was used. Emulation techniques were employed due to the lack of available hardware implementing the BF16 numerical format. A binary analysis tool based on PIN 3.7 was developed to adapt numerical operands to the targeted numerical data format, working seamlessly on frameworks like PyTorch, Tensorflow, or Caffe. The binary analysis tool, compatible with frameworks like PyTorch, Tensorflow, or Caffe, can dynamically change operation modes and intercept dynamic instructions for floating-point operations. It rounds operands to BF16 using the RNE algorithm and can be invoked from a python high-level interface to mitigate overhead. The binary analysis tool can dynamically change operation modes and intercept dynamic instructions for floating-point operations. It rounds operands to BF16 using the RNE algorithm and can be invoked from a python high-level interface. Two optimizations are implemented to reduce overhead, including vectorizing truncation and rounding routines via AVX512 instructions and avoiding redundant operations by identifying instructions sharing input operands. These optimizations decrease the tool's overhead from 100\u00d7 to 25\u00d7 on real hardware. The paper discusses static and dynamic training techniques, with static schemes using the same data representation form for a parameter during its execution. The paper discusses static and dynamic training techniques for floating-point operations. FMA instructions in WU and BN layers use FP32 precision, while the remaining instructions use mixed-precision or BF16 operands. The dynamic method proposed switches between mixed-precision and BF16 techniques during training to improve convergence properties. The EMA threshold is set at 4% for average EMA reduction when using FP32. The paper discusses static and dynamic training techniques for floating-point operations, with the EMA threshold set at 4%. The minimum number of batches for BF16 is 1,000 to reduce unnecessary transitions. The numBatchesMP parameter is set to 10 to maintain benefits in convergence. The study evaluates AlexNet, Inception V2, and ResNet50 models using a reduced ImageNet database with 256,000 images. The study evaluates AlexNet and Inception models using a reduced dataset of 256,000 images divided into 200 categories for training and 10,000 images for validation. AlexNet is trained for 32 epochs with a batch size of 256 and a base learning rate of 0.01. Inception is trained for 16 epochs with a batch size of 64 and a base learning rate of 0.045. The training process for ResNet-50 involves 16 epochs with a batch size of 64 and a base learning rate of 0.05. The model utilizes residual blocks and the MSRA initializer to address vanishing gradients. Results from the evaluation campaign are shown in Figure 3 and Table 1, displaying accuracy over epochs and test accuracy for different network models. The AlexNet model demonstrates good response to lower precision numerical data types, with techniques like MP, Dynamic, and BF16 converging in accuracy. Dynamic achieves the same accuracy as FP32 and MP while using BF16 for 94.60% of FMAs, showing potential for improvement by reducing BF16 usage. The BF16 static technique maintains high precision but experiences a drop in accuracy with additional BF16 FMAs. The validation accuracy fluctuates during training for the Inception V2 model due to its structure and hyperparameters tuning. Dynamic approach shows robust response to changes, reaching a top-5 accuracy of 92.02% after 16 epochs. Evaluation on ResNet-50 demonstrates effectiveness of Dynamic approach on deeper CNNs, achieving state-of-the-art levels of precision with half-precision for 96.4% of FMA instructions. Top-1 accuracy drops just 1.2% compared to BF16 approach after 32 epochs. The dynamic approach shows robust response to changes, achieving a top-5 accuracy of 92.02% after 16 epochs for the Inception V2 model. A sensitivity analysis was conducted for parameters in Algorithm 1, with a focus on numBatchesMP set to 10. Testing different combinations of numBatchesBF16 and emaThreshold showed a total of 9 configurations, with the configuration {10, 1000, 0.04} used for evaluation. Figure 4 displays the accuracy obtained for each configuration during the sensitivity analysis. The sensitivity analysis tested 9 configurations, including Dyn-<emaThreshold> <numBatchesBF16>, BF16, MP, and FP32 executions. Dynamic techniques show accuracies above BF16, approaching MP and FP32 as training progresses. emaThreshold is crucial for precision changes based on loss improvement or degradation. Dynamic fixed-point is effective for training deep neural networks. Our proposals target deeper neural networks than previous approaches, utilizing dynamic data representation schemes for computations. Stochastic rounding has shown benefits for 16-bit fixed-point operations in neural networks. The authors analyze the benefits of stochastic rounding in a custom fully connected neural network and a CNN similar to LeNet-5. Previous approaches used 8-bit floating point numbers and a combination of 8-bit and 16-bit with stochastic rounding for state-of-the-art results. The BF16 numerical format has been applied to specific-purpose hardware targeting deep neural networks. The Dynamic approach can be applied on top of mixed-precision techniques to reduce computing costs. This paper analyzes the instruction breakdown of workloads focused on deep neural network training that rely on mixed-precision training. Two approaches based on half-precision FMAs are proposed to accelerate training without compromising accuracy. The first approach uses BF16 FMAs for most of the workload, achieving strong performance improvement. The second approach dynamically switches between different data representation formats, using BF16 for around 96% of the FMAs while maintaining precision levels. This paper introduces two proposals for utilizing half-precision FMAs in deep neural network training, demonstrating that it can be used extensively on \u226594% of all FMAs without the need for mixed-precision arithmetic."
}