{
    "title": "rJlnxkSYPS",
    "content": "The paper proposes a framework using semi-supervised models to enhance unsupervised clustering by generating high accuracy pseudo-labels through an ensemble of deep networks. This iterative approach outperforms state-of-the-art clustering results for various image and text datasets, achieving 54.6% accuracy for CIFAR-10 and 43.9% for 20news. The paper explores the use of semi-supervised models to improve unsupervised clustering by creating accurate pseudo-labels from unlabelled data. It aims to surpass current clustering results for image and text datasets, achieving high accuracy rates. To autonomously create a high accuracy pseudo-labelled data set, an ensemble of deep networks is trained in an unsupervised manner to cluster input data points. By comparing data points across networks, pairs belonging to the same class are identified with high precision. A similarity graph is then generated using these high-quality pairs, from which tight clusters of data points are extracted as pseudo-labels. This process does not involve clustering the entire dataset. Our method extracts unambiguous samples from a subset of data points to serve as pseudo-labels for semi-supervised learning. Ladder networks are not suitable for initial unsupervised clustering due to potential output degeneration, so we introduce Ladder-IM with information maximization for unsupervised clustering. The Ladder-IM and Ladder-Dot models show improvements over previous state of the art in unsupervised learning. The iterative approach of using high-quality clusters as labels for training new semi-supervised models leads to significant gains in accuracy, up to 17%. The pseudo-semi-supervised learning approach named Kingdra is versatile and effective on various types of datasets, as demonstrated with image and text data in Section 5. Kingdra is a versatile unsupervised classification technique that outperforms current state-of-the-art deep clustering methods on image and text datasets like CIFAR10 and 20news, achieving accuracy gains of 8-12%. Various techniques for generating pseudo-labels have been proposed in the literature, such as using the output class with the highest softmax value or other methods. In this section, the authors evaluate different techniques for generating pseudo-labels, such as using K-means clustering or treating softmax output as confidence. These techniques have not been applied in the context of unsupervised clustering using semi-supervised models. The authors test these pseudo-labeling approaches on a semi-supervised model called Ladder-IM and train it on MNIST and CIFAR10 datasets using only unsupervised loss terms. The authors evaluate different techniques for generating pseudo-labels on a semi-supervised model called Ladder-IM. They use K-means clustering and other methods to provide initial pseudo-labels to the datasets. The model is then trained with supervised loss terms based on these pseudo-labels. The process is iterated until the pseudo-label accuracy stabilizes, resulting in a final clustering accuracy. Initial and final accuracy results for the approaches are shown in Table 1. For MNIST, the unsupervised clustering accuracy of Ladder-IM is 95.4%. The unsupervised clustering accuracy of Ladder-IM is 95.4% for MNIST. Pseudo-labels from K-means and threshold approaches result in lower initial label accuracy. Using these low-accuracy pseudo-labels for further training leads to final clustering accuracies of 60.9% and 91.6% respectively. For CIFAR10, Ladder-IM clustering accuracy is 49%, which remains the same under Argmax. Pseudo-label accuracy using the K-means approach pulls down the final accuracy to 44.8%. Threshold results in a slightly higher initial accuracy of 60.5%, but it doesn't improve the final clustering accuracy for CIFAR10. Using pseudo-labels as supervision can impact final clustering accuracy. High accuracy of initial pseudo-labels is crucial for improving clustering accuracy. Current approaches for identifying pseudo-labels do not deliver high accuracy. Ji et al. (2019) maximizes mutual information between predicted labels of images. Self-supervised learning uses auxiliary tasks with self-generated labels for useful representations. Many methods use spatial information of image patches for this purpose. Our method uses correlation between outputs of input points across an ensemble as a supervisory signal to generate self-supervised pseudo-labels. Semi-supervised learning approaches use sparse labeling of data points, such as label propagation based on nearest neighbors or adjusting label probabilities gradually. Lee (2013) adjusts label probabilities gradually, starting with true labels and incorporating pseudo labels. Rasmus et al. (2015) uses a denoising autoencoder with impressive performance. Tarvainen & Valpola (2017) employs an averaged model as a teacher. Other methods like Xie et al. (2019) and Berthelot et al. (2019) utilize data augmentation and domain knowledge. Miyato et al. (2018) and Shinoda et al. (2017) combine virtual adversarial training with classification loss for semi-supervised classification. However, joint training with unsupervised losses is not effective. Ladder networks work without domain-dependent augmentation, suitable for image and text datasets, and easily trained with supervised methods. Unsupervised ensemble learning involves training models with unlabeled samples, constructing a graph to model agreement, generating high confidence clusters, creating pseudo labels, and training models iteratively. This approach is general and can work with various semi-supervised methods. The Kingdra method involves unsupervised training of an ensemble of models using a novel Ladder-* model, constructing a similarity graph based on model agreement, extracting tight clusters of data points as pseudo-labels, and iteratively generating final clusters. This approach aims to improve semi-supervised training accuracy. The Kingdra method focuses on improving semi-supervised training accuracy by using unsupervised training of an ensemble of models, including a novel Ladder-* model. Pseudo-labels are generated from tight clusters of data points, serving as training data for the semi-supervised training of the ensemble. Multiple iterations are performed for continued improvement. The accuracy of the base model directly impacts the final model's accuracy, with the Ladder-* model outperforming other unsupervised models in most datasets. The ladder architecture has not been used for unsupervised clustering due to potential degeneracy issues. To address this, unsupervised loss terms are added to direct the network to give similar outputs for similar inputs while maximizing diversity. Two variants, Ladder-IM and Ladder-Dot, incorporate IM loss or dot product loss to achieve this objective. The IM loss is based on mutual information between input X and output Y, maximizing entropy and conditional entropy. The Ladder architecture incorporates unsupervised loss terms like IM loss and dot product loss to encourage diverse class assignments. Ladder-IM performs better overall, but Ladder-Dot with Kingdra iterations outperforms when there is a class imbalance. Dot product loss is agnostic to class sample sizes, giving it an advantage in such scenarios. The Ladder architecture uses unsupervised loss terms like IM loss and dot product loss to encourage diverse class assignments. Kingdra utilizes an ensemble of Ladder-* models to enhance unsupervised learning performance. Ensembling in unsupervised learning is challenging due to the absence of stable class assignments across different models. To address this, a simple approach of looking at pairs of data-points is proposed for clustering with high confidence. The proposed method uses a pairwise approach to create clusters with high confidence based on majority agreement among ensemble models. A graph is constructed with nodes representing input data-points, connected by strong positive edges when models agree on class predictions, and strong negative edges when they disagree. The method creates clusters based on ensemble model agreement, with strong positive edges indicating same-class predictions and strong negative edges indicating different-class predictions. Clusters are formed from cliques of strong positive edges, representing high-confidence same-class data-points. The method creates clusters based on ensemble model agreement, with strong positive edges indicating same-class predictions and strong negative edges indicating different-class predictions. Clusters are formed by selecting k cliques to maximize clique size and diversity. A greedy approximation algorithm is used to find nodes with the highest number of strong positive edges, leading to fully connected nodes within clusters. The method creates clusters based on ensemble model agreement, forming k diverse clusters by selecting nodes with strong positive edges. These clusters are then used as pseudo-labels in a semi-supervised clustering approach. The Kingdra algorithm utilizes a semi-supervised method called Ladder-* to train an ensemble of models for finding high-quality clusters. Pseudo-labels are derived from these clusters and used to train the models with both unsupervised and supervised losses, resulting in continued improvements. The Kingdra algorithm uses the Ladder-* method to map clusters to output classes, improving clustering quality iteratively. Cluster sizes increase with each iteration, covering most of the input set. The model's clustering performance improves until it saturates, with stable cluster assignments and reduced variance across multiple runs. Kingdra algorithm's performance improves with more iterations, decreasing variance across runs. It is evaluated on various datasets including MNIST, CIFAR10, STL, and Reuters, using data pre-processing and model sizes from prior work. MNIST contains 70000 handwritten digits, CIFAR10 has 32x32 color images with 10 classes, STL has 96x96 color images with 10 classes, and features from a Resnet-50 network are used for CIFAR10 and STL. The study utilized datasets like Reuters and 20News, applying pre-processing methods from previous research. Evaluation was done using unsupervised clustering accuracy, following a specific methodology. The best mapping between ground truth and model labels was sought. The study compared Kingdra with various clustering algorithms including K-Means, Agglomerative clustering, Deep Autoencoders, Deep Variational Auto-encoders, Deep RIM, DEC, DeepCluster, and IMSAT. Different models were evaluated based on their performance in clustering datasets. The study compared Kingdra with various clustering algorithms including K-Means, Agglomerative clustering, Deep Autoencoders, Deep Variational Auto-encoders, Deep RIM, DEC, DeepCluster, and IMSAT. Kingdra includes an ensemble of Ladder-* networks and semi-supervised iterations. Tensorflow and Keras were used for implementation with a specific model architecture. Kingdra-Ladder-IM, using an ensemble of Ladder-* networks and semi-supervised iterations, outperforms state-of-the-art deep unsupervised approaches like DEC and IMSAT in clustering accuracy across five data sets. In CIFAR10, Kingdra-Ladder-IM achieves 54.6% accuracy compared to 45.6% for IMSAT and 46.9% for DEC, showing an 8% increase in accuracy. Similarly, in 20news, Kingdra-Ladder-IM achieves 43.9% accuracy. Kingdra-Ladder-IM achieves an average accuracy of 43.9% for 20news, outperforming IMSAT and DEC by over 12% in absolute accuracy. Linear RIM achieves 50.9% accuracy on 20news. The use of an ensemble provides small gains, but Kingdra using the ensemble to generate pseudo-labels results in significant improvements, with gains of 4-6% in most data sets. Kingdra-Ladder-Dot provides gains of 9% in MNIST and 17% in STL over the base model. Our approach of generating pseudo-labels from ensembles delivers large gains in unsupervised learning, with Kingdra-Ladder-IM outperforming Kingdra-Ladder-Dot in most datasets. Kingdra shows lower standard deviation and higher accuracy compared to prior approaches. The accuracy of pseudo-labels decreases as more pseudolabels are added during iterations for STL, CIFAR10, and MNIST datasets. Kingdra, a novel pseudo-semi-supervised learning approach for clustering, outperforms current state-of-the-art unsupervised deep learning based approaches. It delivers 8-12% gains in absolute accuracy for CIFAR10 and 20news datasets. The accuracy of pseudo-labels decreases as more are added, but still improves overall clustering accuracy. Our selection algorithm is biased towards easy data points, causing a gap between pseudo-label accuracy and overall clustering accuracy. In the proposed clustering ladder networks, Ladder-IM and Ladder-Dot, the Regularized Information Maximization (RIM) approach for unsupervised learning is utilized. The RIM method minimizes an objective for a classifier by balancing marginal entropy and conditional entropy to encourage uniform distribution over output classes and unambiguous class assignment. In unsupervised learning, regularization loss terms like Self-Augmented Training (SAT) and Ladder networks have been proposed to improve classification performance. SAT loss imposes invariance on original and perturbed input data outputs, while Ladder networks use a deep denoising autoencoder architecture to enhance classification accuracy. The decoder in ladder networks learns a denoising function for each layer, with lateral skip connections from the noisy encoder to the decoder. The objective function combines supervised cross entropy loss and unsupervised denoising loss. The noise acts as a regularizer for the supervised loss, while lateral connections enable higher layer features to focus on task-specific features. The Ladder-IM and Ladder-Dot models utilize unsupervised denoising loss and lateral connections to learn useful features from data. Without supervised loss, ladder networks may degenerate to a trivial solution, but batch normalization helps alleviate this issue. The Ladder-IM approach combines ladder networks with information maximization to address degeneracy issues. It uses clean and noisy outputs for computing mutual information loss, encouraging disparate class assignments. This approach can be seen as part of the RIM framework, with the unsupervised ladder loss serving as a regularization term. In this paper, a regularization loss term is added to the ladder network encoder, based on KL divergence between clean and noisy outputs. This regularization is similar to dropout, adding higher level feature noise. The minimization objective is set with \u03b1 and \u03b2 as one. In the semi-supervised case, a supervised cross entropy term is also included. Additionally, a dot product loss is used to address degeneracy issues. The dot product loss is used to address degeneracy issues in the network outputs, encouraging orthogonality between different inputs. Ladder-IM outperforms Ladder-Dot in most cases, but Ladder-Dot with Kingdra iterations performs better with imbalanced datasets. Ladder-IM shows superior performance compared to IMSAT-RPT and IMSAT-VATHu et al. (2017) on most datasets, especially in semi-supervised settings. In semi-supervised settings, Ladder-IM outperformed vanilla ladder networks in a preliminary analysis. Qualitative analysis of Kingdra on the MNIST dataset showed convergence on labels with fewer inter-cluster linkages. Final clusters generated by Kingdra were highly accurate for MNIST but had lower accuracy for CIFAR10 due to incorrectly labeled examples. The accuracy of KINGDRA-LADDER-IM varied with the number of models in the ensemble. The MNIST accuracy saturates after 10 models in the ensemble. CLadder-IM takes 2mins on a server with four P100 GPUs, while Kingdra with 10 iterations takes 80mins. DeepCluster analysis reveals clustering accuracy decreases with iterations due to bad pseudolabels. Our approach uses high-confidence samples for pseudo-labels. Kingdra performs well but has shortcomings in clustering accuracy. Kingdra performs well in datasets studied, but struggles with increased number of classes. The graph clustering algorithm may need tuning as classes increase. For CIFAR100 with 100 classes, the algorithm struggles to identify diverse classes effectively. Future work includes improving the clustering algorithm and adding diversity to ensemble models. MNIST is a dataset of 70000 handwritten digits, while STL consists of 96-by-96 color images with 10 classes. The datasets used in the study include STL with 10 classes of color images, Reuters with English news stories in four categories, and 20News with newsgroup documents in 20 different newsgroups. Features were extracted using pre-trained deep residual networks and td-idf features were used after preprocessing."
}