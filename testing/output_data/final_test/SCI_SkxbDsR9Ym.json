{
    "title": "SkxbDsR9Ym",
    "content": "Knowledge Graph Embedding (KGE) involves learning entity and relation embeddings for a knowledge graph. Prior methods have used heuristically motivated scoring functions, but we propose a generative approach to address this issue. This involves representing relational triples in the knowledge graph and extending the random generation process. The random walk model of word embeddings is extended to Knowledge Graph Embedding (KGE), establishing a theoretical relationship between joint probabilities and entity embeddings. A learning objective based on this analysis achieves state-of-the-art performance on benchmark datasets, supporting the proposed theory. Knowledge graphs like Freebase BID2 organize information in graph form, with entities as vertices and relations as edges. Embedding entities and relations in a latent space allows for inferring new relations. Existing KGE methods involve representing entities and relations with mathematical structures and using scoring functions to evaluate relational strength. The scoring functions in Knowledge Graph Embedding (KGE) methods aim to optimize entity and relation embeddings to evaluate relational strength. Various scoring functions have been proposed, such as TransE and ComplEx, which capture geometric requirements of the embedding space. Score functions proposed in prior work on Knowledge Graph Embedding (KGE) include TransE, DistMult, RESCAL, and ComplEx. Entity embeddings are vectors in all models, except in ComplEx where they are complex vectors. Relational triples from knowledge graphs are used as positive training instances, with pseudo-negative instances generated by corrupting positives. KGE methods aim to minimize prediction loss over positive and negative instances for learning. Despite good empirical performance, theoretical understanding is lacking. In this paper, the theoretical analysis of Knowledge Graph Embedding (KGE) methods is explored. A generative process is proposed to explain the formation of relations between entities using embeddings. The relationship between the probability of a relation holding and the embeddings of entities is derived, offering a new KGE method with a provable generative explanation. The margin loss, commonly used in KGE methods, is also discussed. In section 3, the margin loss used in Knowledge Graph Embedding (KGE) methods is shown to naturally arise from the log-likelihood ratio computed from p(h, t | R). A new training objective is derived and optimized for learning KGEs that satisfy theoretical relationships. Experimental results on benchmark datasets demonstrate state-of-the-art performance on link prediction and triple classification tasks, supporting the theoretical analysis of KGEs. The goal of Knowledge Graph Embedding (KGE) is to learn embeddings for relations and entities in a knowledge graph so that entities participating in similar relations are close in the entity embedding space, and relations between similar entities are close in the relational embedding space. KGE assumes entities and relations are embedded in the same vector space, allowing for linear algebraic operations. A random walk is characterized by a time-dependent knowledge vector c_k, where k is the current time step. The goal of Knowledge Graph Embedding (KGE) is to learn embeddings for relations and entities in a knowledge graph. Entities h and t are represented by time-independent d-dimensional vectors. Generating a relational triple (h, R, t) in a knowledge graph is a two-step process involving a knowledge vector c_k and relation R. The goal of Knowledge Graph Embedding (KGE) is to learn embeddings for relations and entities in a knowledge graph. Entities h and t are represented by d-dimensional vectors. The process involves evaluating the appropriateness of h and t for the arguments of a relation R using relation-specific orthogonal matrices. Z c is a normalisation coefficient ensuring the probability of h given R and c is 1. Random walker state changes to c = c k+1 after generating h, and the second argument t of R is generated using a relation-specific matrix R 2. The text discusses the use of orthogonal matrices to evaluate the appropriateness of entities for relation arguments in Knowledge Graph Embedding. It emphasizes the importance of orthogonality for mathematical proof and regularization to prevent overfitting. The analysis assumes a slow random walk where knowledge vectors do not change significantly between consecutive time steps. The text discusses the generation of entity arguments in Knowledge Graph Embedding using a two-step process. It assumes that knowledge vectors are distributed uniformly and that entities in the same relational triple have similar knowledge vectors. The probability of entities satisfying a relation is estimated by taking the expectation over the distribution of knowledge vectors. The generation of entity arguments in Knowledge Graph Embedding follows a two-step process where h and t are generated independently given the relation and knowledge vectors. Computing the expectation is challenging due to partition functions Zc and Zc, but Lemma 1 shows that they are narrowly distributed around a constant value. The Concentration Lemma states that entity embeddings satisfy certain conditions with high probability. The Concentration Lemma shows that the mean of Zc is concentrated around a constant for all knowledge vectors c, with bounded variance. The knowledge vector c is rotated by R1 without altering its length, and the partition function Zc is defined using the inner-product between vectors h and R1c. The expectation of a partition function involving inner-products between vectors h and R1c can be approximated using a Gaussian random variable. The variance of the random variable x is determined by the variance of the norms of entity embeddings. The variance of the partition function Zc is calculated using Gaussian random variables with fixed entity embeddings. The mean and variance of Zc are bounded by constants independent of the knowledge vector, allowing for a proof based on a partition function with bounded mean and variance. The proof of the concentration lemma is completed by directly applying bounded mean and variance to Zc. Lemma 1 also applies to the partition function t\u2208V t R 2c. The main theorem states that entity embeddings satisfying certain conditions lead to specific outcomes. The proof sketch involves events F, T1, and T2, with probabilities and constants determined accordingly. In this section, a training objective is derived from Theorem 1 to optimize learning Knowledge Graph Embeddings (KGE). The goal is to validate the theoretical result by evaluating the learned KGEs, which represent relations between entities in knowledge graphs. Knowledge graphs use relational triples to represent information about relations between entities. The joint probability p(h, R, t) from Theorem 1 helps determine if a relation R exists between entities h and t. Predicting missing links between entities or relations, known as the link prediction problem, can expand knowledge graphs and address knowledge acquisition challenges. Positive triples in a knowledge graph (h, R, t) can be used to derive criteria for predicting links. In knowledge graphs, relational triples represent relations between entities. Positive triples indicate known relations, while negative triples are generated by perturbing positive ones. Various methods can be used to generate negative triples, with the goal of learning Knowledge Graph Embeddings (KGEs) that assign higher probabilities to positive triples than negative ones. The likelihood ratio (25) with a threshold \u03b7 > 1 determines higher probabilities for positive triples compared to negative ones. Taking the logarithm of (25) yields the marginal loss L(D,D) as a learning objective in KGE. Theorem 1 requires orthogonal regularisation terms for R 1 and R 2. In KGE, regularisation terms R1 and R2 are added to the objective function with coefficients \u03bb1 and \u03bb2. Gradients are computed w.r.t. parameters h, t, R1, and R2 for optimisation using stochastic gradient descent. KGE methods differ in entity and relation representation and modeling interactions. Entities are often represented by vectors, while relations can be represented by vectors, matrices, or tensors. In KGE, relations are represented by vectors, matrices, or tensors. Various scoring functions are defined to evaluate the strength of a relation between entities in a triple. Different intuitions are encoded in scoring functions, such as vector norms or component-wise multi-linear dot product. KGEs learn to assign scores based on the defined scoring functions. In KGE, relations are represented by vectors, matrices, or tensors, and scoring functions evaluate the strength of relations. KGEs learn to assign scores to relational triples in knowledge graphs by minimizing a loss function. Adversarial learning can be used to generate pseudo negative triples for better learning. TransG is a generative model that uses the Chinese restaurant process to model relations between entities. Instead of directly learning embeddings from a graph, some methods consider vertices visited during random walks as pseudo sentences to learn vertex embeddings using popular word embedding algorithms. The work extends random walk analysis by BID0, connecting co-occurrence probability of words with word embeddings. BID3 extends this model to capture co-occurrences of more than two words, showing the possibility of learning word embeddings that capture relationships. BID3 extended random walk analysis to capture 3-way co-occurrences for learning word embeddings that capture semantic relations more accurately. They used standard benchmarks like FB15k237, FB13, WN11, and WN18RR datasets for empirical evaluation. The model is trained for up to 1000 epochs with early stopping based on validation set performance. Evaluation tasks include link prediction and triple classification using metrics like MRR, MR, and accuracy. The implementation is in OpenKE toolkit and details are in Appendix C. In link prediction, RelWalk achieves state-of-the-art results on WN18RR and FB15K237 datasets, except against ConvE in WN18RR. The model considers global knowledge graph structure, outperforming prior work in KGEs. Experimental results on benchmark datasets show that RelWalk, a generative model of KGE, performs well in link prediction and triple classification tasks. The model considers global knowledge graph structure and outperforms prior work in KGEs. Theoretical motivation for scoring function design is emphasized, with potential for further investigation into generative approaches for KGE in the future. RelWalk, a generative model of KGE, shows strong performance in various benchmark datasets. The proof of Theorem 1 involves probabilistic events and decomposition of expectations using indicator functions and inequalities like Cauchy-Schwarz's. The analysis focuses on bounding terms and random variables to demonstrate the effectiveness of the model. The proof involves bounding terms and random variables to show the effectiveness of the model. By quantifying the worst-case scenario using Abel's inequality, the upper bound for a specific function of a random variable is obtained. Special care is needed due to the uniform distribution on the sphere, leading to a tail bound application. The final bound for a certain variable can also be applied to another variable, resulting in an upper bound for T2. The upper bound for p(h, t | R) can be obtained by ignoring the T2 term in the equation due to the large size of the entity vocabulary in most knowledge graphs. The multiplicative error translates to an additive error, with terms and random variables bounded to show the model's effectiveness. The variables c and c are assumed to be on the unit sphere, leading to a tail bound application for the final bound. The upper bound for p(h, t | R) is obtained by ignoring the T2 term in the equation due to the large entity vocabulary in knowledge graphs. The variables R1 and R2 are orthogonal matrices on the unit sphere, leading to a lower bound on A(c). The model assumptions result in a final bound equation, with c having a uniform distribution over the unit sphere. This approximation holds approximately according to Lemma A.5 in BID1. In this section, the margin loss-based learning objective is extended to learn from multiple negative triples per positive triple, leading to a rank-based loss objective. Considering random perturbation in generating negative triples, training with multiple negatives helps estimate the classification boundary better. Given a positive triple (h, R, t) and K negative triples {(h_k, R, t_k)}_k=1^K, the model aims to assign a higher probability p(h, t | R) to the positive triple. The model assigns a probability to the positive triple higher than any negative triples. The margin loss for misclassification is defined, ensuring the ratio between probabilities meets a threshold. Monotonicity of the logarithm allows for simplification in the loss calculation. The model assigns probabilities to triples and calculates margin loss for misclassification. Training details include selecting negative triples based on highest probability and setting hyperparameters for optimal performance. For FB15K, FB15K237, and FB13 datasets, \u03b1 = 0.01, while for WN18, WN18RR, and WN11 datasets. For FB15K237 and WN18RR, d = 100 was optimal, whereas d = 50 performed best for other datasets."
}