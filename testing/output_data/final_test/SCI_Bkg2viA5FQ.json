{
    "title": "Bkg2viA5FQ",
    "content": "In sparse-reward environments, reinforcement learning agents benefit from goal-conditional policies that incorporate hindsight for efficient learning. This paper introduces hindsight to policy gradient methods, improving sample efficiency across various environments. In traditional reinforcement learning, agents interact with an environment to maximize expected cumulative reward. Goal-conditional policies represent a probability distribution over actions for each state-goal combination, allowing for generalization to unseen goals. This approach has gained attention in machine learning and robotics for its ability to learn goal-based behavior efficiently. Developing goal-based curricula for learning has attracted interest in hierarchical reinforcement learning. Goal-conditional policies enable agents to plan using subgoals, abstracting lower-level decisions. Sparse-reward environments, where agents only receive a reward upon reaching a goal state, pose challenges for traditional reinforcement learning algorithms. This task formulation avoids the problem of reward shaping, which can bias learning towards suboptimal behavior. In sparse-reward environments, agents struggle to receive rewards for reaching desired destinations. Hindsight, introduced by BID0, allows agents to learn from achieving arbitrary goals while aiming for different ones. This concept has been applied to off-policy reinforcement learning algorithms and policy gradient methods, expanding its effectiveness in reinforcement learning. Our approach in reinforcement learning relies on importance sampling to efficiently learn about different goals using information obtained by the current policy. This leads to multiple formulations of a hindsight policy gradient that improve sample efficiency in sparse-reward environments. In reinforcement learning, importance sampling is used to learn about different goals efficiently. This leads to various formulations of a hindsight policy gradient that enhances sample efficiency in sparse-reward environments. The agent interacts with its environment in episodes lasting T time steps, receiving a goal at the start of each episode and observing states, rewards, and choosing actions at each time step. A goal-conditional policy defines a probability distribution over actions for every state-goal combination. The policy in reinforcement learning is based on trajectories and probabilities of state transitions. The probability of a state transition given an action can change across time steps within an episode. The expected return of a policy is calculated based on the rewards obtained. The section presents results for goal-conditional policies in reinforcement learning, establishing the foundation for the next section. Policy gradient methods aim to find policy parameters for maximum expected return, with the gradient of the expected return with respect to the parameters given by a specific formula. The use of a baseline function can help reduce the variance of the gradient estimator. The gradient of the expected return with respect to policy parameters can be optimized using baseline functions, which can reduce variance in the estimator. Actor-critic methods often use baseline functions that approximate the value function. Goal-conditional policy gradient methods introduce hindsight to policy gradient approaches, offering novel ideas for improving performance. Theorem 4.1 introduces hindsight to policy gradient methods, allowing for evaluation of trajectories for alternative goals using importance sampling. The gradient \u2207\u03b7(\u03b8) of the expected return with respect to \u03b8 is calculated by multiplying each reward by the likelihood ratio under an alternative goal compared to the original goal. The per-decision hindsight policy gradient introduces a method for estimating gradients based on alternative goals using importance sampling. The gradient \u2207\u03b7(\u03b8) is calculated by multiplying rewards by a likelihood ratio up to the previous action. The estimator provided is consistent and unbiased for the expected return. The per-decision hindsight policy gradient estimator is consistent and unbiased for the gradient \u2207\u03b7(\u03b8) of the expected return. However, it leads to unstable learning progress due to high variance. A weighted per-decision hindsight policy gradient estimator is proposed to trade variance for bias, normalizing the likelihood ratio across trajectories for each combination of trajectory, goal, and time step. This estimator enables consistency-preserving weighted baselines for active goals during episodes. The proposed estimators for off-policy learning have unique properties that set them apart from previous methods. They rely on active goals corresponding to visited states, allowing for exact computation of expectations when the goal distribution is known. These estimators account for trajectories between states and goals, even in cases where the trajectory is more likely under the original goal. The proposed estimators for off-policy learning rely on active goals for exact computation of expectations. Likelihood ratios for alternative goals may become very small, leading to equivalent ratios in the worst case. Only likelihood ratios associated with active goals affect the gradient estimate. The estimators also allow using arbitrary goals in the dataset to prevent catastrophic forgetting during curriculum learning. The section discusses an empirical comparison between goal-conditional and hindsight policy gradient estimators in diverse environments to prevent catastrophic forgetting during curriculum learning. The experiments use a weighted per-decision hindsight policy gradient estimator that differs from the original estimator. The original estimator (HPG) does not precisely correspond to Expression 12 due to the need for a constant number of time steps T and subsampling of active goals. The compromised estimators (HPG+B) still show remarkable sample efficiency in learning curves and average performance scores. During evaluation, the agent interacts with the environment, selecting actions with maximum probability according to its policy. A learning curve displays the average return obtained during each evaluation step, with a 95% bootstrapped confidence interval. Goals are randomly drawn during training and evaluation, without a held-out set for evaluation to focus on sample efficiency over generalization. Hyperparameters for each estimator are selected through grid search based on average performance scores. In this section, definitive results for small (2) and medium (16) batch sizes are discussed in relation to hyperparameters for each estimator. The agent in a bit flipping environment aims to reach a randomly chosen state by toggling individual bits. The maximum number of time steps is k + 1, making this environment an ideal testbed for evaluation. More details about the experiments can be found in Appendices E.1 and E.2, with unabridged results and empirical studies in Appendix E.3. The environment in the experiment has a maximum of k + 1 time steps, making it suitable for testing reinforcement learning algorithms dealing with sparse rewards. Different policy gradient estimators are compared, with HPG showing early success but later degradation, attributed to poor fitting of the value function baseline. In comparison, HPG shows early success in obtaining excellent policies, but they deteriorate with additional training. GCPG and GCPG+B struggle to perform better than chance due to a lack of reward signal incorporation. HPG demonstrates stable and efficient learning, with HPG+B also achieving good policies that worsen over time. Hyperparameter sensitivity graphs suggest HPG is less sensitive to settings than other estimators. In grid world environments, the agent's goal is to reach a randomly chosen position while navigating obstacles. The agent can move in four directions and stays in place when moving towards walls. Each state or goal is represented by a pair of integers between 0 and 10, with a maximum of 32 time steps. In the empty room environment, the agent starts in the upper left corner without walls. In the four rooms environment BID23, the agent starts in one of the corners of the grid with walls partitioning it into four rooms. There is a 0.2 probability of the agent's action being replaced by a random one. Learning curves show that HPG and HPG+B improve sample efficiency by at least 200 batches in the empty room environment. In the four rooms environment, every estimator obtains unsatisfactory policies, but HPG and HPG+B still improve sample efficiency without causing instability. In the Ms. Pac-man environment, the agent's goal is to reach a randomly chosen position on a grid. HPG achieves the best average performance in grid world experiments, except for one case where HPG+B performs better. Hyperparameter sensitivity graphs show HPG is less sensitive to choices, and ignoring likelihood ratios increases sensitivity in some environments. In the Ms. Pac-man environment, the agent's goal is to move in cardinal directions for 13 game ticks. The state is represented by preprocessing game screens, and a goal is a pair of integers. The environment is challenging with high-dimensional states and enemies. HPG and GCPG show different performance in learning curves, with HPG subsampling only 3 active goals per episode. The FetchPush environment involves a robotic arm pushing a block towards a target position. HPG subsamples 3 active goals per episode, impacting its performance compared to GCPG. Videos of policies obtained using each estimator are available on the project website. The FetchPush environment involves a robotic arm pushing a block towards a target position. A state is represented by a 28-dimensional real vector containing various information. HPG achieves good policies with a medium batch size, using subsampling to reduce computational cost. Policies for goal-conditional learning using hindsight are introduced, with results shown for small batch sizes. Techniques enable exploiting information about goal achievement while another goal was intended. Results include baseline and advantage formulations, based on a goal-conditional policy framework. The text introduces a consistent estimator for goal-conditional learning using hindsight policy gradient. It addresses the challenge of obtaining direct reward signals in sparse-reward environments, allowing for natural task formulations with minimal reward shaping. The main drawback is the computational cost, which can be mitigated by subsampling active goals. The success of hindsight experience replay also depends on active goals. The success of hindsight experience replay in goal-conditional learning depends on an active goal subsampling heuristic. However, employing an inconsistent hindsight policy gradient estimator with a value function baseline can lead to unstable learning, especially in sparse-reward environments. Further experiments are needed to evaluate hindsight in dense-reward environments and explore integrating hindsight policy gradients into systems relying on goal-conditional policies. The text discusses integrating hindsight policy gradients into systems with goal-conditional policies, evaluating different estimators, implementing actor-critic methods, and reducing supervision by approximating the reward function. Theorems are presented to explain the gradient of expected return with respect to parameters. The text presents Theorem 3.1 on goal-conditional policy gradient and Theorem 3.2 on goal-conditional policy gradient with baseline formulation. Lemmas are also discussed for different scenarios and functions. The gradient of the expected return with respect to \u03b8 is calculated using different formulations and functions, including baseline and advantage formulations. Lemmas and theorems are presented to support these calculations. Theorem A.2 discusses the use of importance sampling to estimate values related to a random variable X. Theorem 4.1 and Theorem 4.2 provide gradients of the expected return with respect to \u03b8 for arbitrary goals. The partial derivative of the expected return with respect to \u03b8 is derived using Lemmas and canceling terms, leading to the expression for g. This process involves splitting trajectories into states and actions, and utilizing a baseline function. The gradient of the expected return with respect to \u03b8 is derived by pushing constants outside the summation over actions at time step t. Different baseline functions can be used for each parameter in \u03b8. The gradient is also calculated using importance sampling and rewriting equations with expectations. The gradient of the expected return with respect to \u03b8 is derived using hindsight policy gradient and advantage formulation. The result is obtained by choosing specific values for parameters and subtracting equations. The appendix contains proofs related to estimators and consistency-preserving weighted baselines. The dataset considered involves trajectories obtained using a policy parameterized by \u03b8. The per-decision hindsight policy gradient estimator, given by DISPLAYFORM2, is a consistent and unbiased estimator of the gradient \u2207\u03b7(\u03b8) of the expected return. It is derived using specific values for parameters and involves trajectories obtained using a policy parameterized by \u03b8. The weighted per-decision hindsight policy gradient estimator, given by DISPLAYFORM0, is a consistent estimator of the gradient \u2207\u03b7(\u03b8) of the expected return. The estimator is derived using specific values for parameters and involves trajectories obtained using a policy parameterized by \u03b8. The strong law of large numbers states that Y (g, t, t ) (N ) j converges almost surely to 1. The weighted baseline estimator converges almost surely to zero. The variable X(g, t) is an average of iid random variables with expected value zero. The text discusses the convergence of various estimators and proofs using backward induction. The strong law of large numbers is mentioned, along with the weighted baseline estimator and the variable X(g, t). The text discusses independence properties and the advantage function for a discrete random variable X with real-valued functions f and h. The proof involves the definition of variance and Fermat's theorem to determine a local minimum. The text discusses independence properties and the advantage function for a discrete random variable X with real-valued functions f and h. By the second derivative test, b must be a local minimum. Appendix E.1 details policy and baseline representations, while Appendix E.2 documents experimental settings. In every experiment, a policy is represented by a feedforward neural network with a softmax output layer. The baseline function is trained to approximate the value function using the mean squared temporal difference error. The policy and baseline networks for different environments are designed with specific network architectures and initialization methods. The policy network for Ms. Pac-man is a convolutional neural network with specific filter sizes and fully-connected layers. Design decisions are similar to previous work by BID6. The design decisions for preprocessing images in the Arcade Learning Environment are similar to previous work by BID6. Images are processed by applying elementwise maximum operations, converting to grayscale, cropping, and rescaling. The preprocessed images are stacked into a 84x84x4 input for the policy network, which has three hidden layers with 256 hyperbolic tangent units each. Experimental settings are detailed in Tables 1 and 2. In the experimental settings detailed in Tables 1 and 2, the number of runs, training batches, and batches between evaluations are reported separately for hyperparameter search and definitive runs. The number of training batches is adjusted based on convergence, and the number of batches between evaluations ensures 100 evaluation steps in total. Other settings include policy and baseline learning rates, as well as the number of active goals subsampled per episode. The definitive runs use the best hyperparameter combination for each estimator. The appendix contains experimental results with hyperparameter sensitivity plots and learning curves for different environments and batch sizes. The best performing learning rates were carefully chosen, with rare instances of extreme values leading to decreased performance. The appendix presents learning curves and performance results for various environments and batch sizes, along with an empirical study of likelihood ratios. Plots show the distribution of likelihood ratios during training, highlighting differences across environments. The curr_chunk discusses an empirical comparison between goal-conditional policy gradients (GCPG), hindsight policy gradients (HPG), deep Q-networks (DQN), and a combination of DQN and hindsight experience replay (DQN+HER). The implementations of DQN and DQN+HER are based on OpenAI Baselines and use similar hyperparameters to previous experiments. The curr_chunk describes the implementation details of DQN and DQN+HER, including the use of replay buffers, batch processing, loss function definition, and parameter updates using Adam. The discount factor used is \u03b3 = 0.98. The implementation of DQN and DQN+HER involves updating parameters with Adam, using a discount factor of \u03b3 = 0.98, and implementing a feedforward neural network with specific network architectures. The input to the network consists of state, goal, and time step information, with weights initialized using variance scaling. Additional hidden layers are included in the architectures compared to environments with discrete action spaces. The experimental protocol for evaluating agents involves grid search to select learning rates for DQN and DQN+HER in different environments. Learning rates are chosen based on average performance scores with standard deviation taken into account. Specific candidate sets of learning rates are used for different environments. The experimental protocol involves grid search to select learning rates for DQN and DQN+HER in different environments. Specific candidate sets of learning rates are chosen for each environment to ensure optimal performance. Definitive results are obtained by using the best hyperparameters found for each method in additional runs, making them directly comparable to previous results. In various environments, different methods show varying levels of sample efficiency and stability. HPG and DQN+HER perform well in some environments, while GCPG and DQN struggle to learn. DQN+HER outperforms other methods in the four rooms and Ms. Pac-man environments, showing superior sample efficiency. In the FetchPush environment, HPG outperforms all other methods, with DQN and DQN+HER unable to learn. GCPG shows some learning progress by the end of training. Active goals are subsampled to increase computational efficiency. The decision between HPG and DQN+HER depends on the environment, while hindsight always proves successful. Heuristics known to improve policy gradient methods were not used to avoid introducing confounding factors. Our work aims to extend heuristics to improve state-of-the-art policy gradient approaches like GCPG and HPG in various environments. Additionally, DQN+HER could benefit from additional heuristics and hyperparameter settings to perform well in the FetchPush environment. Comparisons between GCPG and HPG, as well as DQN and DQN+HER, are more conclusive due to minimized confounding factors. Our approach aims to improve policy gradient methods by incorporating hindsight information, which complements previous work. Unlike indiscriminately adding hindsight transitions to the replay buffer, our approach benefits from incorporating all available goal information at every update, eliminating the need for a replay buffer."
}