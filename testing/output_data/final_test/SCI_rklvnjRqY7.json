{
    "title": "rklvnjRqY7",
    "content": "In this paper, a privacy-preserving deep learning framework is proposed for image classification. The framework includes a learnable obfuscator, classifier, and reconstructor to protect sensitive information in images. An adversarial training methodology is designed to optimize the obfuscator for privacy protection. Evaluation results show that the framework effectively safeguards users' privacy while maintaining high accuracy. The framework proposed in this paper ensures user privacy and high accuracy in image classification. Deep neural networks have made significant advancements in various fields like computer vision and speech recognition. Large datasets like ImageNet, MNIST, CIFAR-10/CIFAR-100, Youtube-8M, and AudioSet support DNN training but may pose privacy risks due to sensitive information. The GDPR mandates timely handling of personal data requests and prohibits long-term storage of personal data. The GDPR mandates timely handling of personal data requests and prohibits long-term storage of personal data, including images, within 30 days unless anonymized. This regulation poses challenges for collecting real-world datasets for deep learning models. Anonymized data can be stored indefinitely under GDPR, inspiring the design of a framework to convert images into obfuscated representations for long-term storage while preserving discriminative features for learning tasks. In compliance with GDPR, a framework is designed to convert images into obfuscated representations for long-term storage while preserving discriminative features for learning tasks. The framework includes an obfuscator, classifier, and reconstructor models, focusing on image classification as the learning task. The obfuscator removes sensitive information from input images while extracting useful features for classification. The framework includes an obfuscator, classifier, and reconstructor models for image classification. The obfuscated representation prevents leakage of users' privacy, with the reconstructor aiming to extract sensitive information. The models are trained using an adversarial process. The reconstructor, as the adversary, aims to reconstruct users' images to reveal sensitive information. Image reconstruction quality is used as a measure for privacy preservation. If the reconstructor cannot restore the image, and classification accuracy remains good, the framework is considered secure for protecting users' privacy. The proposed framework uses adversarial training to protect users' privacy in image classification. It is the first study to do so and includes a brute-force experimental evaluation method. The framework effectively balances utility and privacy trade-off. Previous works have addressed privacy in learning tasks through various approaches. In this paper, the focus is on the privacy of datasets in learning tasks. Various methods like kanonymity, l-diversity, and t-closeness are used to increase uncertainty and protect data privacy. However, these approaches are more suitable for low-dimensional data, making it challenging to protect private information in multimedia. Differential privacy is a state-of-the-art mechanism that adds noise to individual records to keep them private, but it mainly affects inserting and deleting data records. The focus is on privacy in learning tasks, with methods like kanonymity, l-diversity, and t-closeness used to protect data. Differential privacy adds noise to individual records for privacy, affecting inserting and deleting data. BID0 explored applying differential privacy to deep learning with a novel DPSGD algorithm. However, there is always a tradeoff between data utility and privacy due to the strict privacy guarantee requiring more noise, limiting its application. Cryptonets, a cloud-based framework, uses cryptographic operations for data-level privacy protection, but has limitations like a privacy-utility tradeoff and sensitivity to homomorphic encryption. DeepSecure is a provably-secure framework for scalable deep learning that addresses the privacy-utility trade-off. It uses homomorphic encryption but is limited to scenarios with fewer than 2600 samples per client. Other approaches like BID4, BID2, and BID19 have similar limitations. Recent works explore using machine learning techniques to protect dataset privacy, such as BID21's client-server model for common CNN separation. In contrast to previous works like BID21 and BID17, we propose a privacy protection framework based on adversarial training. Our framework involves the obfuscator and classifier working together to preserve privacy during classification, while an adversarial reconstructor attempts to reveal private information. In a privacy protection framework based on adversarial training, the obfuscator and classifier collaborate to preserve privacy during classification, while an adversarial reconstructor aims to reveal private information by recovering the image. The reconstruction quality is incorporated into the loss function to improve learning of the obfuscator. Experimental results show that the framework effectively preserves privacy and achieves high classification accuracy. The reconstructor could potentially be used by an attacker, but threats from attacks solely focusing on the training data domain are not addressed in this paper. It is assumed that in the attack model of CIA, an attacker would use a DNN for image reconstruction as obfuscated feature maps may contain unnoticed information. In a privacy protection framework based on adversarial training, the obfuscator and classifier collaborate to preserve privacy during classification. An adversarial reconstructor aims to reveal private information by recovering the image, with the reconstruction quality incorporated into the loss function. The Peak Signal to Noise Ratio (PSNR) is used to evaluate the strength of security against attacks like CIA, with lower PSNR values indicating better defense. The proposed framework is deep learning-based and focuses on privacy preservation. Our proposed framework for privacy-preserving image classification consists of three modules: the obfuscator, classifier, and reconstructor. The obfuscator removes sensitive information while preserving primary data for the classifier, while the reconstructor aims to reconstruct the original image. Training is done through adversarial training, with parameters denoted as D for images and Y for class labels. The obfuscator function is denoted as f(\u00b7; \u03b8f) mapping images to feature maps. The framework for privacy-preserving image classification includes an obfuscator, classifier, and reconstructor. The obfuscator removes sensitive information, the classifier maps feature maps to class labels, and the reconstructor reconstructs the original image. The deep CNN architecture VGG16 BID27 is divided into two parts to extract discriminative features and remove sensitive information from the extracted feature maps. In the framework for privacy-preserving image classification, a deep CNN architecture is divided into an obfuscator and a classifier. The obfuscator aims to minimize classification error and protect privacy by minimizing PSNR between original and reconstructed images. The classifier's goal is to minimize classification error as well. Both parts are trained by minimizing a loss function with a trade-off parameter. The reconstructor in the privacy-preserving image classification framework acts as an attacker, aiming to recover private information from feature maps. Its objective is to maximize the similarity between reconstructed and raw images, in contrast to the adversarial loss in the framework. The roles of the obfuscator and reconstructor are conflicting, with the reconstructor trying to undo the obfuscation done by the obfuscator. The obfuscator and reconstructor roles conflict in a privacy-preserving image classification framework. The obfuscator removes sensitive information to prevent reconstruction, while the reconstructor aims to reconstruct images accurately. This adversarial training process involves fine-tuning parameters to optimize three models. The framework involves optimizing three models: \u03b8 f , \u03b8 g , \u03b8 r using Xaiver initialization BID10. Augmented data is generated from input images, and the obfuscator-classifier is trained for optimal performance. The obfuscator is then frozen, and the reconstructor is trained. Finally, the obfuscator is trained again while the reconstructor and classifier are frozen. The primary task is image classification, with initial training focusing on the classifier and obfuscator. Privacy concerns are addressed after the initial training phase. After initial training of the obfuscator-classifier, privacy concerns are addressed through adversarial training. The reconstructor is trained while the obfuscator is fixed, then the obfuscator is trained while keeping the reconstructor and classifier fixed to counteract improvements in PSNR. This process is repeated until convergence or the maximum number of epochs. Three datasets are used in experiments: MNIST, CIFAR-10, and CIFAR-100. MNIST has 70,000 handwritten digit images, CIFAR-10 contains 60,000 colored images in 10 classes. The CIFAR-100 dataset consists of 60,000 32 \u00d7 32 colored images in 100 classes. Four state-of-the-art reconstructors are implemented as attackers in the framework, with results reported for \u03bb = 1. The reconstructors include a simple autoencoder reconstructor and a U-net reconstructor. The U-net reconstructor #1 (URec#1) and U-net reconstructor #2 (URec#2) are convolutional neural networks used for biomedical image segmentation and generation. The ResNet reconstructor (ResRec) is a deep learning model for image recognition and restoration. Experimental results show the framework's effectiveness in protecting user privacy while maintaining deep learning image classification utility. Classification accuracy for each dataset is presented in TAB2, focusing on the first 100 epochs. The accuracy of the framework is consistent regardless of the reconstructor used. The baseline accuracy for VGG16 on the dataset is compared to the accuracy achieved with privacy-preservation techniques, showing promising results. Our framework outperforms the state-of-the-art work in privacy-preserving networks on the CIFAR-10 dataset. The framework employs adversarial training to reduce reconstruction quality and improve classification accuracy. Reconstruction results on CIFAR-10 show blurred outlines of target objects, representing category information. Average PSNR values for different reconstructors are compared, with lower values indicating significant information removal by the obfuscator. Additionally, a simulation of attacker behavior is conducted alongside adversarial training and testing. In a complementary experiment, a brute-force attack is simulated by assuming the attacker has a pre-trained obfuscator and trains reconstructors to recover sensitive information from a feature map. The obfuscator proves robust against different attackers, as shown by low PSNR values when using a different reconstruction method. The deep learning framework proposed focuses on privacy-preserving image classification tasks. It consists of three modules: obfuscator, classifier, and reconstructor. Adversarial training is crucial in learning the obfuscator, as demonstrated by comparing reconstructed images with and without adversarial training. The obfuscator acts as a feature extractor and sensitive information remover without compromising classifier accuracy. The framework includes an obfuscator, classifier, and reconstructor. Adversarial training is used to protect privacy and maintain high accuracy in image classification tasks."
}