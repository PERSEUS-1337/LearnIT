{
    "title": "HklxbgBKvr",
    "content": "The ability to design biological structures like DNA or proteins has significant medical and industrial implications. A challenging optimization problem arises due to the need for labor-intensive wet lab evaluations. To address this, reinforcement learning (RL) based on proximal-policy optimization (PPO) is proposed for biological sequence design. DyNA-PPO, a model-based variant of PPO, improves sample efficiency by training the policy for a new round offline using a simulator. The simulator model is automatically selected at each round from a pool of diverse models to accommodate the growing number of observations. Biological sequence design aims to optimize sequences for specific functions, such as designing DNA transcription factor binding sites and antimicrobial proteins. DyNA-PPO outperforms existing methods in feasible modeling scenarios, addressing challenges in health and disease research. This optimization problem involves a large search space and relies on costly wet-lab experiments for function evaluation. The current gold standard for biomolecular design is directed evolution, which is sample inefficient and relies on greedy hillclimbing. Machine-learning-guided optimization has shown to find better sequences faster. Reinforcement learning offers a flexible framework for black-box optimization using deep generative sequence models. This paper proposes a method to improve sample efficiency of policy gradient methods for black-box optimization by using surrogate models trained online to approximate f(x). Our method updates the policy's parameters using sequences x generated by the current policy \u03c0 \u03b8 (x), but evaluated using a learned surrogate f w (x), instead of the true, but unknown, oracle reward function f (x). The reward model becomes more reliable over time and can be used as a cheap surrogate for assessing sequences and updating the policy. Cross-validation is an effective heuristic for assessing the model quality. Our DyNA-PPO method outperforms existing methods in optimizing DNA sequences, anti-microbial peptide sequences, and binary sequences based on Ising model energy. The method achieves higher cumulative reward within a given budget compared to standard PPO, cross-entropy method, Bayesian optimization, and evolutionary methods. Our contributions include introducing DyNA-PPO, a model-based RL algorithm for efficient batched black-box function optimization. We address model bias by selecting models of appropriate complexity through cross validation and propose a visitation-based exploration bonus for identifying multiple local optima. Additionally, we present a new optimization task for benchmarking methods in biological sequence design based on protein energy Ising models. The design of a single sequence is formulated as a Markov decision process with state space, action space, transition function, and reward function. Sequences are generated autoregressively left to right, with the state representing sequence prefixes and actions corresponding to the next token. The transition function appends the next token to the current state, and the reward is based on a functional measurement at the last step. To generate variable-length sequences, the vocabulary is extended with an end-of-sequence token for termination. The design of a single sequence involves generating sequences autoregressively left to right, with an end-of-sequence token for termination. A policy is trained to optimize rewards using candidate models and model-based training rounds. Proximal policy optimization (PPO) with a KL trust-region constraint is utilized for training. We implement algorithms using the TF-Agents RL library for training autoregressive models with one fully-connected layer as policy and value networks. The network takes as input the last W characters that are one-hot encoded, where W is a hyper-parameter. The network uses a window size W as a hyper-parameter and receives the time step t for positional encoding. The policy network outputs a distribution over the next token, while the value network approximates future rewards. Model-based RL learns an environment model for additional observations, addressing the high sample complexity of model-free RL in biological sequence design. Model-based RL approximates the MDP with the same state and action space, focusing on approximating the reward function. By using supervised regression to fit a regressor on previous data, the method DyNA-PPO collects additional observations to update the policy in a simulation phase, reducing the need for expensive true environment observations. This approach is similar to the DYNA architecture and aims to address the high sample complexity in biological sequence design. Model-based RL, similar to the DYNA architecture, is used for DNA sequence design. It offers improved sample efficiency with accurate models but can lead to reduced performance with insufficient data. To address this, models of varying complexity are automatically selected, and training stops when model uncertainty increases. Model accuracy is assessed using the R2 score through supervised regression after each experiment round. The model accuracy is quantified using the R2 score through regression. If the score is below a threshold, model-based training is not performed. An ensemble model is built with models above the threshold, and the average prediction is used as a reward. The process stops when model uncertainty increases by a certain factor. The model uncertainty is correlated with unknown model error, hindering training with inaccurate predictions. Various models like nearest neighbor regression, Bayesian ridge regression, random forests, etc., are used with cross-validation for hyper-parameter tuning. Bayesian optimization utilizes Gaussian processes to dynamically increase model capacity with more data. Our method combines generative and discriminative approaches to sequence design, incorporating cross-validation-based model selection. Unlike Bayesian optimization, our approach allows for ignoring unreliable regressors and does not rely on sensitive Bayesian inference. The focus is on learning policies to generate diverse sequences, which is crucial in many applications. In-vitro surrogate f (x) is used for in-vivo functional measurement, which is costly. Optimization should find diverse candidate optima to increase chances of meeting in-vivo criteria. Diversity is important for exploration strategy and training policy \u03c0 \u03b8 (x) may not always lead to good performance. To increase sequence diversity, a simple exploration reward bonus based on proposed sequence density is employed. The reward penalizes similar sequences, encouraging exploration of the search space. The policy learns to explore the search space effectively by avoiding generating related sequences. Edit-distance was used as a distance metric, with a tuned distance radius that improved exploration on high-dimensional problems. An alternative penalty based on nearest neighbor distance of proposed sequences to past sequences was found to be less effective. Machine learning approaches have been effective in optimizing real-world DNA and protein sequences. Existing methods for biological sequence design include evolutionary search, optimization using discriminative models, and optimization using generative models. Evolutionary approaches and optimization methods based on discriminative models are used for biological sequence design. Evolutionary approaches involve direct local search in sequence space, while optimization methods based on discriminative models fit a regressor to the data and define an acquisition function to select sequences. A recent study used a neural network ensemble for optimization, showing that optimizing the acquisition function is a complex problem. Function optimization is a challenging problem, with different approaches like activation maximization and Bayesian optimization being used. Gaussian processes are commonly used for Bayesian optimization but can be difficult to scale for large datasets. Recent work has focused on continuous black box optimization in the latent space. Recent work has focused on continuous black box optimization in the latent space of a deep generative model. A model-based reinforcement learning approach trains a policy to optimize a model f w (x) directly on observations of f (x), allowing for model-free training when the model is too inaccurate. Janner et al. (2019) explore conditions for justifying model usage in policy optimization, while Hashimoto et al. (2018) propose using a cascade of classifiers to guide sampling better candidates. Optimization methods based on generative models aim to learn a distribution p \u03b8 (x) that maximizes the expected value of f (x). Variants of the cross entropy method optimize \u03b8 by sampling x \u223c p \u03b8 (x) and updating \u03b8 to maximize this expectation. Different methods, like hillclimb-MLE and Feedback GAN, use various approaches for training the generative models. Design by Adaptive Sampling (DbAs) performs weighted maximum likelihood estimation (MLE) of variational autoencoders, where a sample's weight corresponds to the probability that f(x) exceeds a fixed threshold. An alternative approach for optimizing this expectation is reinforcement learning (RL), which has been used for generating natural text, small molecules, and RNA sequences, but not yet for DNA and protein sequences. DyNA PPO is related to existing work on model-based RL for sample efficient control. In the context of model-based RL for sample efficient control, recent work has focused on incorporating non-differentiable rewards in sequence generation tasks. This includes methods like reward augmented MLE and fine-tuning MLE-pretrained models using actor-critic techniques. Additionally, reinforcement learning has been applied to combinatorial optimization problems, where sample complexity is less critical. Recent proposals have also explored generative models for protein structures. Recent work has proposed generative models of protein structures and amino acids conditional on protein structure. These methods are not applicable in experimental settings where protein structures are expensive to measure. DNA and protein design differ from small molecule design in terms of the number of sequences measured in parallel and the search space. DyNA-PPO is compared to existing methods on in-silico optimization problems designed to simulate real wet-lab experiments. Ablation experiments are presented to understand DyNA-PPO's behavior. Performance of PPO and DyNA-PPO is compared with other methods discussed in Section 3. The methods compared in the study include RegEvolution, DbAs, FBGAN, Bayesopt GP, Bayesopt ENN, and Random for optimization tasks. Performance is evaluated based on the cumulative maximum of f(x) for proposed sequences and sequence diversity is measured using pairwise hamming distance. The study evaluates various optimization methods including RegEvolution, DbAs, FBGAN, Bayesopt GP, Bayesopt ENN, and Random. Performance is assessed based on the maximum of f(x) for proposed sequences and sequence diversity is measured using pairwise hamming distance. Experiments are replicated with 50 random seeds, and the surrogate model is used for policy optimization rounds before querying f(x) again. Synthetic black-box optimization problems based on the 3D structure of proteins are considered. The study evaluates optimization methods for protein 3D structure prediction, with DyNA-PPO outperforming other methods. The synthetic reward landscape favors models fit with few examples, explaining the success of Bayesian optimization. The study compares optimization methods for protein 3D structure prediction, with DyNA-PPO showing superior performance. DyNA-PPO benefits from updating the policy using a simulator due to the high-quality surrogate model. Automated model selection results in a highly accurate regressor fit, enabling DyNA-PPO to learn to generate high-quality sequences with minimal evaluations. The comparison of methods across transcription factor binding sites shows DyNA-PPO ranking well across tasks. DyNA-PPO outperforms other methods in optimizing f(x) and identifying multiple local optima. Transcription factors are protein sequences that regulate DNA activity. In Barrera et al. (2016), binding affinity of transcription factors to DNA sequences was measured. Performance of entropy regularization in finding local optima is shown, with PPO finding 80% with a density penalty of \u03bb = 0.1. Varying the penalty controls sequence diversity. The optimization methods are tasked with finding DNA sequences that maximize affinity towards transcription factors. DyNA-PPO and PPO outperform other methods in terms of cumulative maximum f(x) found and exploration of local optima. The search space size is 48, with a budget of 10 rounds and a batch size of 100 sequences. Performance is tested on 41 hold-out tasks, with DyNA-PPO and PPO showing superior results. The study compares DyNA-PPO and PPO with other methods in terms of finding maximum f(x) and exploring local optima. DyNA-PPO shows high diversity in proposed sequences and continues to explore the search space. Model-based training and exploration bonus are analyzed, with DyNA-PPO showing only a small improvement over PPO due to model complexity and accuracy issues. The exploration bonus is more effective than entropy regularization in promoting sequence diversity. Antimicrobial peptides (AMPs) are short protein sequences with wide antimicrobial activities. A dataset with 6,760 unique AMP sequences is used for training classifiers to predict antimicrobial activity. Unlike the transcription factor dataset, wet-lab experiments are not available for every sequence in the search space. Random forest classifiers are used for prediction. In this study, random forest classifiers were used to predict antimicrobial activity of sequences towards a specific pathogen. The classifiers showed high accuracy with cross-validated AUC of 0.94 and 0.99. Model-based optimization using DyNA PPO was effective in finding high reward sequences early on, while model-free PPO performed slightly better later. Both methods outperformed others in finding maximum reward sequences. The exploration bonus helped prevent non-unique sequences from being generated. DyNA-PPO, a model-based extension of PPO, prevents non-unique sequences and improves sample efficiency in biological sequence design. It incorporates a reward function for exploration and model selection to optimize sequences in simulation. This approach may have broader applications beyond biological sequence design. Regularized evolution is a variant of directed evolution that keeps a fixed number of individuals alive as candidates for selection. It generates child sequences through crossover and mutation, with potential applications in various domains like agriculture, education, and economics. Regularized evolution involves keeping a fixed number of individuals alive for selection, generating child sequences through crossover and mutation. MCMC and simulated annealing resemble evolution but without crossover, with selection occurring between an individual and its parent. Acceptance of transitions is based on reward increase, with acceptance probabilities defined for both MCMC and simulated annealing. Regularized evolution involves keeping a fixed number of individuals alive for selection, generating child sequences through crossover and mutation. MCMC and simulated annealing resemble evolution but without crossover, with selection occurring between an individual and its parent. Acceptance of transitions is based on reward increase, with acceptance probabilities defined for both MCMC and simulated annealing. Annealing involves adjusting the temperature T to increase the likelihood of accepting a move that decreases the reward. The methodology suggested by Gupta & Zou (2018) was followed, using a quantile cutoff for selecting positive sequences instead of a constant threshold. Various hyper-parameters were tuned, including the quantile cutoff, learning rate, batch size, discriminator and generator training epochs, gradient penalty weight, Gumble softmax temperature, and number of latent variables of the generator. The methodology suggested by Brookes & Listgarten (2018) was also followed. The generative model used in the study is a variational autoencoder with a multi-layer perceptron decoder. Different models, including a DbA with a LSTM, were considered for various problems. Regressors such as Gaussian process with RBF kernel and ensemble of fully-connected neural networks were used. The regressor output was used to compute the expected improvement or posterior mean acquisition function, optimized using gradient ascent. The study utilized a variational autoencoder with a multi-layer perceptron decoder for generative modeling. Different models, including a DbA with a LSTM, were considered. Regressors like Gaussian process with RBF kernel and ensemble of fully-connected neural networks were employed. The acquisition function was optimized using gradient ascent, and the top sequences were selected for measurement in the next round. PPO implementation from TF-Agents RL library was used, with training on collected sequences for multiple steps. Adaptive KL trust region penalty was preferred over importance ratio clipping. In experiments, a policy and value network with one fully connected layer and 128 hidden units were used. The context window was set to the minimum of the total sequence length and 50. Hyperparameters like learning rate, number of training steps, adaptive KL target, and entropy regularization were tuned. Model selection was done through randomized search and five-fold cross-validation using the R2 score. Different candidate models were considered for optimization. For model selection, various candidate models with corresponding hyperparameters were evaluated per round. This included KNeighborsRegressor, BayesianRidge, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, GaussianProcessRegressor, and an ensemble of 10 neural networks. The energy computation for a protein in the Protein Data Bank involved character positions and residue interactions. The energy functions for proteins are determined by the contact map structure, with pair potentials based on co-occurrence probabilities. Local terms are set to zero, but future work may consider non-zero terms. Experiments were conducted on a set of different proteins using a dataset by Barrera et al. (2016) with normalized binding affinities. The study by Barrera et al. (2016) focused on normalized binding affinities of proteins. To optimize computational costs, only the first replicate of each wild type transcription factor was considered, resulting in 41 optimization targets. Local optima were extracted by clustering sequences with high binding affinity and determining the number of clusters based on PCA components explaining 95% of variance. We downloaded dataset 1 from Witten & Witten (2019) and preprocessed sequences to generate non-AMP samples. Excluded cysteine-containing sequences and those shorter than 15 or longer than 50 amino acids. Used random forest classifiers for predicting antimicrobial activity towards different pathogens, with C. alibicani as hold-out target due to low correlation with other pathogens in the dataset. Random forest classifiers showed high accuracy with cross-validated AUC of 0.99 and 0.94. Protein Ising AMP DyNA-PPO is built on PPO, which outperforms other RL methods like k-nearest neighbors, Gaussian processes, or neural networks. PPO has better exploration properties than REINFORCE, DQN, and CatDQN due to sparse rewards. The performance of DQN and CatDQN is sensitive to the choice of exploration-exploitation trade-off parameters. Optimizers based on MCMC are effective for inexpensive black box function evaluation but are inefficient for resource-constrained black box optimization like biological sequence design. DbAS with a LSTM generative model is considered as an alternative to VAE with multi-layer perceptron decoder. DbAS with a LSTM generative model is proposed as an alternative to VAE with multi-layer perceptron decoder for disentangling the choice of generative model in DbAS. The comparison shows that DbAS VAE outperforms DbAS RNN on all problems except for TF bind. Different approaches for promoting diversity are illustrated, including a density-based exploration bonus, distance-based exploration bonus, and standard entropy regularization. The cumulative maximum reward and alternative metrics for quantifying diversity are shown depending on the penalty strength of each exploration approach. PPO's performance in finding optimal solutions and generating diverse sequences is influenced by the penalty strength of exploration approaches. The density-based exploration bonus proves most effective in recovering all optima and controlling diversity. Results are shown for the target CRX REF R1 of the transcription factor binding problem. Diversity positively correlates with distance radius and regularization strength. Optimal trade-off between optimization performance and diversity is achieved with \u03bb = 2 and \u03bb = 0.1. Penalizing neighboring sequences is more effective in maintaining high hamming distance compared to penalizing only exact duplicates. DyNA PPO's optimization performance on the AMP problem is influenced by the uncertainty threshold and maximum number of optimization rounds. A threshold of 0.5 prevents performance decrease due to model inaccuracy. Sensitivity of DyNA PPO depends on the choice of minimum cross-validation score \u03c4 for model-based optimization. Results for transcription factor binding-, protein contact Ising-, and AMP problem are shown. DyNA PPO reduces to PPO if \u03c4 is above the maximum cross-validation score considered during model selection. A cross-validation score between 0.4 and 0.5 is optimal for all problems."
}