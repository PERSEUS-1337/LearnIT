{
    "title": "rkeTDNS3hN",
    "content": "The recent \u201cLottery Ticket Hypothesis\u201d paper by Frankle & Carbin revealed that sparse networks can outperform non-sparse models when trained from the same initial weights. The study delves into the critical components of the Lottery Ticket algorithm, showing that variations in these factors do not significantly impact results. The importance of setting weights to zero, the role of signs in training, and the behavior of masking are explored. The discovery of Supermasks, which can enhance the performance of randomly initialized networks, is also discussed. The recent work by Frankle & Carbin introduced a method for finding sparse subnetworks within neural networks that can achieve high performance. This involves setting weights below a threshold to zero, rewinding the remaining weights to their initial state, and retraining the network with the zero weights frozen. This approach has shown promising results, with sparse networks outperforming non-sparse models. In this paper, ablation studies are conducted on different dimensions of variability in the context of sparse subnetworks within neural networks. The study explores alternate mask criteria, mask-1 actions, and mask-0 actions, revealing insights into why lottery ticket networks work. The existence of Supermasks, which produce above-chance performance on untrained networks, is also discovered. The investigation begins with an analysis of various Mask Criteria functions that determine which weights to keep or prune based on weight values at initialization and after training. The study explores different mask criteria for pruning weights in neural networks, including large_final, small_final, large_init, small_init, magnitude_increase, movement, and random masks. Experiments are conducted on fully-connected and convolutional networks trained on MNIST and CIFAR-10 datasets. The study explores various mask criteria for pruning weights in neural networks trained on CIFAR-10 BID6. Results show that the large_final and magnitude_increase criteria perform competitively, with some cases showing the latter to be significantly better. Further explanations on the effectiveness of different mask criteria are provided in Section 4. The study explores the effectiveness of different mask criteria for pruning weights in neural networks trained on CIFAR-10 BID6. An interesting observation in BID2 showed that pruned LT networks train well when rewound to their original initialization but perform poorly when randomly reinitialized. The study evaluates various reinitialization variants to investigate the impact on LT network training. The study evaluates different reinitialization variants for pruned LT networks, finding that keeping the sign of weights leads to better training performance. Setting kept weights to a constant value also performs well. It is unclear whether setting pruned weights to zero or freezing them contributes more to the improved performance in LT networks. The study explores reinitialization variants for pruned LT networks, finding that freezing weights at zero leads to increased performance. This contrasts with setting weights to random initial values, suggesting zero as a particularly effective value for small final weights. The study found that freezing weights at zero leads to improved performance in pruned LT networks. This is supported by the hypothesis that weights moving towards zero anyway benefit from being frozen at zero. This approach can even outperform the original LT networks when applied to all weights, not just the pruned ones. The study found that freezing weights at zero leads to improved performance in pruned LT networks. This approach, known as \"Supermasks\", involves applying a mask to a randomly initialized network, resulting in better-than-chance accuracy without training the underlying weights. The study introduced the concept of \"Supermasks\" where pruned weights are frozen at zero or their initial values based on magnitude changes. Performance on different models was evaluated, showing improved accuracy without training the underlying weights. The study introduced the concept of \"Supermasks\" where pruned weights are frozen at zero or their initial values based on magnitude changes. Evaluating various mask criteria as Supermasks, a new large_final_same_sign mask criterion was defined, selecting weights with large final magnitudes maintaining the same sign. Networks created using this criterion achieved 80% test accuracy on MNIST and 24% on CIFAR-10 without training. The Supermasks concept involves applying a mask to signed constants for higher test accuracy on MNIST and CIFAR-10. Results across network architectures and pruning percentages are detailed in FIG6. These Supermasks allow for network compression by saving a binary mask and a single random seed to reconstruct the full weights. The Supermasks concept involves applying a mask to weights of the network to completion. The mask criterion produces a masking score for each unmasked weight, ranking them by score and setting a mask value. Actions are taken based on the mask value, either resetting weights to initial values or pruning them. In BID2, weights were pruned by setting them to 0 and freezing them during subsequent training. Table S1 shows the architectures and training hyperparameters used in the study. Convolutional networks in the paper use 3x3 layers with max pooling and fully connected layers. Figure S2 illustrates mask criteria, with weights ranked by score and either kept or pruned based on the score. The text chunk discusses the adjustment of \u03b1 to align percentiles between initial and final weights, the creation of masks with random tie-breaking, and the performance of various mask criteria, reinitialization methods, and treatment of pruned weights for different network architectures on MNIST and CIFAR-10 datasets. Additionally, it explains how the large_final criterion biases weights towards zero during training. The text chunk discusses the creation of Supermasks and their performance on different network architectures. It explores the search space of possible masks and the use of regular optimizers to enhance Supermask performance. The text chunk introduces the concept of Supermasks, which are created by combining an original weight tensor with a mask tensor using a point-wise function. The masks are trained using Bernoulli sampling to add stochasticity and improve training. This approach achieved up to 95.3% test accuracy on MNIST and 65.4% on CIFAR-10 datasets. The Supermasks, trained using Bernoulli sampling, achieved up to 95.3% test accuracy on MNIST and 65.4% on CIFAR-10. The masks were initialized with different magnitudes to control pruning percentages, ranging from 7% to 89%. Learning Supermasks showed significant improvement over heuristic-based masks, with performance close to training the full network. This suggests that powerful subnetworks are present in the network upon initialization. The Supermasks achieved high test accuracy on MNIST and CIFAR-10, with masks initialized for controlled pruning percentages. Training Supermasks showed improvement over heuristic-based masks, indicating the presence of powerful subnetworks in the network upon initialization. The learning of Supermask allows for an optimal pruning rate for each layer, with different learning rates and iterations for various networks. Conv4 and Conv6 exhibited overfitting, requiring early stopping for evaluation. For evaluation, Bernoulli sampling is used to average accuracies over 10 samples. The amount pruned is adjusted by initializing a constant 'm' in each layer, ranging from -5 to 5. Different initializations of 'm' for each layer could provide more control over per-layer pruning rates. Adding an L1 loss could help networks avoid cold start problems. Performance of different mask criteria for four networks at various pruning rates is shown in Figure S3. The effects of various 1-actions for the four networks and pruning rates are shown in Figure S7. Different methods are compared, including \"Reshuffle, init sign\" and \"constant, init sign,\" which perform similarly to the \"rewind\" baseline. Stars indicate significant points at a p = 0.05 level. The large_final_same_sign mask criterion creates the highest performing Supermask by a wide margin, improving network accuracy beyond chance levels without any training. The Supermask outperforms other masks significantly, with every data point on the plot representing the same underlying network but with different masks."
}