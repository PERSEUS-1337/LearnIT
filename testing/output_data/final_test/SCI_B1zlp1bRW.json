{
    "title": "B1zlp1bRW",
    "content": "This paper introduces a two-step approach for learning an optimal map from one distribution to another. The first step involves learning an optimal transport plan, while the second step estimates a Monge map using a deep neural network. The approach is demonstrated to scale better than a recent related method for large sample sizes. Theoretical stability results are provided, showing convergence to the optimal transport and Monge map. Two applications of the approach are showcased: domain adaptation and generative modeling. In domain adaptation and generative modeling, finding a map f such that f(X) \u223c Y is crucial. This mapping aligns source and target distributions, impacting generalization. State-of-the-art methods like generative adversarial networks rely on this mapping for generating models. X is often chosen as a continuous random variable, like a Gaussian distribution, and Y is a discrete distribution of real data, such as the ImageNet dataset. Finding an optimal map f(X) \u223c Y is important in various applications like color transfer, shape matching, and Bayesian inference. This mapping minimizes the cost of transporting mass from X to Y, as per the optimal transport theory initiated by Monge in 1781. The modern approach to optimal transport (OT) relaxes the Monge problem by optimizing over plans, i.e. distributions over the product space X \u00d7 Y, rather than maps. This casts the OT problem as a linear program, which is always feasible and easier to solve. However, solving the linear program takes O(n^3 log n) time, where n is the size of the discrete distribution support. Introducing entropic regularization into the OT problem can help in solving large-scale OT efficiently. Recently, BID14 introduced entropic regularization to the OT problem, making its dual easier to optimize with the Sinkhorn algorithm. However, the algorithm struggles with large sample sizes and continuous probability measures. To address this, recent works have proposed stochastic gradient methods for optimizing variations of the dual OT problem. BID20 suggested optimizing a \"semi-dual\" objective function, but it still requires O(n) operations per iteration. On the other hand, BID1 proposed a formulation specific to the 1-Wasserstein distance, simplifying the dual form with a single variable. The formulation of optimal transport as a neural network parameterized cost function scales well for large datasets. However, a drawback is the constraint of the dual variable being a Lipschitz function. Weight clipping is used as a workaround, but it raises questions about true optimization in an OT sense. Previous works focus on computing the OT objective without addressing the optimal map between distributions. Our two-step approach aims to learn an optimal map f that satisfies f(X) \u223c Y by computing an optimal transport plan. We propose a new dual stochastic gradient algorithm for solving regularized optimal transport, which converges faster than previous approaches. We then learn an optimal map using a neural network to approximate the OT plan, allowing for efficient learning and generalization. Theoretical results show the convergence of regularized optimal plans to the optimal plan. In the context of proposing a new dual stochastic gradient algorithm for solving regularized optimal transport, the curr_chunk discusses the optimal plan and Monge map between continuous measures. It introduces notation for complete metric spaces X and Y, random variables, probability measures, and the Monge Problem involving a cost function. The focus is on demonstrating the approach in domain adaptation and generative modeling. The Monge problem involves finding a map that transports mass from one probability measure to another while minimizing cost. Feasibility depends on the support of the measures, with a key result stating feasibility in certain cases. The existence and uniqueness of optimal maps have been generalized. The existence and uniqueness of Monge maps, also known as optimal maps, have been generalized to more general costs by various authors. The computation of Monge maps remains an open problem for measures supported on high-dimensional spaces. To address this, the Kantorovich Relaxation approach relaxes the Monge problem by minimizing over couplings rather than maps, allowing mass at a point to be transported to multiple locations. This relaxation is a linear program that can be solved using specialized algorithms like the network simplex. Regularized OT was introduced to speed up the computation of Optimal Transport (OT) by adding a negative-entropy penalty to the primal variable. This regularization makes the OT distance differentiable everywhere with respect to the input measures' weights. L2 regularization is also considered for stability, without an exponential term causing overflow. The dual formulation of regularized Optimal Transport (OT) problem allows for efficient stochastic gradient algorithm implementation. Stochastic gradient ascent can be used to maximize the concave objective by sampling batches from the independent coupling \u00b5 \u00d7 \u03bd. Regularized OT addresses the challenge of fulfilling constraints on u and v during gradient iterations. Regularized Optimal Transport (OT) dual formulation relaxes hard constraints by regularizing the primal problem with a convex regularizer. The regularized OT problems can be solved using Fenchel-Rockafellar's duality theorem. Constraints on u and v are enforced smoothly through a penalty term, allowing for efficient stochastic gradient algorithm implementation. The regularized Optimal Transport (OT) dual formulation relaxes hard constraints by regularizing the primal problem with a convex regularizer. Any strictly convex regularizer that is decomposable gives rise to a dual problem that can be solved using Fenchel-Rockafellar's duality theorem. The relaxed dual problem is an unconstrained concave problem that can be maximized through stochastic gradient methods by sampling batches from \u00b5 \u00d7 \u03bd. The regularized Optimal Transport (OT) dual formulation relaxes hard constraints by regularizing the primal problem with a convex regularizer. The stochastic dual maximization approach is used to compute the regularized OT objective. Different parameterizations of dual variables are discussed, with the use of deep neural networks. Convergence rates and computational cost are compared in different settings, highlighting the non-convex nature of the problem in certain cases. The problem in certain cases is non-convex, affecting convergence rates and computational costs. Full-gradient descent converges at O(1/k), while SGD with decreasing step size converges at O(1/ \u221a k) but with a higher cost. BID20 introduced a semi-dual objective with a cost per iteration of O(n), leading to the same convergence rate as SGD. SAG method can improve the convergence rate to O(1/k) but requires storing past stochastic gradients. In the semi-discrete setting, SGD on the semi-dual objective converges at a rate of O(1/ \u221a k). BID20 proposed representing dual variables as kernel expansions, with a cost of O(k^2) per iteration. In contrast, our approach uses neural networks with a cost of O(p^2) per iteration, similar to BID1's algorithm. The proposed algorithm can compute the regularized OT objective and optimal plans between empirical measures supported on large samples. In statistical machine learning, the goal is to estimate the underlying continuous distribution from empirical observations. The algorithm does not require constraining u to be a Lipschitz function or rely on weight clipping like previous methods. The entropy-regularized solution converges fast to the non-regularized optimal transport problem solution. Optimal transport plans converge weakly when measures weakly converge. A theorem establishes the weak convergence of entropy-regularized plans when (n, \u03b5) jointly converge to (\u221e, 0). The entropy-regularized optimal transport solution converges quickly to the non-regularized solution. The weak convergence of optimal transport plans is established when measures weakly converge. The convergence of entropy-regularized plans is proven when (n, \u03b5) jointly converge to (\u221e, 0). The regularization aids in learning the optimal plan between continuous measures in machine learning applications. In machine learning, optimal transport plans have various applications in logistics, economy, and computer graphics. However, in many cases, mappings between continuous distributions are needed instead of joint distributions. Recovering an optimal map from regularized optimal plans is essential for machine learning applications, as it provides an approximate solution to the Monge problem. The barycentric projection of a solution to the OT problem or regularized OT problem can be obtained by computing its density with respect to a reference measure. This projection defines a weighted one-to-many map, where each location is sent to based on the weights. In the special case of a convex cost function, the barycentric projection has a closed-form solution. The barycentric projection is a solution to the Monge problem between source and target measures. It is often used to recover optimal maps from optimal transport plans. The projection is parameterized as a deep neural network for mapping estimations. The barycentric projection is parameterized as a deep neural network for optimal map learning. The objective is to minimize the weighted sum of squared errors by stochastic gradient descent. The opposite barycentric projection can also be computed, but the image of the source measure by the plan is only approximately equal to the target. The barycentric projection, parameterized as a deep neural network, aims to minimize errors by stochastic gradient descent. The image of the source measure by the plan is only approximately equal to the target measure \u03bd. The barycentric projection of a regularized OT plan is close to the Monge map between continuous measures, with theoretical guarantees provided by Brenier (1991) for the Monge problem. The barycentric projection, parameterized as a deep neural network, aims to minimize errors by stochastic gradient descent. The image of the source measure by the plan is only approximately equal to the target measure \u03bd. The barycentric projection of a regularized OT plan is close to the Monge map between continuous measures, with theoretical guarantees provided by Brenier (1991) for the Monge problem. Measures \u00b5 n and \u03bd n converge to \u00b5 and \u03bd respectively. Theorem 1 shows that \u03c0 \u03b5 n converges weakly to \u03c0 = (id, f )#\u00b5. The barycentric projection\u03c0 \u03b5 n also converges weakly to the true Monge map between \u00b5 and \u03bd, as proven in Theorem 2. Theoretical results show that the estimated Monge map can be used for domain adaptation and generative modeling. The training time of the dual stochastic algorithm is evaluated against a stochastic semidual. The barycentric projection aims to minimize errors using stochastic gradient descent, with the image of the source measure converging to the target measure. Theoretical guarantees are provided for the Monge problem, demonstrating the effectiveness of the approach. The training time of the dual stochastic algorithm is compared to a stochastic semidual approach. The regularized optimal transport objective is computed on a spectral transfer problem using two multispectral images from the CAVE dataset. Timing evolution of train losses is shown for different regularization values. In an unsupervised domain adaptation task, the proposed dual algorithm shows faster convergence compared to the primal formulation of the optimal transport problem. The goal is to scale optimal transport based domain adaptation to large datasets, allowing for the use of differentiable ground costs. The process involves learning an optimal map between source and target distributions, followed by training a classifier on the mapped source samples in the target domain. The adaptation process involves mapping source samples to the target set through barycentric projection, learning a classifier on the mapped samples, and withholding some samples during adaptation. Three cross-domain digit image datasets are considered: MNIST, USPS, and SVHN, with different sample sizes and image resolutions. The adaptation is done in three directions: MNIST \u2192 USPS, USPS \u2192 MNIST, and SVHN \u2192 MNIST. The adaptation process involves mapping source samples to the target set through barycentric projection. Adaptation is done in three directions: MNIST \u2192 USPS, USPS \u2192 MNIST, and SVHN \u2192 MNIST. Performance is evaluated using a 1-nearest neighbor classifier. The proposed method is compared to previous OTDA methods that do not provide out-of-sample mapping. In experiments, the proposed approach learns the Monge map with entropy or L2 regularizations. Deep features are extracted using a modified LeNet architecture, and adaptation is performed on these features. The best accuracy over hyperparameters on the target dataset is reported. Regularization parameter values range from 5 to 0.01, and Adam optimizer with batch size is used. The goal is to investigate the relative performances of large-scale OTDA in a fair setting. The parameter values for optimization and regularization are chosen from specific ranges. The Monge map is learned using a neural network with specific architecture and optimization settings. Results show that the proposed approach outperforms previous OTDA algorithms in various scenarios. Our method outperforms other algorithms on USPS\u2192MNIST and SVHN\u2192MNIST adaptation tasks. Learning a parametric mapping provides regularization and improves performance. The Monge map estimation can be used as a generator between continuous and discrete measures for a generative model. Our method utilizes Alg. 1 and Alg. 2 to compute regularized optimal transport between a Gaussian measure \u00b5 and a discrete dataset \u03bd. The generator is optimized with a cost function between latent variable X \u223c \u00b5 and discrete variable Y \u223c \u03bd, ensuring the generator is an optimal map. We preprocess MNIST data by rescaling grayscale values and use a Gaussian with mean and covariance equal to the dataset for easier learning. The target discrete measure is the preprocessed MNIST dataset, treating each grayscale image as a permutation-invariant entity. The study introduces two algorithms for large-scale computation of regularized optimal transport and learning optimal maps between probability distributions. The algorithms use fully-connected neural networks with specific activations and regularization techniques. The approach is applied to the MNIST dataset, treating each image as a 784-dimensional vector. The study introduces algorithms for computing regularized optimal transport and optimal maps in large-scale or continuous settings, enabling wider use in machine learning applications such as unsupervised domain adaptation and generative modeling. The approach is theoretically well-grounded, with future work focusing on investigating convergence rates of empirical regularized optimal plans. The study introduces algorithms for computing regularized optimal transport and optimal maps in large-scale or continuous settings, enabling wider use in machine learning applications. The convergence of the terms in the optimization process is shown, with the first term converging to 0 for \u03b5 n converging sufficiently fast to 0. The convergence result by BID10 provides positive constants for the optimization process. Choosing any (\u03b5 n) such that (21) tends to 0 provides the results. We can take \u03b5 n = \u03bb cn,\u00b5n,\u03bdn ln(n^2 ||Y n||^(1/2) R^n\u00d7d,2 M cn,\u00b5n,\u03bdn) for the convergence of (15) to 0 for Lipschitz function g \u2208 Cl(R^d \u00d7 R^d). This proves the weak convergence of (id,\u03c0 \u03b5n n)#\u00b5 n to (id, f)#\u00b5."
}