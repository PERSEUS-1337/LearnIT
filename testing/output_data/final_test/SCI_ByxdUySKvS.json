{
    "title": "ByxdUySKvS",
    "content": "Data augmentation is commonly used to enhance deep neural network training. AutoAugment has shown significant improvements in image classification tasks by automatically learning augmentation policies. However, it is not practical for large-scale problems. Adversarial AutoAugment is introduced as a computationally-affordable solution, optimizing target object and augmentation policy search loss simultaneously. This method generates adversarial augmentation policies to increase the training loss of a target network, leading to improved generalization. In contrast to prior work, the approach discussed in the curr_chunk focuses on reusing computation in target network training for policy evaluation, resulting in significant reductions in computing cost and time overhead on ImageNet. Experimental results demonstrate performance improvements on CIFAR-10/CIFAR-100 and ImageNet, achieving top-1 test error of 1.36% on CIFAR-10 and top-1 accuracy of 79.40% on ResNet-50 for ImageNet. The use of massive data has greatly contributed to the success of deep learning, with the potential for further improvements with better data augmentation techniques. Data augmentation, like rotation and flipping, can enhance neural network generalization. Manual design of augmentation policies can improve performance but requires expert knowledge. AutoAugment, a reinforcement learning method, automatically learns augmentation policies, yielding significant performance gains in image classification tasks. However, the high computing cost for training and evaluation remains a challenge. AutoAugment is a reinforcement learning method that automatically learns augmentation policies for image classification tasks, improving performance. However, the high computing cost for training and evaluating remains a challenge. The proposed method formulates the process as a Min-Max game, augmenting data with multiple pre-processing components and training a target network to minimize loss. The proposed method formulates the process as a Min-Max game, augmenting data with multiple pre-processing components and training a target network to minimize loss. The augmentation policy network is trained to maximize the training loss of the target network through generating adversarial policies. This efficient data augmentation method dynamically changes the augmentation policy along with the training state of the target network, reducing computing costs. Our method reduces computing costs by directly learning augmentation policies for target tasks through adversarial learning. The augmentation policy network generates aggressive policies based on the training losses of the target network, leading to more efficient and robust training. Our method directly learns augmentation policies for target tasks, improving generalization and outperforming previous methods. For example, we achieve a top-1 test error of 1.36% on CIFAR-10 and improve the top-1 accuracy on ImageNet from 76.3% to 79.4% without extra resources. Our method improves the top-1 accuracy of ResNet-50 from 76.3% to 79.4% without extra data, outperforming AutoAugment. Common data augmentation is used to increase dataset size and improve network generalization. Previous works have attempted to automatically learn data augmentation policies. The Smart Augmentation method merges samples to enhance network generalization. AutoAugment uses an RNN to find optimal data augmentation policies. Population based augmentation dynamically adjusts augmentation policies during training, inspired by PBT. The augmentation schedule learning in PBA involves adjusting policies manually when training a target network. Unlike traditional GANs, the proposed method focuses on finding the best augmentation policy for image transformation during training, rather than synthesizing new images. In this section, the implementation of Adversarial AutoAugment is presented. The motivation for the adversarial relation between network learning and augmentation policy is discussed, along with the search space for dynamic augmentation policy. The joint framework for network training and augmentation policy search is detailed, aiming to introduce more randomness into image transformation for generating effective samples. The implementation of Adversarial AutoAugment is discussed in this section, focusing on the dynamic and adversarial augmentation policy with the training process. The need for adaptability of the learned policy to the target network's training process is highlighted, along with the importance of improving the efficiency of policy search to avoid wastage of computation resources. In this paper, a computing-efficient policy search framework is proposed to reuse prior computation in policy evaluation. A single target network is used to evaluate different policies using training losses of corresponding augmented instances. The augmentation policy network is learned from the target network's intermediate state, resulting in more aggressive and adaptive augmentation policies. This approach leads to the target network learning more robust features to handle harder examples efficiently. The search space structure of AutoAugment is retained, defining an augmentation policy composed of 5 sub-policies with two image operations each. The search space for augmentation policies includes 16 image operations, with each sub-policy containing two operations with corresponding parameters. The 5 best policies are combined to form a single policy with 25 sub-policies. Only one sub-policy is randomly selected for each image in a mini-batch. The probability of each operation is removed to avoid delays in feedback from the target network's intermediate state. The augmentation policy search space includes 16 image operations with corresponding parameters. The magnitude of operations is set in a moderate range for convergence during adversarial learning. The search space for the policy in each epoch has approximately 1.1x10^22 possibilities. The dynamic policy evolves with training, gradually increasing in difficulty. The adversarial framework optimizes network training and augmentation policy search together. The augmentation policy network A(\u00b7, \u03b8) acts as an adversary to increase the training loss of the target network F(\u00b7, w) through adversarial learning. The target network is trained with augmented instances to promote invariant learning, and the augmentation policy network is trained using the losses of different augmentation policies applied on the same data. The learning process involves minimizing the loss function L[F(x, w), y] using vanilla SGD with a learning rate \u03b7 and batch size N. To enhance the convergence performance of DNNs, more efficient data augmentation is implemented with the help of the augmentation policy network. The training procedure involves introducing multiple instances of each input example augmented by adversarial policies. This leads to a larger batch training or an average over instances, reducing gradient variance and accelerating convergence. The augmentation policy network is designed to increase the training loss of the target network with harder augmentation policies, aiming to reduce gradient variance and accelerate convergence. The network is implemented as a RNN controller generating sub-policies with different operations and parameters. In this paper, the AutoAugment setting involves using 5 sub-policies. The RNN controller predicts 20 discrete parameters for the augmentation policy. To address the issue of non-differentiable augmentation operations affecting gradient flow, the REINFORCE algorithm is applied to optimize the augmentation policy network. The training loss is replaced with a moving average over mini-batches to reduce gradient variance. The training procedure involves using a moving average over mini-batches and normalizing it among instances. The adversarial learning of target network training and augmentation policy search is summarized in Algorithm 1. Experiment settings are detailed, and the method is evaluated on CIFAR-10/CIFAR-100, ImageNet, showing state-of-the-art performance with higher efficiency. Target network F(\u00b7, w) and augmentation policy network A(\u00b7, \u03b8) are initialized, input examples x and labels y are provided, and policies are generated and applied to batch data for updating parameters. The RNN controller is implemented as a one-layer LSTM with a hidden size of 100 and an embedding size of 32. Adam optimizer with an initial learning rate of 0.00035 is used to train the controller, with an entropy penalty to avoid rapid convergence. Results are the mean of five runs on CIFAR-10 dataset with 60000 images. Training and test sets have 50000 and 10000 images respectively, each belonging to one of 10 classes. The RNN controller is implemented as a one-layer LSTM with a hidden size of 100 and an embedding size of 32. Adam optimizer with an initial learning rate of 0.00035 is used to train the controller, with an entropy penalty to avoid rapid convergence. Results are the mean of five runs on CIFAR-10 dataset with 60000 images. Training and test sets have 50000 and 10000 images respectively, each belonging to one of 10 classes. The evaluated models include Wide-ResNet-28-10, Shake-Shake, and PyramidNet+ShakeDrop, trained on the full training set with various data augmentation techniques. The RNN controller is implemented with specific parameters. Different learning rate schedules are used for different models. Optimal M is chosen based on performance evaluation. Test accuracy improves with increasing M up to 8. Test error results on CIFAR-10 show better performance compared to previous methods. Top-1 test accuracy improvements are reported for Wide-ResNet-28-10 model. The study achieved a top-1 test error of 1.36% with PyramidNet+ShakeDrop, outperforming the current state-of-the-art. The augmentation policies learned with this method on CIFAR-10 show an increase in certain operations over time. Geometric transformations like TranslateX, TranslateY, and Rotate are favored, unlike color-focused AutoAugment. Large magnitudes gain higher percentages during training, but low magnitudes remain considerable towards the end. Our method does not learn transformations with extreme magnitudes, achieving state-of-the-art performance on CIFAR-100. ImageNet dataset with 1.2 million training images is used to train ResNet models from scratch. Baseline augmentation includes resizing, cropping, and flipping images. AutoAugment and our method improve on these techniques. Our method achieves a top-1 accuracy of 79.40% on ImageNet with ResNet-50, the highest reported for this model. Replacing ResNet-50 with ResNet-50-D further improves accuracy to 80.00%. Different augmentation methods are compared, showing the effectiveness of our proposed method. The study compares different data augmentation methods for training models, including Fixed, Random, and Ours. Fixed shows a 0.99% error reduction compared to Baseline, indicating improved generalization. Random performs 1.02% better than Fixed, reducing overfitting to some extent. The Ours method achieves the best test error of 20.60%, demonstrating the effectiveness of adversarial policies sampled during training. Our method achieves the best test error of 20.60% by augmenting samples with adversarial policies, showing that these policies generated by the policy network are more adaptive to training. The computing cost in policy search becomes negligible as the computation in target network training is reused. Time overhead is minimized by training one target network with a large batch distributedly and simultaneously. The joint optimization of target network training and augmentation policy search significantly reduces time overhead. Comparing our method to AutoAugment, we show a 12\u00d7 lower computing cost and 11\u00d7 shorter time overhead. The transferability of learned augmentation policies is demonstrated by applying them to various models with positive results. The transferability of learned augmentation policies is demonstrated by applying them to various models with positive results. A competitive performance can still be achieved through direct policy transfer, but there is an obvious performance degradation compared to the proposed method. Adversarial learning is introduced into automatic data augmentation to combat overfitting and generate robust features in the target network, leading to significant performance improvement. The augmentation policy search is performed along with the training of the target network, reducing time overhead. The computation in network training is reused for policy evaluation, reducing search cost and enhancing computing efficiency."
}