{
    "title": "rkl4M3R5K7",
    "content": "In this paper, the focus is on designing optimal adversarial noise algorithms to induce misclassification in ensemble classifiers. The problem is framed as a two-player, zero-sum game between a learner and an adversary, highlighting the need for randomization in attacks. The main challenge is developing best response oracles for finding equilibrium strategies. The goal is to create scalable noise generation techniques for robust machine learning. In this study, the focus is on developing scalable noise generation algorithms for deep neural networks to improve adversarial attacks in ensemble classifiers. The approach is proven to be optimal for linear classifiers by minimizing a quadratic function over convex polytopes. The research addresses the sensitivity of machine learning algorithms to noise and instability, showcasing the effectiveness of the proposed noise generation techniques. Adversarial attacks in machine learning focus on inducing misclassification by perturbing data points past decision boundaries. Attacks designed for linear classifiers tend to generalize well to deep neural networks. Recent work has also focused on designing attacks on ensembles of different classifiers. Recent work has shown that attacking an ensemble of classifiers can fool state-of-the-art models. The challenge lies in designing effective attacks on multiple classifiers, as perturbations that fool one model may not work on another due to different decision boundaries. Randomizing over classifiers can also prevent deterministic attacks. In attacking a set of classifiers, randomization is necessary to avoid deterministic attacks. An optimal adversarial attack involves randomizing with equal probability amongst noise vectors, proving to be highly effective. This approach is equivalent to finding strategies at equilibrium in a zero-sum game between a learner and an adversary. The Noise Synthesis Framework (NSFW) is described as a method for generating adversarial attacks in a zero-sum game between a learner and an adversary. It is effective in designing adversarial noise to deceive neural networks using projected gradient descent on a chosen loss function. Projected gradient descent on an appropriately chosen loss function as a proxy for a best response oracle achieves significantly improved performance in adversarial attacks. This approach is well-principled, especially for linear classifiers with a pure Nash equilibrium. However, if the game lacks a pure Nash equilibrium, finding an optimal adversarial attack becomes NP-hard. In Section 2, the problem of designing optimal adversarial noise is formulated as a two-player, zero-sum game. The approach for finding strategies using MWU and proxies for best response oracles is discussed, along with guarantees for linear classifiers. Section 3 presents experiments on adversarial attacks in machine learning classifiers. The field of adversarial attacks has gained attention, with efforts focused on inducing misclassification across multiple models and the effectiveness of ensembles in generating and defending against attacks. Our work empirically demonstrates the effectiveness of ensembles in generating and defending against adversarial attacks. Unlike previous studies focusing on security perspectives and black-box attacks, we approach the problem theoretically and introduce a formal characterization of optimal attacks against classifiers. Our algorithms in the linear setting provide strong performance guarantees, motivating a natural extension for deep learning models. In our experiments, we show how algorithms extend noise in deep learning for state-of-the-art results. An adversarial attack is a set of noise vectors with bounded norms that induce misclassification on a classifier. Our model can handle various norms, and we focus on the case where each vector has an \u21132 norm less than a fixed value \u03b1. Optimal adversarial attacks are equilibrium strategies in a zero sum game, maximizing the minimum 0-1 loss of classifiers in a set C. This is equivalent to a best response in a two-player, zero sum game between a learner and an adversary, where the adversary's payoff is the average 0-1 loss of the learner. The equilibrium strategy maximizes the minimum expected payout between classifiers and noise vectors. The equilibrium strategy of the game is the pair of distributions p, q that maximize the minimum loss. The optimization problem of designing optimal adversarial attacks reduces to finding strategies at equilibrium in a zero sum game. The Multiplicative Weight Updates algorithm is used to efficiently compute equilibrium strategies of zero sum games. The algorithm returns distributions p, q that are within \u03b4 of the equilibrium value of the game with a certain number of calls to a best response oracle. This work focuses on developing attacks on neural networks and linear models. In this work, the focus is on developing attacks on neural networks and linear models. The main challenge lies in computing a best response strategy using projected gradient descent (PGD) on a weighted sum of loss functions. Maximize the loss of the learner by applying PGD to compute best responses effectively. The approach of using loss functions to compute best responses is guaranteed to converge to the optimal solution for linear classifiers. The effectiveness of this approach is demonstrated in improving state-of-the-art attacks. Adversarial noise generation is based on a geometric characterization of attacks, where selecting a distribution over classifiers partitions the input space into regions with associated loss values. Computing a best response strategy for the adversary becomes a search problem. The learner computes a best response strategy for the adversary by searching for points in each region within the noise budget. The key observation is that regions induced by linear classifiers are convex, leading to solving a series of quadratic programs for optimal adversarial attacks. Selecting a distribution over linear classifiers partitions the input space into disjoint, convex sets associated with unique label vectors. The learner designs best response oracles for linear classifiers via convex optimization, focusing on \"one-vs-all\" classifiers. Each region is associated with a unique label vector, and convexity is shown by intersecting hyperplanes. Implementing a best response oracle for linear classifiers involves solving a series of quadratic programs for optimal adversarial attacks. The lemma states that for linear classifiers, implementing a best response oracle involves minimizing a quadratic function over a set of convex polytopes. The loss of the learner can be maximized individually for each point in the input space, with each point associated with a specific convex region. To compute a best response, the learner iterates over all regions to choose the one with the highest loss, minimizing the \u2113 2 norm of a perturbation to find points in each region. The number of polytopes is exponential in the number of regions. The text discusses the exponential relationship between the number of polytopes and classifiers in the context of pure strategy Nash equilibrium. It explains how PGD applied to reverse hinge loss guarantees convergence to a point achieving maximum for binary classifiers. Theorem 1 states that running PGD on a finite set of linear binary classifiers converges to a point within a specified noise budget. Running PGD for T = 4\u03b1/ iterations on the objective converges to a point within the pure strategy Nash equilibrium if it exists. For a finite set of linear multilabel classifiers C, running PGD on the objective converges to a global minimum, which is zero if a pure equilibrium exists. The requirement for a feasible point within T is necessary to avoid brute force search. Designing an efficient algorithm to find the region associated with the highest loss is unlikely as the decision version of the problem is NP-hard even for binary linear classifiers. Theorem 2 states that finding a point with limited noise budget that results in a specific loss for a set of binary linear classifiers is NP-complete. However, this complexity does not hinder our ability to compute optimal adversarial examples, especially when the learner has access to only a small number of classifiers. By solving a convex program over all regions, we can find an optimal adversarial attack even in the absence of a pure Nash equilibrium. NSFW is evaluated for fooling classifiers by comparing against noise generated by state-of-the-art attacks on an ensemble classifier. Theoretical analysis of linear models is used to approximate a best response oracle for optimal adversarial attacks. In this domain, BID4 introduced \u2113 ut. The set of classifiers C for MNIST includes 5 convolutional neural networks trained on 55k images, all achieving over 97% accuracy on the test set. For ImageNet, C consists of InceptionV3, DenseNet121, ResNet50, VGG16, and Xception models with pre-trained weights. Visual comparison of misclassification using adversarial attacks shows noise levels needed for similar misclassification by different attack methods. To evaluate the effectiveness of different attacks on a set of classifiers, various methods such as NSFW, MIM, FGM, and Madry attack were compared. Different noise budgets were required for each attack, with NSFW needing \u03b1 set to 300 and MIM needing \u03b1 set to 2000. Baseline attacks were conducted using Fast Gradient Method, Projected Gradient Method, and Momentum Iterative Method on an ensemble of classifiers. The noise budget \u03b1 was selected by comparing against the average \u2113 2 norm. The noise budget \u03b1 was selected by comparing against the average \u2113 2 distortion reported by similar papers. For MNIST, a budget of 3.0 was chosen based on previous values. However, for ImageNet, a budget of 300 was chosen to ensure visually indistinguishable perturbed images. The noise budget used was 300.0 in the \u21132 norm. NSFW outperformed noise generated by an ensemble classifier for all attack algorithms. Details on model architectures and code will be made public after the review period. Test set accuracies of ImageNet classifiers can be found on the Keras website. The Momentum Iterative Method won the 2017 NIPS adversarial attacks competition. The root mean squared deviation is defined in their paper. Image dimensions for ImageNet are 224\u00d7224\u00d73 and for MNIST are 28\u00d728\u00d71. For experiments, NSFW was run for 50 MWU iterations on MNIST models and 10 iterations on ImageNet classifiers. The optimization process used PGD with the Adam optimizer and untargeted reverse hinge losses. Images were clipped at each iteration. Evaluation involved selecting 100 images randomly from both MNIST and ImageNet datasets. On both MNIST and ImageNet datasets, 100 images were randomly selected from correctly classified test sets. The empirical accuracy of classifiers in set C was evaluated on NSFW and three baseline attacks. Results show that our algorithm optimizes over models with minimal noise on ImageNet. The maximum accuracy under NSFW is 17%, while the best ensemble attack reaches 68%. Increasing noise budget is required for ensemble baselines to match performance. The study compares different attacks' noise budgets to achieve 17% accuracy. NSFW outperforms ensemble attacks on MNIST with a max accuracy of 22.6% vs. 48%. The decision boundary analysis shows NSFW's effectiveness in fooling models. Saliency maps visualize class boundaries of convolutional neural networks. The saliency map highlights relevant features in classifying images for a specific label, aiding in understanding the decision boundary of a model. Ensemble classifiers obscure key information about individual models' decision boundaries compared to analyzing individual gradients. The study compares NSFW's effectiveness in fooling models using different noise functions and budgets. NSFWOracle and NSFW-Untargeted are variations of the NSFW algorithm using different noise functions and budgets. The label iteration method is explained, and experiments were conducted on linear classifiers to analyze attack performance. The ensemble attack optimizes noise on an equal weights ensemble of models, while convergence of NSFW on linear binary classifiers is compared to neural nets. In experiments on linear classifiers, two sets of 5 linear SVM classifiers were trained on MNIST for binary and multiclass classification. Models achieved test accuracies above 90%. A label iteration method was used to generate untargeted noise for comparison across different best response proxies. PGD on the targeted reverse hinge loss for labels other than y is compared to untargeted reverse hinge loss, showing convexity benefits. Results are compared to noise generated by optimal attack on an ensemble of models in C. Optimal attack is computed using best response oracle for single model case in C, with noise scaled to norm \u03b1. Significant difference between optimal attack and other approaches, even for linear models, is observed. Empirical gap is seen between NSFW with best response oracle and NSFW with proxy best. In theory and practice, using appropriately designed best response oracles, such as NSFW equipped with the best response oracle, outperforms NSFW with proxy best response oracles. The adaptivity of MWU is crucial for computing optimal attacks, especially when dealing with diverse sets of classifiers. NSFW is introduced as a principled approach that is provably optimal on linear classifiers, offering significant benefits in designing adversarial attacks. NSFW is a principled approach that is provably optimal on linear classifiers and effective on neural networks. The key technical aspect is designing best response oracles through a geometrical characterization of the optimization landscape. NSFW shows fast convergence on MNIST and ImageNet deep learning models, reaching the equilibrium noise solution almost immediately. Additional experiments and details on experimental setup are provided, with misclassification results shown in Tables 1 and 2. In the experiments setup, hyperparameters for baseline attacks were set based on values from original papers. Different attack methods were run with specific parameters and constraints. The NSFW algorithm was used with a noise budget of \u03b1 = .2k for k \u2208 [5] to compute best responses for linear multiclass classifiers. In the experiments setup, hyperparameters for baseline attacks were set based on values from original papers. The NSFW algorithm was used with a noise budget of \u03b1 = .2k for k \u2208 [5] to compute best responses for linear multiclass classifiers. Additionally, binary classifiers had smaller margins, so NSFW was run with \u03b1 = .05 + .1k for k \u2208 [5]. The weight adaptivity experiment involved training 5 linear binary SVM classifiers on a modified MNIST dataset with 80% of input dimensions zeroed out for each model. The NSFW algorithm improves classification accuracies for deep learning MNIST models under different noise algorithms, ensuring diversity in feature selection. It outperforms state-of-the-art attacks and optimizes over the entire set of classifiers. There is a significant difference between average and maximum accuracy of classifiers, highlighting the need for noise algorithms to inhibit the best model's performance. NSFW equipped with the best response generates noise for linear models more effectively than other approaches. In this section, a theoretical justification is provided for attacking an ensemble of classifiers using gradient-based optimization. The process involves running gradient descent on a loss function to find adversarial examples that are misclassified by the ensemble model. The text discusses the issues with perturbing examples in an ensemble classifier, highlighting how the ensemble can obscure valuable information and not guarantee noise transfer across models. The strength of an ensemble classifier depends on its weakest weighted majority. A distribution over a set of linear classifiers partitions the input space into disjoint sets, each with a unique label vector. The sets are pairwise disjoint and have a bijection with label vectors. The strength of an ensemble classifier depends on its weakest weighted majority. Each classifier predicts a single label for x \u2208 T j. The sets T j are convex, and the expected loss of the learner is determined by the correctness of the predictions of all individual classifiers. The set R d \\ i T i consists of points where there are ties for the maximum valued classifier, forming a subset of points at the intersection of two hyperplanes. The intersection of two hyperplanes, denoted as K, has measure zero. For linear classifiers, a best response oracle reduces to minimizing a quadratic function over convex polytopes. The problem of finding the minimum perturbation v j such that x + v j \u2208 T j can be framed as minimizing a convex function over a convex set. The optimal solution for finding the minimal perturbation v such that x + v \u2208 T j involves iterating over all sets T j and selecting the perturbation with \u2113 2 norm less than \u03b1 that results in the highest loss. This process can be expressed as the minimization of a quadratic function subject to linear inequalities, extending the results from linear classifiers to other methods for multilabel classification. The analysis extends linear classifiers to other methods for multilabel classification, showing how \"all-pairs\" or multivector models can be converted to \"one-vs-all\" classifiers. This transformation allows for the application of results to these approaches. The multivector construction \u03a8(x, y) = 0 ensures convergence in projected gradient descent for linear binary and multilabel classifiers. The text discusses the convergence of projected gradient descent for linear binary and multilabel classifiers, showing the rate of convergence for a convex function. It also mentions the existence of a pure strategy Nash equilibrium and the normalization of weight vectors for multiclass cases. The text discusses the complexity of finding a vector with a bounded norm that minimizes loss for a set of binary linear classifiers. It proves NP-completeness by reducing from Subset Sum problem. The text discusses the complexity of finding a vector that minimizes loss for binary linear classifiers by reducing from the Subset Sum problem. It shows that there is a subset that sums to B if and only if the learner achieves loss B on a region T j. Each T j corresponds to a quadrant of R n, and there exists a feasible point within each set T j, making all subsets valid. The text discusses finding a vector that minimizes loss for binary linear classifiers by reducing from the Subset Sum problem. It shows that a subset sums to B if the learner achieves loss B on a region T j. Instances of Subset Sum only have values in the range [0, 1], which can be normalized to form a valid probability distribution."
}