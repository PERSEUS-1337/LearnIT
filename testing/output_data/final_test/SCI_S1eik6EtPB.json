{
    "title": "S1eik6EtPB",
    "content": "The worst-case training principle, known as adversarial training (AT), is effective for improving adversarial robustness against input perturbations. Research has not extensively explored min-max optimization beyond AT for adversarial attack and defense. A general min-max problem can be formulated to minimize maximal loss induced by a set of risk sources. This framework can address various challenges like attacking model ensembles and devising universal perturbations. The method also provides self-adjusted domain weights for solving these problems. Our method provides self-adjusted domain weights to explain attack and defense difficulty levels across multiple domains, leading to significant performance improvements. Training adversarially robust models, especially for deep neural networks, involves min-max optimization through adversarial training. This approach enhances robustness against input perturbations and addresses challenges like attacking model ensembles and devising universal perturbations. In this paper, the authors explore the potential of different min-max formulations and optimization techniques beyond Adversarial Training (AT) to enhance research in adversarial robustness. They propose a new min-max framework that shows substantial performance gains and the ability to interpret risks autonomously. Their approach involves minimizing the maximal loss induced by a set of risk sources, with a focus on the probability simplex of domains. This method offers a general formulation for min-max optimization, distinct from AT, and demonstrates effectiveness across various adversarial attack and defense tasks. The text discusses a new min-max framework for adversarial attacks and defenses, showing that various problem setups can be reformulated under this framework. Current methods for solving these tasks often rely on simple heuristics, leading to an arms race between adversaries. The text discusses an arms race between adversarial attacks and defenses, with a focus on the transferability of adversarial examples across multiple domains. Research is moving towards devising universal adversarial perturbations over model ensembles, input samples, and data transformations. However, current approaches suffer from a significant performance loss. The text discusses the limitations of current approaches in adversarial attacks and defenses, highlighting the performance loss due to uniform averaging. It introduces a min-max method for comparison and explores generalized adversarial training under multiple perturbations. The min-max framework is inspired by robust learning literature and fits into various attack settings for adversarial exploration. The text discusses a generalized adversarial training framework for improving model robustness under various adversarial attacks. It introduces a min-max method for robust learning across different domains, providing consistently robust worst-case performance. The text discusses a min-max method for robust learning across different domains, providing consistently robust worst-case performance. The inner maximization problem always returns the one-hot value of w, reducing generalizability and inducing instability. Introducing a strongly concave regularizer helps mitigate this issue. The text discusses penalizing the distance between worst-case loss and average loss over multiple domains, using a regularization parameter \u03b3. The weight w indicates the importance of each domain, with larger weights signifying higher importance. This approach can be applied to various adversarial attack and defense problems in machine learning systems. Adversarial attacks aim to find perturbations that can fool multiple machine learning models simultaneously. The difficulty level of attacking each model or example is encoded in the objective function, with the goal of finding universal perturbations that can deceive the models or examples effectively. Adversarial attacks aim to find perturbations that can deceive machine learning models. The difficulty level of attacking each example is encoded in the objective function, with the goal of finding robust attacks that are resistant to data transformations. Adversarial attacks aim to deceive machine learning models by finding perturbations. Generalizing Adversarial Training (AT) across multiple attack types involves using a finite-sum formulation with different types of perturbations. By mapping attack types to domains, AT can be performed against the strongest adversarial attack to avoid blind spots. The problem is simplified to a min-max form to find robust defenses. The text discusses the regularized formulation of Adversarial Training (AT) under multiple perturbations, emphasizing the importance of domain weights in learning the significance of different attacks. The proposed formulation aims to strike a balance between maximum and average attack performance, enhancing the stability of the learning process. Our work focuses on the interpretability of different attacks and proposes the alternating one-step projected gradient descent (APGD) method to efficiently solve problems of robust adversarial attacks. The method involves one-step PGD for outer minimization and one-step projected gradient ascent for inner maximization, with each alternating step having a closed-form expression. The APGD method efficiently solves robust adversarial attacks by using one-step PGD for outer minimization and one-step projected gradient ascent for inner maximization. The computational complexity lies in computing the gradient of the attack loss w.r.t. the input, making APGD computationally efficient like PGD. The key to obtaining the closed-form updating rule is the projection operation, which depends on the value of p in {0, 1, 2, \u221e}. The closed-form solution for projection operation onto a constraint set X is derived under different p norms. The inner maximization involves one-step PGD to update w, utilizing the closed-form projection operation onto the probabilistic simplex. The root \u00b5 is found within a specific interval using the bisection method for convergence analysis. The AMPGD method is proposed to solve a regularized version of a problem in a non-convex non-concave min-max setting, with convergence analysis based on the gradient primal-dual optimization framework. The AMPGD method is proposed for solving a general non-convex non-concave min-max problem, requiring multi-step PGD for inner maximization. The algorithm involves updating domain weights w and adversarial perturbations \u03b4 iteratively, with projection operations derived from closed-form expressions. The theoretical convergence guarantees for this problem remain an open question. The AMPGD method aims to solve a non-convex non-concave min-max problem by utilizing multi-step PGD for inner maximization. Recent studies have shown that promoting diversity among adversarial attacks can enhance the robustness of ensemble models. By measuring the diversity between attacks through perturbation directions, a strong correlation was found, leading to a proposal to enhance diversity through an orthogonality-promoting regularizer. The orthogonality-promoting regularizer in (Pang et al., 2019) encourages diversified prediction of ensemble models by maximizing input gradient orthogonality. The min-max optimization strategy aims to enhance attack effectiveness from diverse perturbation directions while robustifying the model against such attacks. The proposed approach outperforms state-of-the-art methods like ensemble PGD and EOT in three attack tasks. The effectiveness of generalized Adversarial Training (AT) for various adversarial perturbations is demonstrated. The use of trainable domain weights can adjust the risk level of different attacks during training. Promoting diversity of attacks improves adversarial robustness. The APGD/AMPGD algorithm is evaluated on MNIST and CIFAR-10 datasets with diverse image classifiers. The strength of min-max optimization also benefits attack generation. The strength of min-max optimization is also beneficial for attack generation, focusing on crafting adversarial examples using the C&W loss function. An ensemble attack against multiple classifiers is proposed, resulting in significant improvements in attack success rates on MNIST and CIFAR-10 datasets. Our min-max APGD outperforms ensemble PGD in ASR on MNIST and CIFAR-10 datasets. Model C and D are more difficult to attack due to higher test accuracy on adversarial examples. APGD handles the worst case scenarios while slightly sacrificing performance on other models, resulting in improved ASR on {C, D} and slight degradation on {A, B}. More results on CIFAR-10 and GoogLeNet are provided. The study highlights the importance of tracking domain weights for model robustness and understanding attack procedures. The hypothesis that a model with higher robustness corresponds to larger domain weights is supported by empirical evidence. Universal perturbation using APGD is evaluated on MNIST and CIFAR-10 datasets, showing varying levels of model robustness. The study evaluates the min-max strategy (APGD) for universal perturbation on MNIST and CIFAR-10 datasets, achieving higher ASR gp compared to the averaging strategy. The approach successfully attacks 'hard' images and offers interpretability of \"image robustness\" through self-adjusting domain weights. The min-max universal perturbation provides interpretability of \"image robustness\" by associating domain weights with image visualization. Robust adversarial attack over data transformations achieves state-of-the-art performance in producing adversarial examples robust to data transformations. EOT can be derived as a special case when the weights satisfy a specific condition. Various image transformations are applied to input samples to create transformed variants for analysis. ASR avg and ASR gp are used to measure the ASR over all transformed images. Our approach leads to a 9.39% averaged lift in ASR gp over given models on CIFAR-10 by optimizing weights for various transformations. Generalized AT scheme produces models robust to multiple types of perturbation, resulting in stronger overall robustness. Training performance is measured using Acc max adv and Acc avg adv. The test accuracy of MLP is evaluated under different training schemes, including natural training, single-norm, multi-norm, and generalized AT with diversity-promoting attack regularization. Multi-norm generalized AT shows better performance when facing multiple types of attacks simultaneously, outperforming single-norm AT. The min-max strategy slightly outperforms the averaging strategy under multiple attacks. The min-max strategy slightly outperforms the averaging strategy under multiple perturbation norms, boosting adversarial test accuracy. DPAR enhances diversified p attacks in adversarial training, with deeper insights provided in Figure 3 on the performance of generalized AT. Training with fixed \u221e at 0.2 and varying 2 from 0.2 to 5.6 shows the strength of 2-attack increasing, consistent with min-max defense. Min-max or avg training may not always lead to the best performance, especially when strengths of attacks diverge greatly. In this paper, a general min-max framework is proposed for adversarial attack and defense settings, showing significant improvement in various tasks compared to traditional methods. The proposed AMPGD achieves high worst-case robustness and faster convergence, demonstrating the effectiveness of the training scheme. Extensive experiments validate the benefits of the min-max scheme in achieving robust models. The proposed algorithms show significant improvement on attack and defense tasks compared to previous approaches. Specifically, there are improvements in attacking model ensembles, devising universal perturbations, and data transformations under CIFAR-10. The minmax scheme generalizes adversarial training for multiple attack types, achieving faster convergence and better robustness. Additionally, the approach provides a tool for self-risk assessment by learning domain weights. The closed-form solution for the Euclidean projection onto a constraint set X is given for p \u2208 {0, 1, 2}. When p = 1, \u03b4 * is determined by elementwise soft-thresholding. If p = 0 and \u2208 N + , \u03b4 * is given by the -th largest element of \u03b7. The Lagrangian of the problem is minimized by \u03b4 *, obtained through soft-thresholding. The primal, dual feasibility, and complementary slackness are determined by solving equations using the Bisection method. The Lagrangian is minimized to find the optimal solution. The Euclidean projection onto a constraint set X involves clipping elements to the box constraint and then projecting onto the 0 norm. The Euclidean projection onto the 0 norm involves clipping elements to the box constraint. The problem is convex and can be solved using KKT conditions. The approach differs from previous work by incorporating the p norm as a hard constraint and introducing a nonnegative Lagrangian multiplier. This allows for projection onto the intersection of box and p constraints, which can be combined with an attack loss for generating adversarial examples. The experiment setup in 2015 involved using the same architecture as original papers for four models, training them on the CIFAR-10 dataset for 50 epochs with Adam. For CIFAR-10 classifiers, models were trained for 250 epochs with SGD, with a learning rate reduction at epoch 100 and 175. Initial learning rates varied for different models. No data augmentation was used in training. The adversarial examples are generated using 20-step PGD/APGD with a confidence parameter \u03ba = 50. Cross-entropy loss is supported. The proposed algorithms are robust and not greatly affected by hyperparameter choices. Parameters are not finely tuned on the validation set. Deterministic and stochastic transformations are considered in experiments, with specific settings outlined in Table 3 and Table A6. In Table A5, randomness is introduced for drawing samples from the distribution. In Table A5, randomness is introduced for drawing samples from the distribution, including rotating and cropping images. The min-max approach results in a 15.69% improvement on ASR for model ensembles A, E, F, H on CIFAR-10. The proposed min-max framework outperforms static heuristic weighting schemes by a large margin, demonstrating the effectiveness of self-adjusted weighting factors. Our min-max APGD approach shows significant improvement compared to static settings, automatically learning weights for different examples during ensemble attack generation. It results in 4.31% and 8.22% averaged lift over four models on CIFAR-10. Sensitivity analysis on the quadratic regularizer in the min-max framework shows its impact on performance. Figure A1 demonstrates the impact of varying \u03b3 values on the performance of the ensemble attack, with \u03b3 around 4 yielding the best results. The min-max training scheme enhances overall robustness compared to vanilla AT, especially under worst perturbations. Our min-max generalized AT improves robustness under worst perturbations by 6.27% and 17.63% on S compared to single-type AT. The weighting factor w in the probability simplex adjusts automatically in our AMPGD algorithm to defend against the strongest attack. Shafahi et al. (2018) also propose a variant of adversarial training to defend universal perturbations over multiple images. The proposed approach outperforms UAT in defending against per-image p attacks, with an average improvement in adversarial test accuracy. It shows minimal degradation in ATA against universal attacks and similar normal test accuracy compared to UAT. The proposed approach achieves better overall robustness and competitive performance in defending universal perturbations. The min-max training scheme may not always result in the best performance against strong perturbations, especially when attack strengths differ significantly. This finding is analyzed based on recent work by Araujo et al. (2019). Recent work by Araujo et al. (2019) explores the overlap of adversarial examples generated by different attacks on test images. The study compares the p norms of adversarial examples and highlights the limitations of interchanging attacks with different norms. The real range of perturbation magnitude for maintaining intersected norm balls is discussed. The study discusses the limitations of interchanging attacks with different norms, highlighting the real range of perturbation magnitude for maintaining intersected norm balls. Adversarial examples generated by different attacks show that using \u221e attack is sufficient to handle 2-attack in the majority of cases, resulting in better performance. The average p distance of adversarial examples with 2 increasing is also analyzed, showing that the model robustness affects the behavior of attacks. The study discusses the limitations of interchanging attacks with different norms, highlighting the real range of perturbation magnitude for maintaining intersected norm balls. The average \u221e -norm increases substantially as 2 increases from 0.5 to 2.5, with the average \u221e norm getting close to 0.2 at 0.85. The min-max generalized AT leads to faster convergence compared to average-based AT, especially when the strengths of two attacks diverge greatly. Tracking domain weight w of the probability simplex is an exclusive feature of solving problem 1. The strength of weight w in understanding optimization procedures and adversarial robustness is demonstrated through measuring \"image robustness\" and devising universal perturbations for multiple input samples on MNIST. Weight w in APGD is highly correlated under different p norms and related to the minimum distortion required for attacking single images. Adversarial training of MNIST models shows the impact of weight w in single and multiple attacks, with perturbation magnitude \u221e fixed at 0.2 during training. The perturbation magnitude is fixed at 0.2 during training, with changes in parameters from 0.5 to 3.0. Adversarial examples are crafted using 20-step \u221e-APGD with specific parameters. The ratio of adversarial and benign examples in training is set at 1.0, with a hyperparameter \u03bb = 0.1 for DPAR. Weight w indicates image robustness, affecting universal perturbation generation."
}