{
    "title": "BkeSusCcYm",
    "content": "Data-parallel neural network training involves exchanging large gradients, but gradient dropping can slow convergence. To improve convergence, we suggest combining locally computed gradients with sparse global gradients. Empirical results on machine translation tasks show that this approach accelerates convergence by 48% compared to non-compressed multi-node training and 28% compared to vanilla gradient dropping. This method does not compromise the final model quality. Gradient dropping and Deep Gradient Compression are techniques used to compress network traffic in data-parallel training by sending a small fraction of the largest gradients. However, excessive compression can slow down convergence and reduce the final model quality. In vanilla gradient dropping, nodes update with sparse gradients exchanged over the network. Can we combine dense local gradients with sparse global gradients to improve convergence? Three ways are proposed and evaluated. Gradient dropping compresses communication by selecting the top 1% of gradients from each node. An optimizer like SGD or Adam uses the summed sparse gradients from all nodes. Deep gradient compression (DGC) BID10 introduces modifications to reduce the quality damage of gradient dropping in data-parallel training. It combines sparse global gradients with dense local gradients to improve convergence. Gradient dropping significantly reduces communication costs and speeds up parameter updates, but may degrade convergence per update. When network bandwidth is limited, time to convergence is reduced. Our work aims to address the quality damage caused by gradient dropping in data-parallel training. We use local gradients for every parameter update, unlike DGC which relies on a sparse gradient. By combining dense locally computed gradients with compression techniques, we aim to improve convergence in the model. Our work aims to address quality damage from gradient dropping in data-parallel training by combining dense locally computed gradients with compression techniques to improve model convergence. We propose formulas to incorporate local gradients for a better approximation of the compressed gradient, enhancing parameter updates. To address quality damage from gradient dropping in data-parallel training, our approach combines dense locally computed gradients with compression techniques to enhance model convergence. By incorporating local gradients and correcting for double counting, we aim to improve the approximation of the compressed gradient for better parameter updates. In data-parallel training, to address gradient dropping issues, the approach combines locally computed gradients with compression techniques to enhance model convergence. The unsent gradients are applied as local context instead of accumulating them, updating parameters with clearing errors at every step. To prevent parameter divergence, parameters are periodically averaged across nodes, with synchronization done infrequently to maintain training speed. Parameters are also averaged across all nodes every 500 steps using Marian toolkit on nodes with four P100s each. The text discusses training a Romanian-to-English neural machine translation system using data-parallel training with Marian toolkit on nodes with four P100s each. The model consists of a single layer attentional encoder-decoder bidirectional LSTM with 119M parameters, trained for up to 14 epochs using the Adam optimizer. The system is tested on various tasks including Ro-En Machine Translation. The text discusses training an English-to-German machine translation system with 19.1M sentence pairs using a model based on BID16 with 8 LSTM layers and 225M parameters. The model is trained for up to 8 epochs with Adam optimizer, focusing on multi-node settings and adjusting hyperparameters for larger batch sizes. Single model scores are reported instead of ensemble scores. In multi-node settings, synchronous stochastic gradient descent is used with larger batch sizes for training the English-to-German machine translation system. The Adam optimizer is scale-invariant, allowing for parameter adjustments in both single and multi-node settings. In multi-node settings, the learning rate is scaled by 4x to account for larger gradients. Learning rate warm-up is applied to overcome model instability, with a warm-up period of 2000 steps in Ro-En and 4000 steps in En-De experiments. The remaining hyperparameters are adjusted accordingly for fair comparison between single-node and multi-node settings. In multi-node experiments, gradient dropping increases training speed but hinders convergence. DGC is proposed to minimize convergence damage. An experiment with drop ratio warm-up shows improved convergence in multi-node Ro-En settings. The experiment results in FIG1 indicate that compression ratio warm-up can enhance convergence with gradient dropping. The study opts to use compression ratio warm-up for future experiments. Testing local gradient incorporation techniques in a multi-node setting with gradient dropping at 99% drop ratio and warm-up for the first three epochs on the Ro-En dataset. The PARTIAL or SUM update techniques improve early convergence but become unstable after several epochs. After testing local gradient incorporation techniques in a multi-node setting with gradient dropping, it was found that periodic model synchronization through model averaging every 500 steps improved convergence. Without synchronization, the model suffered reduced quality in development and test BLEU scores. Using PARTIAL or SUM with periodic synchronization significantly improved convergence compared to other techniques. Using PARTIAL in gradient dropping technique provides a more stable result and helps the model reach convergence faster, reducing training time. Experimental results show that compressing the gradient improves speed by 3x over a single-node setup. Local gradient update slightly decreases average speed but significantly improves convergence speed compared to vanilla gradient dropping. It reduces training time and quality damage caused by gradient dropping, showing greater improvement in words/second than training time in En-De experiment. The model spends additional time on data and I/O operations, significantly reducing convergence damage. Our approach reduces convergence damage in data-parallelism training by using locally-computed gradients to predict and reconstruct dense gradients. Experimental results show up to 45% faster training compared to non-compressed multi-node systems and 3x faster than single-node systems. Local gradient updates also mitigate quality loss from gradient dropping."
}