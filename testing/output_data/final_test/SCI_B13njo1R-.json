{
    "title": "B13njo1R-",
    "content": "Deep reinforcement learning has shown progress in continuous control problems, with agents displaying skill and agility. An ongoing challenge is integrating policies for multiple skills, each specializing in a specific skill and state distribution. Policy distillation methods are extended to combine expert policies for simulated bipedal locomotion on various terrains. An input injection method enhances existing policy networks with new features, and transfer learning aids in acquiring new skills efficiently, allowing policies to be incrementally augmented. The PLAID method combines progressive learning and integration via distillation to incrementally augment policies with new skills. The PLAID method, Progressive Learning and Integration via Distillation, aims to learn and integrate new skills for complex control tasks through sequential acquisition and integration. It builds on humans' rich motion repertoires and leverages reinforcement learning to enhance movement patterns for different contexts. Progressive Learning and Integration via Distillation (PLAID) aims to efficiently learn new skills informed by existing control policies and integrate them into a single controller for a full motion repertoire. It is a continual learning method effective for multitask solutions and allows for input injection without performance impact. This process aligns with the time course of sensorimotor learning in human motor control. Progressive Learning and Integration via Distillation (PLAID) investigates a progressive approach to integrating new skills into a single controller using deep reinforcement learning. Distillation combines expert policies into one controller, avoiding hierarchical models for scalability and addressing issues like forgetting and generalization. Progressive Learning and Integration via Distillation (PLAID) explores integrating new skills into a controller using deep reinforcement learning. Transfer Learning (TL) is used to improve sample efficiency across tasks, but transferring knowledge may not always be straightforward. The core contribution is a method for efficiently learning new tasks without forgetting old skills. The core contribution of this paper is the method Progressive Learning and Integration via Distillation (PLAiD) for expanding and integrating a motion control repertoire. It involves policy transfer, multi-task policy distillation, and input injection to support new skills while preserving existing capabilities. The method is evaluated in the context of robust locomotion over different terrains and compared against three alternative baselines. Transfer learning and distillation are important in machine learning and reinforcement learning for continuous control environments. Recent works have explored combining multiple expert policies in reinforcement learning using supervised learning for model compression and multi-task policy transfer on discrete action domains. This approach is extended for complex continuous action space tasks and serves as a building block. Transfer learning in continuous control tasks involves appending additional network structure to reduce catastrophic forgetting, as seen in Atari games. Recent works in reinforcement learning have focused on combining expert policies and using selective learning rates for specific network parameters to mitigate forgetting. A hierarchical structure approach allows previously-learned policies to be options for new tasks, while meta-learning enables control policies to adapt quickly to current rewards. The Powerplay method offers a framework for training a general problem solver. The Powerplay method provides a framework for training a general problem solver by inventing new tasks, solving them, and demonstrating the ability to solve previous tasks. The methodology focuses on progressive learning-and-distillation for motor skills, with an evaluation against three baselines. Hierarchical RL uses modularity for transfer learning in robotic tasks. The curr_chunk discusses the use of Hierarchical Reinforcement Learning (HRL) for simplifying motor control problems and examining knowledge transfer. It also explores learned motions shaped by prior mocap clips and integrated into a hierarchical controller within a Reinforcement Learning (RL) framework. The concepts of Transfer Learning (TL) and distillation are introduced within the context of Markov Decision Processes (MDP). The curr_chunk discusses the agent's state, actions, policy, transition probabilities, rewards, and the goal of learning an optimal policy. It mentions the use of a Gaussian distribution for the policy model and optimizing parameters to maximize the expected cumulative reward. The curr_chunk explains the optimization of policy using stochastic policy gradient methods and the advantage function in reinforcement learning. It introduces the Positive Temporal Difference (PTD) update for the advantage function, which is insensitive to scale. Policy Distillation (PD) involves combining skills from expert agents to create a multi-skilled agent. It aims to match action distributions of experts, independent of reward functions used. PD scales well and does not necessarily produce an optimal mix of experts. Transfer Learning BID12 involves using an expert to assist a student in learning a new task efficiently. The success of this method depends on the overlap between the state distributions of the expert's task and the new task. The expert is referred to as the teacher, and the student is the agent learning the new task. This approach is beneficial when there is similarity between the expert's domain and the new task. When learning multiple skills, different integration methods can be used such as learning tasks simultaneously (MultiTasker) or training controllers in parallel and combining policies (Parallel). Learning many skills from scratch can be challenging, with better results achieved for simpler tasks. New tasks learned with the Parallel model may follow a more sequential method. The PLAiD method involves sequential task learning using transfer learning, ending with a distillation step to combine policies for better performance. This approach is effective for both combining learned skills and acquiring new ones. In the acquisition step of the PLAiD method, transfer learning is used to fine-tune an existing policy network for a new task. The focus is on developing new skills without worrying about forgetting previous ones. In the integration step, past skills are combined with the newly acquired skill to improve performance. Traditional approaches involve policy regression using expert policy trajectories for training the student. In the integration step of the PLAiD method, the student's poor behavior is caused by experiencing a different distribution of trajectories than the expert during evaluation. To address this, portions of the trajectories should be generated by the student to align its state distribution with the expert's. This problem is common in learning to reproduce a given distribution of trajectories. The DAGGER algorithm is used for distilling policies, and regression is performed on the critic in the actor-critic RL algorithm. The results cover tasks with similar action and state spaces. The work focuses on demonstrating continual learning between related tasks involving a 2D humanoid walker navigating various terrains. The goal is to maintain consistent forward velocity while matching a natural human walking gait. The tasks include flat ground, inclines, steps, slopes, gaps, and a combination of terrains. The walker receives character and terrain state representations as input. The experimental setup involves a 2D humanoid walker navigating various terrains with character and terrain state representations. The action space is 11-dimensional with torque limits for natural motions. The goal is to learn to traverse all terrain types sequentially and compare against three baselines for effectiveness. The MultiTasker is used for skill retention and as a baseline for learning speed comparison. Transfer Learning (TL) between tasks with a distillation step is evaluated. Results of the PLAiD controller are shown in Video 1 5.1. The pd-biped is trained for walking on flat ground first, then compared to baselines for incline training. TL-Only method learns fast with prior information, while the Parallel method lacks prior knowledge. MultiTasker for incline task is initialized from a terrain injected controller trained for flat ground, subsequent MultiTaskers are initialized from the previous task's model. The MultiTasker controller learns multiple tasks together, struggling as tasks increase. In contrast, PLAiD integrates skills efficiently after each new task. PLAiD learns new tasks faster and integrates skills robustly, while TL-Only also learns efficiently. Using distillation in PLAiD allows for efficient learning without resembling individual policies. The PLAiD model efficiently integrates skills by combining expert controllers without resembling individual policies. Input injection is used to add new terrain features to assist the agent in its task domain. This is achieved by augmenting the policy with additional input features while retaining its original functional behavior. Training the MultiTasker over multiple tasks simultaneously may help the agent learn skills quicker, but scalability becomes an issue with more tasks. Results are good with two or three tasks, but the method struggles with four or more tasks. The struggle is due to potential forgetting of how to perform tasks, as shown in Table 1. The MultiTasker forgets less than PLAiD but has lower average rewards over tasks. As more tasks are added, trade-offs become complex, with the MultiTasker favoring easier tasks. PLAiD combines skills better with more tasks, showing faster skill integration and higher value policies. Additionally, PLAiD offers zero-shot training on new tasks. The final distillation step in training the agent helps mitigate performance loss when learning new tasks, but it is not as effective as PLAiD. There are indications that distillation may hinder training initially by causing disruptions in learned structures. The large change in initial state distribution during transfer learning could be a factor, leading to larger gradients and potential overfitting. Smooth transitions in policy space may not exist. The MultiTasker may struggle with combining skills gracefully due to constraints, favoring tasks with higher rewards and easier tasks. PLAiD scales better with the number of tasks and outperforms MultiTasker in our evaluation. In comparison to PLAiD, the MultiTasker may struggle with combining skills gracefully due to constraints, favoring tasks with higher rewards and easier tasks. PLAiD scales better with the number of tasks and outperforms MultiTasker in our evaluation. The MultiTasker gains benefits from training on other tasks together, but falls behind in reducing the number of simulation samples needed to learn new tasks. Distillation is efficient in reducing simulation steps needed, and data can be collected in groups for batch learning. Integration in distillation helps in pulling policies out of local minima in RL. Transfer Learning with actor-critic method did not show empirical evidence of using value functions for TL. Transfer Learning (TL) may not be beneficial when state distributions and reward functions change between tasks. Value functions are easier to learn than policies and their reuse for transfer is less important. TL effectiveness depends on task difficulty and reward function. TL is most beneficial when overlapping state spaces are difficult to reach and offer high rewards. In transfer learning, the difficulty of reaching high-reward areas plays a crucial role in skill integration for locomotion tasks. Different augmentation and distillation strategies may be more suitable for tasks with varying reward functions or expert selection methods. Overfitting to initial experts can hinder learning new tasks, leading to negative transfer. Balancing distillation training among multiple tasks is a challenge in preserving expert skills effectively. The selection of data for training to maintain expert behavior is a common issue in multi-task learning. In multi-task learning, distillation can lead to unequal distribution of tasks, with low-value tasks potentially receiving more focus than desired. Prioritizing tasks during distillation could help with forgetting issues and relearning tasks. While Mean Squared Error is currently used for distillation, better distance metrics may be beneficial. In this work, the focus is on matching distributions from multiple experts in the discrete action space domain. The method proposed involves progressive learning and integration of motion skills through transfer learning, input injection, and continuous-action distillation. This approach aims to balance multiple experts with respect to their reward functions and prevent rapid policy changes during training. The study focuses on using DAGGER-style learning to integrate motion skills efficiently. The network design is enhanced by adding terrain features to create an agent with sight, improving performance. The policy network models a Gaussian distribution for action selection, with a state-independent standard deviation. The approach aims to balance multiple experts in the discrete action space domain and prevent rapid policy changes during training. The study utilizes DAGGER-style learning to enhance motion skills efficiently. The training process involves linearly annealing exploration probability and using Stochastic Gradient Descent with momentum for network training. Distillation step gradually decreases the probability of selecting an expert action. Evaluation is based on the average reward achieved by the agent over simulation runs. The policy is initialized from the most recently trained policy for each distillation step. The policy utilizes DAGGER algorithm for distillation, annealing from expert to student actions. Exploration noise is added to policies during action generation in simulation. Expert policies \u03c0i and \u03c0i+1 are used for tasks. Only 2 policies are kept at a time. To add new input features to the policy network, a new network is constructed with a similar design as the previous one plus additional parameters. The new network is initialized with random parameters and values from the old network are copied over for the matching portion of the design. The weights connecting the old and new portions are set to 0 to preserve the previous distribution. Feature injection is used to differentiate between different states and assist in learning. In order to distinguish between flat and incline tasks using character features, new terrain features were added to enhance task differentiation. A baseline method was evaluated where transfer learning (TL) was applied to multiple tasks followed by distillation to combine learned skills. The TL-Only method showed effective learning of new tasks, with comparison of forgetting between TL-Only and PLAiD methods in terms of relative loss in average reward. The TL-Only method exhibits a significant drop in policy performance compared to PLAiD, especially for complex tasks. The final distillation step for TL-Only seems to hinder policy performance, possibly due to its complexity. Comparison of average rewards for final policies is provided in TAB1. The agent in the simulation models an average adult's dimensions and masses, with a character state consisting of 50 parameters including position and velocity of links. The agent's state includes 50 parameters for position and velocity of links. The action space has 11 parameters for target joint positions. The reward function consists of velocity, pose difference with a kinematic character, and an L2 penalty on torques. The agent's state includes 50 parameters for position and velocity of links, with an action space of 11 parameters for target joint positions. The reward function consists of velocity, pose difference with a kinematic character, and an L2 penalty on torques. Additionally, torque limits are imposed on joints to reduce unrealistic behavior, and various terrain types are randomly generated per episode, including incline, steps, slopes, gaps, and mixed terrains. The mixed terrain in the environment includes various terrains like incline, steps, slopes, gaps, and flat segments of widths sampled from 2.0 m to 2.5 m. The MultiTasker can learn new tasks faster than PLAiD by splitting its training time across multiple tasks. Comparing the two methods, the MultiTasker shows faster learning on steps in Figure 8a. However, the efficiency of a learning method should also consider the number of simulation samples needed, as the MultiTasker needs to train across all tasks to improve a single task without forgetting the old tasks. The MultiTasker learns new tasks faster than PLAiD by splitting training time across multiple tasks. In Figure 8a, it shows faster learning on steps, flat, and incline terrains."
}