{
    "title": "B1xwcyHFDr",
    "content": "The information bottleneck method is an information-theoretic approach to representation learning, training an encoder to retain relevant information for predicting labels while minimizing superfluous information. This work extends the method to the multi-view unsupervised setting, identifying superfluous information by comparing two views of the same entity. The new multi-view model achieves state-of-the-art results on the Sketchy dataset and MIR-Flickr dataset. The theory is also extended to the single-view setting using data augmentation techniques, showing improved performance. Deep representation learning aims to transform raw input into useful information through supervised and unsupervised approaches. Unsupervised representation learning focuses on generating efficient representations for supervised tasks with fewer labels. Our work is based on the information bottleneck principle, enhancing representation learning in the multi-view unsupervised setting. In the unsupervised setting, recent literature has focused on the InfoMax objective, maximizing mutual information between x and z, instead of minimizing it. This paper extends the information bottleneck method to the unsupervised multi-view setting. In the unsupervised multi-view setting, the information bottleneck method is extended by maximizing mutual information between representations of two views while reducing mutual information between each view and its representation. This results in a representation containing only shared information, eliminating independent variations. The contributions include extending the information bottleneck principle to this setting and providing a theoretical analysis of its application. The text discusses a new model for representation learning in a multi-view setting, achieving state-of-the-art results on Sketchy and MIR-Flickr datasets. By utilizing data augmentation techniques, the model produces robust representations in single-view settings. The goal is to find a distribution that maps data observations into a code space, with a focus on discriminative representations for label prediction. The text discusses the sufficiency of representation z for predicting y, emphasizing the importance of maintaining task information during encoding. Sufficient representations lead to better generalization for unlabeled data instances, with irrelevant information in x affecting prediction tasks. The text discusses the importance of maintaining task information in representation z for predicting y. It emphasizes minimizing superfluous information in I(x; z|y) to achieve a sufficient representation. This can only be directly done in supervised settings, requiring additional assumptions on the predictive task. The text describes a strategy to reduce information content in a representation while preserving label information, even without observing the label y. By utilizing redundant information from multiple views of the same object, a representation z can capture only the shared details, reducing sensitivity to view changes. This theory is supported by jointly observing two data-views, v1 and v2. In the multi-view setting, the analysis extends to sufficiency and robustness by ensuring that a representation z captures shared information from views v1 and v2. Redundancy is defined as v1 being irrelevant for predicting y when v2 is known. When views are mutually redundant, a representation z1 can capture the necessary information for predicting y. In the multi-view setting, redundancy is when v1 is unnecessary for predicting y if v2 is known. If z1 is sufficient for v2, it is as predictive for y as observing both views together. By minimizing unique information in z1 from v1 not shared by v2, z1 can capture the necessary information for predicting y. In the multi-view setting, if v1 is redundant when v2 is known, then the unique information in z1 from v1 not shared by v2 can be safely discarded. This results in a more robust representation for predicting y. The method becomes similar to the supervised information bottleneck method when v1 and v2 share only label information. Conversely, if v1 and v2 are identical, the method degenerates to the InfoMax principle. In the multi-view setting, the objective function for representation z1 of v1 aims to discard irrelevant information while preserving label information. By ensuring I(v1; v2|z1) = 0 and decreasing I(z1; v1|v2), the robustness of the representation is increased. A relaxed Lagrangian objective combining these terms is used for optimization, with a Lagrangian multiplier \u03bb1. This approach is visualized in the Multi-View Information Bottleneck model for both multi-view and single-view settings. The MultiView Information Bottleneck (MIB) model aims to optimize parameters for a robust representation z2 of the second view v2. The model combines symmetrized KL divergence terms to balance sufficiency and robustness, with a hyper-parameter \u03b2 controlling the trade-off. The batch-based computation of the loss function is detailed in Algorithm 1. The loss function in Algorithm 1 summarizes the symmetrized KL divergence and mutual information between representations. Estimators like Jensen-Shannon and InfoNCE require an auxiliary model C to be jointly optimized. The methodology builds redundant views from single observations by leveraging task symmetries. By selecting functions that do not impact label information, redundant views can be created for a task. Two random variables from the chosen class of functions must be mutually redundant for the label. The representations must contain the same predictive information, and independent transformations introduce uncorrelated variations. For example, small translations in the views result in a slight shift. Fine-grained details regarding position are discarded in a representation that only contains common information between the views. In single-view datasets, two views v1 and v2 are generated by sampling functions from the same class T with uniform probability. The conditional distributions p\u03b8(z1|v1) and p\u03c8(z2|v2) can share parameters, allowing for parameter sharing. The Information Plane represents possible representations z of x for a predictive task y based on the information regarding the raw observation I(x;z). The Information Plane visualizes representations of raw observations for a predictive task, focusing on maximizing information about the label while minimizing information from the observations. Recent advancements in mutual information estimation have led to the InfoMax principle gaining attention for unsupervised representation learning. The InfoMax objective aims to preserve all information from the raw observations. Variational Autoencoders (VAEs) define a training objective balancing compression and reconstruction error through hyper-parameter \u03b2. As \u03b2 increases, the representation becomes more compressed, leading to increased generalization and disentanglement. As \u03b2 approaches infinity, I(z; x) decreases to zero in VAEs. The transition between low and high \u03b2 may result in loss of label information. Various factors influence this transition, such as encoder, prior, and decoder architectures. Multi-View InfoMax models aim to maximize mutual information between different data views. Target representation should contain predictive information for the second view. The text discusses the optimization of representations for multi-view InfoMax models, focusing on the relationship between information preservation and sufficiency for classification tasks. Various models are compared on the Sketchy dataset, highlighting the effectiveness of the proposed method. Our work is the first to explicitly identify and discard superfluous information from the representation in the unsupervised multi-view setting. Previous studies have discussed the idea of discarding irrelevant information and its impact on generalization capabilities of deep neural networks. Unlike other models, our approach removes superfluous information selectively, rather than indiscriminately like \u03b2-VAE models. The Multi-View InfoMax (MIB) objective aims to create robust representations by minimizing superfluous information. The model's effectiveness is demonstrated against state-of-the-art baselines in both multi-view and single-view settings. Results are compared using the Jensen-Shannon estimator, showing better performance for MIB and other InfoMax-based models. The comparison includes sketch-based image retrieval tasks. The Sketchy dataset contains 12,500 images and 75,471 handdrawn sketches of objects from 125 classes. 60,502 additional images from ImageNet are included, resulting in a total of 73,002 natural object images. 6,250 sketches are randomly selected for testing, leaving 69,221 sketches for training the model. The sketch-based image retrieval task involves ranking 73,002 natural images based on unseen test sketches. The model generates representations for query sketches and natural images using neural networks with hidden layers. The resulting 64-dimensional representation is evaluated on five train/test splits, showing strong performance in mean average precision and precision at 200. The effectiveness of the model in retrieval tasks is attributed to its ability to capture common class information between pictures and sketches. The MIB model, effective for retrieval tasks, uses symmetrized KL divergence to align representations of images and tags in the MIR-Flickr dataset. It consists of 1M images with hand-crafted features and multihot encodings. The dataset is divided into labeled and unlabeled sets, with training samples reduced to 749,647 pairs. The dataset for the MIB model consists of 1M images with hand-crafted features and multihot encodings. Less than two tags are removed, reducing the training samples to 749,647 pairs. The labeled set includes 5 different splits for train, validation, and test sets. The model is trained on unlabeled pairs of images and tags, followed by training a multi-label logistic classifier on 10K labeled train images. The quality of the representation is assessed based on the performance of the logistic classifier on the labeled test set. The MIB model, consisting of multi-layer perceptrons for image and tag representations, is compared with other multi-view learning models. It outperforms Multi-View InfoMax with fewer labels and achieves higher accuracy by choosing a larger \u03b2 value. This effect is likely due to a violation of the mutual redundancy constraint. The MIB model outperforms other multi-view learning models by choosing a larger \u03b2 value to compensate for a violation of the mutual redundancy constraint. It does not require reconstruction for mutual information estimation, unlike Multi-View VAE and Deep Variational CCA. The focus is on comparing unsupervised learning models for data efficiency and representation coordinates on the Information Plane. The dataset is generated from MNIST by creating two views, v1 and v2, through data augmentation. Encoders are trained using the unlabeled multi-view dataset, and a logistic regression model is trained using resulting representations for evaluation. The focus is on comparing unsupervised learning models for data efficiency and representation coordinates on the Information Plane. The study compares unsupervised learning models for data efficiency and representation coordinates on the Information Plane. Models are trained using mutual information estimation networks on final representations, resulting in better classification performance at low-label regimes. The Multi-View Information Bottleneck method produces robust representations for downstream tasks by utilizing multiple data-views. It outperforms other approaches in tasks such as sketch-based image retrieval and unsupervised representation. The model retains label information effectively, even with minimal labels per data point. The Multi-View Information Bottleneck method shows strong performance in various tasks like sketch-based image retrieval and unsupervised representation learning. It can be applied to tasks with paired observations or artificially produced data. Future work includes exploring more than two views and the role of data augmentation in bridging the gap between the Information Bottleneck principle and invariant literature. The curr_chunk discusses the properties of mutual information and presents Theorem B.1, stating that if a representation z of x has higher mutual information with y than x does, then there exists a label y for which z is predictive while x is not. This is proven by factorizing x as a function of two independent variables. The curr_chunk discusses the properties of mutual information and presents Theorem B.1, stating that if a representation z of x has higher mutual information with y than x does, then there exists a label y for which z is predictive while x is not. This is proven by factorizing x as a function of two independent variables. The proof involves setting z = x and showing that there exists y such that I(y; x) > I(y; z) = 0. The curr_chunk discusses the properties of mutual information and presents Theorem B.1, stating that if a representation z of x has higher mutual information with y than x does, then there exists a label y for which z is predictive while x is not. This is proven by factorizing x as a function of two independent variables. The proof involves setting z = x and showing that there exists y such that I(y; x) > I(y; z) = 0. In the following section, the text delves into the redundancy of views v1 and v2 for y, showing that a representation z1 of v1 that is sufficient for v2 is also sufficient for y. Theorems and corollaries are discussed, considering the independence of random variables x and y within a class of functions. The curr_chunk discusses the independence assumption derived from a graphical model G, showing that two views must be mutually redundant for y when certain conditions are met. Theorems and corollaries are presented to support this argument, highlighting the relationships determined by G. The text also explores the conditional independence of variables x and y within a class of functions. The curr_chunk delves into the mathematical proof of independence relations in a graphical model G, showcasing the redundancy between two views when specific conditions are satisfied. Theorems and corollaries are provided to reinforce this concept, emphasizing the conditional independence of variables x and y within a certain function class. The curr_chunk discusses the proof of independence relations in a graphical model G, highlighting the redundancy between two views under specific conditions. Theorems and corollaries emphasize the conditional independence of variables x and y within a certain function class. The representation z of x must satisfy constraints related to label and observational information. The curr_chunk discusses the mutual redundancy condition between views in a graphical model, showing that the relation is not transitive due to higher-order interactions. An example is provided to illustrate this concept. The curr_chunk discusses the higher-order interactions between multiple views in a graphical model, highlighting the non-transitive nature of mutual redundancy. It also explores the extension of the theory to more than two views and the equivalence between supervised Information Bottleneck and Multi-View Information Bottleneck principles. The curr_chunk discusses the equivalence between supervised Information Bottleneck and Multi-View Information Bottleneck principles when redundant views share only label information. It proves that a representation sufficient and minimal for one view is also sufficient and minimal for the label. The curr_chunk discusses the minimal representation for a view and its equivalence to the label. It proves that a representation sufficient for one view is also sufficient for the label. The process involves creating minimal sufficient representations z1 and z2. The curr_chunk discusses the tight bounds for representations z1 and z2 on the same domain Z. It shows that z2 is sufficient for z1, and vice versa. The equation for the loss function is re-parametrized to obtain a final objective. The experimental procedure involves modeling stochastic encoders with Normal distributions parametrized by neural networks. The symmetrized KL-divergence can be directly computed, while mutual information estimator I\u03b8\u03c8(z1;z2) is used. The hyper-parameter \u03b2 is slowly increased during training to prevent encoder collapse into a fixed representation. The experimental procedure involves modeling stochastic encoders with Normal distributions parametrized by neural networks. The update policy for the hyper-parameter during training has not shown strong influence on the representation, as long as the mutual information estimator network has reached full capacity. The experiments used the Adam optimizer with a learning rate of 10. Input for the sketch-based classification task consisted of 4096 dimensional sketch and image features extracted from VGG-16 network models pre-trained on images and sketches from the TU-Berlin dataset. The feature extractors were frozen during training. Each training iteration used batches of size 128. The sketch and image encoders were multi-layer perceptrons with hidden ReLU units and output sizes parametrizing mean and variance for two Gaussian distributions. The critic architecture includes a multi-layer perceptron with 2 hidden ReLU units of size 512. The \u03b2 update policy starts at 10^-4 and exponentially increases to 1.0 over 250,000 training iterations. Evaluation involves comparing 64-dimensional outputs using Euclidean distance and Hamming distance for fair comparison with other methods. The encoders consist of a multi-layer perceptron with 4 hidden ReLU units of size 1,024. Whitening is applied to handcrafted image features with batches of size 128 for each update step. Evaluation is done using mean average precision (mAP@all) and precision at top rank 200 (Prec@200) on both real and binary representations. The mutual redundancy assumption holds only approximately for tags and category labels in the MIR-Flickr dataset. The encoders in Wang et al. (2016) consist of a multi-layer perceptron with 4 hidden ReLU units of size 1,024. The architecture outputs 2x1,024 parameters defining mean and variance of a Gaussian posterior. The critic is a multi-layer perceptron with 2 hidden ReLU units of size 512. The \u03b2 update policy starts at 10^-8 and exponentially increases to 1.0. Models are trained on unlabeled images, and the resulting vectors are used to train a logistic regression classifier on labeled images. In Table 3, the mean average precision value is reported for the model based on optimal parameters chosen from the validation set. Encoders, decoders, and critic architectures consist of neural networks with specific hidden layers and activations. The \u03b2 update policy involves an exponential increase from an initial value of 10^-3. The \u03b2 value is kept constant from the 50,000th to the 1,000,000th iteration. Trained representations are evaluated using a well-known protocol. Mutual information is maximized during training, and estimations are computed using an energy-based bound. The final values for mutual information are computed by averaging estimations on the dataset, with the lowest and highest 5% removed for consistency. Additional quantitative results and visualizations are included in this section. In this section, additional quantitative results and visualizations are presented for the singleview MNIST experiments. The results include a comparison of input information, label information, and accuracy of a linear classifier trained with different amounts of labeled examples. The Jensen-Shannon and InfoNCE estimators are used for the analysis. A linear projection of the embedding obtained using the MIB model shows that the latent space consists of ten clusters corresponding to different digits. This observation aligns with the empirical measurement of input and label information and the performance of the linear classifier in scarce label regimes. The cluster centroids align with digit labels using 10 examples. Ablation studies show different data augmentation ranges affect model performance. MV-InfoMax model does not benefit from increased corruption levels, while MIB model improves by removing irrelevant data. Performance deteriorates when corruption reaches 100%. The mutual redundancy assumption is violated, leading to deteriorating performances of MIB. Label information is dropped in transitions between regimes due to optimization issues with extremely low corruption probabilities. Increasing examples within the same batch could help mitigate bias. The hyper-parameter \u03b2 determines the trade-off between sufficiency and minimality of representation in MIB. The Multi-View Information Bottleneck model uses the hyper-parameter \u03b2 to balance sufficiency and minimality of representation. Different \u03b2 values impact the trade-off between I(x; z) and I(y; z), with MIB showing a better trade-off compared to \u03b2-VAE. The model's effectiveness is supported by predictive accuracy values."
}