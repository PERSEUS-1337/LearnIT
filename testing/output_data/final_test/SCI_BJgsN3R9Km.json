{
    "title": "BJgsN3R9Km",
    "content": "AntMan is a novel approach that combines structured sparsity with low-rank decomposition to reduce computation, size, and execution time of RNNs while maintaining accuracy. It offers up to 100x computation reduction with less than 1pt accuracy drop and produces 5x smaller models compared to the state-of-the-art. AntMan also demonstrates super-linear speed gains on commodity hardware. Model Compression aims to reduce the computational and memory complexity of DL models like RNNs to address performance, cost, and memory limitations, without significantly affecting accuracy. This is crucial for widespread adoption of DL models on devices with limited resources. Compressing RNNs is challenging due to their computation and memory intensity, especially with long sequence lengths. Inducing sparsity, like pruning up to 90% of connections, is a common approach for RNN compression. However, sparse matrices with irregular patterns of non-zero weights can hinder efficiency. Our work explores a new approach to structured sparsity in RNNs by using predefined compact structures instead of pruning or regularization. This method is inspired by compact CNN structures like group convolutions and depth-wise separable convolutions. Innovative approach in RNNs using compact CNN structures like group convolutions and depth-wise separable convolutions. Replacing matrix-vector multiplications with localized group projections (LGP) to reduce computations. Combining LGP with low-rank matrix decomposition for further efficiency. AntMan is a compression approach that combines structured sparsity and low-rank decomposition to reduce computation cost and model size in RNN models. It uses a teacher-student training paradigm and a new technique to improve training efficiency. AntMan has been evaluated on multiple RNN based models for machine reading comprehension and language modeling. We developed efficient implementations of inference kernels on CPUs to serve models compressed by AntMan, achieving significant performance improvements for large RNN models. Our evaluations demonstrate up to a 100x reduction in computational complexity for language models with minimal drops in perplexity. Additionally, we constructed language models with perplexities ranging from 64 to 70, using 3x to 5x fewer overall model weights compared to the state-of-the-art. AntMan achieves high compression rates for RNNs, outperforming SVD-based methods. It is designed for general RNNs without specific input redundancies like in image vectors. Knowledge Distillation is used in the teacher-student training paradigm. AntMan optimally combines three objective functions, MSE loss, KL divergence loss, and cross entropy of true labels to compress RNN computation efficiently. It outperforms SVD-based methods and is designed for general RNNs without specific input redundancies. Knowledge Distillation, a popular approach for compressing deep networks, is used in the teacher-student training paradigm. AntMan compresses RNN computation by combining structured sparsity and low rank decomposition. It includes localized group projections, group mixing, and low rank approximation to reduce computation and size of RNNs. Various AntMan compression modules offer different compression rates and accuracy impacts, with efficient implementation on CPUs using BLAS libraries. AntMan uses localized group projections (LGP) to replace matrix vector products in RNN cells, restricting information flow within local groups across multiple time steps. This can reduce RNN expressibility and accuracy, so AntMan introduces 'group mixing' to facilitate information flow across multiple localized groups during RNN sequence computation. AntMan introduces group mixing to enable information flow across multiple localized groups in RNN sequence computation. This involves using mixing matrices, such as shuffle mix and dense mix, to enhance memory and computational efficiency. Shuffle mix evenly distributes group elements across the output vector, while dense mix utilizes a dense technique for transformation. AntMan introduces group mixing using shuffle mix and dense mix techniques. Dense mix involves using a dense square matrix for group mixing in non-square matrices, providing a weighted linear combination for accuracy at the cost of additional computation. When combined with low-rank decomposition like SVD, dense mix offers high compression while maintaining accuracy. AntMan introduces group mixing techniques like shuffle mix and dense mix. Dense mix uses a dense square matrix for group mixing in non-square matrices, offering high compression with maintained accuracy when combined with low-rank decomposition such as SVD. The combination of low-rank decomposition and LGP-based sparsity in AntMan compression modules results in efficient and accurate performance across deep learning models. AntMan introduces compression modules like LGP-shuffle, LGP-dense, and LowRank-LGP, which combine structured sparsity with low-rank decomposition. LowRank-LGP decomposes a matrix into an SVD-like form and replaces components with LGP-Dense. These modules aim to achieve higher cost reduction than using SVD alone. LGP-Shuffle discusses the reduction in computation and model size, showcasing the potential for significant cost reduction. The total number of multiply-add operations is reported, representing the size of the weight matrix in the case of MV. LGP-Dense reduces the total cost by \u2248 max(m,n) min(m,n) when g max(m,n) min(m,n). LowRank-LGP can enable higher cost reduction over SVD with large enough g out and g in, while maintaining the same reduced rank. The reduction in computational cost scales as O(r) for SVD and O(r^2) for LowRank-LGP assuming g \u2265 r. The implementation of AntMan modules (LGP-Shuffle, LGP-Dense, LowRank-LGP) on CPUs efficiently reduces the number of parameters and multiply-add operations in matrix-vector operations. For example, with LGP-Shuffle and g = 10, parameters and MADD operations can be reduced to 40K from 400K in a 1000x400 matrix. LGP-Dense with g = 10 reduces them to 200K, and LowRank-LGP with g = 10 and r = 4 reduces them to 24K. AntMan modules can be efficiently implemented on commodity hardware like CPUs, using Intel MKL for matrix-vector multiplication and OpenMP for parallelization. Knowledge distillation improves model accuracy by training a compressed model to imitate the output distribution of the original uncompressed model. The loss function of the compressed model is a combination of three losses - raw loss from target labels, MSE, and KL divergence losses. The coefficients for these losses significantly impact the model's performance, and finding appropriate values can be done through grid or random search. The coefficients for the three losses in the compressed model significantly impact performance. An efficient method to determine these coefficients involves scaling each loss to have a similar magnitude. This is achieved by training the model separately with each loss and adjusting the coefficients accordingly. This approach optimizes accuracy in the compressed model training process. The coefficients for the three losses in the compressed model significantly impact performance. Training the model separately with each loss and adjusting the coefficients accordingly optimizes accuracy. The compressed model achieved the lowest perplexity by setting coefficient values of C target = 1, C mse = 30, and C KL = 1000, with each term in Eqn. 1 roughly equal to 4. Combining all three losses gives the best achievable perplexity. The AntMan model achieves significant computation reduction for language modeling and machine reading tasks while maintaining accuracy. It also constructs models with fewer parameters than state-of-the-art models. AntMan demonstrates real speedup on CPUs and super-linear computational efficiency on large RNNs, showing practical value on commodity hardware. AntMan model achieves up to 50x reduction in word level completion task without accuracy loss. For machine reading compression task, it achieves up to 25x reduction with minimal F1 score drop. Using Penn Tree Bank dataset, the teacher model has 2 layered LSTMs with hidden dimension of 1500. The student model replaces MVs with LGP-Shuffle and achieves 10x reduction with 3pt better perplexity. TAB2 displays perplexity values for different computation reductions. AntMan achieves 50x reduction with g = 50. AntMan achieves significant computation reduction with g values of 10 and 100, with improvements in perplexity. It outperforms state-of-the-art compressed models in BID26, reducing computations by 5-10x. MRC tasks have gained popularity in NLP and computer vision communities. The dataset used is Stanford Question Answering Dataset (SQuAD) BID22. The teacher model is BiDirectional Attention Flow Model (BiDAF) BID24 with 6 layers. AntMan achieves significant computation reduction by compressing the layers with RNNs in the BiDAF model. Three compressed models were created with different levels of compression using AntMan. The models include LGP-Shuffle, LowRank-LGP 1, and LowRank-LGP 2, each with specific parameters for compression. The results of the computation reduction for each LSTM are shown in Table 4. Results from Table 4 demonstrate that LGP-Shuffle and LowRank-LGP achieve significant computation reduction compared to the original model. The reduction surpasses existing work ISS BID26, with improved EM and F1 scores. ISS compresses RNNs by reducing the hidden dimension, leading to a proportional reduction in computation per LSTM step. LGP-Shuffle utilizes structured sparsity to compress both input and hidden layers efficiently. AntMan improves the Pareto curve of model size against accuracy targets, achieving higher reduction in computation across all layers compared to ISS and LGP-Shuffle. LowRank-LGP leverages dense mix to enrich connections among localized groups, further reducing computation through low-rank decomposition. Smaller models are desired for given accuracy targets, while higher accuracy is desired for given model sizes. AntMan improves model size accuracy, compressing models with same accuracy targets as recent models. Teacher Model: AWD-LSTM BID16 with 3-layer LSTMs. Compressed Models: AntMan replaces MVs in AWD-LSTM, achieving lower perplexity with fewer parameters. LGP-Shuffle (g = 5) achieves perplexity of 63 with fewer parameters than NAS-Cell BID33. LGP-Shuffle (g = 10) and (g = 50) also achieve lower perplexity with significantly fewer parameters. LGP-Shuffle (g = 50) achieves a perplexity of 74 with significantly fewer LSTM parameters and total parameters compared to Var-LSTM-avg1 BID10, improving the Pareto curve for model sizes against different accuracy targets. The AntMan results achieved without regularization, and attempting to match AWD-LSTM perplexity with AntMan would require extensive hyper-parameter tuning. Table 5 shows the measured speedup on CPU using LGP-Shuffle and LowRank-LGP compared to theoretical speedup for various input and hidden dimensions. In practice, theoretical speedup from LGP-Shuffle and LowRank-LGP can be turned into actual speedup using efficient implementation of AntMan. Actual speedup can exceed theoretical speedup for large problem sizes. Evaluation results are presented in Table 5, measuring execution time of LSTMs with and without AntMan across varying input and hidden dimensions. AntMan combines time steps into a single matrix multiplication, using 4 hidden MVs per step. Experiments run on a single core of Intel CPU E5-2650 v4 @ 2.20GHz. Intel MKL library used for GEMM implementation. For small problem sizes, AntMan offers no speedup due to GEMM performance degradation. However, for medium-sized problems, AntMan provides good actual speedup compared to theoretical speedup. AntMan combines time steps into a single matrix multiplication, offering good actual speedup compared to theoretical speedup. Even with modest sparsity or computation reduction, significant performance gains are seen at problem sizes 400 and 800. For larger sizes, the actual speedup surpasses the theoretical speedup, as reducing memory footprint allows for better efficiency in L3 cache. This practical value of AntMan on commodity hardware is demonstrated through these results. AntMan combines structured sparsity and low-rank decomposition to reduce computation, size, and execution time of RNN models significantly. It achieves up to 100x reduction in computation without sacrificing accuracy, surpassing the benefits of quantization techniques. This compression efficiency enables deployment of RNN-based models in practice. Pruning techniques can generate significant computation reduction by creating sparsity in RNN models. While unstructured sparsity is not efficient, structured sparsity can achieve over 10x reduction in computation. However, converting theoretical computational reductions into practical performance gains remains a challenge. AntMan achieves 30x performance gain with 10x reduction for PTB like models, while structured sparsity requires specialized kernels for computation reduction. ISS and AntMan show good computation reduction with super linear speedups using BLAS libraries like Intel MKL. Comparing AntMan with smaller RNN models trained using larger teacher models demonstrates efficiency. AntMan achieves higher accuracy than SVD and BTD for compressed RNN models. BTD is limited to compressing input vectors only, hindering performance on RNNs with large hidden vectors. AntMan's generic approach outperforms ISS without knowledge distillation. When trained without knowledge distillation, AntMan and ISS show complementary strengths. AntMan struggles to generalize without a teacher on the PTB dataset, while ISS incurs minimal loss in perplexity. AntMan successfully compresses BiDAF with minimal reduction in F1 score, while ISS struggles due to its compression method. The ISS method compresses LSTMs by reducing hidden dimensions, affecting accuracy in models with small hidden dimensions like BiDAF. However, it works effectively with models like PTB that have large input and hidden dimensions. All models in the evaluation section were trained using the ADAM optimizer with default hyperparameters from standard implementations on github."
}