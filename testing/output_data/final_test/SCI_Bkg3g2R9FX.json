{
    "title": "Bkg3g2R9FX",
    "content": "Adaptive optimization methods like AdaGrad, RMSprop, and Adam aim to speed up training by adjusting learning rates. However, they often struggle to generalize well or converge due to unstable rates. New algorithms like AMSGrad have tried to address this issue but with limited success. In our study, we show that extreme learning rates can lead to poor performance. We introduce AdaBound and AMSBound, variations of Adam and AMSGrad, which use dynamic rate bounds for a smoother transition to SGD. Experimental results demonstrate the effectiveness of these new methods. Recent work has introduced new variants of adaptive optimization methods like AdaBound and AMSBound, which aim to eliminate the generalization gap between adaptive methods and SGD. These variants maintain higher learning speed early in training and show significant improvement, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/AdaBound. Recent work has proposed adaptive methods like ADAM, ADAGRAD, and RMSPROP that scale the gradient by square roots of past gradients. These methods show faster progress in training but may plateau on unseen data. Recent state-of-the-art works in natural language processing and computer vision have shown that SGD performs better than adaptive methods. AMSGrad, a variant of ADAM, aims to address the performance gap between adaptive methods and SGD. Empirical studies show that both extremely large and small learning rates exist in ADAM, leading to poor generalization. Introducing non-increasing learning rates, a key feature of AMSGrad, helps mitigate the impact of large rates but may neglect the effects of small ones. The text discusses the limitations of extreme learning rates in adaptive methods like RMSPROP and ADAM, which can prevent convergence to optimal solutions. New variants, AD-ABOUND and AMSBOUND, are proposed to address this issue by employing dynamic bounds on learning rates that smoothly converge to a constant final step size. These variants transition from adaptive methods to SGD (or with momentum) during training. The text introduces new variants, AD-ABOUND and AMSBOUND, which address the limitations of extreme learning rates in adaptive methods by smoothly transitioning to SGD (or with momentum) during training. These variants offer rapid initial training and strong generalization performance, especially on complex deep networks. Experimental results show higher learning speed early in training and improved generalization compared to other methods. Online convex programming is a framework for iterative optimization methods where a player (algorithm) interacts with an adversary by choosing decisions from a convex feasible set and incurring losses based on the adversary's chosen convex loss functions. The regret in online convex programming is the difference between total loss and its minimum value for a fixed decision. The goal is to devise an algorithm with little regret, ensuring performance converges to the optimal one. Online gradient descent and stochastic gradient descent are used interchangeably. A generic framework of optimization methods is provided in Algorithm 1. The provided text discusses a generic framework of optimization methods in Algorithm 1, which encapsulates various adaptive and non-adaptive methods. The algorithm outlines the input, initial step size, and sequence of functions. It mentions the use of a decreasing step size for theoretical proof of convergence, but notes that a constant step size usually works better in practice. The text discusses optimization methods, highlighting popular methods like ADAM, RMSPROP, and ADAGRAD. It compares their use of scaling terms and averaging of past gradients. AMSGRAD is mentioned as well, with a unique definition for \u03c8 t. The focus is mainly on ADAM due to its generality. In this section, the primary defect in current adaptive methods like ADAM is discussed, with a focus on the non-convergence caused by extreme learning rates. While AMSGRAD was proposed to address this issue, recent work has shown it does not significantly outperform ADAM. The authors suggest that both extremely large and small learning rates of ADAM may contribute to its subpar performance. The study examines the impact of extreme learning rates in ADAM on generalization ability. Learning rates of weights and biases in ResNet-34 on CIFAR-10 are sampled, showing a mix of tiny rates less than 0.01 and huge rates exceeding 1000 near convergence. The study analyzes the impact of extreme learning rates in ADAM on generalization ability. It shows that both tiny and huge learning rates can affect the final stage of the training process. AMSGRAD may help with huge learning rates but neglects the impact of tiny rates. Questions arise about whether tiny learning rates harm ADAM's convergence and if a larger initial step size can mitigate small rates. Undesirable convergence behavior in ADAM and RM-SPROP can be caused by extremely small learning rates, even with a large initial step size. The study examines the impact of extreme learning rates in ADAM on generalization ability. It shows that both tiny and huge learning rates can affect the final training stage. Undesirable convergence behavior in ADAM can be caused by extremely small learning rates, even with a large initial step size. Theorem 1 states that for an online convex optimization problem, ADAM will have non-zero average regret regardless of the initial step size \u03b1. This issue does not occur with vanilla SGD, where the average regret asymptotically approaches 0 with various initial step sizes. The problem becomes more apparent in later training stages when the algorithm may get stuck in suboptimal points due to gradients approaching 0 and varying second-order momentum. This can lead to infrequent \"correct\" signals being unable to guide the optimization process effectively. Theorem 2 and Theorem 3 show that for any constant \u03b21 and \u03b22 with \u03b21 < \u221a \u03b22, ADAM has non-zero average regret in online convex optimization and stochastic convex optimization problems, respectively. This occurs regardless of the initial step size \u03b1. The analysis shows that for any constant \u03b21 and \u03b22 with \u03b21 < \u221a \u03b22, ADAM fails to converge to the optimal solution in a stochastic convex optimization problem. The condition \u03b21 < \u221a \u03b22 is crucial, and extreme learning rates can hinder generalization ability. New optimization methods are developed to address this issue. Our aim is to devise a strategy that combines adaptive methods' fast initial progress with the good final generalization properties of SGD. Inspired by gradient clipping, we propose ADABOUND in Algorithm 2, which clips learning rates in ADAM. This new strategy involves using dynamic lower and upper bounds for the learning rate, \u03b7 l (t) and \u03b7 u (t), instead of constant values. ADABOUND is a strategy that combines adaptive methods' fast initial progress with the good final generalization properties of SGD. It uses dynamic lower and upper bounds for the learning rate, \u03b7 l (t) and \u03b7 u (t), instead of constant values. ADABOUND behaves like ADAM initially and gradually transforms into SGD(M) as the bounds become more restricted. The regret of ADABOUND is upper bounded by O( et al. (2018). ADABOUND combines adaptive methods' fast initial progress with the good final generalization properties of SGD. The regret of ADABOUND is upper bounded by O( et al. (2018). AMSBOUND, a variant of ADABOUND, also holds a regret of O( \u221a T ) and has similar performance in various tasks. Compared to a similar approach by BID6, AMSBOUND addresses the uncertainty of a fixed turning point between ADAM and SGD with a continuous method. ADABOUND combines adaptive methods' fast initial progress with the good final generalization properties of SGD. AMSBOUND, a variant of ADABOUND, also holds a regret of O( \u221a T ) and has similar performance in various tasks. To address the uncertainty of a fixed turning point between ADAM and SGD, a continuous transforming procedure is used in our method. An extra hyperparameter is introduced to decide the switching time in ADAM and SGD, which can be challenging to fine-tune. Our approach introduces two bound functions to provide flexibility. An empirical study compares new variants with popular optimization methods like SGD(M), ADAGRAD, ADAM, and AMSGRAD on tasks like MNIST and CIFAR-10 image classification. The study focuses on tasks like MNIST and CIFAR-10 image classification, as well as language modeling on Penn Treebank. Experiments are run three times with specified initialization methods. Optimization hyperparameters are tuned, including step size using a logarithmically-spaced grid. The goal is to achieve the lowest training loss by the end of the experiments. The study focuses on tasks like MNIST and CIFAR-10 image classification, as well as language modeling on Penn Treebank. Experiments are run three times with specified initialization methods. Optimization hyperparameters are tuned, including step size using a logarithmically-spaced grid. The goal is to achieve the lowest training loss by the end of the experiments. Specifically, hyperparameters are tuned for SGD(M), ADAGRAD, ADAM, and AMSGRAD using different initial step sizes and accumulator values. For the multiclass classification problem on MNIST dataset, various optimization methods like ADAM, AMSGRAD, ADABOUND, and AMSBOUND are compared. Hyperparameters are tuned accordingly, and a fully connected neural network with one hidden layer is trained for 100 epochs. The learning curve for each method on training and test sets is shown in FIG0. In image classification on CIFAR-10 dataset, various optimization methods are compared. SGD performs slightly better than ADAM and AMSGRAD on the test set. ADABOUND and AMSBOUND show slight improvement but still lag behind their prototypes. DenseNet-121 and ResNet-34 models are used with a fixed budget of 200 epochs. Adaptive methods initially outperform non-adaptive ones, but SGDM performs better after learning rate decay at epoch 150. Our proposed methods, ADABOUND and AMSBOUND, show faster convergence and slightly higher accuracy than SGDM on the test set. They also exhibit a significant improvement of approximately 2% in test accuracy compared to their prototypes. The performance on ResNet-34 and DenseNet-121 models is similar, with ADABOUND and AMSBOUND even surpassing SGDM by 1%. Despite the generalization issues of adaptive methods, our methods overcome this by setting bounds for learning rates, achieving top accuracy on the test set for both models on CIFAR-10. Additionally, experiments on language modeling with LSTM networks show promising results for our methods. In experiments with LSTM networks, different models with varying layers were trained on Penn Treebank for 200 epochs. ADAM showed fast initial progress but performed worse than SGD and the proposed methods. ADABOUND and AMSBOUND had smoother curves than SGD. Comparing models with 1, 2, and 3 layers, there was a noticeable difference in improvement, with the simplest model showing a slight 1.1% enhancement. In experiments with LSTM networks, different models with varying layers were trained on Penn Treebank for 200 epochs. The simplest model showed a slight 1.1% enhancement over ADAM, while the most complex model exhibited over 2.8% improvement in perplexity. ADABOUND and AMSBOUND demonstrated faster convergence and higher test accuracy compared to ADAM and AMSGRAD, confirming the impact of learning rates on convergence. In experiments with different models of varying complexities, including a perceptron, two deep CNNs, and an RNN, it was observed that both large and small learning rates can affect convergence. The perceptron on MNIST showed slight improvement with the proposed methods, while DenseNet and ResNet models exhibited significant increases in test accuracy. The complexity of the model, with different convolutional layers playing distinct roles, may lead to variations in parameter gradients. Extreme learning rates are more common in complex models like ResNet, but the proposed algorithms aim to avoid them, resulting in greater performance enhancements in complex architectures. The text discusses the performance enhancement in complex architectures by avoiding extreme learning rates. It also mentions the need to explore ways to improve simple models and the uncertainty surrounding the success of SGD in various machine learning applications. Additionally, it suggests that dynamic bounds on learning rates are just one approach, and other methods like well-designed decay should be explored further. The text discusses the need to avoid extreme learning rates for better convergence behavior in adaptive algorithms like ADAM and AMSGRAD. It introduces new algorithms, AD-ABOUND and AMSBOUND, which use dynamic bounds on learning rates to transition smoothly to SGD while maintaining the advantages of adaptive methods. The authors thank reviewers for feedback and acknowledge Xu Sun as the corresponding author. Lemma 1 by Xu Sun provides a proof for a mathematical inequality involving projection operators and convex feasible sets. Lemma 2 discusses the convergence behavior of a specific algorithm with dynamic bounds on learning rates. The text discusses mathematical inequalities derived from Cauchy-Schwarz and Young's inequality, with a focus on the update of ADAM in Algorithm 1. It also introduces a function sequence for linear functions and a condition for convergence. The text discusses the execution of the ADAM algorithm for a function sequence with bounded gradients and conditions satisfied for ADAM. Lemma 3 states that there exists a time step where a certain parameter is non-negative. Lemma 3 states that there exists a time step where a certain parameter is non-negative. For t \u2265 \u03c4, inequalities are observed leading to specific updates in the ADAM algorithm. The constants and bounds are derived based on the conditions satisfied for ADAM. Lemma 3 states that for t \u2265 \u03c4, there is a time step where a parameter is non-negative. The proof involves using mathematical induction to show that x k \u2265 0 for all k \u2265 Ct + 1. The ADAM algorithm updates are considered, leading to specific inequalities and bounds. The ADAM algorithm updates involve specific inequalities and bounds, leading to a regret of at least 1 for every C steps. Theorem 2 generalizes the optimization setting used in Theorem 1. The example provided in Appendix B satisfies the constraints in Theorem 2, where linear functions are considered with specific conditions. The proof shows that as T approaches infinity, the ratio R T /T converges to 0. The example in Appendix C also meets the conditions in Theorem 3, with additional details provided for completeness. The proof considers stochastic optimization over the domain [-1, 1] with function ft(x) = \u03b4x. The optimal point is x* = -1, but ADAM's step drifts away from it. The proof concludes by showing that the optimal solution exists and provides a rearranged inequality. The proof involves stochastic optimization over the domain [-1, 1] with function ft(x) = \u03b4x. It concludes by rearranging an inequality and showing the existence of an optimal solution. The regret of ADABOUND is upper bounded by O( \u221a T ). Theorem 5 provides conditions for sequences {xt} and {vt} from Algorithm 3, with specific constraints on \u03b21 and \u03b7 values. ADABOUND algorithm outperforms SGDM in terms of regret for all step sizes, even without careful tuning of hyperparameters. The form of bound functions has minimal impact on ADABOUND's performance, making it less sensitive to hyperparameters compared to SGDM. Empirical study shows ADABOUND achieves higher or similar performance to SGDM without fine-tuning, indicating better performance regardless of bound function choice. In an empirical study on ADABOUND's learning rates evolution over time, a ResNet-34 model was used on CIFAR-10 dataset. Learning rates increase rapidly at the start, then gradually decrease and converge to a final step size, transitioning from ADAM to SGD."
}