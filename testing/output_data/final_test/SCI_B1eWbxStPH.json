{
    "title": "B1eWbxStPH",
    "content": "Graph neural networks have been successful in predicting quantum properties of molecules by representing them as graphs. However, these models lack directional information crucial for empirical potentials. To address this, directional message passing is proposed, embedding messages with associated directions in coordinate space. These embeddings are rotationally equivariant and utilize directional information through message transformations based on angles. Spherical Bessel functions are used for constructing orthogonal message passing schemes akin to belief propagation. DimeNet, a directional message passing neural network, utilizes spherical Bessel functions to create an orthogonal radial basis with superior performance compared to Gaussian radial basis functions. It outperforms previous GNNs by 77% on MD17 and 41% on QM9, revolutionizing the use of machine learning in predicting molecular properties with reduced computation time. GNNs update atom embeddings in high-dimensional space by passing messages between atoms to predict potential energy. They lack the ability to model bond angles and rotations directly, relying on complex interactions instead. Extending GNNs to include these features is challenging. In this paper, the authors propose a method to extend Graph Neural Networks (GNNs) by using directional message embeddings to model interactions between atoms. These embeddings preserve relative directional information between neighboring atoms by considering distances and angles, which are invariant to translation, rotation, and inversion of the molecule. This approach allows for more effective representation of bond angles and rotations in GNNs. The authors introduce DimeNet, a directional message passing neural network that utilizes spherical Bessel functions and spherical harmonics to represent distance and angle information effectively. DimeNet outperforms previous GNNs in learning molecular properties and atomic forces, showcasing a 76% improvement on MD17 and a 31% improvement on QM9 datasets. Key contributions include directional message passing and orthogonal basis representations based on spherical functions. The Directional Message Passing Neural Network (DimeNet) utilizes spherical Bessel functions and spherical harmonics for molecular predictions, outperforming previous methods. It sets a new state of the art for molecular properties and dynamics simulations, surpassing traditional machine learning approaches. Graph neural networks (GNNs) were first proposed in the 90s and 00s, inspired by molecular graphs. Recent progress has focused on more powerful GNNs, achieving breakthrough performance in various tasks. Some GNNs use directed edge embeddings, unlike previous models that do not leverage directional information. Recent advancements in graph neural networks (GNNs) have led to models that outperform traditional methods in molecular predictions. Some recent GNNs have incorporated directional information for chain-like molecules, while group equivariance has been generalized to spheres, molecules, volumetric data, and general manifolds. Our proposed solution overcomes limitations of previous models by not introducing computational overhead or constraints on model construction. Machine learning has been used to predict various molecular properties, focusing on scalar regression targets in this work. A molecule is uniquely defined by atomic numbers and positions. In this work, a machine learning model for molecular prediction is defined by atomic numbers and positions, without including auxiliary features. Symmetries and invariances in molecular predictions must adhere to basic laws of physics, with translational and rotational invariance being essential. Incorporating these symmetries explicitly can lead to duplicate weights and increased training complexity. The essential symmetries for molecular predictions include translational and rotational invariance, permutation invariance, and symmetry under parity. To model molecular dynamics, the force field must be a conservative vector field satisfying conservation of energy. Predicting a potential function and obtaining forces via backpropagation can be used for training models for MD simulations. Graph neural networks are used to model molecules as graphs, with atoms as nodes and edges defined by pairwise distances. This approach incorporates forces in the training loss and requires the model to be continuously differentiable for stable simulations. Discontinuous transformations like ReLU non-linearities cannot be used, and pre-computed auxiliary information such as bond types is not applicable. Graph neural networks (GNNs) represent atoms as nodes with pairwise distances, utilizing atom embeddings updated through message passing along molecular edges. Edge embeddings depend on interatomic distances and can incorporate bond information, updated via neural networks. This approach ensures physical invariances in molecular simulations without introducing discontinuities in the energy landscape. Graph neural networks (GNNs) utilize pairwise distances and bond information to update atom embeddings through message passing. However, GNNs use a cutoff distance to avoid passing messages globally between all pairs of atoms, which can lead to overfitting. This can result in the inability to distinguish between certain molecules with similar neighborhoods, which can be addressed by modeling directions to neighboring atoms. To address the limitations of Graph Neural Networks (GNNs) in distinguishing molecules with similar neighborhoods, a solution is proposed by modeling directions to neighboring atoms instead of just their distances. This approach ensures continuous equivariance with respect to rotations, specifically to the SO(3) group, by considering the rotational invariance of individual atoms and the introduction of additional degrees of freedom by neighboring atoms within a cutoff distance. The model introduces directional embeddings for atoms and neighbors, utilizing angle and distance information to create a joint 2D representation using spherical Bessel functions and spherical harmonics. The model utilizes directional embeddings for atoms and neighbors, incorporating spherical Bessel functions and spherical harmonics for a better inductive bias. Message embeddings are used for atom pairs, updating messages based on incoming information and aggregating them using a radial basis function representation of interatomic distance. This aggregation scheme shows similarities to belief propagation and outperforms alternative methods. Our model incorporates directional embeddings for atoms and neighbors, utilizing spherical Bessel functions and spherical harmonics to improve inductive bias. The message embeddings represent atom pairs and enable the model to learn angular potentials directly. By using a joint representation of angles and distances, our model can distinguish molecules that a regular graph neural network cannot. Incorporating directional embeddings for atoms and neighbors using spherical Bessel functions and spherical harmonics to improve inductive bias. Proposing the use of an orthogonal basis for parameter efficiency and deriving a proper basis representation for quantum systems. Constructing a joint 2D basis for interatomic distances and angles to distinguish molecules effectively. The joint 2D basis for interatomic distances and angles is constructed using spherical Fourier-Bessel basis functions, which are orthogonal and real-valued. The basis is designed to bound high-frequency components, ensuring stability in predictions. The number of radial basis functions used is found to be sufficient for the model. The DimeNet model uses a lower number of radial basis functions compared to PhysNet and SchNet. The model fine-tunes Bessel wave numbers for a small boost in prediction accuracy. It integrates directional message passing and spherical Fourier-Bessel representations for generating predictions. DimeNet is a model that uses directional message passing and spherical Fourier-Bessel representations for predicting molecular properties and conducting molecular dynamics simulations. It is invariant to atom permutations and can learn and predict atomic forces via backpropagation. The model ensures energy conservation and stability to small deformations, with an architecture overview shown in Fig. 4. The DimeNet model utilizes directional message passing and spherical Fourier-Bessel representations for predicting molecular properties and conducting molecular dynamics simulations. It is invariant to atom permutations and can learn and predict atomic forces via backpropagation. The model ensures energy conservation and stability to small deformations. The architecture includes an embedding block followed by multiple stacked interaction blocks implementing specific functions. The 2D representations undergo transformations to make them independent of subsequent bilinear layers, with experiments showing better performance using element-wise multiplication for radial basis representation. The DimeNet model utilizes directional message passing and spherical Fourier-Bessel representations for predicting molecular properties and conducting molecular dynamics simulations. It includes an embedding block followed by multiple stacked interaction blocks implementing specific functions. The model ensures energy conservation and stability to small deformations. The interaction block transforms message embeddings using residual blocks inspired by ResNet. The output block uses radial basis functions for continuous differentiability and improved performance. Multiple model choices were made to achieve twice continuous model differentiability. DimeNet achieves twice continuous model differentiability by using a self-gated Swish activation function, multiplying radial basis functions with an envelope function, and relying solely on atom types and positions. Comparison with 6 state-of-the-art models is provided in the study. DimeNet's performance is tested for predicting molecular properties using the QM9 benchmark, consisting of 130,000 molecules with up to 9 heavy atoms. The training set includes 110,000 molecules, validation set has 10,000, and test set has 13,885 molecules. The mean absolute error (MAE) for each target and overall mean standardized MAE are reported. The model's performance is also tested in molecular dynamics simulations. DimeNet's performance in predicting molecular properties using the QM9 benchmark dataset is tested. The model is trained separately for each molecule to provide accurate predictions for energy and atomic forces. Results show that DimeNet can match state-of-the-art performance, but the accuracy is still two orders of magnitude below DFT calculations. DimeNet outperforms SchNet and performs similarly to sGDML when trained on only 1000 samples. Ablation studies show that directional message passing and the Fourier-Bessel basis contribute to DimeNet's improved performance. DimeNet's improved performance is attributed to using 64 Gaussian RBFs and 6 Bessel basis functions, incorporating directional information, and using node embeddings. Ignoring these factors leads to significant increases in error, showing the importance of these contributions. In this work, directional message passing is introduced as a more powerful interaction scheme for molecular predictions. The model learns to leverage directional information in addition to interatomic distances, using spherical Bessel functions and 2D spherical Fourier-Bessel basis functions. This innovation leads to the construction of DimeNet, a graph neural network suitable for molecular analysis. DimeNet is a graph neural network that directly models important degrees of freedom in molecules, showing state-of-the-art performance on QM9 and MD17 datasets. Future work aims to incorporate additional terms for improved predictions on larger molecules. Non-directional GNNs struggle to distinguish between molecules with the same bond lengths but different structures, like Cyclohexane and Cyclopropane. DimeNet is a graph neural network that models molecules effectively, outperforming other models on QM9 and MD17 datasets. It addresses the challenge of distinguishing molecules with similar bond lengths but different structures, like Cyclohexane and Cyclopropane, by considering the direction of each bond in its model architecture and hyperparameters optimization. DimeNet is a graph neural network that effectively models molecules, outperforming other models on QM9 and MD17 datasets by considering the direction of each bond in its architecture and optimizing hyperparameters. The network utilizes filters and weight matrices to represent radial basis and spherical Fourier-Bessel functions, as shown in Fig. 5 for the first 15 elements of the filter."
}