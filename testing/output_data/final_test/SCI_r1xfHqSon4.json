{
    "title": "r1xfHqSon4",
    "content": "Power-efficient CNN-DSA chips are used in mobile devices for computer vision applications. Recent work on text classification and sentiment analysis using two-dimensional CNN models has achieved state-of-the-art results through transfer learning from vision to text. The implementation of these applications on mobile devices using CNN-DSA chips includes compact network representations with low power consumption. The network is further compressed for edge devices by approximating external Fully Connected layers within the chip. The workshop showcased two system demonstrations for NLP tasks using the CNN-DSA chip. The first demo classified English Wikipedia sentences into 14 classes, while the second demo classified Chinese online-shopping reviews as positive or negative. The CNN-DSA accelerator achieved power efficiency with low power consumption and high performance for on-device NLP applications. The Raspberry Pi is connected to a keyboard for text input and a monitor for display. Two demos were showcased, one classifying English Wikipedia sentences and the other classifying Chinese online-shopping reviews. Different network architectures like RNN and LSTM are used for NLP tasks, but the Super Characters method using two-dimensional word embedding showed promising results in text classification and sentiment analysis. The Super Characters method involves drawing characters onto an image for text classification using two-dimensional CNN models. Transfer Learning is used to fine-tune pretrained models on large image datasets. NLP applications on mobile devices are implemented using this method on a CNN-DSA chip. The Super Characters method involves pre-processing text input from a keyboard to create a Super Characters image for classification using a CNN-DSA chip. This method works well for Asian languages with squared characters like Chinese, Japanese, and Korean. To improve performance for English, a Squared English Word (SEW) method is used. The Squared English Word (SEW) method proposes casting English words in a squared shape as a glyph. Each word takes a square space of size lxl, with longer words having smaller space for each alphabet. The CNN-DSA chip receives Super Characters image through USB connection and outputs classification scores for 14 classes in Wikipedia text classification. The CNN-DSA chip processes Super Characters images and provides classification scores for 14 classes in Wikipedia text classification. It completes complex computations with low power consumption, but does not directly support inner-product operations of FC layers. Executing FC layers on a mobile device increases memory, computation, and storage requirements, as well as power consumption. The GnetFC model addresses high power consumption in mobile devices by approximating FC layers with multiple 3x3 convolution layers. The model architecture includes six major layers, with reduced channels in the fifth layer to save memory for the sixth layer. The sub-layers in each major layer have specific details on channels, bits-precision, and padding. The GnetFC model reduces power consumption in mobile devices by approximating FC layers with multiple 3x3 convolution layers. The architecture includes six major layers with specific details on channels, bits-precision, and padding. The final output is an array of 14 scalars, obtained through an argmax operation. The CNN-DSA chip's memory is power-efficient and limited, supporting a maximum of 9MB for coefficients and activation map. The precision varies between 3-bits and 1-bit in different layers. The CNN-DSA chip uses 1-bit precision for four major layers and 5-bit precision for activations to save on-chip data memory. The compression rate allows VGG16 convolutional layers with 58.9MB coefficients to be compressed to about 5.5MB within the chip. This compact representation maintains 71% Top1 accuracy on ImageNet BID0 data. The model coefficients from the third major layers are only using 1-bit precision for efficient on-chip memory usage. The GnetFC model on the CNN-DSA chip achieves an accuracy of 97.4%, slightly lower than the original VGG model's 97.6%. This drop is due to approximation and bit-precision compression, resulting in minimal accuracy loss but significant savings in power consumption and increased inference speed. The CNN-DSA chip achieves significant savings on power consumption and increased inference speed. It processes text classification in 21ms, satisfying real-time requirements for NLP applications. The two-dimensional embedding method converts text into images for efficient processing on the chip, with less than 0.2% accuracy drop from the original VGG model. The demo system shows less than 0.2% accuracy drop from the original VGG model, with potential use cases in intension recognition for smart speakers or Chatbots."
}