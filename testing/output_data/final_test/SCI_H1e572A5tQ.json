{
    "title": "H1e572A5tQ",
    "content": "We propose a communication protocol for deep reinforcement learning agents in a collaborative multi-agent setting. The protocol enables targeted communication and multi-stage coordination before taking actions in various environments. The approach is evaluated on cooperative tasks with different numbers of agents in diverse environments, showing the benefits of targeted and multi-stage communication strategies. Effective communication is crucial for collaborative multi-agent systems, allowing agents to coordinate, strategize, and utilize their combined sensory experiences. This ability has wide-ranging applications for artificial agents in various scenarios, from multiplayer games to networks of self-driving cars and teams of robots on search-and-rescue missions. Targeted communication strategies learned by agents are interpretable and intuitive, offering benefits in real-world scenarios. In collaborative multi-agent systems, targeted communication allows for more flexible collaboration strategies in complex environments. Agents can selectively send messages to specific recipients based on their roles and goals. This approach is supported by a collaborative multi-agent deep reinforcement learning method that enables agents to actively choose who to communicate with. Targeted communication is facilitated by a signature-based soft attention mechanism, where the sender includes a key with the message to indicate the intended recipients. The communication mechanism in collaborative multi-agent systems involves a key that encodes message relevance for receivers. This mechanism allows agents to communicate agent-goal-specific messages, adapt to variable team sizes, and be interpretable through attention probabilities. Multi-agent systems combine game theory, distributed systems, and Artificial Intelligence. Our work in deep multi-agent reinforcement learning is related to prior research in centralized training and decentralized execution, emergent communication protocols, and tasks in novel application domains. Centralized training involves a central controller processing local observations from all agents, while decentralized execution allows for adaptability to variable team sizes and interpretability through attention probabilities. The BID19 architecture proposes a centralized controller invariant to agent permutations, using weight sharing and averaging. In contrast, Hoshen (2017) suggests an attentional mechanism for targeted interactions between agents. While similar, Hoshen's work focuses on fully supervised tasks, while we tackle reinforcement learning with long-term planning. Centralized controllers become impractical in real-world tasks with many agents and high-dimensional spaces, so we adopt centralized learning but decentralized execution. Our work focuses on learning a continuous communication protocol in a decentralized execution setting to solve downstream tasks. This approach allows for global coordination while maintaining tractability, with lower-dimensional communicated messages. In a decentralized execution setting, our work focuses on learning a continuous communication protocol for global coordination in solving tasks. Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) extend partially observable Markov decision processes for cooperative multi-agent scenarios. In a decentralized execution setting, the focus is on learning a continuous communication protocol for global coordination in solving tasks. Agents aim to maximize the total expected return by adjusting policy parameters to maximize the objective. Policy gradient methods are used to take steps in the direction of maximizing the expected return. Actor-Critic algorithms learn the action-value functionQps, aq by temporal-difference learning. Multi-Agent Actor-Critic algorithms are adapted for centralized learning and decentralized execution, with each agent having its own policy and a centralized Critic estimating the joint action-value. Each agent in a decentralized system must select actions and communicate messages to maximize global reward. Policies are implemented as a 1-layer Gated Recurrent Unit, updating the hidden state with observations and messages for better team performance. The agent's policy predicts actions and produces outgoing messages based on internal state representation. Shared parameters among agents speed up learning. A centralized critic guides individual agent policies during training by estimating joint action-value at each timestep. During training, a centralized critic is used to estimate joint action-value, leading to lower variance in policy gradient estimates. At test time, policy execution becomes fully decentralized. Each agent predicts a query vector to compute attention weights for message values, including self-attention to improve performance in coordinated navigation. During training, a centralized critic estimates joint action-value to reduce variance in policy gradient estimates. At test time, policy execution is decentralized. Agents predict query vectors for attention weights in coordinated navigation tasks. Internal states are used to predict message vectors for communication in multi-agent architectures. Models are trained with a batched synchronous ActorCritic using RMSProp with specific parameters. The SHAPES dataset was introduced for visual reasoning tasks, consisting of synthetic images of colored shapes. Each image is converted into an active environment where agents can be spawned, observe a local patch, and communicate using message vectors. The dataset includes 3 shapes, 3 colors, and 2 sizes in total. In the SHAPES dataset, agents navigate to specified goal states in the environment, such as 'red', 'blue square', or 'small green circle'. They communicate and coordinate to find their respective goals, with rewards based on team performance. The dataset includes scenarios with different numbers of agents and goal types, showcasing collaborative problem-solving. Agents in the SHAPES dataset work together to find specific goals like 'red' or 'blue square' in a large environment. They use communication and cooperation to converge on the goals, with rewards based on team success. The communication involves agents predicting signatures and values in messages, with a query vector to focus on incoming information. The setup allows for testing different environment sizes, agent numbers, and goal configurations. The communication in the SHAPES dataset involves agents predicting signatures and values in messages, with attention probabilities determined by sender's signature and receiver's query vectors. This targeted communication ensures that both sender and receiver play a role in deciding how much of each message to attend to. For example, agent 2's signature encoding the color blue leads to high attention from agent 4, whose query also focuses on blue goals. This system allows agents to cooperate and converge on specific goals in the environment. Agent 2's message encodes coordinates for agent 4 to navigate to at time 21. The simulated traffic junction environment in BID19 consists of cars moving along pre-assigned routes. Each car has limited visibility but can communicate with others. The action space for each car is gas and brake, with rewards and penalties based on time and collisions. Comparing our approach with CommNets BID19 on easy and hard tasks shows promising results. Quantitative Results: Comparing our approach with CommNets BID19 on easy and hard tasks in the traffic junction environment. Easy task has one junction of two one-way roads on a 7\u02c67 grid, while hard task has four connected junctions of two-way roads on an 18\u02c618 grid. Model Interpretation: Learned policies show braking probabilities near traffic junctions and high attention probabilities in the 'internal grid'. The study compares the performance of TarMAC with varying message size and rounds of communication in a traffic junction environment. The attention probabilities of cars are highlighted, showing adaptability to variable numbers of agents without handcoding. The Spearman's rank correlation between the total number of cars and cars being attended to is 0.49, indicating dynamic targeting behavior learned from task rewards. The study evaluates TarMAC performance in a traffic junction task with varying message size and rounds of communication. Multiple rounds of communication outperform increasing message size, showcasing the advantage of multistage communication. TarMAC is also tested on a cooperative point-goal navigation task in House3D, demonstrating adaptability in diverse 3D indoor environments. TarMAC is an architecture for multi-agent reinforcement learning that allows agents to communicate and move towards a specified target in the environment. Success rates on a find[fireplace] task in House3D show that TarMAC achieves the best performance compared to other navigation policies. Communication vectors in TarMAC are more compact, making it suitable for scaling to large teams. TarMAC is an architecture for multi-agent reinforcement learning that enables targeted interactions and collaborative reasoning. It shows improved performance in diverse environments with intuitive attention behavior, using team reward as supervision. The model aims to be benchmarked on more challenging 3D navigation tasks to leverage decentralized targeted communication for scaling to a large number of agents. Investigating combinations with other approaches like spatial memory and planning networks within the TarMAC framework is of interest."
}