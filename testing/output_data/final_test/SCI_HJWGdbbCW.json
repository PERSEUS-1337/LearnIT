{
    "title": "HJWGdbbCW",
    "content": "Our approach utilizes demonstration data to enhance deep reinforcement learning for robot manipulation tasks. By training visuomotor policies end-to-end, we enable the agent to learn a direct mapping from RGB camera inputs to joint velocities. Our method combines reinforcement and imitation learning to solve contact-rich tasks that were previously unsolvable. Additionally, our policies demonstrate zero-shot sim2real transfer, showcasing the effectiveness of our approach in handling visual and dynamics variations. Despite advancements in other areas, applying deep reinforcement learning to control and robotic manipulation remains challenging. While there have been successful demonstrations of deep RL for manipulation and applications on real robotic hardware, there are few examples of learned controllers for complex tasks even in simulation. Robotics presents unique challenges such as relying on partial observations from noisy sensors and facing significant generalization hurdles due to task variations. Training on real robotics hardware is difficult due to limited data collection capabilities. Due to constraints on training data collection and safety considerations, deep reinforcement learning for robot arm manipulation tasks faces challenges in exploration and reward function design. A new method presented in this paper aims to address these issues and solve tasks directly from pixels, leveraging reduced exploration difficulty in continuous domains. The key insight of the new method presented in this paper is to simplify exploration in continuous domains by using human demonstrations, stabilize learning of complex manipulation policies from vision, and improve generalization by increasing training condition diversity. The method combines imitation learning with reinforcement learning, requiring only a small number of human demonstrations to simplify the exploration problem dramatically. It demonstrates zero-shot transfer from simulation to real hardware. Generative Adversarial Imitation Learning leverages human demonstrations to train a robot in a simulated environment before transferring the learned policy to a real robot. This approach combines reinforcement and imitation learning, enabling the agent to solve dexterous manipulation tasks that traditional methods struggle with. The method simplifies exploration in continuous domains, stabilizes learning from vision, and improves generalization by increasing training condition diversity. Our method learns an end-to-end visuomotor policy that maps RGB camera observations to joint space control over the full 9-DoF arm. By embracing the sim2real paradigm, we can simulate complex physical interactions in a contact-rich environment using a physics engine and high-throughput RL algorithms. This allows us to train the robot arm without the constraints of real hardware, ensuring safety and system reset concerns are eliminated. Additionally, we can exploit privileged information during training to enhance the learning process. During training, new techniques are used to exploit privileged information about the true system state, including learning policy and value separately, an object-centric GAIL discriminator, and auxiliary tasks for visual modules. These techniques stabilize and accelerate policy learning from pixels. Diversifying training conditions improves generalization and transfer from simulation to reality. The method is demonstrated on six robot arm manipulation tasks, outperforming state-of-the-art techniques and human demonstrations. This approach provides insights into a principled deep visuomotor learning pipeline. The approach discussed in the curr_chunk focuses on deep visuomotor learning using reinforcement learning methods for continuous control problems. It highlights the use of various RL algorithms such as GPS, DPG, NAF, TRPO, and PPO for sim2real policy transfer. These algorithms are known for their robustness and scalability in solving control problems both in simulation and reality. The curr_chunk discusses the use of GPS, NAF, and DPG for training visuomotor controllers on robotics hardware, focusing on sample efficiency and large-scale data collection. The curr_chunk discusses training visuomotor controllers for robots in a self-supervised setting, including strategies like training in simulation and transferring to real hardware, using visual variations for object detectors, and combining randomization with supervised learning. Demonstrations play a crucial role in initializing policies, designing cost functions, guiding exploration, and augmenting training data. Cost functions can be derived from demonstrations via tracking objectives, inverse RL, or adversarial learning. Expert actions or policies can be used for behavioral cloning or DAgger. Expert trajectories can also be used for off-policy algorithms like DPG. Methods for third person imitation have been proposed. Other papers have presented results on manipulation tasks using human demonstrations for exploration and learning tasks. In robot manipulation tasks, BID33 uses demonstrations for learning complex tasks with behavioral cloning and data augmentation. BID30 and BID28 focus on transferring skills from simulation to reality by randomizing visual appearance and robot dynamics. Peng et al. transfer a block-pushing policy to a Fetch robotics arm, while Pinto et al. explore tasks with visual input and end-effector position control. The goal is to develop a deep visuomotor policy using RGB camera observations and proprioceptive features for joint positions and angular velocities. This enables zero-shot policy transfer to the real robot. The model overview includes a deep visuomotor policy that encodes pixel observations and proprioceptive features, utilizing CNN and MLP networks. The features are combined and passed to an LSTM layer to generate joint velocities. The network is trained end-to-end, extending upon GAIL and PPO for visuomotor skills. Inverse Reinforcement Learning (IL) involves learning a behavior policy from demonstrations. Traditional IL methods use behavior cloning, training a policy \u03c0 \u03b8 : S \u2192 A using maximum likelihood. However, GAIL is more efficient, allowing the agent to learn from its own experiences by interacting with the environment. GAIL utilizes two networks, a policy network \u03c0 \u03b8 : S \u2192 A and a discriminator network D \u03c8 : S. GAIL uses a policy network \u03c0 \u03b8 and a discriminator network D \u03c8. The objective is to have the policy close to the expert policy \u03c0 E. Policy \u03c0 \u03b8 is trained with policy gradient methods to maximize the reward function r gail. PPO is a scalable approximation to TRPO that relies on first-order gradients and can be easily implemented with recurrent networks. The key idea of PPO is to use the KL divergence dynamically. In distributed PPO, the coefficient of a regularization term is adjusted using the KL divergence to ensure policy updates adhere to constraints. Sparse piecewise constant rewards are used to train agents in continuous domains, overcoming the limitations of reward shaping. A hybrid reward function combining imitation reward and sparse task rewards is designed for effective training. In BID21, a hybrid reward function is designed to combine imitation reward with sparse task reward. Maximizing this hybrid reward involves reinforcement and imitation learning simultaneously. Experiments show that a balanced mix of these rewards allows agents to solve tasks that neither GAIL nor RL can solve alone. The final agents achieved higher returns than human demonstrations due to exposure to task rewards. The use of a simulated system provides access to physical states, which can be advantageous for training policies. Four techniques are proposed to leverage physical states in simulation for policy training: demonstration curriculum, learning value from states, building object-centric discriminator, and auxiliary tasks. Demonstration as a curriculum involves altering the start state distribution with demonstration states clustered in different stages of a task to improve policy learning in continuous domains. During training, PPO uses a learnable value function V \u03c6 to estimate the advantage for policy gradient. Each worker executes the policy for K steps, using the discounted sum of rewards and the value as an advantage function estimator. To accelerate learning, the value function is trained with low-level physical states instead of pixel inputs. The value function V \u03c6 is trained separately from the policy using object-centric representations to stabilize training and improve agent performance. The discriminator in GAIL also utilizes object-centric features for manipulation tasks, masking out arm-related information to prevent stagnation in policy training. To improve learning efficiency and performance in deep RL methods, auxiliary tasks like state prediction are utilized. A state prediction layer is added on top of the CNN module to predict object locations from camera observations. The auxiliary task is trained by minimizing the loss between predicted and ground-truth object locations. Policy transfer is demonstrated on a real-world Kinova Jaco robot arm, with manual alignment of simulation visuals and dynamics to match the real arm. Our approach to sim2real policy transfer involves domain randomization of camera position and orientation instead of using professional calibration equipment. We train an end-to-end, pixels to velocities, feedback control policy without creating intermediate position goals using object position information. Additionally, we fine-tune our policies while subjecting them to action dropping to address latency issues on the real robot. Our proposed approach offers a general framework for visuomotor policy learning and is evaluated on a Kinova Jaco arm with 9 degrees of freedom. The robot arm has six arm joints and three actuated fingers, interacting with various objects on a tabletop. The visuomotor policy controls the robot using joint velocity commands, producing 9-dimensional continuous velocities. Proprioceptive features include arm joint positions and angular velocities, with real-time RGB observations collected using a positioned camera. The MuJoCo physics simulator is used for training, with a variety of objects randomized in terms of physical properties. The approach involves domain randomization for policy transfer and is evaluated on a Kinova Jaco arm with 9 degrees of freedom. The study utilized a 3D motion controller to operate the robot arm and collected data on various tasks. Visualizations of manipulation tasks in simulated and real environments showed discrepancies. Six tasks presented learning challenges, with a focus on sim2real policy transfer for block lifting and stacking tasks. The study focused on sim2real policy transfer for block lifting, stacking, and clearing tasks. Various random factors were varied to evaluate the model's robustness, including arm dynamics, lighting conditions, and block properties. The tasks involved grasping, lifting, stacking, and clearing blocks, presenting challenges for exploration and controller dexterity. The study examined sim2real policy transfer for manipulation tasks involving procedurally generated 3D shapes. Tasks included clearing tabletop and pouring liquid, testing the model's ability to generalize across object variations in complex tasks. The study focused on sim2real policy transfer for manipulation tasks with procedurally generated 3D shapes. One task involved pouring simulated liquid using small spheres, challenging due to the required dexterity. Another task was order fulfillment, placing toy planes and cars into designated boxes. The model successfully completed all six tasks with occasional failures, showcasing its ability to generalize across object variations in complex tasks. The study compared a full model with three baseline methods using the same setup but different hyperparameters. The full model achieved the highest returns in all tasks except for the block lifting task, where the baseline model performed equally well. The full model outperforms both reinforcement learning and imitation learning baselines in five tasks, showing the effectiveness of combining both approaches. Using demonstration as a curriculum in RL significantly improves learning efficiency by reducing exploration burden. The trained agents can surpass human demonstrations in various tasks, with RL learning faster initially but the full model eventually outperforming. GAIL outperforms RL in pouring liquid task, showing the effectiveness of imitation in shaping agent behaviors. In contact-rich domains, a controller learned solely from demonstrations struggles with complex object dynamics. Ablation study in block stacking task shows two groups of agents: those that learn to stack and those that only learn to lift. Hybrid RL/IL reward impacts agent performance. The study shows that using a hybrid RL/IL reward, learning value function from states, and object-centric discriminator are crucial for learning good policies. Optional components like LSTM, state prediction tasks, and actions in discriminator input also impact performance. The model is robust with \u03bb values between 0.3 to 0.7, balancing RL and GAIL rewards. Zero-shot transfer to a real Jaco arm without additional training was successful, indicating the simulation-trained policy's robustness. The study demonstrated successful zero-shot transfer of policies to a real Jaco arm, despite a noticeable reality gap. The lifting policy had a 64% success rate over 25 trials, while the stacking policy had a 35% success rate over 20 trials. 80% of stacking trajectories included successful lifting behavior. The study showed successful transfer of policies to a real Jaco arm, with a 64% success rate for lifting and 35% for stacking. 80% of stacking trajectories exhibited successful lifting behavior. The combination of reinforcement and imitation learning improved agents' ability for dexterous manipulation tasks. The proposed method involves three stages: collecting demonstration data, using physical simulation for training, and transferring to real-world deployment. Future work aims to enhance sample efficiency. The policy network utilizes pixel observations and proprioceptive features as input, with the proprioceptive feature describing joint positions and velocities of the Kinova Jaco arm. The 24-dimensional proprioceptive feature includes positions and velocities of arm joints and fingers. Finger velocities are excluded due to noisy sensory readings. Adam BID16 is used to train the neural network for real-world deployment. The neural network parameters were trained using Adam BID16 due to noisy sensory readings on the real robot. The learning rates for policy, value, discriminator, and auxiliary tasks were set accordingly. The pixel observation was processed by a two-layer convolutional network, followed by a recurrent layer before the policy and value outputs. The policy output represents a conditional Gaussian distribution over joint velocities, with different initial policy standard deviations for different tasks. An auxiliary head with a separate MLP was added to the policy network. The policy includes a 3-layer MLP on top of a convolutional network, with hidden units of 200 and 100 in the first two layers. The discriminator is a 3-layer MLP with hidden units of 100 and 64. The networks use tanh nonlinearities. The visuomotor policies were trained using distributed PPO with synchronous gradient updates from 256 CPU workers. Each worker completes an entire episode before parameter updates are computed. Episode lengths are set based on task difficulty, with the longest being 1000 time steps. K = 50 for computing K-step returns and training LSTM units. To improve sim2real transfer, observation frequencies are lowered to 5Hz for pixel observations and 10Hz for proprioceptive features. Domain variations include adding Gaussian noise to proprioceptive features and uniform noise to pixels. Additionally, variations in the shade of grey on the Jaco arm, table top color, and light source location are applied. In block lifting tasks, the dynamics of the robot arm are adjusted to improve performance. Delays in action execution on the real robot negatively impact the agent's performance. To address this, the trained agent is fine-tuned in simulation with a random chance of dropping actions. Each action has a 50% chance of immediate execution. After fine-tuning with action dropping, the agent's success rate in block lifting tasks increased from 48% to 64%. The robot arm operates at a control frequency of 20Hz, with each time step taking 0.05 seconds. The task episodes have a fixed length determined by human demonstration time, and are segmented into stages to track progress. The block stacking task is segmented into stages representing an agent's progress. Functions on the physical state determine the stage, used to cluster demonstration states for training episodes. This approach allows for defining reward functions without hand-engineering, using piecewise constant rewards for each task stage. Stages, reward functions, auxiliary tasks, and object-centric features are detailed for the six tasks in the experiments. Each episode lasts 100 time steps. The experiments involve six tasks with defined stages and rewards. For block lifting, the stages are initial, reaching the block, and lifting the block. The auxiliary task is predicting 3D coordinates. For block stacking, stages include reaching the orange block, lifting it, and stacking it onto the pink block. The auxiliary task is predicting 3D coordinates of the blocks. Clearing the table with blocks involves stages like reaching the orange block and lifting it. The experiments involve tasks with defined stages and rewards, such as block lifting and stacking. The auxiliary task is predicting 3D coordinates. Clearing the table with blocks also involves specific stages. In the experiments, tasks involve stages with specific rewards, like pouring liquid and order fulfillment. The auxiliary task is predicting 3D coordinates. The object-centric feature includes positions of objects and their relationships. The number of objects varies across episodes, with rewards based on correct placements. Objects nearest to the gripper are represented for the auxiliary task. The auxiliary task involves predicting the 3D coordinates of the nearest plane and car to the gripper, while the object-centric feature captures the relative positions from the gripper to these objects."
}