{
    "title": "SJaP_-xAb",
    "content": "We propose a new output layer for deep neural networks that allows training with logged contextual bandit feedback, which can be obtained in large quantities at low cost. Our Counterfactual Risk Minimization (CRM) approach uses BanditNet to train deep networks without conventionally labeled images, demonstrating its effectiveness for object recognition. This method leverages log data from online systems like search engines and recommender systems. The interaction logs of an ad-placement system for banner ads contain contextual-bandit feedback, limited to the actions taken by the system. This feedback provides partial information, as we do not see how the user would have responded to different actions. Learning from log data is fundamentally different from traditional supervised learning. The paper proposes a new output layer for deep neural networks that allows training on logged contextual bandit feedback, enabling knowledge acquisition at unprecedented scale without the need for full-information feedback. This approach opens up opportunities for deep learning in domains where manual labeling of feedback is not feasible. The algorithm, Batch Learning from Bandit Feedback (BLBF), does not require interactive interventions and utilizes a counterfactual training objective for training deep neural networks. BanditNet proposes a counterfactual risk minimization objective for deep neural networks, based on equivariant estimator of true error using contextual bandit feedback. This training objective differs from conventional cross-entropy objective by requiring only propensity-logged feedback. It enables large-scale training through decomposition for stochastic gradient descent optimization. BanditNet presents an empirical evaluation of a deep neural network architecture trained in the BLBF setting, specifically a BanditNet version of ResNet for visual object classification. Despite using cheaper data, Bandit-ResNet can achieve similar performance as ResNet with cross-entropy on annotated images. The implementation of BanditNet is shared for experimentation in other applications. Related works have explored weak supervision approaches for deep learning and label corruption on CIFAR-10. In contrast to previous approaches using weak supervision, we work in the BLBF setting and modify loss functions for risk minimization. Previous methods for learning from bandit feedback employ risk minimization principles and use IPS estimator with SGD. The self-normalized estimator has been shown to be more suitable for BLBF. Our work demonstrates efficient optimization of a reformulated self-normalized estimator using SGD for batch learning from bandit feedback (BLBF). We compare deep learning models trained with BLBF to existing approaches on a benchmark dataset. Additionally, our algorithm, along with independent work, shows success with off-policy variants of the REINFORCE algorithm. The reformulated estimator for batch learning from bandit feedback is connected to the REINFORCE algorithm. It highlights the importance of baseline hyper-parameters and proposes a method for selecting baselines in the off-policy setting. This approach aims to create an equivariant counterfactual learning objective in contextual bandit settings. In this paper, a neural network is seen as implementing a stochastic policy \u03c0, where the network predicts by sampling an action y \u223c \u03c0 w (Y | x). Existing network architectures can be compatible with this view, allowing for actions to be sampled from a conditional distribution. The goal is to find a policy that minimizes risk by using logged contextual bandit feedback to train a neural network policy with low risk. The logged data consists of observed context, actions taken by the logging policy, propensity, and received loss. Counterfactual risk minimization (CRM) is a method used to train deep neural networks with low risk by directly minimizing an empirical risk estimated from logged bandit data. This approach addresses the question of how well a policy would have performed if it had been used instead of the logging policy. While minimizing empirical risk is common in machine learning, obtaining a reliable estimate from incomplete logged data is challenging. The logged bandit data D is incomplete and biased, making it challenging to estimate empirical risk accurately. Existing methods either require full knowledge of the loss function or the ability to draw new samples interactively. However, in our setting, we have a fixed dataset limited to samples from the logging policy. To address bias and variance in risk estimation, we use importance sampling to correct for sampling bias and handle missing data. This approach removes the distribution mismatch between the logging policy and the target policy. The IPS estimator is used to estimate expectations on a sample of bandit-feedback examples. Training parameters directly to optimize this estimate faces obstacles like lack of equivariance and bias-variance trade-off. Efficient algorithms for minimizing empirical risk and handling exploration policy distance are needed for high-capacity models. The IPS estimator faces challenges like propensity overfitting due to lack of equivariance, impacting the variance of empirical risk estimates. Optimizing Eq. FORMULA4 directly within a learning algorithm can lead to severe problems. The self-normalized IPS estimator (SNIPS) addresses propensity overfitting by minimizing risk estimates and exploiting knowledge of denominator expectations. SNIPS estimator has lower variance than IPS estimator and bias asymptotically vanishes at a rate of O(1/n). The SNIPS estimator minimizes risk estimates and exploits denominator expectations to address propensity overfitting. The training objective requires efficient optimization methods, as stochastic gradient descent is not applicable. A reformulation is proposed to retain desirable properties of SNIPS while allowing the use of established SGD training algorithms. The SNIPS estimator minimizes risk estimates and exploits denominator expectations to address propensity overfitting. To find the optimal value for the control variate, a grid search is conducted for S * and the constrained optimization problem is solved for each value. The overall optimal estimate is then determined by taking the minimum of the solutions. SGD can be used to solve the equality constrained risk minimization problem without the need for constrained optimization. To find the optimal value for the control variate in risk estimation, the Lagrangian of the constrained optimization problem is considered. The roles of \u03bb and S are reversed to explore a certain range in the search for the optimal solution. The solution does not depend on S, allowing for the computation of S after finding the minimum. The optimal S corresponding to a given \u03bb is determined using optimality conditions. The sequence of \u03bb j produces solutions \u0175 j corresponding to a sequence of {S 1 , . . . , S k }. To identify the sensible range of S, we can use Eq. (9) which concentrates around its expectation of 1 for each \u03c0 w as n increases. The exploration of S can be steered via \u03bb, as the resulting S changes monotonically with \u03bb. The key computational problem is finding the solution of Eq. (15) for each \u03bb j. The key computational problem is finding the solution of Eq. (15) for each \u03bb j in an unconstrained optimization problem. This allows for re-purposing existing fast methods for training deep neural networks, such as SGD with momentum, to optimize the objective scalably. Loss translations have been used in on-policy reinforcement learning, but the off-policy setting considered here presents a different situation. In the off-policy setting, sampling new roll-outs from the current policy is not possible, making standard variance-optimal estimators like REINFORCE unusable. Using the expected loss of the learned policy as a baseline is not optimal, and the key issue in batch learning from bandit feedback remains unclear. The approach proposed justifies picking the value of \u03bb by minimizing a specific equation, with the option to add variance regularization for improved risk estimate robustness. The empirical evaluation focuses on training deep models effectively using a new approach, comparing performance under different data and training objectives, and exploring the effectiveness of the approximate SNIPS objective. The ResNet20 architecture is adapted for BanditNet experiments, replacing the cross-entropy objective with counterfactual risk minimization for evaluation on the CIFAR-10 dataset. The Bandit-ResNet is evaluated on the CIFAR-10 dataset using bandit feedback data generated from a hand-coded logging policy. The model is trained using this data and compared against a conventional ResNet using full-information training. The Bandit-ResNet model is compared to a conventional ResNet using full-information training on the CIFAR-10 dataset. The Bandit-ResNet uses the same network architecture, hyperparameters, data augmentation scheme, and optimization method as the conventional ResNet. The only parameter adjusted for Bandit-ResNet is the learning rate, which was lowered to 0.1. Test performance is reported after 1000 training epochs. The Bandit-ResNet model outperforms a conventional ResNet on the CIFAR-10 dataset, even with bandit feedback with a 49% error rate. The prediction error of Bandit-ResNet ranges from 13% to 8.2%, approaching the performance of the conventional ResNet trained on full-information. This indicates that Bandit-ResNet can converge to the same performance level with enough bandit feedback training data. The choice of Lagrange multiplier \u03bb significantly impacts prediction performance in bandit feedback training data. Values between 0.8 to 1.0 yield good results, while other values lead to performance degradation. Different methods for selecting \u03bb were explored, with the IPS estimator resulting in worse performance than the logging policy. The expected loss of the learned policy was also considered as a baseline. The expected loss of the learned policy serves as a baseline, with values between 0.130 and 0.083 for the best policies found. Control variate values in the SNIPS estimate increase above 1 as \u03bb is increased, indicating propensity overfitting. Solutions with control variate values outside the range of [0.974, 1.026] should be discarded due to overfitting concerns. The new output layer for deep neural networks allows training with logged contextual bandit feedback, enabling training on large amounts of data. The approach achieves comparable predictive accuracy to conventional training for visual object recognition using the ResNet architecture. BanditNet introduces a new output layer for deep neural networks that enables training with logged contextual bandit feedback. This approach achieves comparable predictive accuracy to conventional training for visual object recognition using the ResNet architecture. The paper suggests future directions for research, such as exploring new applications with readily available contextual bandit feedback, combining BanditNet with propensity estimation techniques in infeasible logging settings, and improving BanditNet with smarter search techniques and more efficient counterfactual estimators. BanditNet introduces a new output layer for deep neural networks that enables training with logged contextual bandit feedback. The approach achieves comparable predictive accuracy to conventional training for visual object recognition using the ResNet architecture. The paper suggests future research directions, such as exploring new applications with contextual bandit feedback, combining BanditNet with propensity estimation techniques in infeasible logging settings, and improving BanditNet with smarter search techniques and more efficient counterfactual estimators. Let p \u2264 \u03c0 0 (y | x) be a lower bound on the propensity for the logging policy, then constraining the solution of Eq. (11) to the w with control variate S \u2208 [1 \u2212 , 1 + ] for a training set of size n will not exclude the minimizer of the true risk w * = arg min w\u2208W R(\u03c0 w ) in the policy space W with probability at least DISPLAYFORM4. The text discusses the issue of overfitting in the context of using the naive IPS estimator with a dataset of BLBF samples. It introduces the vanilla IPS risk estimate and the need to construct an unbiased gradient estimate for stochastic optimization. The goal is to maximize the estimate by randomly selecting samples from the dataset. The text discusses constructing an unbiased gradient estimate for stochastic optimization by randomly selecting samples from the dataset of BLBF instances. The text discusses how constructing an unbiased gradient estimate for stochastic optimization using a sub-sample of data can lead to variance differences in the risk estimators, affecting the optimization process. The text introduces a new training objective to control overfitting by optimizing the upper confidence interval based on variance estimates of risk estimators. Taylor-majorization is used to bound the objective, leading to convergence to a local optimum with a small number of iterations. Taylor-majorization is a form for easily computable constants A and B, allowing for SGD optimization."
}