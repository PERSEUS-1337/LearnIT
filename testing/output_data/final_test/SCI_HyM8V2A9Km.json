{
    "title": "HyM8V2A9Km",
    "content": "Hindsight Experience Replay (HER) addresses sparse reward in reinforcement learning by relabeling goals. However, HER has limited applicability due to a lack of universal goal representation. Augmenting experienCe via TeacheR's adviCE (ACTRCE) extends HER using natural language as goal representation, efficiently solving challenging 3D navigation tasks. ACTRCE allows agents to generalize to unseen instructions and lexicons, highlighting the importance of using language goal representations. Using hindsight advice is crucial for solving challenging tasks, with even a small amount being sufficient for learning to progress practically. Designing complex reward functions for deep reinforcement learning applications can be nontrivial and require significant effort, leading to biased learning and unexpected behaviors. Sparse and binary reward functions can be used as an alternative to avoid the complexity of defining intricate reward structures. The reward function in deep reinforcement learning can be simplified using a sparse and binary approach, but this can make learning difficult. Hindsight Experience Replay (HER) addresses this issue by converting failed experiences into successful ones. This method assumes that there is a goal for every state in the environment, which can be achieved by choosing the goal space to be the state space. However, representing the goal using the state space is inefficient and contains redundant information. In the context of deep reinforcement learning, the HER framework combines natural language representation of goals to efficiently describe and achieve desired outcomes in various tasks and environments. This approach allows for flexible and compact goal descriptions, reducing redundancy in state representations. Combining the HER framework with natural language goal representation, the ACTRCE technique provides advice to agents in natural language after each episode to improve reinforcement learning. This method helps alleviate the sparse reward problem and allows agents to efficiently solve challenges in 2D and 3D environments. Our work combines reinforcement learning with natural language goal representation, demonstrating the importance of hindsight advice in solving challenging tasks. This approach is crucial for general understanding of natural language and language grounding. Our work combines reinforcement learning with rich language advice to address the problem efficiently. It involves an agent interacting with a Markov Decision Process to maximize expected cumulative return. The text discusses Q-learning, an off-policy, model-free reinforcement learning algorithm based on the Bellman equation. It uses semi-gradient descent to minimize the squared Temporal Difference error and Deep Q-Network builds on Q-learning by using a neural network to approximate the optimal Q-function. The text introduces goal-oriented reinforcement learning, where a neural network approximates Q* and a replay buffer and target network improve training stability. It extends the Markov Decision Process with a goal space G, where a fixed goal g induces a reward function r_g. The agent's objective is to maximize the expected discounted cumulative return given the goal. Goal-oriented reinforcement learning involves a sparse reward function r_g(s_t, a_t) = f_g(s_t+1), where f_g acts as a predicate determining success. HER addresses this issue by collecting goal-specific experiences and using off-policy learning with goal transformations. A representation map T: S \u2192 G allows for flexible relabeling with desirable goals. The goal representation in goal-oriented reinforcement learning aims to satisfy a specific predicate for each state. Constructing this mapping is not straightforward, as a trivial approach leads to redundant and uninformative goal representations, limiting the algorithm's applicability. In scenarios like navigating towards an object in a 3-D environment, the variety of possible approaches results in diverse states that meet the goal, making it challenging to abstractly represent the goal. The objective is to maximize the expected discounted cumulative return for each goal in a goal-oriented Markov Decision Process (MDP) using a parameterized agent. In goal-oriented reinforcement learning, the goal representation mapping T describes the goal satisfied at each state. Using natural language for goal representation reduces redundancy and provides an abstract representation. A teacher gives a natural language description of the goal achieved in each state, improving the representation of goals in the learning process. In goal-oriented reinforcement learning, a teacher can describe the achieved goal in each state using natural language. This description can be used to convert a failure trajectory into a successful one by relabeling the original goal. Positive and negative reward signals are both important for training. In goal-oriented reinforcement learning, a teacher can describe the achieved goal in each state using natural language. This description can be used to convert a failure trajectory into a successful one by relabeling the original goal. Negative reward is given for unachieved goals, with multiple possible goals in a state. Teachers provide advice in natural language, such as \"Reach a blue torch\" or \"Reach the largest blue object\". In goal-oriented reinforcement learning, teachers provide advice in natural language to achieve different goals. Each teacher corresponds to a different goal description, and their advice is used to relabel the original goal with positive or negative rewards. The approach involves augmenting the replay buffer with trajectories based on the advice given. The method is formally described in Algorithm 1, considering both MDP and POMDP settings for goal representation. In goal-oriented reinforcement learning, teachers provide advice in natural language to achieve different goals. Two standard ways to convert a language goal into a continuous vector representation for neural network training are explored. One way is to use a recurrent neural network (RNN) to embed each word into its hidden state. The other way is to use a pre-trained language component to represent the given language instructions, allowing the agent to gain better natural language understanding and be more robust to language advice. In goal-oriented reinforcement learning, teachers provide advice in natural language to achieve different goals. The architecture of the model includes three modules: a language component converting instructions into a continuous vector, an observation processing component using convolution neural networks, and a fusion component using gated attention to combine goal information and observation. Experimental results demonstrate the effectiveness of the proposed method in two environments, KrazyGrid World and ViZDoom. The experimental setup includes two environments, KrazyGrid World and ViZDoom. The study compares different goal representations using hindsight advice, generalization, and semantic similarities. It shows that GRU and pre-trained embeddings outperform the one hot vector approach as the number of instructions increases. The study also demonstrates that pre-trained embeddings can generalize to goals with out-of-vocabulary words. Additionally, it shows significant improvement in sample efficiency when using hindsight language advice. In challenging tasks, significant improvement in sample efficiency is shown by using teachers' advice. Even limited advice can lead to notable enhancements, with low burden in practice. The method was tested in KrazyGrid World, a 2D grid environment, and ViZDoom, a 3D environment. ViZDoom BID14 BID4 is a 3D learning environment based on the game Doom, with language goals involving natural instructions like \"Go to the green torch\". Singleton tasks are individual language goals, while challenging tasks involve compositions of these tasks using \"and\" and \"or\" functions. In experiments, the agent completes tasks A and B within an episode, considering them complete when one task is achieved. Different task combinations were tested for KGW and ViZDoom. DQN algorithm was used for reinforcement learning. Multi-environment training involved sampling 16 environments, collecting data, updating agents, and further training. The study compares language-based goal representations with non-language representations, showing the benefits of using language for goal representation in learning. The study compares language-based goal representations with non-language representations, showing the benefits of using language for goal representation in learning. Language goal representations are more effective as tasks become more difficult, allowing for generalization to unseen goals in training. Pre-trained sentence embeddings enhance the robustness of language goal representations. InferLite BID16 is used as a pre-trained sentence representation with fixed parameters during training. It is a lightweight sentence encoder trained for natural language inference, similar to InferSent BID7 but without RNNs. The original embedding vector dimension is 4096, projected down to 256. The non-language baseline uses one-hot vectors for instructions, while language-based goal representations are shown to be more effective, especially for difficult tasks and generalization to unseen goals. In the ViZDoom environment, goal representations with hindsight advice are compared for learning singleton and compositional tasks. One-hot representations perform well in singleton tasks but struggle in compositional tasks, achieving only a 24% success rate compared to 97% with language representations. Language representations allow for better generalization to unseen instructions, as shown in zero-shot learning results in Table 1. With GRU language goal representation, the agent can generalize to unseen instructions 83% of the time, demonstrating strong generalization ability. A visualization analysis revealed significant differences in learned embeddings for different goal representations, with correlation matrices plotted in Figure (3). The study compared GRU and InferLite embeddings, finding similar block-like structures for colours and shapes in GRU and InferLite. t-SNE embeddings showed meaningful clustering for language goals and sporadic embeddings for one-hot goals. Pre-trained embeddings were used for model generalization to unseen lexicons at test time. The agent trained with InferLite goal representations showed 83% generalization to unseen instructions. The agent achieved tasks above 66% of the time with the ability to understand sentences of similar meanings. Understanding synonyms can improve learning robustness in noisy settings. Hindsight advice plays a crucial role in learning, as shown in a comparison between the method \"ACTRCE\" and the algorithm DQN without hindsight advice. Without hindsight advice, DQN struggles to learn challenging tasks, but even a small amount (1%) of advice can significantly improve learning. Recurrent neural networks are used for embedding language goals in both our method and the baseline. Experiments on KGW with different grid configurations show our method quickly learns and achieves good results compared to the baseline DQN. In ViZDoom experiments, our agent trained in 3 configurations shows promising results. In ViZDoom experiments, the agent trained in 3 configurations: 5 objects in easy and hard mode, and 7 objects in hard mode with larger room size. Only the agent trained with ACTRCE was able to learn in the more difficult 7-objects environment. Comparison of success rates between ACTRCE and DQN is shown in Figure 4. Multitask and Zero-Shot generalization performance of the agent is summarized in Table 1. In KGW experiments, our method quickly learns and outperforms the baseline DQN in different grid configurations. In ViZDoom experiments, the agent trained with ACTRCE efficiently learned and achieved good performance in a challenging environment with 5 objects in easy mode. The agent was able to learn well with hindsight advice, while the baseline DQN failed to learn. A variant of ACTRCE with limited teacher advice also showed successful learning in a single target mode with 7 objects in the ViZDoom environment. The agent learned well with limited advice in ViZDoom experiments, showing robustness in practical settings. Previous approaches used natural language advice in reinforcement learning, such as translating advice into a program or shaping rewards with human feedback. The agent used multi-modal embedding to self-monitor progress in beating Atari games. Language was used as a latent parameter space for few-shot learning problems, including policy search. Reinforcement learning was also used for task-oriented language grounding, mapping language instructions to actions in different environments. Our work builds on previous research that used a gated-attention architecture to combine language and image features for executing written instructions in a 3D environment. We also incorporate experience replay techniques to enhance learning speed and credit assignment processes. The ACTRCE method uses natural language as a goal representation for hindsight advice in reinforcement learning. It shows that language goal representations can efficiently solve challenging 3D navigation tasks and generalize to unseen instructions, even with unseen lexicons. The ACTRCE algorithm efficiently uses natural language advice for reinforcement learning in a 2D grid environment called KrazyGrid World. The environment consists of tiles with different functionalities and colors, and the agent's goal is to reach goals of different colors by taking discrete actions. The grid state is represented by concatenating functionality and color attributes into one-hot vectors, with the agent's position also represented in a one-hot vector. The experiments are conducted in a 9x9 grid with 3 goals present. In experiments, a 9x9 grid environment with 3 goals of distinct colors is used. Various numbers of lavas are tested, ensuring at least 1 lava of each color. The episode ends when the agent reaches a lava or goal, or after a maximum of 25 time steps. The goal space is expanded to include compositions of goals, leading to modifications in the environment. Episodes now end when the agent reaches a lava or 2 different goals, or after 50 time steps. An extra action called \"flag\" is added for the agent to end the episode when it believes the goal is achieved. ViZDoom BID14 BID4 is a 3D learning environment based on the game Doom, where the agent navigates a room with random objects using actions like turning and moving forward. The goal is to follow a natural language instruction to reach a specific object, with the episode ending when the object is reached or after 30 time steps. An additional action allows the agent to end the episode when it believes the goal is achieved. The ViZDoom BID14 BID4 environment involves navigating a room with random objects to reach a specific object based on natural language instructions. Difficulty modes vary in object and agent distribution. Compositional instructions consist of two single object instructions joined by \"and\". No superlative instructions are included to ensure clarity. In the ViZDoom BID14 BID4 environment, compositional instructions involve reaching specific objects with a HUD displaying reached objects. The instructions are designed to be unambiguous, with mutually exclusive sets of valid objects for each instruction. The HUD in the ViZDoom environment displays up to 2 reached objects, and the episode ends when a second object is reached. Synonym instructions are generated by replacing words in the original instructions. Positive and negative feedback is given based on whether objects are reached or not. In the ViZDoom environment, instructions are given based on the number of objects reached. For 0 or 1 object, singleton advice is provided, and not giving any advice in these cases also works well. If 2 objects are reached, instructions are generated for each object and combined with \"and\" for positive advice, while for negative advice, instructions are generated for the unreached objects and combined. In ViZDoom, instructions are given based on the number of objects reached. For 0 or 1 object, singleton advice is provided. For 2 objects, instructions are generated for each object and combined with \"and\" for positive advice, while for negative advice, instructions are generated for the unreached objects. The process involves using convolution layers and LSTM for processing grid observations and language sentences, followed by fully connected layers to predict action values. The text chunk describes the use of convolution layers and LSTM for processing grid observations and language sentences in ViZDoom. It involves passing the observations through several convolution layers with ReLU activation functions and using bidirectional LSTM with Luong attention. The final step includes processing the observation with two more convolution layers and using language attention to gate back the observation. The text chunk discusses the architecture used for processing grid observations and language input in ViZDoom. It involves passing observations through convolution layers and a fully connected layer to predict action values. The language input is processed using an embedding matrix and a Gated Recurrent Unit (GRU) to process word embeddings. The final hidden vector from the GRU is passed through a fully connected layer. The text chunk discusses the architecture used for processing grid observations and language input in ViZDoom. It involves passing observations through convolution layers and a fully connected layer to predict action values. The language input is processed using a Gated Recurrent Unit (GRU) to process word embeddings, followed by a fully connected layer for attention vector calculation and subsequent processing through a LSTM for predicting action values in KrazyGrid World experiments. Hyperparameters are tuned for learning rate, replay buffer size, and training frequency, with Double DQN and Huber loss used for stability. The ViZDoom environment utilizes a set of training instructions and test instructions for agent training and evaluation. The implementation of DQN is based on previous work and includes a cyclic buffer replay buffer. Training starts after collecting the first 1000 frames with a linear decay of epsilon. Double Q-learning is used for stability in training. The training in ViZDoom begins with a range of frames selected. Double DQN is used to reduce Q-value overestimation with Huber loss for stable gradients. The Adam optimizer with a learning rate of 0.0001 is utilized. The network is updated every 4 frames on easy mode and 16 frames on difficult mode. Sampling from the replay buffer involves selecting 32 consecutive frames from a random episode. Sequential mini-batches allow for n-step Q learning, and running 16 parallel threads helps alleviate sample correlation. In ViZDoom training, 16 parallel threads are used to send gradient updates to shared network parameters. The training thread model is synchronized with the shared network before computing loss and gradient. The target network is synced with the model every 500 time steps. A separate thread evaluates multi-task success rate. The section discusses teacher types and selecting desired goals for the agent to perform in the environment. In ViZDoom training, 16 parallel threads are used for gradient updates to shared network parameters. The training thread model is synchronized before computing loss and gradient. The target network is synced every 500 time steps. The section discusses teacher types and selecting desired goals for the agent in the environment. The agent receives advice from different types of teachers based on its performance, including Optimistic, Knowledgeable, and Discouraging teachers. The study compares this method to the DQN algorithm. The study compared a method using different types of teachers to the DQN algorithm in KrazyGrid World. Results showed that the method with knowledgeable and discouraging teachers, ACTRCE, outperformed DQN in learning tasks with varying levels of difficulty. After 32 million frames of training, ACTRCE performed worse, dropping from 83% to 63%. Having knowledgeable teachers helped speed up learning, even as the task became harder with more lavas. Language advice was provided when the agent reached lava, maintaining the same amount of advice per step. ACTRCE learned at a similar rate in the more difficult setting, with a significant performance gap from ACTRCE \u2212 (80% vs 63%). The study also raised concerns about agents only learning easier tasks if desirable tasks are very difficult. Learning easier tasks from hindsight was considered for potential learning benefits. Learning easier tasks from hindsight can provide a learning signal for more difficult tasks. A transfer learning experiment was conducted where agents were pretrained with a pessimistic teacher before training with optimistic and discouraging teachers. Pretraining with the pessimistic teacher led to faster learning compared to unpretrained agents, even though the goals did not overlap. In experiments on ViZDoom, pretrained agents learned faster than unpretrained ones, especially in environments with 3 lavas. Learning easier goals can provide signals for harder goals. DQN struggled with 5 objects on hard mode, while ACTRCE showed low variance across seeds. In ViZDoom experiments, pretrained agents learned faster, especially in environments with 3 lavas. DQN struggled with 5 objects on hard mode, while ACTRCE showed low variance across seeds. A3C had lower sample efficiency compared to DQN/ACTRCE on the easy task. On the hard task, results couldn't be reproduced due to limited computational budget. Average episode length decreased slowly with ACTRCE in 3 ViZDoom scenarios. The average episode length is decreasing with ACTRCE in ViZDoom scenarios, while the baseline DQN remains stable. A cumulative success rate curve is constructed to evaluate model performance based on episode length and success rate. The curve shows the overall success rate of the model, with a larger area indicating better performance. The Multi-task cumulative success rate for 3 ViZDoom tasks using GRU hidden state language encoding is shown in FIG0. ACTRCE outperforms baseline DQN in successful trajectories in various scenarios, with different episode lengths and target objects. The Multi-task cumulative success rate for 3 ViZDoom tasks using GRU hidden state language encoding is shown in FIG0. ACTRCE outperforms baseline DQN in successful trajectories in various scenarios, with different episode lengths and target objects. In one scenario, reaching two target objects adjacent to each other takes 10 time-steps, while reaching non-adjacent targets requires over 20 time-steps, involving careful navigation to avoid obstacles."
}