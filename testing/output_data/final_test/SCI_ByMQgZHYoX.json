{
    "title": "ByMQgZHYoX",
    "content": "Bayesian methods have been successfully applied to sparsify weights of neural networks and remove structure units. The approach is further developed for gated recurrent architectures, specifically sparsifying preactivations of gates and information flow in LSTM. This speeds up the forward pass, improves compression, and results in interpretable gate sparsity. Recurrent neural networks (RNNs) can be compressed significantly without losing quality, with methods based on matrix factorization. For RNN compression, sparsification techniques like pruning and Bayesian sparsification are used to remove unnecessary weights from the model without extensive hyperparameter tuning. Bayesian sparsification treats weights as random variables and sets weights with low signal-to-noise ratio to zero. This approach can be easily extended to eliminate intermediate variables in the network's computational graph. In this work, the focus is on sparsifying individual weights and eliminating neurons in RNNs, particularly LSTM, by introducing multiplicative variables. The introduction of multiplicative variables on preactivations of gates and information flow in LSTM simplifies the structure, speeds up computations, and leads to positive effects. The authors propose Sparse variational dropout (SparseVD) for neural network sparsification, using a Bayesian technique with a log-uniform prior over weights and fully factorized normal approximate posterior. The model aims to sparsify individual weights, gates, and information flow in LSTM, leading to higher overall model compression. The authors propose Sparse variational dropout (SparseVD) for neural network sparsification, using a Bayesian technique with log-uniform and normal priors on weights. They introduce group variables for neurons to compress layers and remove neurons, aiming to sparsify individual weights in RNNs. The model introduces group variables for neurons in LSTM layers, aiming to sparsify individual weights. Group variables are learned in a similar way as weights, with a fully factorized normal distribution. After learning, weights and group variables with low signal-to-noise ratio are set to 0, simplifying computations during the forward pass. In the proposed model, group variables are introduced for neurons in LSTM layers to sparsify individual weights. These group variables are learned similarly to weights, with a fully factorized normal distribution. After learning, weights and group variables with low signal-to-noise ratio are set to 0, simplifying computations during the forward pass. The model does not remove components from LSTM but makes them constant, achieving compression and acceleration while maintaining the correct LSTM structure. Experiments were conducted on text classification and language modeling tasks using LSTM architecture. Four models were compared in terms of quality and sparsity, including a baseline model without regularization and a standard SparseVD model for weights sparsification only. In SparseVD models, weights matrices of all layers are sparsified using different approaches: standard sparsification for weights only (W), group sparsification for neurons (W+N), and group sparsification for gates and neurons (W+G+N). Additional multiplicative weights are used for input vocabulary sparsification in group sparsification cases. Sparsity level is measured by calculating the compression rate of individual weights. Input neuron sparsification in language modeling tasks reduces model quality and hidden neuron sparsity. The proposed gate sparsification technique in SparseVD models reveals a work-flow structure of LSTM networks for different tasks. Multiplicative variables for neurons increase group sparsity level without quality drop, while additional variables for gates further increase group sparsity. Constant gates may have large or small bias values, making them always open or closed. Output gates are crucial for language modeling tasks to store input information in memory and output predictions. For text classification tasks, models do not use output gates as they only need to output the answer once at the end of the sequence. The character level language modeling task is more challenging than the word level one, requiring the whole gating mechanism to solve it. Two standard datasets, IMDb and AGNews, are used for evaluation in text classification tasks. The Penn Treebank corpus is used for language modeling evaluation. The Penn Treebank corpus BID10 is used for text classification tasks with a vocabulary of 50 characters or 10,000 words. Networks have one embedding layer of 300 units, one LSTM layer of 128 / 512 hidden units for IMDb / AGNews, and a fully connected layer. Embedding layer is initialized with word2vec BID12 / GloVe BID16 and SparseVD models are trained for 800 / 150 epochs on IMDb / AGNews. Training is done using Adam BID4 with batches of size 128 and a learning rate of 0.0005. Baseline networks overfit, so results are presented with early stopping. To solve character/word-level tasks, networks with one LSTM layer of 1000/256 hidden units and a fully-connected layer are used for language modeling. SparseVD models are trained for 250/150 epochs on character-level/word-level tasks. Weight matrices are initialized orthogonally, biases with zeros, and initial hidden and cell values are set to zero. For character-level task, networks are trained on 100-character sequences in mini-batches of 64 with a learning rate of 0.002. For word-level task, networks are unrolled for 35 steps with mini-batch size of 32 and learning rate of 0.002. The models are trained with a learning rate of 0.002 and clip gradients with a threshold of 10 using mini-batches of size 32. Results for baseline networks are presented with early stopping due to overfitting."
}