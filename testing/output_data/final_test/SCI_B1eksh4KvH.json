{
    "title": "B1eksh4KvH",
    "content": "In this work, a novel Adaptive Curriculum Learning loss (CurricularFace) is proposed for deep face recognition. The approach embeds curriculum learning into the loss function to address easy samples in the early training stage and hard ones in the later stage, aiming to enhance discriminability between different classes. Our CurricularFace approach adjusts the importance of easy and hard samples during different training stages based on their difficulty. Experimental results show its superiority over competitors in face recognition. The success of Convolutional Neural Networks in face recognition is attributed to training data, network architectures, and loss functions. Designing appropriate loss functions is crucial for training deep face CNNs, with current methods mainly using softmax-based classification loss. Several margin-based variants like CosFace, Sphereface, ArcFace, and Adacos have been proposed to enhance features' discriminative power in open-set face recognition. These margin-based loss functions aim to increase intra-class compactness and inter-class discrepancy. However, they do not explicitly emphasize each sample's importance. Triplet loss and SV-Arc-Softmax integrate margin and mining motivations for deep face recognition, with Triplet loss adopting semi-hard mining techniques. Margin-based loss functions like CosFace, Sphereface, ArcFace, and Adacos aim to enhance features' discriminative power in open-set face recognition by increasing intra-class compactness and inter-class discrepancy. Triplet loss and SV-Arc-Softmax integrate margin and mining motivations, with Triplet loss using semi-hard mining techniques to enlarge the margin between triplet samples. SV-Arc-Softmax defines hard samples as misclassified ones and emphasizes them by increasing the weights of their negative cosine similarities. However, drawbacks exist in the training strategies of both margin-and mining-based loss functions, as the mining strategy is often ignored, leading to convergence issues when using a large margin on small backbones like MobileFaceNet. In ArcFace, the constant similarity value is fixed at 1 for all samples during training. Different methods like SV-Arc-Softmax and CurricularFace modulate negative cosine similarities of hard samples to reduce loss during early training stages. This modulation helps in distinguishing between easier and harder samples. In CurricularFace, a novel adaptive curriculum learning loss is proposed for deep face recognition. It emphasizes easier samples during early training stages and modulates hard samples' negative cosine similarities with cos \u03b8j. The constant t plays a key role in model convergence, with a slightly larger value hindering convergence. This new training strategy is inspired by human learning, where easy cases are learned first. Our CurricularFace approach incorporates Curriculum Learning into face recognition in an adaptive manner, differing from traditional methods in two key aspects. Firstly, the curriculum construction is adaptive, with samples randomly selected in each mini-batch and the curriculum established adaptively by mining hard samples online. Secondly, the importance of hard samples is dynamic and can be adjusted during different training stages. The importance of hard samples in each mini-batch is determined by their difficulty level, with mis-classified samples chosen as hard samples and weighted based on cosine similarities. An adaptive curriculum learning loss for face recognition is proposed, utilizing a coefficient function determined by adaptively estimated parameters and sample difficulty angles. The proposed adaptive curriculum learning loss for face recognition prioritizes easy samples before hard ones, utilizing a novel modulation coefficient function to connect positive and negative cosine similarities. Extensive experiments show the superiority of CurricularFace over existing methods in facial benchmarks. Margin-based losses have been proposed for open-set face recognition to improve discriminative performance compared to softmax loss. However, these losses do not consider the difficulty of each sample. In contrast, CurricularFace prioritizes easy samples before hard ones, which is more effective. Mining-based loss functions like Focal loss and Online Hard Sample Mining are common in object detection but not widely used in face recognition. These methods focus on hard samples and may discard easy ones, unlike CurricularFace. The recent work SV-Arc-Softmax introduces a soft mining variant for deep face recognition, emphasizing hard samples by adjusting weights. In contrast, our method does not always prioritize hard samples, assigns weights based on difficulty, and adaptively estimates hyper-parameters. Curriculum Learning involves training from easier to harder samples, a strategy not requiring manual tuning of additional hyper-parameters. Learning from easier samples first and harder samples later is a common strategy in Curriculum Learning. The key problem is defining the difficulty of each sample. Self-Paced Learning (SPL) considers examples with lower losses as easier and emphasizes them during training. Our method, CurricularFace, focuses on easier samples in the early training stage and emphasizes hard samples later. It introduces a novel modulation function for negative cosine similarities to achieve adaptive assignment. CurricularFace introduces a modulation function for negative cosine similarities to achieve adaptive assignment in different training stages. The original softmax loss is modified to enhance discriminative features for open-set face recognition problems. Several variants are proposed to improve the original softmax loss. The proposed modulation function for negative cosine similarities in CurricularFace aims to adaptively assign values in various training stages, enhancing discriminative features for open-set face recognition. Several variants have been introduced to improve the original softmax loss. The proposed adaptive curriculum learning loss in CurricularFace adjusts negative cosine similarities based on sample difficulty, focusing on hard samples by increasing their negative cosine similarity. The parameter t controls this adjustment, but a large value may cause convergence issues. This approach enhances discriminative features for open-set face recognition, representing the first attempt to introduce adaptive curriculum learning in deep face recognition. The positive and negative cosine similarity functions are defined in the general form, with the modulation coefficient of hard sample negative cosine similarity depending on the values of t and \u03b8 j. Easy samples are emphasized in the early training stage, while hard samples are focused on as training progresses. This adaptive curriculum learning approach aims to enhance discriminative features for open-set face recognition. In CurricularFace, weights of hard samples are emphasized by enlarging them. The parameter t is automatically estimated to indicate model training progress. Exponential Moving Average is used to stabilize estimation in the presence of noisy statistics from extreme data in mini-batches. In CurricularFace, the parameter t increases with model training, emphasizing hard samples gradually. Exponential Moving Average is used to stabilize estimation in the presence of noisy statistics from extreme data in mini-batches. In CurricularFace, the parameter t increases with model training to emphasize hard samples gradually. The loss function is formulated with adaptive negative cosine similarities. The training process involves updating parameters and computing back-propagation errors. Illustrations show the adaptive parameter t and gradient modulation coefficients. The loss transitions from ArcFace to CurricularFace during training. During training, CurricularFace emphasizes hard samples gradually by increasing the ratio with cos\u03b8 j. The positive cosine similarity of a perceptual-well image is often large, but negative cosine similarities may also be significant in early stages. CurricularFace can be optimized with stochastic gradient descent using deep features and logit f j for classification. During training, CurricularFace focuses on hard samples by adjusting the cosine similarity with cos\u03b8 j. It can be optimized using deep features and logit f j for classification. In the backward propagation process, the gradient of x i and W j can be divided into three cases. The gradient magnitude of hard samples is determined by the negative cosine similarity N (\u00b7) and the value of t. Comparing CurricularFace with ArcFace and SV-Arc-Softmax, the differences are discussed. CurricularFace focuses on hard samples by adjusting cosine similarity during training. ArcFace introduces a margin function for positive cosine similarity, while SV-Arc-Softmax adds margin for negative cosine similarity. CurricularFace adapts weights of hard samples in different training stages, changing decision boundaries accordingly. CurricularFace focuses on adjusting cosine similarity for hard samples during training, with weights adaptively determined in different stages. Comparison with Focal loss shows a clearer definition of hard samples in CurricularFace, focusing on mis-classified samples. Datasets used include CASIA-WebFace and refined MS1MV2 for fair comparisons on popular benchmarks like LFW and CFP-FP. The study focuses on adjusting cosine similarity for hard samples during training, with weights adaptively determined in different stages. They use datasets like CASIA-WebFace and refined MS1MV2 for fair comparisons on popular benchmarks such as LFW and CFP-FP. The models are trained using ResNet50 and ResNet100 on NVIDIA Tesla P40 GPUs with specific training settings and learning rate schedules. The study focuses on adjusting cosine similarity for hard samples during training, with weights adaptively determined in different stages. They use datasets like CASIA-WebFace and refined MS1MV2 for fair comparisons on popular benchmarks such as LFW and CFP-FP. The models are trained using ResNet50 and ResNet100 on NVIDIA Tesla P40 GPUs with specific training settings and learning rate schedules. The training process involves epochs at 10, 18, 22, and finishing at 24 epochs with a scale of 64 and margin of 0.5. Modifying the loss function does not introduce additional time complexity for inference. The study investigates the effect of adaptive estimation of parameter t, showing that learning from easier samples first and hard samples later is more effective. The study explores different statistics for estimating t in the loss function, comparing the mean of positive cosine similarities with the mode. It also discusses the accuracy of positive cosine similarity versus predicted ground truth probability in indicating training stages. The study highlights issues with ArcFace when using small backbones like MobileFaceNet, necessitating the incorporation of softmax loss for pre-training. Using MobileFaceNet on CASIA-WebFace, the study demonstrates the robustness of their loss function in addressing convergence issues, achieving high accuracy on LFW compared to ArcFace. Our loss achieves 99.25% accuracy on LFW, outperforming ArcFace which does not converge. Setting the margin m to 0.45 allows both losses to converge, with our loss performing better at 99.20% vs. 99.10%. Our loss converges faster in the beginning due to reduced losses of hard samples, but slightly larger than ArcFace later on as we emphasize hard samples. Learning from easy samples first and hard samples later benefits model convergence. Results on various benchmarks show the effectiveness of our CurricularFace model trained on dataset MS1MV2 with ResNet100. Our CurricularFace model achieves high performance on various face verification benchmarks, including LFW, CFP-FP, CPLFW, AgeDB, and CALFW. It outperforms competitors on LFW and shows superiority on CFP-FP and CPLFW compared to general and cross-pose methods. SV-Arc-Softmax performs better than ArcFace but still worse than CurricularFace. Additionally, CurricularFace achieves the best performance on AgeDB and CALFW. In the IJB-B dataset, CurricularFace shows promising results in 1:1 verification tasks. In the IJB-B and IJB-C datasets, our method achieves the best performance in 1:1 verification tasks. We use MS1MV2 and ResNet100 for fair comparison with recent methods. Results on MegaFace Challenge are also evaluated. Results on MegaFace Challenge show that our method outperforms strong competitors like CosFace, ArcFace, Adacos, P2SGrad, and PFE in both identification and verification tasks under different protocols. Our novel Adaptive Curriculum Learning Loss contributes to this success. The paper proposes Adaptive Curriculum Learning Loss for deep face recognition, focusing on easy samples early and hard ones later. It is easy to implement, robust, and outperforms competitors in facial benchmarks. Future research can explore better functions for negative cosine similarity and the impact of noise samples."
}