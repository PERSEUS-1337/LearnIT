{
    "title": "BJzvEqrj2V",
    "content": "Federated learning involves jointly learning over massively distributed partitions of data generated on remote devices. q-Fair Federated Learning (q-FFL) is a novel optimization objective that encourages a fair accuracy distribution across devices in federated networks. q-FedAvg is a scalable method to solve q-FFL in federated networks, validated through simulations on federated datasets. This approach is particularly relevant with the increasing use of IoT devices for data collection and processing outside of data centers. Federated learning aims to train statistical models on distributed devices like mobile phones. The challenge lies in achieving fair model performance across devices due to heterogeneous data. Researchers propose q-Fair Federated Learning (q-FFL) to address this issue, with q-FedAvg as a scalable method for optimization. This is crucial for the growing use of IoT devices in data processing outside of centralized centers. In this work, q-FFL is proposed as a novel optimization objective to address fairness issues in federated learning by minimizing an aggregate reweighted loss parameterized by q. q-FFL minimizes an aggregate reweighted loss parameterized by q to encourage less variance in accuracy distribution. A lightweight and scalable distributed method, qFedAvg, efficiently solves q-FFL, considering communication-efficiency and low device participation. Empirical results show q-FFL reduces accuracy variance by 45% while maintaining overall average accuracy, demonstrating fairness, efficiency, and flexibility compared to existing baselines in Machine Learning. Our work enforces fairness during training by optimizing accuracy distribution across devices in federated learning. This approach differs from protecting a specific attribute. Previous works have optimized objectives under fairness constraints, such as 'minimum accuracy,' but these methods may not be practical in federated settings. Additionally, a minimax optimization scheme called Agnostic Federated Learning (AFL) has been proposed to optimize for the performance of the single worst device. Fair resource allocation has been extensively studied in fields such as network management and wireless communications. The problem involves allocating a scarce shared resource among many users, where directly maximizing utilities can lead to unfair allocations. Various measurements, including \u03b1-fairness, have been proposed to balance fairness and total throughput. In the context of fair resource allocation, \u03b1-fairness is a unified framework that allows for tuning fairness through a single parameter, \u03b1. Inspired by this metric, a modified objective function called q-Fair Federated Learning (q-FFL) is proposed to promote fair accuracy distribution in federated training. This approach aims to address challenges in federated learning such as expensive communication and variability in hardware and network connections. Incorporating recent advancements in federated learning, methods are designed to address challenges like expensive communication, hardware variability, and heterogeneous data distribution among devices. The q-FFL objective promotes fair accuracy distribution across all devices, with q-FedAvg being an efficient solution. In Section 3.3, q-FedAvg is described as an efficient distributed method for federated learning, aiming to minimize the global objective by fitting a model on data from numerous remote devices. The method involves defining local objectives based on empirical risks over local data samples, with prior work using subsampling and local optimizers like Stochastic Gradient Descent for updates. FedAvg is a leading method for efficient communication in federated learning, but it can introduce unfairness among devices by biasing the model towards those with more data points. Fairness criteria for federated learning are defined to address this issue. In federated learning, fairness criteria are important to address bias introduced by methods like FedAvg. To reduce variance in device performance, a reweighting objective q-FFL is proposed, inspired by \u03b1-fairness in wireless networks. The proposed objective q-FFL aims to introduce fairness in federated learning by emphasizing devices with higher losses, reducing variance in accuracy distribution. Two solvers, q-FedSGD and q-FedAvg, are introduced to achieve this objective efficiently. In optimizing q-FFL, the key is to set q appropriately for fairness. Gradient-based methods are used with step-size tuning based on the Lipchitz constant, which changes with different q values. To address the issue of the Lipchitz constant changing with different q values in optimizing q-FFL, the proposal suggests estimating the local Lipchitz constant of the gradient for the family of q-FFL by using the inferred constant at q = 0. This approach allows for dynamic adjustment of the step-size for the q-FFL objective, eliminating the need for manual tuning for each q value. Additionally, Lemma 2 formalizes the relationship between the Lipschitz constant for q = 0 and q > 0, providing an upper-bound for the local Lipchitz constant of the gradient. The proposed method involves using mini-batch SGD on a subset of devices to estimate local Lipchitz constants for gradients of local functions. These estimates are averaged to obtain a Lipchitz constant for the gradient of q-FFL. The step-size is then adjusted based on this estimate. The q-FedAvg algorithm selects a subset of devices at random, sends parameters to them, and updates them using SGD. When q=0, q-FFL reduces to the normal federated learning objective. The experimental setup explores convex and nonconvex models on federated datasets. Results on the Vehicle dataset show improved fairness with q-FFL compared to FedAvg and other baselines. In comparing q-FFL with FedAvg and other baselines, the study shows that q=5 results in more fair testing accuracy distributions with lower variance. This objective maintains average testing accuracy while reducing variance significantly across various datasets. Additionally, q-FFL outperforms a heuristic that samples devices uniformly in terms of testing accuracy distribution. The statistics of accuracy distribution on all datasets are provided in Table 3 in the appendix. Our method produces fair solutions in testing accuracies compared to the 'weighing each device equally' heuristic. q-FFL has better generalization properties due to its dynamic nature, leading to faster convergence than q-FedSGD. Setting q > 0 in q-FFL decreases variance in accuracy distribution and improves the worst 10% accuracy. The only work addressing fairness in federated learning is q-FFL, outperforming AFL when q is appropriately set. q-FFL converges faster and allows tuning q for a tradeoff between accuracy variance and average accuracy. Running Algorithm 1 with multiple q's in parallel can provide multiple final global models. Running Algorithm 1 with multiple q's in parallel can provide multiple final global models, allowing each device to select the best model based on performance on validation data. The efficiency of q-FedAvg is demonstrated by comparing it with q-FedSGD, showing that q-FedAvg runs local updates on selected devices while q-FedSGD uses all local training data for gradient descent. The proposed method q-FedSGD involves the server choosing a subset of devices at random, sending the current model to them, and aggregating the updates received from each device. The method aims to improve fairness and is compared with uniform sampling schemes in terms of testing accuracy. The proposed method q-FedSGD involves the server choosing a subset of devices at random, sending the current model to them, and aggregating the updates received from each device to improve fairness in testing accuracy distributions. q-FFL with q > 0 leads to fairer test accuracy distributions by shifting towards the center, producing more fair solutions than uniform sampling. The testing accuracy of the worst 10% devices tends to increase, and the variance of the final testing accuracies is smaller. Our method, q-FedAvg, converges faster in communication rounds compared to q-FedSGD. It produces fairer solutions in testing accuracies by increasing the lowest accuracies. q-FFL outperforms AFL in this aspect and offers flexibility with the parameter q for balancing accuracy improvements. Our method, q-FedAvg, converges faster in communication rounds compared to q-FedSGD, improving testing accuracies by increasing the lowest accuracies. It is lightweight and easily integrated into existing federated learning algorithms like TensorFlow Federated (TFF). The datasets used in our experiments are detailed, including statistics on the number of devices, samples, and data points. The synthetic model involves learning a global W and b with additional heterogeneity. Samples and local models on each device follow a specific covariance matrix. The Vehicle 1 model uses a dataset with sensors as devices, training a linear SVM. The Sent140 dataset consists of tweets. The Sent140 dataset contains tweets for text sentiment analysis, modeled as a binary classification problem. A non-stochastic version of AFL is implemented with a grid search on hyperparameters. Data is split into training, testing, and validation sets on local devices. The study implemented Fair Resource Allocation in Federated Learning with different q values on validation sets to report accuracy distributions on testing sets. Results showed similar accuracies for q=0 and q>0 objectives. Uniform sampling generally had higher training accuracies but was prone to overfitting. Optimal q values for specific datasets were identified through the process. In Federated Learning, optimal q values for different datasets are identified, with specific learning rates and tuning parameters used. The study also explores the impact of q-FFL on accuracy distributions across devices, showing that it leads to fairer accuracy outcomes. The study demonstrates that q-FFL results in fairer accuracy distributions across devices while maintaining similar testing accuracies. Additionally, q-FFL shows faster convergence compared to AFL, especially for the single worst device. The performance gap between AFL and q-FFL widens as the number of devices increases. In Federated Learning, q-FFL shows comparable convergence speed to q = 0 for selected q values, leading to fairer results. q-FFL is solved with q values ranging from 0 to 10 in parallel, reducing accuracy variance and increasing average accuracy. This approach increases local computation and communication load without affecting the number of communication rounds. Our solver q-FedAvg converges faster than q-FedSGD. We investigate if q-FFL objective slows convergence compared to FedAvg. Empirical results show that choosing q values for fair solutions does not significantly affect convergence."
}