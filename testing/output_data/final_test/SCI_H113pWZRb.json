{
    "title": "H113pWZRb",
    "content": "The paper introduces the topology adaptive graph convolutional network (TAGCN), a novel approach for applying convolutional neural networks to graph-structured data. TAGCN uses adaptive filters that adjust to the topology of the graph, enabling convolution on irregular graphs like social networks or knowledge graphs. The outputs of TAGCN are a weighted sum of filter outputs, capturing both vertex features and correlations between vertices. The proposed TAGCN can be used with directed and undirected graphs, inheriting properties of convolutions in CNN for grid-structured data and being consistent with convolution in graph signal processing. TAGCN outperforms existing graph-convolution-approximation methods without needing any approximation to the convolution. TAGCN is a computationally simpler method that utilizes polynomials of degree two of the adjacency matrix. Graph CNNs have gained interest in generalizing deep learning to graph-structured data, focusing on designing graph CNN for feature extraction on arbitrary graphs. In this paper, the topology adaptive graph convolutional network (TAGCN) is proposed as a unified convolutional neural network for learning nonlinear representations of graph-structured data. TAGCN applies fixed-size learnable filters on the graph to extract vertex features and correlation strength between vertices, adapting to the local topology. It unifies filtering in both the spectrum and vertex domains, suitable for directed and undirected graphs. Existing graph CNNs can be categorized into spectral domain and vertex domain techniques, with convolution achieved through a pointwise product in the spectrum domain. The paper proposes a modification to the graph convolution step in CNNs for learning representations of graph-structured data. Previous methods utilized spectrum-based filters like Chebyshev and Cayley polynomials, but were limited to undirected graphs due to symmetric adjacency matrix assumptions. A simplified spectrum method achieved state-of-the-art performance by obtaining a filter in the vertex domain. Other researchers focused on feature propagation models and transforming graph data to embedding vectors. The challenge remains in extending CNNs from grid-structured to arbitrary graph-structured data with local feature extraction capability. The paper proposes a modification to the graph convolution step in CNNs for graph-structured data, introducing TAGCN which utilizes graph signal processing techniques. It defines graph convolution as multiplication by polynomials of the adjacency matrix, providing a theoretical foundation for convolution in CNNs for graph data. This approach avoids computing the graph Laplacian spectrum, offering a solid alternative to existing methods. The proposed method, TAGCN, utilizes graph signal processing techniques to modify the graph convolution step in CNNs for graph-structured data. It avoids computing the spectrum of the graph Laplacian and uses polynomials of the adjacency matrix with maximum degree 2, resulting in lower computational complexity compared to existing methods. The method outperforms other approaches and offers a theoretical foundation for convolution in CNNs for graph data. The proposed method, TAGCN, introduces a general K-localized filter for graph convolution in the vertex domain, offering better performance than existing methods. It is based on graph signal processing, applies to directed and undirected graphs, and has lower computational complexity. TAGCN is a method that utilizes polynomials of the adjacency matrix with maximum degree 2, outperforming existing graph CNN methods in vertex classification tasks. It does not require convolution approximation and consistently achieves better performance on commonly used datasets. The weighted adjacency matrix of a graph can be used for graph convolution in tasks like vertex semisupervised learning, where limited labeled vertices are classified. The method utilizes polynomials of the adjacency matrix with maximum degree 2 and outperforms existing graph CNN methods in vertex classification tasks. The curr_chunk discusses the design of a meaningful convolution on a graph with arbitrary topology, focusing on graph shifts and filters to process graph-structured data. It emphasizes the use of a normalized adjacency matrix for computational stability. The curr_chunk discusses the adoption of different filters in graph convolution, following the CNN architecture with ReLU activation. It also explores the convolution operator's properties and its role as a feature extraction operator in traditional CNNs. The curr_chunk discusses designing a graph CNN architecture similar to GoogLeNet BID21 by using a combination of size 1 and size 2 filters for best performance in feature extraction. In traditional CNN, filters of different sizes are used for image classification tasks to achieve better performance. In designing a graph CNN architecture similar to GoogLeNet BID21, a general K-localized filter is proposed for graph CNN. The convolution operation G DISPLAYFORM0 k is equivalent to using filters with sizes from 1 up to K for local feature extraction on the graph. Each k-size filter is k-localized in the vertex domain, with paths of length m defined on the graph. The adjacency matrix A represents a directed graph with different length paths. The weight of a path is the product of edge weights. The entry \u03c9(p k j,i) in A k is the sum of weights of length-k paths. The component c is the weighted sum of input features of each vertex. The output feature map in graph convolutional layers is a weighted sum of convolution results from filters with different sizes. Each neuron is connected to a local region in the vertex domain, adaptive to the graph topology. The strength of correlation is explicitly utilized in the operation. The topology adaptive graph convolutional network (TAGCN) utilizes filters to cover paths of different lengths between vertices, contributing to the convolution operation. The convolution operator can be seen as a filter in the spectrum domain, adapting to the graph's topology. TAGCN is equivalent to performing graph convolution on either the vertex domain or the spectrum domain. Implementing TAGCN in the vertex domain is preferred due to computational efficiency. Filtering directly in the vertex domain can be computed in O(M N) by performing a breadth-first search, while filtering in the spectrum domain may require additional matrix and vector multiplications. Performing graph convolution on a matrix with N vertices and M edges can be computed efficiently in O(M N) by using breadth-first search. For sparse graphs, where M is often less than N, a small filter size K=2 is optimal for detecting local features. Using polynomial filters in the vertex domain instead of the spectrum domain saves computational costs. The proposed TAGCN shows connections and differences with existing methods, with two types of graph convolution operators for CNN architecture. In the spectrum domain, convolution is defined by the multiplication of the inverse Fourier transform matrix with filtered results. Different approaches include using a feature propagation model in the vertex domain and approximating convolution through eigendecomposition of the normalized Laplacian matrix. The BID3 method approximates convolution in the spectrum domain using a kth order matrix Chebyshev polynomial. Referred to as ChebNet for comparison, this method is limited to undirected graphs due to the symmetric Laplacian matrix assumption. While BID3 requires Laplacian matrix polynomials up to order K = 25, TAGCN achieves better performance with adjacency matrix polynomials up to order 2. Additionally, BID10 obtains a graph convolutional network (GCN) through a first-order approximation. The simplified convolution operator with eigenvalues in [0, 2] is obtained by setting K = 1, \u03bb max = 2, and \u03b8 0 = \u03b8 1 = \u03b8. A renormalization trick is applied by letting A = A + I and D i,i = j A i,j. The convolutional operator is approximated, showing better performance than the spectrum method BID3. This method propagates vertex features on the graph, similar to TAGCN, but with differences. TAGCN leverages information at a farther distance and is not a simple extension of GCN Kipf & Welling BID20. Our graph convolution method defines multiplication by polynomials of the graph adjacency matrix, filtering with graph filters as in graph signal processing. It differs from GCN in Kipf & Welling BID20 by using 2nd order Chebyshev polynomials of Laplacian matrix. Our method of graph convolution differs from GCN by using 2nd order Chebyshev polynomials of the Laplacian matrix. It is a principled approach that captures the underlying graph structure without approximation in the convolution operation. Simonovsky & Komodakis proposed the edge convolution network (ECC) to extend the convolution operator to arbitrary graphs. The convolution operator in graph networks varies between models like GCN, ECC, and MoNet, with different weightings and propagation models. DCNN introduces a diffusion process over the graph using transition probabilities from the normalized adjacency matrix. Comparing these methods with TAGCN shows differences in their approaches. TAGCN can be considered a special case of GCN, ECC, and MoNet, as it includes information propagation terms that these methods do not utilize. TAGCN offers a systematic way to design adaptive filters for convolution on graphs, avoiding costly eigendecompositions. It also satisfies the convolution theorem and requires fewer weights to be learned in each hidden layer compared to other methods. In experiments, K = 2 is selected using cross validation. ChebNet in BID3 suggests a 25th degree Chebyshev polynomial for a good approximation to the graph Laplacian spectrum. The proposed TAGCN achieves the best performance on commonly used graph-structured datasets for vertex semisupervised learning. TAGCN is evaluated on Cora, Citesser, and Pubmed datasets. The TAGCN model is evaluated on three citation network datasets: Citeseer, Cora, and Pubmed. Each dataset contains labeled documents and citation links represented as graph vertices and edges. The goal is to classify unlabeled documents using limited labels. Detailed statistics of the datasets are provided in TAB0, showing the number of nodes, edges, and document classes in each dataset. The TAGCN model is applied to three citation network datasets: Citeseer, Cora, and Pubmed. A graph is constructed for each dataset with nodes representing documents and edges linking papers with citation relationships. The adjacency matrix is obtained and normalized. A TAGCN with two hidden layers is designed for semi-supervised node classification, using 16 hidden units and dropout after each layer. The softmax activation function is applied to the output for final classification. The TAGCN model is applied to three citation network datasets: Citeseer, Cora, and Pubmed, with a graph constructed for each dataset. The softmax activation function is applied to the output of the second hidden layer for final classification. Ablation study evaluates TAGCN performance with different kernel sizes. TAGCN with 8 filters is compared to 16 filters for classification accuracy. Model training uses Adam BID9 with a learning rate of 0.01 and early stopping. Hyperparameters are selected by cross-validation, and the same split of training, validation, and testing sets is followed for fair comparison. The proposed TAGCN method is evaluated for classification accuracy using cross-entropy error. Results are compared with other graph CNN methods and a graph embedding method called Planetoid. The experiments follow standard criteria, and the performance of different filter sizes is analyzed. Results show that filter size K = 2 consistently outperforms other sizes. The performances for filter size K = 2 are consistently better than other sizes, validating the importance of local feature extraction. Comparing different numbers of filters, it is shown that using 8 filters achieves comparable accuracy to using 16 filters in each hidden layer for TAGCN. Even with a similar number of parameters, TAGCN outperforms GCN, MoNet, ECC, and DCNN methods. Our method, TAGCN, outperforms A 2 in classification accuracies for Pubmed and Cora datasets. TAGCN is a novel graph convolutional network that adapts to graph topology and rearchitects the CNN architecture for graph-structured data. It achieves superior performance compared to GCN, MoNet, ECC, and DCNN methods, even with a similar number of parameters. TAGCN inherits properties of convolutional layer in classical CNN for local feature extraction and weight sharing. It can extract correlation strength between vertices in filtering region and unifies graph CNN in spectrum and vertex domains. This leads to improved classification accuracy on graph-structured datasets with low computational complexity. The graph filtering in the vertex domain satisfies the convolution theorem and interprets spectrum filtering for directed and undirected graphs. Matrix F defines the graph Fourier transform, and the input feature spectrum map is a linear mapping from the vertex domain to the spectrum domain. The spectrum of the graph filter is represented by a polynomial, and convolution on the vertex domain becomes multiplication in the spectrum domain for graph-structured data. In the 1D cyclic form, the graph's adjacency matrix corresponds to the discrete Fourier transform matrix. The convolution operator in classical signal processing is consistent with the defined operator."
}