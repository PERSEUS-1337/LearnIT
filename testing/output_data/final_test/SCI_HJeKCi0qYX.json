{
    "title": "HJeKCi0qYX",
    "content": "Recently, there has been a surge of interest in designing graph embedding methods that can scale to large graphs with millions of nodes. The MultI-Level Embedding (MILE) framework introduces a methodology to allow contemporary graph embedding methods to scale by repeatedly coarsening the graph into smaller ones and applying existing embedding methods. MILE is agnostic to the underlying graph embedding techniques and can be applied to many existing methods without modification. The MultI-Level Embedding (MILE) framework significantly boosts the speed of graph embedding while generating high-quality embeddings for node classification on large-scale datasets. Existing methods struggle with graphs containing millions of nodes due to computational and memory constraints. MILE can handle graphs with 9 million nodes and 40 million edges efficiently, where other methods fail on modern workstations. The existing embedding methods struggle with large-scale graph datasets due to memory constraints. The goal is to scale up graph embedding techniques in a generic way to handle millions or billions of nodes efficiently. The focus is on improving the quality of embeddings and whether they can be directly applied to larger datasets. The MultI-Level Embedding (MILE) framework aims to enhance the quality of graph embeddings by incorporating a holistic view of the graph structure. It involves a three-step process of coarsening the graph, computing embeddings, and refining them using a graph convolution network. MILE is designed to be agnostic to the underlying graph embedding methods, making it generalizable to different datasets. The MILE framework is scalable and improves the quality of embeddings by leveraging various graph embedding techniques. It can enhance scalability by up to 30-fold, reduce running time and memory consumption, and generate high-quality embeddings. Various methods like DeepWalk, Node2Vec, LINE, and SDNE have been proposed for graph embedding, each with its unique approach. The curr_chunk discusses the lack of scalability in current network embedding methods and introduces a new approach that aims to address this issue. It mentions the limitations of existing efforts and highlights the potential for even greater speedup by combining different scalability strategies. The closest related work is HARP, which focuses on a hierarchical paradigm for graph embedding. The curr_chunk introduces a general-purpose framework for scaling up graph embedding methods, treating them as black boxes. It aims to address the lack of scalability in current network embedding methods by designing a new approach that can be extended to various graph embedding techniques. The curr_chunk introduces the extension of BID4, BID6, and BID19 to a directed graph, defining graph embedding as learning vector representations for nodes to preserve graph properties. The MILE framework aims to speed up graph embedding methods without compromising quality, consisting of three key phases. The MILE framework aims to accelerate graph embedding methods while maintaining quality through three key phases: graph coarsening, base embedding, and embeddings refining. The coarsening process involves collapsing nodes into super-nodes using a hybrid matching technique, preserving global structure. An example is illustrated in FIG3. In the coarsening process of the MILE framework, matrix A1 of the graph is obtained using Structural Equivalence Matching (SEM). This method identifies structurally equivalent nodes in the graph, such as nodes D and E in FIG3. Heavy edge matching is used for graph coarsening, where edge weights are normalized based on the degree of the connected vertices to penalize edges connected to high-degree nodes. In the coarsening process of the MILE framework, the adjacency matrix A1 is obtained using Structural Equivalence Matching (SEM) to identify structurally equivalent nodes. A hybrid matching method is used for graph coarsening, combining SEM and normalized heavy edge matching (NHEM) to construct G i+1 from G i. The matching matrix M i,i+1 stores the matching information between G i and G i+1, with nodes collapsed into super-nodes in G i+1. The adjacency matrix M i,i+1 represents node matching in the coarsening process of the MILE framework, reducing graph size significantly. Graph embeddings are obtained using a chosen method on the final coarsened graph G m, followed by a refinement phase using the matching matrices M m-1,m and node embeddings E m. The approach aims to derive node embeddings of G0 from Gm by iteratively inferring embeddings of consecutive graphs using a graph-based neural network model. The matching information between graphs allows for easy projection of node embeddings, refining them from coarse-grained to fine-grained graphs. The proposed approach involves using a graph convolution network for embedding refinement in a graph-based neural network model. This model derives node embeddings on graph Gi based on projected embeddings and the graph adjacency matrix Ai. The neural network model for embedding refinement involves a graph convolution network with layer-specific weight matrices. The model integrates structural information from the graph into the projected embeddings through spectral graph convolution. The weight matrix \u0398 (k) is learned for each layer of the refinement model. The refinement model involves learning \u0398 (k) for each layer according to Eq. 4. The loss function is constructed by comparing predicted embeddings with ground-truth embeddings. The proposed method is to learn \u0398 (k) on the coarsest graph and reuse them across all levels for refinement. The loss function is defined as mean square error. This learning task is referred to as double-base embedding learning, but has two key drawbacks. The refinement model involves learning \u0398 (k) for each layer by comparing predicted embeddings with ground-truth embeddings. However, the process of adding an extra base embedding introduces overheads and may not provide a desirable \"ground truth\" for refined embeddings. The embedding spaces of different graph levels can be totally different, even if base embeddings are learned independently. One solution is to use an alignment procedure to force the embeddings to be aligned. In this paper, a simple method is proposed to address the issues of graph coarsening and alignment in embedding spaces. By constructing a dummy coarsened graph, the need for an additional level of coarsening is eliminated, reducing computational costs. The embeddings of the graphs are guaranteed to be in the same space without drift, allowing for efficient model learning using gradient descent with back-propagation. The same set of parameters is applied in subsequent refinement steps to infer refined embeddings. The MILE framework efficiently refines graph embeddings using a hybrid matching method. The algorithm involves coarsening the input graph, performing base embedding on the coarsest graph, learning weights, computing projected and refined embeddings, and inferring refined embeddings using sparse matrix multiplications. The process is cost-effective and ensures embeddings are in the same space without drift. The MILE framework refines graph embeddings using a hybrid matching method by computing projected and refined embeddings on different graph levels. Various popular graph embedding methods are explored to evaluate the quality of the embeddings through multi-label node classification. Performance of MILE on different datasets with various base embedding methods is summarized in FIG4. The MILE framework enhances graph embeddings by refining them using a hybrid matching method on different graph levels. It significantly speeds up the explored embedding methods, with speedups ranging from 1.5\u00d7 to 14.4\u00d7 while maintaining comparable quality. MILE also improves the quality of embeddings, especially for smaller coarsening levels across various datasets and methods. MILE-enhanced embeddings offer qualitative improvement and speedup over original methods on various datasets. MILE supports multiple embedding strategies and consistently improves quality and efficiency, with significant speedups observed. GraRep shows smaller qualitative improvements with MILE, while Line's time complexity is enhanced. Applying the MILE framework on top of Line improves speed-up due to a larger constant factor. MILE enhances quality and scalability for DeepWalk and Node2Vec, with varying coarsening levels impacting the trade-off between speed and quality. The MILE framework improves the quality of embeddings significantly while reducing the time taken, especially on larger datasets like Flickr and YouTube. The Micro-F1 score increases by 5.3% on YouTube with MILE (DeepWalk) at m=1, consuming only half the time of the original DeepWalk. As the coarsening level m in MILE increases, the running time decreases exponentially while the quality of embeddings only slightly decreases. MILE consolidates existing embedding methods and continues to outperform DeepWalk in terms of quality and speed. Comparing MILE with HARP, MILE shows a nice trade-off between effectiveness and efficiency. MILE generates embeddings of comparable quality with HARP, performing better on PPI and Blog datasets and slightly worse on Flickr and YouTube. However, MILE is significantly faster than HARP on all datasets, with a 31\u00d7 speedup on YouTube due to reduced computational overhead. The MILE framework demonstrates improved efficiency compared to HARP, achieving comparable quality embeddings on various datasets with significantly reduced computational overhead. For instance, MILE outperforms other methods in terms of running time and Micro-F1 score, making it easier to conduct graph embedding on large datasets like Yelp. The MILE framework improves efficiency compared to HARP, achieving quality embeddings with reduced computational overhead. It enhances the scalability of graph embedding algorithms, reducing running times significantly while maintaining high Micro-F1 scores. MILE enhances scalability of graph embedding algorithms by reducing running time and memory consumption, improving node embeddings quality. It learns a refinement strategy based on graph properties and embedding method. Future plans include generalizing MILE for information-rich graphs and expanding its applications. Datasets used in experiments include PPI, Blog, Flickr, and YouTube networks with specific labels indicating biological states and interests. MILE enhances graph embedding algorithms by improving scalability and node embeddings quality. It works with different graph embedding methods like DeepWalk, Node2Vec, and Line. These methods aim to preserve proximities in large-scale graphs. The MILE framework enhances graph embedding algorithms by improving scalability and node embeddings quality. It works with various graph embedding methods such as DeepWalk, Node2Vec, and Line, aiming to preserve proximities in large-scale graphs. Additionally, it incorporates methods like GraRep, NetMF, and graph convolution network models with specific parameter settings for optimal performance. The MILE framework enhances graph embedding algorithms by improving scalability and node embeddings quality. It works with various graph embedding methods such as DeepWalk, Node2Vec, and Line, aiming to preserve proximities in large-scale graphs. The self-loop weight \u03bb is set to 0.05, the number of hidden layers l is 2, and tanh(\u00b7) is used as the activation function. The learning rate is 0.001, and the number of training epochs is 200. The Adam Optimizer is used for model training on a machine running Linux with an Intel Xeon E5-2680 CPU and 128 GB of RAM. The MILE framework is implemented in Python, with code and data available for replicability. TensorFlow package is used for embeddings refinement learning. Available parallelism on 28 cores is leveraged for each method. Evaluation of embeddings quality is done through multi-label node classification. The MILE framework enhances graph embedding algorithms for scalability and node embeddings quality. It utilizes various graph embedding methods and conducts multi-label node classification using learned embeddings. The time complexity estimation considers factors like graph structure and convergence rate. The embedding inference part is sparse. The MILE framework improves graph embedding algorithms by reducing time complexity through embedding learning and refinement at the coarsest graph level. The embedding inference part involves sparse matrix multiplication with time complexity O(k2 * Ei). The overall time complexity is reduced to T(DISPLAYFORM1) from T(V, E), with a small constant factor k. The MILE framework reduces time complexity in graph embedding algorithms by learning and refining embeddings at the coarsest graph level. The constant factor k in the complexity term is typically small, potentially speeding up the embedding process. Many existing embedding strategies require hyperparameter tuning for optimal performance, leading to repeated algorithm runs. Applying MILE can save runtime across multiple runs with different hyper-parameter settings. Performance evaluation details are available in TAB3: Performance of MILE. The MILE framework aims to reduce time complexity in graph embedding algorithms by refining embeddings at the coarsest graph level. It introduces design choices related to coarsening and refinement procedures, such as Random Matching (MILE-rm), to systematically examine performance. The MILE framework introduces different refinement methods for graph embeddings: MILE-proj uses a simple projection, MILE-avg averages neighborhood embeddings, and MILE-untr uses fixed parameters without training. These methods aim to improve performance by refining embeddings at the coarsest graph level. The MILE framework introduces various refinement methods for graph embeddings, such as MILE-proj, MILE-avg, and MILE-untr. These methods aim to enhance performance by refining embeddings at the coarsest graph level. In comparison, MILE-2base replaces the loss function for model training, while MILE-gs replaces the graph convolution network with GraphSAGE for refinement. The MILE framework introduces refinement methods for graph embeddings like MILE-proj, MILE-avg, and MILE-untr to enhance performance. In comparison, MILE-2base replaces the loss function for training, while MILE-gs replaces the graph convolution network with GraphSAGE for refinement. In another study, max-pooling is used for aggregation with 100 sampled neighbors, and concatenation is done instead of replacement during propagation. Results show that MILE outperforms MILE-rm in generating embeddings using DeepWalk or NetMF as base methods, despite MILE-rm being slightly faster. The refinement learning methodology in MILE is effective, with MILE-proj performing worse. MILE-avg and MILE-untr also perform poorly without training the refinement model. Learning the refinement model is crucial for MILE's effectiveness, tailoring it to the specific graph. The overhead cost of learning varies depending on the base embedding used. The refinement learning methodology in MILE is effective, with Graph convolution refinement learning outperforming GraphSAGE. Double-base embedding learning is not effective due to issues with unaligned embeddings. The performance gap between MILE and MILE-2base provides empirical evidence supporting this argument. The base embeddings of level m and level m + 1 may not align in the same space, causing a gap in performance between MILE and MILE-2base. MILE-2base has a longer running time and higher memory consumption compared to MILE. The impact of MILE on reducing memory usage is studied using GraRep and NetMF as base embedding methods. The memory consumption of MILE significantly reduces as the coarsening level increases, with reductions of 64% for GraRep and 42% for NetMF even with one level of coarsening. This reduction continues until it reaches 4 levels, where memory consumption is mainly due to storing the graph and embeddings. This reduction is consistent with the decrease in the objective matrix size with coarsening levels. The paper discusses reusing parameter \u0398 (k) across different levels in the embedding refinement process to balance efficiency and effectiveness in scaling up graph embedding. Sharing parameters helps in reducing memory consumption as the coarsening level increases, leading to a significant decrease in objective matrix size. The authors found that sharing filter parameters \u0398 (k) over the whole graph, as in the original GCN BID13, is efficient and effective for embedding quality. Using random values for \u0398 (k) during refinements resulted in much worse quality. The study focuses on the performance gain of using MILE on top of base embedding methods like DeepWalk, Node2Vec, and LINE, which are different in nature. The methods DeepWalk, Node2vec, LINE, GraRep, and NetMF utilize different approaches for latent feature representation. GraRep and NetMF construct objective matrices based on random walk Laplacians to generate embeddings. NetMF has been shown to approximate DeepWalk, Node2vec, and LINE through implicit matrix factorization. The limitations of matrix factorization methods like NetMF and GraRep in embedding large graphs within the MILE framework are highlighted. Solutions include constructing random-walk Laplacians for directed graphs and symmetrizing the graph to enable the use of various embedding strategies. Within the MILE framework, various embedding strategies can be used, including the coarsening technique. Symmetrization of directed graphs is important for effectiveness, with 5% - 20% of nodes being structurally equivalent. SEM is highly dependent on graph structure, with YouTube and Yelp showing different percentages of SEM nodes during coarsening levels."
}