{
    "title": "r1xOmNmxuN",
    "content": "We study the robustness of GNNs to symmetric label noise by combining neural message-passing models with loss correction methods for graph classification. Experiments show improved test accuracy under noisy settings. Large datasets are crucial for modern machine learning models, but collecting exact labels can be expensive. Crowdsourcing for data creation introduces labeling errors and requires significant human effort. Graph neural networks (GNNs) are susceptible to high generalization error with noisy label data, leading to a widening generalization gap as label noise increases. GNNs learn from graph-structured data, classifying graph vertices or the entire graph based on input graph structure and data. This new class of neural networks allows end-to-end learning from diverse data sources. This paper introduces a noise-correction approach to train graph neural networks with noisy labels. Two state-of-the-art models, Graph Isomorphism Network and GraphSAGE, are tested under artificial label noise and then denoised using label noise estimation and loss correction techniques to create a denoising graph neural network model (D-GNN). The task involves graph classification with noisy labels. A neural network model is trained to predict graph labels, with a noise correction process to handle label corruption. The approach includes learning a graph-level feature vector using techniques like GCN to approximate Fourier transformations of signals on graphs. The approach involves using GraphSAGE and GIN models for graph classification with noisy labels. The models utilize neural message passing and surrogate loss functions to handle label corruption and improve classification accuracy. The backward loss correction procedure is applied to a graph neural network to handle label corruption in a symmetric noise setting. A Markov matrix is used to describe the noisy process, where labels are kept with 0.8 probability and corrupted with 0.2 probability. The graph neural network model utilizes message passing and aggregation functions to process vertex features. The D-GNN model utilizes different noise estimators (Conservative, Anchors, Exact) for training, with a focus on correcting loss in label corruption scenarios. The model is trained using a Markov matrix to handle symmetric noise settings, incorporating message passing and aggregation functions for processing vertex features. The model is trained using a correction matrix for loss correction in label corruption scenarios. It is tested on 9 datasets for graph classification, including bioinformatics and social network datasets. The corruption probability is estimated using the Conservative Estimator, and neural networks are trained with different noise configurations. In label corruption scenarios, the model is trained using a correction matrix for loss correction. The noise probability is estimated using the Conservative Estimator, and neural networks are trained with different noise configurations. The anchor estimation method is used to estimate noise probability, and similarity results are demonstrated in Table 2. In label corruption scenarios, the model is trained using a correction matrix for loss correction. The noise matrix N is assumed to be known exactly in this experiment. The diagonal of the correction matrix C can be adjusted as a hyperparameter under the symmetric noise assumption. The model is compared with GIN and GraphSAGE models with fixed hyperparameters. The noise rate is set at 20% for experiments in TAB2, reporting mean accuracy after 10 fold cross-validation. The study introduces loss correction for Graph Neural Networks to address symmetric graph label noise. Experimental results show that D-GNN-A and D-GNN-E outperform the original model by correctly approximating the correction matrix C. The D-GNN-C model shows potential under higher label noise settings, with improvements in performance. The study introduces loss correction for Graph Neural Networks to address symmetric graph label noise. Empirical results show improvement in noise tolerance when the correction matrix C is correctly estimated, which can be considered a hyperparameter and tuned using clean validation data."
}