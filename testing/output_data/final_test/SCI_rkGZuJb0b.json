{
    "title": "rkGZuJb0b",
    "content": "The paper demonstrates a method for tensorizing neural networks using the Multi-scale Entanglement Renormalization Ansatz (MERA) to replace linear layers. Testing on CIFAR-10 dataset shows that MERA outperforms tensor trains in terms of compression and accuracy, with MERA-layers having significantly fewer parameters and only a slight reduction in accuracy compared to fully connected layers. The curse of dimensionality in machine learning is a bottleneck due to the exponential growth of variables with the number of modes in a dataset. Representations in neural network parameters can be compressed without significant loss in performance. Quantum mechanical systems also face challenges in representing d-dimensional particles, with memory costs scaling as d^n. Richard Feynman suggested quantum computation as a solution, but in the absence of quantum computers, compressed representations of quantum states are used. Compressed representations of quantum states are achieved by factorizing the tensorial description of the quantum wavefunction. The optimal factorization structure depends on the correlations in the quantum system. Entanglement, which characterizes the distribution of correlations and information in a state, has revolutionized quantum mechanics. Tensorial approaches have been successfully applied in solid state physics and quantum chemistry for the past 25 years. Recent efforts aim to combine neural networks in machine learning with tensorial methods in quantum physics. Recent work has shown that entanglement is a useful quantifier for neural network performance. The matrix product state, a simple factorization in quantum systems, has been used to compress neural networks with minimal loss in accuracy. Another tensor factorization, the Multi-scale Entanglement Renormalization Ansatz (MERA), encodes information hierarchically through a process of coarse graining. MERA is a concrete realization of renormalization with a multi-scale structure. It outperforms tensor train decomposition in neural networks for CIFAR-10 classification, providing better accuracy and compression levels. In this report, the linear layers of a standard neural network are replaced with tensorial MERA layers. This involves expressing a linear layer as a tensor by reshaping a matrix W into a higher dimensional array. The MERA factorization is discussed as a replacement for fully connected linear layers in deep learning networks. The main results and connections with existing literature are presented, along with potential developments of the work. The tensor graphical notation represents a rank 2n tensor with elements of size d. Closed legs indicate summed indices, while free legs indicate unsymmetrical indices. This tensor factorization reduces parameters exponentially from d^2n to n(Dd)^2. Tensor trains are effective for capturing correlations in physics up to log D scales. However, they result in highly local representations. A hierarchically structured tensor network, like the tree tensor network, can better represent correlations across linear layers by imbuing a causal structure. The tree tensor network represents correlations across linear layers with a causal structure. The network has a true tree structure and represents a coarse graining of the original MERA. Linear elements are isometries of the network's definition. The network partitions the system at each branching, controlling correlations between neighboring inputs. The MERA BID25 factorization introduces rank 4 tensors called disentanglers to capture longer length scale correlations in the tree tensor network. These disentanglers connect to adjacent tensors in the network, treating correlations on the same length scale similarly. The MERA BID25 factorization utilizes disentanglers, rank-4 tensors, to capture longer length scale correlations in the tree tensor network. These disentanglers connect to adjacent tensors in the network, treating correlations on the same length scale similarly. The construction of a rank-N MERA layer involves creating a tree tensor layer and introducing disentanglers to capture correlations between neighboring input indices. The performance of a neural network was evaluated by replacing the two penultimate fully connected layers with MERA layers, similar to the BID15 study on compression using tensor trains. The comparison included fully connected layers with varying nodes and tensor train layers with varying internal dimensions. The networks consisted of convolutional layers with 3x3 kernels and 64 channels, followed by hidden layers that were either fully connected, MERA layers, or TT-layers. The neural network performance was evaluated by replacing the penultimate fully connected layers with MERA layers or TT-layers. The layers had different sizes and dropout probabilities during training. Batch-normalization was applied, and Gaussian weight initialization was used. Two varieties of fully-connected layers were considered, with one serving as a benchmark for comparison. The second neural network had a 4096 \u00d7 10 fully connected layer followed by a 10 \u00d7 64 layer. It was trained to compare MERA and tensor train layers to a fully connected model with a similar number of parameters. The first MERA layer reshaped the input into a rank-12 tensor and contained a column of 6 rank-4 tree elements, followed by 3 tree elements and a single tree element. The second MERA layer in the neural network has an identical structure to the first layer, with 64 nodes in its output. MERA weights were initialized using elements of randomized orthogonal matrices. The random orthogonal matrix construction method involves Householder transformations. The study involved training a network with tensor train layers as a comparison to MERA layers on the CIFAR-10 dataset. Training data included 50,000 images with augmentation techniques. Test accuracy was monitored every 500 iterations during training. The network was trained using backpropagation and the Adam optimizer, with test accuracy recorded every 500 iterations. Training stopped when test accuracy did not improve for 10 successive tests. The MERA networks showed a significant reduction in parameters compared to the fully connected model, with a modest drop in accuracy. MERA-2 compressed the fully connected layers by a factor of nearly 5500 with a 1.48% accuracy drop. Increasing the internal dimension of the MERA to 3 resulted in a compression rate decrease to approximately 3900, with an accuracy of 86.2%, a drop of less than 1% compared to the fully connected model. The MERA network structure chosen significantly impacted the results, outperforming a fully connected layer with 100 times more parameters. The MERA network also compared favorably to tensor train methods. The tensor train network TT-3 had a comparable compression rate to MERA methods but a more significant drop in accuracy, 2.22%. Increasing the internal dimension to 7 for a tensor train network resulted in an accuracy drop of only 1%, but with an inferior compression rate compared to MERA-2. The time to optimize MERA layers scales with input size N and bond order D as N log 2 D, while for tensor train and fully connected layers, the scaling is N D 2 and N 2, respectively. MERA can accommodate correlations at all scales even at low bond order, whereas tensor trains require a bond order that scales exponentially with the length scale of correlation. The experimental results for different models are compared in Table 1. The fully convolutional layers have significantly more parameters compared to fully connected, MERA, or tensor train layers. An ablated network was created to study the behavior of MERA and tensor train layers without convolutional layers. In an ablated network, a MERA-2 layer outperformed a tensor train layer in terms of accuracy, with 47.36% and 46.65% respectively. The MERA-2 layer had 232 parameters while the tensor train layer used a mixture of TT-2 and TT-3 with 236 parameters. This simplified network aimed to minimize the number of parameters in convolutional layers. Efforts have been made to reduce the number of parameters in neural networks without compromising accuracy. Some models use tensor decompositions like MERA, resulting in superior accuracy. Tree tensor network models struggle to capture long-range correlations effectively compared to MERA due to the lack of disentanglers. MERA works through coarse graining or renormalization. In this report, the fully connected layers of a deep neural network were replaced with MERA layers, resulting in significant efficiency gains with minimal accuracy reduction. The model outperformed compact fully connected layers, showing that MERA layers can be a more efficient alternative. The model with MERA layers achieved high accuracy with less than 1% error, outperforming fully connected layers with significantly more parameters. Additionally, MERA layers showed better performance than tensor trains in terms of accuracy and compression. The factorized layers can handle larger input data sets efficiently and prevent overfitting. However, training networks with factorized layers may take longer due to the number of tensor contractions needed. The results suggest directions for future inquiry, such as improving the existing model by reshaping input into a rank-12 tensor before using the MERA layer. The optimal method for reshaping and initializing MERA layers remains open questions. One possibility is to train a two-dimensional MERA directly on image data without reference to a neural network, similar to previous work with matrix product states on MNIST. An alternative approach to training neural networks involves replacing convolutional layers with a two-dimensional MERA, inspired by the relationship between quantum physics and machine learning. Entanglement measures have been used to optimize network design, and exploring these ideas with MERA could be intriguing. Variational approximations to quantum wavefunctions can also be used to replace linear layers in networks. Quantum computers could replace linear layers in neural networks, as they are described by a type of tensor network. This could be part of a hybrid quantum/classical neural computation scheme."
}