{
    "title": "Syx7A3NFvH",
    "content": "This paper explores multi-agent reinforcement learning (MARL) in networked system control, where each agent learns a decentralized control policy based on local observations and messages from connected neighbors. The networked MARL (NMARL) problem is formulated as a spatiotemporal Markov decision process, with a spatial discount factor introduced to stabilize agent training. A new communication protocol, NeurComm, is proposed to reduce information loss and non-stationarity in NMARL. Experimental results show that the spatial discount factor enhances learning curves, and NeurComm outperforms existing protocols in learning efficiency and control performance. Reinforcement learning (RL) is a data-driven approach for adaptive control policies, with recent advances in deep neural networks (DNNs) improving learning capacity. Algorithms like deep Q-network (DQN), deep deterministic policy gradient (DDPG), and advantage actor critic (A2C) have been successful. However, scalability issues in real-world control problems are addressed through multi-agent RL (MARL), where each agent learns its policy from local observations. MARL introduces challenges in model training and execution due to non-stationarity and partial observability in a decentralized MDP. Various learning methods and communication protocols are proposed to stabilize training and improve performance. This paper discusses networked MARL (NMARL) in the context of networked system control (NSC), where agents cooperate through a communication network for decentralized control. NSC is widely studied and applied in various fields like connected vehicle control, traffic signal control, distributed sensing, and networked storage operation. The increasing trend of NMARL controllers is expected with advancements in communication technologies. Recent works have focused on decentralized NMARL with global observations and local rewards. In networked system control (NSC), practical constraints include distributed control infrastructures, limited global observations, offline model training, and online model monitoring for re-training. Each agent in the networked system is represented as a node in a graph, with communication links between them forming the network structure. In multi-agent MDP, agents have local state and action spaces, follow decentralized policies to choose actions, and aim to maximize long-term global return. The system can also be viewed as a centralized MDP, with a state-value function V\u03c0(s) representing the expected return. MARL offers scalability in this framework. MARL provides a scalable solution for controlling networked systems, introducing partial observability and non-stationarity in decentralized MDP of each agent. Practical constraints enforce local observations and neighborhood communications, making MARL more challenging. The communication in MARL is limited to neighborhoods, with each agent observing its neighbors' information. A spatiotemporal MDP is defined where local transitions are independent given neighboring agents. The Markovian property holds both temporally and spatially, with the next state depending only on neighborhood states and policies. In networked control systems like traffic and wireless networks, agents are connected to a limited number of neighbors. Spatiotemporal MDP is decentralized during model execution, extending properties of MDP. A spatiotemporally discounted return is introduced to scale down reward signals further away. Each agent can perform local greedy control or global coordination based on the value of \u03b1. Each agent is assumed to be A2C with parametric models. Within its closed neighborhood, each agent is assumed to be A2C with parametric models for fitting the optimal policy and value function. Global information is provided through neighborhood communications, allowing for fitting return R. In offline training, global and future information is available, while online execution only allows for local information. Actor and critic updates are based on losses, with a coefficient for entropy loss. Efficient information sharing is proposed for adaptive spatiotemporal RL with A2C. NeurComm is a new communication protocol proposed for efficient information sharing in adaptive spatiotemporal RL with A2C. It involves message encoding and extracting functions, including state, policy, and agent belief in the message. The protocol can be extended for multi-pass communication, with communication attentions and passes denoted by k. NeurComm is a communication protocol for spatiotemporal RL with A2C, utilizing global information to optimize control performance. Agents learn beliefs and messages for multi-step propagation, connected by communication links in a meta-DNN structure. The meta-DNN in NeurComm utilizes decentralized gradient propagation based on local loss signals, expanding parameters spatially through neighborhood communication. Mathematically, the actors' parameters are updated to improve performance, while the policy execution remains fully decentralized. NeurComm is versatile and has connections to other communication protocols, with CommNet performing a more lossy aggregation in message reception. NeurComm utilizes decentralized gradient propagation for parameter updates, while CommNet and DIAL focus on message aggregation in MARL environments. Two NSC environments, ATSC and CACC, are designed for intelligent transportation systems with spatiotemporal MDP assumptions. ATSC aims to minimize traffic congestion by adjusting signal phases based on real-time measurements. In ATSC scenarios, real-time road-traffic measurements are implemented using a synthetic traffic grid and a real-world traffic network from Monaco city. Each episode simulates peak-hour traffic with a 5s control interval to prevent frequent traffic light switches. The total number of approaching vehicles is measured by induction-loop detectors, and the cost for each agent is the sum of queue lengths along incoming lanes. The traffic grid consists of two-lane arterial streets and one-lane avenues with different speed limits. Peak-hour traffic dynamics are simulated with time-variant traffic flows, including major and minor flows with specific origin-destination pairs. After 15 minutes, the flows shift, and all agents in the homogeneous grid have the same action space with five pre-defined signal phases. The Monaco traffic network is illustrated in the scenario. The Monaco traffic network includes controlled intersections in blue and a heterogeneous network with various observation and action spaces. Four traffic flow collections are generated to simulate peak-hour traffic, with different unit flows and O-D pairs. Two CACC scenarios, \"Catch-up\" and \"Slow-down\", aim to minimize car-following headway and speed perturbations through real-time vehicle-to-vehicle communication. In the \"Catch-up\" and \"Slow-down\" CACC scenarios, a string of 8 vehicles is simulated for 60s with a 0.1s control interval. Each vehicle shares headway, velocity, and acceleration with neighbors within two steps. Safety constraints include h \u2265 1m, v \u2264 30m/s, and |a| \u2264 2.5m/s^2. A heuristic optimal velocity model is used for longitudinal vehicle control, with hyper-parameters affecting behavior. NMARL is trained to recommend appropriate parameters for each controller. In the Catch-up and Slow-down CACC scenarios, 8 vehicles are simulated with safety constraints. A penalty is assigned for collisions, and additional costs are included in training. Different scenarios are considered for exploring collision-free CACC strategies using MARL approaches with A2C agents. Independent IA2C performance is evaluated with neighborhood observation and communication allowed. IA2C, ConseNet, and FPrint are non-communicative policies that utilize only neighborhood information, while DIAL, CommNet, and NeurComm are communicative policies requiring higher communication bandwidth. The implementation details of the algorithms include using DNN hidden layers with 64 units, training each model over 1M steps with specific learning rates, and different seeds for generalization. Training takes about 30 hours on a 32GB memory machine. Ablation studies are performed in proposed scenarios, sorted by task difficulty. ATSC Monaco > ATSC Grid > CACC Slow-down > CACC Catch-up. The curr_chunk discusses the comparison of different algorithms based on task difficulty, network heterogeneity, and leading vehicle profile changes. It also mentions the visualization of learning performance through learning curves. The impact of spatial discount factor on learning curves is investigated. The learning curves are compared for different values of \u03b1 for IA2C and CommNet algorithms. The results show that \u03b1 * CommNet is consistently higher than \u03b1 * IA2C. The curr_chunk discusses the sensitivity of learning performance to \u03b1 values in different scenarios. It highlights the impact of \u03b1 on IA2C convergence and the importance of \u03b1 * being high enough for effective policy learning. Additionally, the chunk explores the impact of NeurComm under \u03b1 = 1, comparing it to existing protocols. The chunk evaluates different protocols in MARL scenarios, including \"Concat Only\" and \"FPrint Only\", as well as their combination NeurComm. Results show similar learning curves in easy scenarios, with enhancements in certain scenarios. Tuned \u03b1 values vary for communicative and non-communicative policies, with lower \u03b1 preferred in challenging scenarios. This demonstrates the effectiveness of \u03b1 in enhancing MARL performance. NeurComm enhances MARL performance, especially in challenging tasks like ATSC Monaco. It outperforms other policies in CACC scenarios, showing better sample efficiency and learning stability. In CACC scenarios, the \u03b1-enhanced FPrint policy performs the best, indicating that delayed information sharing may not be helpful in real-time and safety-critical tasks. NeurComm achieves the best execution performance for ATSC tasks, showing sustainable traffic control in ATSC Grid. Top policies in ATSC scenarios are evaluated, with NeurComm standing out for its immediate grid recovery after the loading phase. During the unloading phase, CommNet prevents queue increase while non-communicative policies fail to do so. FPrint causes sudden congestion at 1000s, NeurComm achieves lowest saturation rate in ATSC Monaco. Communicative policies reduce intersection delay in ATSC Grid, but increase it in ATSC Monaco. Communicative algorithms capture traffic patterns in homogeneous networks but may overfit in realistic networks. The study investigates the robustness of top policies in CACC scenarios, comparing communicative and non-communicative controllers. NeurComm and FPrint are identified as the top controllers. MARL controllers achieve steady state for the first vehicle but struggle to eliminate perturbations throughout the platoon. The study introduces a spatiotemporal MDP for decentralized NSC with neighborhood communication. A spatial discount factor is added to improve non-communicative MARL algorithms. NeurComm, a neural communication protocol, is proposed for adaptive MARL algorithms. The paper emphasizes the need for scalable and robust MARL controllers for NSC, suggesting improvements in recurrent units for decentralized control of spatiotemporal information flows within meta-DNNs. The paper by Tong Zhang and Tamer Ba\u015far introduces a spatiotemporal MDP for decentralized NSC with neighborhood communication. It discusses the learning method based on A2C Mnih et al. (2016) and the actor-critic model under global observations. The actor and critic are adapted for restricted observations and communications within neighborhoods, ensuring the best observability. The paper introduces a spatiotemporal MDP for decentralized NSC with neighborhood communication, utilizing the A2C learning method. It adapts the actor-critic model for restricted observations and communication within neighborhoods to improve observability. The paper presents a spatial gradient propagation lemma in NeurComm, where messages optimize other agents' performance. The algorithm for model training is synchronous, updating and sending messages in four iterations. The algorithm for decentralized model execution in an asynchronous way involves measuring traffic, sending and receiving messages, and performing control. It includes various methods like ConseNet, FPrint, NeurComm, DIAL, and CommNet for different updates and computations. In the ATSC environment, NeurComm outperforms other methods in handling complex and heterogeneous action spaces, with various intersections and key metrics summarized in Table 3. NeurComm outperforms baselines in minimizing queue length and intersection delay, with better performance in reducing intersection delay shown in visualizations. CommNet and NeurComm have the best overall performance. Table 4 summarizes key metrics in CACC, with best headway and velocity averages close to h* = 20m and v* = 15m/s. Collision number metric is used to count episodes with collisions. In CACC experiments, collision number metric counts episodes with collisions, although safe RL is not the main focus."
}