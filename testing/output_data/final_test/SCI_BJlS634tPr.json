{
    "title": "BJlS634tPr",
    "content": "Partially-Connected DARTS (PC-DARTS) is a novel approach that reduces redundancy in exploring network space during architecture search. By sampling a small part of the super-net, it achieves a more efficient search without compromising performance. Edge normalization is introduced to address inconsistencies in selecting super-net edges caused by sampling different channels. PC-DARTS reduces memory cost, allowing for training with larger batch sizes and faster performance. Neural architecture search (NAS) is a key aspect of automatic machine learning (AutoML) that has gained attention from academia and industry. PC-DARTS, a method for architecture search, reduces memory cost, enabling training with larger batch sizes for faster speed and higher stability. Results show an error rate of 2.57% on CIFAR10 and a top-1 error rate of 24.2% on ImageNet under the mobile setting. The code for PC-DARTS is available at a provided link. Recent advancements in neural architecture search (NAS) have led to oneshot approaches like DARTS, which reduce search costs significantly. However, DARTS still faces challenges due to a large and redundant space of network architectures, resulting in heavy memory and computation overheads. Efforts have been made to reduce the search space, but this may lead to approximations that impact the search process. In this paper, a new approach named Partially-Connected DARTS (PC-DARTS) is introduced to reduce memory and computation burdens in neural architecture search. By randomly sampling a subset of channels in each step, the approach aims to lower costs and prevent falling into local optima during operation search. However, the method may lead to unstable channel connectivity selection, which is addressed by introducing edge normalization. Edge normalization is introduced in PC-DARTS to stabilize network connectivity search by learning an extra set of edge-selection hyper-parameters. This strategy allows for a larger batch size by reducing memory burden, leading to faster and more stable network search. Experimental results show an error rate of 2.57% in less than 0.1 GPU-days on a single Tesla V100. The PC-DARTS algorithm achieved an error rate of 2.57% in less than 0.1 GPU-days on a single Tesla V100 GPU, surpassing DARTS' result of 2.76% in 1.0 GPU-day. PC-DARTS also set a new state-of-the-art record with a top-1 error of 24.2% on ImageNet in 3.8 GPU-days using eight Tesla V100 GPUs. This advancement in performance is attributed to the rapid development of deep learning and the emergence of neural architecture search (NAS) as a research field. Neural architecture search (NAS) approaches aim to replace handcrafted architectures with automated methods. Existing NAS methods can be categorized into evolution-based, reinforcement-learning-based, and one-shot approaches. Evolution-based methods use genetic algorithms to evolve architectures for better performance, with techniques like hierarchical representation and gene decomposition. Aging evolution has been proposed to enhance tournament selection in NAS. Proposed aging evolution improved upon standard tournament selection in neural architecture search. Reinforcement learning approaches trained a meta-controller to guide the search process, reducing computational costs. Evolution and RL methods still have high computation costs, prompting the search for more efficient approaches. Researchers have developed efficient methods for architecture search, such as one-shot architecture search, which involves training a super-network once to generate multiple sub-networks. This approach reduces the computational costs of evaluating each candidate network. In architecture search, a subset of channels is sampled and connected to the next stage to reduce memory consumption. Extra edge-level parameters are added to minimize uncertainty. Various methods like HyperNet and ENAS have been used to optimize over-parameterized networks. DARTS introduced a differentiable framework for architecture search, combining search and evaluation stages. Despite some drawbacks like instability, improved approaches like ProxylessNAS have been developed. ProxylessNAS and P-DARTS are advanced methods in architecture search that go beyond DARTS. DARTS decomposes the network into cells represented as directed acyclic graphs with nodes defining network layers. The goal is to choose operations from a predefined space to connect nodes, with the core idea being a weighted sum over operations for information propagation. DARTS formulates information propagation as a weighted sum over operations in a cell, with nodes representing network layers. The output of a node is the sum of input flows, and the entire cell output is a concatenation of node outputs. The framework is differentiable for architecture search, preserving operations with the largest weights. However, DARTS is criticized for memory inefficiency. The DARTS framework for architecture search involves preserving operations with the largest weights, but it is criticized for memory inefficiency. To address this issue, a solution is proposed using partial channel connection, which involves defining a channel sampling mask to select specific channels for computation while bypassing others. This approach aims to reduce memory usage and improve efficiency during the search process. The partial channel connection method reduces memory overhead and allows for a larger batch size in architecture search. This leads to reduced computing costs and the ability to sample more training data for increased stability. Varying the proportion of selected channels helps balance accuracy and efficiency in architecture search. The impact of sampling channels on neural architecture search includes positive effects like reducing bias in operation selection by feeding a small subset of channels. This regularization favors weight-free operations over weight-equipped ones in the early search stages. In the early search stages, weight-free operations are preferred by the search algorithm due to their consistency in outputs. Weight-equipped operations may propagate inconsistent information initially, making it challenging for them to outperform weight-free operations even after optimization. This phenomenon is more pronounced when the proxy dataset is difficult, hindering satisfactory architecture search on ImageNet. PC-DARTS, with partial channel connections, shows more stable and superior performance on ImageNet. The architecture parameters in a cell are optimized by randomly sampled channels, leading to unstable connectivity. To address this, edge normalization is introduced, weighing each edge explicitly with \u03b2 i,j. The connectivity of edge (i, j) is determined by \u03b1 o i,j and \u03b2 i,j, with shared \u03b2 i,j values throughout training. The learned network architecture selects edges with large weights as in DARTS. Channel sampling and edge normalization are key contributions of the approach discussed. Channel sampling, a novel technique in NAS, accelerates and regularizes the search process. It is more efficient than other regularization methods like Dropout and DropPath. Edge normalization, on the other hand, improves search stability by weighing each edge explicitly with shared values. The architecture search algorithm benefits from both speed and stability, making them crucial factors in the search process. The approach discussed focuses on channel sampling and edge normalization, which improve search stability and speed in NAS. These components enhance accuracy on ImageNet and reduce search cost. Other methods like ProxylessNAS and PARSEC have also addressed memory consumption in DARTS. Our solution for neural architecture search focuses on preserving all operations, achieving higher accuracy on challenging datasets like ImageNet. Compared to Progressive-DARTS, which eliminates operators for memory efficiency, our approach preserves all operators and performs sub-sampling on the channel dimension. This strategy works well on large-scale datasets like ImageNet. Experiments were conducted on CIFAR10 and ImageNet, with CIFAR10 consisting of 60K images equally distributed over 10 classes and ImageNet containing 1,000 object categories. In neural architecture search, ImageNet dataset contains 1,000 object categories with 1.3M training images and 50K validation images. The input image size is fixed at 224 \u00d7 224 with a limit of 600M multi-add operations in the testing stage. The search stage aims to find the best hyper-parameters for each edge (i, j) by partitioning the training set and optimizing network parameters. The curr_chunk discusses various neural architecture search methods and their performance metrics, including accuracy and computational cost. It also mentions the hardware used for testing and the validation process for selecting the best model. The neural architecture search process involved training on 45K images and validating on 5K images. The best model achieved a test error of 2.57%. The search included 8 choices for operations, and an efficient implementation for partial channel connections was proposed. In the search scenario, an over-parameterized network is constructed with 8 cells, each consisting of 6 nodes. The network is trained for 50 epochs on CIFAR10 data, with channel shuffling for faster computation. The batch size is increased to 256 with 1/4 features sampled on each edge, and network parameters are tuned in the first 15 epochs through a warm-up process. The network weights are optimized using momentum SGD and Adam optimizer. The search process only takes 3 hours on a GTX 1080Ti GPU or 1.5 hours on a Tesla V100 GPU. The network consists of 20 cells with 18 normal cells and 2 reduction cells. The network consists of 20 cells (18 normal cells and 2 reduction cells) with an initial number of 36 channels. PC-DARTS achieved an error rate of 2.57% in just 0.1 GPU-days, surpassing DARTS significantly. ProxylessNAS reported an error rate of 2.08% with a longer architecture search time. P-DARTS slightly outperforms PC-DARTS by searching over a deeper architecture. The network architecture is modified to fit ImageNet, with 8 cells stacked beyond three convolution layers. Two subsets from the ImageNet training set are randomly sampled for training and updating hyper-parameters. A subsampling rate of 1/2 is used to preserve more information. 50 epochs are trained with frozen architecture hyper-parameters for the first 35 epochs. A momentum SGD with an initial learning rate of 0.5 is used for network weights. The network weights are optimized using momentum SGD with an initial learning rate of 0.5, a momentum of 0.9, and weight decay of 3 \u00d7 10 \u22125. The search process utilizes eight Tesla V100 GPUs with a total batch size of 1,024, taking around 11.5 hours. The network architecture includes 14 cells (12 normal cells and 2 reduction cells) trained from scratch for 250 epochs using a batch size of 1,024. The network architecture includes 14 cells trained from scratch for 250 epochs using a batch size of 1,024. Various models such as Inception-v1, MobileNet, ShuffleNet, NASNet, AmoebaNet, PNAS, MnasNet, DARTS, SNAS, ProxylessNAS, and P-DARTS were evaluated with different accuracies and computational costs. The optimization process used momentum SGD with specific parameters and additional enhancements like label smoothing and auxiliary loss tower during training. Results are summarized in Table 2, showing impressive performance on CIFAR10 and ImageNet datasets. The top-1/5 error rates are significantly lower than those reported by DARTS, with the best performance to date on ImageNet. Ablation study on CIFAR10 and ImageNet in Table 3 shows the effectiveness of reducing memory consumption in the network architecture. The study evaluates the impact of different sampling rates on channel selection in network architectures. A sampling rate of 1/4 shows superior performance in terms of both time and accuracy compared to rates of 1/2 and 1/1 on CIFAR10. However, a rate of 1/8, while reducing search time, results in a significant drop in accuracy. This highlights the tradeoff between accuracy and computational efficiency in channel sampling. The experiments show a tradeoff between accuracy and efficiency in architecture search, revealing the redundancy of super-network optimization in NAS. Differentiable NAS approaches can easily over-fit on the super-network, but channel sampling acts as regularization to bridge the gap between search and evaluation. PC-DARTS components like partial channel connections and edge normalization contribute to regularization, even with fully-connected channels. Edge normalization is a cost-effective way to improve performance across various approaches. Edge normalization is a cost-effective method that improves stability in architecture search. Without it, there is low stability in network parameters and accuracy. With edge normalization, testing errors are more consistent, showing a maximum difference of 0.15% in five runs on CIFAR10. This demonstrates the effectiveness of edge normalization in stabilizing architecture search, especially when combined with partial channel connections. The stability of different approaches in architecture search is evaluated through 5 independent search runs. Results show that the proposed approach has lower standard deviations compared to DARTS-v1 and DARTS-v2. The robustness of the search algorithm to hyper-parameters is also studied, with observations on the impact of varying search stage lengths. Additionally, the search is further expanded to explore larger search spaces. PC-DARTS is more robust than DARTS in different evaluation scenarios due to its regularization mechanism, which adjusts to dynamic architectures and avoids large pruning gaps. This is demonstrated through experiments with enlarged search spaces, where PC-DARTS outperforms DARTS-v2 in maintaining accuracy. Using the architecture found by PC-DARTS, it was plugged into an object detection framework called Single-Shot Detectors (SSD) and trained on the MS-COCO dataset. Results showed that with the PC-DARTS backbone, an AP of 28.9% was achieved with only 1.2B FLOPs, outperforming SSD300 and SSD512 in terms of accuracy and computational efficiency. In this paper, a simple and effective approach named PC-DARTS was proposed for object detection. The approach allows for a complete search within 0.1 GPU-days on CIFAR10 or 3.8 GPU-days on ImageNet, achieving significant advantages in AP compared to other models. The core idea involves randomly sampling channels for operation search, making the framework more memory efficient and enabling the use of larger batch sizes for higher stability. Edge normalization is also introduced to enhance search stability without additional computation. This research demonstrates the efficiency of PC-DARTS in achieving state-of-the-art classification accuracy on ImageNet. It highlights the instability of differentiable architecture search compared to conventional neural network training, emphasizing the benefits of regularization and larger batch sizes. The study also uncovers the redundancy of super-network optimization in NAS, showing the importance of regularization in closing the gap between optimization and architecture improvement. These insights can inspire future research in the field. The research highlights the importance of regularization and larger batch sizes in differentiable architecture search. It emphasizes the redundancy of super-network optimization and the need for regularization to close the gap between optimization and architecture improvement. These insights can inspire future research in the field."
}