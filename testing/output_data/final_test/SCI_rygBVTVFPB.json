{
    "title": "rygBVTVFPB",
    "content": "Conservation laws are fundamental laws of nature with broad applications in various fields. Solving differential equations related to conservation laws is a key aspect of computational mathematics. The recent success of deep learning has inspired the exploration of using deep reinforcement learning to solve nonlinear conservation laws. This paper focuses on 1-dimensional scalar conservation laws and utilizes deep reinforcement learning to train a policy network for approximating numerical solutions. The paper focuses on using deep reinforcement learning to train a policy network for approximating numerical solutions of conservation laws in a sequential and spatial-temporal adaptive manner. This approach views solving conservation laws as a sequential decision-making process, ensuring long-term accuracy in numerical schemes. The paper uses deep reinforcement learning to train a policy network for approximating numerical solutions of conservation laws in a sequential and spatial-temporal adaptive manner. The learned policy network determines a good local discrete approximation based on the current state, making it a meta-learning approach. The proposed method uses deep reinforcement learning to train a policy network for approximating numerical solutions of conservation laws. Conservation laws are fundamental laws of nature with broad applications in various fields. The policy network learns to discretize for a given situation, mimicking human experts. The code is available at \\url{https://github.com/qwerlanksdf/L2D}. Incorporating machine learning, especially deep learning, into traditional numerical methods in mathematics has been a recent trend. This integration aims to simplify the usage and potentially enhance the performance of algorithms like the ENO scheme, which traditionally require expert knowledge and complex coding. The recent trend in artificial intelligence is to replace expert systems with 'connectionism' like deep neural networks, leading to the bloom of AI. Introducing deep learning in traditional numerical solvers of conservation laws has been explored, with neural networks applied to solving ODEs/PDEs and inverse problems in various ways. Recent works have explored using neural networks to establish mappings between PDE parameters and solutions, as well as solving high-dimensional PDEs by converting them to stochastic control problems. Another focus is on the connection between deep neural networks and dynamic systems, observing connections between DNNs and dynamic systems like differential equations. The focus of recent works is on using neural networks to solve inverse problems by combining deep learning with traditional tools from applied mathematics. These methods are related to numerical differential equations as learning proper discretizations is crucial for accurately recovering the form of the equations. Other works aim to use NNs for learning new numerical schemes, mainly in the setting of supervised learning. Recent works have focused on integrating neural networks into high-order numerical solvers for predicting artificial viscosity and identifying troubled cells in conservation laws. The authors of these studies trained neural networks separately from the numerical scheme, raising questions about long-term accuracy and the potential for learning new numerical schemes. The paper proposes using reinforcement learning to autonomously design new numerical schemes for conservation laws. They are the first to treat numerical PDE solvers as a Markov Decision Process and use deep reinforcement learning to learn new solvers that prioritize long-term accuracy. The method is carefully designed to generate high accuracy schemes that generalize well in various situations. Using RL to solve conservation laws involves interpreting numerical solvers as a sequential decision-making process, formulated as a Markov Decision Process. RL algorithms optimize the policy to consider long-term accumulated rewards, ensuring the accuracy of learned schemes over time. This approach prioritizes long-term accuracy and can handle various situations effectively. RL can improve traditional numerical schemes by optimizing for long-term accuracy and effective exploration, especially in areas with unclear design principles. It can select better templates near singularities, outperforming existing schemes like WENO. Non-smooth norms like the infinity norm are used to evaluate the performance of learned numerical schemes. The error is used to evaluate the performance of learned numerical schemes. Computing the gradient of the infinity norm can be problematic for supervised learning, but not for RL. Learning the policy within the RL framework allows for meta-learning-like algorithms to decide on numerical approximations based on the current state of the solution. This differs from regular learning where algorithms directly make inferences on numerical schemes without additional networks. The paper discusses the effectiveness of meta-learning-like methods in various applications, such as image restoration. It also highlights the connection between MDP and numerical PDE solvers, proposing the use of RL to improve existing solvers or develop new ones. The hope is to inspire more research in combining RL and computational mathematics. The paper discusses using RL to tackle bottleneck problems in computational mathematics. It reviews conservation laws and WENO schemes, formulates solving conservation laws as a Markov Decision Process, and trains a policy network to choose discrete schemes adaptively. Numerical experiments show the trained network outperforms the state-of-the-art WENO scheme. The proposed RL framework shows potential for designing high-performance numerical schemes for conservation laws. The learned policy network generalizes well to different situations. The paper concludes with future research directions and discusses solving 1-D conservation laws using finite difference methods. The WENO method is a high-order accurate finite difference scheme for solving hyperbolic conservation laws. It uses a nonlinear adaptive procedure to choose the smoothest local stencil for reconstructing the numerical flux. A new term introduced in this paper is the \"numerical flux policy,\" which is the policy to be learned using RL. The WENO method uses a nonlinear adaptive procedure to choose the smoothest local stencil for reconstructing the numerical flux. The key idea is to average all interpolants with properly designed weights to obtain the final reconstruction. WENO schemes are consistent and require upwinding for stability. The WENO method focuses on determining the upwind direction using the sign of the Roe speed. It achieves optimal accuracy in smooth regions but lower accuracy at singularities. The key lies in computing the weight vector based on local smoothness, with potential for improving accuracy near singularities. In this section, the use of reinforcement learning to solve conservation laws is discussed. The focus is on incorporating a policy network with the WENO scheme to improve estimation and simplify coding. Key aspects include learning to choose better weights for flux combination and automatically judging upwind direction without complex logical judgments. In this section, reinforcement learning is used to solve conservation laws by incorporating a policy network with the WENO scheme. The procedure involves learning the numerical flux policy \u03c0 f using RL, while traditional numerical schemes are used for the temporal scheme. The process is formulated as an MDP with states, actions, rewards, and transition dynamics. Algorithm 2 demonstrates how RL is integrated into the procedure using a single RL agent. The RL agent computes state using a state function, determines numerical fluxes, incorporates linear weights for fluxes, and minimizes error with a reward function. Transition dynamics are deterministic and depend on temporal scheme choice. The process involves using multiple agents in a mesh point scenario, but due to impracticality, agents at different points share the same weight, reducing to a single agent. This single agent acts like a human designer making decisions based on the current state, similar to traditional numerical methods. The agent computes actions and rewards, ultimately leading to a well-trained RL model. The well-trained RL policy \u03c0 RL is used to transfer actions to weights of WENO fluxes, improving stability and generalization. The RL policy acts as a meta-learner, generating weights for numerical fluxes computed using different stencils in WENO. The upwind direction is automatically determined by the RL policy, enhancing the efficiency of the system. The proposed RL conservation law solver uses a policy network to automatically determine upwind directions for 1D scalar cases. Training is done on Burger's equation with specific shift values. Further comparisons and discussions are available in the appendix. The proposed RL conservation law solver uses a policy network with specific shift values to generate actions for 1D scalar cases. The policy network is a feed-forward Multi-layer Perceptron with 6 hidden layers and uses the Deep Deterministic Policy Gradient Algorithm for training. To ensure generalization, 20 initial conditions are randomly sampled. The proposed RL conservation law solver uses a policy network called RL-WENO to generate initial conditions for training. The computation domain is defined with specific parameters for temporal discretization. The performance of RL-WENO is compared to traditional WENO methods, and generalization to different temporal conditions is tested. The RL-WENO policy network outperforms traditional WENO methods by achieving smaller errors and better generalization to different temporal conditions. Training was conducted with specific parameters, and the results show improved accuracy over high order accurate methods. RL not only achieves smaller errors but also generalizes well to longer evolving time, new time discretization schemes, new mesh sizes, and a new flux function. The solutions generated by RL-WENO show clear advantages over WENO near singularities, with the ability to select stencils in a different way and lead to more accurate results. The proposed RL framework for solving 1-dimensional conservation laws via deep reinforcement learning outperforms human experts in designing numerical schemes. The framework utilizes a numerical flux policy to make decisions based on the current state of the solution, showing superiority over high order WENO methods in numerical experiments. The proposed RL based solver outperforms high order WENO in various cases and is well generalized. Future works include using numerical flux policy for more complicated fluxes, generating adaptive grids and schemes, and considering conservation laws in higher dimensions. The RL solver is less restricted compared to neural network based solvers, making proper comparisons challenging. The proposed RL PDE solver outperforms high order WENO in various cases and is well generalized. Future works include using numerical flux policy for more complicated fluxes, generating adaptive grids and schemes, and considering conservation laws in higher dimensions. It is challenging to design SL methods without formulating the problem into an MDP due to differences in stencils between dense and coarse grids. Simple imitation of WENO on dense grids is not effective. One approach to improve PDE solvers is to use back-propagation instead of RL algorithms to optimize the policy network. By training the weights using supervised learning, the learned discrete operators may not guarantee conservation properties, leading to poor generalization. Minimizing the error between approximated and true values can be one way to train the weights effectively. The framework to train the SL network involves minimizing error between approximated and true values using back-propagation. However, using SL to optimize stencils may not enforce long-term accuracy, leading to inferior performance compared to RL-WENO. Experiments show that the SL-trained policy does not perform well. To improve stability in training, designing the loss of SL as an accumulated loss over multiple prediction steps may be suggested. However, due to the non-linear dynamics of the problem, gradient flow through multiple steps can be numerically unstable. RL-WENO, which improves upon WENO, achieves higher accuracy near singularities. Additional demonstrations show how RL-WENO performs in smooth and singular regions by recording approximation errors at every location. The distribution of errors on spatial-temporal grids with multiple initial conditions is computed, showing RL-WENO outperforming WENO near singularities. Inference time comparison between RL-WENO and WENO is reported, with RL-WENO achieving better accuracy despite higher computational complexity. The test compares RL-WENO and WENO performance on GPU and CPU for solving a problem with specific parameters. Results show RL-WENO outperforming WENO near singularities, with inference time and relative error reported. The GPU used is a custom NVIDIA GTX 1080. Results show that as the grid becomes denser, all methods except RL-WENO (GPU) require significantly more time to finish computations. The GPU version of RL-WENO can compute all approximations in parallel, resulting in consistent computation time regardless of grid density. RL-WENO (GPU) can be faster than well-optimized WENO on a denser grid by leveraging parallel processing. Reinforcement Learning (RL) combined with deep neural networks has been successful in various tasks like playing video games, Go, and robotics control. RL tackles sequential decision making problems formulated as Markov Decision Processes (MDP). RL involves interactions between an agent and the environment forming trajectories and policies. The value of a state in Reinforcement Learning is defined as the expected return of all trajectories under a policy \u03c0. The Bellman Equation connects the value of a state with its successor state, aiming to find a policy to maximize long-term rewards. In Reinforcement Learning, the goal is to find a policy \u03c0 to maximize the expected sum of rewards starting from the initial state s0. The policy can be optimized using the policy gradient theorem, especially in cases where the action space is continuous. Various algorithms like DDPG and Trust Region Policy Optimization have been proposed for such scenarios."
}