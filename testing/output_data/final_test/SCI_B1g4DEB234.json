{
    "title": "B1g4DEB234",
    "content": "Our work shows that layer rotation, the evolution of cosine distance between weight vectors and initialization during training, is a consistent indicator of generalization performance. It is easily monitored and controlled, with an optimum when all layers reach a cosine distance of 1 from initialization, leading to up to 20% higher test accuracy. Layer rotation can also explain the impact of weight decay and adaptive gradient methods on generalization in deep neural networks. In this paper, empirical evidence supports the discovery of layer rotation as a novel indicator of generalization in deep neural networks. Larger layer rotations consistently lead to better generalization performance across various experiments. This new perspective on generalization suggests that layer rotation also impacts ICTEAM. Layer rotation in deep neural networks is a beneficial indicator of generalization. It can be easily monitored and controlled during optimization, with a network-independent optimum. This approach provides a unified framework to explain the impact of weight decay and adaptive gradient methods on generalization, unlike other indicators that lack a clear optimum or methodology for tuning. Our work presents guidelines for tuning layer rotations and a controlling tool to optimize network performance with minimal hyper-parameter tuning. The experimental study is structured into three steps: 1. Development of tools for monitoring and controlling layer rotation; 2. Systematic study of layer rotation configurations; 3. Study of layer rotation configurations in standard training settings. Layer rotation is defined as the evolution of cosine distance between each layer's weight vector. The evolution of layer rotation during training is visualized by recording the cosine distance between each layer's current weight vector and its initialization. Layca is an algorithm that controls the amount of rotation performed by each layer's weight vector based on layerwise learning rates. This allows for a systematic study of the relation between layer rotations and generalization. Layca controls layer rotation by applying orthogonal projection and normalization operations on SGD's updates. This allows for control of the cumulative rotation since the start of training, based on the angle between weight vectors and learning rates of each layer. The purpose of this section is to conduct a systematic experimental study of layer rotation configurations using tools provided in Section 2. Layca is used to vary rotation rates for different layers and global rotation rates in experiments on various tasks with different network architectures and dataset complexities. The study explores layer rotation configurations by adjusting learning rates globally and per layer. Global learning rate decreases during training, while layer-wise multipliers control rotation rates between layers. The parameter \u03b1 influences the speed of rotation, with values near -1 indicating faster rotation of initial layers. The study investigates layer rotation configurations by adjusting learning rates globally and per layer. Different values of \u03b1 correspond to varying rotation speeds, with larger rotations leading to better generalization performance in classification tasks. The study found that larger layer rotations result in better generalization performance. Most configurations do not reach the maximum cosine distance of 1, with even slight rotations achieving 100% training accuracy. It is crucial for rotations to coincide with improvements in training error to prevent hindering the training process. The experiments in Section 3 investigate layer rotation configurations that naturally emerge during training with different methods like SGD, weight decay, and adaptive gradient methods. These experiments provide evidence for a rule of thumb proposed earlier and show that studying training methods from the perspective of layer rotation can offer insights into their behavior. The experiments are conducted on five tasks of TAB0, with learning rate parameters tuned independently for each setting through grid search. The experiments in Section 3 investigate layer rotation configurations during training with different methods like SGD, weight decay, and adaptive gradient methods. The test accuracies obtained in standard settings are compared to the best results obtained with Layca. SGD reaches lower test performance than Layca, despite extensive tuning of the learning rate. The experiments in Section 3 compare layer rotation configurations during training using methods like SGD with weight decay. Results show that weight decay helps synchronize layer rotations and improve test performance, aligning with the rule of thumb for generalization. The recent rise of adaptive gradient methods in machine learning has shown to impact generalization, potentially influencing layer rotations. Testing these methods against a rule of thumb, results indicate worse generalization compared to Layca's optimal configuration, with different layer rotations and lower test accuracies. The layer rotations induced by adaptive gradient methods differ from those of SGD, potentially impacting generalization in deep learning. When Layca is used to control layer rotation on top of adaptive methods, test accuracies can match those of SGD + weight decay. This offers a new perspective on the generalization properties of adaptive gradient methods. The visualizations show significant differences in generalization ability across configurations, with a trend indicating that higher layer rotation leads to better performance (\u2248 100% accuracy in all configurations)."
}