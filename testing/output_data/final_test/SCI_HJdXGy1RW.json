{
    "title": "HJdXGy1RW",
    "content": "We introduce CrescendoNet, a deep convolutional neural network without residual connections. It outperforms networks on CIFAR10, CIFAR100, and SVHN datasets with only 15 layers. CrescendoNet with 15 layers matches DenseNet-BC with 250 layers in performance. Its success may stem from implicit ensemble behavior. CrescendoNet, a deep convolutional neural network without residual connections, shows superior performance on various datasets with only 15 layers. Its success may be attributed to implicit ensemble behavior and the introduction of a new path-wise training procedure to reduce memory requirements. This contrasts with FractalNet, another deep CNN without residual connections. Residual networks and DenseNet have utilized residual connections to train very deep CNNs, achieving state-of-the-art accuracy. FractalNet BID14 and ResNet BID3 achieved high accuracy on benchmark datasets. Residual connections in deep CNNs have been studied for their impact on network performance, with different perspectives on their effectiveness. BID27 viewed ResNet as an ensemble of shallow paths, while BID2 argued against this explanation. FractalNet's success was attributed to unrolled iterative estimation of features along the longest path. CrescendoNet is a new deep convolutional neural network with ensemble behavior, created by stacking Crescendo blocks with increased convolution and batch-norm layers. It shows competitive performance on benchmark datasets like CIFAR10, CI-FAR100, and SVHN. CrescendoNet achieves high performance on CIFAR10, CI-FAR100, and SVHN datasets without residual connections. Its network design increases convolutional layers and parameters linearly in Crescendo blocks, showing ensemble behavior. Longer paths outperform shorter ones, but combining different length paths yields even better performance. Unlike FractalNet, where the longest path alone achieves similar performance as the entire network, CrescendoNet's paths work independently, allowing for a new pathwise training procedure. The CrescendoNet introduces a pathwise training procedure by training paths in each building block independently and sequentially, reducing memory usage. The Crescendo block with linearly increased layers shows high performance without explicit residual learning. The entire CrescendoNet outperforms any subset, suggesting improving model performance by increasing the number of paths. The CrescendoNet introduces a pathwise training approach to lower memory requirements without loss of accuracy. The Crescendo block consists of two layers - convolution with activation function and batch normalization. The structure of the block is defined by hyperparameters scale and interval. The Crescendo block in CrescendoNet architecture is designed for exploiting more feature expressiveness through different depths of parallel paths, leading to features in different abstract levels. It supports ensemble effects for efficient training and anytime classification. The structure of the block is defined by hyperparameters scale and interval, with an element-wise averaging operation for feature maps from each path. CrescendoNet Architecture consists of stacked Crescendo blocks with max-pooling layers, followed by fully connected layers and a soft-max layer for classification. The network structure is simple, with the ability to modify the entire network by tuning the Crescendo block. A path-wise training procedure is proposed to reduce memory consumption during training, training each path individually in a repetitive manner. Parameters of other paths are frozen while training one path. Path-wise training in CrescendoNet involves freezing parameters of other paths to reduce memory requirements for convolutional layers. This method allows for efficient training with various optimizers and regularizations, such as dropout and drop-connect. Regularization techniques like dropout and drop-path are used in deep neural networks to improve performance. Drop-path BID14 is a variant that further enhances performance by dropping paths during training. In the Crescendo block, branches are dropped with a predefined probability. L2 norm of weights is used as an additional term for fully connected layers. Models are evaluated on CIFAR10, CIFAR100, and SVHN datasets, each containing RGB images of size 32 \u00d7 32 pixels. The SVHN dataset consists of color images sized 32 \u00d7 32 pixels, with 604,388 training images and 26,032 testing images. Data augmentation includes padding images with zero pixels, cropping randomly, and flipping horizontally. Images are preprocessed by subtracting the mean and dividing the variance of pixels. Models are trained using Mini-batch gradient descent on TensorFlow with NVidia P100 GPU. Optimization techniques like Adam and Nesterov Momentum are used. For Adam optimization, the learning rate is set to 0.001 with momentum decay hyper-parameters \u03b21 = 0.9 and \u03b22 = 0.999. The default setting for AdamOptimizer class in TensorFlow is used. For Nesterov Momentum optimization, momentum is set to 0.9. The learning rate is decayed for CIFAR after 512 epochs and for SVHN after 42 and 63 epochs respectively. Truncated normal distribution is used for parameter initialization with specific standard deviations. A batch size of 128 is used for training on each replica. In the study, a CrescendoNet model with three blocks, each containing four branches, was used to investigate model performance under different block widths, the ensemble effect, and path-wise training performance. Three different width configurations were studied: equal width globally, equal width within the block, and increasing width. The number of feature maps for convolutional layers varied in each configuration, with gradual increases in feature maps for each branch in three blocks. In CrescendoNet, the number of feature maps for each branch in three blocks gradually increases to (128, 256, 512) correspondingly. The exact number of maps for each layer is determined by a specific equation. The ensemble behavior of CrescendoNet is analyzed by comparing models with and without drop-path technique and different branch combinations in each block. Branch combinations are denoted by a set P containing branch indexes. Table 1 compares CrescendoNet with other models on CIFAR and SVHN datasets. CrescendoNet outperforms networks without residual connections and matches DenseNet-BC's error rate on SVHN. It has a simpler structure and higher accuracies compared to FractalNet. The performance of CrescendoNet improves with the number of feature maps and shows potential for further enhancement by scaling up. The drop-path technique benefits the models, and Adam optimization method performs similarly to Nesterov Momentum on CIFAR10 and SVHN but worse on CIFAR100. Training CrescendoNet with Adam may be preferable when abundant training data is available due to its adaptive learning rate scheduling. Pathwise training results in a model with lower memory requirements but slightly reduced performance, still outperforming networks without residual connections. The ensemble behavior of the model improves with more paths in the network, leading to better classification error rates on CIFAR10/CIFAR100/SVHN datasets. The compared models in the study include Network in Network, ALL-CNN, Deeply Supervised Net, Highway Network, FractalNet, ResNet, ResNet with Stochastic Depth, Wide ResNet, and DenseNet. The whole net outperforms any single path network by a large margin, with the longest path showing an inference error rate of 10.69% for CIFAR10. CrescendoNet exhibits implicit ensemble behavior, differentiating it from FractalNet. The dynamic behavior of subnets is investigated by testing error rate changes during training using Adam. During training, the error rate changes of subnets are tested using Adam with the CrescendoNet structure on CIFAR10 for 450 epochs. The behavior of different path combinations is illustrated in FIG2, showing ensemble effects as the whole net and subnets grow in accuracy simultaneously. The anytime classifier behavior, similar to FractalNet, allows for quick rough inference with short paths and gradual accuracy increase with more paths, beneficial for time-critical applications like autonomous driving systems. Conventional deep CNNs like AlexNet and VGG-19 face challenges with vanishing gradient problems in training very deep networks. Recently, new building blocks like Inception have been introduced to improve the performance of deep CNNs. Inception blocks have four branches of shallow CNNs with different convolutional kernel sizes, allowing for diversified feature extraction. GoogLeNet, built on Inception, consists of 9 stacked Inception blocks to create a deep macro neural network. GoogLeNet, based on Inception blocks, has 9 stacked Inception blocks creating an exponential combination of feed-forward paths. It was further improved with new blocks like Xception and Inception-v4 for more powerful models. Inception-v4 used convolution factorization and label-smoothing regularization for scalability. FractalNet introduced stacked Fractal blocks for very deep neural network training. FractalNet architecture design achieved deep supervision and student-teacher learning through a fractal architecture. However, the exponential expansion of convolution layers reduced scalability compared to ResNet. Skip-connections, like those in ResNet, allow for training of very deep convolutional neural networks. DenseNet introduced extremely dense connections between layers for improved performance. CrescendoNet is a simple convolutional neural network without residual connections. It uses a Crescendo block with convolutional layers of size 3 \u00d7 3 and joins feature maps by averaging. The number of convolutional layers grows linearly, reducing computational complexity compared to FractalNet. CrescendoNet is a simplified CNN without residual connections, utilizing a Crescendo block with 3x3 convolutional layers. It reduces computational complexity by linearly increasing the number of layers. The network can be trained as an anytime classifier with dropout and drop-path regularization. CrescendoNet synergizes well with Adam optimization and does not require manual learning rate scheduling. It differs from FractalNet in performance on CIFAR10/100 and SVHN datasets. FractalNet BID14 demonstrates the student-teacher effect, where the longest path achieves similar performance to the entire network. The rest of FractalNet acts as a scaffold for training but becomes dispensable later. In contrast, CrescendoNet shows that the entire network outperforms any subset, highlighting the potential of increasing the number of paths in deep CNNs for improved performance."
}