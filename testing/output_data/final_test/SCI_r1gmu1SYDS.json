{
    "title": "r1gmu1SYDS",
    "content": "Learning can be framed as encoding mutual information between input and output while discarding irrelevant information. The difficulty of learning a task is quantified by calculating an observed mutual information score. This score is affected by the entropy of the data, with higher entropy leading to increased error in estimating mutual information. The representation of data can significantly impact observed entropy and mutual information. Experimental analysis of image-based input data shows that performance outcomes align well with the calculated score, emphasizing the importance of matching data representation with model encoding for better learning outcomes. To ensure better learning outcomes, data representations need to be tailored to align with the implicit distribution of the model. The way data is encoded and decoded by a model must match for effective information extraction. Different data representations, like the heliocentric vs. geocentric view of the solar system, can impact how information is conveyed and extracted. The new perspective on planetary trajectories simplified them to ellipses. In machine learning, researchers experiment with various data representations for tasks like speech recognition and face detection. The problem of choosing the right data representation for learning is about encoding the relationship between input and output efficiently. In this work, the focus is on learning a task by encoding the relationship between input and output efficiently. Different data representations are evaluated to understand their impact on learning outcomes and network optimization. The study introduces the concept of Observed Mutual Information (OMI) score to predict relative learning outcomes. This work introduces the concept of Observed Mutual Information (OMI) score to understand the impact of data representations on learning outcomes. Frame Theory provides a rich theory of linear invertible representations, with examples like Wavelets and Curvelets. Empirical testing on Imagenet suggests that using a data representation similar to JPEG encoding can improve the performance of residual network architectures. In a robotics context, Grassmann and Kahrs evaluated learning performance on approximating robot dynamics using common data representations. Weight Agnostic Neural Networks have been designed to function on a task even with randomly initialized weights. Measuring learning difficulty involves information theoretic measures like information gain. The information gain and information gain ratio were introduced in decision trees by Quinlan. Other approaches to assessing task difficulty include feature selection and judging feature suitability for learning problems. The relationship between selecting good features and task difficulty remains unclear. The objective of learning is to minimize uncertainty in output while discarding irrelevant information. The Information Bottleneck Lagrangian aims to find an encoding that maximizes mutual information with output y and minimizes mutual information with input x. The ideal learning task involves all information in x being highly relevant to y. The Information Bottleneck Lagrangian aims to maximize mutual information with output y and minimize with input x. Evaluating task difficulty involves measuring mutual information I(x; y) and adjusting for estimation errors by dividing by entropy H(x). Observed Mutual Information accounts for uncertainty in estimates and bounds deviation from true mutual information. Mutual Information is crucial for extracting information from data. Paninski (2003) provides a bound for entropy estimation error. This bound helps in estimating mutual information error, especially in the distillation regime where samples have high entropy and few in number compared to their entropy. Object detection tasks typically fall into this regime. In the distillation regime, images have high entropy while labels are compactly represented. The number of image samples is small compared to all possible samples. Mutual information bias is bounded for discrete random variables x and y with plug-in estimates on their entropy. The deviation of estimated mutual information to the true mutual information is concluded in the distillation regime. Theorem 3 by Shamir et al. (2010) discusses the relationship between input-output mutual information and estimated entropy in the context of bottleneck variables. It highlights the importance of reliable estimates for achieving high mutual information. The metaphor of predicting birthrate in Mongolia from stock market movements illustrates this concept. The concept of predicting birthrate in Mongolia from stock market movements illustrates the idea of spurious correlation and the importance of reliable estimates for achieving high mutual information. The observed mutual information score (OMI) captures both the magnitude of estimated mutual information and the uncertainty of the estimate, similar to the information gain ratio (IGR) used in decision trees for feature selection. Data representations play a crucial role in the learning process, with a focus on mutual information and OMI values. Different data representations can impact learning outcomes, even though they share the same information content. For effective learning, datasets should be represented in a way that directly maps to the desired output while retaining additional information for input reconstruction. The impact of data representation on learning is explored through expected coding length and the concept of choosing the \"wrong code\". Efficient encoding is bounded by the entropy of the distribution being compressed. Candidate distributions must fit the true distribution, with the expected coding length never smaller than the entropy. Function families have associated candidate distributions to measure uncertainty, such as linear regression using a normal distribution. The impact of data representation on learning is explored through expected coding length and the concept of choosing the \"wrong code\". The model uses a normal distribution to assess uncertainty, but assuming a candidate distribution may lead to overestimating entropy. The representation of the data distribution influences observed entropy by changing the alignment of the true distribution to the assumed candidate distribution. The wrong code theorem invalidates the assumption that estimated mutual information does not change with an invertible transformation. Invertible transformations can align true and assumed distributions, affecting mutual information and OMI score. Data representations impact entropy, with better alignment leading to smaller entropies. Theoretical findings were empirically studied across various datasets and network architectures. The study evaluated over 8000 networks with different datasets and architectures. The experiments included four diverse datasets for vision tasks: KDEF and Groceries for classification, Lane Following and Drone Racing for regression. Lane Following dataset was generated using Duckietown Gym simulation environment. The full code and data for replication are available at a provided URL. The dataset used in the experiments includes Lane Following, KDEF, and Drone Racing datasets. Images were downsampled to reduce training costs and add diversity. KDEF dataset consists of images showing actors expressing emotions, while Drone Racing dataset is based on drone racing data. The Drone Racing dataset by Delmerico et al. (2019) was used in the experiments. The mDAVIS data subsets 3, 5, 6, 9, and 10 were utilized, with focus on recovering rotational DOF from grayscale images. IMU data was matched to grayscale images using linear interpolation. The Freiburg Groceries Dataset was also used, with image size reduced to 120 x 120 pixels for classification into 25 categories. The entropy values for images were calculated using a multivariate Gaussian distribution, with a lower bound of 2 to avoid underestimation. Automated methods are preferred over manual hyperparameter tuning for neural networks to prevent bias in experiments. Bayesian optimization is used to find optimal hyperparameters for neural networks, accounting for performance fluctuations and uncertainty. It can optimize categorical and continuous dimensions concurrently and adapt to different datasets. Three basic architectures were investigated in the optimization process. The study investigated three basic architectures for neural networks, optimizing hyperparameters using Bayesian optimization. Different network architectures were constrained to the same domain for dataset representations. The architectures included variable numbers of convolutional and fully connected layers, with parametrized kernel sizes and filters. Dense Neural Networks consisted of variably sized fully connected layers with optimized activation functions. ResNets were composed of variable numbers of residual layers with convolutions. The study focused on neural network architectures with variable convolutional and fully connected layers. It explored different representations for images, including RGB and YCbCr, to analyze the sensitivity of learning processes to input data. Sample images for each representation are provided in the appendix. The representation of images as a combination of luminance and chrominance channels is useful for compression due to the human eye's sensitivity. This method decorrelates color channels based on previous work on preconditioning convolutional neural networks. Eigenvalues and eigenvectors are calculated to apply a transformation to the image dataset. The 2D type II discrete cosine transform is used for frequency-based representation. The 2D type II discrete cosine transform (DCT) is a frequency-based representation used for face detection. DCT coefficients are invariant to uniform changes in brightness. Block DCT involves applying the transform to 8x8 non-overlapping patches of each channel, commonly used in JPEG compression. Training of networks was done using an Adam optimizer and terminated after a specific number of iterations. After evaluating 5753 networks, 3702 finished training, confirming the importance of representations. RGB and YCbCr were consistently the best, followed by PREC and blockwise DCT, with DCT performing poorly. Hyperparameters greatly influenced results, showing the significance of network architecture on representation performance, especially for lane following tasks. The study highlighted the impact of network architecture on representation performance, particularly for lane following tasks. Dense networks performed well for RGB and YCbCr datasets but fell behind for other representations. The introduced OMI scores showed strong correlation with architecture search results and could predict differences between representations. However, there were some inconsistencies in predictions, such as the linear score on the KDEF dataset. Overall, the study emphasized the significant effect of representation on estimated entropy, OMI scores, and performance. The OMI score considers mutual information and variance, potentially aiding in automatic representation or architecture search. Understanding network architecture distributions is crucial for its success. Further research is needed to reduce bias in mutual information estimation. The OMI score helps mitigate bias and allows for principled evaluation of representations. Small entropy estimation errors are common, especially with normal distributions. When assuming a normal distribution, entropy is calculated using the logarithm of eigenvalues of the covariance matrix. However, errors can occur with small eigenvalues, leading to negative mutual information values. Bayesian optimization can be run in parallel asynchronously, but we focus on batch parallelization with 6 points evaluated at a time using the minimum constant liar strategy. The base estimator is used after 10 points. The acquisition function is optimized by sampling it at n points for categorical dimensions and 20 iterations of L-BFGS for continuous dimensions. Some generated networks cannot be trained due to geometric properties or memory constraints, with common failure modes being too many pooling layers and running out of memory. Approximately 35% of networks did not complete training in experiments. In experiments, some networks fail to complete training due to geometric properties or memory constraints. To prevent the Bayesian optimizer from re-evaluating these points, a large artificially generated loss is reported at the crashing point, chosen to be significantly higher than the expected loss. The impact of this approach requires further investigation in future studies."
}