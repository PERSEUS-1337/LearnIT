{
    "title": "B1gqEZwpvE",
    "content": "We examine combining generalized policies with search algorithms like Action Schema Network (ASNet) using deep learning for probabilistic planning. ASNet is effective with local knowledge but may struggle with generalization. Monte-Carlo Tree Search (MCTS) is a state space search algorithm for decision making, achieving good results with domain-specific knowledge but requiring many simulations without it. By combining these techniques, we aim to exploit strengths and overcome weaknesses in solving probabilistic planning problems. By combining ASNets with MCTS, we enhance the ASNet's generalization capability and improve search space navigation. This paper focuses on combining UCT BID11 with ASNets to improve planning by utilizing forward-chaining state space search and domain-specific knowledge. ASNet uses deep learning to learn generalized policies for planning problems, aiming to solve all problems in a planning domain effectively. ASNets are effective for problems where local knowledge can help avoid traps, outperforming traditional planners. They can generalize to any problem size but may fail on difficult, untrained problems. Suboptimal policies can result from poor hyperparameter choices. Contributions are applicable beyond ASNets to any method of learning a policy. Monte-Carlo Tree Search (MCTS) is a state-space search algorithm for optimal decision making that relies on performing simulations to estimate state values. MCTS-based game-playing algorithms, like AlphaGo, have achieved state-of-the-art performance. A limitation of vanilla MCTS is the need for a large number of simulations for reliable estimates. UCT, a variant of MCTS, balances exploration and exploitation trade-offs. Combining ASNets with UCT improves generalization, learning with sub-optimal hyperparameters, and robustness to environment changes. The paper formalizes probabilistic planning and introduces ASNets. The paper discusses planning as solving a Stochastic Shortest Path problem and introduces ASNets and MCTS. It defines a framework for Dynamic Programming UCT (DP-UCT) and examines techniques for combining policy learned by an ASNet with DP-UCT. Results are presented and analyzed, with a summary of contributions and discussion on related and future work. An optimal policy \u03c0 * selects actions to minimize expected cost in reaching a goal. For SSPs, a deterministic optimal policy can be found by solving the Bellman optimality equation. Dead ends are handled with a fixed dead-end penalty and a give-up action. ASNet is a neural network architecture for learning generalized policies in probabilistic planning problems. ASNet is a neural network architecture for learning policies in probabilistic planning problems. It consists of alternating action and proposition layers, with sparse connections between modules to ensure relevance. ASNet is a neural network architecture for learning policies in probabilistic planning problems. It consists of alternating action and proposition layers with sparse connections between modules to ensure relevance. The weights are shared between corresponding action modules and proposition modules, allowing scalability to any problem within the same domain. However, ASNets have a fixed number of layers and may struggle with domains requiring long chains of reasoning. Improper training could lead to failure in generalizing to new problems. Improper training of neural networks like ASNet can lead to failure in generalizing to new problems. MCTS is a state-space search algorithm that builds a search tree incrementally by performing trials until a computational budget is reached at each decision step. The algorithm consists of four phases: selection, expansion, simulation, and backpropagation. In the backpropagation phase of MCTS, trial results are updated through the tree nodes. UCT addresses exploration-exploitation trade-off by using UCB1. THTS is a framework combining MCTS, dynamic programming, and heuristic search planning algorithms. In a THTS algorithm, five ingredients are specified: action selection, backup function, heuristic function, outcome selection, and trial length. BID10 introduces three new algorithms - MaxUCT, DP-UCT, and UCT* - with superior theoretical properties over UCT. DP-UCT, focused on minimizing the cost to a goal, outperforms original UCT and MaxUCT. This modified version of DP-UCT from THTS is designed for SSPs with dead ends and introduces the simulation function. The simulation function is introduced as a generalization of random rollouts in MCTS. The search tree consists of alternating decision nodes and chance nodes, with state-value estimates and successor nodes. Chance nodes include action and action-value estimates. The search tree initially contains a single decision node. UCT is an online planning algorithm that interleaves planning with execution. It starts with a single decision node representing the initial state. At each decision step, UCT selects the chance node with the highest action-value estimate and applies its action. A trial consists of selection, expansion, simulation, and backup phases. In the selection phase, nodes are traversed by alternating between action and outcome selection until a new root is set. In the expansion phase of UCT, the tip node is expanded, and Q-values of its child chance nodes are optionally initialized. Estimated Q-values are calculated by considering transition probabilities. In the simulation phase of UCT, a rollout of the planning problem is performed from the tip node's state until a goal or dead-end state is reached, or the trial length is exceeded. This phase differs from THTS as it includes a simulation function to choose actions and sample next states based on transition probabilities. During the simulation phase of UCT, a heuristic estimate is added to the rollout cost if a goal or dead end is not reached. The trial length determines the lookahead capability of DP-UCT, with a small trial length focusing the search on nodes closer to the root. In SSPs, traditional MCTS-based algorithms use a random simulation function, which may not be suitable. In MCTS-based algorithms for planning, the simulation phase is often neglected and replaced by a heuristic estimate due to the inefficiency of using a random simulation function. However, in some cases, using a random simulation function or a domain-specific knowledge-influenced simulation function may be more beneficial to calculate estimates accurately. After the simulation phase in MCTS-based algorithms, the backup phase involves updating state-value and action-value estimates by propagating information gained during the simulation back up the search tree. DP-UCT considers Bellman backups that take probabilities of outcomes into account when updating action value estimates. The probabilities of outcomes are considered in Bellman backups for updating action value estimates in DP-UCT. UCT with Bellman backups is asymptotically optimal when all nodes are explored infinitely often. Combining DP-UCT with ASNet's stochastic policy \u03c0(a | s) ensures accurate selection of ingredients for exploration. Using ASNets in DP-UCT simulations can lead to faster convergence to the optimal policy by leveraging state-value and action-value estimates. However, the robustness of this approach depends on the accuracy of the learned policy. ASNets in DP-UCT simulations can lead to faster convergence to the optimal policy by leveraging state-value and action-value estimates. The robustness of this approach depends on the accuracy of the learned policy. DP-UCT with ASNets may require more simulations to converge compared to random simulation functions. ASNet-based simulation functions are asymptotically optimal if ingredient selection ensures exploration of all nodes. Choosing between STOCHASTIC ASNETS and MAXIMUM ASNETS depends on the confidence in applying each action. To determine which ASNet-based simulation function to use, consider the confidence in applying each action. MAXIMUM ASNETS bias simulations towards the best action when the probability distribution is skewed, while STOCHASTIC ASNETS are better for uniform distributions. The UCB1 term helps balance exploration and exploitation in the search tree. The ASNet's influence is integrated into UCB1 to bias action selection towards promising actions. A simple ASNet action selection method is used to maximize a stochastic policy learned by ASNet, with an influence constant M controlling the exploitation of ASNet's policy for exploration. Higher M values increase the ASNet's influence in action selection. The influence of the ASNet in action selection diminishes with more frequent actions, but SIMPLE-ASNET maintains UCB1's optimality. Higher M values require more trials to combat uninformative information. Ranked ASNet Action Selection is introduced as an extension to SIMPLE-ASNet action selection. It ensures that all chance nodes are visited at least once before using SIMPLE-ASNet. Unvisited nodes are explored in decreasing order of their probability in the policy \u03c0, allowing DP-UCT to focus its search efficiently. Ranked ASNet Action Selection is an extension to SIMPLE-ASNet, focusing on the most promising parts of the state space. It may require fewer trials to converge to the optimal action but can be less robust if the learned policy is misleading. DP-UCT with ASNet-influenced action selection is more robust to misleading information compared to an ASNet-based simulation function. The experiments were conducted using an ASNet-based simulation function, which required a larger number of trials to explore optimal actions. The experiments were performed on an Amazon Web Services EC2 c5.4x large instance with 16 CPUs and 32GB of memory. Two baseline planners were considered: the original ASNets algorithm and UCT*. The experiments used ASNet-based simulation functions with two baseline planners: original ASNets algorithm and UCT*. Four parametrizations of algorithms were considered, each with different action selection and simulation functions. ASNet hyperparameters were consistent with previous studies, and a two-hour training limit was imposed. ASNets were trained with a strict two-hour time limit using an LRTDP-based BID4 teacher. DP-UCT configurations utilized h add BID3 as the heuristic function and set the UCB1 bias parameter B to \u221a2. Q-value initialization was enabled for problems with dead ends to avoid selecting a chance node that may lead to a dead end. ASNets were trained with a 10-second time cutoff and limited to 10,000 trials per decision step. Dead-end penalty was set to 500, with a maximum of 1 hour per planning round and 100 execution steps. 30 rounds per planner were run for each experiment in the Stack Blocksworld domain, where the goal is to stack n blocks into a single tower. However, the network failed at this task due to non-overlapping training and testing problem distributions. In an experiment with ASNets, DP-UCT was able to overcome misleading information learned by ASNet policy. ASNets were trained on unstack problems with 2 to 10 blocks, while DP-UCT and ASNets were evaluated on stack problems with 5 to 20 blocks in the Exploding Blocksworld domain. This domain features detonating blocks, creating unavoidable dead ends that a good policy must avoid. In an experiment with ASNets, DP-UCT was able to overcome misleading information learned by ASNet policy. ASNets were trained on unstack problems with 2 to 10 blocks, while DP-UCT and ASNets were evaluated on stack problems with 5 to 20 blocks in the Exploding Blocksworld domain. The domain features detonating blocks, creating unavoidable dead ends that a good policy must avoid. We train an ASNet for 5 hours on a selected set of 16 problems, including those with avoidable and unavoidable dead ends, to navigate the search space and improve suboptimal learning. CosaNostra Pizza involves safely delivering a pizza to a customer while avoiding toll booth operators who may crush your car if not paid. The optimal strategy is to pay toll operators only on the trip to the customer to prevent any dead ends. This problem showcases avoidable dead ends and can be learned by an ASNet. The ASNet learns to pay toll operators only on the trip to the customer, scaling up to large instances. Determinisation-based techniques underperform due to avoidable dead ends in the CosaNostra domain. Heuristics like delete relaxation also struggle in this domain. UCT* may not scale up due to long reasoning chains, but combining DP-UCT with an ASNet's optimal policy is expected to be effective. By combining DP-UCT with an ASNet's optimal policy, scalability to larger instances is expected. Experiments on CosaNostra Pizza problems show that UCT* achieves near-full coverage for up to 20 blocks, while ASNets achieve zero coverage. Simple ASNets with different influence constants demonstrate how DP-UCT can overcome misleading information provided by ASNet. Running experiments using ASNets as a simulation function would result in near-zero coverage for DP-UCT. Simple ASNets can reliably achieve near-full coverage for problems with up to 17 blocks for M = 10, 15 blocks for M = 50, and approximately 11 blocks for M = 100. The time taken to reach a goal generally increases as M increases, but Simple ASNets can learn what ASNet has not learned and correct bad actions suggested by ASNet through search, eventually converging to the optimal solution. For DP-UCT flavors in Exploding Blocksworld, the UCB1 bias parameter was increased to 4 and the maximum number of trials set to 30,000 for more exploration. Combining DP-UCT with ASNets using Ranked ASNets with M set to 10, 50, and 100. Results show that ASNets' training set may not be representative of evaluation problems, leading to suboptimal performance for easier problems compared to UCT*. In Exploding Blocksworld, DP-UCT combined with ASNets through Ranked ASNets showed improved performance compared to UCT*. Even with zero coverage, ASNet's general knowledge helped navigate the search tree effectively. For example, in p08, Ranked ASNets with M = 50 outperformed other configurations with a coverage of 10/30. This demonstrates the ability to exploit ASNet policies for better results than UCT* and the network itself, even if the policy is suboptimal. Ranked ASNets has shown promising results in improving the suboptimal policy learned by the network, outperforming UCT*. The optimal policy for CosaNostra Pizza takes 3n + 4 steps, with ASNets achieving full coverage for all problems. The trial length using ASNets as a simulation function is set to 1.25 \u00b7 (3n + 4) for better performance. FIG3 displays the results, showing that ASNets and Maximum ASNets overlap, as well as Simple and Ranked ASNets. Using ASNets in UCB1 action selection provides marginal improvements over UCT* when the number of reachable states increases exponentially with the problem size. ASNets as a simulation function allows for more efficient exploration of the state space. In this study, the use of ASNets in DP-UCT as a simulation function is explored to efficiently navigate the search space and identify optimal actions. The ASNet-based simulation function aids DP-UCT in learning unexplored areas, particularly in domains with avoidable dead ends like Tireworld. The results show similarities between Triangle Tireworld and CosaNostra experiments, highlighting the effectiveness of ASNets in finding optimal policies. Various techniques are discussed to enhance search using generalized policies, including the integration of ASNet-learned policies in different flavors of DP-UCT through STOCHASTIC ASNETS and MAXIMUM ASNETS. Through the use of ASNets in DP-UCT, we have developed generalized policies that improve state-value and action-value estimates in the search tree. Our experiments demonstrate the capability of our algorithms to generalize beyond trained distributions and enhance sub-optimal learning. By combining DP-UCT with ASNets, we can bias action exploration towards optimal actions, leading to quicker convergence. Harnessing the power of search allows us to overcome challenges and improve overall performance. Our experiments show that utilizing search can help overcome misleading information from ASNets due to environmental changes. We achieved three goals: learning new information, improving sub-optimal learning, and being robust to environmental changes. These contributions are applicable to learning generalized policies and trial-based search algorithms. In a deterministic setting, generalized policies have been used to guide heuristic Best First Search. Our work focuses on using MCTS as the search algorithm and incorporating neural network policies to bias the UCB1 action selection rule. This differs from previous approaches that combine deep learning and MCTS. There are still many potential avenues for future research in this area. Future research may explore automatically learning the influence parameter M for SIMPLE-ASNET and RANKED-ASNET action selection, combating bad information from ASNet simulations by mixing with random simulations, and interleaving planning with learning using UCT with ASNets as a 'teacher' for training an AS."
}