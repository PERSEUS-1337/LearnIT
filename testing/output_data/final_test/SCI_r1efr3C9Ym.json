{
    "title": "r1efr3C9Ym",
    "content": "The paper introduces a new deep learning architecture for supervised learning with sparse and irregularly sampled multivariate time series data. It utilizes a semi-parametric interpolation network followed by a prediction network to share information across dimensions. The approach is tested on classification and regression tasks, demonstrating superior performance compared to baseline and recent models. There has been progress in developing models for sparse and irregularly sampled time series data. A new model architecture, Interpolation-Prediction Networks, allows for end-to-end learning with multivariate sparse and irregularly sampled data without the need for interpolation or imputation. The Interpolation-Prediction Networks model utilizes semi-parametric interpolation layers and a prediction network, such as GRU networks BID6, to handle sparse and irregularly sampled time series data. The interpolation network enables information from each input time series to contribute to the interpolation of all others, while the prediction network leverages any standard deep learning model. This approach removes restrictions associated with positive definite covariance matrices and allows for an explicit multi-timescale representation of input time series, isolating information about short duration events. Our architecture utilizes a semi-parametric intensity function representation to isolate information about transients and broader trends in input time series. It produces three output time series: a smooth interpolation for broad trends, a short time-scale interpolation for transients, and an intensity function for local observation frequencies. Motivated by problems in analyzing electronic health records, this work addresses the scarcity of dense physiological data streams in hospital systems. The proposed architecture aims to handle sparse and irregularly sampled physiological time series data in electronic health records. It outperforms baseline and advanced models in classification and regression tasks, including comparisons with Gaussian process models. Ablation testing is conducted to assess the impact of information channels on performance. The focus is on learning supervised machine learning models from multivariate time series data. Sparse and irregularly sampled multivariate time series data pose challenges for machine learning models in various fields such as electronic health records, climate science, ecology, biology, and astronomy. This type of data can hinder both supervised and unsupervised learning methods. Additionally, performing supervised learning in the presence of missing data in irregularly sampled time series differs from the missing data problem in a fixed-dimensional feature space. In irregularly sampled time series data, methods for handling missing data in supervised learning include pre-applying imputation methods and learning joint models of features and labels. The problem of irregular sampling can be addressed by discretizing the time axis into intervals with missing values. This approach is used by BID25 and BID21, requiring a choice of interval length. When dealing with irregularly sampled time series data, handling missing data involves discretizing the time axis into intervals. The choice of interval length impacts the amount of missing data and learning difficulty. The sequence of observation times can provide valuable information, which can be easily conditioned on using missing data indicators. This technique has been successfully applied in recommender systems and GRU models to improve performance. In handling irregularly sampled time series data, models can directly use the data without pre-discretizing the time axis. Gaussian process models can represent continuous time data using mean and covariance functions. Kernel-based methods can produce similarity functions between irregularly sampled time series, with a generalization to Gaussian process models. The re-parameterization trick can also be applied for model improvement. Gaussian process models can represent continuous time data using mean and covariance functions. BID19 extended these ideas by enabling end-to-end training of deep neural network models stacked on top of a Gaussian process layer. The model was later extended to multivariate time series using a multi-output Gaussian process regression model BID12. Dealing with the challenges of modeling multivariate time series, BID12 used a sum of separable kernel functions to limit the expressiveness of the model. These models allow for incorporating information from all available time points into a global interpolation model. Several interpolation models have been developed, with variants using either the posterior mean or the whole posterior when solving the final supervised problem. BID4 introduced methods based on GRU networks combined with imputation techniques like mean imputation and forward filling. They also explored decaying input values over time towards the mean and decaying hidden states towards zero. Yoon et al. (2017) presented a similar approach using multi-directional RNNs operating across streams. The model proposed in this work focuses on modeling multi-rate multivariate time series data, addressing the issue of unaligned observations. Unlike previous models that use interpolation techniques for missing data, this model aims to capture the global structure of time series directly. The proposed model in this work focuses on modeling multi-rate multivariate time series data using semi-parametric, deterministic, feed-forward interpolation layers. These layers allow for flexible interpolation within and across layers, producing regularly sampled interpolants for deep classification and regression networks. This approach contrasts with prior models that used Gaussian process representations, which could be computationally expensive and challenging to design covariance functions for. The proposed model focuses on modeling multi-rate multivariate time series data using semi-parametric, deterministic, feed-forward interpolation layers. This approach contrasts with prior models that used Gaussian process representations. The model includes information about the times of observations and directly models the sequence of observation events as a point process in continuous time using a semi-parametric intensity function. The model architecture consists of an interpolation network and a prediction network. The interpolation network interpolates the sparse multivariate time series against a set of reference time points. Time series are defined within a common time interval, with evenly spaced reference time points chosen. The model architecture includes a two-layer interpolation network that interpolates time series data at evenly spaced reference time points. The prediction network uses the output of the interpolation network to make predictions for the target variable, utilizing various standard neural network architectures. Additionally, an auto-encoding component is included to provide unsupervised learning signals for training the interpolation network. The interpolation network provides interpolants for each dimension of a multivariate time series at reference time points. It outputs smooth trends, transients, and observation intensity information. The model uses three outputs per dimension as input for the prediction network. The prediction network utilizes smooth trends, transients, and intensity functions from the interpolation network to generate predictions for the target values of the multivariate time series data. The matrix\u015d n can be converted into a single long vector for input to a feedforward network like a GRU or LSTM. Experiments use a GRU network with a composite objective function including supervised and unsupervised components to learn model parameters. Semi-parametric RBF interpolation layers prevent overfitting by setting kernel parameters to large values. To prevent overfitting in high-capacity autoencoders, masking variables are introduced to hold out some observed data points during learning. This helps the interpolation layers learn to properly interpolate input data without trivially memorizing it. The masking variables determine which data points are included in the reconstruction loss calculation. The proposed framework uses masking variables to prevent overfitting in high-capacity autoencoders. The learning objective includes loss functions for the prediction and interpolation networks, along with regularizers. Experiments are conducted on classification and regression tasks with sparse and irregularly sampled multivariate time series data. The model framework is tested on two real-world datasets: MIMIC-III, a multivariate time series dataset, and UWaveGesture, a univariate time series dataset. The MIMIC-III dataset includes physiological signals collected at Beth Israel Deaconess Medical Center, while the UWaveGesture dataset consists of gesture patterns. The framework is used for classification and regression tasks with multivariate time series, as well as for assessing training time and performance relative to univariate baseline models. The proposed model is compared to various baseline models including Logistic Regression, Support Vector Machines, Random Forests, and AdaBoost for classification tasks, and Linear Regression, Support Vector Regression, AdaBoost Regression, and Random Forest Regression for length of stay prediction tasks. Temporal discretization with forward filling is used to create fixed-size feature representations for non-neural network baselines. The curr_chunk discusses comparing the proposed model to existing deep learning baselines for mortality prediction, including models using GRUs and Gaussian processes for handling irregularly sampled and missing data in time series classification. The models compared include GP-GRU, GRU-M, and GRU-F. The curr_chunk discusses different methods for handling missing values in time series data, including GRU-F, GRU-S, GRU-D, and GRU-HD. These methods involve replacing missing values with the global mean, last observed measurement, or introducing decay in the input or hidden layer. Results of classification and regression experiments are presented using the UWaveGesture dataset to evaluate training time and performance. The UWaveGesture dataset is used to evaluate training time and classification performance compared to baseline models. Results include training time for convergence and accuracy on the test set. A new dataset is created for MIMIC-III, with results from a 5-fold cross-validation experiment reported in terms of AUC score, AUPRC score, and cross-entropy loss for classification. Metrics for the regression task include median absolute error and explained variation. Training and implementation details are provided in the appendix. Classification performance on the UWaveGesture dataset is shown in FIG2, comparing the proposed model with the Gaussian process adapter BID19. The proposed model outperforms baselines on the UWaveGesture dataset, achieving similar performance to the Gaussian process adapter but with a 50x speed up. In the MIMIC-III dataset, the proposed model consistently achieves the best average score across metrics, showing statistically significant improvements over baseline models. AUPRC BID9 is noted to provide better insights in highly skewed datasets like MIMIC-III. The proposed model in the experiment shows statistically significant improvements over baseline models in all metrics except median absolute error. Ablation study indicates that using only two outputs (transients and intensity function) further improves results. Comparison with multiple baselines on the MIMIC-III benchmark dataset demonstrates the proposed approach outperforming prior methods. The paper presents a new framework for supervised learning with sparse and irregularly sampled time series data. The approach includes an interpolation network and a prediction network to handle the complexity of the data. It outperforms prior methods on benchmark datasets and introduces novel elements like semi-parametric interpolation layers. The study introduces a framework for supervised learning with sparse and irregularly sampled time series data, utilizing semi-parametric interpolation layers. It focuses on predicting in-hospital mortality and length of stay using physiological variables from the first 48 hours of data. The dataset includes various physiological signals, medications, diagnostic codes, and outcome measures. The dataset consists of sparse and irregularly sampled time series data with 12 vital sign dimensions. The goal is to predict in-hospital mortality and length of stay using a binary indicator for mortality and real-valued regression target for length of stay. There are 53,211 data cases in total. The dataset includes real-valued regression targets for length of stay, represented as the log of stay in days. The UWave dataset consists of univariate time series data with simple gesture patterns divided into eight categories. It has 3582 train and 896 test instances, with 30% of training data used for validation. Each time series has 945 observations, and 10% of observation points are randomly sampled to create a sparse dataset. The model is learned using the Adam optimization method in TensorFlow with gradients provided via automatic differentiation. The multivariate time series representation used is based on the union of all time stamps in the input time series. Undefined observations are represented as zeros, and a missing data mask is used to track available observations. Equations 1 to 5 are modified to exclude unavailable data. This implementation supports parallel computation across all dimensions of the time series. The learning problem can be solved using a doubly stochastic gradient with mini batches. The interpolation network assigns the starting point value of the time series to the global mean before applying the two-layer interpolation network. Logistic Regression model is trained with cross entropy loss. Support vector classifier is used with a RBF kernel. Cross entropy loss is used on the validation set to select the optimal number of estimators. The optimal number of estimators for Adaboost and Random Forest in both classification and regression tasks is selected based on cross entropy loss and squared error on the validation set. Deep learning baselines in classification minimize cross entropy loss, while the proposed model uses a composite loss with cross-entropy and interpolation loss. For regression, baseline models minimize squared error, while the proposed model uses a composite loss with squared error and interpolation loss. The number of hidden units and hidden layers are determined using a multi-task Gaussian process implementation. The Gaussian process implementation by BID12 treats hidden units and layers as hyper-parameters. GRU-based models use specified parameters from BID4 and are optimized using Adam. Early stopping is applied on a validation set. In classification, GRU outputs are used in a logistic layer, while in regression, they are input for a dense hidden layer followed by a linear output layer. Hyper-parameters are independently tuned for each baseline method. Hidden units for GRU are searched over a range. Baseline models are evaluated on the test set for training time and accuracy. Gaussian process model uses squared exponential covariance. In this section, the relative information content of different outputs produced by the interpolation network in the proposed model for the MIMIC-III dataset is addressed. The interpolation network generates three outputs for each of the 12 vital sign time series: a smooth interpolation output (SI), a non-smooth or transient output (T), and an intensity function (I). A set of ablation experiments is conducted to assess the impact of each output, using all sub-sets of outputs for analysis. In ablation experiments, different outputs from the interpolation network were analyzed for classification and regression tasks. Smooth interpolation (SI) performed best for classification, while intensity output was most effective for regression. Combining all three outputs yielded the best results for classification tasks. The study compared the performance of different outputs in classification and regression tasks. The intensity output was found to be more effective for regression, while a combination of intensity and transients output performed better for explained variance score. However, the transients output did not contribute much to classification tasks. Combining smooth interpolation and intensity showed a significant performance boost in classification. The model's performance was evaluated on a MIMIC-III benchmark dataset focusing on predicting in-hospital mortality. In predicting in-hospital mortality using the first 48 hours of data, the proposed model is compared to baselines from BID13. The time-series data is discretized into 1-hour intervals, with mean or last observation assigned if multiple observations are present. Logistic regression and LSTM networks are compared in a multitask setting where supervision is provided at each time step. The proposed model uses an LSTM for prediction in a multitask setting with supervision at each time step, matching baselines in the experiment."
}