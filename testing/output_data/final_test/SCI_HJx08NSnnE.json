{
    "title": "HJx08NSnnE",
    "content": "Deep Convolutional Networks (DCNs) are sensitive to Universal Adversarial Perturbations (UAPs), which are input-agnostic perturbations that fool models on a dataset. Our work shows that procedural noise patterns, like Gabor noise patterns, also act as UAPs. This behavior and its implications require further study. Universal Adversarial Perturbations (UAPs) for Deep Convolutional Networks (DCNs) contain structure in their noise patterns, similar to Gabor noise. DCNs trained on natural image datasets learn convolution filters that resemble Gabor kernels and color blobs. This suggests that DCNs are sensitive to procedural noise perturbations. In this paper, the sensitivity of different DCN architectures to Gabor noise on the ImageNet image classification task is demonstrated. It is observed that random Gabor noise patterns can effectively generate Universal Adversarial Perturbations (UAPs). The injection of Gabor noise poses a threat to the security and reliability of DCNs, as it exploits specific features learned by convolutional filters. Compared to standard adversarial examples, UAPs reveal more general features that DCNs are sensitive to. The previous paragraphs demonstrate how random Gabor noise patterns can create Universal Adversarial Perturbations (UAPs) that exploit specific features learned by convolutional filters in DCNs. Different methods, such as the DeepFool algorithm and Generative Adversarial Nets (GANs), have been used to generate UAPs targeting specific layers in DCNs. The singular vector method proposed by BID4 maximizes the L p -norm of differences in activations for a specific layer, with constraints on the L q -norm, to effectively generate UAPs. The previous paragraphs discussed how different methods can generate Universal Adversarial Perturbations (UAPs) targeting specific layers in DCNs. In contrast, the current chunk explains how procedural noise, specifically Gabor noise, can efficiently create UAPs for DCNs by combining sparse white noise and a Gabor kernel. Gabor noise is defined by parameters such as magnitude, width, frequency, and orientation, and is expressed as a convolution function. Gabor noise, an expressive noise function with many parameterizations, is used to create UAPs for DCNs. Anisotropic Gabor noise with uniform orientation and thickness is employed, and the variance spectrum is normalized. Experiments are conducted using pre-trained ImageNet DCN architectures like Inception v3, ResNet-50, and VGG-19. Inception v3 has input dimensions of 299\u00d7299\u00d73, while ResNet-50 and VGG-19 have dimensions of 224\u00d7224\u00d73. The fixed kernel size \u03ba = 23 ensures Gabor kernels fill the entire image. The Gabor noise parameters \u0398 = {\u03c3, \u03c9, \u03bb} are controlled for testing model sensitivity with random perturbations. 1,000 Gabor noise perturbations are generated with parameters \u03c3, \u03bb \u2208 [1.5, 9] and \u03c9 \u2208 [0, \u03c0]. Evaluation is done on 5,000 random images with an \u221e norm constraint of \u03b5 = 12 on the noise. Sensitivity to Gabor noise is compared to uniform random noise perturbations to show non-trivial effects. The universal sensitivity of a model to Gabor noise perturbations is defined with a norm constraint on the perturbation. The universal evasion rate is considered for classification tasks, focusing on how small changes to the input affect the model's predictions. This analysis disregards ground truth labels and emphasizes the impact of perturbations on model outputs. The universal sensitivity and evasion metrics are used to analyze model sensitivity to perturbations. Inception v3, ResNet-50, and VGG-19 are ranked from least to most sensitive models. The models show significant sensitivity to Gabor noise compared to random noise. Inception v3 is insensitive to random noise but moderately sensitive to Gabor noise. ResNet-50 is more sensitive to random noise than VGG-19. The results suggest that ResNet-50 is more sensitive to random noise than VGG-19, while VGG-19 is more sensitive to Gabor noise. The models show sensitivity to perturbations, with Gabor noise affecting a third or more of the input dataset. The top 10 perturbations that VGG-19 is sensitive to also affect the other models. The evasion rates for these perturbations range from 37.9% to 71.4%. The study found that there is a strong correlation between universal sensitivity and evasion rates across different models, with perturbations transferring effectively. The universal evasion rate of perturbations seems to be unaffected by Gaussian width and orientation. However, for small \u03bb values, the sensitivity falls below average, indicating that Gabor noise may not impact the model's decision. Lambda corresponds to the width of bands in the image. The model's sensitivity varies across the input dataset, with some inputs being stable while others are more susceptible to perturbations. The average evasion rate shows a bimodal distribution, indicating subsets of data that are very sensitive or insensitive to perturbations. Inception v3 has a larger fraction of data points unaffected by Gabor perturbations. The dataset shows that DCN models are sensitive to Gabor noise, with a large fraction of inputs affected. This sensitivity has implications for security and reliability, allowing for inexpensive black-box attacks. Gabor noise can also be used for adversarial training to improve DCN robustness. In future work, analyzing the sensitivity of hidden layer activations to different noise patterns and reducing DCNs sensitivity to perturbations is important. Sensitivity metric values for random noise are smaller than Gabor noise. Adversarial examples with top perturbations are shown in figures. A large part of the input dataset is insensitive to random noise, with about 60% having near 0% average evasion over random noise perturbations. About 60% of the dataset shows near 0% average evasion over random noise perturbations for all three models."
}