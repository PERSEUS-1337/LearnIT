{
    "title": "SkxqZngC-",
    "content": "Topic modeling of text documents is crucial for representation learning. The iTM-VAE is a Bayesian nonparametric topic model with variational auto-encoders. It has infinite topics and adapts the topic number automatically. The inference is done by neural networks, allowing for rich representation capacity. Two variants, iTM-VAE-Prod and iTM-VAE-G, improve performance by modeling the generative process differently. Experimental results show that these models outperform existing methods on 20News and Reuters RCV1-V2 datasets. The proposed models in the Reuters RCV1-V2 datasets outperform state-of-the-art models in terms of perplexity, topic coherence, and document retrieval tasks. Probabilistic topic models focus on discovering abstract topics in documents and representing them as a mixture of these topics. Inference of topic distributions requires approximation methods like MCMC sampling or variational inference, making it challenging to adapt the model to changes. The inference process for topic models becomes more complex as the model becomes more expressive, hindering the discovery of latent semantic structures in data. Black-box inference methods like Auto-Encoding Variational Bayes (AEVB) are desirable for topic models as they require limited model knowledge and offer efficient approximate inference. AEVB includes an inference network that directly maps documents to variational posteriors, making topic models more flexible. An increasing number of work has been proposed to combine topic models with AEVB, such as BID23 BID37 BID5 BID24. Deciding the number of topics is a challenge, with options like model selection or Bayesian nonparametric (BNP) models like Hierarchical Dirichlet Process (HDP) that adapt the number of topics to data. HDP extends LDA by potentially having an infinite number of topics that can grow as more documents are observed. Unlike black-box inference models, traditional methods need to be redesigned when necessary. In this work, a new infinite Topic Model with Variational Auto-Encoders (iTM-VAE) is proposed to address the challenge of adapting inference methods to changes in the generative process of topic models. iTM-VAE combines Bayesian nonparametric techniques with deep neural networks to capture uncertainty in the number of topics and simplify inference. The model uses a stick-breaking process to generate mixture weights for an infinite set of topics and neural networks to approximate variational posteriors. Key contributions include the introduction of iTM-VAE as the first Bayesian nonparametric topic model with AEVB and iTM-VAE-Prod, which models the distribution over words as a product of experts. The iTM-VAE-Prod model is proposed, utilizing a product of experts for word distribution. Experimental results show that iTM-VAE and its variants outperform state-of-the-art models in document modeling tasks. Neural topic models have shown promising performance in various applications, but do not explicitly model the generative story of documents. Several recent works have proposed models that explicitly model the generative procedure of documents using deep neural networks, making them interpretable, powerful, and easily extendable. For example, the AVITM model combines the original LDA formulation with AEVB and achieved state-of-the-art performance on topic coherence metrics. Bayesian nonparametric topic models have infinite topic capacity and can adapt the topic number to data. However, there are no topic models that combine BNP. The AVITM model combines LDA with AEVB for interpretable and powerful topic modeling. Bayesian nonparametric models like SB-VAE and iTM-VAE-G adapt topic numbers to data. iTM-VAE uses elegant techniques to decide topic numbers, unlike RSB-TF which is complex. The iTM-VAE model utilizes nonparametric techniques to determine the number of topics, outperforming RSB-TF. It involves the stick-breaking process, Variational Auto-Encoders, and the Kumaraswamy distribution. The stick-breaking process generates an infinite sequence of mixture weights for the generative procedure, denoted as \u03c0 \u223c GEM(\u03b1). VAE is a successful generative model that mathematically generates samples. The iTM-VAE model utilizes nonparametric techniques with the stick-breaking process and VAE to determine topics. It involves a mathematical generation process with a prior and conditional distribution. The ELBO of VAE can be optimized using SGVB and RT, requiring a differentiable parameterization for the latent variable z. However, this is not always satisfied by the Beta prior in iTM-VAE. The Kumaraswamy distribution BID17 is used in iTM-VAE to address the limitations of the Beta prior in the stick-breaking process. It is a continuous distribution defined for x \u2208 [0, 1] and a, b > 0, with a simple inverse CDF formulation. Kumaraswamy is preferred over Beta for SGVB as it satisfies the DNCP requirement and has a closely approximated KL divergence. This distribution is employed in the inference procedure of iTM-VAE, a Bayesian nonparametric topic model with VAE. The generative process of iTM-VAE involves drawing DISPLAYFORM0 from the GEM distribution to ensure \u221e k=1 \u03c0 k = 1 and 0 \u2264 \u03c0 k \u2264 1. Each topic \u03b8 k = \u03c3(\u03c6 k ) is a probability distribution over vocabulary, with \u03c6 k \u2208 R V as the parameter. iTM-VAE has an unlimited number of topics denoted by \u0398 = {\u03b8 k } \u221e k=1 and \u03a6 = {\u03c6 k } \u221e k=1. The document generation in iTM-VAE is described mathematically as DISPLAYFORM1 with \u03b1 as a hyper-parameter. The generative process of iTM-VAE involves drawing from the GEM distribution to ensure topic distribution \u03c0 sums to 1. Each topic \u03b8 k is a probability distribution over vocabulary. The joint probability of a document with N words, topic distribution \u03c0, and sampled topics \u03b8 can be written as a mathematical equation. The parameters of topic \u03b8 k are factorized for simplicity. The iTM-VAE generative process uses parameter \u03a6 for potentially infinite model capacity. Unlike traditional nonparametric topic models, iTM-VAE's topics are part of model parameters, shared across all documents. Inference in iTM-VAE involves drawing \u03c0 from a document w 1:N using a K \u2212 1 dimensional vector \u03bd. The iTM-VAE model uses a K \u2212 1 dimensional vector \u03bd to draw \u03c0 from a document w 1:N. The infinite sequence of mixture weights \u03c0 is truncated to K elements, with \u03bd K always set to 1. This truncation does not affect the GEM prior, allowing iTM-VAE to model uncertainty in the number of topics and adapt to data. Truncation-free posteriors can be used, but may not be as effective according to BID28. The iTM-VAE model utilizes truncation-free posteriors, but it is noted by BID28 that they are not as effective. In contrast, the truncated-fashion posterior of iTM-VAE is simple and practical. Optimization of iTM-VAE involves maximizing the Evidence Lower Bound (ELBO) by replacing \u0398 with \u03a6 for parameter optimization. One drawback is the inability to make predictions sharper than the mixed distributions, which can lead to poor quality topics. One solution is to replace the mixture of multinomials with a weighted product of experts for sharper predictions. The iTM-VAE model uses a weighted product of experts for sharper predictions, referred to as iTM-VAE-Prod. It computes a mixed topic distribution for each document, with the concentration parameter \u03b1 impacting the number of topics. A Gamma(s1, s2) prior is placed on \u03b1 for automatic adjustment to data. The ELBO of iTM-VAE is affected by \u03b1 and can be expressed as a generative process. The ELBO of iTM-VAE is derived with a Gamma(s1, s2) prior on \u03b1 for automatic adjustment to data. iTM-VAE-Prod outperforms iTM-VAE in experiments, evaluated on 20News and RCV1-V2 benchmarks using the same data and vocabulary as BID37. Comparison is made with state-of-the-art models like DocNADE, NVDM, NVLDA, and classical topic models like LDA and HDP. The experiments used a two-layer neural network for g(w 1:N ; \u03c8) with different configurations for 20News and RCV1-V2 datasets. Batch-Normalization BID12 was used for stabilization, and hyper-parameter \u03b1 for GEM distribution was cross-validated. Adam BID14 was used for optimization with a fixed learning rate. Perplexity was used to measure model fit. The code for iTM-VAE and its variants is available online. The experiments utilized a two-layer neural network for g(w 1:N ; \u03c8) with various setups on 20News and RCV1-V2 datasets. Perplexity was used to evaluate model fit, with RSB and GSM achieving the lowest perplexities on both datasets. iTM-VAE-Prod outperformed state-of-the-art models with lower perplexities. Finite topic models showed optimal performance with around 10 to 20 topics. iTM-VAE discovered an effective number of topics around 19, showcasing its ability to determine suitable topics. The iTM-VAE model can automatically determine the number of topics. Topic coherence, measured by Normalized Pointwise Mutual Information (NPMI), is used to match human judgment. The Effective Topic is defined as the top-1 significant topic among training samples. Average topic coherence is computed using top-5 and top-10 words across multiple runs for stability. The iTM-VAE-Prod model outperforms other topic models in terms of topic coherence on 20News and RCV1-V2 datasets, closely matching human judgment. It also shows superior performance in document retrieval tasks compared to LDA. The document representations learned by iTM-VAE and iTM-VAE-Prod are compared with other models like LDA, DocNADE, NVLDA, and ProdLDA. The retrieval task involves ranking documents in the database based on cosine similarities to the query document. Precision/Recall curves are computed for each model, with hidden representations of length 128. The mean of the variational Gaussian posterior is used as features for NVLDA. The mean of the variational Gaussian posterior is utilized as features for NVLDA and ProdLDA in iTM-VAE and iTM-VAE-Prod. The effective topic numbers are dynamic and smaller than 128, with a weighted sum of topic factor vectors used. iTM-VAE-Prod shows competitive results and outperforms others. The concentration parameter \u03b1 of GEM(\u03b1) impacts the number of topics and performance significantly, requiring cross-validation for better results. The number of effective topics increases with \u03b1, as shown in FIG4 (a). iTm-VAE-G proposes a Gamma(s1, s2) prior on \u03b1 to increase the number of effective topics. The model addresses the issue of latent representation collapsing to the prior by adjusting \u03b1 automatically. Unlike traditional methods, iTM-VAE-G enhances adaptive power by relaxing the prior on the latent space instead of regularizing the decoder. This approach proves more effective in improving model adaptability. To compare adaptive power, iTM-VAE-Prod with KL annealing and decoder regularization is compared to iTM-VAE-G on subsets of 20News dataset with different class sizes. Various techniques are used to alleviate collapse issues in iTM-VAE-Prod, while iTM-VAE-G utilizes a non-informative prior on \u03b1. SGD optimizer is used for optimization without KL annealing. The number of effective topics learned by iTM-VAE-G on subsets of 20News dataset is shown in Table 2. Training tricks like KL-annealing and decoder regularization do not help much when the decoder is strong. However, by placing a prior on the concentration parameter \u03b1, iTM-VAE-G can increase the adaptive power of the model. Table 3 illustrates the corpus-level variational posterior of \u03b1 and the number of effective topics learned by iTM-VAE-G. Before training, the expectation of \u03b1 given the variational posterior is 4, which is adjusted to the training set after training. The table shows \u03b3 1, \u03b3 2, the expectation of \u03b1, and the number of effective topics learned from data. The iTM-VAE-G model can adjust the number of effective topics based on the classes of documents in the training set. With more classes, the model discovers more topics to explain the dataset. The topic coverage increases as more topics are utilized to reach the same level of coverage. The iTM-VAE-G model can adapt to data by adjusting the number of effective topics. It is the first Bayesian nonparametric topic model using Variational Auto-Encoders, with two proposed variants - iTM-VAE-Prod and iTM-VAE-G. The iTM-VAE and its variants use a stick-breaking process to adapt the concentration parameter automatically. Inference is done by feed-forward neural networks, allowing for flexibility in incorporating more information sources. Experimental results show that iTM-VAE outperforms state-of-the-art baselines significantly, with diverse and high-quality topics learned by iTM-VAE-Prod. The stick-breaking prior encourages sparse representation and adjustment of topics based on the data. The comparison of representation sparsity is shown in FIG5 (a), highlighting the lack of diversity in topics learned by ProdLDA BID37. This results in redundant topics and a latent representation with poor discriminative power. The Evidence Lower Bound (ELBO) of iTM-VAE is computed to adapt the concentration parameter automatically. The text discusses topics related to Religion and Hardware, highlighting key words such as jesus, christian, scripture, faith, god, christ, heaven, resurrection for Religion, and floppy, controller, ide, scsi, ram, motherboard, windows, modem for Hardware. The comparison shows a lack of diversity in topics learned by ProdLDA BID37, resulting in redundant topics with poor discriminative power. The text discusses the top 10 redundant topics learned by ProdLDA, focusing on the SGVB estimator and reparameterization trick for approximating intractable expectations. It explains the inference network, generative procedure, and factor dimensions involved in the model. The text discusses the computation of the Evidence Lower Bound (ELBO) for iTM-VAE-G, involving KL divergence and the softmax function. It explains the derivation of various terms in the ELBO equation and the prior of the stick length variable \u03bd k. The prior of the stick length variable \u03bd k is Beta(1,\u03b1) and the variational posterior of the concentration parameter \u03b1 is a gamma distribution. The calculation of expectations in Equation 22 involves the gamma distribution in its exponential form. The calculation in Equation 22 involves computing E q(\u03b1|\u03b31,\u03b32) [log \u03b1] using the digamma function. The Taylor expansion is applied to E q(\u03bd k |w 1:N ) [log(1 \u2212 v k )], resulting in an infinite sum of Kumaraswamy's mth moment. The KL divergence of gamma distributions q(\u03b1; \u03b31, \u03b32) and p(\u03b1; s1, s2) is evaluated."
}