{
    "title": "S1xcwNr22E",
    "content": "We analyze the dynamics of training deep ReLU networks and their implications on generalization capability. A novel relationship between the gradient received by hidden student nodes and the activations of teacher nodes for deep ReLU networks is discovered. Student nodes whose weights are initialized close to teacher nodes converge faster, and in over-parameterized regimes, some nodes converge to teacher nodes while others converge to zero. This framework sheds light on phenomena like over-parameterization and implicit regularization in deep learning. Experiments show negative biases in BatchNorm of pre-trained VGG11/16 models. Experiments validate theoretical predictions on biases of pre-trained VGG11/16 models. Fundamental questions in neural networks remain unsolved, such as SGD optimization, generalization, and overparameterization. In this paper, a theoretical framework for multilayered ReLU networks is proposed to explain the phenomena of overparameterization leading to better generalization and the existence of lottery tickets. The framework involves a teacher-student setting where student nodes compete to explain teacher nodes, with lucky student nodes converging faster to teacher nodes. The theoretical framework for multilayered ReLU networks explains how student nodes converge to teacher nodes, leading to better generalization and over-parameterization. This framework clarifies how neural networks can fit both structured and random data, and why over-parameterization results in fast convergence. The theoretical framework explains how student nodes converge to teacher nodes, leading to better generalization. Training models with just enough capacity yield worse performance. Deep networks often converge to \"flat minima\" with good generalization, while sharp minima lead to poor generalization. Lucky student nodes converge to the teacher, while others become irrelevant, resulting in flat minima. Explanation of implicit regularization: When data labels are structured, a small teacher network with few nodes leads to faster convergence of student nodes that overlap with the teacher nodes, resulting in good generalization. On the other hand, in datasets with random labels, over-parameterization can handle a larger teacher network and achieve zero training error. The lottery ticket phenomenon demonstrates successful training with over-parameterization, where lucky student nodes converge with high weight magnitude to the teacher nodes. The lottery ticket phenomenon shows that initializing certain nodes with the same weight and retraining the model can improve test performance. It also highlights how structured data leads to implicit regularization, where student nodes converge to teacher nodes, aiding generalization. The snapping behavior enforces winner-take-all, ensuring that a teacher node is fully covered by a few student nodes after optimization. This phenomenon explains why networks trained with structured data can generalize well to the test set. Lottery Tickets (6; 7) demonstrate that resetting salient weights and pruning others can maintain or improve test performance, while reinitializing these weights leads to poorer performance. Salient weights overlap with teacher nodes after initialization, allowing them to converge to the same nodes during optimization. Resetting these weights reduces interference with irrelevant nodes, potentially enhancing performance. Reinitializing them results in unfavorable regions, leading to decreased performance. The proposed theory explains how the relationship between teacher and student nodes in multilayered ReLU networks affects performance. The gradient update rule for each layer is determined by correlations and modulation matrices. The proposed theory explores the impact of teacher-student node relationships in multilayered ReLU networks. It is based on correlations and modulation matrices, making assumptions about node activations. Two theorems are proven: convergence of student weights to teacher weights when the numbers match, and convergence of weights in the over-parameterization setting. The proposed theory examines the relationship between teacher-student nodes in multilayered ReLU networks. It focuses on the convergence of weights towards the teacher weights and the impact of top-down modulation on irrelevant weights. Assumption 3 states that well-separated activation fields of teacher nodes are necessary. Analysis of BatchNorm bias in pre-trained VGG11 and VGG16 supports this assumption, showing that a majority of bias parameters are negative, especially in the top layers. The majority of BatchNorm bias parameters are negative, especially in the top layers of fully connected (FC) and ConvNet networks. The student networks are over-parameterized with 10x more nodes/channels at each layer. BatchNorm is added after ReLU activation. Gaussian inputs with mean 0 and std 10 (GAUS) and CIFAR-10 dataset are used in the experiments. The teacher network for GAUS has weakly overlapped weights sampled from specific ranges. In the experiments, student nodes converge to the teacher network with BatchNorm. The FC case has an accuracy of 54.95%, while the ConvNet case has an accuracy of 86.4%. Normalized correlation \u03c1 and Mean Rank r are used to evaluate the convergence. The experiments show that student nodes converge to the teacher network with BatchNorm. The final evaluation accuracy for CIFAR-10 is often \u223c 1% higher than the teacher. Using BatchNorm accelerates accuracy growth and improves rank, but does not accelerate correlation growth. The theory predicts that top-down modulation \u03b2 aids convergence in multilayered ReLU networks. Correlation \u03c1 and mean rank r steadily increase over training on GAUS, with BatchNorm enhancing both in particular for CNNs. Layer-0 shows the best match with teacher nodes and mean rank. BatchNorm helps achieve better correlation and lower rank, especially for CNNs. Various studies have explored gradient descent, training pruned neural networks, and efficient neural network learning. The curr_chunk discusses the mathematical framework notation for student and teacher networks, including activation functions, ReLU gating, and backpropagated gradients. It also mentions weight representation between nodes in the student network. The curr_chunk focuses on the activation regions of nodes in multi-layered ReLU networks, the relationship between teacher and student networks, and the backpropagation of gradients through the layers. It discusses the weight representation between nodes in the student network and the correlation between gradients and node activations. The curr_chunk discusses how the gradient pushes student nodes to align with teacher nodes, even at intermediate layers where there is no direct supervision. The theorem presented shows that correlation between teacher and student nodes at the same layers is possible, allowing for different numbers of nodes on each side. In the over-parameterization setting, a novel gradient update rule is derived based on Theorem 1, showing correlation between teacher and student nodes in deep ReLU networks. Notations for various parameters are defined, and the dynamics of multi-layer ReLU networks are analyzed using Eqn. 8 and Assumption 2 (Lipschitz condition). Batch Normalization is used to speed up training and improve test performance of neural networks. The majority of biases in BatchNorm layers are negative in VGG11/16 trained on ImageNet. The total \"energy\" of incoming weights of each node is conserved over training iterations. Theorem 3 states that weights of each node remain constant in training with Batch Normalization. This helps stabilize the network by preventing energy leakage between layers. The assumption is made that weights are constant and gradients are orthogonal. Weight recovery is possible under certain conditions. Theorem 3 states that weights of each node remain constant in training with Batch Normalization to prevent energy leakage between layers. Under certain conditions, relevant weights converge to W* and irrelevant weights to 0. Faster convergence near W* is expected due to larger h*jj when wj approaches W* leading to a winner-take-all mechanism. The importance of the projection operator P\u22a5wj is highlighted for stability. The projection operator P\u22a5wj is crucial for stability in training with Batch Normalization. In over-parameterization cases, variables are divided into W = [Wu, Wr], where Wu converges to W* and the convergence of Wr needs consideration. The dynamics of the upper layer play a role in training stability with Batch Normalization. When Vr becomes small, W r can be changed without affecting the network's outputs, leading to flat minima. This insight can help understand the presence of flat directions in trained networks and why many eigenvalues of the Hessian are close to zero. Theorem 5 relates different network pruning methods: pruning small weights and pruning based on Hessian. It introduces a structured pruning method based on top-down modulation. Accelerated convergence is achieved by using a uniform learning rate schedule with initial small values that increase over iterations. Theorem 5 introduces structured pruning based on top-down modulation for network pruning methods. It shows that a uniform learning rate schedule can lead to accelerated convergence, but if the learning rate becomes too large, fluctuations occur and need to be reduced. Many-to-one mappings are discussed, with the potential for multiple student nodes to converge towards a teacher node. Random initialization is highlighted as requiring only small initial differences between weights for convergence. The analysis extends to multi-layer cases, where lucky student nodes' weights converge to teacher nodes while irrelevant weights depend on initialization. Irrelevant nodes connecting to lucky nodes have fan-out weights converging to zero, while those connecting to other irrelevant nodes have undetermined final values. The upper-layer irrelevant nodes eventually meet with zero weights if going up recursively, as the top-most output layer has no over-parameterization. The gradient backpropagated to node j follows a specific form, and the weight update for weight w jk connecting node j and node k can be written down. This formulation works for overparameterization. The formulation works for overparameterization, with a compact gradient update in Batch Norm layer. The back-propagated gradient is zero-mean and perpendicular to the input activation, as shown in the illustration. Unlike previous analyses, this approach provides a clear geometric interpretation. The analysis in Thm. 1 for Batch Norm layer does not make approximate assumptions. Using Lemma 1, Thm. 3 can be proven. By considering the property E x g lin j f lin j = 0 and the weight update rule w jk = E x g lin j f k, it is shown that rl j = 0. Lemma 2 provides bottom bounds, assuming all weights are equal to 1. Assumptions 2 and 3 lead to specific conclusions. Lemma 3 gives top bounds under the same assumptions. Lemma 4 discusses quadratic fall-off for diagonal elements of L. Lemma 4 discusses the quadratic fall-off for diagonal elements of L, showing that the volume of the affected area and weight difference are proportional to \u03b4w j. The matrix form is written when \u03b2 = \u03b2* = 11T, and it suffices to check if the projected weight vector of w j onto the complementary space of the ground truth node goes to zero. The text discusses the convergence of a finite step with a small learning rate. It introduces lemmas related to top-layer contraction, bottom-layer contraction, and weight separation. The proof is decomposed into these three lemmas, showing the progression of the iteration. If all lemmas hold true, then the induction hypotheses are also true. Lemma 5 is proven by showing the top-layer contraction and discussing relevant nodes for different components. The proof demonstrates that the weight bounds hold for the next iteration. The convergence speed and training performance of teacher networks with different sizes and the impact of BatchNorm on convergence are studied. A comparison between finite and infinite datasets reveals that node similarity convergence stalls with a finite dataset due to limited data points in activated regions. This behavior is not observed in datasets like CIFAR-10. Visualization of H* and \u03b2* matrix before and after optimization using GAUS shows that student node indices are reordered based on teacher-student node correlations. After optimization, student nodes with high correlation to teacher nodes also have high \u03b2 entries, especially in the H* matrix combining \u03b2* and activation patterns of student nodes."
}