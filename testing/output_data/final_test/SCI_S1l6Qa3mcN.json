{
    "title": "S1l6Qa3mcN",
    "content": "An important type of question in Explainable Planning is the contrastive question \"Why action A instead of action B?\". These questions can be answered with a contrastive explanation that compares properties of the original plan with A against the contrastive plan with B. This type of explanation highlights differences in decisions made by the planner and provides insight into the planning process. Generating this explanation involves compiling user questions into constraints added to the planning model to represent the contrastive plan in a formal description in a planning setting. In Explainable Planning, user questions are compiled into constraints for a formal description in a planning setting. AI Planning is crucial in Explainable AI, as it offers transparent decision-making mechanisms. Various approaches aim to provide explanations in AI planning, with some focusing on model reconciliation and real-time plan representation in first-order logic. In Explainable Planning, user questions are translated into constraints for planning. AI Planning is essential for transparent decision-making in Explainable AI. This paper focuses on local explanations for temporal and numeric planning problems, aiming to improve user understanding and trust in the system. The explanations can be specific to a plan or general to how the planning system operates. The goal is to explain why a planner made a particular decision, providing insight into the decision-making process. In Explainable Planning, user questions are translated into constraints for planning. The user may gain global insight into the planner's decision-making process. Explanations should adapt to the user's mental model. BID6 identifies ten question types about intelligent systems, with why and why not questions being most beneficial for understanding and trust. Miller suggests asking contrastive questions like \"Why action A rather than action B?\" for decision understanding. In Explainable Planning, user questions are translated into constraints for planning. The user may gain global insight into the planner's decision-making process. Explanations should adapt to the user's mental model. BID6 identifies ten question types about intelligent systems, with why and why not questions being most beneficial for understanding and trust. Miller suggests asking contrastive questions like \"Why action A rather than action B?\" for decision understanding. Contrastive questions capture the context of the question and identify gaps in the user's understanding of a plan that needs to be explained. Contrastive explanations compare the original plan against a contrastive plan that accounts for user expectations, improving understanding without the need for a full causal analysis. The formal question is used to generate a hypothetical model (HModel) which leads to a hypothetical plan (HPlan) for comparison with the original plan. The user can iterate the process by refining the HModel, combining different compilations to create more meaningful explanations until satisfactory. This paper focuses on compiling the formal question into a hypothetical model for temporal and numeric planning, addressing challenging research problems in finding optimal solutions for such problems. The paper formalizes domain-independent compilations from formal contrastive questions to PDDL2.1, necessary for providing contrastive explanations in planning processes. It covers a range of questions users commonly have about classical and temporal plans. The paper formalizes domain-independent compilations from formal contrastive questions to PDDL2.1 for providing contrastive explanations in planning processes. It covers a range of questions users commonly have about classical and temporal plans, organized into sections describing planning definitions, a running example, formal questions, and conclusions. The paper formalizes domain-independent compilations from formal contrastive questions to PDDL2.1 for providing contrastive explanations in planning processes. It covers a range of questions users commonly have about classical and temporal plans, organized into sections describing planning definitions, a running example, formal questions, and conclusions. A set of action schemas, called operators, and arity is a function mapping symbols to their respective arity in the planning instance. The problem P rob = Os, I, G, W is a tuple where Os is the set of objects, I is the initial state, G is the goal condition, and W is a set of time windows. Grounding is the process of forming propositions by applying predicate symbols to objects, and primitive numeric expressions are formed by applying function symbols to objects. The function symbols map symbols to their respective arity in the planning instance. A state consists of time, a logical part, and a numeric part describing values. The initial state is at time t=0. The goal is a set of constraints over P and V that must hold at the end of an action sequence for a plan to be valid. A simple plan is valid if the final state satisfies the goal. Each time window is a tuple with lower bound, upper bound, and a proposition or numeric effect. Each ground action in the planning instance has a duration, start and end conditions, invariant condition, and add effects. The set of ground actions is generated from the substitution of objects for operator parameters. The planning model involves actions with start and end conditions, add effects, and numeric effects on certain values. The domain includes predicates for robot location, connections between waypoints, and marking waypoints as visited. The problem specifies objects and initial state propositions, such as the robot's initial location and travel time values. The goal is to specify a constraint over P \u222a V, where the robot visits all locations. An example plan is shown in FIG4, which may seem sub-optimal as the robot returns to wp2 after visiting wp1. However, this plan is optimal due to the connectivity of waypoints as shown in FIG3. The robot's optimal plan involves returning to waypoint wp2 after visiting wp1, as the connectivity of waypoints dictates this decision. This behavior is also observed between waypoints wp3 and wp5, showcasing the utility of Explainable AI Planning (XAIP) in understanding such decisions. XAIP is useful for generating contrastive plans based on user questions, helping to understand the reasoning behind certain actions in a plan. An explanation problem consists of a planning model, the generated plan, and a specific user question. The goal is to provide insight to help the user answer their question. In this paper, a set of formal questions is proposed to provide insight into the reasoning behind actions in a plan. The questions address various scenarios such as why a certain action is chosen over another, why it is included or excluded in the plan, and why it is timed a certain way. These questions aim to help users understand the decision-making process in temporal plans. The paper proposes a set of formal questions to understand the reasoning behind actions in a plan, addressing scenarios like why certain actions are chosen, included/excluded, or timed a certain way. These questions help users grasp the decision-making process in temporal plans by systematically assessing counterfactual situations and generating plans for them via compilations. The user can use \u03a0 as the input model to ask iterative questions about a model, allowing them to stack questions and increase their understanding of the plan. Combining compilations in this way provides a wider set of constraints. The HModel is solved to give the HPlan, with new operators renamed back to their original form. The HPlan is validated against the original model \u03a0. Users can ask formal questions about why certain operators are used in a state, helping them understand the reasoning behind actions in a plan. The user can ask iterative questions about the model \u03a0 to increase their understanding of the plan. A compilation is formed to generate the HPlan, with ground actions replacing original actions. Time windows are created for durative actions still executing in the state. The plan is then generated from this new state with updated time windows for the original goal. The HPlan is formed by concatenating initial actions of the original plan with new actions, and the HModel includes final state, variables, actions, conditions, and time windows. Violations in action conditions are detected during plan validation against the original model. The new initial state I is shown with the robot at waypoint wp4, having visited waypoints wp2, wp1, and wp4. The problem is unsolvable due to lack of connections from wp4. By applying the user's constraint, it is shown that there are no more applicable actions to complete the goals. The compilation keeps the replaced action in the plan, but it may not be optimal as re-planning only occurs after the action is performed. The user's constraint shows that there are no more applicable actions to complete the goals. Re-planning only occurs after the action is performed, which may not be optimal. The robot's plan involves visiting wp4, and a compilation is formed to ensure the plan is valid by introducing a new predicate. The HModel includes the user's suggested action being applied. The operator goto waypoint is extended with additional effects and a new goal proposition is included. A formal question is asked about why a specific operator is used in a plan. The robot has already achieved the goal of visiting wp2 with the action (goto waypoint kenny wp0 wp2). The second action (goto waypoint kenny wp1 wp2) may seem redundant. The HModel is updated to include a new predicate not done action to represent actions not yet performed. The operator o is modified to delete the new predicate as an additional effect. The initial state and goal are adjusted to include the user-selected grounding of not done action. This ensures that the user-suggested action is not executed, as it invalidates the plan. An HPlan is generated without the action (goto waypoint kenny wp1 wp2). In Figure 6, a formal question is asked about why a specific operator is used outside of a certain time window in a given plan. The planning model is compiled to restrict the use of the operator to a specific time frame by replacing it with two operators that include extra constraints. The planning model is compiled to restrict the use of an operator to a specific time frame by replacing it with two operators that include extra constraints. The new operators extend the original operator with additional constraints, ensuring that actions can only be performed within a specified time window. The operator can be applied using the new operator o \u00aca, ensuring actions are only performed within a specified time window. This approach restricts the operator from appearing in the plan outside of the designated time frame. The user may question why a specific action is not used within a certain time window in the plan. The HPlan demonstrates a better plan without the action in that time frame, but the user may want to see the action performed within their specified time window. This constraint forces the action to be applied in the time window and allows for it to be applied at other times in the plan. This constraint is beneficial for scenarios like a robot needing to refuel between waypoints due to fuel depletion. The planning model generates the HPlan by forcing a specific action to be used within a time window, while still allowing it to be applied at other times. This constraint is beneficial for scenarios like a robot needing to refuel between waypoints due to fuel depletion. The planning model generates the HPlan by forcing a specific action to be used within a time window, while still allowing it to be applied at other times. This constraint is beneficial for scenarios like a robot needing to refuel between waypoints due to fuel depletion. The user may ask why an action is used at a specific time instead of earlier or later, especially in domains with resources that are depleted and replenished by specific actions, such as refueling a vehicle. The user can adjust the timing of actions in the planning model to prevent redundant actions and optimize the robot's path. This involves forcing a specific action to be used within a time window, allowing for more efficient waypoint navigation. The planning model allows for adjusting the timing of actions to optimize the robot's path by enforcing actions within a specified time window. This ensures efficient waypoint navigation and prevents redundant actions. The planning model optimizes the robot's path by adjusting action timing within a specified time window, ensuring efficient waypoint navigation and preventing redundant actions. The HPlan involves asking formal questions about the order of actions in a plan, leading to the compilation of a directed-acyclic-graph to represent user-suggested action orderings. The user suggests actions and orders them in a directed-acyclic-graph (DAG) encoded into the model \u03a0. New predicates are added for each edge in the DAG, and new operators are created for disallowed actions. The HModel \u03a0 includes various components like P s, V s, As, arity, Os, I, G, and W. The operator o a extends o with preconditions and effects related to traversing edges in a directed-acyclic-graph (DAG). This maintains the user's selected ordering within the HPlan. The combined arity of o a includes the original operator and all sink nodes of a, resulting in a large set of possible ground actions. The operator o a extends o with preconditions and effects related to traversing edges in a directed-acyclic-graph (DAG), maintaining the user's selected ordering within the HPlan. Ground propositions are added to represent existing edges, reducing the search space. New operators are added to the domain, extending existing ones. The approach compiles formal contrastive questions into domain independent constraints for explanations within the XAI paradigm. This formalizes a series of stages from user question to explanation in PDDL. This paper formalizes compilations in PDDL 2.1 for temporal and numeric domains and planners, defining questions for plan scenarios. Future work will investigate compilations to cover all possible contrastive questions. In future work, the focus will be on extending the formalized compilations in PDDL 2.1 to cover all possible contrastive questions. This includes exploring contrastive explanations with preferences, developing a language for expressing questions and constraints on plans, and defining the semantics using LTL. Additionally, there will be an emphasis on showing the difference between two causal chains and incorporating concepts related to plan structure. The curr_chunk discusses the need for additional concepts to improve explanations in causal support for goals. It also mentions the importance of functional and human behavioral evaluations for effectiveness."
}