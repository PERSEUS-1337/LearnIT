{
    "title": "SJxRKT4Fwr",
    "content": "Many real-world applications involve multivariate, geo-tagged time series data with missing values. State-of-the-art methods use Recurrent Neural Networks (RNN) for imputation, but self-attention mechanisms have shown better performance by explicitly modeling relationships between time stamps. This paper introduces the adaptation of self-attention for multivariate, geo-tagged time series data to capture relationships across different dimensions. The proposed Cross-Dimensional Self-Attention (CDSA) approach captures self-attention across different dimensions (time, location, sensor measurements) sequentially and in an order-independent manner. Extensive experiments on real-world datasets, including a newly collected NYC-traffic dataset, show the superiority of CDSA for imputation and forecasting tasks compared to state-of-the-art methods. Various monitoring applications, such as air quality, health-care, and traffic, use networked observation stations to record multivariate, geo-tagged time series data. Missing data in these time series poses challenges for imputation due to diverse patterns, making traditional methods less effective. Deep learning methods have been proposed to capture temporal relationships in time series data, but training recurrent neural networks (RNN) is time-consuming and cannot directly model relationships between distant time stamps. The self-attention mechanism introduced by Transformer has been suggested to overcome these limitations and accelerate training time. The self-attention mechanism introduced by Transformer has been adapted to impute missing data in multivariate time series, improving performance significantly. This method captures the relevance between time stamps explicitly, accelerating training time and enhancing performance on NLP tasks. The Cross-Dimensional Self-Attention mechanism updates Value vectors across all dimensions to capture intra-correlation in unique multi-dimensional data. It proposes a novel approach to capture attention jointly yet in a decomposed manner. This is a significant contribution in applying self-attention to data imputation. The study introduces the Cross-Dimensional Self-Attention (CDSA) mechanism for imputing multivariate, geo-tagged time series data. CDSA speeds up training, models relationships between data values, and outperforms existing models in data imputation and forecasting tasks. Our model, CDSA, outperforms state-of-the-art models for data imputation and forecasting tasks. The learned attention weights validate CDSA's ability to capture important cross-dimensional relationships. Self-attention, introduced by Vaswani et al. (2017), has been widely applied in this context. In this paper, self-attention is applied for multi-dimensional data imputation, exploring various modeling choices across different data dimensions. The conventional self-attention mechanism in NLP is reviewed, followed by the proposal of three methods for computing attention maps across different dimensions. Details of using CDSA for missing data imputation are presented, showing the mapping of words into Query, Key, and Value vectors for language translation tasks in NLP. The paper explores applying self-attention for multi-dimensional data imputation, proposing three methods for computing attention maps across different dimensions. To adapt self-attention from NLP to multivariate time series data, the approach involves viewing all data in a time stamp as one word embedding and modeling self-attention over time. The three proposed solutions for Cross-Dimensional Self-Attention (CDSA) include modeling attention within each dimension independently, crossing all dimensions jointly, and crossing all dimensions in a joint yet decomposed manner. The input X is reshaped into three input matrices X T , X L and X M. Three streams of self-attention layers process each input matrix in parallel. The output of each stream's last layer is fused through element-wise addition with trainable parameters. The hyper-parameters for each stream are set separately. Each unit X(p) is mapped to Q(p, :) and K(p, :) of d-dim as well as V(p, :) of v-dim. The input X is reshaped into input matrices X T , X L , X M, and X. Each unit X(p) is mapped to vectors Q(p, :), K(p, :), and V(p, :) of different dimensions. A novel Decomposed manner is proposed to capture dimension-specific and cross-dimensional attention. This approach uses input matrices X T , X L , and X M to build attention maps A T , A L , A M individually. The attention maps A T , A L , and A M are reshaped and applied on the Value vector in a specific order. The weight in A is decomposed as the product of weights in three dimension-specific attention maps. The arrangement \u03c3 determines the order of attention maps to update V. The proposed CDSA is based on the Decomposed, which forms a cross-dimensional attention map using three dimension-specific maps. Compared to the Joint and Independent approaches, the Decomposed significantly reduces FLOPs and requires fewer variables. Detailed comparisons are provided in Section 4.3. The CDSA mechanism is applied in a Transformer Encoder with 8 layers and in a Transformer framework with 9 layers for both encoder and decoder. A fully connected Feed Forward network is used to reconstruct missing values in the input. The input is normalized by subtracting the mean and dividing by the standard deviation. The Adam optimizer is used to minimize the Root Mean Square Error (RMSE). The model is trained on a single NVIDIA GTX 1080 Ti GPU using the Adam optimizer to minimize the RMSE between prediction and ground truth. Data is collected from street cameras in Manhattan over a month, with vehicle detection done using a faster-RCNN model. Time series data is aggregated in 5-minute intervals, resulting in 186 time series with a 5-minute gap between timestamps. The dataset consists of 186 time series with a 5-minute gap between timestamps. The natural missing rate is 8.43%, and entries are further removed for imputation experiments. A Burst Loss pattern is used for data removal, with missing rates varying from 20% to 90%. The model is trained on 432 consecutive time slots for each missing rate, with the average RMSE evaluated over 5 trials. The dataset includes Air Quality and Meteorology data recorded hourly, with 11 locations and 12 measurements. The dataset, used for traffic speed forecasting, contains data from 207 locations recorded every 5 minutes for 4 months. 80% of the data is used for training, and the remaining 20% for testing. Time series of consecutive 2 hours are enumerated for forecasting, with the first hour as input and the second hour to be predicted. Our CDSA method consistently outperforms traditional and recent RNN-based methods in traffic speed forecasting, leveraging self-attention to model relationships between distant data. It achieves significant improvements on cross-dimensional data imputation tasks as well. Our CDSA method outperforms previous methods in traffic speed forecasting, especially at long-term forecasting like 60 min. It directly models the relationship between distant data values, showing clear improvement. Different training losses were compared, with RMSE achieving the best performance. Ablation study on different cross-dimensional self-attention manners was conducted, comparing performance on three datasets. The CDSA method outperforms previous methods in traffic speed forecasting, especially at long-term forecasting like 60 min. It directly models the relationship between distant data values, showing clear improvement. Different training losses were compared, with RMSE achieving the best performance. Ablation study on different cross-dimensional self-attention manners was conducted, comparing performance on three datasets. The attention modeling in CDSA mechanism determines computational complexity, with the Decomposed method achieving the best performance on the three datasets evaluated. The study proposes a cross-dimensional self-attention mechanism for imputing missing values in multivariate, geo-tagged time series data. By utilizing PM10 and PM2.5 data at different time points, the model shows strong correlation and achieves superior results in forecasting. Visualization examples are provided in the supplementary material. The study introduces a cross-dimensional self-attention mechanism for imputing missing values in multivariate, geo-tagged time series data. The model demonstrates strong correlation between PM10 and PM2.5 data, outperforming state-of-the-art methods in forecasting tasks. Future plans include extending the mechanism to higher-dimensional inputs with multiple data modalities. The study applies a normalization process for each measurement in parallel, using a fully connected feed-forward network with ReLU activation function. Increasing the depth of the network shows significant improvement in the experiment. In the imputation task, a modified attention map with mask is used to prevent units from contributing to their own estimation. This ensures that the estimation of a unit depends on all other units except itself. The performance improvement of this mask is demonstrated in Table 9, showing enhanced inference ability. Additionally, in the Joint model, different kernels are used to map each input unit for building attention among different units. In the Joint model, different kernels are used to map each input unit to a 1-D Query vector and a 1-D Key vector individually. The attention map is a scaled dot-product between Query and Key, where each value is the scaled numerical multiplication between input units. To calculate dimension-specific attention maps, the input is reshaped into matrices, with units corresponding to the same timestamp mapped into dimension-specific Query and Key vectors. This introduces more parameters into the vector-vector mapping. In the Shared model, the dimension-specific attention maps are calculated individually by mapping each unit X(p) into a vector. However, the learning ability of the intra-correlation is limited, affecting the effectiveness of the cross-dimensional attention map in modeling relationships among input units. The original attention maps A T, A L, and A M are reshaped to A T, A L, and A M by setting T = L. The original attention maps A T, A L, and A M are reshaped using a matrix structure. The reshape operation is determined by an index mapping function. In the CDSA framework for data forecasting, the time series forecasting task uses current observations to estimate future time series. The CDSA mechanism is applied in the Transformer framework for time series forecasting. In the Transformer framework for time series forecasting, the encoder and decoder are set with N = 9, without using a CDSA module. The decoder in NLP tasks generates translated sequences by predicting word vectors. This process can be applied to forecasting scenarios by using ground truth as input to predict future time stamps. In the Transformer framework for time series forecasting, the decoder is modified to prevent leftward information flow when forecasting future time stamps. The Attention Map on Time is masked to block out values corresponding to leftward flow, and illegal units for future time stamps are masked out as well. The decoder then generates predictions based on the updated values. During testing in the Transformer framework for time series forecasting, the decoder generates predictions based on the updated values. Integrated sampling is used to mitigate discrepancies between input distributions of training and testing, but it leads to exhausted training time due to repeated runs for forecasting different time stamps. During testing in the Transformer framework for time series forecasting, the decoder generates predictions based on the updated values. Integrated sampling is used to mitigate discrepancies between input distributions of training and testing, but it leads to exhausted training time due to repeated runs for forecasting different time stamps. The performance of outputs in the last run (Final) is better than that of Step mode, indicating the presence of leftward information flow despite using a mask on input data. Models are trained in one GPU with training times less than 50 hours. Missing values in the dataset are replaced with the global mean. Comparison between input with Mean Filling and Complemented Input shows no performance improvement with the latter, but an increase in training workload. The encoder and decoder share the same input to reduce memory usage and training time, improving long-term prediction performance. Data aggregation on the KDD 2018 dataset involves selecting common locations and concatenating measurements. Burst loss simulation identifies areas where data is continuously missed for a certain time period. In the analysis, time slots of burst loss were identified with a mean of 6.350773 and standard deviation of 9.809643. The generation of burst loss was modeled as a Gaussian process. Attention maps showed high correlation between measurements like PM2.5 and PM10. The estimation of PM2.5 and PM10 heavily relied on each other, with neighboring locations sharing higher attention weight. The estimation of air quality at different locations relies on available data from neighboring locations. For example, location 11 is mainly dependent on location 8 due to their proximity to expressways. Attention is also given to cross-dimensional data when predicting missing values. Additionally, a comparison of running times for testing segments was conducted based on model hyper-parameters. The study compared the running times of different methods for one testing segment. The Joint method had the highest running time, while Shared had a slightly lower time than Decomposed. Data from NYC-Traffic was used for forecasting, with one segment containing data from the first 20 days and the other from the remaining 3 days. Imputed data from the first segment was used to forecast the second segment. The comparison was done based on different missing ratios. The study compared different methods for forecasting based on missing data ratios. Imputed data was used from the first segment to forecast the second segment, with various statistical and deep learning models evaluated for performance. The proposed model consistently outperformed other methods in terms of RMSE. The dataset in METR-LA has a high missing data rate of 91%, leading to the existence of all-zero samples. These samples have no contribution during training and are not counted in evaluation metrics during testing and validation. Z-score normalization is applied to measurements and missing values are filled with 0. The Adam optimizer is used with a specific learning rate schedule. In KDD-2018, a GAN-based model with content loss is used for imputing missing values in data segments. The experiment evaluates imputation performance using available data as ground truth for content loss calculation. The NYC-Traffic experiment follows KDD-2018 setup with noise replaced by available data. Training, validation, and testing divisions are not used as training loss is not calculated from held available data. The method is also applied to KDD-2015 following Yi et al. (2016); Cao et al. (2018) setup. Entries are manually removed to simulate missing data for PM2.5 imputation. The study evaluates imputation performance by predicting values of manually removed entries for PM2.5, Temperature, and Humidity. Results show that the proposed method outperforms traditional methods for PM2.5 and achieves comparable MAE as IIN, while outperforming state-of-the-art methods for Temperature and Humidity. The method reduces computational complexity significantly and achieves the best performance on KDD-2015 datasets. The study evaluates imputation performance by predicting values of manually removed entries for PM2.5, Temperature, and Humidity. The imputation task assumes completed data has no label, with loss calculated on Available data and evaluation metric on Manually removed data. Data is split into training and testing sets with model structure parameters set."
}