{
    "title": "HymYLebCb",
    "content": "We propose a novel subgraph image representation for classifying network fragments based on 2D image embeddings of adjacency matrices. This representation is used in two modes: as input for machine learning algorithms and as input for transfer learning. Deep learning with structured image features outperforms graph kernel and classical feature-based methods. Transfer learning is effective with minimal user interference and is robust against small data. With the rise of big data, graphical representation of information is increasingly popular. The challenge of classifying graphs is addressed using structured image representations of graphs, as adjacency matrices are not ideal for machine learning. By reordering vertices in a subgraph image, distinguishing properties can be learned effectively. This approach offers a solution to identifying the nature of parent networks based on small network fragments. The structured image representation of subgraphs is used to classify subgraphs in two modes: (i) Deep learning models on the structured image representation as input. (ii) The structured image representation is used as input to a transfer learner in a pure transfer learning setting. Caffe outputs top-k categories that best describe the image, providing human-friendly descriptions for real-world images but not for network-images. The Caffe classification vectors do not have intuitive meaning, so we map them to compute similarity between network images. Our approach transforms graph classification into image classification by proposing an image representation for the adjacency matrix. This representation yields top performance in graph classification and can be used as input for transfer learning in unrelated image domains. The first step is to construct the image representation from a sample subgraph, followed by image embedding and structuring. The workflow involves creating an image representation using a novel method that produces an invariant adjacency matrix. Deep learning classifiers are trained on this representation, outperforming other methods. Transfer learning can be used with this representation when data is limited or labels are missing. The image representation created using a novel method allows for transfer learning between unrelated domains, enabling classification in the graph domain to benefit from techniques in the image domain. The opensource Caffe deep learning framework, trained on ImageNet data, provides pre-trained models for recognizing everyday objects. The Caffe deep learning framework provides a distribution over image classes, which are then mapped back using distance-based heuristics. Good performance can be achieved with as little as 10% of the training data, thanks to transfer learning from unrelated domains. The structured image representation allows for effective classification in real-world networks. Caffe maps structured images to distributions over classes, enabling knowledge transfer to graph classification. The problem of identifying parent networks from subgraphs is approached in supervised and unsupervised settings. Previous work used graph kernels for similarity computation and SVM for classification, but scalability is an issue for large graphs. Comparisons are made with existing methods, and alternative approaches involve constructing feature vectors. One approach involves constructing feature vectors from topological attributes of subgraphs. However, it is challenging to determine a universal set of features that can represent graphs across different domains. Logistic regression and transfer learning methods are compared, with transfer learning being beneficial for leveraging knowledge from related domains. Self-taught learning is introduced as a method that utilizes irrelevant unlabeled data. BID20 introduced self-taught learning to utilize irrelevant unlabeled data for performance improvement. BID30 discussed heterogeneous transfer learning using text data to enhance image classification. BID19 created a new representation from kernel distances to large unlabeled data points for image classification with reference prototypes. The paper details subgraph classification approaches, presents performance results, and suggests future directions. Training samples are used in structured image representation for classifiers in transfer and unsupervised settings. In the unsupervised setting, training samples are fed to Caffe framework to obtain label-vectors. To classify a test subgraph, its label-vector is computed through Caffe, distance is calculated from training vectors, and classification is done based on majority class of nearest-k vectors. An adjacency matrix of a graph is treated as a monochrome image, with a novel technique used for permutation-invariant ordering. The authors suggest sorting methods like page rank and degree-based sorting, with BFS-like approach found to work best. Our approach involves using a novel technique for permutation-invariant ordering of adjacency matrices, resulting in structured images that can be easily distinguished by the human eye. The goal is to classify subgraphs to their parent networks using deep learning, with a focus on image representation. Various methods were tested, with details provided in the Appendix. The methods tested in a supervised learning framework include DBN, CNN, SdA, DCNN, GK, and LR, each using different input representations. Caffe, a deep learning framework, is utilized with a pre-trained model from ImageNet for tasks like image classification and style recognition. The pre-trained model used in this study is based on ImageNet, a dataset with over 10 million annotated images. The model, implemented in Caffe, provides specific image classification results. Despite not being trained on graph embeddings, it can still extract relevant information from images. In this work, the authors utilize the maximally specific categorization option in Caffe, using cardinality-10 vectors in their experiments. They treat label vectors as unordered lists of labels and compute similarity using Jaccard distance. Future work may explore more sophisticated metrics using probabilities from Caffe. The k-nearest neighbor approach classifies test examples by finding the k nearest training vectors using Jaccard distance. It can extend to any number of parent network classes and was applied to various datasets. Deep networks learn signatures from image representations of graph classes, with principal component analysis used to analyze network signatures. Samples were grouped into categories and reshaped for analysis. Our approach leverages the structured image representations of sample 64-node subgraphs from 9 datasets. The deep networks are able to classify the subgraphs well due to the distinctness of the images. We describe our experimental setup and present the results obtained from the two approaches discussed earlier. The graph classification task was performed using 9 parent networks. In the graph classification task, 9 parent networks were used to create 4 datasets with 45,000 samples each. Each dataset was divided into validation and testing sets. The accuracy score was calculated using the confusion matrix, with the best score reported for each classifier. The best accuracy score was achieved by the CNN classifier, while DCNN and GK performed poorly. Off-the-shelf deep learning with graph image features outperformed other methods tested. Higher accuracy was obtained with an increase in the number of nodes per sample. Even with only 8 nodes, the performance was significantly better than random guessing and outperformed graph kernel methods and logistic regression. Logistic regression struggles with feature selection when dealing with graphs from different domains. In a study comparing different classification methods for graphs from various domains, it was found that untuned CNNs outperformed other methods with minimal user effort. Hybrid datasets with more samples of higher complexity showed better classification performance. Graph kernel and feature-based methods performed better than DCNN but not as well as image embedding methods. Kernel methods, known for their complexity and slowness, performed poorly, making them less attractive options. Logistic regression had comparable accuracy scores to other methods. Our approach is completely lossless, preserving all information in the structured image representation of the graph. Unlike LR, which approximates the graph and selects a few features, our method does not compromise on data quality. DCNN is unique among neural network models as it takes a graph as input, requiring both an adjacency matrix and a design matrix containing node information. Our transfer learning approach for graph classification is highly resilient to sparse training data, achieving respectable accuracy with only 10% of the data used for training. The design matrix contains information about each node in the adjacency matrix, including average degree and clustering coefficient. Other properties like assortativity and centrality did not significantly improve performance as they can be calculated from the graph input. The neural network is expected to have learned these features already. Caffe's transfer learning approach for graph classification showed high accuracy with only 10% of the data used for training. The results of the experiments showed accuracy scores in the high 90s and 80s for one set and around 60s for another. The multi-class scores, while not as high as in other experiments, are still noteworthy. The transfer learning approach in Caffe for graph classification demonstrated high accuracy with only 10% of the data used for training. The approach leverages a pre-trained recognition engine in the image domain, making it robust and resilient to sparse training data. The impact of the neighborhood size (k) on accuracy was studied, showing that as long as k > 15, tuning k is not a significant concern. The structured image representation of graphs for classification is robust and lossless, containing all information from the adjacency matrix. Deep network models can predict parent networks with high accuracy even with limited information. The image embedding approach outperforms graph kernel and feature-based methods, and transfer learning from a different domain is utilized for graph classification using pre-trained image classifiers. Our approach converts graphs into 2D image embeddings using a pre-trained image classifier (Caffe) to obtain label-vectors. Accuracies range from 70% to 94% for 2-way classification and 61% for multi-way classification in real-world data sets. The approach is resilient to sparse training samples and shows promise for applications with limited training data, such as terrorist networks. Future work includes improving transfer learning by enhancing the distance function between label-vectors and generalizing the approach to other domains like classifying radio frequency map samples. Convolutional Neural Networks (CNNs) are effective in image classification. Deep Belief Networks (DBNs) consist of multiple layers of Restricted Boltzmann Machines (RBMs) and are trained greedily. The input layer contains nodes for each entry in the feature vector, while the hidden layers consist of RBMs. The output layer returns probabilities for each class label, with the highest probability chosen as the classification. Convolutional Neural Networks (CNNs) are essential for image classification tasks, utilizing convolution layers, ReLU layers, pooling layers, and fully connected layers. Stacked De-Noising Auto-Encoder (SdA) uses a greedy training algorithm to reconstruct noisy inputs, providing a flexible representation of graphical data. DCNNs provide a flexible representation of graphical data by learning diffusion-based representations. A graph kernel approach using the Weisfeiler-Lehman test of isomorphism is utilized for feature extraction. Feature-based classification is performed using logistic regression with 15 classic features computed for each graph sample. The curr_chunk discusses various types of networks including a citation network from Arxiv HEP-PH, a social network from Facebook, a road network of Pennsylvania, and a web network. These networks are characterized by different properties such as node connectivity, edge connectivity, average eccentricity, diameter, average shortest path, average degree, and more. In this network, there are over 1 million nodes and 1.5 million edges. It includes web pages with hyperlinks between them. Another network is a Wikipedia hyperlink graph with 4,604 articles and 119,882 links. Additionally, there is a product co-purchase network of Amazon with over 300,000 nodes and nearly 1 million edges. Lastly, a co-authorship network called DBLP has over 300,000 nodes and 1 million edges. The results show that the performance remains consistent across different mixtures of datasets in the supervised setting. Varying k from 7 to 47 only results in a slight variation in accuracy, as long as k is greater than 15. This demonstrates the robustness of the approach."
}