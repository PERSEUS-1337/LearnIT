{
    "title": "ryl-BQRisQ",
    "content": "The aim of the present work is to enhance label efficiency of large neural networks on audio data through multitask learning with self-supervised tasks. An end-to-end audio feature extractor based on WaveNet was trained to feed into task-specific neural networks. Three self-supervised learning tasks were described to operate on unlabeled audio data, showing significant performance improvement in supervised classification tasks when trained with these additional tasks. Incorporating data augmentation into multitask learning with self-supervised tasks improves performance. The model follows the WaveNet architecture, with task-specific neural networks built on a task-agnostic trunk. The trunk consists of stacked, dilated, and causal convolutions, with outputs fed into task-specific heads. The WaveNet trunk in the model consists of N blocks with S dilated causal convolution layers, each labeled with \"Filter\" and \"Gate\" computations. The layers are indexed within each block, and the computations involve hidden state vectors and layer outputs. The WaveNet trunk consists of N blocks with S dilated causal convolution layers. Each block has an effective receptive field of 1 + b(2 S \u2212 1), resulting in a total receptive field of \u03c4 = 190. Each task-specific head processes input data after passing through the shared trunk. The WaveNet trunk consists of N blocks with S dilated causal convolution layers, resulting in a total receptive field of \u03c4 = 190. Each task-specific head processes input data independently with its own objective function and optimizer. Primary tasks are supervised, while auxiliary tasks are self-supervised. Head architectures are designed to be simple to learn representations suitable for multiple audio tasks. The next-step prediction head in the WaveNet architecture uses a 2-layer stack of 1x1 convolutional layers to predict the next value in a sequence of audio frames. It takes in \u03c4 frames of data from the trunk and treats the prediction as a regression problem, aiming to minimize the mean squared error. The WaveNet architecture treats next-step prediction as a regression problem, using mean squared error as the loss function. Noise reduction is achieved by adding noise to the clean audio waveform. The model is trained to predict the clean sample by minimizing a smoothed L1 loss between the clean and noisy versions of the waveform inputs. The noise reduction head has a structure similar to the next-step head, and a smooth L1 loss is used for stability in convergence. An unsupervised upsampling task can also be created with a virtually identical structure to the denoising task. The study used an upsampling head with a structure similar to the denoising task. They employed a smooth L1 loss function for tasks with manually-verified and non-verified labels. The test set consisted of 1600 audio clips with manually-verified labels from the Librispeech dataset. Models were trained using clips from \"train-clean-100\" or \"train-other-500\" versions, with varying hours of unlabeled data. The duration of each utterance was limited to 2 seconds due to memory constraints. The dataset used for training auxiliary tasks had utterances limited to 2 seconds. The model was trained on raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets using PyTorch framework. Audio samples were cropped to 2 seconds and downsampled to 16 kHz. The model was jointly trained on all tasks simultaneously, with important parameters detailed in the accompanying supplement. Audio tagging was the main task investigated. In this study, audio tagging was the main task investigated. The addition of self-supervised tasks to the baseline model showed an average improvement in MAP@3 score. Training with three additional tasks yielded the best results with a 5.33% improvement over the main task. Convergence in every multitask setting was stable with gradual improvements. The experiments showed stable convergence and gradual improvements in multitask settings. Adding self-supervised tasks significantly improved the main task's performance, even with a fixed amount of unlabeled data. Increasing the amount of unlabeled data for auxiliary tasks further enhanced performance. Increasing the size of unlabeled data for auxiliary tasks improves multitask benefits. MAP@3 scores show progressive improvement with more unlabeled data, peaking at 0.694 with 500 hours, an 8.94% increase over baseline. Data augmentation techniques like noise injection and pitch shifting are compared with multitask learning. Results are shown in TAB2 for data augmentation experiments on test sets. In data augmentation experiments, a peak MAP@3 score of 0.703 was achieved with pitch shifting. Combining data augmentation with multitask learning resulted in the best score of 0.726. Performance improved with increasing amounts of unlabeled data, showing a smooth increase in MAP@3 scores. The performance of supervised tasks improves with additional self-supervised tasks and unlabeled data. A peak performance boost of 8.94% was achieved with 3 self-supervised tasks and 500 hours of unlabeled data. Multitask learning benefits from data augmentation, suggesting additive performance gains. It may be interesting to explore multitask learning with different augmentation approaches. The results are presented within a coherent multitask learning framework."
}