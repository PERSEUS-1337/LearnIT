{
    "title": "BJxX9mx8y7",
    "content": "The task of visually grounded dialog involves learning goal-oriented cooperative dialog between autonomous agents exchanging information about a scene through questions and answers. Humans stick to a common language due to social nature, making it easier to communicate. A multi-agent dialog framework where agents interact and learn from each other results in more relevant and coherent dialog without sacrificing task performance. Intelligent assistants like Siri and Alexa are increasingly important in daily life. Intelligent assistants like Siri and Alexa are becoming crucial in daily life, interacting with each other to achieve goals. Conversations between goal-driven agents are made interpretable through natural language, ensuring transparency and ease of debugging. This paper focuses on goal-driven visual dialog agents, where a Question bot and an Answer bot communicate to form an accurate mental representation of an image. The Q-Bot and Answer bot aim to accurately represent unseen images through question-answer exchanges. They are trained individually and then interact via reinforcement learning to improve task performance. However, their communication becomes non-grammatical and semantically meaningless, reducing transparency. To address this, a multi-agent dialog framework is proposed where agents interact with multiple agents to promote coherent language. The multi-agent dialog system improves communication between Q-Bots and A-Bots, resulting in more coherent and human-interpretable dialog without compromising task performance. The system aims to make agents seem more helpful, transparent, and trustworthy, and the code will be available as open-source. The system focuses on ensuring coherent and informative dialog between agents in a reinforcement learning framework. The goal is to maintain clarity and task performance while avoiding incoherent responses like combining multiple questions or answers into one. The problem of visual dialog is a new area of research introduced by BID1, who created the VisDial dataset for visually grounded dialog. The dataset involves pairing annotators on Amazon Mechanical Turk to chat about an image in a 'multi-round' VQA task. BID2 later proposed a Reinforcement Learning setup for the Question bot and Answer bot to predict unseen images. Our multi-agent framework aims to prevent bots from developing a specialized language and becoming incoherent during dialog. Interleaving supervised training with reinforcement learning helps in generating informative answers for visual dialog tasks. The framework for visual dialog involves training an answer bot to generate informative answers for questions, which are then ranked by a discriminator. This model requires candidate answers for training, limiting it to a supervised setting. BID16 improved upon this by using a GAN framework with an adversarial discriminator distinguishing between human and machine-generated dialogs. The discriminator in the visual dialog framework utilizes an attention network that incorporates dialog history to predict the next answer, ensuring coherence and consistency. The question bot architecture, inspired by BID2, consists of 5 parts including a fact encoder, state-history encoder, caption encoder, image regression network, and question decoder. The fact encoder uses an LSTM network to encode question-answer pairs, while the state-history encoder incorporates hierarchical encoding of the dialog. The architecture includes a fact encoder, state-history encoder, caption encoder, image regression network, and question decoder. A separate LSTM is added to compute a caption embedding, preventing repetitive questions in dialog. The encoder embedding is used to generate questions. The A-Bot model architecture, inspired by Q-Bot, consists of a question encoder, state-history encoder, and answer decoder. It utilizes 512 hidden layer size for LSTMs and fully connected layers, 4096 dimensional image feature vector, and 300 dimensional word embeddings. The history encoding uses a two-level hierarchical encoder with attention computed using the question embedding. The image embedding is computed using the fc7 feature embedding of a pretrained VGG-16 BID13 model. The A-Bot model architecture includes question encoder, state-history encoder, and answer decoder with 512 hidden layer size for LSTMs and fully connected layers. It utilizes 4096 dimensional image feature vector and 300 dimensional word embeddings. The training process involves supervised pretraining for 15 epochs followed by reinforcement learning via a curriculum. The A-Bot model architecture includes question encoder, state-history encoder, and answer decoder with 512 hidden layer size for LSTMs and fully connected layers. It utilizes 4096 dimensional image feature vector and 300 dimensional word embeddings. The training process involves supervised pretraining for 15 epochs followed by reinforcement learning via a curriculum. In the supervised part of training, both the Q-Bot and A-Bot are trained separately using supervision from the VisDial dataset. For the remaining 10-K rounds, they are trained via reinforcement learning. The Q-Bot optimizes minimizing the Mean Squared Error (MSE) loss between true and predicted image embeddings. The Q-Bot generates the next question for the A-Bot based on dialog history. Issues with supervised training include repetitive dialogs, generic responses, and lack of interaction between agents. To address this, self-play is introduced where the bots interact by asking and answering questions using a shared vocabulary. The state space is partially observed and asymmetric. The Q-Bot and A-Bot have partially observed and asymmetric state spaces. They both have action spaces for generating output sequences within a fixed vocabulary. The bots aim to cooperate and incentivize information gain in each dialog round. The Q-Bot also predicts the visual representation of the input image. The reward in dialog rounds is based on the change in image embedding distance. Despite using neural networks for modeling, the RL optimization problem remains ill-posed. To address non-grammatical dialog, a multi-bot architecture is proposed for diverse and coherent interactions. The dialog round at time t involves Q-Bot generating a question, A-Bot answering, both bots encoding the exchange as a fact, and Q-Bot predicting the image representation. The REINFORCE algorithm is used to update network parameters based on rewards, incentivizing informative exchanges. The reward is positive if the image representation is closer to the ground truth than the previous round, promoting information gain in each dialog round. The MultiAgent Dialog architecture aims to prevent agents from developing a private language by encouraging interaction with multiple agents. This approach involves creating multiple Q-bots to interact with a single A-bot, promoting conformity to a common language. The image feature regression network is trained via supervised gradient updates on the L-2 loss. The MultiAgent Dialog architecture involves creating multiple Q-bots to interact with a single A-bot or vice versa. Agents have dialogues consisting of 10 question-answer pairs about images from the VisDial dataset and update their weights based on rewards using the REINFORCE algorithm. Histories are not shared across batches, and the process is repeated for each batch of images. MADF details can be understood using the pseudocode in Algorithm 1, utilizing the VisDial 0.9 dataset. The VisDial 0.9 dataset is used for a task involving dialogues between two annotators about images. The dialogues consist of 10 rounds of questions and answers, with one annotator asking questions based on the image caption and the other answering based on visual information. Experiments were conducted on VisDial v0.9 with 83k dialogs on COCO-train and 40k images. The VisDial v0.9 dataset contains 83k dialogs on COCO-train and 40k on COCO-val images, totaling 1.2M dialog question-answer pairs. The dataset is split into train, validation, and test sets. Evaluation metrics include Mean Reciprocal Rank, Mean Rank, Recall@k, and Image Retrieval Percentile. The Q-bot's prediction accuracy is compared to the ground truth using various evaluation metrics such as Mean Rank, MRR, Recall@k. Our agent architectures outperform previously proposed generative architectures in terms of MRR, Mean Rank, and Recall@10. Our approach outperforms previous generative architectures in MRR, Mean Rank, and R@10, indicating consistently good answers. MADF-trained agents are able to surpass all previous models. Image retrieval scores decrease for SL but remain stable for RL versions. Evaluation of dialog quality is done through ordinal ranking by 20 evaluators. The results of the evaluation show that having multiple A-Bots interacting with the Q-Bot improves dialog coherence significantly. The Q-Bot Relevance rank is best in RL-1Q,3A, while the A-Bot Relevance rank is best in RL-3Q,1A. The MADF-trained dialog systems outperform previous models in dialog quality. The human evaluators found that MADF agents model human responses better than other agents. RL-1Q,1A system has diverse but low-quality responses, while RL-3Q,1A responses are grammatically correct and relevant. SL results in repetitive dialog, with Q-Bot repeating questions and A-Bot giving inconsistent answers. The paper proposes a Multi-Agent Dialog Framework (MADF) to enhance AI agents' dialog quality. Training agents in isolation with supervised learning leads to uninformative dialog. Allowing agents to interact and learn from each other via reinforcement learning improves their performance significantly. The Multi-Agent Dialog Framework (MADF) enhances AI agents' dialog quality through reinforcement learning. Agents develop their own private language, leading to non-grammatical and nonsensical statements, causing a loss of interpretability and sociability in the dialog system. The Multi-Agent Dialog Framework (MADF) improves dialog quality by allowing a single A-Bot to interact with multiple Q-Bots, preventing the development of a private language. Human evaluation shows significant enhancements in relevance, grammar, and coherence. Future plans include exploring different multi-bot settings, incorporating perplexity-based rewards, and experimenting with a discriminative answer decoder. The current work focuses on ranking generated answers and training the answer decoder using ranking performance. Future plans involve using richer image feature embeddings, implementing an image generation GAN, and exploring incorporating multiple Q-Bots and A-Bots in the framework. The Q-Bot and A-Bot are trained by maximizing likelihood of training data and minimizing Mean Squared Error loss. The A-Bot's question encoder is fed with ground-truth questions and history, while the image encoder is fed with QA pairs and image. The A-Bot is trained to maximize likelihood of training data. The Q-Bot receives only the caption, while the A-Bot receives the image and caption as inputs. The Q-Bot and A-Bot are trained by maximizing likelihood of training data and minimizing Mean Squared Error loss. The A-Bot's question encoder is fed with ground-truth questions and history, while the image encoder is fed with QA pairs and image. The A-Bot is trained to maximize likelihood of training data. The Q-Bot receives only the caption, while the A-Bot receives the image and caption as inputs. At each time step, the Q-Bot's fact encoder, state/history encoder, and caption encoder generate embeddings used by the feature regression network and question decoder to produce outputs. The Q-Bot is trained via REINFORCE using rewards based on the change in distance between predicted and ground truth values, while also minimizing Mean Squared Error loss. At time step t, the A-Bot's question encoder is fed with the generated question q t, state/history/image encoder with all QA pairs up to t-1 and the image I. Encoders generate embeddings fed into answer decoder to produce a t. A-Bot receives same reward as Q-Bot, trains via REINFORCE, minimizing loss DISPLAYFORM0 with Monte-Carlo return at step t."
}