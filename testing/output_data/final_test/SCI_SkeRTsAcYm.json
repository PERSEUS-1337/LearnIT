{
    "title": "SkeRTsAcYm",
    "content": "Most deep learning-based models for speech enhancement focus on estimating the magnitude of spectrogram and reusing the phase from noisy speech for reconstruction. To improve performance, the phase estimation problem is tackled through a Deep Complex U-Net model, a polar coordinate-wise complex-valued masking method, and a weighted source-to-distortion ratio (wSDR) loss function. The model was evaluated on a mixture of the Voice Bank corpus and DEMAND database. The proposed method for speech enhancement, evaluated on a mixed dataset, achieves state-of-the-art performance, surpassing previous approaches significantly. Deep learning has greatly improved the performance of speech enhancement tasks, crucial for various applications from speech recognition to hearing aid systems. When using audio signals in deep learning models, transforming time-domain waveforms to spectrograms via STFT is common. Spectrograms are decomposed into magnitude and phase components for tasks like speech enhancement. Neglecting complex-valued phase in favor of reusing noisy phase information has limitations, especially in low SNR conditions. Speech enhancement techniques often optimize masks to reconstruct clean speech from noisy input audio. The complex-valued ratio mask (cRM) has shown promise for phase estimation, outperforming other ideal masks like the phase-sensitive mask (PSM). This approach is particularly effective in low signal-to-noise ratio (SNR) conditions. The Deep Complex U-Net (DCUnet) framework is designed to estimate a complex ratio mask for speech enhancement. It utilizes deep learning building blocks adapted to complex arithmetic and is trained to estimate clean speech in polar coordinates. This approach eliminates the need for separate models and can convert spectrograms into time-domain waveforms using inverse short-time-Fourier-transform (ISTFT). The Deep Complex U-Net introduces a novel loss function optimizing source-to-distortion ratio (SDR) and proposes a new neural architecture combining deep complex networks and U-Net for improved performance. It also designs a complex-valued masking method based on polar coordinates and a weighted-SDR loss function. Phase estimation for audio signal reconstruction is a key focus in audio source separation. Neural network-based methods like the Griffin-Lim algorithm and end-to-end models are being explored for phase estimation in audio signal reconstruction. Other approaches include separate neural network modules for magnitude and phase estimation, as well as using additional layers with trainable discrete values for phase estimation. The cRM approach for joint magnitude and phase estimation has limitations in previous studies, which will be addressed in Section 3. Complex-valued networks have shown potential in improving singing voice separation performance. The curr_chunk discusses the limitations of previous studies in switching real-valued networks to complex-valued ones for phase estimation. It introduces the Deep Complex U-Net model, a masking framework, and a new loss function for phase optimization in speech signal separation. The curr_chunk introduces the Deep Complex U-Net model, a modified version of the U-Net structure using complex building blocks for handling complex domain operations in speech signal separation tasks. The Deep Complex U-Net model is a refined version of the U-Net architecture for speech signal separation tasks, utilizing complex-valued building blocks for convolution operations. Complex convolutions are implemented as two real-valued convolutions with shared filters, and activation functions like CReLU are used for best results. Details on batch normalization and weight initialization can be found in BID28. The Deep Complex U-Net model enhances the original U-Net by using complex convolutional layers with Glorot's criteria for weight initialization. Complex batch normalization is applied, and max pooling is replaced with strided complex convolutional layers in the encoding stage. Strided complex deconvolutional operations are used in the decoding stage. The activation function is modified to leaky CReLU for stability during training. All experiments in Section 4 are conducted with these modifications. The proposed model in Section 4 can handle complex values and aims to estimate cRM for speech enhancement. Real-valued ratio masks only change magnitude scale, while cRM correct phase errors by performing a rotation on polar coordinates. The estimated speech spectrogram is computed by multiplying the estimated mask on the input spectrogram. The real and imaginary values of the estimated cRM are unbounded, which poses a problem. The estimated cRM values are unbounded, making optimization challenging due to the infinite search space. Techniques have been attempted to bound the range of cRM, such as directly optimizing a complex mask into a cIRM compressed to a heuristic bound. However, this method often leads to performance degradation. Another approach involves a rectangular coordinate-wise cRM with sigmoid compressions, followed by computing the MSE between clean source Y and estimated source \u0176 in the STFT-domain for training the model. The proposed masking method in the STFT-domain for training the model has issues with phase estimation. To address this, a polar coordinate-wise cRM method is suggested, applying nonlinearity to the magnitude part to bound it within [0, 1). This allows for a unit-circle bound in complex space, with the phase mask obtained by dividing the model output by its magnitude. The proposed complex-valued maskM t,f is estimated using a cRM method. A popular loss function for audio source separation is mean squared error (MSE) in the STFT-domain. However, optimizing with MSE fails in phase estimation due to phase randomness. Using a loss function in the time-domain with inherent phase information is suggested. An improved loss function, weighted-SDR loss, is proposed to optimize the source-to-distortion ratio. The original loss function for optimizing source-to-distortion ratio (SDR) has flaws, including fluctuation in loss values and inability to learn from noisy-only data. To address these issues, the loss function was redesigned with modifications to improve performance. The loss function was redesigned to improve performance by making modifications to Equation 5. This included making the lower bound independent of the source y, adding a noise prediction term, and balancing the contributions of each loss term based on the energy of each signal. The final form of the suggested weighted-SDR loss function was introduced to address issues with the original loss function. The loss function was redesigned to improve performance by modifying Equation 5, including making the lower bound independent of the source y and adding a noise prediction term. STFT and ISTFT operations are implemented as 1-D convolution and deconvolution layers with fixed filters initialized with the discrete Fourier transform matrix. The proposed loss function details are in Appendix C. Experimental setups used the same settings as previous works for direct performance comparison. Mixed audio inputs for training were created by mixing noise and clean speech recordings from specific datasets with different signal-to-noise ratio settings and noise types. The study conducted experiments using various noise levels and speakers to create conditional patterns for speech samples. Pre-processing involved downsampling raw waveforms and obtaining complex-valued spectrograms. Implementation was done using the NAVER Smart Machine Learning platform. The speech enhancement performance of the method was compared with other algorithms. The study compared the performance of their method with Wiener filtering and various deep-learning models for speech enhancement. The models included SEGAN, Wavenet, MMSE-GAN, Deep Feature Loss, and DCUnet-20. The study also showcased results from a larger DCUnet-20 model. The study compared the performance of their method with Wiener filtering and various deep-learning models for speech enhancement, including SEGAN, Wavenet, MMSE-GAN, Deep Feature Loss, and DCUnet-20. Results show that the proposed method outperforms previous state-of-the-art methods by a large margin, especially with larger models like DCUnet-20. The phase estimation quality of the method is identified as a key factor for the significant improvement. Different masking strategies and loss functions were evaluated, with the BDT mask showing better results in DCU-10 and DCU-16, while UBD mask performed better in DCU-20. The study compared the performance of their method with Wiener filtering and various deep-learning models for speech enhancement. Results show that the proposed method outperforms previous state-of-the-art methods, especially with larger models like DCUnet-20. Different masking strategies and loss functions were evaluated, with wSDR loss giving the best results overall. Validation on complex-valued network and mask showed the effectiveness of complex neural networks. The study evaluated the effectiveness of phase estimation using complex-valued spectrograms and different masking strategies. Models were trained with weighted-SDR loss and varying parameters to show consistent results. Evaluation results in TAB3 showed that the cRMCn approach outperformed conventional methods, highlighting the importance of phase correction and the benefits of using complex-valued networks. The study found that using complex-valued networks consistently improved network performance. Qualitative evaluations were conducted by comparing denoised audio samples from different algorithms in listening tests with 30 participants. The study showed that complex-valued networks improved network performance. Listening tests with 30 participants revealed that DCUnet outperformed other methods in preference scores across all SNR conditions. The results were statistically significant, with the combination of proposed methods showing the best fit in distribution patterns of estimated cRMs. The study demonstrated that accurate phase estimation is crucial, especially in harsh noisy conditions. Visualizations and quantitative analysis supported the effectiveness of the proposed method in correcting phase errors in noisy speech. The phase distance between target and estimated spectrograms is defined as the weighted average of angles between corresponding complex TF bins, emphasizing the importance of each bin. Results show that the best phase improvement is achieved with wSDR loss, while Spc loss yields the worst results. In this paper, the Deep Complex U-Net model is introduced for speech enhancement using complex-valued spectrograms. A new complex-valued masking method with weighted-SDR loss is proposed for precise phase estimation, leading to state-of-the-art performance. The method outperforms previous algorithms in both quantitative and qualitative studies. Future plans include applying the system to speaker separation and extending it to handle multichannel tasks. The proposed model for speech enhancement using complex-valued spectrograms includes a new complex-valued masking method for precise phase estimation. It can be extended to handle multichannel audio tasks and other audio-related tasks like dereverberation and phase estimation networks for text-to-speech systems. The complex-valued convolution is explained as two real-valued convolution operations with shared parameters. The complex-valued convolution has double the parameters of a real-valued convolution. To ensure a fair comparison, a real-valued network and a complex-valued network with the same number of parameters were built. Different model architectures are described in FIG7, 8, 9, and TAB0, with both networks having the same size of convolution filters but different channel numbers. The complex-valued network in TAB0 follows convolution with batch normalization and activation functions. The weighted-SDR loss properties are discussed, including bounded range and conditions for minimum value. Gradients in noise-only input case are explained. FIG0 shows encoder and decoder block details. The convolution filter sizes, stride sizes, and number of channels in complex-valued and real-valued networks are discussed. The loss function wSDR is defined, with its properties and bounds explained. The minimum value of the loss function is discussed in relation to target and noise energy ratios. The minimum value of the loss function wSDR is attained when \u0177 = y and x = cy for all c in R. This is achieved by satisfying the equality condition of the Cauchy-Schwarz inequality, leading to t1 = 1 and t1 = t2. The network can learn from noise-only training data by adding a small number to the denominators of the equation. Equation 9 is modified by adding a small number to the denominators. The loss function has a non-zero gradient even when the target source is empty. Two irreducible errors are illustrated: phase error due to lack of phase estimation and error induced by bounding the mask range. Scatter plots of cIRM from the training set are shown based on SNR values. The scatter plots show TF bins dominated by noise or source based on SNR values. Higher SNR results in more bins dominated by clean source. Phase of estimated speech is visualized. The importance of phase estimation in speech processing is highlighted, especially under low SNR conditions. Group delay analysis reveals hidden structures in speech signals, showing similarities between clean and noisy speech. The estimated phase by the model demonstrates harmonic patterns similar to clean speech, emphasizing the limitations of conventional approaches without phase estimation. Estimation of phase information is crucial under low SNR conditions. When SNR is low, phase estimation becomes more important as it allows for improvement due to higher irreducible phase error. This is illustrated in FIG4 and supported by data in TAB6. In high SNR, source dominance makes ground truth estimation easier, while in low SNR, source is not dominant leading to higher irreducible phase error. The importance of phase estimation is highlighted under low SNR conditions, where higher irreducible phase error is likely. Empirical data in TAB8 shows a correlation between phase improvement and performance difference between conventional method and proposed method. Results indicate that in low SNR, both phase improvement and performance difference are relatively higher. The study emphasizes the significance of phase estimation in low SNR conditions, where there is a higher irreducible phase error. Results show that both phase improvement and performance difference are relatively higher in low SNR conditions, supporting the assumption. Pairwise preference scores of different models, including DCUnet, are provided in Table 7, with significance levels indicated."
}