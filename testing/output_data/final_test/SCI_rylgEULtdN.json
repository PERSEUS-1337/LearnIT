{
    "title": "rylgEULtdN",
    "content": "Recent advances in deep generative models have led to significant progress in synthesizing high-quality images. The next challenge is to apply these models to videos, which requires capturing both temporal dynamics and visual presentation. However, current progress in generative models of video is hindered by the lack of qualitative metrics. A new metric called Fr\u00e9chet Video Distance (FVD) has been proposed to address this issue, based on FID. A human study confirmed that FVD aligns well with human judgment of generated videos. Video generation models are a challenging task as they need to capture temporal dynamics and visual presentation. While progress has been made, models struggle to synthesize more than a few seconds of video. Learning a good dynamics model remains a major challenge in generating real-world videos. Qualitative metrics are needed to measure progress in video synthesis. To measure progress in video synthesis, new metrics like Fr\u00e9chet Video Distance (FVD) are introduced. FVD considers visual quality, temporal coherence, and diversity of generated samples, unlike traditional metrics like PSNR or SSIM. Extensive experiments are conducted to evaluate FVD's effectiveness in assessing generative models of video. A generative model of videos must capture the underlying data distribution with which the observed data was generated. The distance between the real world data distribution and the distribution defined by the generative model is a key evaluation metric. The popular Fr\u00e9chet Distance between these distributions is difficult to solve for in the general case, but has a closed form solution when they are multivariate Gaussians. The Fr\u00e9chet Distance between real world data distribution and generative model distribution has a closed form solution when they are multivariate Gaussians. Using a learned feature embedding, the distance is calculated by feeding samples through a trained Inception network and computing the Fr\u00e9chet Inception Distance using means and covariances. The feature representation learned by the pre-trained neural network greatly affects the quality of the metric. Different layers of the network encode features at different abstraction levels. To obtain a suitable feature representation for videos, a pre-trained network considering temporal coherence is needed. The Inflated 3D Convnet (I3D) is a variation of the Inception architecture for sequential data, trained to perform well. The I3D network, a variation of the Inception architecture for sequential data, excels at action recognition on the Kinetics dataset. Two different feature representations (logits, avg. pool) learned by I3D networks pre-trained on Kinetics-400 and Kinetics-600 are explored. Using Maximum Mean Discrepancy (MMD) as an alternative to estimating Gaussian distributions over the feature space is considered. MMD is a kernel-based approach that calculates the distance between empirical distributions without assuming a specific form. Bi\u0144kowski et al. (2018) proposed using a polynomial kernel to calculate the Kernel Video Distance (KVD) from features of the I3D network. FVD is introduced as a general metric for videos, with code available for computation. A study on FVD's sensitivity to sample size and resolution, as well as experiments on adding noise to videos, are presented. The study introduced FVD and KVD metrics for videos, comparing distortions in different datasets using various embeddings. The results showed that all variants could detect injected distortions to some extent. The study compared different video distortion detection variants, with the pre-trained Inception network performing poorly on temporal distortions. The I3D model pre-trained on Kinetics-400 had the best correlation with noise intensities. Human judgment was used to evaluate the visual fidelity of generated videos in different scenarios. Results of human evaluation studies show that FVD outperforms other metrics in determining video quality. FVD is able to distinguish between models when other metrics cannot, aligning well with human judgment. The Fr\u00e9chet Video Distance (FVD) is a new evaluation metric for generative models of video, outperforming other metrics like SSIM and PSNR in aligning with human judgment. FVD accurately evaluates videos with static and temporal noise, as confirmed by experiments and a large-scale human study. The study applied various types of noise distortions to video frames, including Gaussian noise and Salt & Pepper noise, as well as temporal noise by swapping frames locally and globally, interleaving frames from different videos, and switching between videos. These distortions were applied at different intensities unique to each type. The noise study was conducted on HMDB, correlating noise intensity with the Fr\u00e9chet Video Distance (FVD) metrics. The study focused on applying various noise distortions to video frames on HMDB to analyze the correlation between noise intensity and FVD metrics. The accuracy of FVD in calculating the distance between generated video distributions and target distributions was discussed, emphasizing the importance of sample size in obtaining accurate estimates for \u00b5 and \u03a3. The impact of noise on estimation processes for FVD was compared to that of FID. The FVD metric is used to compare video distributions, even when distributions are identical, FVD will typically be larger than zero due to noisy parameter estimates. It is crucial to use the same sample size when comparing FVD values across models. Over 3,000 different models were tested on the BAIR dataset, generating videos by combining 2 frames of context with 14 output frames. PSNR and SSIM scores were obtained by generating 100 videos for each input context. We obtain PSNR and SSIM scores by generating 100 videos for each input context and selecting the best frame-averaged value. Human studies are conducted on different subsets of trained models, considering models indistinguishable by a single metric. Ten models with similar metric values close to the best quartile are chosen to evaluate human raters' ability to distinguish video quality. In a second setting, we select models with similar metric values to evaluate human judgment on video quality. By choosing models with clear differences in video quality, we aim to assess the agreement between metrics and human perception. The study compared human ratings of video quality with ratings from various metrics. FVD was found to be superior in judging subjective video quality. The results of human evaluation for KVD and Avg. FID metrics were also included in the analysis. The study compared human ratings of video quality with ratings from various metrics. FVD was found to be superior in judging subjective video quality. Avg. FID performs worse compared to FVD in most scenarios, except on SSIM where it achieves slightly better performance. KVD is highly correlated with FVD but slightly worse in terms of agreement with human judgment. Overall, FVD is the superior choice compared to all other metrics tested. The study found that FVD is the best metric for judging subjective video quality compared to other metrics tested. Results from FVD and spr. FVD columns are crucial for user experience. Table 3 also shows agreement among raters in comparing generated videos. The reproducibility of FVD results is high, but the significance of small differences in FVD needs further investigation. The study found that FVD is the best metric for judging subjective video quality compared to other metrics tested. Human raters compared videos with varying FVD points to determine meaningful differences. Results showed that differences of 50 FVD points or more typically correspond to noticeable changes in video quality. Differences of 50 FVD points or more typically correspond to noticeable changes in video quality, as shown by the fraction of human raters agreeing with FVD on which model is better."
}