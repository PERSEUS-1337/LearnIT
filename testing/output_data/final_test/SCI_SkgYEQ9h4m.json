{
    "title": "SkgYEQ9h4m",
    "content": "Convolutional neural networks (CNNs) excel in recognizing and representing audio, images, videos, and 3D volumes with regular graph structures. In this work, convolutional autoencoders are generalized to mesh surfaces by performing spectral decomposition and applying convolutions directly in frequency space. Max pooling and upsampling are used to represent meshes in a low dimensional space. Despite limited training data, the Convolutional Mesh Autoencoder outperforms PCA models of faces with lower error and fewer parameters. In recent years, CNNs have shown success in generating images and audio due to their multi-scale hierarchical structure. Despite this success, CNNs have mainly been used in Euclidean domains with grid-based data, such as images, videos, text, and audio. Extending CNNs to irregular structures like graphs and meshes has recently gained attention. The text discusses the introduction of a convolutional mesh autoencoder architecture for representing high-dimensional meshes of 3D human faces. It highlights the variability of human faces due to factors like age, gender, and ethnicity, as well as the challenge of capturing non-linear facial expressions. The use of CNNs is proposed as a more efficient method compared to linear models, despite the limited size of current 3D facial datasets. The text introduces a convolutional mesh autoencoder for 3D facial data, emphasizing the challenge of limited datasets. CNNs with fewer parameters than linear models can effectively learn with limited data due to locally invariant convolutional filters. Recent work has utilized thousands of 3D scans for detailed models of 3D faces, enabling a rich non-linear representation not easily captured by linear models. The mesh autoencoder includes mesh downsampling, upsampling layers, and fast localized convolutional filters on the mesh surface. The text introduces a mesh autoencoder for 3D facial data, emphasizing the challenge of limited datasets. The autoencoder uses fast localized convolutional filters on the mesh surface and outperforms linear PCA models with fewer parameters. It provides detailed 3D meshes for facial expressions and offers a new tool for 3D mesh modeling. The work aims to apply CNNs to graphics problems involving 3D meshes. Mesh Convolutional Networks provide a comprehensive overview of generalizations of CNNs on non-Euclidean domains like meshes and graphs. Different methods have been introduced to define mesh convolutions, including parameterizing local patches using geodesic polar coordinates, anisotropic heat kernels, and d-dimensional pseudo-coordinates. Other approaches involve using Gaussian kernels with trainable mean vector and covariance matrix as weight functions, as well as dynamic filtering on graphs where filter weights depend on input data. These methods aim to enhance the application of CNNs to graphics problems involving 3D meshes. The work focuses on generalizations of CNNs on non-Euclidean domains like meshes and graphs. Various methods have been proposed for mesh convolutions, but they do not reduce the dimensionality of the meshes. The proposed mesh autoencoder combines mesh convolutions with mesh downsampling and upsampling operators to efficiently handle these issues. The text discusses the generalization of CNNs on graphs by utilizing the graph Laplacian and Fourier basis to create spectral filters for graph convolutions. Different methods have been developed to simplify spectral graph convolutions, such as using truncated Chebyshev polynomials and efficient graph pooling. However, these techniques are not directly applicable to 3D meshes. The proposed mesh autoencoder is similar to previous work using truncated Chebyshev polynomials and efficient graph pooling, with the addition of a mesh upsampling layer. The text introduces a mesh autoencoder for 3D faces, achieving state-of-the-art results in realistic modeling. It discusses various methods for representing facial expressions, including linear spaces, PCA, and multilinear models. The text discusses different methods for representing facial expressions, such as sparse linear models and hierarchical multiscale approaches. Unlike existing methods, the mesh autoencoder uses convolutional layers to represent faces with fewer parameters, avoiding memory constraints of volumetric convolutional methods. The text introduces a graph representation with vertices and edges in 3D space, using a sparse adjacency matrix to denote connections. The non-normalized graph Laplacian and graph Fourier transform are defined, enabling fast spectral convolutions. Mesh filtering with a kernel is addressed using recursive Chebyshev polynomials to overcome computational expenses. The text discusses filtering with a kernel using recursive Chebyshev polynomials for mesh sampling in a neural network. The filter is parametrized as a Chebyshev polynomial of order K, with spectral convolution defined for the decoder architecture. Mesh sampling operators handle the downscaling and upscaling of mesh features in the network. The downsampling and upsampling of mesh features in a neural network are achieved using transform matrices. Downsampling involves contracting vertex pairs iteratively while maintaining surface error approximations. Loss-less upsampling is not feasible, so the upsampling matrix is built during downsampling. Vertices kept during downsampling are preserved during upsampling, while discarded vertices are mapped into the downsampled mesh surface. The weights in Q u are updated based on w i, w j, w k, and 0 otherwise. Downsampling and upsampling affect 3D face meshes, with finer details lost during upsampling. The encoder in the neural network consists of Chebyshev convolutional filters and downsampling layers, transforming the face mesh into an 8-dimensional latent vector. The decoder in the mesh autoencoder transforms an 8-dimensional latent vector to reconstruct the mesh in R 8192\u00d73. It consists of 4 convolutional layers with upsampling layers, increasing vertices by 4x. The autoencoder is trained for 300 epochs with stochastic gradient descent and L1 loss optimization. The dataset used for the study consists of 12 classes of extreme facial expressions from 12 different subjects, captured with a multi-camera active stereo system. The dataset contains 20,466 3D Meshes with 120,000 vertices each, pre-processed to reduce dimensionality. The data is preprocessed by adding fake vertices to increase dimensionality to 8192 vertices, enabling pooling and upsampling. Implementation details include using Tensorflow for network implementation and Scikit-learn for computing PCA coefficients. Training each network takes about 8 hours on a single Nvidia Tesla P100 GPU, with models trained for 300 epochs. Performance evaluation is based on interpolation and extrapolation abilities compared to a PCA model. The encoded latent vectors lie in R 8, with a smaller number of parameters in the Mesh Autoencoder model compared to the PCA model. The autoencoder model outperforms the PCA model by up to 50% in capturing non-linear facial expressions, with 75% fewer parameters in the CNN. The mean Euclidean distance and median error are evaluated on test samples split from the dataset, showing significant improvement in performance. The Mesh Autoencoder model outperforms the PCA model in capturing facial expressions with 75% fewer parameters in the CNN. Results show more realistic reconstructions and better capture of extreme facial expressions. Generalization capability is evaluated by comparing performance with a PCA model and FLAME BID29. The Mesh Autoencoder model demonstrates superior generalization capability compared to PCA and FLAME BID29 in reconstructing unseen facial expressions. Through 12 cross-validation experiments, the model consistently outperforms the other methods in capturing extreme expressions, as shown in Table 5 and FIG3. Our model outperforms PCA and FLAME BID29 in capturing extreme facial expressions. Mesh Autoencoder captures 84.9% of vertices for 2 mm accuracy, while PCA captures 73.6%. We evaluate our model by replacing the expression model of FLAME with our autoencoder and varying the latent vector size. Our convolutional Mesh Autoencoder generalizes better for unseen 3D faces with fewer parameters but is limited to a fixed topology. The adjacency matrix A is crucial for mesh sampling layers but does not consider vertex positions, impacting sampling performance. Future plans include incorporating this information into the learning framework. Data scarcity for high-resolution faces limits Mesh Autoencoder performance, suggesting the need for more diverse data. The Mesh Autoencoders for 3D face representations aim to improve performance by addressing data scarcity and noise in reconstructions. A generalization of convolutional autoencoders for mesh surfaces with downsampling and upsampling layers, along with localized convolutional filters in spectral space, reduces filter parameters. Evaluation on a dataset of extreme facial expressions shows promising results. Our Mesh Autoencoder surpasses linear PCA models by 50% in interpolation experiments and performs better on unseen facial expressions, capturing surface details missed by linear models. This direction in modeling high-dimensional face meshes can benefit various computer graphics applications, especially in capturing detailed static and dynamic facial expressions."
}