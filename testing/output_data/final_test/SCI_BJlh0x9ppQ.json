{
    "title": "BJlh0x9ppQ",
    "content": "Knowledge bases (KB) are represented as a collection of facts in the form (HEAD, PREDICATE, TAIL), where HEAD and TAIL are entities linked by a binary relationship. Research focuses on KB completion methods, including link prediction for incomplete repositories. Numerical facts in KBs link entities to numerical values via numerical predicates, suffering from incompleteness. To address this, the numerical attribute prediction problem is introduced, where queries involve numerical predicates and the answer is a numerical value. This helps explain the relational structure of entities. Knowledge bases (KBs) contain facts in the form (head, predicate, tail) representing relationships between entities. Numerical values associated with entities help explain the relational structure of the knowledge base. Knowledge base embedding methods are used to predict numerical attributes, outperforming baselines in experiments on FREEBASE and YAGO datasets. KBs play a crucial role in AI applications like recommender systems and chatbots. Knowledge bases (KBs) are essential for AI applications like recommender systems and chatbots. Research on knowledge base completion methods, such as relationship extraction and knowledge graph matching, has been ongoing due to the incompleteness of KBs. Link prediction methods have gained more attention recently in addressing this issue. Link prediction methods have gained more attention recently in addressing the incompleteness of knowledge bases. These methods aim to find missing links between entities based on existing information in the KB by ranking answer candidates for queries. Recently, numerical attributes have been integrated to improve link prediction performance. In addressing the incompleteness of knowledge bases, link prediction methods aim to find missing links between entities by ranking answer candidates for queries. Numerical attributes have been integrated to improve performance, focusing on completing queries with numerical predicates where the answer is a numerical value. Examples include queries like (Apple Inc., revenue, ?) or (California, average). The paper introduces the problem of predicting numerical attributes in knowledge bases, creating benchmark datasets and proposing baselines for this regression problem. The paper proposes supervised and semi-supervised approaches for predicting numerical values in knowledge bases, outperforming baselines in all datasets. It discusses related work, formalizes the problem, describes approaches and baselines, reports experimental settings, and presents results. The paper discusses approaches for predicting numerical values in knowledge bases, outperforming baselines in all datasets. It focuses on logical rules and KB embedding methods for generating feature representations used in machine learning models. Entity-type classification, related to link prediction, has received less attention in research. The paper discusses predicting numerical values in knowledge bases, emphasizing logical rules and KB embedding methods for machine learning models. It highlights the similarities between link prediction and entity-type classification, with a focus on relational information for KB completion. Additionally, it mentions the difference between link prediction and numerical attribute prediction, as well as the concept of value imputation in statistics. The technique of mean imputation is commonly used for value imputation in statistics, altering the variable distribution to be more peaked at the mean. Regression imputation is another popular approach where missing values are estimated using a regression model. Some research has explored using text to predict numerical attributes of entities, leveraging Word2Vec embeddings for regression models. This approach differs from assuming the existence of text information for prediction. The curr_chunk discusses the importance of utilizing structural information in knowledge bases (KBs) for value imputation. A KB is defined as a collection of entities and relation types, with the addition of numerical attributes. This enriched KB, denoted as GNA, associates entities with numerical values via numerical predicates. The approach aims to fill missing values in KBs while leveraging the structural information provided by the KB. The curr_chunk discusses the prediction of numerical attribute values in a knowledge base by learning a function to fill missing values. It suggests utilizing a generative model to determine the relational structure of the KB based on known numerical values. The curr_chunk proposes using a model to determine the relational structure of a knowledge base based on numerical attributes. It suggests that a model exploiting graph structure information may outperform simple value imputation methods. The baseline method discussed is mean imputation using the sample mean of attribute-specific training data as a predictor for missing values. The curr_chunk discusses mean and median imputation methods for predicting missing values in numerical predicates. The Global model assigns the sample mean of known entities with the same attribute, while median imputation is used for MAE evaluation. The curr_chunk discusses replacing the sample average with the median in a relational graph structure for node classification. The weighted-vote relational neighbor baseline estimates a node's class label as a weighted average of its neighbors' labels. This adaptation estimates a numerical attribute value for an entity as the average of its neighbors' attribute values. The neighborhood of a node is defined as the set of nodes connected to it through any relation type. The curr_chunk discusses using local and global baselines in a relational graph structure for predicting numerical attributes. Knowledge base embeddings are leveraged to learn feature representations of entities for prediction. The curr_chunk discusses using TransE as a KG embedding method to train machine learning models for predicting numerical attributes. The methodology is agnostic to the chosen KG embedding method, and the scoring function used is g(d | \u03b8) = ||h + p \u2212 t|| 2. The curr_chunk explains the use of negative sampling in TransE for KG embedding. It involves generating N triples by sampling entities randomly. Parameters are learned to minimize the logarithmic loss with stochastic gradient descent. The embeddings are then used for numerical attribute prediction with regression models. In this work, the learned feature representations are used for numerical attribute prediction with a linear regression model. The prediction is computed by applying the regression function to the entity. Entities with known numerical attributes are defined as E a, while entities with missing values are defined as Q a. Numerical attribute values are considered as labels in this approach. The text discusses the use of label propagation for semi-supervised learning in numerical attribute prediction. It mentions the creation of a k-nearest neighbor graph using learned representations to propagate numerical attribute information across the graph. The adjacency matrix A represents similarities between entities using a similarity metric \u03c1. The transition matrix T is computed by normalizing A and can propagate numerical information across the graph iteratively. The problem can also be solved in a closed form using vectors containing numerical attribute values for labeled and unlabeled nodes. Numerical Attribute Propagation (Nap) involves using predicted values of numerical attributes for unlabeled nodes. Previous work used label propagation for link prediction in web ontologies. The feature representations learned by models like TransE are assumed to be meaningful for predicting numerical attributes. However, there may be cases where numerical attribute values do not fully relate to the relational structure of the knowledge base. This motivates the question of whether these models can accurately predict numerical attributes. To answer the question of whether models like TransE can benefit from incorporating numerical attribute information, a new approach called TransE++ is introduced. This approach combines the learning objective of TransE with numerical attribute information, using stochastic gradient descent to learn all parameters. Different numerical attributes with varying scales can lead to prediction errors during training. To address prediction errors during training in TransE++, numerical attribute values are normalized to zero-mean and unit-variance. Min-max scaling was also tested but showed worse performance. Instead of using regression models from TransE++, new regression models are trained for each numerical attribute to avoid computational difficulties in tuning hyperparameters. This approach is crucial for achieving good performance due to the exponential growth of the hyperparameter space with the number of attributes. The parameter space grows exponentially with the number of attributes in learning TransE++ embeddings. The proposed methods are evaluated on completion queries using benchmark datasets FB15K-237 and YAGO15K. The FB15K-237 dataset contains 29,395 numerical facts divided into 116 predicates, evaluated on the top 10 numerical attributes. This reduces the dataset to 22,929 samples. The datasets FB15K-237 and YAGO15K are split into training, validation, and test sets. The training data includes numerical facts for learning knowledge base embeddings. Evaluation is done on queries with numerical answers using MAE and RMSE metrics. The datasets and splits will be publicly available for future comparisons. The evaluation metrics MAE and RMSE are used in regression problems. For TransE and TransE++, the embedding dimension is fixed at 100. The weight \u03b1 of TransE++ is set to 1. Adam is used for parameter learning with a learning rate of 0.001. The number of epochs is 100 with a mini-batch size of 256. The negative sampling parameter N is set to 50. Parameters are initialized following a specific method. Ridge regression is implemented using Scikit-learn for Lr and Lr++. The regularization term \u03bb is tuned using specific values for Nap and Nap++. The regularization term \u03bb is tuned using specific values for Nap and Nap++. The number of neighbors (k) and \u03c3 of the RBF kernel are validated for each numerical predicate and evaluation metric. Performance of approaches is compared to baselines, with Nap++ or Nap often outperforming. Global outperforms Local for 'location.area' and 'population.number' attributes. Global outperforms Local for 'location.area' and 'population.number' attributes, indicating a disconnect in the data set's relational structure. Local performs well in predicting 'latitude' and 'longitude' due to specific predicates in the graph. Neighborhood information is beneficial for predicting attributes like 'date of birth' and 'date of death'. All approaches excel in predicting 'person.height mt'. Overall, Lr++ and Nap++ outperform Lr and Nap for most numerical predicates, with Nap-based models performing better than Lr-based models. The regularization term \u03bb a for numerical attributes in TransE++ may impact performance, as setting it to 0 during training could explain why Lr++ and Nap++ do not always outperform their counterparts. The numerical attribute propagation approaches learn from labeled and unlabeled data, while regression models only learn from labeled data. Nap's predictions are a weighted average of observed values, preventing large mistakes, unlike Lr-based models which can predict non-plausible values. Non-linear regression models did not improve performance. Knowledge graphs and numerical facts suffer from data sparsity, necessitating the study of model performance under sparse data regimes. Artificially removing numerical facts from the training set creates data sparsity for evaluation. The study investigates the impact of numerical fact sparsity on model performance by artificially removing numerical facts from the training set. Results show that as sparsity increases, Local degrades more rapidly compared to Nap++, which remains robust. In YAGO15K, Global and Local performance results are listed, with Local outperforming Global for most numerical attributes in FB15K-237. This suggests that numerical attributes partially explain the relation structure between entities. The study examines the impact of numerical fact sparsity on model performance, showing that Nap++ is more robust than Local as sparsity increases. Nap++ performs best for most numerical attributes in YAGO15K, particularly excelling in 'happenedOnDate'. The comparison is complicated by numerical attributes having different value ranges. Nap++ outperforms baselines for numerical attributes in FB15K-237 and YAGO15K datasets, showing significant error reduction in MAE and RMSE. Experiments exclude 'location.area' and 'population.number' as they do not relate to graph structure. Embeddings learned from graph structure are effective predictors of entity numerical attributes. Qualitative comparison in Table 8 shows Nap++ superiority. The experimental section aims to show the benefit of adding numerical information during representation learning. Table 3 highlights the behavior of methods regarding 'date of birth' and 'date of death' attributes. While MAE performance is similar, RMSE differs significantly, with MAE being more robust to outliers. Nap-based models use embeddings to propagate numerical information in a similarity graph. The experimental section demonstrates the advantage of incorporating numerical information in representation learning. Nap-based models utilize embeddings to propagate numerical data in a similarity graph, leading to predictions based on attention values. A comparison between Nap and Nap++ reveals that the nearest neighbors identified by Nap are topically similar to the query entity, while Nap++ retrieves entities more relevant to the queried numerical attribute. Nap++ focuses on entities relevant to the queried numerical attribute, using Euclidean distance for k-nearest neighbor graph construction. Entity embeddings in Table 8 encode various relational and numerical information, with Mahalanobis metrics capturing different entity similarities. Incorporating a nearest neighbor loss slightly improved performance for some attributes in knowledge base embeddings, suggesting potential for future research. The paper introduces a new problem of numerical attribute prediction in knowledge bases, different from link prediction. It suggests that KB embedding methods can be used to learn representations that predict numerical attributes. Experimental results validate this premise and show the usefulness of incorporating numerical attribute information in KB representations. The authors believe this new problem will spark further research interest in the community. The curr_chunk mentions that the entity is non-negative and row normalized in FB15K-237, with Julius Caesar being categorized as a Politician in the same dataset."
}