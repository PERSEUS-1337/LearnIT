{
    "title": "SkgKO0EtvS",
    "content": "Graph Neural Networks (GNNs) are trained to imitate steps of classical graph algorithms like breadth-first search and Prim's algorithm, focusing on learning in the space of algorithms. Maximisation-based message passing neural networks are found to be well-suited for such objectives, leading to positive transfer between tasks. Machine learning approaches have been applied to classic tasks over graph-structured inputs, such as shortest paths and sorting, as well as intractable tasks like travelling salesman and boolean satisfiability. Positive transfer between tasks has been observed when learning a shortest-path algorithm alongside a reachability algorithm. Machine learning approaches have been applied to classic tasks over graph-structured inputs, such as shortest paths and sorting. Recent work relies on advancements in graph representation learning with graph neural networks. Ground-truth solutions are used to drive learning, allowing the model to find a mapping from raw inputs to solutions. Inspired by previous work on program synthesis, learning multiple algorithms simultaneously is explored. In 2015, Reed & De Freitas introduced neural graph algorithm execution, where multiple algorithms are learned simultaneously with a supervision signal for knowledge transfer. A message-passing neural network with a maximisation aggregator is suggested for making discrete decisions over graph structures, showing performance benefits in tasks like breadth-first search and Bellman-Ford algorithm for shortest paths. Our approach complements Reed & De Freitas (2015) by showing that a simple graph neural network can learn and transfer algorithms among tasks without explicitly denoting subroutines. The graph neural network is applied to graphs for reachability and shortest paths using breadth-first search and the Bellman-Ford algorithm. The graph neural network uses fixed G and varies meta-data for nodes and edges in the input sequence. Each node has node features x (t) i \u2208 R Nx and each edge has edge features e (t) ij \u2208 R Ne. The algorithm produces node-level outputs y (t) i \u2208 R Ny, which may be reused as inputs in the next step. The network aims to learn and execute various algorithms based on the input provided. The graph neural network follows the encode-process-decode paradigm. An encoder network is defined for each algorithm A, which processes input features and previous latent features. The processor network, shared among all algorithms, processes the encoded inputs to produce latent node features. The decoder-network calculates node and algorithm specific outputs, with the processor network also deciding on algorithm termination. The processor network decides algorithm termination using an algorithm-specific termination network, T A, which provides the probability of termination \u03c4 (t). If the algorithm hasn't terminated, computations are repeated and node values are predicted by the neural executor. All algorithm-dependent networks are linear projections, with the majority of representational power in the processor network P. The processor network utilizes a graph neural network (GNN) layer, comparing graph attention networks (GATs) against message-passing neural networks (MPNNs). The processor network is algorithm-agnostic and can execute multiple algorithms simultaneously. Edge-level outputs and graph-level inputs/outputs are possible but not used in experiments. In our experiments, we generated various types of undirected graphs for our learner, including ladder graphs, 2D grid graphs, trees from Pr\u00fcfer sequences, Erd\u0151s-R\u00e9nyi graphs, Barab\u00e1si-Albert graphs, 4-Community graphs, and 4-Caveman graphs. Self-edges were added to support easier retention of self-information through messages. In our experiments, various types of undirected graphs were generated for the learner, including ladder graphs, 2D grid graphs, trees from Pr\u00fcfer sequences, Erd\u0151s-R\u00e9nyi graphs, Barab\u00e1si-Albert graphs, 4-Community graphs, and 4-Caveman graphs. Self-edges were added for easier retention of self-information through message passing. Real-valued weights were attached to every edge, drawn uniformly from the range [0.2, 1]. These weights serve as edge features for algorithm execution tasks. The algorithm aims to apply to arbitrarily large graphs and ignores corner-case inputs like negative weight cycles. Two classical algorithms are considered for parallel execution. The experiments involved generating various types of undirected graphs with real-valued edge weights. Two classical algorithms, breadth-first search and Bellman-Ford, were considered for parallel execution on the graphs. The algorithms initialize with a randomly selected source node and propagate information accordingly to determine reachability and shortest paths. The Bellman-Ford algorithm computes the shortest path and predecessor nodes for each node, crucial for reconstructing shortest paths. To ensure numerical stability, entries are set to the longest path length + 1. Executing both algorithms simultaneously involves concatenating relevant values. Learning to execute these discrete decision algorithms is well-suited for the MPNN with the max-aggregator. Prim's algorithm for minimum spanning trees focuses on selecting new nodes to connect to the partially constructed MST, ensuring the lightest possible edge is chosen. Updates to the algorithm's state are made at each step, similar to Bellman-Ford in tracking predecessor nodes. This execution paradigm aligns well with the setting of single-node focus in classical graph algorithms. The algorithm at step t focuses on selecting a node to attach to the MST based on neighbourhood edge weights. Different neural network architectures are considered for the task, including MPNN and GAT update rules with various aggregators and attention mechanisms. Refer to Figure 2 for a visualisation of the alignment between the graph algorithm and neural graph executors. The algorithm at step t focuses on selecting a node to attach to the MST based on neighbourhood edge weights. Various neural network architectures are considered, including GAT with two attention heads and LSTM. Previous work has shown that MLPs are unsuitable for reasoning tasks like these. In this study, an LSTM architecture is used with serialised graphs fed into it. Neural networks compute 32 features and are optimized using Adam SGD. Different loss functions are applied simultaneously. Early stopping is performed on the validation accuracy for predecessor node predictions. If the termination network does not stop within a certain number of steps, it is assumed terminated. The study utilizes an LSTM architecture with serialized graphs, computing 32 features optimized with Adam SGD. Various loss functions are applied, with early stopping based on validation accuracy for predecessor node predictions. The neural algorithm executor optimizes categorical cross-entropy for Prim's algorithm, scoring node-pairs using an edge-wise network, and softmax over neighbors for prediction. Accuracy is reported for reachability and predecessor node prediction in parallel algorithms execution. The study uses an LSTM architecture with serialized graphs and 32 features, optimized with Adam SGD. Loss functions are applied, with early stopping based on validation accuracy for predecessor node predictions. The MPNN-max model shows superior generalization performance on reachability and shortest-path predecessor node prediction compared to GAT-like models. The performance gap widens as the test graph size increases. The study found that the MPNN-max model outperformed GAT-like models in reachability and shortest-path predecessor node prediction. The gap in prediction accuracy increases with larger test graph sizes. Additionally, the MPNN-sum model may lead to outputs of exploding magnitude, making it difficult to control for larger graphs. Two additional studies were conducted, focusing on predicting predecessors directly from the inputs without intermediate algorithm computations. The study shows positive knowledge transfer between reachability and shortest-path tasks using MPNN-max. Supervising on distance information leads to performance improvement compared to standard approaches. These results encourage further exploration of this learning setup. The study demonstrates positive knowledge transfer between reachability and shortest-path tasks using MPNN-max. Curriculum learning strategy for BFS and Bellman-Ford algorithms is found to perform worse than learning both simultaneously. MPNN-max shows favorable generalization on larger graphs, with consistent algorithm execution performance across different metrics. The study shows positive knowledge transfer between reachability and shortest-path tasks using MPNN-max. It also demonstrates that training on specific types of graphs can lead to better generalization for graphs of the same type. This is validated by training on Erd\u0151s-R\u00e9nyi graphs or trees of 20 nodes and testing on 100-node graphs across different categories, confirming the model's capability to bias itself towards certain graph types. The MPNN-max model can bias itself to structural regularities in input graphs but still outperforms other models in generalization. The study suggests that aggregation metrics may not accurately assess algorithm performance. Visualizations in Figures 1-2 show test accuracies for each timestep of the algorithm for 100-node graphs. The study demonstrates the performance of different graph neural network architectures in executing Prim's algorithm. Results show that MPNN-mean outperforms other models in accuracy, with a steady improvement curve. GNNExplainer model is used to detect graph substructures contributing to predictions. The study compares different graph neural network architectures in executing Prim's algorithm, showing MPNN-max outperforms other models in generalization. The results also highlight the applicability of neural graph execution in sequential algorithm tasks, expanding its potential applications. In a novel approach, neural networks are optimized to mimic individual steps and outputs of classical graph algorithms, both in parallel and sequentially. Extensive evaluation shows the effectiveness of maximization-based message passing neural networks in tasks like reachability, shortest paths, and minimum spanning trees. The study suggests benefits for multi-task learning and positive transfer due to shared subroutines among classical algorithms. This work motivates further exploration into learning multiple algorithms simultaneously and leveraging similarities in their subroutines. The study explores the benefits of neural networks mimicking classical graph algorithms' steps and outputs. It discusses the advantages of learning multiple algorithms simultaneously and leveraging shared subroutines. The study investigates the benefits of neural networks emulating classical graph algorithms and the advantages of learning multiple algorithms simultaneously. It demonstrates that providing information upfront reduces uncertainty in the learning process. Experimental results show the superiority of MPNN-based models on larger graphs, even beyond the studied \"programmer\" regime. The predictive power of the model is analyzed based on specific testing categories. The study analyzes the predictive power of the MPNN-max model on different testing categories. Results show that performance varies across categories, with trees being the easiest to learn on and grids/community graphs the hardest due to different complexities in decision-making and node degree distribution. The MPNN-max model may need to aggregate messages over larger neighborhoods as graphs increase in size. A qualitative analysis of the model's performance in algorithm execution is provided using a GNNExplainer-like approach. The explainer focuses on identifying the node in the neighborhood of a given node that influences the reachability prediction. The best performing model, MPNN-max, is used to demonstrate the explanation process. The adjacency mask is used to train the model on reachability loss, encouraging the removal of edges from the immediate neighborhood of a node while maintaining correct reachability updates. GNNExplainer identifies predecessor relationships in graphs, with purple edges indicating these relationships. In the explanation process, paths connecting nodes in a graph are traced from the source node using the BFS algorithm. Errors in explanations may result from incorrect predictions or explanations. Experimental results show high accuracy in path explanations compared to ground-truth predecessors, demonstrating the effectiveness of MPNN-max algorithm execution."
}