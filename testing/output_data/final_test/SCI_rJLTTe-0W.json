{
    "title": "rJLTTe-0W",
    "content": "Time series forecasting is important in various fields like marketing and finance. Existing methodologies like ARIMA and Holt-Winters can be affected by change points and anomaly points in real data. A new state space time series model is proposed in this paper to address these issues, along with trend and seasonality. A Bayesian framework is used for inference, providing distributions and forecasting intervals with theoretical backing. The methodology is implemented using an iterative algorithm with MCMC, Kalman filter, and Kalman smoothing, showing improved results in both synthetic and real data applications. Time series forecasting is crucial in business operations, with the main goal being to develop a model based on past observations to predict future values. The proposed methodology in this study outperforms existing methods in forecasting accuracy, change point detection, and anomaly detection. Internet companies, for example, use forecasting to predict daily active users and reach target goals. The Autoregressive Integrated Moving Average model is commonly used in time series forecasting. The Autoregressive Integrated Moving Average (ARIMA) model is a popular choice for time series forecasting, along with the Seasonal ARIMA and Holt-Winters method. State space models, Exponential Smoothing State Space Model (ETS), deep learning with LSTM, bidirectional dynamic Boltzmann machine, and coherent probabilistic forecast are also utilized for time series analysis. This paper focuses on robust time series forecasting in the presence of change points. This paper discusses robust time series forecasting methods in the presence of change points and anomalies. Google's Bayesian structure time series (BSTS) model and Facebook's Prophet approach are used for Internet time series forecasting. Special events like holidays or new product launches can cause anomalies or change points in the data, leading to inaccurate forecasts by traditional models. The presence of change points and anomalies can lead to inaccurate forecasts in time series data. Existing models may not effectively detect and adjust for these changes, resulting in forecasting discrepancies. Anomaly detection models do not handle trend/change points, while change point detection models struggle with seasonality. Manual adjustments are often arbitrary. This forecasting gap caused by abnormal and change points has not been adequately addressed. This paper aims to bridge the gap in forecasting caused by abnormal and change points by developing a state space time series forecasting model in the Bayesian framework. The model can detect anomaly and change points while incorporating learned structure information to enhance predictions. An iterative algorithm based on Bayesian approximate inference with MCMC, Kalman filter, and Kalman smoothing is proposed to solve the optimization problem. The paper proposes a robust Bayesian state-space time series forecasting model that can capture change points, anomaly points, trend, and seasonality. It outperforms existing models in identifying anomalies and change points, showing better performance on both synthetic and real datasets. The proposed state space modeling method improves model prediction and detection of abnormal and change points. An algorithm based on approximate inference using MCMC is used to solve optimization problems with guaranteed forecasting paths. The method outperforms existing techniques in time series forecasting, detecting anomalies and change points accurately with low false discovery rates. It is flexible to capture various time series structures and can be applied in different settings. The proposed state space modeling method improves model prediction by addressing outliers, change points, and anomalies in time series analysis. It aims to decompose time series observations into trend, seasonality, change points, and anomaly points for accurate forecasting. The state space modeling method decomposes time series into trend, seasonality, change points, and anomaly points for accurate forecasting. It uses \u00b5 to model trend, \u03b3 for seasonality, and z a to indicate anomaly points. An observation equation models the deviation between y and its mean, with anomalies captured by o. Transition equations govern trend and seasonality. The state variable \u00b5 and \u03b3 have intrinsic structures. Transition equations for trend and seasonality are presented separately. The trend equation includes a slope component \u03b4 to measure trend changes over time, while the seasonality equation includes noise w. Change points are incorporated in the trend equation using a binary vector z. The seasonality component is defined by S as the length of one season. The observation and transition equations define how hidden variables generate the data. Bayesian methods are used for analysis, easy to implement, and produce posterior distributions. The model assumes normally distributed noise with parameters for standard deviation. Anomaly and change points are modeled as binary vectors. The model represents anomaly and change points as Bernoulli random variables with probabilities for each point. The Bayesian framework allows for graphical representation of the model, with hidden variables except observations. Prior information can be easily incorporated into the model if available. The model incorporates prior information using proper priors and classifies unknown variables as parameters or latent variables. The joint likelihood function captures the discrepancy between these categories, with a clear distinction in behavior. The model uses proper priors and classifies variables as parameters or latent variables. It generates time series based on given parameters and can be viewed as a generative model. An algorithm outlines the generative procedure for creating time series with anomalies or change points. The section discusses inferring unknown variables in a Bayesian setting. It involves sequentially updating hidden variables using different schemes, such as Markov chain Monte Carlo. Specifically, the focus is on updating \u03b1 using Gibbs sampler to obtain the posterior distribution. The \"fake-path\" trick is a method used to address the issue of obtaining the joint distribution required for Gibbs sampler when combining Kalman filter and Kalman smoothing. It involves exploiting the fact that the covariance structure of p(\u03b1 t |y) is not dependent on the means. The \"fake-path\" trick involves obtaining the covariance by other means and adding it to {E(\u03b1 t |y)} n t=1 to sample from p(\u03b1|y). This method includes generating a time series from a vector, using Kalman filter and smoothing to obtain {E(\u03b1 t |\u1ef9)} n t=1, and sampling from the conditional distribution using {\u03b1 t \u2212 E(\u03b1 t |\u1ef9) + E(\u03b1 t |y)} n t=1. After obtaining the covariance by other means and adding it to {E(\u03b1 t |y)} n t=1, the calculation for anomaly detection involves determining P(z a t = 1) = p a and P(z a t = 0) = p 1. The coordinates in z are independent Bernoulli random variables, and for change point detection, z can be generated by sampling independently. An additional segment control step is required to ensure the change points detected meet certain length requirements. The text discusses the detection of change points and anomaly points in time series data. It mentions the use of a minimum segment length parameter to differentiate between change points and anomalies. A more complex criterion is needed for real-world time series data. The proposed segment control method combines various factors to accurately identify change points and anomalies. The proposed segment control method combines factors to accurately identify change points and anomalies in time series data. Parameters like seasonality length are used, and update equations are provided for different standard deviations. In some cases where no change point or anomaly is detected in z, updates for \u03c3 o and \u03c3 r are not well-defined, so they remain the same. Initialization of \u03c3 is set to the standard deviation of y. The trend a 1 is initialized with the average of y values, while slope and seasonality are set to 0. Updates for a 1 are based on \u03b1 information, with trend and slope matching \u03b1 1 and seasonality matching \u03b1 S+1 for convergence and robustness. The initialization and updating of parameters such as \u03b1, p, and z are crucial for convergence and robustness in forecasting. By setting appropriate values and avoiding certain updates, the algorithm can avoid getting stuck in iterations. Once all latent variables and parameters are tuned, accurate forecasts can be made. The future forecasting process involves using latent variables \u03b1 and \u03c3 to generate future time series y future. The forecasting is not deterministic due to randomness from the inference of \u03b1 and the forecasting function itself. The forecasting process involves intrinsic noise like t, u t, v t, and w t, leading to different paths even with fixed \u03c3 and \u03b1 n. By obtaining distribution and predictive intervals, averaging multiple forecasting paths can provide a reliable forecast, showing a combination of linear trend and seasonality. This surprising yet intuitive approach is observed in both synthesis and real data analysis. The future forecasting process relies on collecting information from observed time series, including trend, slope, and seasonality. Theorem 1 explains the linearity of future predictions in mean and standard deviation. It generates multiple future paths with average values that follow a normal distribution. The linear relationship between the forecasted values and time steps is maintained, assuming no anomalies or changes in the time series. The proposed methodology involves three main parts: initialization, inference, and forecasting. Algorithm 3 outlines the process, including change point detection, anomaly points, and forecasting results. The method utilizes linear algebra and Gaussian distribution for future predictions, assuming no anomalies or changes in the time series. The proposed methodology involves three main parts: initialization, inference, and forecasting. Change point detection, anomaly points, and forecasting results are generated using Kalman filter and Gaussian distribution. Parameters are initialized with empirical standard deviation of the time series data. Kalman smoothing and \"fake-path\" trick are used to infer alpha.likelihood function is calculated for updating parameters. The proposed methodology involves three main parts: initialization, inference, and forecasting. The remaining coordinates are equal to those of \u03b1 S+1. Calculate the likelihood function L a1,p,\u03c3 (y, \u03b1, z) and use Algorithm 1 to generate future time series. Combine predictive paths to give the distribution for future forecasting, calculate point-wise quantiles for predictive intervals, and use the point-wise average as the final result. The methodology is compatible with simpler state space time series models, with options to assume no change point or anomaly point in the time series. The proposed methodology involves three main parts: initialization, inference, and forecasting. In this section, synthetic data is generated from a model with parameters set to 0, reducing it to a classic state space time series model. The generative procedure is used to create time series with weekly seasonality for training and evaluation. Parameters for \u03c3, \u03b1 1, and p are specified for the model. The synthetic data is generated with specific parameters to ensure the existence of change points and anomaly points. The separation between training and testing sets is marked in the top panel of FIG9, with the blue dashed line indicating the change points and yellow dots showing anomaly points. The results of the proposed algorithm on the dataset are illustrated in FIG9, with fitting and forecasting results highlighted. The proposed algorithm yields perfect detection on change points and anomalies in the dataset. A generative model is run 100 times to produce different time series for comparison with various forecasting methodologies such as Bayesian Structural Time Series (BSTS), Seasonal Decomposition of Time Series by Loess (STL), Seasonal ARIMA, Holt-Winters, Exponential Smoothing State Space Model (ETS), and the Prophet R package. Performance evaluation is done using mean absolute percentage error (MAPE), mean square error (MSE), and mean absolute error (MAE). The proposed algorithm outperforms existing methods in forecasting accuracy, as demonstrated by mean absolute percentage error (MAPE), mean square error (MSE), and mean absolute error (MAE) comparisons. The algorithm is compatible with cases ignoring change points or anomalies by adjusting parameters, and achieves the best performance in comparison tests. Our proposed algorithm outperforms existing methods in forecasting accuracy by incorporating both change point and anomaly point structures. Comparison tests show better performance in detecting change points and anomalies, measured by True Positive Rate (TPR) and False Positive (FP). High TPR and low FP indicate accurate detection. Our proposed algorithm demonstrates superior performance in detecting change points and anomalies compared to existing methods. The comparison tests show that our method outperforms others in terms of True Positive Rate (TPR) and False Positive (FP) rates. In this section, our proposed method is implemented on real-world datasets, including the Well-log dataset and an internet traffic dataset. The results are compared with existing time series forecasting methodologies, showing better TPR but worse FP compared to ours. The Well-log dataset, collected during well drilling, provides geophysical information with 4050 points split for analysis. The dataset with 4050 points is split into a training set of 3000 points and an evaluation set of 1000 points. There is no seasonality or slope structure in the dataset, leading to the exclusion of these components in the model. The proposed algorithm outperforms other methods in forecasting performance. Ignoring anomaly points slightly improves performance, indicating model mis-specification. Performance considering anomaly points or not is comparable. Our algorithm successfully captures changes in the sequence of Internet traffic data from a major Tech company. It outperforms other methods in forecasting performance, as shown in the comparison table. The algorithm identifies a change point at the 576th point, indicated by a vertical red dashed line in the results. The algorithm accurately detects the only change point at the 576th point in the Internet traffic data, outperforming other methods. It returns a posterior distribution peaking at this point with a probability value of 0.5, while other methods return multiple change points with lower probabilities. Our proposed method achieves the best change point detection in this dataset. Our work utilizes Bayesian modeling to sample posterior and estimate hidden components in the dataset, focusing on change points and anomalies. Compared to other models, our method achieves superior change point detection in the real dataset."
}