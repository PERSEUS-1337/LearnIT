{
    "title": "BkabRiQpb",
    "content": "In social dilemmas, agents aim to cooperate with pure cooperators, avoid exploitation by defectors, and encourage cooperation from others. Strategies based on past rewards, known as consequentialist conditional cooperation, can be effective in a variety of games. Deep reinforcement learning techniques are used to construct these strategies, which have shown success beyond simple matrix games. However, it is important to consider both the consequences of actions and the intentions behind them. Deep reinforcement learning focuses on constructing agents that can learn optimal behavior in complex environments, particularly in social dilemmas where individuals may act against socially optimal outcomes. Strategies for social dilemmas involve partially observed information about a partner's actions or the environment. The Prisoner's Dilemma is a simple example where mutual cooperation leads to the highest payoffs, but individual reward can be maximized regardless of the partner's choice. In the context of social dilemmas, the Prisoner's Dilemma shows that mutual cooperation leads to high payoffs, but individual reward can be maximized by defecting regardless of the partner's choice. Tit-for-tat (TFT) is a well-studied strategy that rewards cooperation and avoids exploitation in repeated interactions. TFT has intuitive appeal and can be extended to more complex Markov games by learning cooperative and selfish policies. Tit-for-tat (TFT) is a strategy that rewards cooperation and avoids exploitation in repeated interactions. TFT can be applied to various domains such as single agent decision problems, board and card-based games, video games, multi-agent coordination problems, and the emergence of language. TFT is an example of a conditionally cooperative strategy that cooperates when a certain condition is fulfilled. Our main contribution is to use RL methods to construct conditionally cooperative strategies for games with imperfect information. When information is imperfect, the agent must estimate whether a partner is acting cooperatively and respond accordingly. We introduce consequentialist conditional cooperation (CCC), where the agent cooperates if observed rewards exceed a time-dependent threshold computed using RL and self-play. This strategy cooperates with cooperators, avoids exploitation, and ensures a good payoff. In the Fishery game, CCC agents cooperate with others, avoid exploitation, and earn high payoffs by leaving young fish for their partner. This strategy is efficient in complex games and involves estimating partner behavior to determine cooperation levels. In the Pong Player's Dilemma (PPD), CCC is compared to amTFT in a modified version of Atari pong. CCC proves to be a successful and robust strategy in the game, promoting cooperation over selfish actions. However, it does not completely dominate forward-looking strategies like amTFT. In bilateral relationships, game theorists have studied cooperation emergence under perfect and imperfect observation. The expected rewards of non-cooperation in the Pong Player's Dilemma are similar to the Prisoner's Dilemma, leading to similar actions by agents like amTFT. However, low probability of losing points can delay detection of defectors. Empirical evidence shows that CCC agents can be exploited by defectors in short risky games, while amTFT agents are more resilient. Progress is being made towards agents that can effectively use both intention and outcome information in decision-making. The curr_chunk exclusively studies repeated matrix games, focusing on proving equilibrium existence for cooperation. Algorithms have been developed for computing these strategies, but they are for perfectly observed games played iteratively. Designing a good agent for social dilemmas can differ from computing equilibrium strategies. For example, tit-for-tat is considered a good strategy in the repeated PD, but it is not an equilibrium when both players use it. Multi-agent learning literature examines how agent properties affect outcomes. The curr_chunk focuses on studying how agent properties affect behavior dynamics in multi-agent games. It explores designing agents that can 'teach' their learning partners, blending learning approaches with trigger strategy methods. The work involves partially observed Markov games and suggests future research directions in this area. Markov games (POMG) are multi-agent generalizations of partially observed Markov decision problems. A POMG consists of states, actions for each player, transition and observation functions, and reward functions. Agents choose policies based on observations, similar to belief-free strategies in repeated games. The value function is defined as the average reward when players start in a given state. The value function in Markov games represents the average reward when players start in a state and follow specific policies. Reward-ergodic POMGs have a well-defined long-run reward rate independent of the starting state, making them suitable for analysis. The POMG will be reward ergodic if it has a single positive recurrent chain. Cooperative Policies aim to maximize the joint rate of reward in POMGs with specific restrictions. The Social Dilemma assumption addresses cooperation and coordination problems among players. Kleiman-Weiner et al. (2016) discuss this issue further. Solving the coordination problem, such as through communication, is a potential avenue for future work. To construct a CCC agent, access to a policy pair forming a Nash equilibrium is needed. The strategies generalize defection in the Prisoner's Dilemma, with lower payoff rates than socially optimal strategies in the long run. The agent maintains a persistent state at each time period and plays according to a threshold T. Theorem 1 states that a threshold-based strategy for player 1 promotes cooperation over defection in the long run for player 2 in any ergodic game. This strategy incentivizes rational behavior and ensures that cooperation is a better strategy than defection. The CCC strategy promotes cooperation, prevents exploitation by defectors, and guarantees payoffs against rational learning agents. Future work aims to make conditional cooperative strategies robust against adversarial agents. Generalizing the strategy for finite time and using RL methods for computation are also discussed. The strategy of CCC promotes cooperation, prevents exploitation by defectors, and guarantees payoffs against rational learning agents. It can be extended to non-payoff ergodic games by using inequality as a summary statistic. The policies \u03c0 C and \u03c0 D are constructed following the training procedure of Lerer & Peysakhovich (2017), with self-play to compute the selfish and prosocial policies. Our agents, trained using deep RL, consider not only their own rewards but also their partner's. Learning policies in POMDPs present unique challenges, with policy gradient being preferred due to nondeterministic optimal policies. In the Fishery game, RNN augmentation is used to track past states for policy optimization. In the Fishery game, RNN augmentation was unnecessary for the policy, but value-based methods were avoided due to aliasing of values. Thresholds for conditional cooperation need to account for various sources of finite time variance. The per-turn threshold is computed by performing rollouts of the full game based on learned policies. In the Fishery game, thresholds for conditional cooperation are computed based on rollouts of sample paths. The algorithm allows for a trade-off between false positives and false negatives, with parameters q and \u03b1 tuning the balance. An example of threshold computation and precision/recall with actual opponents is provided. In the Fishery game, two agents live on opposite sides of a lake with fish spawning randomly. They can catch young fish for 1 reward or mature fish for 3 rewards. Cooperative strategies involve leaving young fish for the partner, but there is a temptation to defect. The game is imperfect as agents cannot see their partner's behavior. Thresholds for conditional cooperation are computed based on sample paths, balancing false positives and false negatives with parameters q and \u03b1. The experimental results in Figure 1 show that trajectories of CC agents remain above the threshold, indicating low false positives, while CD agents mostly fall below, indicating low false negatives. Training under selfish reward schemes leads to suboptimal strategies, while prosocial training results in good policies. CCC agents quickly avoid exploitation by defectors while maintaining cooperation with cooperators. A tournament is conducted to assess if CCC meets the desired criteria outlined in the introduction. In a tournament, random policies from a pool of 50 are drawn to play a Fishery game. Metrics from Lerer & Peysakhovich (2017) are used to compare strategies, including SelfMatch, Safety, and IncentC. A matrix of policy performance against each other is provided. CCC can be used in games of partially observed games. In the Pong Players' Dilemma (PPD), agents receive rewards for scoring points in Atari Pong, but the optimal move is not to play. Selfish agents may defect to score points, decreasing total social reward. CCC is compared to the forward-looking amTFT strategy, which conditions cooperation on future rewards. In Fishery, agents use the game's Q function to estimate gains from actions. They track partners' 'debit' and learn cooperative strategies through social training. CCC incentivizes cooperation and is not easily exploited. The curr_chunk discusses intention-based strategies in agents, comparing selfish and prosocial training methods. Selfish agents aim to score points, while cooperative agents hit the ball back and forth. The performance of CCC agents is evaluated in the context of the Fishery experiment. In the Fishery experiment, CCC agents cooperate with cooperators and themselves but are not exploited by defectors. CCC performs similarly to the more computationally expensive amTFT in PPD games. However, in finite length risky PPD games, CCC loses its ability to incentivize cooperation and avoid exploitation. CCC agents perform just as well as amTFT agents in tournaments, indicating that CCC does not completely dominate amTFT in perfectly observed games. In shorter timescales, intention-based strategies can be effective. Modifying the reward structure in games can lead to exploitation of CCC agents by defectors. Results in Coins game mirror those in the PPD game. Consequentialist conditionally cooperative strategies are introduced in this work. In this work, consequentialist conditionally cooperative strategies are introduced as useful heuristics in social dilemmas, even with imperfect information. Using one's own reward stream as a summary statistic for cooperation is effective in ergodic games. The time scale for a CCC agent to detect exploitation is related to the mixing time of the game and reward stochasticity. Consequentialist and forward-looking models are compared in social dilemmas. The random Dictator Game (rDG) is used as an example, where individuals make choices based on intentions or outcomes. RL trained agents benefit from conditioning on intentions like amTFT for forward thinking. Trained agents benefit from conditioning on intentions like amTFT for forward thinking in social dilemmas. CCC is a simple strategy that works in POMDPs, while amTFT is complex and expensive to implement. Combining consequentialist and intention-based signals is necessary to solve social dilemmas. Experimental evidence shows that humans rely more on consequences than optimal behavior would suggest, as seen in the rDG. Humans have social heuristics that work across various environments, including hybrid ones with artificial agents. Constructing artificial agents for such environments requires moving beyond traditional optimality theorems. Human social preferences consider factors like inequity and social norms, which AI researchers need to account for to be successful. AI researchers need to understand human social heuristics and create agents aligned with human moral and social intuitions to be successful. The proof of the main theorem relies on the convergence of random variables and the behavior of CCC agents in certain scenarios. The CCC agent's total payoff exceeds the threshold by R s, then the agent plays \u03c0 C with high probability. This leads to convergence of payoffs for both agents to \u03c1 CC. If both players continue to play \u03c0 C forever, the CCC agent's accumulated reward is no less than \u2212t 0 r max. The CCC agent's accumulated reward in t 0 turns is no less than \u2212t 0 r max. If the agent's total payoff exceeds t 0 r max + tT, they will always play \u03c0 C. Setting R s to t 0 R max ensures both players achieve average reward of \u03c1 CC with probability at least 1 \u2212 2. If agent 1's average reward converges to \u03c1 < T, then CCC plays a fixed policy \u03c0 D 1 from a certain point onward. At t 0, CCC plays a fixed policy \u03c0 D 1. The assumption that the payoff of the pair (\u03c0 C+D, \u03c0 C) to agent 1 is at least \u03c1 CC is overly restrictive. A slight modification to CCC allows for a weaker condition where a policy switches between \u03c0 C and \u03c0 D at most once every k periods. This modification still ensures cooperation can work effectively. The modified CCC in the Fishery game uses two thresholds to switch between policies, with rewards growing linearly as time goes on. The game has a 5x10 state space, and models are trained via policy gradient to encourage prosocial policies. Value-based methods were found to be ineffective in training prosocial policies in Fishery. The actor-critic model was ineffective in training prosocial policies in Fishery due to aliasing of states with different values. Policy gradient with batch baselining successfully trained policies for both Coins and Fishery after playing 100,000 and 40,000 games respectively. CCC statistics were computed at test time by rolling out games, and the ALE environment was modified for 2-player play with adjusted rewards. The policy is trained directly from pixels via A3C with modified rewards. Inputs are rescaled and normalized, with 38 threads used for training over 38,000 games. The policy is implemented as a convolutional neural network with four layers. The LSTM layer used in the library is found to be unnecessary. CCC maintains cooperation in the Coins game. CCC agents in the Coins game can maintain cooperation by picking up coins of their own color, avoiding exploitation from selfish agents. Coins randomly appear on the board with different colors, and agents receive points for picking them up, but lose points for picking up coins of the other agent's color. The cooperative strategy is to only pick up coins of one's own color to maintain cooperation. In a fully observed Markov game called Coins, agents can maintain cooperation by picking up coins of their own color to avoid exploitation. Training 10 copies of each strategy type under different reward schemes, it is found that CCC is as effective as intention-based TFT strategies in incentivizing cooperation."
}