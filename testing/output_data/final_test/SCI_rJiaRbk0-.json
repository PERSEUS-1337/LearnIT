{
    "title": "rJiaRbk0-",
    "content": "Long Short-Term Memory (LSTM) is a widely used recurrent structure in sequence modeling. A new LSTM training method is proposed in this paper, aiming to push gate values towards 0 or 1 to better control information flow and prevent overfitting. The Gumbel-Softmax trick is utilized to make the model trainable with standard methods. The proposed method utilizes the Gumbel-Softmax trick to train LSTM models with standard backpropagation. Experimental results show improved interpretability of gate values and better generalization across various tasks. The models are robust to low-precision and low-rank approximations of gate parameters due to a flat loss surface. In this paper, a new method for training LSTM models is explored by pushing gate values to the boundaries of their ranges (0, 1). This approach aims to address the inefficiency and overfitting issues caused by the large number of parameters in LSTM gates. Pushing gate values to 0/1 aligns with the original purpose of gates in LSTM development. Training towards binary-valued gates can improve model generalization by lying in a flat region of the loss surface. In this paper, the authors discuss the challenge of pushing LSTM gates towards binary values to improve model generalization. They propose using the Gumbel-Softmax trick to generate approximated samples for categorical latent variables, which can help in training the model effectively. In a stochastic computational graph, the Gumbel-Gate LSTM (G 2 -LSTM) method is used to train LSTM models efficiently by applying the Gumbel-Softmax trick to gates. Experimental results show that the model generalizes well and is not sensitive to changes in parameters. The Gumbel-Gate LSTM method efficiently trains LSTM models using low-precision and low-rank approximation in gates. Results show improved model performance and interpretability of learned gates. Related work discusses sharp and flat minima in loss functions. Recent papers have explored modifying the training process to help models converge to flat regions instead of sharp minima, improving generalization ability. Techniques such as small-batch training, new objective functions, and dropout have been proposed to achieve this. Additionally, methods like applying dropout to feed-forward connections and recurrent units in RNNs, as well as using Zoneout to introduce stochastic identity connections between time steps, have been shown to enhance model performance. Our method introduces stochastic identity connections between time steps to bias the optimization process and prevent overfitting. It is complementary to dropout in RNNs and can be combined with it. BID15 and BID23 develop a continuous relaxation of discrete random variables in stochastic computational graphs using the Gumbel-Softmax trick. The Gumbel-Softmax trick is used to sample categorical variables in various applications like VAEs, GANs, and language generation. This method is now being introduced in LSTM for robust training. The new training algorithm for LSTM involves learning binary-valued gates to optimize model parameters using backpropagation. In recurrent neural networks, hidden states are sequentially processed to construct representations for prediction or decision making. Deep recurrent networks use hidden states from one layer as inputs to the next. Each hidden state is trained to remember task-relevant aspects and incorporate new inputs via a recurrent operator. LSTM is a carefully designed recurrent structure that introduces binary-valued gates for robust training. LSTM introduces a memory cell for long-term storage, computed via gate functions. The forget gate connects the current cell to the previous timestep, controlling information flow. Each gate has weight matrices and bias vectors denoted by subscripts. An LSTM is formally defined with sigmoid functions and element-wise products. The LSTM unit requires more parameters than a simple RNN unit, with a large percentage used for computing gate functions. Pushing gate outputs towards 0 or 1 can lead to a flat loss function, promoting better generalization. Training towards binary-valued gates is challenging but can improve generalization. In this work, the Gumbel-Softmax trick is utilized to approximate discrete distributions efficiently, particularly for learning discrete random variables in stochastic computational graphs. The trick's ability to approximate the Bernoulli distribution is discussed, offering a method to improve optimization towards binary-valued gates. The Gumbel-Softmax trick is used to approximate discrete distributions efficiently, including the Bernoulli distribution. Proposition 1 defines a random variable D \u03b1 \u223c B(\u03c3(\u03b1)) and G(\u03b1, \u03c4 ) = \u03c3( DISPLAYFORM0. The distribution of G(\u03b1, \u03c4 ) approximates B(\u03c3(\u03b1)), with convergence rates characterized by inequalities. The Gumbel-Softmax trick is utilized to efficiently approximate discrete distributions like the Bernoulli distribution. By repeatedly sampling the output of a gate using a specific method, gradient-based algorithms can push parameters towards optimizing binary-valued gates. This method extends to vector-valued functions, where elements are independently sampled from a uniform distribution. The Gumbel-Gate LSTM (G2-LSTM) method utilizes the Gumbel-Softmax trick to optimize binary-valued gates in LSTM units during training. Independent sampling of values for U in each time step is followed by updating LSTM units and calculating loss using negative log likelihood. Gradient-based methods can then be used to update model parameters. The proposed training algorithm for G2-LSTM utilizes gradient-based methods to update model parameters. It was tested on language modeling and machine translation tasks, achieving competitive results in machine translation based on BLEU scores. The model architecture for LSTM included a stacked three-layer LSTM with drop-connect on recurrent weights. The training algorithm for G2-LSTM utilizes gradient-based methods and a variant of ASGD for optimization. The temperature used in G2-LSTM is set to 0.9. A neural cache model is added on top of the trained language model to improve perplexity. Two datasets are used for experiments on neural machine translation: IWSLT2014 German\u2192English and English\u2192German translation datasets. The English\u2192German translation dataset in WMT'14 is commonly used as a benchmark task for evaluating NMT models. It contains 4.5M English\u2192German sentence pairs, with Newstest 2014 as the test set and Newstest 2012 and Newstest 2013 concatenated as the validation set. BPE was used to create a vocabulary of 30k sub-word units for both languages. The German\u2192English dataset utilized a stacked two-layer encoder-decoder framework with word embedding and hidden state sizes set to 256. For the larger English\u2192German dataset, a stacked three-layer encoder-decoder framework was used with word embedding and hidden state sizes set to 512 and 1024 respectively. The encoder in the experiments had a bi-directional first layer and used dropout in training stacked LSTM. The temperature \u03c4 for G2-LSTM was set to 0.9, minibatch size was 32/64 for German\u2192English/English\u2192German respectively. Models were trained with AdaDelta on one M40 GPU with gradient clipping norms set to 2.0. Evaluation was done using tokenized case-sensitive BLEU and a beam size of 5 during inference. Experimental results are compared with two algorithms, Baseline and Sharpened Sigmoid. In experiments, a sharpened sigmoid function (\u03c4 = 0.2) was used to improve generalization, resulting in better performance than baseline models. G2-LSTM outperformed baselines in language modeling and machine translation tasks, demonstrating the effectiveness of the proposed training method. Training and validation loss curves for G2-LSTM and baselines are compared in FIG1. The G2-LSTM model outperformed baseline models in language modeling and translation tasks, showing improved generalization. The validation loss of G2-LSTM continued to decrease even after the 30th epoch, indicating better generalization compared to baseline LSTM. Additionally, G2-LSTM achieved state-of-the-art performance in German\u2192English translation, surpassing previous works. The experiments tested the sensitivity of learnt models to compressed gate parameters in a three-layer model. Two ways of parameter compression were considered, reducing precision in input and forget gates to compress the model size. Two settings of low-precision compression were tested: rounding parameters and clipping rounded values to a fixed range. Different parameters were set for language modeling and other tasks. In experiments testing model sensitivity to compressed gate parameters, different settings were used for language modeling and neural machine translation. Parameters were compressed using single value decomposition to reduce model size and improve matrix multiplication speed. For language modeling, input/forget gate matrices were compressed to rank 64/128, while for neural machine translation, rank was set at 16/32. Results showed that both baseline and learnt models were robust to low-precision compression in language modeling, with the learnt model performing better. Our model outperforms the baseline with low-rank compression, showing robustness to parameter compression in both language modeling and neural machine translation. The proposed method consistently performs better than the baseline, even with significant compression rates. The models trained with our method are less sensitive to compression, maintaining comparable translation accuracy to the baseline model with full parameters. In experiments testing the effectiveness of G2-LSTM for German\u2192English translation, the values of gates were analyzed. Comparing classic LSTM and G2-LSTM, the output gate values showed significant differences. While both models performed well, the gate value distributions were distinct, with LSTM showing uniform distributions and G2-LSTM exhibiting varied values. The gate values in G2-LSTM are concentrated near 1 for input gates and near 0 or 1 for forget gates, indicating successful training. Additionally, a case study on a sampled sentence showed the average values of input and forget gate functions for each word in the first layer. In the previous section, the gate values in G2-LSTM were analyzed, showing high values for input gates and either 0 or 1 for forget gates. The current section discusses how G2-LSTM maintains information in input gates for all words, unlike LSTM which sometimes drops information for meaningful words. Additionally, G2-LSTM effectively forgets function words and boundaries within sentences to improve translation results. The paper introduces a new training algorithm for LSTM using the Gumbel-Softmax trick to improve model robustness. Experiments on language modeling and machine translation have shown its effectiveness. Future work includes testing on deeper models, exploring new applications, and releasing the training code to the public. The paper introduces a new training algorithm for LSTM using the Gumbel-Softmax trick to improve model robustness. Experiments on language modeling and machine translation have shown its effectiveness. Future work includes testing on deeper models, exploring new applications, and releasing the training code to the public soon. Additional experiments on language modeling demonstrate the model's insensitivity to hyperparameters."
}