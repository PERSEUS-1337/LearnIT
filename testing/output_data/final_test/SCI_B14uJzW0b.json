{
    "title": "B14uJzW0b",
    "content": "Deep learning models can be optimized efficiently via stochastic gradient descent, with little theoretical evidence to support this. The focus is on understanding when the optimization landscape of a neural network is suitable for gradient-based optimization. A two-layer ReLU network with two hidden units is studied, showing that all local minimizers are global. This, combined with previous work, demonstrates that gradient descent converges to the global minimizer. The population loss function is defined, and the landscape of the function over the manifold is studied in this paper. In this paper, the landscape of the function f over the manifold R = { w 1 = w 2 = 1} is studied. The manifold gradient descent algorithm is described as well as the characterization of critical points for global convergence. It is shown that there are no spurious local minimizers on the manifold R. Theorems 4.1 and 4.2 demonstrate that manifold gradient descent with random initialization converges to the global minimizer with probability one. The function f has local minima at w 1 = w. The gradient on the manifold needs to be computed and checked if it's zero. Manifold gradients are calculated using polar coordinates. The problem needs to be divided into cases due to the piecewise linear nature of the angles. The resulting functions are linear in the angles. After calculating critical points in the 2D case, a direction of negative curvature is found. The problem is then reduced to a three-dimensional case using a lemma to simplify it. Even in the three-dimensional case, identifying all critical points explicitly remains challenging. In the three-dimensional case, identifying all critical points explicitly remains challenging. The properties of critical points show that saddle points and local maximizers have a direction of negative curvature. Lemmas 4.5 and 4.6 capture important geometrical properties of critical points. After computing the Hessian, it was determined that all critical points exhibit negative curvature, leading to the proposition. Detailed proofs are provided to support the main theorem, showing that in the general case, only three dimensions are needed. Lemma 5.1 states that for a critical point (w1, w2), there exists a standard orthogonal basis (e1, e2, e3) where w1 and w2 lie in the span of these vectors. If \u03b8 w1,w2 < \u03c0, then there exists a real number r such that the four vectors w1, w2, w*1, and w*2 are linear dependent with rank at most 3. Critical points can be represented in a standard orthogonal basis where w1 and w2 lie in the span of these vectors. The text discusses properties of critical points in a three-dimensional case, showing restrictions on their positions in specific domains. Lemmas 5.3 and 5.4 constrain the critical points further. A new function F is constructed for more precise analysis, leading to Lemma 5.5 which reveals a relationship between w1 and w2. Although explicit identification of critical points is not possible, geometric insights are provided. In the three-dimensional case, the text discusses restrictions on the positions of critical points in specific domains. Lemmas further constrain the critical points, and a new function F is constructed for more precise analysis. Geometric insights are provided, showing the direction of negative curvature when w1 and w2 are nonorthogonal and form an acute angle. The text divides the plane into four parts based on the angles of w1 and w2 in polar coordinates. The only critical points, apart from global minima, occur when w1 and w2 are in the same part. In the same part, critical points occur when both w1 and w2 are on the bisector of w*1 and w*2. The proof details are in appendix E, with similar techniques as the original problem. Experiments were conducted to verify theoretical results, focusing on cases with K = 2 hidden units. ReLU networks do not have the property that all local minima are global minima, as shown through numerical simulation. The study considers cases with 2 to 11 hidden units, where true parameters are orthogonal. Projected gradient descent with 300 random initializations is used to count the number of local minima with non-zero training error. The bar plot in FIG2 illustrates the occurrence of spurious local minima, showing an increase for K \u2265 7 hidden units. In this paper, recovery guarantee of stochastic gradient descent for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation functions, and Gaussian inputs is provided. Experiments verify the results. Possible future directions include relaxing some assumptions, such as the orthogonal teaching weights. The discussed neural network is considered simple and not practical. The neural network discussed in this paper is simple and not practical. Experiments show that with seven hidden nodes and orthogonal true parameters, there are bad local minima. The research aims to analyze the loss surface with three or more hidden units, providing insights into the non-convexity of loss surfaces. Consider a neural network with 2 hidden nodes and ReLU activation function. The loss function for a neural network with 2 hidden nodes is analyzed in terms of population loss. The objective function is defined and gradients are calculated for the weights. Critical points in 2D cases are discussed by translating weights to polar coordinates. The text discusses analyzing the loss function for a neural network with 2 hidden nodes in terms of population loss. It simplifies functions using variables \u03b8 1 and \u03b8 2, and analyzes critical points in 2D cases by translating weights to polar coordinates. The norm of the manifold gradient is considered, and various cases are analyzed to specify positions and properties of critical points. The text discusses critical points in the analysis of a neural network's loss function with 2 hidden nodes, focusing on population loss. It simplifies functions using variables \u03b8 1 and \u03b8 2, and examines critical points in 2D cases by converting weights to polar coordinates. The norm of the manifold gradient is evaluated, and different scenarios are explored to determine the positions and characteristics of critical points. The text discusses symmetry in critical points of a neural network's loss function with 2 hidden nodes. It examines the relationship between angles \u03b8 1 and \u03b8 2, and how symmetry affects the gradients and critical points. The analysis involves converting weights to polar coordinates and exploring different scenarios to determine the positions of critical points. The text discusses symmetry in critical points of a neural network's loss function with 2 hidden nodes, exploring the relationship between angles \u03b8 1 and \u03b8 2. It concludes that there are 4 remaining cases to consider and analyzes the conditions for critical points to occur. The text discusses critical points in a neural network's loss function with 2 hidden nodes, analyzing angles \u03b8 1 and \u03b8 2. It concludes that there are 4 remaining cases to consider and explores conditions for critical points to occur. The text discusses critical points in a neural network's loss function with 2 hidden nodes, analyzing angles \u03b8 1 and \u03b8 2. It concludes that there are 4 remaining cases to consider and explores conditions for critical points to occur. If m(w 1 ) = m(w 2 ) = 0, then h 3 (\u03b8 2 ) = \u2212h 1 (\u03b8 1 ). Different scenarios are examined based on the values of h 1 (\u03b8 1 ), leading to various critical points. Based on the analysis of critical points in a neural network's loss function with 2 hidden nodes, it is concluded that there are no critical points in the discussed cases. The Hessian on the manifold is examined, leading to the determination of four critical points in the 2D case. In conclusion, there are four critical points in the 2D case: one global maximal point and three saddle points. If a point is a critical point, there exists a standard orthogonal basis where the vectors lie. In the 2D case, there are four critical points: one global maximal point and three saddle points. The vectors w1, w2, w*1, and w*2 are linear dependent with rank at most 3. Critical points can be represented in a standard orthogonal basis. In the 2D case, critical points are represented in a standard orthogonal basis. The vectors w1, w2, w*1, and w*2 are linear dependent with rank at most 3. Lemma D.3 shows that w13 * w23 = 0, and Lemma D.4 proves that w13 * w23 < 0. In lemma D.2, it is shown that \u03b8 w1,w2 < \u03c0 and both w1 and w2 are > 0. This implies k < 0. Adapting from lemma D.4, we have DISPLAYFORM14. Similarly, from the first component of FORMULA0 and FORMULA0, we get DISPLAYFORM16. By defining k0 = -k, \u03b81 = \u03c0 - \u03b8 w2,w*1 and \u03b82 = \u03c0 - \u03b8 w2,w*, we can simplify the expressions. Lemma D.10 states that F(\u03b8) is either strictly decreasing or first decreases and then increases when \u03b8 is within a certain range. The proof involves defining H(\u03b8) and analyzing its relationship with F(\u03b8). Ultimately, it is shown that F(\u03b8) follows a specific pattern based on the values of H(\u03b8) and \u03b8. The function F(\u03b8) is shown to be either strictly decreasing or decreasing then increasing within a specific range. The proof involves analyzing H(\u03b8) and its relationship with F(\u03b8), leading to the conclusion that F(\u03b8) follows a distinct pattern based on these values. The function F(\u03b8) exhibits a specific pattern based on the values of H(\u03b8). The function h transitions into four different functions, with key properties such as h1(\u03b8) + h1(\u03b1 - \u03b8) = 0 and \u03b81 + \u03b82 = \u03b1. Lemma E.1 states m(w1) \u2264 0, with a detailed inequality analysis provided. The function F(\u03b8) transitions into four different functions with specific properties. When m(w1) = m(w2) = 0, h1(\u03b81) + h1(\u03b82) = 0. For \u03b8 \u2208 [\u03b1, \u03c0], h2(\u03b8) is always negative, leading to no critical points. For \u03c0 \u2264 \u03b81 \u2264 \u03b82 \u2264 \u03c0 + \u03b1, h3(\u03b8) + h3(2\u03c0 + \u03b1 - \u03b8) = 0. Lemma E.2 states that m(w1) \u2264 0 is proven by showing that there are only two critical points in the case where m(w1) = 0, which are (\u03b81, \u03b82) = (\u03c0, \u03c0 + \u03b1) or (\u03b81, \u03b82) = (\u03c0 + \u03b1/2, \u03c0 + \u03b1/2)."
}