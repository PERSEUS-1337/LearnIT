{
    "title": "Syxc1yrKvr",
    "content": "We introduce a new autoencoding model that combines the strengths of variational autoencoders (VAE) and generative adversarial networks (GAN). By optimizing \u03bb-Jeffreys divergence, our model overcomes the mode collapsing issue of GANs and utilizes the best features of both VAE and GAN objectives. The model consists of two parts, one optimized through adversarial training and the other following the VAE objective. To address limitations of explicit likelihoods like Gaussian or Laplace, we propose training the VAE model with an implicit likelihood using adversarial training. In experiments on CIFAR-10 and TinyImagent datasets, a novel approach to training the VAE model with an implicit likelihood by an adversarially trained discriminator achieved state-of-the-art generation and reconstruction quality. The model balances mode-seeking and mode-covering behavior by adjusting the weight \u03bb in the objective. VAE is popular for modeling high-dimensional distributions and has advantages like learning low-dimensional representations and conditional generation. However, explicit reconstruction terms come at a price. The VAE model tends to generate unrealistic objects due to maximum likelihood estimation. Pairwise comparisons of log-densities and optimal densities are used to prevent mode-collapse. Balancing between mode-seeking and mass-covering behavior is achieved by adjusting the weight \u03bb in the objective. Substituting each KL term with GAN and VAE losses is a straightforward approach. Proposing a novel approach to train VAE models adversarially to address limitations with simple distributions like Gaussian or Laplace. Introducing Implicit \u03bb-Jeffreys Autoencoder (\u03bb-IJAE) to estimate implicit likelihood using an adversarially trained discriminator. The model minimizes the \u03bb-Jeffreys divergence and is evaluated on CIFAR10 and TinyImagenet datasets for generation and reconstruction tasks. Our model, the Implicit \u03bb-Jeffreys Autoencoder (\u03bb-IJAE), demonstrates a balance between generating realistic images and reconstruction quality by adjusting the weight \u03bb in the objective function. Through experimental studies on CIFAR10 and TinyImagenet datasets, we establish a default choice for \u03bb that maintains a compromise between mode-seeking and mass-covering behavior. Additionally, VAE-based models aim to minimize the upper bound on the forward KL divergence, with recent research exploring the integration of a discriminator for measuring object similarity. The models discussed aim to improve approximate inference in VAE models by considering more complex variational distributions and applying adversarial training to match distributions. These methods remain within the scope of maximum likelihood estimation framework. The vanilla GAN framework is equivalent to minimizing Jensen-Shanon divergence. Modifications to the generator loss can lead to minimizing reverse KL divergence. Various autoencoder models utilize these divergences. Some methods introduce additional entropy loss to address non-identifiability issues. Other methods use reverse KL divergence as an additional term to encourage mode-seeking behavior. The \u03bb-IJAE model introduces an additional term to encourage mode-seeking behavior and is related to AS-VAE and SVAE methods. SVAE differs from \u03bb-IJAE in that it minimizes the KL divergence between marginal distributions for arbitrary \u03bb and its loss function did not provide good reconstructions in experiments, leading to the introduction of additional data-fit terms. The \u03bb-IJAE model encourages mode-seeking behavior and trains implicit distributions, unlike SVAE. VAE and GAN have different properties in fitting data distributions, with VAE covering all modes but generating unspecific images, while GAN concentrates on a few modes and may miss significant parts of the data distribution. The contrast between VAE and GAN lies in their optimization of different divergences between data distributions. VAE uses Variational Inference with an encoder model to address intractability, while GAN employs Adversarial Training in a game between a generator and discriminator. Adversarial Training involves a game between a generator and discriminator, where the loss of the generator is equivalent to the Jensen-Shanon divergence. This framework is related to Density Ratio Estimation and allows for different generator objectives while keeping the same objective for the discriminator. It is important to consider the validity of gradients obtained in practice, as the discriminator is trained for specific generator parameters. VAEs provide a theoretically sound way to learn generative models with a coherent encoder, but they often produce blurry and unspecific samples due to the limitations of the forward KL divergence in capturing local behavior. Adversarial learning objectives like JSD and D KL are not affected by this issue, ensuring unbiased gradients for better sample quality. The limitations of VAEs in capturing local behavior lead to blurry samples, as the model is forced to cover all modes of the data distribution. A proposed solution is a more balanced divergence that seeks modes while still covering all modes adequately. The \u03bb-Jeffreys divergence is advantageous over other divergences like Forward KL, Reverse KL, and JSD in capturing modes effectively. It uses one mixture component for the most probable mode and the other for mass-covering, addressing the limitations of VAEs in mode collapse. The optimization of the \u03bb-Jeffreys divergence involves two parts: minimizing the reverse KL D KL (p \u03b8 (x) p * (x)) using GAN optimization and maximizing the forward KL D KL (p * (x) p \u03b8 (x)) through ELBO in VAE. The ELBO term can be decomposed into a reconstruction term, which poses challenges in cases of implicit formulation. The text discusses extending the class of possible likelihoods for a model by using implicit ones, allowing for more expressive generators. This technique aims to address issues such as generating unrealistic or blurry images when the model's capacity is limited. The decoder in VAE maps the latent code to a space X, which is then used to parametrize the distribution of decodings. The proposed method involves using the output of the generator to parametrize an implicit likelihood, enhancing the model's capabilities. The text discusses using implicit likelihoods to extend the class of possible likelihoods for a model, allowing for more expressive generators. A discriminator is introduced to classify fake and real triplets, and the generator is trained using binary cross-entropy objective. The text introduces a discriminator to classify fake and real triplets, and the generator is trained using binary cross-entropy objective. It discusses maximizing the reconstruction term by minimizing a slightly different loss for the generator, ensuring unbiased gradients in the optimal discriminator case. The text discusses optimizing the generator by minimizing a loss function and using gradient-based methods. It explores the choice of r(y|G \u03b8 (z)) to encourage realistic reconstructions and penalize distorted images. The distribution used for r(y|x) is symmetric with a mode in x, setting implicit likelihood of reconstructions. The text discusses the non-optimality of D \u03c4 (x, z, y) as a form of regularization to convert explicit function r(y|x) into implicit likelihood for realistic reconstructions of x. The KL term in L ELBO (\u03b8, \u03d5) can be computed analytically or adversarially, with the latter approach enabling an implicit variational distribution q \u03d5 (z|x) defined by a neural sampler. Training a discriminator D \u03b6 (x, z) distinguishes pairs (x, z) from p * (x)q \u03d5 (z|x), leading to an objective that includes a reverse KL term. In practice, discriminators are not optimal, so the model is trained by alternating gradients and maximizing objectives. Experiments evaluate generation and reconstruction ability on CIFAR-10 and TinyImageNet datasets using a standard ResNet architecture for the encoder, generator, and discriminators. The complete architecture description and hyperparameters for \u03bb-IJAE can be found in the Appendix. Comparison with other autoencoding methods is done using official and publicly available code for baselines. In experiments, for symmetric likelihoods r(y|x) and r (y|x) we use a continuous distribution over cyclic shifts in all directions of an image x. To sample from it, we choose one of four directions (top, bottom, right, left) equally probable and then sample the size of a shift (maximum size S = 5 pixels) from 0 to S. For r (y|x), we use N (y|x, \u03c3I) as a symmetric likelihood with \u03c3 = 10 \u22128. Cyclic shifts are found to achieve better results compared to small rotations for r(y|x) as an implicit likelihood. Evaluation of the model includes generation and reconstruction tasks using Inception Score (IS) and LPIPS metrics. Ablation study compares \u03bb-IJAE with modifications using standard Gaussian or Laplace distributions instead of implicit conditional likelihood r(y|x). In Figure 2, \u03bb-IJAE is compared with \u03bb-IJAE-L 2 and \u03bb-IJAE-L 1 in terms of IS and LPIPS metrics, showing significant outperformance. Evaluation on CIFAR-10 and TinyImageNet datasets against baselines with visual and quantitative results in Figure 3 and Table 1 demonstrates \u03bb-IJAE's superior trade-off between reconstruction and generation quality. The reconstruction quality metrics of various baseline models were compared, with lower values indicating better performance. WAE had a score of 4.18, ALI had a score of 5.34, ALICE had a score of 6.02, and VAE (resnet) had a score of 3.45. Different models showed varying performance in terms of IS score and LPIPS, with the choice of \u03bb impacting the trade-off between these metrics. The \u03bb = 0.3 was chosen as a trade-off between generation and reconstruction ability of \u03bb-IJAE, achieving state-of-the-art balance. The fusion of VAE and GAN models in \u03bb-IJAE combines sharp, coherent samples with low-dimensional representations. The model's objective is equivalent to the Jeffreys divergence, demonstrating a good balance between generation and reconstruction quality. The Jeffreys divergence is confirmed as the right choice for learning complex high-dimensional distributions with limited model capacity. The second term is shown to be zero based on certain assumptions. The plot was generated using a target distribution mixture and a model with an equiprobable mixture of two Gaussians. The optimal \u03b8 was found through stochastic gradient descent iterations on Monte Carlo estimations, with 50 independent runs to explore different optima."
}