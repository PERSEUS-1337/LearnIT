{
    "title": "SJeLO34KwS",
    "content": "In this paper, the proposed method, Dimensional reweighting Graph Convolutional Networks (DrGCNs), aims to address the variance in dimensional information in node representations of GCNs. DrGCNs can reduce variance by connecting the problem to mean field theory. A new measure K is introduced to quantify the effect of dimensional reweighting in GCNs and guide its usage. The dimensional reweighting block is lightweight and flexible, compatible with most GCN variants. Experiments on node classification benchmark datasets demonstrate the effectiveness of DrGCNs. Deep neural networks (DNNs) have been widely applied in various fields, including computer vision, natural language processing, and speech recognition. Graph neural networks (GNNs) and graph convolutional networks (GCNs) have attracted interest for learning node presentations of networked data. DrGCNs demonstrate superior performance over existing approaches on node classification benchmark datasets and industrial datasets. In this paper, the authors propose Dimensional reweighting Graph Convolutional Networks (DrGCNs) to improve GCNs by reweighting node representations based on covariance between dimensions. This reweighting process reduces input covariance by 68% on the Reddit dataset and decreases misclassified cases by 40% compared to previous methods. DrGCNs outperform existing approaches on node classification benchmarks and industrial datasets. The experimental performance of GCNs can be greatly improved by reweighting the input of each layer with global node representation information. This reweighting scheme also enhances the stability of fully connected networks, providing insights to GCNs. A new measure is developed to quantify the effectiveness of the reweighting scheme under different contexts, showing predictable improvements on public datasets over state-of-the-art GCNs. Additionally, two well-known benchmarks (Cora, Citeseer) are fixed for duplicates and feature-label information leaks. The proposed DrGCNs address issues with duplicates and feature-label leaks in datasets, providing refined data for fair comparisons. The DrGCNs are deployed on a recommendation system, showing performance improvements through offline evaluations. The focus is on undirected graphs with node and edge sets, and node features. The input and output node representations are denoted for GCN layers, with H0 as the input representation of the first layer and Hl as the output representation of the lth layer. A GCN layer uses input node set V, adjacency matrix A, and input representations R in to generate output representations R out using a linear aggregator. The procedure involves a matrix multiplication on a refined adjacency matrix \u00c3 with a bias added, formulated as Wx + b. Development on GCNs focuses on different ways to generate \u00c3, with variants like simply taking \u00c3 = AD^-1 for uniform average among neighbors. Some GCN variants propose different methods for generating adjacency matrix \u00c3, such as weighted by degree or using attention mechanisms. To improve scalability, sampling procedures are used in some variants, where a subset of the neighborhood is aggregated. The proposed DrGCN aims to learn a reweighting vector to adjust node representations in each dimension. The proposed DrGCN aims to learn a reweighting vector to adjust node representations in each dimension. A DrGCN layer is formulated by generating a global representation and feeding it into a neural network to generate a reweighting vector. The dimensional reweighting block (Dr Block) can be easily combined with existing GCN variants without affecting the calculation of other parameters. The proposed Dr Block in DrGCN adjusts node representations without affecting other parameters. It can be combined with existing methods without contradictions. The complexity of Dr Block is O(ag), where g is the dimension of g, introducing negligible computational cost. The study connects to mean field theory. The proposed Dr Block in DrGCN adjusts node representations without affecting other parameters, connecting to mean field theory. The Dr Block reduces learning variance from input perturbations, ensuring stable updates in the long run. The stability gained by Dr Block is quantified, with theoretical analyses provided for fully-connected networks. The analysis aims to provide insights for GCNs based on fully-connected networks. In the context of adjusting node representations, neural networks with constant width and ReLU activation are analyzed using a pre-activation recurrence relation. The mean field approximation is applied to characterize the fluctuation of the random variable, focusing on the covariance structure. The analysis aims to provide insights for fully-connected networks. The covariance matrix is updated using mean field approximation in a dynamical system with a fixed point. Reduction of V \u03c6 focuses on the second slot, and the derivative of C l measures algorithm sensitivity to data structures. Dr can alleviate this sensitivity, with a fixed point for most activation functions. The recursive map for activation functions has a fixed point, where linearization is most useful. Derivatives are linear maps from symmetric matrices to symmetric matrices. The method brings variance reduction and stability to updating algorithms. In this section, a quantified measurement of the stability improvement of updating algorithms for DrGCNs is defined using a relative covariance measure. The measure, denoted as K, is homogeneous of degree 0 with respect to the entries of the covariance matrix C. The Frobenius norm is used to measure covariance, and the analysis is carried out under the dimensional normalized assumption by replacing \u03c6 with d \u03c6. The exponential growth rate of the deviation of C from the fixed point C* is proportional to K. The derivative of V d\u03c6(I, C) has a clear eigenspace decomposition. The linear operators satisfy the DOS and ultrasymmetry conditions. The Frobenius norm of the component in VG is proportional to K. Consider the orthogonal aspect in terms of Frobenius norm. The effect of Dr is to reduce the RG\u2212component at each step to stabilize the dynamic system. The orthogonal eigendecomposition reduces recall and focuses on the relative magnitude of s li. The proposed Dr Block's effectiveness measure involves the BSB1 fixed point and requires symmetry and dimensional normalization. The proposed Dr Block aims to improve stability in the learning process by reducing the RG-component and focusing on the relative magnitude of s li. The effectiveness of Dr Blocks is measured by the ratio of variance reduction and its impact on the exponential growth rate. Experimental results on various datasets show the performance of DrGCNs compared to state-of-the-art methods. The performance of DrGCNs is evaluated on public benchmark node classification datasets like Pubmed, Reddit, and PPI. Issues with the Cora and Citeseer datasets are identified, including duplicated samples and information leaks. To address these problems, duplicated samples are removed, features are modified using word and text embeddings, and two refined datasets are constructed. Using word and text embeddings to reduce information leak, two refined datasets, CoraR and CiteseerR, were constructed. The DrGCN model is compared with various competitors on citation networks like CoraR, CiteseerR, and Pubmed. Additionally, the DrGCN is evaluated on inductive datasets like the PPI dataset, showing promising results compared to state-of-the-art methods. The DrGCN model is compared with various competitors on citation networks like CoraR, CiteseerR, and Pubmed. DrGCNs achieve superior performances on all datasets when combined with different variations of GCN models. Performance improvements are attributed to a stability measure proposed in Equation (20). The Dr-ASGCN model demonstrates significant improvements in performance on different datasets, with a focus on stability and refinement of representation. The K-value analysis shows that the model achieves a massive improvement on the Reddit dataset due to specific characteristics in the first layer. Additionally, the Dr method outperforms previous methods on the inductive PPI dataset and real-world recommendation datasets, showcasing substantial improvements in various metrics. The DrGCN model shows improvement on industrial measure recall@50, comparing favorably with Batch-Norm and Layer-Norm methods on the Reddit dataset. Combining Dr with Layer-Norm yields even better results for ASGCN on Reddit. The idea of using neural networks for graph-based data can be traced back to Graph Neural Network (Scarselli et al., 2009) and GCN (Kipf & Welling, 2017). GraphSAGE (Hamilton et al., 2017) introduced a sample and aggregation framework for inductive node embedding. Methods like FastGCN and GAT (Veli\u010dkovi\u0107, 2019) focus on identifying important nodes in the graph through weighted sampling and aggregation. Feature imbalance has been a long-standing issue (Blum & Langley, 1997). The idea of refining hidden representations in neural networks can be traced back to Network in Network (Lin et al., 2014) and Squeeze and Excitation Networks (Hu et al., 2018) which propose methods for dimensional reweighting in neural networks. These methods have been successful in computer vision tasks and may also be useful in node representation. In refining neural network representations, normalization techniques like BatchNorm and LayerNorm have been effective. Various studies have shown the benefits of batch normalization, such as improved convergence rates and enabling larger learning rates. Our approach involves applying these methods to analyze DrGCNs. The Cora and CiteSeer datasets are widely used in research, provided by Planetoid. They are citation networks where each node represents a research paper, and edges represent citation relationships. Papers are labeled by sub-field in computer science and have bag-of-word features. Cora has 2,708 papers with a dictionary size of 1,433, while Citeseer has 3,327 papers with a dictionary size of 3,703. The Cora dataset, originating from (McCallum et al., 2000), includes 2,708 papers with labels and BOW feature vectors. Planetoid (Yang et al., 2016) reorders nodes to create the benchmark dataset. Issues include duplicated papers and information leaks in the dictionary generation process. The dictionary generation process in the Cora dataset may lead to information leaks, as some papers can be easily classified based on their labels. CiteSeer, a digital library, provides a web API for each paper. Planetoid reorders nodes in the dataset to address issues like duplicated papers. The Citeseer dataset contains 3,186 papers with title and abstract information, with 161 papers being duplicates. 61.8% of the papers have labels in their title or abstract, potentially leading to information leaks. The dataset shares similarities with Cora in terms of embedding generation method. The Citeseer dataset has been modified to address issues with duplicated nodes and information leaks. New datasets CoraR and CiteSeerR have been created, with pre-trained word embeddings used as node features. Errors in paper titles have been corrected, and problematic papers have been manually searched for and identified. In the Cora dataset, 32 out of 2,708 papers were found to be duplicates, while 9 papers were missing. The titles of the remaining 2,680 papers were corrected and used to generate features. Averaged GLOVE-300 word vectors were applied to the titles, along with additional dimensions for title length. This resulted in a 302-dimensional feature representation for each node in CoraR. The dataset was split into training, validation, and test sets, following a similar approach as in previous studies. Additionally, manual searches were conducted to supplement the dataset with additional documents. In the refined CiteSeerR dataset, 161 papers were identified as duplicates and combined, while 55 papers were removed due to untraceability. The dataset now consists of 3,191 nodes and 4,172 edges. Each paper has a 768-dimensional feature generated from the title and abstract using BERT. The dataset was split into training, validation, and test sets, with 1,691, 500, and 1,000 nodes respectively. The code for both CoraR and CiteseerR datasets is available. For dimensional reweighting, the encoder dimension is set to the square root of the input node representation dimension. Pooling weight is uniform. ELU activation is used for \u03c3 g and sigmoid activation for \u03c3 s. Dropout is not applied, but weight regularization is used. Other settings such as learning rate, batch size, and regularization are kept the same as the original models. All methods are run 20 times and average accuracy is reported. We evaluate the performance of methods based on released code and paper. Our method implementation details include using GCN with 2 layers and 64 hidden units, and GAT with dataset-specific structures and early stopping strategy. Dropout rate of 0.6 is used for certain datasets. For CoraR, CiteseerR, and Pubmed datasets, a dropout rate of 0.35 was found to be optimal for the GAT model. Different dataset-specific structures were used, with varying numbers of hidden units and attention heads. The structure for Pubmed did not perform as well as for Cora and Citeseer. The GAT model with a three-layer structure is used for the PPI dataset, with specific configurations for hidden units and attention heads. A graph embedding model is utilized for the industrial A* dataset, focusing on item recommendation with a large training set. In the A* company, a graph embedding model is used to recommend items to users based on their attributes. The model calculates the top-N items using K nearest neighbor with Euclidean distance. The recall@50 metric is used to evaluate the model, which achieved a 5.264% improvement from the baseline model. Additionally, batch-norm and layer-norm GCN results are provided in Table 12. The results of batch-norm and layer-norm GCN on publication datasets are provided in Table 12. Theorem 1 is proven in the main article, showing that DOS operators do not require fixed point structure information to hold. DOS operators involve entries with dimension and a basis M ij = E ij + E ji. The dimension and basis of entries in DOS operators are defined as M ij = E ij + E ji. The span of L i is denoted as L d, ensuring linear independence with elements in M d. The eigen properties of TM ij = wM ij and TL i = uL i can be calculated using the definition equations of DOS(u, v, w) on a component level. The theoretical analysis is based on the pre-activation setting, differing slightly from common GCN methods using post-activation. In the post-activation setting, there is a difference between pre and post activation as pre-activation activates the input feature. Pre-activation was also experimented on GCN, with results averaged among 20 runs."
}