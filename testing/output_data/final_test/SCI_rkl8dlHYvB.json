{
    "title": "rkl8dlHYvB",
    "content": "We propose a learning-based iterative grouping framework to discover 3D parts for objects in unseen categories. Our approach transfers knowledge of parts learned from 3 training categories to 21 unseen testing categories without annotated samples, achieving state-of-the-art performance on the PartNet dataset. In the study of vision systems, perceptual grouping determines regions of visual input that belong together. Deep learning allows for learning grouping cues from annotated datasets, but current algorithms are inferior for unknown object categories. The focus is on zero-shot part discovery for 3D shapes, transferring knowledge from training to unseen testing categories without annotated samples. In the study of vision systems, perceptual grouping determines regions of visual input that belong together. Deep learning allows for learning grouping cues from annotated datasets, but current algorithms are inferior for unknown object categories. The focus is on zero-shot part discovery for 3D shapes, transferring knowledge from training to unseen testing categories without annotated samples. The research emphasizes using 3D data for shape segmentation due to its robustness to distortions compared to 2D image data. Deep neural networks have shown state-of-the-art performance in segmenting shape parts by leveraging global context to improve part semantics and shape structures. The study focuses on improving cross-category generalization in shape segmentation. While deep learning methods excel on training categories, they struggle with unseen categories due to global shape differences. Classical methods using manually designed features with local context perform better on unseen categories but give inferior results on training categories. The goal is to develop a learning-based framework that avoids excessive context information to enhance generalization. Our algorithm aims to improve cross-category generalization in shape segmentation by proposing superpixel-like sub-parts for each shape and learning a grouping policy to progressively group these sub-parts. Unlike prior deep segmentation work, we focus on learning part-level features and use a learning-based agglomerative clustering framework inspired by Reinforcement Learning. This approach restricts features to convey information within the local context of a part, making a step towards generalizable part discovery. The algorithm aims to improve cross-category generalization in shape segmentation by proposing sub-parts for each shape and learning a grouping policy. It introduces a learning-based agglomerative clustering framework for part discovery in unseen object categories, demonstrating state-of-the-art results. The curr_chunk discusses various methods for mesh segmentation and mesh cosegmentation, including watershed, K-means, core extraction, graph cuts, random walks, spectral clustering, and primitive fitting. It also mentions the use of point clouds for shape segmentation and the segmentation of point cloud shapes under part convexity constraints. The approach in the curr_chunk learns shared part priors from training categories to improve cross-category generalization in shape segmentation. The curr_chunk discusses recent supervised learning approaches in shape segmentation using various representations such as 2D images, 3D voxels, sparse volumetric representations, point clouds, and graph-based representations. These approaches have led to advancements in the state-of-the-art in shape segmentation tasks. Our work focuses on learning context-free part knowledge and performing part discovery in a zero-shot setting on unseen object classes. Previous works have attempted to reduce supervision for shape part segmentation by using sparsely labeled data, active learning frameworks, and semi-supervised optimization models. Our work focuses on zero-shot part discovery in point cloud data, related to recent research on learning-based 2D instance segmentation methods. Various approaches utilize per-pixel embedding and clustering algorithms for segmentation, while our work differs significantly. Our work focuses on zero-shot shape part discovery on 3D point clouds in unseen object categories. Unlike previous methods, we do not rely on a fully convolutional neural network for processing the entire scene, allowing for better generalization to unseen categories by reducing the influence of context. Our approach involves exploiting part prior encoded in the dataset and utilizing only part-level inductive biases to reduce overfitting of global contextual information. Shape part discovery on point clouds involves identifying interesting semantics in groups of points sampled from 3D models. A set of part proposals comprises regions useful for various tasks, with the goal of predicting these parts accurately. Ground-truth proposals are manually labeled parts for downstream tasks, and the algorithm aims to predict these parts within a specified limit. Zero-shot shape part discovery focuses on unseen object categories, utilizing part-level biases to reduce overfitting of global context. Our method for zero-shot shape part discovery involves proposing small superpixel-like sub-parts of shapes and iteratively grouping them to produce larger sub-parts based on ground-truth information. No ground-truth part proposals are provided for shapes in testing categories. Our method involves proposing small sub-parts of shapes and grouping them to produce larger sub-parts based on ground-truth information. The perceptual grouping process is formulated as a contextual bandit, where a policy network selects pairs of sub-parts for verification. The policy network consists of a purity module and a rectification module. The rectification module inputs binary information to decide which pair to select, with technical network design choices described in Section 4.1. The on-policy training scheme from Reinforcement Learning is used to match data distribution during training and inference stages. The Rectification Score Purity Module groups sub-parts belonging to the same ground-truth part based on a purity score U (P). A purity module predicts the purity score using a PointNet architecture. The rectification module corrects failures of the purity module by considering local information to select the best pair of sub-parts for grouping. The rectification module compares two sub-part features for decision making, selecting the best pair for grouping based on purity and rectification scores. The policy score is defined as the product of purity and rectification scores, with the policy being a distribution over all possible pairs. The policy \u03c0(P i , P j |P) is a distribution over pairs with a softmax layer. The reward M (P i , P j ) determines if two sub-parts can be grouped based on instance labels and purity scores. A verification network is trained to decide if the pair should be grouped. The verification network, a combination of the purity and rectification modules, focuses on selected pairs for double verification. It uses PointNet to extract features and employs Reinforcement Learning for joint training with the policy network. The policy network and verification network are trained on-policy using ideas from Reinforcement Learning. The training alternates between data sampling and network training steps, with trajectory data used to compute losses and update network parameters through gradient descents. The on-policy training algorithm involves using gradient descents to update network parameters. An epsilon-greedy strategy is adapted during training, starting with 80% random sampling and gradually decreasing. Sub-parts are preprocessed and grouped, with data batches used to update modules and networks. Random actions aid in exploration and data augmentation. The use of a replay buffer in training networks allows for efficient data utilization and improved transfer performance in unseen categories. Batch data is sampled from the replay buffer to update policy networks using losses and gradients. The framework utilizes a replay buffer to sample batch data for updating the rectification module and training the purity and verification networks. Quantitative evaluations and comparisons with previous methods are conducted using the PartNet dataset in a zero-shot part discovery setting. In a zero-shot part discovery setting, the study utilizes the PartNet dataset to analyze how discovered part knowledge transfers across different object categories. PartNet offers detailed part annotations for 3D models from various object categories, with up to three levels of part segmentation. The study focuses on three categories for training and tests on 21 unseen categories, aiming to propose useful parts for various use cases. The study evaluates the predicted part pool coverage of human-defined semantic parts using Mean Recall as the metric. Mean Recall calculates the fraction of ground-truth parts with IoU over a threshold, comparing the approach to previous methods like PartNet-InsSeg. The study compares deep learning-based methods with a non-learning based method called WCSeg, which shows good generalization to unseen categories but is limited by the part convexity assumption. The deep learning methods leverage global shape context for shape part segmentation but struggle with transferring knowledge to new categories. The comparison is done using the Mean Recall metric on PartNet. The study compares deep learning-based methods with a non-learning based method called WCSeg for shape part segmentation. WCSeg requires point normals for local patch continuity, which are approximated from input point clouds. Three levels of part proposals are obtained by manually tuning hyperparameters for training categories. The segmentation levels may not share consistent part granularity across categories. The proposed method involves gathering part proposals from different levels for evaluation on unseen categories. Quantitative and qualitative evaluations show that the approach outperforms baseline methods on average and excels in 11 out of 20 categories. The core of the method is to transfer learned local-context part knowledge from training to novel categories, including non-transferable category-specific information. The proposed method involves transferring learned local-context part knowledge from training to novel categories, including non-transferable category-specific information. Training on various object categories is beneficial for generalizable knowledge, but selecting categories for training is crucial for achieving the best performance. Similar categories like tables and chairs share common part patterns, making it easier to transfer knowledge. The study focuses on transferring part knowledge across category boundaries using a data-driven iterative perceptual grouping pipeline. Experimental results show that chairs and tables transfer part knowledge effectively, while lamps perform poorly in generalizing to other categories. The method aims to learn part-level features within local contexts to generalize the part discovery process to unseen categories. Thorough evaluation and comparisons with state-of-the-art algorithms demonstrate the effectiveness of the proposed approach. Our method successfully extracts part knowledge from training categories and transfers it to unseen categories, outperforming four baseline algorithms on the PartNet dataset. We use a sub-part proposal module to sample seed points and extract local PointNet features for segmentation. The sub-part proposal module extracts features from 512 points around seed points to create a segmentation mask. Sub-parts with a purity score below 0.8 are removed to ensure high quality. The sub-part proposal module extracts features from 512 points around seed points to create a segmentation mask. The input of this learning module is constrained in a local region and is not affected by the global context. To validate its performance, the module is trained on Chair, Storage Furniture, and Lamp of level-3 annotations and tested on all categories with fine-grained level annotations. Experiments are conducted on the effects of involving more context for training models on seen and unseen categories, adding a branch to the verification network for decision making. The sub-part proposal module extracts features from 512 points around seed points to create a segmentation mask. It encodes more context information for decision making, with two branches used to determine grouping. The original binary branch relies on local context, while the newly added branch incorporates more context. Testing the effectiveness of involving more context, the model is trained on Chair, Lamp, and Storage Furniture, with decisions made using either branch based on the size of the sub-part pool. The sub-part proposal module uses additional local context to improve grouping decisions in most scenarios. Involving context helps enhance performance on seen categories but has negative effects on unseen categories. Training the model with more context and using it on unseen categories only when similar context is found during training can lead to higher performance on seen categories without degrading performance on unseen categories. The sub-part proposal module uses additional local context to improve grouping decisions. Involving more context in the late grouping process is restricted to a local region. The policy network learns to pick pairs of sub-parts and uses on-policy training for performance boost. The network includes the purity module and the rectification module for processing unary and binary information, respectively. The quantitative results validate the effectiveness of the approach. The purity module outputs purity scores based on unary information, serving as a partness score. The rectification module corrects failures in the purity network, preventing convergence issues with unbalanced sub-part sizes. The effectiveness of these components is validated through quantitative results. The rectification module corrects unbalanced sub-part sizes to prevent performance drops, as shown in Table 5. On-policy training matches the inference process without the need for a sampling strategy, while off-policy training results in decreased performance. The rectification module corrects unbalanced sub-part sizes to prevent performance drops. It encourages the selection of equal size pairs of sub-parts, improving model accuracy. The relative size of selected pairs is defined to ensure balance, with a minimum value of 2. The rectification module helps to choose more equal size pairs during the grouping process, preventing performance drops and improving model accuracy. It ensures balance by selecting pairs with a minimum size value of 2, as shown in Figure 6. The proposed method aims to improve model accuracy by forming regular intermediate sub-parts during the grouping process. Training will be done on three categories (Chair, Lamp, and Storage Furniture) and tested on all categories. The input size for the proposed method is 1024 points, providing advantages in GPU memory cost. Semantic segmentation loss is removed as the task does not require semantic labels. The training process for deep learning methods involves specific settings and hyper-parameters. PartNet-InsSeg uses default settings with 200 output instances and a total of 1.93 \u00d7 10^6 parameters, taking 4 days to train. SGPN follows a similar experiment setting with a max group number of 200, 1.55 \u00d7 10^6 parameters, and takes 3 days to train. GSPN has its own specifications for detection per... The GSPN model processes shapes in the inference phase, with a maximum of 200 detections per shape and 512 points per instance. NMS with a threshold of 0.8 is applied after proposal generation. Training involves 20 epochs for GSPN and another 20 epochs jointly with R-PointNet. The model has 14.80 \u00d7 10^6 parameters, with GSPN accounting for 13.86 \u00d7 10^6 parameters. WCSeg requires outward normals as input, generated using ball-pivoting to reconstruct surfaces for fair comparison. The study uses WCSeg with outward normals generated using ball-pivoting for surface reconstruction. Grid search is conducted on 300 object instances from seen categories (Chair, Lamp, Storage Furniture) to tune parameters (\u03b8 1 , \u03b8 2 , \u03b8 3 , \u03c3) in WCSeg, with relative shifts applied to form 81 parameter sets. The set with the highest mean recall is chosen for further testing. In the experiments, parameter sets (\u03b8 1 = 0.950, \u03b8 2 = 0.704, \u03b8 3 = 0.403, \u03c3 = 0.069) for Level-1, (\u03b8 1 = 1.426, \u03b8 2 = 0.845, \u03b8 3 = 0.504, \u03c3 = 0.086) for Level-2, and (\u03b8 1 = 1.188, \u03b8 2 = 0.563, \u03b8 3 = 0.504, \u03c3 = 0.069) for Level-3 were selected based on the highest mean recall. The MATLAB code was executed on an Intel(R) Xeon(R) Gold 6126 CPU cluster with 16 cores. WCSeg took about 2.2 minutes per shape per CPU core for inference and 4 days to complete testing on PartNet's dataset. Sub-part proposals were generated using a proposal module with specific training parameters. During training, the proposal module is trained on level-3 annotations and used for training the policy and verification networks on all three levels. The initial sub-parts pool is created by gaining 128 proposals and removing those with purity scores below 0.8. The policy network, purity module, and rectification module are used to select pairs of sub-parts during the grouping process, with the verification network determining whether to group the pair. New grouped parts are added to the sub-parts pool if they should be grouped. During training, the policy network, purity module, and rectification module are used to group sub-parts by selecting pairs and determining whether to group them using the verification network. The process iterates until no more pairs are available. Training data is collected from generated trajectories, with 64 pairs sampled per iteration. To speed up training, 10 pairs are sampled from the 64 pairs for verification. The number of random sampling pairs decreases with each epoch using an epsilon-greedy strategy. In the inference phase, policy scores are calculated for all pairs. During the inference phase, policy scores are calculated for all pairs, and the pair with the highest score is sent to the verification network. The on-policy manual selection is crucial for performance, with specific criteria for grouping pairs at different levels. Training data is collected from trajectories to form replay buffers for each module, limited to data from 4 trajectories per buffer. During training, the modules are trained on replay buffers with data from 4 trajectories. The batch sizes vary for each module, with the whole training process taking 4 days on a single 1080Ti GPU. Inference takes about 3 seconds per shape, and the model has 0.64 \u00d7 10^6 parameters. Performance metrics are presented in Table 6, with a focus on seen categories. The paper was published at ICLR 2020. The paper presents qualitative results of various methods for zero-shot part discovery, trained on Chair, Lamp, and Storage Furniture of level-3 PartNet Dataset and tested on unseen categories. Ground-truth annotations are provided for reference. Table 6 shows quantitative evaluation with different algorithms. The text chunk provides information on different average metrics (Avg, SAvg, WSAvg, UAvg, WUAvg) calculated for segmentation results in PartNet, categorized by seen and unseen categories and weighted averages."
}