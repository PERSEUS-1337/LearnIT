{
    "title": "BJgLpaEtDS",
    "content": "The Poincar\u00e9 Wasserstein Autoencoder is a reformulation of the Wasserstein autoencoder framework on a non-Euclidean manifold, specifically the Poincar\u00e9 ball model of hyperbolic space. It utilizes the intrinsic hierarchy of the hyperbolic latent space to impose structure on learned representations, allowing for recovery of structure in low-dimensional latent spaces for datasets with hierarchies. The model is demonstrated in the visual domain to analyze its properties. Variational Autoencoders (VAE) are unsupervised machine learning models that use approximate inference to parametrize the posterior distribution. They are commonly used for generative modeling but can be limited by assuming a Gaussian standard prior for latent variables. Real-world datasets often have structured relationships between variables, which the standard VAE cannot capture due to its unimodal distribution assumption. Attempts have been made to address the limitations of VAEs by either enhancing the posterior distribution or imposing structured priors. The use of non-Euclidean geometry, such as hyperbolic spaces, has shown promise in improving machine learning tasks by capturing hierarchical relationships in datasets with tree-like structures. In this work, the Poincar\u00e9 Wasserstein Autoencoder (PWA) is proposed as a model that parametrizes a Gaussian distribution in the hyperbolic space Hn. The latent space is treated as a Riemannian manifold with constant negative curvature, allowing for a hierarchy to be imposed on the latent space representation. This approach is demonstrated on a synthetic dataset and evaluated using a distortion measure for Euclidean and hyperbolic spaces. The Poincar\u00e9 Wasserstein Autoencoder (PWA) parametrizes a Gaussian distribution in hyperbolic space Hn. It uses the Maximum Mean Discrepancy (MMD) objective to match prior and posterior distributions. The model architecture and sampling procedures are described, with comparisons to the Euclidean VAE on tasks like MNIST digit generation and semi-supervised link prediction. The paper also reviews related work and defines probability distributions on Riemannian manifolds. The Poincar\u00e9 Wasserstein Autoencoder (PWA) uses the MMD objective to match prior and posterior distributions. Various extensions to the original VAE framework have been explored, including improving the approximation of the posterior and imposing structure on the latent space. The use of discrete latent variables has also been investigated in related works. Several works have explored hyperbolic geometry and Riemannian manifolds in relation to graph generation and complex network analysis. Different models and methods, such as hyperspherical latent space and Bayesian inference, have been proposed to enhance learning performance and develop word ontology embeddings. The natural gradient method and particle approximations have also been utilized in this research area. Three concurrent works have explored hyperbolic latent spaces for learning performance enhancement. One approach uses a Wasserstein formulation, another employs a wrapped Gaussian distribution on the Lorentz model, and a third utilizes an adversarial autoencoder model. Differential geometry concepts are briefly outlined to define the model on a Riemannian manifold. A Riemannian manifold is defined as a tuple (M, g), where tangent spaces are defined at every point x on the manifold M. The Riemannian metric g consists of inner products on the tangent spaces. Smooth curves on the manifold can be computed using the Riemannian metric to determine curve length and geodesics. The exponential map allows mapping a vector in the tangent space to a point on the manifold, particularly in the Poincar\u00e9 ball model of hyperbolic space. The Poincar\u00e9 ball model of hyperbolic space is geodesically complete and defined on the whole tangent space. It features constant negative curvature and is one of the five isometric models of hyperbolic space. The geodesic distance on the Poincar\u00e9 ball is determined by the hyperbolic metric. The Poincar\u00e9 ball model of hyperbolic space relies on gyrovector spaces for arithmetic operations. Gyrovector addition and Hadamard product are used for reparametrization. Exponential and logarithm map operators map points onto the manifold. Gaussian decoder network is symmetric to the encoder network, with a Gaussian distribution as a common prior for VAE style models. The Poincar\u00e9 ball model uses a maximum entropy generalization of the Gaussian distribution as a prior for VAE style models in hyperbolic space. The Gaussian probability density function is defined using the Fr\u00e9chet mean and dispersion parameter, with geodesic distance and a dispersion dependent normalization constant. Radius for curvature is assumed to be 1 in polar coordinates. The closed form of the hyperbolic Gaussian distribution allows for vectorial or full covariance matrix representations, enhancing expressivity in learned representations. Maximum mean discrepancy estimation via samples eliminates the need for a closed form definition of the posterior density, enabling richer latent space representations. Our model utilizes a variational autoencoder architecture with hyperbolic space mapping for richer latent space representations. The encoder and decoder networks have three fully-connected layers with ReLU activations. Hyperbolic feedforward layers are used for encoding variational parameters. Posterior samples in hyperbolic space are obtained using mean parametrization with a hyperbolic feedforward layer. The mean parametrization in the encoder network utilizes a hyperbolic feedforward layer for the weight matrix and Riemannian stochastic gradient descent for bias parameters. The outputs from the Euclidean network are transformed using the hyperbolic feedforward layer map with a hyperbolic nonlinearity. The reparametrization trick involves using a differentiable function to make the sampling operation differentiable, allowing for backpropagation through the stochastic layer. The reparametrization trick for the Gaussian distribution in the hyperbolic space involves using gyrovector operators to obtain posterior samples for the parametrized mean and dispersion. Samples are generated from the hyperbolic standard prior and projected to the tangent space before being scaled and translated back to the manifold. The hyperbolic standard prior N H (0, I) is chosen as the prior distribution for generating samples. The standard prior N H (0, I) is used to generate samples in hyperbolic space. Quasi-uniform samples on the Poincar\u00e9 disk are obtained, and a rejection sampling procedure is used for radius samples. The variational autoencoder relies on the evidence lower bound (ELBO) for tractable optimization of the Kullback-Leibler divergence. The KLD integral has a closed-form expression in the Euclidean VAE formulation. The KLD integral has a closed-form expression, simplifying the optimization procedure. The ELBO can be extended to non-Euclidean spaces using the volume element of the manifold. Approximating the ELBO with Monte-Carlo samples is considered suboptimal due to high variance. The approach of using Monte-Carlo approximations for the integral is considered suboptimal due to high variance. To address this, a Wasserstein Autoencoder (WAE) formulation is proposed for variational inference, focusing on solving the optimal transport problem in the latent space. This approach differs from the GAN literature by matching distributions in the latent space rather than the data distribution directly. Kantorovich's formulation of the optimal transport problem involves finding the optimal couplings between distributions, which is a challenging task. The WAE model addresses the challenging task of finding optimal couplings between distributions by focusing on the latent space. It involves sampling a latent variable z from p(z) and mapping it to the output space using a parametric decoder f \u03b8 (x|z). The optimization is over the encoders q \u03c6 (x) instead of the couplings between distributions. The WAE objective relaxes constraints on the posterior q using a Lagrangian multiplier and an appropriate divergence measure. The Maximum Mean Discrepancy (MMD) metric with a positive definite RKHS kernel is a divergence measure used in matching high-dimensional distributions. The Laplacian kernel is chosen for its positive definiteness in hyperbolic spaces and heavier tails compared to the Gaussian RBF kernel, which helps with outlier gradients. The MMD loss function is defined over two probability measures in an RKHS unit ball. Parameter updates in the hyperbolic latent space require Riemannian stochastic gradient descent. Gyrovector arithmetic is used for updating gradients in the hyperbolic parameters to avoid numerical problems. Euclidean parameters are updated separately. The model updates Euclidean parameters in parallel using Adam optimization. Two experiments measure distortion in latent space embeddings. The first dataset includes synthetically generated noisy binary trees with vertices from a normal distribution. Monotonous growth of tree vertices' norms is enforced for a good embedding in a hyperbolic space. In a hyperbolic space, norms of tree vertices grow monotonously with depth by rejecting samples with smaller norms than parent vertices. Model trained on 100 trees for 100 epochs with \u03c3 i = 1 and \u03c3 j = 0.1. Comparison of distortion values in latent space embeddings between Euclidean VAE and PWA models shows PWA model has less distortion in d = 2 space. Experiment also includes comparison with T-SNE technique on generating MNIST digits. In this experiment, the model is applied to generating MNIST digits to understand the properties of the latent hyperbolic geometry. The visual distribution of latent codes in the Poincar\u00e9 disk space is of interest, comparing the model to the Euclidean VAE approach. Training on binarized MNIST digits, the generated samples show decreasing quality with higher dimensionality despite lower reconstruction error, due to a mismatch between selected and intrinsic latent space dimensionality. In this experiment, the advantages of using a hyperbolic latent space for predicting links in a graph are explored. Training is done on three citation network datasets: Cora, Citeseer, and Pubmed. The Poincar\u00e9 disk structure pushes samples towards the disk's outer edge due to exponential growth in hyperbolic spaces. Quality sample generation requires overlap between prior and posterior in latent space, with the issue alleviated in higher dimensions as the distribution shifts towards the ball surface. The study explores using a hyperbolic latent space for training a model on citation network datasets: Cora, Citeseer, and Pubmed. The Variational Graph Auto-Encoder (VGAE) framework is utilized in an unsupervised manner, with performance measured in terms of average precision (AP) and area under curve (AUC). Results show improvements over a Euclidean latent space baseline on the Cora and Citeseer datasets, with comparisons made to a hyperspherical autoencoder (S-VGAE). The smaller dimensionality of the hyperbolic latent space is attributed to the hierarchical nature of the dataset's latent manifold. The study explores using a hyperbolic latent space for training a model on citation network datasets. The PWA outperforms the Euclidean VAE on two of the three datasets, but the hyperspherical graph autoencoder (S-VGAE) outperforms both models. The algorithm for amortized variational inference on the Poincar\u00e9 ball model of hyperbolic space shows improved performance on tasks with a hierarchical structure. Future work aims to address issues with the MMD metric in hyperbolic space and extend current results. The model's capabilities will be demonstrated on tasks with a latent hyperbolic manifold, exploring mixed curvature settings. Operations in gyrovector addition are detailed, assuming a Poincar\u00e9 ball radius of c=1. In gyrovector addition, the formula is x \u2295 y = (1 + 2x, y + ||y||^2)x + (1 - ||x||^2)y."
}