{
    "title": "ryTp3f-0-",
    "content": "Reinforcement learning agents struggle with sparse rewards, hindering learning progress in tasks like booking flights or replying to emails. To address this, a proposed method involves constraining exploration using demonstrations to induce high-level workflows. These workflows guide the agent's actions, accelerating the discovery of successful sequences while avoiding overfitting issues associated with expert pre-training. The approach involves using workflows to guide reinforcement learning agents in exploring web tasks efficiently, achieving state-of-the-art results on the World of Bits benchmark. This method improves sample efficiency over behavioral cloning by more than 100x, aiming to train agents to navigate human-readable web interfaces for tasks like booking flights or replying to emails. Pre-training the agent with expert demonstrations via behavioral cloning in web tasks with sparse rewards can be slow. Warm-starting with behavioral cloning often fails to improve over pure RL due to overfitting in diverse and complex website environments. Simple strategies like using fewer parameters may not effectively combat overfitting. In this work, a method for leveraging demonstrations is proposed to constrain exploration and accelerate the agent's ability to discover sparse rewards. Demonstrations are used to prune bad exploration directions, allowing for faster learning. The process involves inducing a workflow lattice from demonstrations, sampling workflows, updating policies based on rewards, and periodically updating the policy with sampled episodes. Workflow-guided exploration (WGE) involves inducing workflow lattices from demonstrations to guide exploration. The workflow policy \u03c0 w samples episodes from workflows, saving successful ones to a replay buffer for training the neural policy \u03c0 n. This approach allows for a sophisticated neural policy with reduced overfitting risk, using high-level workflows to constrain exploration. The workflow-guided exploration (WGE) framework involves extracting workflows from demonstrations to guide exploration. A workflow exploration policy is defined to select workflows and sample actions, gradually learning through reinforcement learning. This approach leverages environment-blind workflows to constrain exploration and train a neural policy with reduced overfitting risk. The policy learns workflows through reinforcement learning, using reward-earning episodes for training a neural network policy. The state space in web tasks includes structured (HTML) and unstructured inputs (natural language, images), leading to the proposal of a novel neural network policy (DOMNET) for flexible relational reasoning. Evaluation includes tasks like MiniWoB benchmark and flight booking interface for Alaska Airlines, addressing challenges like noisy environments and variation in natural language. Our system achieves high success rates with only 3-10 demonstrations per task in reinforcement learning. Agents learn policies through trial-and-error by observing states and choosing actions to maximize expected return. The agent learns policies through trial-and-error by observing states and adjusting their policy based on the results of episodes with delayed and sparse rewards. The agent is given a goal, represented as a structured key-value mapping or a natural language utterance, and its state consists of the goal and the current state of the web page. The action space is restricted to click and type actions. The agent learns policies through trial-and-error by observing states and adjusting their policy based on the results of episodes with delayed and sparse rewards. The action space is restricted to click and type actions, where actions are \"similar\" to demonstrated actions in expert demonstrations. Workflows specify similar actions at each time step using a sequence of workflow steps. The agent learns policies through trial-and-error by observing states and adjusting their policy based on the results of episodes with delayed and sparse rewards. Workflows are induced from expert demonstrations by enumerating possible workflow steps at each time step, allowing for shortcut steps to handle noisy demonstrations. These induced workflows are represented as paths in a workflow lattice. The agent learns policies through trial-and-error by observing states and adjusting their policy based on the results of episodes with delayed and sparse rewards. Induced workflows from expert demonstrations are simplified by ignoring shortcut steps. The effectiveness of workflow steps varies, with some being too specific and others too general. The workflow policy learns which steps to use based on the demonstration scenario. The workflow policy samples a workflow and actions in a demonstration episode based on a provided goal. Each time step involves sampling a workflow step and selecting an action accordingly. The overall probability of exploring an episode is determined by these steps. The workflow policy makes decisions based on a provided goal, sampling actions in a demonstration episode. It is environment-blind, using fewer parameters than a state-dependent policy, enabling quicker learning and preventing overfitting. The policy cannot solve the task but learns good behaviors to assist the neural policy. Training involves using a variant of the REINFORCE algorithm to approximate the gradient after rolling out an episode. The neural policy \u03c0n is trained using on-policy and off-policy updates with A2C algorithm. Successful episodes with positive rewards are stored in a replay buffer for training. A neural architecture called DOMNET captures the DOM tree's structure for model learning. The DOMNET neural architecture captures the structure of the DOM tree. A novel DOM embedder is designed to capture interactions between DOM elements by computing base embeddings and neighbor embeddings. Spatial neighbors within 30 pixels are considered for embedding. The DOMNET neural architecture captures the structure of the DOM tree by computing base embeddings and neighbor embeddings for elements within 30 pixels. Depth-k tree neighbors are defined based on the least common ancestor in the DOM tree. The approach is evaluated on interactive web tasks including MiniWoB, MiniWoB++, and Alaska benchmarks. The MiniWoB and MiniWoB++ benchmarks consist of tasks with a 160px \u00d7 210px environment and text-specified goals. Tasks provide a sparse reward of +1 for success or -1 for failure. Partial rewards were disabled for consistency. The agent interacts with the environment using a Selenium web driver interface. The MiniWoB benchmark includes 80 tasks, with 40 tasks selected for actions within the action space. Specialized reasoning tasks were excluded. 10 demonstrations were collected for each task using Amazon Mechanical Turk. We collected 10 demonstrations using Amazon Mechanical Turk, recording mouse and keyboard events and DOM state. Success rate is the key metric, representing the percentage of test episodes with reward +1. We compare success rates of different approaches on MiniWoB tasks, including SHI17 pre-trained with behavioral cloning and fine-tuned with RL, and our proposed neural policy, DOMNET+BC+RL, pre-trained with behavioral cloning and fine-tuned with RL. During behavioral cloning, early stopping is applied based on validation set rewards. DOMNET+WGE is our neural policy trained with workflow-guided exploration on 10 demonstrations. Comparing SHI17 with DOMNET+BC+RL, we evaluate the contribution of our new neural architecture DOMNET. DOMNET+BC+RL empirically improves success rates over SHI17 on most tasks. Workflow-guided exploration enables DOMNET to perform even better on difficult tasks. The proposed WGE model outperforms the BC+RL model by an average of 42% absolute success rate on challenging tasks like click-checkboxes-large and multi-layouts. The WGE model outperforms BC+RL by 42% on challenging tasks like click-checkboxes-large and multi-layouts. WGE mitigates common failure modes of BC+RL, such as premature termination of episodes and cyclic behavior. The workflow policy learned by WGE is too simplistic for test time due to ignoring environment state differences. The workflow policy learned by WGE is too simplistic for test time due to ignoring environment state differences. The workflow constraint language lacks expressivity to specify certain actions, such as clicking on synonyms of a particular word in click-checkboxes-soft. The neural policy can achieve high success rates even when the workflow policy performs poorly. The approach can be applied to natural language goals using a training dataset collected with the BID47 technique. The WGE model can understand natural language goals with a 93% success rate. The workflow policy requires structured inputs due to the constraint language limitations. The approach was applied on the Alaska benchmark. The approach was applied on the Alaska benchmark for a flight search task on the Alaska Airlines mobile site. The task involves completing a flight search form with provided information, with a sparse reward system based on correct form fields. The environment includes over 200 DOM elements and requires at least 11 actions per episode. The probability of a random agent receiving a positive reward is less than 10^-20. Our approach achieved high rewards on the Alaska Airlines task with minimal demonstrations, showcasing its efficiency in handling complex tasks on real-world websites. Comparing with other methods, our approach demonstrated superior performance in terms of demonstration efficiency. Our approach achieved high rewards on the Alaska Airlines task with minimal demonstrations, showcasing its efficiency in handling complex tasks on real-world websites. Comparing with other methods, our approach demonstrated superior performance in terms of demonstration efficiency. In a study comparing DOMNET+WGE with DOMNET+BC+RL trained on varying numbers of demonstrations, it was found that increasing the number of demonstrations improved the performance of BC+RL but WGE trained with only 10 demonstrations still achieved much higher test rewards than BC+RL with 1000 demonstrations, showing over 100x sample efficiency improvement. Sparse rewards can hinder successful learning, even with moderate demonstrations. Policies are challenging to learn but offer flexibility. Various methods address sparse rewards without prior knowledge, such as exploration techniques and shaping rewards. Imitation learning leverages signals from experts to assist the agent in receiving additional rewards. Various methods leverage signals from experts to augment training data for agents. DAGGER and AGGREVATE query expert policies, while inverse reinforcement learning infers a reward function from expert demonstrations. Recent work combines demonstrations with reinforcement signals by pre-training agents with different objective functions and regularization. Our work uses demonstrations to guide exploration instead of direct training. Our work utilizes expert demonstrations to guide exploration by exploring trajectories in a neighborhood surrounding the demonstration. The neighborhood is defined by a workflow, allowing action sequences similar to the demonstrated actions. This approach differs from previous works that define neighborhoods based on action similarity rather than state similarity, making it particularly useful for web tasks. Hierarchical reinforcement learning (HRL) methods decompose complex tasks into simpler subtasks that are easier to learn. Main HRL frameworks include abstract actions, abstract partial policies, and abstract states. These frameworks require varying amounts of prior knowledge. Our work is closest to the line of work on constraints in robotics. Our approach utilizes expert demonstrations to guide exploration by exploring trajectories in a neighborhood surrounding the demonstration, defined by a workflow. Our workflow-guided framework automatically induces constraints from user demonstrations, leveraging targeted information and inductive bias for exploration. This approach combines demonstrations, abstractions, and neural policies to prevent overfitting. The workflow-guided framework induces constraints from user demonstrations to prevent overfitting. The constraint language is kept minimal and general, with object selectors for selecting objects based on properties or spatial alignment. Constraints are limited to 3 nested elementSet applications to avoid useless constraints. Literal values for tags, strings, and classes are extracted from demonstration states. In the demonstration state, tags, strings, and classes are extracted. Username \"ashlea\" and password \"k0UQp\" are entered and login is pressed. Email by Ilka is found and forwarded to Krista. Constraints are induced from user demonstrations to prevent overfitting."
}