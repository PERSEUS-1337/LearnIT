{
    "title": "Byef4anQcE",
    "content": "Providing transparency in AI planning systems is essential for practical applications. Utilizing causality within planning models and argumentation frameworks can help extract causalities and create explanations. Explainable AI Planning (XAIP) focuses on explaining AI systems to users, increasing trust, efficiency in human-AI collaboration, and better implementation in real-world settings. Past work includes explaining planner decision-making processes and forming explanations from models. Research in model-based explanations includes an iterative approach and using explanations for intuitive communication with the user. The performance of human-AI teaming improves with helpful explanations. Contrastive questions like 'Why F rather than G?' are used to understand user motivations, but they may not cover cases where users lack alternative ideas or have general questions about the model. Model reconciliation research aims to address incomplete or inaccurate user understanding of the model. Model reconciliation research aims to address incomplete or inaccurate user understanding of the model by providing explanations that include relevant state information, improving communication efficiency with the user, especially for long and complex plans. In planning, extracting information about action-state causality from complex models is essential. Causality is fundamental in determining plan complexity and heuristics, with planners creating causal graph visualizations for user interaction. The general structure of causality in planning is 'action causes state', with different causal chains identified as 'temporal' or 'unfolding' for action-state causality, and 'opportunity chain' for action-action causality. Argumentation is a suitable method to represent causality in models. Argumentation is a suitable method to represent causality in models, allowing for the distinction of multiple types of causality. This approach can be used to form more robust explanations in planning scenarios. In a logistics scenario, three trucks are tasked with delivering packages to different locations. The user has a general understanding of the model and goals, knowing the trucks can move between waypoints with connecting roads, refueling stations at specific waypoints, and subgoals for each package delivery. Trucks and packages start at waypoint A. Even with a simple problem like this, questions may arise. In a logistics scenario, three trucks are tasked with delivering packages to different locations. Questions may arise about the causal consequences of actions in achieving goals. State information tracking can aid in providing explanations for actions taken. The use of ASPIC+ for constructing explanations is suggested in this paper. The paper suggests using ASPIC+ for constructing explanations in a logistics scenario. It describes using formal argumentation systems to reason, focusing on constructing arguments using strict and defeasible rules from a knowledge base. The argumentation system ASPIC+ is used to construct explanations in logistics scenarios by utilizing strict and defeasible rules from a knowledge base. Causal links between actions in a plan are extracted and abstracted into a framework, independent of the planner, using an algorithm to form a knowledge base for argumentation engines. The argumentation system ASPIC+ utilizes causal links between actions in a plan to construct arguments. An algorithm is used to extract these causal relationships and form a knowledge base for the argumentation engine to address questions like 'Why A?' and 'How G?'. The process involves detecting action dependencies and constructing arguments with defeasible rules. The argumentation system ASPIC+ utilizes causal links between actions in a plan to construct arguments with defeasible rules. These arguments can be presented to a user seeking explanations, providing a higher-level abstraction of causality. A causal 'chunk' is defined as a subsection of the causal chain extracted from the plan, focusing on one 'topic' to show the causal structure. The argumentation system ASPIC+ utilizes causal links between actions in a plan to construct arguments with defeasible rules, providing a higher-level abstraction of causality. A causal 'chunk' represents a subsection of the causal chain, focusing on one 'topic' to show the causal structure. Arguments like A 11 show action-action causalities involving the object truck 1. Two methods of structuring explanations are proposed, including engaging the system in a dialogue to query causal relationships. The user can unpack causal chunks by asking why certain actions lead to others, showing the causalities enabling actions to be applied. This can be done using a forward approach to reach subgoals, and a reverse approach to explain how goals are reached. Another method involves extracting causalities within the plan's state space and representing them as a knowledge base. An algorithm iterates through action effects to extract altered state variables, allowing for answering questions about causal relationships. The dependencies between actions in a plan can be represented as statements in a logical language. These statements show how actions cause changes in state variables. For example, in a logistics scenario, it is clear that the truck's fuel level in the initial state limits its ability to go anywhere besides waypoint B. However, it is not immediately clear why the truck doesn't just stay put. These rules provide a partial explanation but further analysis is needed. The rules in a logical language explain how actions cause changes in state variables. In a logistics scenario, the truck's fuel level limits its movement. Combining causal and opportunity traces provides a strong basis for explanations. Defeasible rules show why truck 3 had to refuel but couldn't deliver packages. Future work includes seamlessly creating explanations from this structure. Future work involves extracting both defeasible and strict rules governing causal effects related to a specific topic. Determining relevant rules for user questions and combining them to form higher-level causal chunks is ongoing. One method is to extract rules related to a specific 'topic' like 't3 fuel', including actions that alter it and (sub)goals containing it. These form a causal chunk representing causes of changes to 't3 fuel' and its relationship to (sub)goals. Arguments in the causal chunk are unpacked iteratively, with the conclusion of A3 being a subgoal of the problem. The causal chunk analysis involves unpacking arguments related to 't3 fuel' to explain actions like driving truck 3 to waypoint B. The process continues until the subgoal 't3 fuel >5' is reached. Identifying relevant state variables based on user questions is challenging, requiring deduction from the plan, problem, and domain. Another approach is using a graph structure for explanations. The approach involves identifying relevant causal chunks in a graph to explain queries about actions. This system can show inapplicable actions and is crucial for future development. Further work is needed to define the types of questions that can be answered using this method and how user queries are captured. The curr_chunk discusses the need for further research on how user questions can be captured and converted for system understanding. It suggests using dialogue systems or Natural Language Processing techniques for communication. Additionally, it mentions the importance of expanding the set of questions that can be addressed and improving methods for creating causal 'chunks' for specific user queries. The use of argumentation schemes to identify relevant topics and causal chains from the framework is also highlighted. The curr_chunk proposes an initial approach to explainable planning using argumentation to extract causal chains and create causal 'chunks' for user explanations. Future research areas include exploring different ways of presenting explanations and assessing their effectiveness through user studies. The curr_chunk suggests using causal chains to create explanatory 'chunks' for hierarchical explanations, aiming to leverage argumentation for causal explanations."
}