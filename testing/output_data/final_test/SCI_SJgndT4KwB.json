{
    "title": "SJgndT4KwB",
    "content": "We prove the scaling of mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network at finite depth and width. The standard deviation is exponential in the ratio of network depth to width, showing that the NTK is not deterministic even with infinite overparameterization. For deep and wide networks, the NTK evolves during training, with the mean of its first SGD update also exponential in the depth-to-width ratio. This contrasts with shallow and wide networks, suggesting that deep and wide ReLU networks can learn data-dependent features even in lazy training regimes. In overparameterized neural networks, finding interpolating parameter values through stochastic gradient descent is unexpected. The individual parameters in these networks can be hard to interpret, and understanding training involves kernel gradient descent updates for the network function values. The current batch discusses the relationship between the empirical inner product, neural tangent kernel, and optimization of neural networks. It compares the parameter space view with the function space view and highlights the simplification of the Riemannian metric when network depth is fixed and width tends to infinity. This observation is supported by the universal approximation theorem. The universal approximation theorem states that the manifold M N fills out any ambient linear space of functions. Results show that the kernel K N remains frozen throughout training, converging to its average E[K N ] at initialization. This mapping between parameter space SGD and kernel gradient descent is characterized by the convergence of K N distribution to a delta function on its mean E[K N ] in the infinite width limit. In the infinitely wide regime, neural networks converge to a minimum loss with SGD, while kernel methods provide generalization guarantees. However, training with a fixed kernel may not capture feature learning, as networks with large but finite width can outperform predictions at infinite width. In the context of neural networks converging to a minimum loss with SGD in the infinitely wide regime, it is important to study finite width corrections to the Neural Tangent Kernel (NTK). The NTK evolves through training at finite width, capturing data-dependent features not seen in the infinite width limit. The specific NTK scaling of weights at initialization and the small learning rate limit can obscure aspects of SGD dynamics. Even in the infinite width limit, the exact dependence of NTK on network depth is not well understood, especially in deep networks. The joint statistical effects of depth and width on NTK in finite size networks remain unclear. The article aims to explore the combined effects of depth and width on the Neural Tangent Kernel (NTK) in finite size networks. It reveals that the NTK is not deterministic at initialization when the ratio of depth to width is not close to zero. Additionally, it shows that the standard deviation of NTK is at least as large as its mean when depth and width are both large, indicating that its distribution is not close to a delta function. The square loss of the SGD update \u2206K N (x, x) to K N (x, x) from a batch of size one containing x satisfies where n 0 is the input dimension. If d 2 /nn 0 > 0, the NTK can evolve in a data-dependent way. The evolution may have a well-defined expansion in d/n if n 0 is comparable to n and d/n > 0. The results hold at finite d, n, and the implicit constants are independent of d, n. The precise results hold for networks with variable layer widths. The parameter widths, acting as an inverse temperature, is explored in various articles following the original NTK work. ReLU networks with specific dimensions and linear output layers are considered, with biases initialized to zero. The network computes a given output based on input x, following specific probability measures. The NTK is a measure on R with a density symmetric around 0. Results extend to orthogonal weight initialization. The on-diagonal NTK is emphasized, with biases kept as trainable parameters. The Mean and Variance of NKT on Diagonal at Init are bounded by universal constants. In the deep and wide double scaling limit, the NTK does not converge to a constant. In the deep and wide double scaling limit, the NTK does not converge to a constant in probability. The square loss NTK is not frozen during training, with the Mean of Time Derivative of NTK on Diagonal at Init bounded by universal constants. The pre-factor in front of exp (5\u03b2) scales like 1/n when d is fixed and n i = n \u2192 \u221e. The article discusses the behavior of the update to K N (x, x) from a batch {x} at initialization in the context of deep and wide double scaling limits. It explores the impact of the growth of d, n, and n 0 on the update, as well as the relationship between the average update and K N (x). The article outlines the proofs of Theorems 1 and 2, providing an informal explanation of the strategy for computing moments of K N and its time derivative. Additionally, it introduces notation about paths and edges in the computation graph of N to be used in the proofs. The proofs of Theorems 1 and 2 are presented in Appendix Sections \u00a7B- \u00a7D. The computations in \u00a7B explain handling contributions to K N and \u2206K N from network weights. Sections \u00a7C and \u00a7D discuss adapting the method to biases and mixed bias-weight terms in K N , K 2 N, and \u2206K N. The arguments are simplified in these cases, omitting some details to focus on key differences. The proof of Theorem 1 involves estimating quantities related to the loss on a batch of data, while Theorem 2 requires controlling computations involving weights in the network. The main result focuses on pure weight contributions to K N, which are computed by summing over paths in the network. Proposition 3 discusses these pure weight moments and is proven in \u00a7B. The proof of Proposition 3 in \u00a7B contains key ideas for handling the remaining moments. Propositions 4 and 5 are derived from Proposition 3, showing pure bias moments and mixed bias-weight moments for K N, \u2206K N. Theorems 1 and 2 for general n i follow directly from Propositions 3-5. The asymptotics for n i n are obtained through routine algebra. The proof structure for all three Propositions is similar, with Proposition 3 being the most complex. The focus is on obtaining the first 2 moments of K w in a neural network. The biases are initialized to zero and do not affect K w. The output of the network can be expressed as a weighted sum over paths in the computational graph. The weight of a path includes the product of weights along the path and the condition that every neuron in the path is open at x. The path starts at an input neuron and passes through subsequent layers until reaching the output neuron. The derivative of wt(\u03b3) with respect to a weight We on an edge e of the computational graph of N involves indicator functions of neurons being open at x. The kernel Kw is a sum of derivatives squared, taking the form of a sum over collections of two paths and edges in the graph. The mean E[Kw] is computed under the assumption of mean zero weights, contributing only when every edge is traversed by an even number of paths. The formula for E[Kw] simplifies when there are exactly two identical paths in the computational graph of N. However, the expression for w is more complex, involving sums over four paths. The analysis of E[K2w] is complicated by the combinatorics of interactions between the paths and the requirement that certain edges belong to specific pairs of paths. The expectation is estimated in several steps, including obtaining an exact formula for the expectation. The expectation in (8) is calculated by obtaining a formula for F(\u0393, e1, e2) and observing its dependence on e1, e2. The formula is simplified by changing variables to find the Jacobian(E, e1, e2) which counts collections of four paths with the same edges. The Jacobian for paths passing through specific edges is calculated, with a focus on interactions between paths in the computational graph. Changing variables leads to a simplified expression for the Jacobian, highlighting the \"cost\" of interactions between paths. The 1/n terms in the sum result in an average over collections V of two independent paths in the computational graph of N. Proposition 10 provides estimates above and below the expectation, matching up multiplicative constants. Figures 1 and 2 depict paths between layers with and without interactions. Theorems 1 and 2 demonstrate the behavior in fully connected networks. Theorems 1 and 2 show that in fully connected ReLU nets that are deep and wide, the neural tangent kernel K N evolves during training and may learn data-dependent features in the overparameterized limit. The fluctuations of K N and its time derivative are exponential in the inverse temperature \u03b2 = d/n. The trajectory of K N during training may be data-dependent, suggesting different behaviors in the double descent curve based on network overparameterization. The results suggest that for fully connected ReLU nets, gradient-based training is stable when d/n is relatively small. There may exist a regime where the network is stable yet flexible enough to learn data-dependent features. This regime could exhibit weak lazy training, allowing the model to be described by a stochastic positive definite kernel. The study raises questions about the generalizability of these findings to non-linearities other than ReLU and different network architectures. In this section, notation is introduced for the proofs of Theorems 1 and 2, adapted from Hanin & Nica (2018). It discusses the computational graph of a ReLU network and defines a path in the graph from one neuron to another. The focus is on adapting techniques to more general settings beyond fully connected networks. The computational graph of a ReLU network is defined with paths from one neuron to another. Each edge in the graph is assigned a weight, and the weight of a path is calculated based on the openness of neurons along the path for a given input. This notation is useful for proving Theorems 1 and 2 in more general network settings. Theorems 1 and 2 are proven by associating an unordered multi-set of edges to every \u0393 \u2208 \u0393 k ( n). This is done by defining unordered multisets of edges and their endpoints. The computational graph of a ReLU network is used to calculate weights on edges, with a focus on paths between neurons. The weight of a path in a ReLU network is defined by the product of weights along the path and the condition that every neuron in the path is open at a certain point. Differentiating this formula gives expressions for derivatives with respect to x and trainable parameters. For the NTK and its first SGD update, weight contributions are calculated as a sum-over-paths. The weight of a path in a ReLU network is defined by the product of weights along the path and the condition that every neuron in the path is open at a certain point. Weight contributions are calculated as a sum-over-paths, with Lemma 6 proving the expressions for K 2 w and \u2206 ww are almost identical. The main difference lies in the restrictions on the paths, leading to a convergence to zero for E[\u2206 ww ]/E[K w ] when d < \u221e and n i \u2192 \u221e. Lemma 7 further discusses the expectation of K w, K 2 w, and \u2206 ww as sums over 2, 4 paths. Lemma 7 is proved in \u00a7B.2, showing the simplicity of evaluating expression (20) due to the delta function in H(\u0393, e). The proof involves evaluating (21) and (22) in parallel, where the expression F(\u0393, e1, e2) satisfies certain conditions. The advantage of F*(\u0393) is its independence from e1, e2. The sum over \u03934 even(n) in (21) and (22) is discussed, highlighting symmetry in the calculations. The text discusses evaluating expressions (21) and (22) in parallel, focusing on the symmetry in the calculations. It introduces the indicator function for paths passing through the same edge and simplifies the counts in I j, * ,i1,i2 and II j, * ,i1,i2. The event C(E, i 1 , i 2 ) is informally described. The text discusses the event C(E, i 1 , i 2 ) indicating collisions of paths in \u0393 before layers i 1 , i 2. It defines a loop at layer i if edges in E start at the same vertex in layer i-1 but different vertices in layer i. Lemma 8 evaluates counting terms in (30) and (31) for E \u2208 \u03a3 4 aj ,even. The text discusses the relationship between unordered multi-sets in E[K 2 w ] and E[\u2206 ww ]. It explains how every unordered multi-set four edge multiset E \u2208 \u03a3 4 even can be obtained from some V \u2208 \u0393 2. The map from \u0393 2 to \u03a3 4 even is surjective but not injective. The sizes of the fibers are computed in Lemma 9. The relation (38) shows that E[K 2 w ] and E[\u2206 ww ] are bounded above/below by a constant. The proof of Theorems 1 and 2 involves evaluating expectations to leading order in i 1/n i using Proposition 10. The estimates are bounded above and below by constants, leading to a conclusion for each layer in the product. The contribution for each layer in the product is bounded above and below by constants. The initial and terminal conditions are irrelevant. The probability of X i is 1/n i + O(1/n 2 i ). The lower bound in Proposition 3 is obtained by combining expressions (45) and (23). The upper bound is derived similarly. The proof involves evaluating expectations to leading order in i 1/n i using Proposition 10. The contribution for each layer in the product is bounded above and below by constants. The proof involves evaluating expectations to leading order in i 1/n i using Proposition 10. On the event A, F * (V) only depends on layers 1 \u2264 i \u2264 i 1 and layers < i \u2264 d. Lemma 11 states that with probability 1, there exists i such that y (i) exists. This event defines a co-dimension 1 set in the space of all weights. Lemma 11 shows that for our fixed x, the derivative of each \u03be (i) j vanishes with probability 1. This implies that for any edge e in the computational graph of N, the formulas for K N, K 2N hold. The result for \u2206K N is derived from the loss L on a single batch containing only x. By applying Lemma 11, it is shown that the last term has a mean of 0, completing the proof of Lemma 6. The last term in the proof has a mean of 0, derived from the assumption of symmetric weight distribution around 0. Lemma 7 is related to Theorem 3 in Hanin (2018) and Proposition 2 in Hanin & Nica (2018), with a difference in bias assumptions. The inner expectation is computed using sigma algebra and events defined based on post-activations. Supposing e is not in layer d, the expectation becomes We have Thus, the expectation in (48) becomes Note that given F d\u22122 , the pre-activations y. Recall that by assumption, the weight matrix. This replacement leaves the product. On the event S d\u22121 (which occurs whenever y = 0 with probability 1 since we assumed that the distribution of each weight has a density relative to Lebesgue measure. Hence, symmetrizing over \u00b1 W (d), we find that. Similarly, if e is in layer i, then we automatically find that \u03b3 1 (i \u2212 1) = \u03b3 2 (i \u2212 1) and \u03b3 1 (i) = \u03b3 2 (i), giving an expectation of 1/n i\u22121 1. Proceeding in this way yields which is precisely (20). The proofs of (21) and (22) are similar. As before let us first assume that edges e 1 , e 2 are not in layer d. Then, The inner expectation is 1 each weight appears an even number of times. Again symmetrizing with respect to \u00b1 W (d) and using that the pre-activation of different neurons are independent given the. The text discusses the decomposition of edges in a computational graph and counting the number of certain configurations of neurons in different layers. The key idea is to decompose edges into loops and analyze the relationships between layers within these loops. By proceeding layer by layer, the text aims to count the number of specific configurations satisfying certain conditions. The text discusses the decomposition of edges in a computational graph and counting the number of certain configurations of neurons in different layers. It analyzes the relationships between layers within loops of edges, determining how certain conditions affect the configurations. The initial conditions and specific edge properties determine the configurations within the loops. The text discusses the decomposition of edges in a computational graph and counting the number of certain configurations of neurons in different layers. It concludes the proof in the case j = 1, with differences in cases j = 2, 3, 4. The proof of Lemma 9 is similar to Lemma 8, with a focus on estimating the bias contribution K b to the neural tangent kernel. The text discusses the computation of pre-activations of neurons and the proof of Lemma 12 regarding paths in a computational graph. The proof is a modification of Lemma 6. Lemma 14 is also discussed, which is a simplified version of the computation of E[K^2w]. The text discusses the computation of pre-activations of neurons and the proof of Lemma 12 regarding paths in a computational graph. The proof is a modification of Lemma 6, and Lemma 14 is also discussed, which is a simplified version of the computation of E[K^2w]. The text then delves into the details of neuron connections and variable changes in the computation process."
}