{
    "title": "Bke89JBtvB",
    "content": "The method presented trains large neural networks with improved accuracy and lower computational cost by gating the deep-learning architecture at a fine-grained level. A new residual block architecture gates convolutional channels finely, and a tool called batch-shaping matches features in the network to a specified prior distribution. Results on CIFAR-10, ImageNet, and Cityscapes datasets show that this method can slim down large architectures conditionally, maintaining computational efficiency. The method presented gates deep-learning architecture at a fine-grained level to train large neural networks with improved accuracy and lower computational cost. Results on ImageNet show that ResNet50 and ResNet34 gated networks achieve higher accuracy compared to the baseline ResNet18 model, using more features for difficult examples and fewer for simple ones. This approach utilizes natural prior information to enhance neural networks. The use of natural prior information can enhance neural networks by employing conditional computing or gating architectures. By selectively activating parts of the architecture, large capacity networks can be trained with improved accuracy and lower computational cost, offering a better accuracy/computation cost trade-off compared to fully feed-forward models. The text discusses the benefits of using conditional computation with channel gating in neural networks to improve accuracy and reduce computation cost. The proposed fine-grained gating architecture allows for individual input and output convolutional maps to be turned on or off, leading to features that are individually gated for a better trade-off between accuracy and computation cost. In this paper, a generally applicable tool named batch-shaping is proposed to match the aggregated posterior of a feature in the network to a specified prior. This helps networks match activation distributions for better quantization or behavior similar to batch-normalization. The application of batch-shaping in helping the network learn conditional features improves performance by controlling gates to be more conditional on input data. State-of-the-art results are shown compared to other conditional computing architectures. The literature discusses adaptively-dropped neural networks with gating connections for deep neural networks. Various methods for reducing model complexity, such as tensor factorization and channel pruning, are also explored. These techniques aim to create sparse network architectures and improve efficiency. Several methods, including VIBnets, Bayesian Compression, and L0-regularization, remove full channels during training to reduce model capacity while maintaining accuracy. Our method allows for higher model capacity with similar inference times. Some networks like Branchynet and Multi-scale dense net have early exiting nodes for complex examples. Our approach assigns less computation to simpler examples but does not have early-exiting paths. Adaptive spatial attention for faster inference has also been explored in the literature. Works focus on adaptive spatial attention for faster inference, proposing models that dynamically adjust the number of executed layers for different image regions. Various methods, such as difficulty-aware region convolutions and tiling-based sparse convolution algorithms, aim to speed up processing time. Other approaches, like ConvNet-AIG and SkipNet, conditionally activate full residual blocks based on input. Dynamic Channel Pruning turns individual features on or off, similar to the discussed approach, but selects the top-k features instead. Gaternet introduces batch-shaping to match the aggregated posterior distribution in neural networks to a specified prior distribution. This method improves gate conditionality without unnecessary overhead, offering potential value for various applications like training auto-encoders or network quantization. The goal is to distribute parameterized features in neural networks according to a chosen probability density function. The Cram\u00e9r-von-Mises criterion is used to shape the distribution of X(\u03b8) according to a chosen PDF in neural networks. This criterion allows for differentiation with respect to \u03b8 and is more suitable than other statistical distance functions like KL-divergence. Gradients can be derived with respect to each sample x using this criterion. The Cram\u00e9r-von-Mises criterion shapes the distribution of X(\u03b8) in neural networks using a chosen PDF. Gradients can be derived for each sample x with this criterion, optimizing the batch-shaping loss function with a parameter \u03bb. Differentiation with respect to \u03b8 is possible through the sorting operator, making the whole loss term differentiable. The Batch-Shaping loss function shapes the distribution of features in a neural network to match any chosen PDF. It can be used to encourage activations to be Gaussian or follow a uniform distribution for purposes like fixed-point quantization. The gating network introduced in the next section uses a ResNet structure to condition gates. The overview of a channel gated ResNet block includes a residual function defined as F = W2 * r(W1 * x), with a gating module G that determines whether to skip or compute the convolution operation based on the output of the gating function. This design aims to save computation by sparsely activating the gates. The gating function in the channel gated ResNet block is positioned after the ReLU activation to prevent unnecessary updates. It only gates the representation between layers in the residual block, improving training stability. By squeezing global spatial information into a channel descriptor, the proposed gating setup outperformed other configurations. The gating module in the channel gated ResNet block utilizes channel-wise global average pooling and a simple feed-forward design with two fully connected layers. It dynamically selects relevant filters for the input by mapping the output to a binary vector. Training binary-valued gates is challenging due to the non-differentiable nature of the gate. The module is computationally inexpensive, with an additional overhead of 0.018% - 0.087% of a ResNet block MAC usage. In this paper, Gumbel-Softmax sampling is used to address the issue of back-propagating through a non-differentiable gate. The Binary concrete relaxation BinConcrete(\u03c0, \u03c4) is considered, with a sigmoid function used for the backward pass. A batch-shaping loss is applied to regulate features to be sometimes on and sometimes off, with a Beta distribution prior on each gate. The Beta distribution with parameters a = 0.6 and b = 0.4 is used as a prior on gates to induce sparsity. A strong coefficient \u03bb is applied at the start of training to encourage conditional activations, gradually annealing as training progresses. The batch-shaping loss aims to find a balance between conditional sparsity and task loss. For sparsification during training, a modified version of the L0-loss without stretching is used, with parameters controlling the level of sparsification. Introducing this loss too early can reduce network capacity and potentially harm performance if not carefully chosen. In our experiments, we introduce an L0-loss after a delay to prevent premature turning off of convolutional maps during training. We evaluate our method on CIFAR-10, ImageNet, and Cityscapes benchmarks using various ResNet architectures. Comparisons with other conditional computation methods are made, and we analyze the learning patterns of gates in semantic segmentation tasks. The pyramid scene parsing network (PSPNet) with ResNet-50 backbone was used for semantic segmentation. Training details and hyperparameters for gated networks on CIFAR10, ImageNet, and Cityscapes are provided. Batch-shaping loss with beta distribution prior was applied with a coefficient of \u03bb = 0.75, linearly annealed to zero until epoch 100. L0-loss was applied starting from epoch 100 with increasing coefficient until epoch 300. Different trade-off points were generated using \u03b3 values. Experimentation was also done with only batch-shaping loss (no L0-loss) with fixed \u03bb = 0.75. Our batch-shaped channel gated ResNet20 and ResNet32 models, ResNet20-BAS and ResNet32-BAS, outperform other adaptive computation methods ConvNet-AIG, SkipNet, and SGAD. Results show that ResNet32 can achieve better performance with lower average computation than ResNet20. Gated ResNet32 model outperforms a gated ResNet20 model, as shown in the results. Competing methods show equal or lower performance than their smaller sized counterparts. The ResNet20-BAS and ResNet32-BAS models outperformed other methods in terms of accuracy. Experimenting with different batch-shaping distributions showed a significant performance degradation. The L0-loss provided a better balance between accuracy and computation costs. When applied to ImageNet, the batch-shaping loss with linear annealing improved model performance. The study experimented with different \u03b3 values to create trade-off points in performance. Performance differences were more pronounced in ImageNet compared to CIFAR-10 due to the dataset's complexity. Lower compression rates showed increased performance, similar to findings in compression literature. The trade-off between computation cost and accuracy was illustrated for the gated network ConvNet-AIG and ConvNet-FBS. Our ResNet-BAS models outperform ConvNet-AIG and ConvNet-FBS models in terms of computation cost and accuracy. The ResNet50-BAS and ResNet34-BAS gated networks achieved higher top-1 accuracy compared to ResNet18 models. The batch-shaped models showed the highest accuracy when using batch-shaping loss with a fixed \u03bb. Our gated PSPNet model achieved an IoU of 0.747 and pixel accuracy of 0.948 using 95% of the PSPNet MAC count (\u03bb = 0.05), outperforming the original PSP network. The study compared the performance of ResNet20 models on CIFAR10 with different width factors, showing higher accuracy with increased parameters. Inference time was compared to Convnet-AIG on CPU and GPU, with similar accuracy at different batch sizes. The study compared ResNet20 models on CIFAR10 with different width factors, showing higher accuracy with increased parameters. Inference time was compared to ConvNet-AIG on CPU and GPU, with similar accuracy at different batch sizes. Batch-size CPU: 1, GPU: 128. The effectiveness of the proposed batch-shaping loss was validated by comparing ResNet-BAS networks using both the batch-shaping loss and L0 complexity loss versus only using the L0 complexity loss. Models using the batch-shaping loss consistently outperformed those using only the L0 complexity loss, with ResNet20-L0 and ResNet32-L0 showing more rapid accuracy degradation. However, L0-gated models still outperformed ConvNet-AIG and SkipNet architectures due to the specific channel gated network architecture allowing for dynamic selection of channels or filters. The network architecture allows for dynamic selection of channels or filters in a layer, compared to models that skip whole layers. Gating larger non-BAS models did not improve over smaller models at similar computation cost. The distribution of learned gates provides insight into network features. Gates are categorized as always on/off based on input frequency. The histogram in Figure 6 illustrates the execution frequency of individual filters in each layer of a ResNet block. Filters are sorted by frequency over the validation set in the ResNet34-BAS model. The network has gates that are always on and others that fire conditionally based on input, allowing for dynamic selection of specialized filters. The ResNet34-BAS model has filters that activate conditionally, contributing to saving computation and forming conditional expert sub-networks. In contrast, the ResNet34-L0 model achieves sparsity by fully turning off a large number of gates. The execution patterns of filters in gated networks show small differences between similar classes and large differences between distinct classes. The paper presents a fine-grained gating architecture for conditional computation in deep networks, achieving state-of-the-art accuracy on CIFAR10 and ImageNet datasets. The gating method reduces inference computation while maintaining high accuracy, outperforming lower capacity base networks. On ImageNet, ResNet50-BAS and ResNet34-BAS models improve accuracy by more than 4.8% and 2.8% over a ResNet18 model at the same computation cost. Additionally, a novel batch-shaping loss is proposed to match the marginal distributions. The paper introduces a fine-grained gating architecture for conditional computation in deep networks, achieving high accuracy on CIFAR10 and ImageNet datasets. A novel batch-shaping loss is proposed to match marginal distributions and improve performance significantly. Future research directions include applications in autoencoders, quantized models, and continual learning. The gating setup allows for distilling smaller sub-networks and mitigating catastrophic forgetting. The paper introduces a fine-grained gating architecture for conditional computation in deep networks, achieving high accuracy on CIFAR10 and ImageNet datasets. A novel batch-shaping loss is proposed to match marginal distributions and improve performance significantly. The training process involved using Nesterov's accelerated gradient descent with specific parameters and data augmentation techniques. The model was trained on Cityscapes dataset for semantic urban scene understanding with 19 semantic classes. Data augmentation included random mirror, resize, and crop-size of 448 \u00d7 672. The PSPNet network was used for training and testing with single-scale input and whole image inference. The initial learning rate was divided by 10 at specific epochs. The PSPNet network with ResNet-50 back-end was trained from scratch with a mini-batch size of 6 for 150k iterations. Momentum and weight decay were set to 0.9 and 1e \u22124 respectively. The learning rate policy involved multiplying the initial learning rate by (1 \u2212 itercurrent /itermax) 0.9. Gated PSPNet was trained with weight decay set to 1e \u22126 for the layers in the gating units. Gates' output is computed before convolutional operations inside a resnet block and fed to preceding and following convolutional layer kernels. In practical settings like real-time inference, convolutional weights can be loaded conditionally from DDR memory to local compute memory for sliced tensor computation. In Pytorch, computation can be done on sliced tensors for batch computing. Custom convolutional kernels can use calculated masks on the fly. For each ResNet block, gating patterns are used to generate masks for slicing weight tensors and applying conv2d layers. GPU measurements involve recording gating patterns of images in the validation set. The validation set images' gating patterns were recorded for sparse model computation time analysis. Models using batch-shaping loss outperform those with only L0 complexity loss, but L0-gated models still outperform ConvNet-AIG."
}