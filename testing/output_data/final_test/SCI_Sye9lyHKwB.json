{
    "title": "Sye9lyHKwB",
    "content": "Uncertainty estimation is crucial for evaluating the robustness of deep learning models in computer vision, especially in risk-sensitive areas. Most current models struggle to provide uncertainty estimation without significant modifications. A new approach is introduced to enable training-free uncertainty estimation for deep learning models. Three simple and scalable methods, infer-transformation, infer-noise, and infer-dropout, are proposed for analyzing output variance of a trained network under tolerable perturbations during inference. These methods do not require re-training or fine-tuning the model, yet produce comparable or better uncertainty estimation compared to other methods. The uncertainty from these methods can also be used to improve neural network training. Deep learning has shown remarkable performance in various tasks, with a shift towards prioritizing reliability and robustness in risk-sensitive areas like autonomous driving and healthcare. Novel approaches incorporating uncertainty estimation during training and inference have emerged, including probabilistic formulations for neural networks and leveraging randomness in training methods like dropout and ensembling. Utilizing randomness in training and inference to estimate uncertainty is crucial in deep learning. However, implementing uncertainty estimation in pre-trained models can be challenging, especially for deterministic models without stochasticity. One solution is to add dropout layers and fine-tune the model, but this may not be practical for state-of-the-art models trained on large datasets like ImageNet. Utilizing randomness in training and inference to estimate uncertainty is crucial in deep learning. However, implementing uncertainty estimation in pre-trained models can be challenging, especially for deterministic models without stochasticity. To address this, a new approach is proposed for training-free uncertainty estimation, focusing on black-box and gray-box scenarios. Simple and scalable methods are introduced to analyze output variance without the need for re-designing or re-training the model. The main idea is to add tolerable perturbations into inputs or feature maps during inference to analyze output variance. Three methods are proposed: infer-transformation, infer-noise, and infer-dropout, each aiming to generate diverse outputs for uncertainty estimation without re-training the model. Infer-dropout is a method for performing inference-time dropout in chosen layers, even without dropout during training. It is comparable to MC-dropout for uncertainty estimation in classification tasks, utilizing entropy for training-free uncertainty estimation. The training-free method is evaluated in classification tasks and shows satisfactory uncertainty estimation. The focus is on regression tasks where output distributions are not available. The proposed methods use infer-transformation or infer-noise injection to estimate uncertainty effectively and efficiently. The methods are able to generate uncertainty estimates without the need for training. Probabilistic neural networks consider input and model parameters as random variables, improving uncertainty estimation in large-scale regression tasks compared to baseline methods like MC dropout and deep ensemble. The proposed methods can enhance neural network training by effectively estimating uncertainty without the need for training. One major limitation in neural network training is the computational expense of propagating distributions, leading to methods like dropout and batch normalization as approximations to Bayesian neural networks. MC dropout injects noise during training and inference to form a distribution on the output, while sampling-free probabilistic neural networks offer a lightweight Bayesian treatment. Training-free uncertainty estimation involves applying infertransformation and infer-noise. Training-free uncertainty estimation involves applying infertransformation T and infer-noise or infer-dropout P to a trained neural network F during inference for N times. Examples of uncertainty maps generated using this method are shown in tasks like single image super resolution and monocular depth estimation. Other scalable strategies for uncertainty estimation include deep ensemble methods, which train multiple networks from scratch with induced randomness, and training the network to have both original outputs and uncertainty predictions. The approach in this paper focuses on imposing perturbations on a model's input or feature maps to address uncertainty in neural networks. It distinguishes between black-box and gray-box cases, offering methods for each. The design aims to avoid re-training and heavy implementation overhead while optimizing for both original outputs and uncertainty predictions. The paper proposes methods for uncertainty estimation in neural networks by applying tolerable perturbations to input or intermediate-layer neurons. Different approaches are used for black-box and grey-box cases, including infer-transformation, infer-noise, and infer-dropout. These methods manipulate input transformations or internal representations to generate multiple outputs and estimate uncertainty. The method proposed involves introducing tolerable-perturbation transformations to the input, exploiting the model's dependence on input transformations. This is achieved through rotations and flips applied to the input, canceling out inverse operations to estimate variance. In the experiment, rotations and flips are used to generate multiple outputs without interpolation noise. Applying transformations during training may reduce variance in model outputs. The study evaluates models in super resolution and depth estimation without augmentation, finding comparable error rates to baseline methods like MC dropout. Another method considered involves manipulating feature maps to generate multiple representations. Infer-noise and infer-dropout are methods for generating diverse outputs by introducing noise or dropout at intermediate representations in a trained model. This allows for modulating perturbation strength and ensuring tolerability, with infer-noise distributing Gaussian noise evenly at feature maps and infer-dropout dropping features at redundant locations for more tolerable perturbations. Our method involves using dropout during inference to generate output samples for uncertainty estimation. By perturbing network layers before and after dropout, we can create an output distribution centered around the network's prediction. The variance of the output samples can be computed over multiple inferences, with proper location tuning improving uncertainty performance. This approach is different from MC-dropout as it focuses on the impact of perturbations on network layers. In this section, three proposed approaches for uncertainty estimation in large scale regression tasks are evaluated. The focus is on comparing these methods with MC dropout and deep ensemble in tasks like superresolution and depth estimation. The Single Image Super Resolution task aims to restore high-resolution images from low-resolution inputs, with a focus on analyzing the SRGAN model. SRGAN produces deterministic restorations, unlike methods that incorporate uncertainty estimation through dropout layers. The text discusses uncertainty estimation in image restoration tasks using the SRGAN model. The model produces deterministic restorations, and the proposed methods aim to evaluate its uncertainty. Experiments are conducted on two models trained with different loss functions, and various transformations are applied to the input images for evaluation. The text discusses uncertainty estimation in image restoration tasks using the SRGAN model. Experiments involve applying transformations to the trained model during inference, such as adding Gaussian noise and dropout layers at different locations with specific parameters. Multiple samples are generated to calculate pixel-wise variance for evaluation. The study explores uncertainty estimation in image restoration using the SRGAN model. Experiments involve adding dropout layers with varied rates at specific locations during inference. Different sample numbers are evaluated, and two baselines, MC-dropout and deep ensemble, are tested for comparison. The study compares two baselines, MC-dropout and deep ensemble, for uncertainty estimation in image restoration using the SRGAN model. The deep ensemble model is trained multiple times with added randomness in initial weights. A state-of-the-art model, FCRN, is used for depth estimation with a Huber loss. The model used for uncertainty estimation in image restoration is based on the SRGAN model. It includes a dropout layer for MC dropout during training and inference. The model is evaluated on the NYU Depth Dataset V2, with adjustments made to avoid applying 90-degree rotation to input for infer-transformation. The model for uncertainty estimation in image restoration, based on the SRGAN model, includes dropout layers for MC dropout during training and inference. Horizontal flip is applied instead of 90-degree rotation to input for uncertainty estimation. Two samples are generated for uncertainty estimation, with experiments conducted using infer-dropout and baseline MC dropout. Evaluation metrics include pixel-wise correlation, mean correlation, and block-wise correlation. Sample numbers of 2 or 8 are evaluated for both dropout cases. The evaluation metrics for uncertainty estimation in image restoration include pixel-wise correlation, mean correlation, and block-wise correlation. Block-wise correlation is a new metric proposed in the work, where local segmentation is used to generate clusters with similar low-level context to visualize uncertainty in each sub-region. The evaluation metrics for uncertainty estimation in image restoration include pixel-wise correlation, mean correlation, and block-wise correlation. Tolerable perturbations play a crucial role in obtaining effective uncertainty estimation, with better tolerability leading to better uncertainty estimation. High variance in our methods occurs in areas with high randomness and high-frequency spectral contents for the SR task, while for depth estimation, high variance usually occurs in areas with high spatial resolution and large depth. The correlation for different amounts of perturbations in different locations is shown in Figure 4. The optimal cases for generating uncertainty maps with high correlation require low loss after perturbation (high tolerability). Different methods achieve high tolerability differently: MC dropout involves evaluating various dropout rates and noise levels. Adding dropout layers during training increases model robustness against perturbations, with intermediate layers being optimal locations for dropout layers in uncertainty estimation. Adding dropout layers in intermediate locations during inference can help keep the loss relatively small and alleviate disturbance. Perturbation in intermediate layers is optimal for uncertainty estimation with MC-dropout and infer-dropout, different from conventional methods. Infer-noise with small standard deviation limits perturbation level effectively. Further tuning of locations and noise levels is required for infer-noise. Our methods infer-transformation and infer-dropout provide comparable results with MC-dropout for super-resolution tasks. SRGAN outperforms SRresnet due to sensitivity to perturbation. In depth estimation, infer-dropout in intermediate layers is superior. Tuning of locations and noise levels is necessary for infer-noise. Our methods infer-transformation and infer-dropout provide comparable results with MC-dropout for super-resolution tasks. MC dropout only perturbs before the last convolutional layer, leading to a localized variance map with poor correlation. By allowing MC-dropout in intermediate layers and re-training the model, a correlation value comparable to infer-dropout can be achieved. Even with only 2 samples, infer-transformation can outperform all other methods. Uncertainty maps estimated by our methods can be used to improve the training process, such as using pixel-wise uncertainty as a weight term for regression loss to penalize highly uncertain regions. Our methods infer-transformation and infer-dropout provide comparable results with MC-dropout for super-resolution tasks. Uncertainty maps estimated by our methods can be used to improve the training process by penalizing highly uncertain regions in regression loss. The model is fine-tuned using active learning with the 25% validation data showing improved performance. Three simple and effective methods for uncertainty estimation are proposed, achieving comparable or better results than state-of-the-art methods. Adding tolerable perturbations is demonstrated to be beneficial. Our future work involves evaluating methods on different model types and noise levels, as well as generalizing for more complex noise types. For classification tasks, entropy calculation is a common method for uncertainty estimation, compared to MC-dropout sampling. Experiments include classification on CIFAR100 with Resnet and binary segmentation on a biomedical dataset with UNET. Correlation analysis is conducted between uncertainty and error maps. Using entropy outperforms MC-dropout in correlation metric on biomedical dataset from SNEMI3D challenge. Models evaluated are UNET for segmentation on SNEMI3D dataset and Resnet for classification on CIFAR100 dataset. SRGAN model generates more photo-realistic outputs but is worse than SRresnet in standard metrics like SSIM and PSNR. SRresnet trained for 100 epochs, SRGAN trained for 2000 epochs using SRresnet as pre-trained model on DIV2K train dataset. For depth estimation task, the model avoids 90 degree rotation on input to maintain spatial direction importance. Only horizontal flips are applied to inputs, generating 2 samples with satisfying correlation value, better than MC dropout. Horizontal flips simulate depth inference from human perspective. In the task of depth inference, horizontal flips are compared to simulate human vision or stereo camera views. A dropout layer is added at different locations with varied rates, and sampling numbers are tested. The baseline method is MC-dropout with a dropout layer before the final FC layer. In depth inference, sampling is performed at the final FC layer during inference with varying dropout rates. Different sampling numbers are tested, and error metrics such as pixel-wise L1 and L2 loss are defined. Uncertainty estimation and new evaluation metrics like block-wise correlation are also introduced. The block-wise correlation is a new metric proposed in this work. It involves applying the output from a trained model with a local segmentation algorithm to generate clusters with similar low-level context. The variance of pixels inside each cluster is averaged and used to calculate block-wise loss. The pixel-wise correlation is then calculated using the updated values. Variance weighted loss is applied by using the dot multiplication of variance with L2 loss for each pixel. The SRGAN model is finetuned by adjusting the loss term and keeping the adversarial loss term constant. It is trained on the DIVI2K dataset and evaluated on the DIVI2K test set. 25% of test images with high uncertainty are selected for fast fine-tuning. The model is re-trained for additional epochs and evaluated on the Set14 dataset. Correlation results using dropout rates for different models are consistent with SRGAN results. The uncertainty maps generated with dropout layers in different locations are consistent with the results of the SRGAN model. Uncertainty and error maps for super resolution and depth estimation tasks can be visualized in Figures 8, 9, and 10."
}