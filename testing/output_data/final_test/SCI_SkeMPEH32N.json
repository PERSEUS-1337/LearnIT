{
    "title": "SkeMPEH32N",
    "content": "In supervised learning, wider networks have shown to improve test error in the over-parametrized setting. This phenomenon is being investigated in reinforcement learning as well, with empirical results supporting the hypothesis. However, tuning hyperparameters for different network widths is crucial for optimal performance in environments where hyperparameters vary significantly. Increasing the width of a neural network can lead to decreased test error, as wider networks have been shown to generalize better in various architectures and hyperparameter settings. This phenomenon challenges the traditional bias-variance tradeoff in supervised learning. In an over-parametrized setting, wider networks in reinforcement learning show improved performance beyond default widths. This challenges the traditional bias-variance tradeoff in supervised learning. In reinforcement learning, wider networks in an over-parametrized setting lead to improved performance, challenging the traditional bias-variance tradeoff in supervised learning. The goal is to minimize expected risk by learning a hypothesis from a training set using a learning algorithm. The generalization performance of a learner is viewed through the lens of a tradeoff between approximation error and estimation error. The decomposition of class H, R(h S ) into approximation error and estimation error is presented as a tradeoff by Shalev-Shwartz & Ben-David. It involves the Bayes error and the best hypothesis in H. Larger H is associated with smaller approximation error and larger estimation error. The universal approximation property of neural networks highlights this tradeoff. The universal approximation property of neural networks highlights the tradeoff between approximation error and estimation error. This tradeoff is similar to the bias-variance tradeoff in model complexity. In designing a good learning algorithm, it is important to optimize this tradeoff. In reinforcement learning, training examples are not independent or identically distributed due to the policy changes over time. This makes it nonobvious if supervised learning phenomena extend to reinforcement learning. Experiments were conducted with various environments and learning algorithms, varying the width of the shared policy and value network. The environments used were CartPole, Acrobot, MountainCar, and Pendulum, with learning algorithms including PPO, A2C, ACER, and ACKTR. We used ACKTR algorithm up to width 512 due to its second-order nature. Hyperparameters were tuned on networks of width 64 from RL Baselines Zoo. Training typically lasted around 1 million time steps. Simple tasks were chosen for faster training and to prevent overfitting. In reinforcement learning experiments, simple tasks like CartPole were chosen to prevent overfitting. The experiments showed that wider networks up to width 2048 did not see degraded performance, with peak performance reached at width 64 for PPO and A2C, and at width 128 for ACER. The policy and value networks shared the same architecture with 2 hidden fully connected layers. In Acrobot, peak performance is observed as early as width 16 for PPO, ACER, and A2C, indicating the simplicity of the task. Peak performance is maintained through width 2048 for all three learners. In Pendulum, wider networks outperform the default width (64), with no degradation in performance up to width 2048. However, in MountainCar, PPO's performance starts to degrade at width 2048, suggesting potential evidence against the hypothesis. RL algorithms show performance degradation at different network widths, with ACER degrading at width 512, A2C at width 1024 to 2048. Hyperparameter settings, especially learning rate, play a crucial role in performance. Scaling network width in supervised learning may not optimize learning rate for each network. Fair comparison across widths requires optimal hyperparameters for each. In reinforcement learning, scaling the learning rate with different schemes does not improve performance for larger networks. This phenomenon contrasts with supervised learning, where increasing network width typically reduces test error. Optimal hyperparameters are crucial for fair comparisons across different network widths. In reinforcement learning, scaling the learning rate with different schemes does not improve performance for larger networks. This contrasts with supervised learning, where increasing network width typically reduces test error. Optimal hyperparameters are crucial for fair comparisons across different network widths. The phenomenon of monotonically lower test error without a U curve extends to reinforcement learning, as seen in preliminary experiments with CartPole, Acrobot, and Pendulum. However, performance consistently degrades in the MountainCar experiments due to suboptimal hyperparameters chosen for width 64. Future experiments will use an automated tuning procedure to choose hyperparameters for each width individually, aiming to replicate results similar to CartPole and Acrobot. Future experiments will aim to replicate results similar to CartPole and Acrobot across more learning algorithms and environments, including Pendulum."
}