{
    "title": "BketCg9p6X",
    "content": "Knowledge bases are essential for modern applications but lack data compared to the vast information available on the web. The bottleneck in knowledge base creation is the reliance on domain experts to extract data from web sources. Automated systems have low precision and recall, hindering large-scale extractions. MIDAS is a system that automates suggesting high-quality web sources to improve knowledge creation processes. MIDAS automates suggesting high-quality web sources and extracting valuable web source slices to enhance knowledge bases. It introduces a novel concept of web source slices, defines a profit function to quantify their value, and develops scalable algorithms to derive high-profit slices. MIDAS outperforms baselines on real-world and synthetic datasets, improving knowledge base coverage and correctness for applications like search engines. There is a gap between facts on the Web and in knowledge bases, with most knowledge bases being incomplete. For example, Freebase does not provide sufficient facts for cocktails like Margarita. Midas uses automatically-extracted facts to identify the right web sources for a semi-automated knowledge extraction process, resolving a major bottleneck in the industry standard. Automated knowledge extraction process augments knowledge base with new facts from the Web using domain experts, crowdsourcing, and wrapper induction. Achieves high precision and recall but limited scalability due to manual source selection. Midas 1 is a proposed system for automated knowledge extraction that aims to improve accuracy in extracting facts from a wide variety of web sources. The current industry standard automated extraction process suffers from low accuracy, especially for uncommon facts, due to limited training data. This limitation results in a significant amount of web information being unexploited. Midas 1 is a system that uses automated extraction to identify valuable web sources and improve industry standard bottleneck. The system extracts high-confidence facts from web pages, aiming to overcome the limitations of current extraction processes. Automated extraction systems struggle with precision and recall due to lack of training data, but correct facts provide valuable insights. Entities are described by subjects, predicates, and objects. Entities can be grouped based on common properties. New facts about rocket families sponsored by NASA are absent from Freebase, suggesting the potential to enhance the database. Insight: Freebase can be augmented by extracting facts about \"rocket families sponsored by NASA\" from web sources. Web source slices contain groups of entities with common properties, making annotation easier. The objective is to discover new facts from web sources to enhance the database. The challenge lies in evaluating web source slices for suitability in augmenting knowledge bases. The exponential growth of slices with facts poses scalability issues due to the vast number of web sources. Extracted facts from a real-world pipeline may contain new information absent from Freebase. Midas addresses challenges in producing web source slices efficiently and measuring their utility through a profit function. The algorithm generates high-profit slices from single and multiple web sources, evaluated on real-world and synthetic datasets to show efficiency and scalability. Midas efficiently identifies and customizes valuable web sources for knowledge base augmentation. It can derive high-profit slices from web sources, providing new information for existing knowledge bases. The web sources often contain semi-structured data, making them easy for annotation. The algorithm formalizes the problem of slice discovery for knowledge base augmentation using web source slices. The URL hierarchies of web sources offer access to different granularities, dividing contents into smaller subsets to reduce extraction effort. Web domains like https://www.cdc.gov classify their contents using URL hierarchies, making extraction challenging due to varied and spread-out content. Extracted facts from web sources correspond to various entities but can share common properties. The entities extracted from web sources can share common properties, such as the rocket families \"Atlas\" and \"Castor-4\" sponsored by NASA. These common properties are abstracted and formalized as web source slices, allowing for efficient retrieval of relevant facts. Fact tables organize these facts, with each row containing facts corresponding to the same entity. The fact table F W organizes facts extracted from web source W, with each row corresponding to the same entity. It has a primary key (subject) and attributes for distinct predicates. Properties and web source slices are derived from F W, with properties defined as pairs (pred, v) where pred is an attribute in F W and v is a value. A web source slice, denoted by S(W) or S for short, is derived from a fact table F W and a set of facts T W extracted from web source W. It consists of properties where the value is strictly derived from the domain of pred. Midas does not consider properties on the subject attribute as subjects are typically identification numbers in real-world datasets. A web source slice, denoted by S(W) or S for short, is a triplet consisting of a collection of properties C, a set of entities \u03a0, and a set of facts \u03a0*. Different slices may correspond to the same set of entities but have different semantic interpretations. For example, a slice on properties {c5, c6} corresponds to entity e5, representing projects sponsored by NASA and started in 1957. In Midas, canonical slices are chosen based on the maximum number of properties among slices corresponding to the same set of entities and facts. This approach reduces redundancy and complexity in reporting. Canonical slices are unique and can be used to infer unreported slices by validating entities with subsets of properties. All six slices in Figure 4 are canonical slices selecting at least one fact. The web source suggestion problem aims to maximize the profit of a set of slices by extracting unique new facts from web sources to augment an existing knowledge base. The value of a set of slices is quantified as the gain minus the cost, with gain measured by the number of new facts and cost estimated based on common knowledge-base augmentation procedures. The web source suggestion problem focuses on maximizing profit by extracting new facts from web sources to enhance a knowledge base. The gain and cost of slices are calculated based on the number of new facts and common augmentation procedures. The gain is determined by the difference between the number of new facts and the cost, which includes crawling, de-duplication, and validation costs. Midas uses a profit function to identify web source slices for augmenting knowledge bases. The slice discovery problem aims to maximize total profit, proven to be NP-complete and APX-complete due to its non-linear objective function. The optimal slice discovery problem is NP-complete and APX-complete. An algorithm called Midas alg identifies good slices in a single web source. Midas extends this algorithm to detect good slices from multiple web sources. The number of slices in a single web source can be exponential in the number of extracted facts. Our approach, Midas alg, efficiently constructs slice hierarchies by avoiding irrelevant property combinations and guaranteeing result quality through trimmed hierarchy traversal. The algorithm only builds slices as needed in a bottom-up fashion, pruning them intelligently during construction. This hierarchy is determined by the properties of slices, where a subset of properties in one slice corresponds to a superset of entities compared to another slice. Midas alg generates slices at different levels of granularity based on entity properties. Each entity is associated with facts from a fact table, and slices are created for each combination of properties. The algorithm assigns a level to each slice based on the number of properties defining it. Midas alg constructs and prunes a slice hierarchy in a bottom-up manner based on entity properties. Initial slices are created at maximal granularity levels, with each slice assigned a level corresponding to the number of defining properties. The algorithm evaluates each slice's profitability and canonical status, pruning those that do not meet criteria. Midas algorithm constructs a slice hierarchy by generating parent slices and pruning non-canonical slices. Parent slices are created by removing one property at a time, and only canonical slices with the maximum number of properties are reported. The Midas algorithm efficiently identifies canonical slices by determining if a slice is initial or has at least two canonical children. It works at two levels of the hierarchy at a time to record children slices correctly after pruning. The Midas algorithm works efficiently by constructing parent slices before pruning them. It updates the children list of the parent slice when removing a non-canonical slice. Pruning low-profit slices reduces the number of slices to examine. The Midas algorithm efficiently constructs parent slices before pruning them based on a heuristic that marks a slice as low-profit if its profit is negative or lower than the total profit achievable from its subtree. This reduces the number of slices to examine. The Midas algorithm efficiently constructs parent slices before pruning them based on a heuristic that marks a slice as low-profit if its profit is negative or lower than the total profit achievable from its subtree. This is related to agglomerative clustering, but Midas alg is much more efficient in reducing the number of slices at each level. The hierarchy construction effectively prunes a large portion of slices in advance, significantly reducing the number of slices to consider. However, redundancies or heavily overlapped slices may still be present in the trimmed slice hierarchy. The Midas algorithm efficiently constructs parent slices before pruning them based on a heuristic that marks a slice as low-profit if its profit is negative or lower than the total profit achievable from its subtree. The second step of Midas algorithm traverses the hierarchy top-down to select a final set of slices. Midas algorithm prioritizes valid slices at higher levels of the hierarchy for higher profit and coverage. The time complexity of Midas algorithm is O(m |P|), where m is the maximum number of distinct (subject, predicate) pairs, and |P| is the number of distinct predicates in the web source. The optimal slice discovery problem is APX-complete. The optimal slice discovery problem is APX-complete, making it impossible to derive a polynomial time algorithm with constant-factor approximation guarantees. However, the Midas algorithm is efficient at identifying multiple slices for a single web source in practice. A naive approach of applying Midas algorithm on every web source leads to low efficiency and accuracy due to ignoring hierarchical relationships among web sources from the same domain. The naive approach of applying Midas algorithm on multiple web sources leads to redundant slices, reducing total profit. A new framework is introduced to efficiently explore web source slices based on their natural hierarchy. The framework efficiently explores web source slices by starting from the finest grained sources and reusing derived slices. It improves execution efficiency and avoids redundant reporting. Key components include sharding, detecting slices, and consolidating for total profit optimization. Midas algorithm optimizes web source slices to maximize profit by pruning overlapping slices in parent sources. It outperforms baseline algorithms in identifying the best sources for knowledge base augmentation. Evaluation results show its efficiency on real-world datasets, with synthetic dataset results in the appendix. The ProLiant DL160 G6 server has 16GB RAM, two 2.66GHZ CPUs with 12 cores each, running CentOS release 6.6. Evaluation of algorithms is done on real-world datasets with an empty initial knowledge base. ReVerb dataset includes 15M facts extracted from 20M URLs, while NELL project continuously extracts facts from webpages with confidence score above 0.75. NELL extracts facts from webpages with confidence score above 0.75 and follows a pre-defined ontology. The dataset includes 2.9M facts from 340K URLs. Evaluation is based on precision of returned slices. ReVerb and NELL datasets provide input for slice discovery but do not contain optimal output. The ReVerb-Slim and NELL-Slim datasets are created by manually labeling content from sampled web sources. These datasets contain subsets of optimal slices for knowledge bases of varied coverage. ReVerb-Slim has 859K slices while NELL-Slim has 508K slices. The ReVerb-Slim and NELL-Slim datasets contain 859K and 508K facts respectively. Evaluation Setup involves selecting web sources and manually generating the Silver Standard. Comparison is made with a na\u00efve baseline and a greedy algorithm for deriving slices with maximum profit from web sources. Techniques are also compared with agglomerative clustering. Our proposed algorithm, AggCluster, compares with agglomerative clustering BID31 using a new objective function as the distance metric. The Midas algorithm organizes candidate slices hierarchically to derive slices from a single source and supports parallel execution of alternative algorithms like Greedy and AggCluster. Our framework supports running algorithms in parallel and evaluates their effectiveness and efficiency based on precision, recall, f-measure, and execution time. Evaluation includes comparing methods on real-world datasets like ReVerb and NELL, focusing on smaller datasets with silver standard for quality assessment. Methods are tested on ReVerb-Slim and NELL-Slim datasets with 100 web sources, assessing precision and execution efficiency. The study evaluates algorithms using input knowledge bases with coverage ranging from 0 to 80%. Precision-recall curves are shown for coverage ratios of 0, 0.4, and 0.8, with results indicating Midas outperforms alternative algorithms, especially on the ReVerb-Slim dataset. However, there is a decline in performance with increased coverage due to overlap with existing facts in the silver standard. Midas outperforms alternative algorithms, especially on the ReVerb-Slim dataset. Greedy performs poorly on both datasets. AggCluster struggles with datasets containing more entities and predicates. Na\u00efve ranks web sources based on new facts, with accuracy depending on the portion of sources with high-profit slices. The Na\u00efve baseline algorithm performs poorly in terms of precision on the ReVerb and NELL datasets, with values below 0.25 and 0.4, respectively. This is due to Na\u00efve not considering correlations among facts, leading to inaccurate results. Midas outperforms Na\u00efve by maintaining precision above 0.75 for both datasets. Greedy may miss high-profit slices but remains high in precision for the top-100 returns. AggCluster performs well on the NELL dataset but struggles with ReVerb due to more entities and predicates. AggCluster is comparable in precision but lacks scalability and is significantly slower than other methods. Knowledge extraction systems extract facts from diverse data sources and generate facts in either fixed ontologies or unlexicalized format. ClosedIE systems like KnowledgeVault, NELL, PROSPERA, DeepDive/Elementary, and systems in the TAC-KBP competition typically generate facts in fixed ontologies. OpenIE systems, on the other hand, operate differently. Time efficiency varies among methods, with Na\u00efve being the fastest, while AggCluster struggles with sources containing a large number of facts. Midas leverages extracted and cleaned facts to identify web source slices for knowledge base augmentation. The quality of these slices depends on the performance of extraction systems like TAC-KBP, OpenIE, and data cleaning tools. Midas uses customized gain and cost functions to evaluate the profit of web source slices, different from source selection techniques. Collection Selection is crucial in distributed systems. Collection Selection BID9 BID8 is a key issue in distributed information retrieval, involving selecting relevant documents from different servers or databases based on pregenerated descriptions and similarity metrics. The slice discovery problem, related to collection selection, focuses on finding the right web sources to fill knowledge gaps using existing knowledge bases. This process differs from traditional keyword queries and involves integrating results from multiple sources into a coherent ranked list. The slice discovery problem differs from collection selection in several ways: it involves hierarchical web sources, generating descriptions on the fly, and is related to clustering entities. Existing clustering techniques are not suitable for solving the slice discovery problem. This paper introduces Midas as an effective solution. Midas is a highly-parallelizable system for detecting high-profit web source slices to fill knowledge gaps. It defines a web source slice as a selection query and uses the Midas alg algorithm to detect high-quality slices. The system is scalable to millions of web sources and has shown effectiveness and efficiency in real-world scenarios. Challenges remain due to the quality of current extraction systems and missing extractions. In future work, the plan is to improve web source quality by extending techniques to overcome extraction limitations. The first step is crawling and extracting facts from web sources, with a cost model for training and estimating web source size. De-duplication is essential to remove redundant facts before augmentation. The augmentation process involves de-duplication of facts from web sources before adding them to the knowledge base. The cost of de-duplication is proportional to the number of selected facts and is subject to normalization factors. Validating new facts also incurs a cost based on the number of contributions from the web source slice. The overall cost of slices in the same web domain includes crawling, de-duplication, and validation steps. Adjustable normalization factors play a key role in computing these costs. Adjustable normalization factors are crucial in computing the costs of de-duplication, crawling, and validation steps for web source slices. The factors are set based on the execution time of techniques used, with default values of f p = 10, f c = 0.001, f d = 0.01, and f v = 0.1. The profit of a slice is determined by the difference between gain and cost, with validation being the most expensive operation after training. The profit function for web source slices prioritizes productivity, specificity, and dissimilarity. It aims to maximize the gain and minimize the cost of slices extracted from web sources. Midas simplifies the state-of-the-art procedure by assuming linear gain and cost functions with respect to the number of facts in web source slices. The profit function aims to identify the best slices for augmenting a knowledge base, considering overlapping slices. In Figure 4, selecting slice S 5 is the most effective option as it covers all new facts in the web source. The optimal solution for the slice discovery problem is determined by the optimal solution of the set cover problem. Slice S 5 is the most effective option due to its lower de-duplication and crawling costs compared to other slices. Removing or adding slices will result in a decrease in gain. The optimal solution for the slice discovery problem is based on the optimal solution of the set cover problem. It is proven that the slice discovery problem is NP-Complete. A slice is canonical if it has at least two children slices. Proposition 12 BID1 states that a slice is canonical if it has at least two children slices that are also canonical. This is proven through a contradiction argument, showing that if a slice is not canonical, it leads to a contradiction. The algorithm for identifying slices from a single web source is demonstrated in FIG9. During slice hierarchy construction, Midas algorithm creates slices at different levels from entities and generates parent slices for lower level slices. Non-canonical slices are pruned, and the algorithm works on two hierarchy levels simultaneously to record children slices accurately. The Midas algorithm constructs slices at different levels from entities, creating parent slices for lower level slices. Non-canonical slices are pruned based on profit lower bounds, with desired slices having current profit greater or equal to the lower bound. Slices with lower profit are eliminated during the pruning stage. The algorithm works on two hierarchy levels simultaneously to accurately record children slices. The Midas algorithm prunes non-canonical and low-profit slices, reducing the number of slices at level 2 significantly. It also eliminates slices with negative profit, resulting in a more refined selection of high-profit slices from web sources. The Midas algorithm prunes non-canonical and low-profit slices, reducing the number of slices at level 2 significantly. It also eliminates slices with negative profit, resulting in a more refined selection of high-profit slices from web sources. The evaluation of web source slices is based on two criteria: new information provided and ease of annotation. Human workers label statistics based on new facts for covered entities and entities that provide homogeneous information. The experiment sets K = 20 and marks a slice as \"correct\" if both statistics are above 0.5. The evaluation setup for ReVerb-Slim and NELL-Slim datasets involves selecting web sources, generating an Initial Silver Standard, and manually labeling slices and web sources returned by algorithms. Adjustments are made to the existing knowledge base based on the labeled silver standard slices. In an experiment, the performance of different methods is evaluated against knowledge bases with varied coverage. Synthetic data is used to analyze the tradeoffs between algorithms Greedy, Midas, and AggCluster, as well as the effectiveness of Midas' pruning strategies. Greedy and Na\u00efve perform poorly, while AggCluster competes with Midas but is slower. Synthetic data is created by randomly generating facts in a web source based on user-specified parameters. In a real-world scenario, randomness is introduced while generating facts in optimal slices. The selection rules ensure certain probabilities for conditions. Among k slices, m are chosen as optimal slices, and the knowledge base is constructed accordingly. Greedy, Midas, and AggCluster are compared based on running times and f-measure scores. In experiments with fixed parameters, Midas consistently detects web source slices accurately. In experiments with fixed parameters, Midas is highly accurate in detecting web source slices. However, its execution time grows linearly with the number of facts. AggCluster tends to make more mistakes with more facts and runs slower than Midas. Greedy runs faster but detects fewer optimal slices. When tested with 5000 facts and 20 slices, AggCluster is slower and less effective than Midas in identifying optimal slices. In experiments, Midas is accurate in detecting web source slices, with linear growth in execution time. Greedy is faster but detects fewer optimal slices. Midas prunes non-canonical and low-profit slices while constructing the hierarchy, leading to higher probability of reaching a local optimum. Comparing pruning strategies, Midas-PruneAll, MidasPruneNonCan, and Midas-NoPrune, shows varying numbers of slices with increasing facts. In experiments, Midas is accurate in detecting web source slices, with linear growth in execution time. Greedy is faster but detects fewer optimal slices. Midas prunes non-canonical and low-profit slices while constructing the hierarchy, leading to higher probability of reaching a local optimum. Comparing pruning strategies, Midas-PruneAll, MidasPruneNonCan, and Midas-NoPrune, shows varying numbers of slices with increasing facts. FIG1 demonstrates the number of slices with fixed number of facts (n = 5000) and an increasing number of optimal slices (m = 1 \u223c 10). Midas-PruneAll generates significantly fewer slices than Midas-PruneNonCan and Midas-NoPrune, which needs to examine every non-empty slice in the web source, thus producing several orders of magnitude more slices."
}