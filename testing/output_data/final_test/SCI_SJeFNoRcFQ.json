{
    "title": "SJeFNoRcFQ",
    "content": "Random Matrix Theory (RMT) is used to analyze weight matrices of Deep Neural Networks (DNNs), showing signatures of regularized statistical models without explicit regularization. A theory is developed to identify 5+1 Phases of Training, revealing Implicit Self-Regularization in DNNs. Tikhonov regularization is compared to Heavy-Tailed Self-Regularization in state-of-the-art DNNs, showing dependence on training process knobs. Generalization gap phenomena are exploited to demonstrate how changing batch size can cause a small model to exhibit all 5+1 phases of training. The inability of optimization and learning theory to explain NN properties has long been suspected, with VC theory not applying to these systems due to local minima in the energy/loss surface. The presence of local minima in the energy function was initially thought to limit the functions realizable by neural networks, but this was later found to not be a practical issue. Despite the existence of other theories of generalization based on statistical mechanics, VC theory continued to be popular in the machine learning community. Recent theoretical and empirical results suggest that the energy landscape of modern deep neural networks resembles that of a zero-temperature Gaussian Spin Glass. Theoretical Question: Why is it challenging to apply traditional regularization methods to deep neural networks due to the complexity of DNNs and the multitude of factors affecting training accuracy? Theoretical Question: Why is regularization in deep learning different from other areas of ML, and what theoretical framework is needed to investigate regularization for DNNs? Practical Question: How can one adjust the many parameters in modern DNN systems in a theoretically-principled way to train models efficiently and monitor their effects on the Energy Landscape? Seeking a Practical Theory of Deep Learning that provides tools for practitioners to engineer better DNNs and answers fundamental questions on why Deep Learning works. The main empirical results evaluate the Energy Spectral Densities (ESDs) of weight matrices in DNN models, revealing self-regularization patterns. Older/smaller models show Tikhonov-like self-regularization, while modern models exhibit heavy-tailed self-regularization. The ESDs of larger, modern DNN models deviate from Gaussian-based MP model, showing Heavy-Tailed Self-Regularization without a clear \"size scale\" separating \"signal\" from \"noise\". The main theoretical results provide an operational theory for DNN Self-Regularization using ideas from Random Matrix Theory. The text discusses using Random Matrix Theory concepts to create a visual taxonomy for 5+1 Phases of Training, focusing on modeling noise and signal in weight matrices. It also introduces a practical taxonomy for regularization based on different signal sizes. Based on Random Matrix Theory concepts, a visual taxonomy for 5+1 Phases of Training is constructed, each phase characterized by distinct signatures in DNN weight matrices. The phases include RANDOM-LIKE, BLEEDING-OUT, BULK+SPIKES, BULK-DECAY, HEAVY-TAILED, and RANK-COLLAPSE, showing Self-Regularization patterns. The theory is evaluated using a MiniAlexNet model by analyzing ESDs with and without explicit regularization. In the context of Random Matrix Theory, different phases of training in DNNs are characterized by distinct signatures in weight matrices. By adjusting training parameters, such as batch size, the ESDs of fully-connected layers can transition from RANDOM-LIKE to HEAVY-TAILED, improving generalization accuracy. These results highlight the Generalization Gap phenomenon and the implicit Self-Regularization associated with smaller batch sizes. The use of RMT provides insights into the behavior of neural networks during and after training. Random Matrix Theory (RMT) considers the density of singular values of random rectangular matrices. It makes strong statements about the distribution shape, bounds, and finite-size effects. RMT describes properties of the Eigenvalue Spectral Density (ESD) and compares predictions with empirical results, with a focus on Gaussian distributions. This leads to the basic MP theory, which is discussed in this section. The basic MP theory is discussed in this section, focusing on Universality classes of Heavy-Tailed distributions and the Gaussian Universality class. It involves modeling a random matrix W with elements from a Gaussian distribution, leading to the ESD of the correlation matrix X = W^T W having a limiting density given by the MP distribution \u03c1(\u03bb). Finite-size Fluctuations at the MP Edge show that all fluctuations concentrate sharply at the MP edge, \u03bb \u00b1, with the distribution of maximum eigenvalues \u03c1 \u221e (\u03bb max) governed by the TW Law. The TW Law governs the maximum eigenvalues \u03c1 \u221e (\u03bb max) and describes the edge statistics of finite-sized matrices. Violations of the MP Law indicate the onset of more regular non-random structures in DNN weight matrices, suggesting Self-Regularization. MP-based RMT is not generally applicable to matrices with strongly-correlated elements, common in well-trained DNNs modeled by Heavy-Tailed distributions in statistical physics. Recent results from MP/RMT have shown new Universality classes for matrices with elements from Heavy-Tailed distributions. These extensions are used to develop a theory of Regularization in Deep Learning, including Self-Regularization and Heavy-Tailed Self-Regularization. The theory is inspired by the Spiked-Covariance model and the application of MP/RMT tools in quantitative finance. The curr_chunk discusses the relation of Heavy-Tailed phenomena to Self-Organized Criticality in Nature and presents a generalized MP theory with various extensions. It highlights basic results and universality classes for modeling strongly correlated matrices. Additional details can be found in the physics and mathematics literature. The curr_chunk discusses modeling strongly correlated matrices with elements drawn from a Heavy-Tailed distribution. It compares basic MP theory, the Spiked-Covariance model, and Heavy-Tailed extensions of MP theory, including Universality classes. The ESD \u03c1 N (\u03bb) exhibits Heavy-Tailed properties globally or locally at the bulk edge. There are three Universality classes for applying MP theory to matrices with Heavy-Tailed distributions. The curr_chunk discusses the behavior of the edge of the ESD for matrices with Heavy-Tailed distributions. It explains the differences in fluctuations at the bulk edge compared to standard MP theory and describes the effects of finite size on distinguishing the edge versus the tail. The text also delves into different regimes based on the heaviness of the tails, highlighting the modeling challenges and statistical properties of the maximum eigenvalues. In the Heavy-Tailed regime with 0 < \u00b5 < 2, the ESD \u03c1 N (\u03bb) follows a Power Law distribution for all finite N, converging to a PL distribution with tails \u03c1(\u03bb) \u223c \u03bb \u22121\u2212\u00b5/2 as N \u2192 \u221e. There is no bulk edge, and maximum eigenvalues follow Frechet statistics. Fitting PL distributions to ESD plots helps identify the exponent \u03b1, which in turn determines the Universality class (0 < \u00b5 < 2, 2 < \u00b5 < 4, or 4 < \u00b5). The Clauset-Shalizi-Newman approach is used for fitting PL distributions. Observing a Heavy-Tailed ESD may indicate the presence of a scale-free DNN with strongly correlated matrix elements. Well-trained DNNs are likely to exhibit Heavy-Tailed behavior in their ESD for weight matrices. The ESDs of small and large DNNs differ significantly. Small models follow the MP theory well, while larger models have ESDs that deviate from theoretical predictions. RMT is used to compare ESDs of older and modern DNNs, such as LeNet5 and models like AlexNet and InceptionV3. The FC1 Layer of a model trained on the MNIST dataset has 100.00% training accuracy and 99.25% test accuracy. The ESD analysis reveals a MP-like shape for eigenvalues below a certain threshold, with outliers beyond that range. The overall shape of the density distribution aligns well with the MP theory. The statistics and shape of the local bulk edge in AlexNet's FC2 layer do not align well with standard MP theory, showing a lack of good fit and an indeterminate bulk edge. The ESD of the layer is concave everywhere, with a PL fit indicating \u03b1 \u2248 2.25. Similar Heavy-Tailed properties are observed in other pre-trained DNNs like VGG16, VGG19, ResNet50, InceptionV3. Fits for these models range from 1.5 < \u03b1 < 3.5. In most cases, the ESDs of DNN layers exhibit a range of 1.5 < \u03b1 < 3.5, with Power Law (PL) or Truncated Power Law (TPL) fits being the best. The ESDs also fall into the 2 < \u00b5 < 4 Universality class, with older models like LeNet5 showing a good fit to the theoretical MP density \u03c1 mp (\u03bb). The Spiked-Covariance model by Johnstone is a perturbative extension of the standard MP theory, demonstrating implicit self-regularization in DNN training. Large DNNs show deviations from traditional RMT, resembling strongly-correlated disordered systems with Heavy-Tailed behavior. This regularization is related to observations of implicit Tikhonov-like regularization on LeNet5, similar to behavior in strongly-correlated physical systems modeled by random matrices from non-Gaussian Universality classes. The ESDs of large, modern DNNs exhibit Heavy-Tailed properties, indicating strong correlations modeled by non-Gaussian random matrices. Unlike the Spiked-Covariance case, these DNNs show correlations on every size scale, blurring the distinction between the MP bulk and spikes. This suggests a new form of Heavy-Tailed Self-Regularization in state-of-the-art DNNs, leading to the development of an operational theory for DNN Self-Regularization. The MP Soft Rank is defined to capture the noise part of a matrix relative to its largest eigenvalue. It ranges from 0 to 1, with 1 indicating a purely random matrix. Visual Taxonomy categorizes Self-Regularization in DNNs into 5+1 phases during training. The ESD in RMT provides insights into the global and local properties of weight matrices in DNNs during training. RMT helps differentiate between the 5+1 phases of training by modeling weight matrices as \"noise plus signal.\" The ESD in RMT helps differentiate between the 5+1 phases of training by modeling weight matrices as \"noise plus signal.\" In the RANDOM-LIKE phase, the ESD is well-described by traditional MP theory with a random matrix from the Gaussian Universality class. In subsequent phases (BLEEDING-OUT, BULK+SPIKES), and for small networks like LetNet5, a small correction \u2206 is added to the random matrix W rand, allowing vanilla MP theory to be applied to the bulk of the ESD. In the BULK+SPIKES phase, the model resembles a Spiked-Covariance model, and Self-Regularization is similar to Tikhonov regularization. For more strongly-correlated phases, W rand becomes weaker, and Heavy-Tailed Self-Regularization is used to model the properties of \u2206. In the RANK-COLLAPSE phase, a degenerate case predicted by the theory, a MiniAlexNet model was analyzed. The model consists of 2D Convolutional layers with Max Pooling and Batch Normalization, followed by Fully Connected layers with ReLU activations and a final FC layer with softmax activation. The architecture includes matrices WFC1 (4096x384), WFC2 (384x192), and WFC3 (192x10). The model is trained using Keras 2.x with TensorFlow, using SGD with momentum and a learning rate of 0.01. Weight matrices are saved at the end of each epoch, and the properties of WFC1 and WFC2 matrices are analyzed. The matrix Entropy gradually decreases, while the Stable Rank shrinks, correlating with the increase in training/test accuracies. The ESD shows a shift from a RANDOM-LIKE phase to a BULK+SPIKES phase in a few epochs. During training, the model transitions from a RANDOM-LIKE phase to a BULK+SPIKES phase. As \u03bb max increases from 3.0 to 4.0, indicating Self-Regularization, spike eigenvectors become more localized than bulk eigenvectors. Explicit regularization leads to a decrease in complexity metrics, with eigenvalues in the spike pulled to larger values in the ESD. Eigenvector localization is more prominent with explicit regularization, showcasing all five main phases of learning. In the previous section, the model transitions through different learning phases based on the value of \u03bb max. Explicit regularization leads to eigenvector localization and a decrease in complexity metrics. In the following section, the impact of batch size on generalization in DNN training is explored. The impact of batch size on DNN training is explored by training the MiniAlexNet model with varying batch sizes. Both MP Soft Rank and Stable Rank decrease as batch size decreases, leading to an increase in test accuracy. Training and test accuracy decrease for larger batch sizes, with systematic changes in the ESD as batch size decreases. The ESD distribution changes as batch size decreases, with outliers appearing and spikes growing larger. Smaller batch sizes lead to more well-regularized models, impacting generalization gap. Training with smaller batch sizes leads to more well-regularized models, improving results by extracting finer-scale correlations from the data. This self-regularization phenomenon arises due to the DNN training process implicitly leading to stronger correlations, which larger batch sizes fail to capture. Our theory suggests that correlations at all size scales in DNNs lead to implicit self-regularization, influenced by various training parameters. By adjusting the batch size, we can observe different training phases in a small model, indicating the potential for exploring new directions in research. Large, well-trained DNN architectures exhibit Heavy-Tailed Self-Regularization, leading to implicitly regularized models with larger batch sizes. This explains the generalization gap phenomena in machine learning and artificial intelligence. The trend towards very large and deep neural networks may result in complex models that are difficult to optimize effectively. Current large and complicated models in machine learning are hard to train, sensitive to parameter settings, and difficult to understand. Theoretical work in ML, optimization, and statistics fails to provide practical guidance, leading to confusion in interpreting empirical results. For instance, optimization theory struggles to explain the Generalization Gap, while statistical learning theory like VC-based methods also fall short in providing insights for stochastic algorithms on non-convex landscapes. Theoretical work in machine learning struggles to explain the behavior of complex models like neural networks. VC-based methods fail to provide guidance for models with unlimited capacity that generalize well. The inability of optimization and learning theory to predict NN properties has been a long-standing issue. The presence of local minima in energy functions was initially thought to limit the functions realizable by neural networks, but this was later found not to be a practical problem. Despite the limitations of VC theory, it remained popular in machine learning communities. Recent theoretical results suggest that the energy landscape of modern deep neural networks resembles certain statistical mechanics theories. The Energy Landscape of modern DNNs resembles a zero-temperature Gaussian Spin Glass, and VC theory does not describe DNN properties. The Spin Glass analogy may help understand overtraining in DNNs. Questions about regularization and optimization in DNNs persist, with unclear definitions of DNN regularization. Regularization in traditional ML can be explicit or implicit, involving adding a capacity control term to the loss function. Regularization in traditional machine learning can be explicit or implicit, with the latter involving adjustable operational procedures like early stopping or truncating small entries of a solution vector. Tuning the regularization parameter through cross-validation is common. The challenge in applying these ideas to deep neural networks (DNNs) lies in the numerous adjustable parameters that can impact training accuracy. The Energy Landscape of DNNs resembles a zero-temperature Gaussian Spin Glass, and questions about regularization and optimization in DNNs persist. The Energy Landscape in Deep Learning affects training accuracy and regularization techniques. Over 50 different regularization methods exist, including Weight Norm regularization, early stopping, batch size adjustments, Batch Normalization, and Dropout. Evaluating these methods is challenging due to the large number of options and system constraints. Deep Learning avoids cross-validation and focuses on driving training error to zero. Leakage of test information into the training process is a concern. Regularization in deep learning differs from other ML areas, leading to the need for unsupervised metrics. Theoretical questions focus on the framework for investigating regularization for DNNs, while practical questions aim to control and adjust the many parameters in modern DNN systems. A Practical Theory of Deep Learning is sought to provide prescriptive tools for practitioners. A Practical Theory of Deep Learning is needed to characterize and control the Energy Landscape for better DNNs. This theory would offer tools for practitioners and answer fundamental questions about why Deep Learning works. VC theory does not provide this type of theory. The Energy Landscape of a DNN with L layers can be described by its optimization function. The Energy Landscape of a DNN with L layers can be controlled by minimizing the loss between the DNN and labels y i. Regularization techniques, such as constraining the norm of weight matrices, are crucial to prevent overtraining. Advanced methods can be used for implicit regularization. Using advanced methods from Random Matrix Theory (RMT), we analyze the distribution of eigenvalues in DNN layer weight matrices during and after training. The Energy Landscape, representing the optimization problem, changes at each epoch as we adjust the values of weight matrices and bias vectors through Backprop training. We analyze the distribution of eigenvalues in DNN layer weight matrices using Random Matrix Theory (RMT) for large, pre-trained models like LetNet5, AlexNet, and Inception. The Empirical Spectral Density (ESD) can be well-described by traditional RMT or Heavy-Tailed behavior, with exceptions like the Spiked-Covariance model. The study focuses on developing a practical theory of Implicit Self-Regularization in Deep Neural Networks (DNNs) by analyzing the behavior of weight matrices in various DNN models. The main empirical results involve evaluating the Empirical Spectral Densities (ESDs) for these models, revealing different forms of Self-Regularization. The study explores Implicit Self-Regularization in Deep Neural Networks by analyzing weight matrices in different models. Capacity control metrics like Matrix Entropy and Stable Rank are used to track Self-Regularization. Older/smaller DNN models show weak Self-Regularization, resembling Tikhonov regularization. The study analyzes Implicit Self-Regularization in Deep Neural Networks by examining weight matrices in various models. Larger, modern DNN models exhibit Heavy-Tailed Self-Regularization, deviating from Gaussian-based models. The main theoretical results provide an operational theory for DNN Self-Regularization using ideas from Random Matrix Theory. Our theory utilizes Random Matrix Theory to provide a visual taxonomy for 5 + 1 Phases of Training, modeling noise and signal in weight matrices. The approach includes Gaussian and Heavy-Tailed Universality classes for different signal sizes, leading to 5+1 Phases of Regularization. The 5+1 Phases of Regularization in DNN weight matrices include Random-like, Bleeding-out, Bulk+Spikes, Bulk-decay, Heavy-Tailed, and Rank-collapse. The Rank-collapse phase represents over-regularization with dominant eigenvalues. Well-optimized large DNNs are expected to exhibit Heavy-Tailed Self-Regularization. The theory is evaluated using a smaller MiniAlexNet model without explicit regularization. Implementation details are provided for methods underlying the theory. All 5+1 phases of regularization in DNN weight matrices can be exhibited by modifying training process knobs. The study explores the impact of decreasing batch size on the ESDs of fully-connected layers in MiniAlexNet, showing a continuous variation from Random-like to Heavy-Tailed distributions. This leads to increased generalization accuracy, highlighting the Generalization Gap phenomena and the role of Self-Regularization with smaller batch sizes. Additionally, extreme Weight Norm regularization can induce the Rank-collapse phase. The main methodological contribution involves using empirical observations and RMT developments to create a practical predictive DNN theory, treating DNN training as novel laboratory experiments. The training of different DNNs follows the scientific method, observing and analyzing large pretrained models and smaller models during training. The Energy Landscape, defined by DNN weight matrices, is challenging to study directly but can be analyzed through the weight matrices and their correlations. This approach addresses both scientific and engineering questions in deep learning. In deep learning, the Energy Landscape defined by DNN weight matrices can be analyzed through correlations, providing insights into the class of functions being learned. Weight matrices exhibit strong correlations and can be modeled by random matrices from the Universality class of Heavy-Tailed distributions, impacting the functions learned. This connects to the Energy Landscape, as Heavy-Tailed random matrices have a different Energy Landscape compared to Gaussian-like random matrices. Section 2 covers capacity metrics and Backprop transitions, while Sections 3 and 4 review RMT background. In Sections 3 and 4, the background on Random Matrix Theory (RMT) is reviewed to understand experimental methods and initial results. Section 5 presents the main theory of 5+1 Phases of Training, followed by evaluation in Sections 6 and 7, showing the impact of explicit regularization on the generalization gap phenomenon. Section 8 discusses the results in a broader context. The accompanying code is available for reference. In this section, simple spectral metrics are used to characterize DNN weight matrices and observe capacity properties during training. A DNN's architecture is defined by its weights and biases at each layer. A capacity control metric is sought for a learned DNN model that can describe changes in weight matrices during training and identify structural changes. One approach is to measure the Euclidean distance between initial and current weight matrices, but this distance is not scale invariant. Weight matrices may shift in scale during training, gaining or losing Frobenius mass or variance. The BatchNorm layer aims to prevent shifts in weight matrix scale during training. Two scale-invariant measures of capacity control, Matrix Entropy (S) and Stable Rank (R s), are defined in terms of the spectrum of weight matrices. Eigenvalues and singular values are used to compute these metrics. The Hard Rank, Matrix Entropy, and Stable Rank are matrix complexity metrics based on singular values and eigenvalues of weight matrices. Matrix Entropy measures the randomness of a matrix, while Stable Rank is a robust variant of Hard Rank. Lower Matrix Entropy values indicate more structure in the matrix. The Stable Rank and Hard Rank are complexity metrics based on singular values and eigenvalues of weight matrices in neural networks. They measure matrix capacity and show similar behavior during training. An example is illustrated on a 3-layer Multi-Layer Perceptron (MLP) with fully connected layers and ReLU activation. The model is trained on CIFAR10 with 4 layer weight matrices initialized with Glorot normalization. Training lasts up to 100 epochs with SGD and a stopping criteria on MSE loss. Layer entropy and stable rank decrease during training for FC1 and FC2, with stable rank being more informative. Matrix entropy also decreases slightly. The scree plots for the weight matrices of FC1 and FC2 layers in the MLP3 show changes in eigenvalues, but lack detailed insight into subtle entropy and rank changes during Self-Regularization. These metrics provide a coarse picture and do not capture the nuanced changes in singular values and vectors that are important for analysis. The density of singular values and eigenvalues in the weight matrices of the FC2 layer of the MLP3 model is analyzed to gain detailed insight into changes during training. The initial density resembles a quarter circle, while the final density consists of a bulk quarter circle with spikes beyond the edge. The variance does not change much between the initial and final weight matrices. The variance, or Frobenius norm, remains stable between initial and final weight matrices. The final eigenvalue density shows spikes beyond the edge, with the largest eigenvalue around 7.2. The stable rank for FC2 decreases by approximately 2X due to a larger squared Spectral norm. RMT analysis reveals fine-scale structure hidden from the initial density plot, suggesting fruitful insights. The Marchenko-Pastur (MP) theory, a more general form of Random Matrix Theory (RMT), is applicable to rectangular matrices like DNN weight matrices. It considers the density of singular values of random rectangular matrices, providing insights beyond traditional RMT approaches. MP theory, a form of Random Matrix Theory, analyzes the density of eigenvalues of matrices like DNN weight matrices. It makes strong statements about distribution shape, bounds, finite-size effects, and convergence rates. The theory assumes statistical properties of weight matrices that are independent of specific details and holds even at finite size, borrowing from Universality concepts in Statistical Physics. To apply RMT, specify matrix dimensions and assume elements are drawn from a specific distribution within a Universality class. Random Matrix Theory (RMT) analyzes the density of eigenvalues of matrices like DNN weight matrices, with a focus on Universality classes such as Gaussian distributions. MP theory describes the ESD of the correlation matrix, X = W T W, with a limiting density given by the MP distribution \u03c1(\u03bb). This theory holds even at finite size and allows for comparison with empirical results. The MP distribution for different aspect ratios and variance parameters is illustrated in FIG8. The shape of the distribution depends on variance \u03c3^2mp and aspect ratio Q. As Q increases, the support narrows and the distribution becomes less skewed. Conversely, as Q approaches 1, the support widens and the distribution skews more. The shape of the MP distribution depends on variance \u03c3^2mp and aspect ratio Q. As Q increases, the support narrows and the distribution becomes less skewed. Conversely, as Q approaches 1, the support widens and the distribution skews more. The Quarter Circle Law applies when Q = 1, resulting in a very peaked eigenvalue density with a bounded tail. The density of singular values takes the form of a Quarter-Circle. The density of singular values of W l, \u03c1(\u03bd), follows a Quarter-Circle shape. Fluctuations at the MP edge concentrate sharply at \u03bb \u00b1 in the infinite limit. Even for finite-sized matrices, the upper edge of \u03c1(\u03bb) is very sharp. Violations of the MP Law indicate the onset of non-random structure in DNN weight matrices, interpreted as Self-Regularization. Empirical eigenvalues extend beyond the sharp edge predicted by the MP fit, distinguishing between finite size effects and outliers. Fluctuations in \u03bb max follow TW statistics within MP theory, with maximum deviation in DNNs being small. Outliers can be identified visually or through ensemble runs, studying eigenvalues for information content. TW theory can also be applied for further analysis. MP-based RMT is applicable to a wide range of matrices, but not when elements are strongly correlated, as seen in many well-trained DNNs. Heavy-Tailed distributions are used to model such systems in statistical physics, showing similar statistical behavior to natural phenomena with strong correlations. New Universality classes have been discovered for matrices with elements from certain Heavy-Tailed distributions. The text discusses the use of Heavy-Tailed distributions in building a theory of Regularization in Deep Learning. It also mentions the application of Heavy-Tailed extensions to justify certain statistical behaviors in matrices. The theory of Self-Regularization and Heavy-Tailed Self-Regularization in Deep Learning is inspired by various models and tools in quantitative finance. The use of Heavy-Tailed distributions in modeling correlated matrices is highlighted, with references for additional details in the physics and mathematics literature. The ESD \u03c1 N (\u03bb) exhibits Heavy-Tailed properties when matrices have elements drawn from a Heavy-Tailed distribution. The Universality of RMT applies to a broad range of problems, including DNN Regularization. Heavy-Tailed phenomena have subtle properties, but we focus on simple cases. In this section, we discuss Heavy-Tailed extensions to MP theory, focusing on three Universality classes: Weakly Heavy-Tailed, Moderately Heavy-Tailed, and the absence of bulk edge in the latter case. Finite-size effects lead to difficulty in distinguishing the edge from the tail at finite N. In the regime of \u00b5, the global ESD can be modeled by \u03c1 N (\u03bb) \u223c \u03bb \u2212(a\u00b5+b) for all \u03bb > \u03bb min, with large finite-size effects. The maximum eigenvalues follow Frechet statistics, and the ESD is Heavy-Tailed for 0 < \u00b5 < 2, converging to a power-law distribution with tails \u03c1(\u03bb) \u223c \u03bb \u22121\u2212\u00b5/2 as N \u2192 \u221e. There is no bulk edge in this regime. The eigenvalues follow Frechet statistics. Finite-size effects are smaller compared to the 2 < \u00b5 < 4 regime. Log-log histogram plots of the ESD for three Heavy-Tailed random matrices with different values of \u00b5 are visualized. Different Heavy-Tailed Universality classes are described. Visual exploration and classification of ESDs are performed by plotting them on different coordinate systems. Data from power-law and Gaussian distributions have distinct appearances on different plots. The log-log plot shows data from a Gaussian distribution appearing as a bell-shaped curve. Examining data from an unknown ESD on different axes suggests a classification. A visual and operational approach is provided to understand limiting forms for different \u00b5, as shown in FIG9 with log-log histograms for ESD \u03c1 N (\u03bb) for three Heavy-Tailed random matrices M N (\u00b5). The ESD remains Heavy-Tailed even with an infinite limit. Different \u00b5 values affect the distribution, with larger effects for higher values. The tails decay at different rates for various Heavy-Tailed Universality classes. Fitting PL distributions to ESD plots helps identify the distribution visually. Fitting a power-law (PL) distribution to the empirical spectral density (ESD) visually using a log-log histogram helps determine the exponent \u03b1. The Clauset-Shalizi-Newman (CSN) approach is used for this purpose. Care must be taken to ensure linearity on a log-log scale before applying the PL estimator, which works well for exponents in the range 1.5 < \u03b1 < 3.5. The CSN estimator performs well for 0 < \u00b5 < 2, but deviations occur for 2 < \u00b5 < 4 due to finite-size effects, and no reliable results are obtained for 4 < \u00b5. Identifying the Universality class involves matching \u03b1 to corresponding \u00b5, determining the appropriate Heavy-Tailed Universality class (0 < \u00b5 < 2, 2 < \u00b5 < 4, or 4 < \u00b5). Observing a Heavy-Tailed ESD may indicate a scale-free DNN, suggesting strong correlation and the need for a more complex model. This does not necessarily imply regularity. In DNN regularization, the PL exponent \u03b1 is fit using the CSN estimator for the ESD \u03c1 emp (\u03bb) of a random Heavy-Tailed matrix W(\u00b5). Finite-size effects are modest for 0 < \u00b5 < 2, while for 2 < \u00b5 < 4, significant finite-size effects are observed. The CSN method fails for \u00b5 > 4. Plots are shown for varying Q in (6(b)) and (6(c), with M and N fixed, indicating a Heavy-Tailed distribution in the matrix elements of W l. The text discusses the behavior of weight matrices in well-trained DNNs, noting that they exhibit Heavy-Tailed behavior. Eigenvectors in random matrices drawn from Gaussian Universality class are typically delocalized, while in other models they can be localized. Eigenvector delocalization is modeled using the Thomas Porter Distribution in traditional RMT. The components of a typical bulk eigenvector should be Gaussian distributed for maximum entropy. The text discusses eigenvector localization metrics for Gaussian distributed components in weight matrices of well-trained DNNs. Metrics include Generalized Vector Entropy, Localization Ratio, and Participation Ratio, with lower values indicating more localization. Empirical results show differences in ESDs between small and large DNNs. The text discusses differences in eigenvector spectral densities (ESDs) between small and large DNNs. Small models align with theoretical predictions, while larger models exhibit different ESD functional forms. Various models are analyzed, including LeNet5 for smaller models and AlexNet, InceptionV3, and others for larger models. LeNet5, a prototype early DNN model from the late 1990s, is used as a benchmark. LeNet5, a prototype early DNN model dating back to the late 1990s, is the most widely-known example of a Convolutional Neural Network (CNN) used for recognizing hand written digits. The model consists of 2 Convolutional and MaxPooling layers, followed by 2 Dense layers (FC1 and FC2). It inspired modern DNNs for image classification like AlexNet and VGG models. The model was recoded and retrained using Keras 2.0 with 20 epochs of the AdaDelta optimizer on the MNIST dataset, achieving 100.00% training accuracy and 99.25% test accuracy. The ESD of the FC1 Layer is analyzed, with the matrix WFC1 being a 2450 \u00d7 500 matrix with Q = 4.9. The FC1 matrix WFC1 is a 2450 \u00d7 500 matrix with Q = 4.9, yielding 500 eigenvalues. The ESD for FC1 of LeNet5 shows a large MP-like shape for eigenvalues \u03bb < \u03bb + \u2248 3.5, with some eigenvalue mass bleeding out from the MP bulk for \u03bb \u2208 [3.5, 5]. Beyond this, there are clear outliers or spikes ranging from \u2248 5 to \u03bb max 25. The ESD for FC1 of LeNet5 shows a large MP-like shape for eigenvalues \u03bb < \u03bb + \u2248 3.5, with outliers or spikes ranging from \u2248 5 to \u03bb max 25. The global bulk fit and local bulk edge align well with standard MP theory. AlexNet, a pioneering DNN, achieved remarkable performance on the ImageNet ILSVRC2012 classification task. AlexNet, a pioneering DNN, achieved remarkable performance on the ImageNet ILSVRC2012 classification task with a 16.4% lead over the runner up. It resembles a scaled-up version of LeNet5 with 5 layers, including 2 convolutional and 3 FC layers. The version analyzed is distributed with pyTorch (version 0.4.1), with specific matrix dimensions and quality factors for each FC layer. ESDs for weight matrices of AlexNet for Layers FC1, FC2, and FC3 are presented, with best MP fits determined by adjusting the \u03c3 parameter. The eigenvalues of AlexNet's FC1 layer show deviations from the best standard MP fit, with a convex shape near the bulk edge. In contrast, the ESD for FC1 of LeNet5 has an excellent MP fit capturing all bulk mass with few outlying spikes. The eigenvalues of AlexNet's FC1 layer deviate from the standard MP fit, with an indeterminate bulk edge and edge fluctuations that do not resemble a TW distribution. The ESD differs significantly from standard MP theory, with no good MP fit found. The bulk edge is only defined by a crude fit, and does not exhibit TW behavior. The ESDs of the layers deviate from MP theory, with concave shapes and poor fits. The local edge properties also do not align with predictions. The bulk edge shows a breakdown in RMT behavior. In the years following AlexNet, new DNNs like ZFNet, VGG, GoogLeNet, and ResNet emerged as winners in the ILSVRC ImageNet competitions. These models exhibit similar properties to AlexNet, with power law fits performed on their Linear/FC layers. In particular, the Inception model stands out for its unique characteristics. In 2014, VGG and GoogLeNet were close competitors in the ILSVRC2014 challenges, with GoogLeNet winning the classification challenge and VGG performing better in localization. VGG and GoogLeNet were top performers in the ILSVRC2014 challenges. VGG, with 19 layers, outperformed in localization, while GoogLeNet, with 22 layers, won the classification challenge. VGG's deep architecture uses smaller filters to capture correlations better, but it has a downside of high parameter count and memory usage. GoogLeNet, similar to VGG but more computationally efficient, has fewer parameters and a different architecture with no internal FC layers. The GoogLeNet architecture, with its sparse design using Inception modules, replaces FC layers with global average pooling for efficiency. An Auxiliary block is included for deep networks, connecting to the labels. This results in a single rectangular 768 \u00d7 1000 tensor. For analysis of InceptionV3 BID186, we select layer L226 from the Auxiliary block and the final FC layer L302. ESDs for both layers show deviations from MP theory, with L226 displaying bimodal ESDs. These results suggest Inception models may not account for all data correlations. The ESD of layer L226 in InceptionV3 BID186 shows deviations from MP theory, with bimodal distribution. The fit for the bulk edge at \u03bb + \u2248 10 overestimates bulk variance and underestimates spikes, indicating a poor statistical confidence test for an MP distribution. Numerous spikes extend to \u03bb max \u2248 30, suggesting a heavier tail than any MP fit. The distribution of L226 is unclear if it is truly heavy-tailed or appears so due to bimodality. The ESD of layer L226 in InceptionV3 shows deviations from MP theory with a bimodal distribution, suggesting a heavier tail than any MP fit. The ESD for layer L302 is slightly bimodal but not as strongly as L226, with significant eigenvalue mass extending into a long tail. The shape of the global ESD is wrong, and the MP fit is concave near the edge, where the ESD is convex. The ESD of L302 resembles that of the Heavy-Tailed FC2 layer of AlexNet. The ESD of L302 is similar to the Heavy-Tailed FC2 layer of AlexNet, with a small bimodal structure. A more rigorous approach is needed to determine the specific distribution type and Universality class. Various pre-trained models from Computer Vision and NLP, such as VGG16, VGG19, ResNet50, InceptionV3, and models from AllenNLP BID98, have been analyzed for their properties. See Table 5 for details. The ESD of selected layers from pre-trained ImageNet and NLP models exhibit Heavy-Tailed properties and Power Law fits, with most fits falling in the range 1.5 < \u03b1 < 3.5. Hard Rank deficiency was also observed in layers of several models. See Table 5 for detailed results. The ESD of selected layers from pre-trained ImageNet and NLP models exhibit Heavy-Tailed properties and Power Law fits, with most fits falling in the range 1.5 < \u03b1 < 3.5. Comparison with other distributions such as Truncated Power Law (TPL) shows that either PL or TPL fits best. Nearly all ESDs fall into the 2 < \u00b5 < 4 Universality class, with PL exponents \u03b1 displayed in figures for both PyTorch and AllenNLP models. Overall, the ESD of layers from pre-trained ImageNet and NLP models exhibit Heavy-Tailed properties, with most layers having \u03b1 \u2208 [2, 4]. RMT predicts that for matrices with Q > 1, the minimum singular value will be greater than zero, which is mostly true for ImageNet models but not for all FC layers. Some layers have \u03bd min \u223c 0, with a few having \u03bd min < 0.00001. In these cases, the ESD still shows Heavy-Tailed properties, but with a rank loss ranging from one eigenvalue equal to 0 up to 15% of the eigenvalue mass. The ESD of layers from pre-trained models exhibit Heavy-Tailed properties, with most layers having \u03b1 \u2208 [2, 4]. RMT predicts that for matrices with Q > 1, the minimum singular value will be greater than zero. In some cases, the ESD still shows Heavy-Tailed properties, with a rank loss ranging from one eigenvalue equal to 0 up to 15% of the eigenvalue mass. For NLP models, there is no rank collapse, with all AllenNLP layers having \u03bd min > 0. In some cases, MP theory applies to the bulk of the ESD, with only a few outlying eigenvalues larger than the bulk edge. The ESDs of DNN weight matrices do not align with standard RMT/MP theory, likely due to non-random, correlated weight matrices. Most DNNs exhibit Heavy-Tailed properties, suggesting the need for a new theory using modern RMT to analyze weight matrices. The ESDs of DNN weight matrices, especially for older and smaller models like LeNet5, can be well-fit to theoretical MP density \u03c1 mp (\u03bb), with distinct spikes (\u03bb > \u03bb + ). This behavior is consistent with the Spiked-Covariance model and traditional Tikhonov regularization, suggesting a form of Self-Organization and implicit Self-Regularization in the DNN training process. The trained model shows deviations from traditional Random Matrix Theory (RMT) in large, deep neural networks, resembling strongly-correlated systems with Heavy-Tailed behavior. This behavior is related to implicit Tikhonov-like regularization on LeNet5, where the weight matrices exhibit Heavy-Tailed properties. The ESDs of modern DNNs align with Heavy-Tailed variants of RMT, indicating a strong correlation in the weight distribution. Modern DNNs exhibit a new form of Heavy-Tailed Self-Regularization, with correlations appearing on every size scale. A new theory for DNN Self-Regularization is developed and tested on miniature AlexNet, addressing differences between older and newer models and the impact of adjustments during training. The theory presented in the curr_chunk explores the relationship between various training parameters and regularizers in Deep Neural Networks. It introduces the concept of MP Soft Rank, a metric designed to capture the noise level in weight matrices. This metric is based on MP theory and makes predictions that are tested in subsequent sections. The MP Soft Rank is a metric based on MP theory that measures the noise level in weight matrices. It differs from the Stable Rank and is defined based on how well MP theory fits the matrix ESD \u03c1 N (\u03bb). The Stable Rank indicates the number of eigencomponents needed for a low-rank approximation, while the MP Soft Rank describes the fit of MP theory to the ESD. The MP theory fits part of the matrix ESD \u03c1 N (\u03bb) and correlates with R s and R mp. There may be no good low-rank approximation of layer weight matrices W l in a well-trained DNN. Implicit Self-Regularization in DNNs during training is characterized as a visual taxonomy of 5+1 Phases of Training, each phase showing progressively more SelfRegularization. The phases can be ordered based on Stable Rank / MP Soft Rank, with earlier phases corresponding to older/smaller models and later phases to more modern models. The taxonomy of 5+1 phases of training in DNNs allows for comparison of different architectures and regularization levels. Global properties of the ESD, such as fitting to MP or Heavy-Tailed distributions, and local properties like fluctuations around \u03bb + or \u03bb max, are key considerations. The shape of the ESD near \u03bb + varies between different phases. The 5+1 phases of learning in DNN training involve observing Bulk+Spikes and Heavy-Tailed patterns in existing models like LeNet5 and AlexNet/InceptionV3. A simple model, MiniAlexNet, exhibits all 5+1 phases. FIG1 illustrates the decrease in MP Soft Rank and fitting of ESD using MP theory during Backprop training. There are eigendirections below \u03bb + that fit the MP bulk well, while those above \u03bb + correspond to spikes. RMT can differentiate between the 5+1 Phases of Training using simple models that describe the shape of each ESD. Each phase models weight matrices as \"noise plus signal,\" with the \"noise\" from a random matrix and the \"signal\" as a correction. The theoretical model for each phase is summarized in Table 7, using RMT to describe the global and local shapes of the weight matrices. In the Bulk+Spikes phase, the model resembles a Spiked-Covariance model, and the SelfRegularization resembles Tikhonov regularization. Later phases (Bulk-decay, Heavy-Tailed) exhibit outlying spikes and possible Heavy-Tailed behaviors. The ESD is well-described by traditional MP theory in the Random-like phase, with a relatively-small perturbative correction in subsequent phases. The weight matrix is modeled as a vanilla W mp matrix, with a large MP Soft Rank. In later phases, such as Bulk-decay and Heavy-Tailed, the model becomes more complex with \u2206 dominating over W rand. The MP Soft Rank collapses in these strongly-correlated phases, leading to a Heavy-Tailed SelfRegularization. The noise term W rand is treated as small, and properties of \u2206 are modeled with Heavy-Tailed extensions of vanilla MP theory. The Rank-collapse phase is a degenerate case predicted by the theory. In the Random-like phase, DNN weight matrices resemble a Gaussian random matrix and can be fit to an MP distribution. Even well-trained DNNs may have Random-like ESDs, indicating structure that can be modeled as a sum of a random \"noise\" matrix and a small-sized matrix. In the Bleeding-out phase, the ESD still appears random except for a few eigenvalues extending beyond the MP edge \u03bb +. By modeling the weight matrix as a sum of a noise matrix and a medium-sized signal matrix \u2206 medium, spikes emerge from the bulk when \u03bb max \u2212 \u03bb + is small. In the Bleeding-out phase, spikes emerge from the bulk when \u03bb max \u2212 \u03bb + is small, making it challenging to distinguish spike from bulk eigenvalues. The transition from Random-like to Bleeding-out phase corresponds to the BPP phase transition, representing a condensation of the largest eigenvalue's eigenvector onto the rank-one eigenvalue. In the Bulk+Spikes phase, the ESD appears mostly random with a few eigenvalues extending beyond the MP edge \u03bb +. The model involves a noise matrix W rand and a moderately large-sized matrix \u2206 large representing learned \"signal\". Identifying spike locations may be challenging in a single run, but an ensemble of runs can help. In the Bulk+Spikes phase, spike locations can be challenging to identify in a single run, but become clearer in an ensemble of runs. The empirical bulk variance is smaller than the full elementwise variance, indicating a distinct separation of spikes from the bulk. This phase corresponds to vanilla MP theory with a large low-rank perturbation, observed in the LeNet5 model. In the Bulk+Spikes phase, Tikhonov-like Self-Regularization is evident in the Spiked Covariance model. Eigenvalues bleed out forming spikes due to large perturbations, with spikes exhibiting Gaussian fluctuations. Eigenvector localization on extreme eigenvalues can diagnose Spike eigenvectors. The Bulk-decay phase is characterized by Heavy-Tailed behavior in the ESDs, with a concentration of \"information\" on a small number of eigenvectors associated with outlier eigenvalues. This phase is intermediate between the Bulk+Spikes and Heavy-Tailed phases, showing similarities to Bleeding-out and Bulk+Spikes but quantitatively different. The Bulk-decay phase exhibits Heavy-Tailed behavior in the ESDs, with a concentration of information on outlier eigenvalues. The properties of the Bulk region become inconsistent with MP theory, with \u03bb max potentially large and the MP Soft Rank smaller. The ESDs show Heavy-Tailed properties at the bulk edge, suggesting a shift towards a Heavy-Tailed form overall. The ESDs may align with a weakly Heavy-Tailed Universality class. The curr_chunk discusses the Heavy-Tailed behavior in edge statistics due to finite-size effects and the challenges in estimating the scale during Backprop training. It also mentions the need for more precise measurements and theories for optimization. The Heavy-Tailed phase in edge statistics is characterized by an ESD resembling a random matrix with entries drawn from a Heavy-Tailed distribution. It involves modeling W as the sum of a noise matrix and a strongly-correlated signal matrix learned during training. The ESD visually appears differently in this phase. In the Heavy-Tailed phase, the ESD visually appears Heavy-Tailed, making it difficult to fit the layer weight matrices W using standard Gaussian-based MP/RMT. This phase corresponds to a variant of MP theory where elements are chosen from a non-Gaussian Universality class, leading to strong implicit Self-Regularization. In the Bulk+Spikes phases, the Stable Rank decreases due to Frobenius mass moving from the bulk to the spikes. Heavy-Tailed Self-Regularization does not show a distinct \"size scale\" in eigenvalues separating signal from noise. Comparing Heavy-Tailed ESDs is best done with log-log histograms and PL fits. The ESD for FC3 of pretrained AlexNet and a random matrix with Heavy-Tailed Pareto distribution are shown in FIG1. The ESDs of WFC3 of AlexNet are Heavy-Tailed, resembling a random matrix with entries from a Heavy-Tailed distribution. A PL fit can estimate \u03b1 to determine the Universality class. If \u03b1 < 2, it indicates a (very) Heavy-Tailed Universality class; if 2 < \u03b1 < 4, \u03b1 is well-modeled by \u03b1 \u2248 b + a\u00b5, suggesting a (moderately, or \"fat\") Heavy-Tailed Universality class. The Rank-collapse Phase is an additional phase expected based on MP theory. In the Rank-collapse Phase, the minimum singular value is strictly positive for many parameter settings. The MP distribution can have a spike at the origin, indicating a rank-deficient matrix. This phenomenon occurs when Q > 1 in vanilla Gaussian-based MP theory and more generally in Heavy-Tailed MP theory. The validation and application of the theory involved training with the MiniAlexNet model. Sections 6.1 to 6.4 discuss the setup, baseline results, technical details, and the effect of explicit regularization. The impact of changing batch size is addressed in a later section. The basic setup for empirical evaluation involves analyzing a Deep Neural Network model called MiniAlexNet, a scaled-down version of AlexNet trained on CIFAR10. The architecture includes 2D Convolutional layers with Max Pooling and Batch Normalization, followed by Fully Connected layers with ReLU activations and a final FC layer with 10 nodes and softmax activation. Batch Normalization is applied to Conv2D layers but not to the FC layer. The experimental setup involves training MiniAlexNet, a scaled-down version of AlexNet on CIFAR10. Batch Normalization is applied to Conv2D layers but not to the FC layer. Models are trained using Keras 2.x with TensorFlow backend, using SGD with momentum and a learning rate of 0.01. Weight matrices are saved at the end of every epoch for analysis of model complexity. Experimental runs include analyzing ESDs during Backprop training in a single run. The experimental setup involves training MiniAlexNet on CIFAR10 with Batch Normalization applied to Conv2D layers. Weight matrices are analyzed during training runs, including final ESDs from 1 run and an ensemble of final ESDs from multiple runs to compensate for finite-size effects and clarify scientific claims about the learning process. The Glorot Normal initialization BID101 involves a truncation step, leading to an empirical variance larger than one. Rescaling the empirically-fit \u03c3 is necessary to apply MP theory. Various RMT-based quantities are computed for each layer weight matrices, including Matrix Entropy, Hard Rank, Stable Rank, and MP Soft Rank. The study involves analyzing RMT-based metrics for weight matrices in neural networks, including Eigenvalue Spacing Distributions (ESDs), Eigenvector localization metrics, and various training process parameters like number of epochs, Weight Norm regularization, Dropout, and batch size. Baseline results for MiniAlexNet are presented with specific settings for the learning process. The study analyzes RMT-based metrics for weight matrices in neural networks, including Eigenvalue Spacing Distributions and Eigenvector localization metrics. Baseline results for MiniAlexNet with specific learning process settings are presented, showing Matrix Entropy and Stable Rank for layers FC1 and FC2 as a function of epochs. Training and test accuracies increase as Matrix Entropy and Stable Rank decrease, leveling off as accuracies stabilize. The study examines RMT-based metrics for weight matrices in neural networks, focusing on Eigenvalue Spacing Distributions and Eigenvector localization. Baseline results for MiniAlexNet show changes in Eigenvalue Spectrum during training, indicating Self-Regularization. The initial weight matrices exhibit Random-like phases that transition to Bulk+Spikes phases, with minor changes continuing in the Eigenvalue Spacing Distributions. The study analyzes RMT-based metrics for weight matrices in neural networks, focusing on Eigenvalue Spacing Distributions and Eigenvector localization. For layer FC2, the weight matrix resembles an MP distribution with a smaller Q value than FC1. The Eigenvalue Spectrum shows changes during training, with eigenvectors in the spike being more localized than those in the bulk. The study discusses technical issues with applying Random Matrix Theory (RMT) to neural networks, including the effects of running ensembles versus single runs and fluctuations in eigenvalues. These fluctuations can provide diagnostic information about the learning phase and differentiate between different patterns in the weight matrices. Finite-size effects are also considered in the analysis. The study examines finite-size effects in Random Matrix Theory (RMT) by shuffling weight matrices to estimate eigenvalue spectra. It discusses fitting the bulk edge and variance in different scenarios, noting that Heavy-Tailed distributions pose more complexity due to larger finite-size effects. Estimating the bulk edge \u03bb + and variance \u03c3 2 bulk in Random Matrix Theory can be challenging, especially with Heavy-Tailed distributions. Choosing \u03bb + to reproduce the ESD accurately may result in \"missing mass\" below \u03bb +. Selecting \u03c3 2 bulk as \u03c3 2 bulk = \u03bb + 1 + 1/ \u221a Q \u22122 is recommended. Identifying the appropriate fit can be difficult in a single run, highlighting the complexity of fitting \u03c3 2 bulk accurately. Estimating the bulk edge \u03bb + and variance \u03c3 2 bulk in Random Matrix Theory can be challenging, especially with Heavy-Tailed distributions. Selecting \u03c3 2 bulk as \u03c3 2 bulk = \u03bb + 1 + 1/ \u221a Q \u22122 is recommended. To compare, a baseline variance for any weight matrix W can be defined by shuffling it elementwise and finding the MP \u03c3 2 shuf from the ESD of the shuffled matrix. The empirical bulk variance \u03c3 2 bulk will always be slightly less than the shuffled bulk variance \u03c3 2 shuf. The text discusses how explicit regularization, specifically L2 Weight Norm and Dropout regularization, affects properties of learned DNN models. It suggests shuffling weight matrices multiple times to estimate variance and compares results with and without regularization using plots. The complexity metrics show a greater decrease with explicit regularization, as expected. The text discusses how explicit regularization affects DNN models, showing greater decreases in complexity metrics with regularization. The Eigenvalue Spectrum comparisons with RMT reveal differences in spike values with explicit Dropout regularization. This leads to smaller bulk MP variance parameters and smaller values for \u03bb +. In this section, the text demonstrates that changing a single knob of the learning process can exhibit all five main phases of learning. The batch size, not traditionally considered a regularization parameter, plays a role in the generalization gap phenomenon in DNNs. The batch size in DNN training affects generalization performance, with larger batches contradicting traditional stochastic optimization theory. The mechanism behind this phenomenon is of interest, prompting investigation using different batch sizes in the training algorithm. The MiniAlexNet model was trained with varying batch sizes (b \u2208 {500, 250, 100, 50, 32, 16, 8, 4, 2}). The test accuracy increased for extremely small batch sizes and then decreased as batch size increased. Different patterns in the Eigenvalue Spectrum Density (ESD) were observed with changes in batch size, ranging from Random-like to Bulk-decay. The ESD plot changes with batch size, showing different patterns from Random-like to Bulk-decay. Smaller batch sizes lead to more well-regularized models, reducing the generalization gap in DNN training. Training with smaller batch sizes leads to more well-regularized models, improving results by capturing finer-scale correlations in the data. Large batch sizes fail to capture this fine-scale structure, resulting in less strongly-correlated models. Decreasing the learning rate significantly may not fully compensate for this effect. To improve model regularization, one must decrease the learning rate significantly. Related work includes large-batch learning, Energy Landscape approaches, weight matrices, Heavy-Tailed Universality classes, RMT approaches, statistical physics approaches, and fitting to noisy versus reliable signal. The curr_chunk discusses the failures of VC theory in relation to neural networks. It highlights that VC theory does not apply to NNs due to the assumption of sampling a large hypothesis space, which is not the case for DNNs. The FC layers in neural networks cover only one Universality class of Heavy-Tailed matrices with PL exponent \u00b5 \u2208 [2, 4]. Self-Regularization and Heavy-Tailed Self-Regularization collapse the space of available functions that can be learned, making transfer learning more effective. This phenomenon is unique to NN/DNN learning systems and suggests a need to revisit traditional VC theory. Our results suggest revisiting recent suggestions of Information Bottleneck Theory for DNNs, which explains the two phases of training observed in modern DNNs. The theory implies that DNNs lose some capacity during training, which aligns with our observations. Our approach provides a posteriori guarantees and an unsupervised theory to minimize the risk of \"label leakage.\" The initial fast phase in training corresponds to the initial drop in entropy observed. The initial fast phase in training corresponds to the drop in entropy, while the slower phase involves squeezing out more correlations from the data. Using Gaussian Spin-Glass models may lead to misleading results for realistic systems, as Heavy-Tailed Levy distributions can show different behaviors. Based on the results presented, well-trained DNNs are expected to have a ruggedly convex global energy landscape, unlike a landscape with many degenerate local minima. The slow training phase in DNN optimization is likened to a glassy system with slow Heavy-Tailed behavior, leading to counterintuitive learning phenomena. When training DNNs with larger batch sizes, adjusting the learning rate adaptively can lead to better generalization performance. The results suggest that DNNs operate near a finite size form of a spin-glass phase transition, similar to certain spin glass models being Bayes optimal. These ideas have motivated empirical results and theoretical formulations. Self-Organization in natural and engineered phenomena involves self-regularization during DNN training, as described by Sornette. This can manifest as Bulk+Spikes, Power Laws, or Heavy-Tailed phenomena, representing different universality classes. A Heavy-Tailed ESD could be a signature of such self-organization. The Heavy-Tailed ESD could be a diagnostic for Self-Organization induced by adjusting a single internal control parameter. RMT is particularly appropriate for analyzing weight matrices, with the matrix X being an empirical correlation matrix of the weight layer matrix W l. The limit Q \u2192 \u221e is usually considered in mathematical statistics and traditional VC theory for DNNs. In the context of analyzing weight matrices in DNNs, the regime of MP theory is considered with the limit (M \u2192 \u221e, N \u2192 \u221e) where Q is a fixed constant. This is related to the Statistical Mechanics Theory of Generalization (SMTOG) and the Thermodynamic Limit, providing insights into the energy landscape of DNNs. RMT is valuable for studying the energy landscape of DNNs, especially in cases of overtrained DNNs resembling a spin glass phase. Self-Averaging is crucial for RMT analysis, assuming the ESD is independent of specific matrix realizations. Replicas from statistical physics are used in early RMT, making statements about empirical ESD of large random matrices like X. When applying Random Matrix Theory (RMT) to Deep Neural Networks (DNNs), it is important to have self-averaging behavior for accurate interpretation of the Eigenvalue Distribution (ESD) of weight matrices. Overtraining in DNNs can lead to non-self-averaging behavior, resulting in poor generalization. A well-generalizing DNN can be trained on different subsets of data and still perform well, while an overtrained model describes the training data well but may not generalize effectively. The model class is atypical, describing training data well but test data poorly. Overtraining can lead to a spin glass phase, affecting generalization. Random Matrix Theory may not apply to overtrained DNNs. Practical questions include design principles for good models and the impact of adversarial training on weight matrices. Our approach aims to characterize robust and interpretable models by analyzing the effects of adversarial training on weight matrices. It can also help identify when to stop training by examining empirical properties of the weight matrices. Initial results suggest that our RMT-based theory may be applicable to other types of layers and data, but further exploration is needed in these promising directions."
}