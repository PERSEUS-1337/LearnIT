{
    "title": "BJeGZxrFvS",
    "content": "Saliency methods explain deep net decisions by assigning scores to features/pixels in the input based on gradients. Recent research questions the validity of these methods as they fail sanity checks when tested. We propose a fix called \"competition for pixels\" to improve existing saliency methods by computing saliency maps for all labels in a classification task and using competition to remove less relevant pixels. The method is theoretically justified and empirically demonstrated on popular methods for explaining deep net decisions. The curr_chunk discusses various methods for interpreting deep neural network decisions, such as Gradient Input, Layer-wise Relevance Propagation, Deep-Lift, and DASP Ancona et al. These methods involve analyzing the gradient, back-propagation, and Shapley values to assign importance to pixels in the input data. The current paper evaluates saliency methods by performing sanity checks on them, showing that corrupted parameters and data produce visually indistinguishable results from original ones. The authors propose a simple modification called Competition for pixels to pass these sanity checks. Competition for pixels addresses issues with previous saliency methods by incorporating information from non-chosen labels in the multiclass setting. It relies on the axiom of completeness and modifies pixel scores based on their \"votes\" for different labels. This modification helps pass sanity checks in the evaluation of saliency methods. Section 4 introduces a theory explaining the modification in saliency maps for multi-class settings, emphasizing approximate completeness over completeness. It also presents an empirical finding that saliency methods not designed for completeness still satisfy approximate completeness. Section 5 discusses experiments applying the competition idea to Gradient Input, LRP, and DASP, showing sensible saliency maps and passing sanity checks. The testbeds and methods used are largely borrowed from previous work, with the inclusion of DASP inspired by cooperative game theory. The interplay between tests/evaluations of saliency methods and principled design of new methods is discussed, focusing on controversies surrounding interpretations of \"saliency\" and the failure of axioms to ensure desirable properties in evaluations. The issue of whether changes in saliency maps are solely due to the method or other unresolved factors is highlighted. The ROAR evaluation method by Hooker et al. (2018) assesses the quality of saliency methods by greying out informative pixels and retraining the classifier. Popular methods often perform poorly on this evaluation, with ensemble methods showing better results. ROAR aims to identify every pixel that justifies the output label, acknowledging the redundancy in real-life images. The current paper focuses on the sanity checks evaluation of Adebayo et al. (2018) to uncover serious problems in model performance. Most methods failed the sanity checks, including the model parameter randomization test. The model parameter randomization test compares saliency method outputs on trained and untrained models of the same architecture. Data randomization test compares saliency method outputs on labeled and randomly permuted data sets. If maps are similar in both cases, the saliency method fails the sanity checks. No subsequent paper has designed saliency methods that pass these checks. Many saliency methods fail sanity checks as they pick up irrelevant features in the input data, such as sharp edges. This phenomenon was observed in experiments using AlexNet on MNIST and ImageNet datasets. Adebayo et al. (2018) highlighted that saliency maps can capture incidental features that may not be relevant to the final classification. Saliency maps can capture incidental features that may not be relevant to the final classification. To address this issue, a saliency map can be created by combining information from all labels to filter out or downgrade the importance of incidental features. This approach aims to design a simple classification method that considers information from maps of different labels. To design a simple competition among labels for pixels, completeness property ensures that the sum of pixel scores equals the logit value, allowing for an apples-to-apples comparison of saliency scores from different labels. This property enables viewing the scores as \"votes\" for a label, facilitating a fair comparison. When analyzing saliency maps for non-chosen labels, it allows for a better understanding of the relevance of pixels to the chosen label. Positive and negative pixel scores should be interpreted differently, with positive scores considered as \"votes\" for a label. This competition idea helps in fine-tuning the estimate of a pixel's relevance to the chosen label. The competition idea in saliency maps suggests interpreting positive and negative pixel scores differently, with positive scores supporting the chosen label and negative scores opposing it. The method involves calculating saliency scores for each label and zeroing out pixels based on certain criteria to refine the relevance estimation for the chosen label. The competition in saliency maps involves zeroing out pixels based on certain criteria to refine relevance estimation for the chosen label. Adebayo et al. (2018) used linear models to explain why methods like Gradient Input fail randomization tests, showing that with k-way competition among labels, the saliency map would be expected to become almost blank in randomization tests. Adding competition in saliency maps helps refine relevance estimation by zeroing out pixels based on certain criteria. The final map is very sparse, but understanding why competition yields a reasonable saliency map for properly trained nets is not straightforward. The saliency map is not random and depends on the input, as the completeness property requires the sum of pixel saliencies to be the label logit. A simple model is proposed to explain this dependence and motivate further theory on aggregating information from different labels. The saliency map is a result of a stochastic process in training deep nets, with randomness from dataset choice, initialization, and SGD. A modeling assumption is made that correlates the saliency method with ground truth saliency, where certain pixels are considered salient for a label. This assumption involves a \"noisy signal\" mixed with \"white noise\" and has experimental support. The saliency map is generated by a stochastic process in deep net training, incorporating a mixture of \"noisy signal\" and \"white noise\" with experimental support. The model allows for variations in means and variances across pixels, affecting the distribution of saliency scores. The saliency map is transformed after competition, resulting in a new map based on the winning label. The saliency map is transformed after competition, resulting in a new map based on the winning label. Under plausible conditions, a vector can approximate completeness with a scale factor, as shown through measure concentration. After competition, saliency maps are rescaled to satisfy completeness with a scale factor, showing qualitative results even when not disjoint. The additive error has a smaller impact on larger logit values. Some methods inherently satisfy completeness, while others, like Ancona et al. (2019) and Shrikumar et al. (2017), approximate completeness in practice. The saliency methods discussed in the study by Ancona et al. (2019) and Shrikumar et al. (2017) approximate completeness in practice. Various saliency methods, such as Gradient Input, LRP, and DASP, are enhanced by adding competition to create CGI, CLRP, and CDASP. These methods are tested on different architectures like VGG-19, VGG-16, and CNN models. The study shows that even though these architectures may not be the most powerful, they are suitable for testing saliency methods. The study discusses saliency methods like Gradient Input, LRP, and DASP enhanced with competition to create CGI, CLRP, and CDASP. Testing on architectures like VGG-19 shows modifications do not degrade saliency maps. Experiments aim to pass sanity checks and involve cascading and layerwise randomization. Results are shown in figures with original and modified saliency maps. The study introduces saliency methods like Gradient Input, LRP, and DASP enhanced with competition to create CGI, CLRP, and CDASP. Testing on architectures like VGG-19 shows modifications do not degrade saliency maps. Experiments involve cascading and layerwise randomization, with results shown in figures displaying original and modified saliency maps. The Gradient Input method consistently displays the bird in saliency maps regardless of layer randomization, while the study's method loses bird structure when any layer is randomized. The study compares the sensitivity of CGI, CLRP, and CDASP to parameter randomization with Gradient Input. Results show that CGI is more sensitive to randomization. CLRP benefits LRP maps, maintaining structure after randomization, while CDASP reduces structure in DASP maps. The study compares the sensitivity of different saliency methods to parameter randomization. Results show that CGI is more sensitive to randomization, while CLRP maintains structure in LRP maps after randomization. The study compares the sensitivity of different saliency methods to parameter randomization. Results show that CGI is more sensitive to randomization, while CLRP maintains structure in LRP maps after randomization. The modification removes the underlying structure of the original input image, showing results with greater than 98% accuracy on MNIST data. The Gradient Input method preserves the input structure, while CGI removes it. Competition among labels is a simple modification that combines information from all labels for saliency maps. The evaluation of saliency methods (Gradient Input, LRP, and DASP) passes sanity checks and may improve map quality by zero-ing out irrelevant features. The competition idea for methods satisfying approximate completeness is discussed, with a focus on designing new saliency maps. Future work includes exploring the optimum way to combine information from all labels and incorporating spatial correlation in designing competitions. In designing competitions, future work includes devising less disruptive sanity checks. Integrated gradients compute the path integral of the gradient as the input varies. Layerwise Relevance Propagation decomposes the neural network output into relevance scores. Taylor decomposition is also discussed by Montavon et al. (2018). The DeepLIFT explanation calculates the importance of input by comparing neuron activations to a reference activation. Different methods like Integrated gradients and Layerwise Relevance Propagation decompose neural network output into relevance scores. Kindermans et al. and Shrikumar et al. showed equivalence between LRP rules and Gradient Input. Ancona et al. demonstrated findings for ReLU networks. Ancona et al. (2018) found that -LRP and DeepLIFT (Rescale) methods are equivalent to Gradient Input for ReLU networks. DASP Ancona et al. (2019) approximates Shapley values efficiently. Layerwise Randomization shows CGI results in blank saliency maps, while Gradient Input maintains bird structure regardless of layer randomization. The saliency map for layer-wise randomization of learned weights shows that using CGI results in blank maps, while Gradient Input maintains the structure of the bird regardless of layer randomization."
}