{
    "title": "H1eerhIpLV",
    "content": "In this paper, a case study on reproducing the results of the AlphaZero algorithm is presented. The Minigo system, a reproduction of AlphaZero using Google Cloud Platform infrastructure and TPUs, can play evenly against strong Go AIs after ten days of training. The challenges of scaling a reinforcement learning system and monitoring hyperparameter configurations are discussed. In March 2016, Google DeepMind's AlphaGo BID0 defeated world champion Lee Sedol using deep neural networks and Monte Carlo Tree Search. AlphaGo Zero BID1, published in October 2017, could train itself without human data, requiring vast amounts of compute. AlphaZero BID2 refined the AlphaGoZero pipeline by removing the gating mechanism. In a refinement of the AlphaGoZero pipeline, the gating mechanism was removed. AlphaGo Zero automated and streamlined the bootstrapping process, surpassing human levels of play. The creation of Minigo involved rebuilding infrastructure for selfplay workers and monitoring systems. The creation of Minigo involved rebuilding infrastructure for selfplay workers and monitoring systems. Monitoring systems are crucial in the discovery process, as seen in the AlphaZero architecture. Minigo uses a reinforcement learning loop similar to AlphaZero, generating games through selfplay and using them as training data. Minigo utilizes a variant of the UCT algorithm from the AlphaGo Zero paper to select game tree variations. The neural network predicts moves and game outcomes, integrating them into the search tree. Moves are chosen based on visit counts or weighted samples. The final move considers opponent responses and game continuations, with visit counts used for policy network training. Game data updates the neural network's weights for training. The neural network in Minigo is updated using stochastic gradient descent, minimizing policy and value error. Selfplay and training compute ratio is determined by the number of readouts invested into each move. Minigo uses 800 Cloud TPUs for selfplay and 1 Cloud TPU for training, orchestrated using Google Kubernetes Engine. Selfplay workers write training data to Cloud BigTable, and the trainer trains on samples from a sliding window of training data. Network weights are published to Google Cloud Storage, and selfplay workers periodically download updated weights. The neural network in Minigo is updated using stochastic gradient descent, minimizing policy and value error. Monitoring includes calibration jobs for game statistics, StackDriver for cluster health, and TensorBoard for training job statistics. Key metrics tracked include value head outputs, winrates, game lengths, MCTS search depth, time per inference, and more. Minigo's core components have evolved from inception to their current state, with a focus on improving the throughput of the selfplay cluster. The selfplay process is the most resource-intensive part of the pipeline, requiring 100-1000 times more compute than training. A frontend was created for easy access to metrics like model ratings, opening patterns, game responses, and branch percentages. The project initially started with a python-based prototype. The selfplay cluster for Minigo was prototyped with a python-based engine on a 9x9 board. A small cluster of cores could keep up with a single P100 GPU for training. Google Kubernetes Engine was used to manage containers for playing games and generating training data. Minigo reached a strong amateur level but lacked a professional human 9x9 dataset for further improvement. After prototyping Minigo on a 9x9 board, the team scaled up to a 19x19 version using 2000 NVIDIA K80 preemptible GPUs on Google Kubernetes Engine. They implemented a vectorized MCTS to reduce Python overhead in tree search, facing challenges with the computational complexity of the full network. The computational complexity of the full network scales to a large polynomial complexity, with various factors contributing to the increase. Batching techniques were utilized to scale up the throughput of selfplay workers, with running multiple games in parallel being the easiest technique, despite the drawback of increased latency in game completion. Virtual losses in MCTS BID7 is a technique used to select multiple variations from a single MCTS game tree. MCTS algorithm determines the best leaf to explore using probability priors and simulations from that node. By marking a chosen leaf as a loss and rerunning selection, additional leaves can be chosen. This allows for more efficient exploration of the game tree. The number of simultaneous pending virtual losses in MCTS is a tuneable parameter that affects throughput and selfplay quality. The team ran the Minigo pipeline on a K80 cluster multiple times, but wanted to test Cloud TPUs for improved performance. They estimated a higher overhead with TPUs and rewrote the selfplay engine in C++ to integrate MCTS closely with Cloud TPU inference. With 800 TPUs, they aimed for a full run in about a week with larger batch sizes for maximal throughput. To increase throughput, the team played multiple games simultaneously on Cloud TPUs, with each worker playing 32 games in parallel. They switched models whenever a new one was published, even in the middle of a game, to maintain balance in the pipeline. This approach did not seem to affect performance significantly. One operational risk of playing multiple games simultaneously on Cloud TPUs is that each resign threshold is calibrated to a specific model. If a new model skews value output significantly, many games near the resign threshold may all resign at once, causing a thundering herd problem and potentially overloading Cloud BigTable with heavy write volume. To address this, a small number of TPUs are set aside for playing calibration matches with a lower parallel game configuration to minimize game completion latency. A ringbuffer is used to store recent calibration results and compute the threshold. With changes made to the calibration process and buffer size, over 100 petaops of compute were efficiently utilized in Google's cloud using Kubernetes. The cluster achieved 1.3-1.5ms per inference, playing 1.8M games daily with a 20 block network. Minigo's training involves continual dataset evolution, requiring TensorFlow to restart training for data shifts. TF Agents BID8 addresses this issue by maintaining an in-memory circular buffer. The trainer system was designed to pause if the selfplay cluster slowed down to prevent overfitting. Training code was rewritten to utilize TensorFlow Datasets and Estimator for Google Cloud TPUs, improving efficiency. Instead of Python iteration, TensorFlow's parallel, buffered I/O features were used to feed data directly to Cloud TPUs from GCS and Cloud BigTable. Shuffling was a crucial part of the pipeline, with AlphaGo Zero requiring uniform sampling from a large dataset. Uniform sampling from a dataset can be challenging due to the need to shard the data appropriately. In practice, an approximation to uniform shuffling is required for Minigo's training data. Different approximations were used at different points, each with drawbacks. Improving shuffling dramatically improved Minigo's strength, similar to findings in the AlphaZero paper. Proper shuffling is crucial due to Go's gameplay pattern of placing stones sequentially, leading to persistent subpatterns in every position. Improper shuffling led to Minigo overfitting by memorizing positions, causing overconfidence in games. Performance dropped in predicting human professional games. Validating 5% of selfplay games showed network's near-random performance on untrained games. To shuffle effectively in Minigo, it was crucial to scramble each source of correlation in the data, including intra-game and generational correlation. The initial shuffler implementation involved reading through thousands of tiny files from selfplay workers, sampling positions, and training on chunks in shuffled order. However, this method failed to meet the shuffling criteria, leading to overfitting and decreased performance in predicting human professional games. To address issues with shuffling criteria, a machine with 64GB memory was used to perform a perfect shuffle on positions sampled from recent games. Additionally, applying one of 8 symmetries to each position further boosted performance. The shuffler implementation was later replaced with Cloud TPUs, leading to a bottleneck with reading TFExample files from GCS. This was resolved by implementing a Python multiprocessing pool and transitioning to a Cloud BigTable solution for improved efficiency. In our Cloud BigTable pipeline, TFExamples were stored in sequential rows based on game number and move number for random access. Cloud Bigtable offers probabilistic sampling for bandwidth savings, with move counts per game stored separately for accurate sampling probability calculation. The move count per game was stored in a separate keyspace in the table for accurate sampling probability calculation. A final in-memory shuffle of sampled row keys was executed before requesting the row keys from CBT, reducing training time by 50%. The calibration process for Minigo involved determining a resignation threshold to terminate selfplay, as described in the AlphaGo Zero paper. The AlphaZero paper specified a resignation threshold based on completion of 10% of games and less than 5% incorrectly resigned. Incorrectly calibrated thresholds could destabilize training by creating a pessimistic loop, causing divergence. Tracking winrate, game lengths, and value network output helped detect this issue. To prevent a pessimistic loop scenario in training, the resignation threshold was adjusted to a 3% false positive rate, leading to longer games. The latency in computing the threshold was reduced by automating the process, as manual execution was infrequent, resulting in a conservative threshold with a false positive rate below 5%. With the Cloud BigTable rewrite, the resignation threshold was computed directly, leading to a more lightweight configuration mechanism. The lowered latency improved the early stages of the pipeline, shifting the training data distribution towards early-and mid-game examples. Testing the bot on an online Go server revealed basic mistakes in end-game positions due to lack of training data. Additionally, a BigQuery dataset of moves played was set up. To address end-game deficiencies, the team trained on resignation-disabled games and conducted evaluation games between different models using a small cluster of GPUs. They used a modified Bradley-Terry system to compute a single parameter rating, converted to Elo numbers for display. This helped measure the relative strength of their models for various purposes. The team used a modified evaluation methodology to find the strongest models in a run, comparing rating-over-time graphs to previous papers. They played models against a diverse array of opponents to avoid transitive cycles, reducing uncertainty in each model's performance. The team used a modified evaluation methodology to find the strongest models in a run by playing cross-run evaluation games between the best models at regular intervals. This helped assess the impact of hyperparameter changes and pipeline bug fixes on overall strength. The evaluation cluster was also used for one-off tests such as tuning hyperparameters and testing against networks trained via different approaches. In reproducing AlphaGo Zero, the team monitored various metrics to ensure bug-free implementation. They found Figure 3 from the original paper to be a useful guidepost and wished for more training metrics for comparison. Healthy metrics were necessary but not sufficient for success, with the evaluation cluster being the true indicator of success."
}