{
    "title": "S1xDcSR6W",
    "content": "Neural embeddings are widely used in Natural Language Processing (NLP) to provide compact representations that capture word similarity and achieve high performance in various linguistic tasks. Research has also explored their applications in graph-structured data, where embeddings of vertices can enhance vertex similarity and improve tasks like edge prediction and vertex labeling. These embeddings are typically learned in high-dimensional Euclidean spaces for both NLP and graph-based tasks. Recent work has shown that embedding complex networks in hyperbolic space outperforms Euclidean embeddings for vertex classification tasks. Hyperbolic embeddings provide benefits such as encapsulating similarity, compactness, and improved performance in machine learning models, particularly for graph-structured data. Neural embedding models, a type of embedding learned through backpropagation, have shown improved performance in various tasks without labels. Previous work assumed the embedding space is Euclidean, but recent research suggests hyperbolic embeddings may outperform Euclidean embeddings for vertex classification tasks. Recent research suggests that complex networks, like the Internet or academic citations, can be better described using non-Euclidean geometries such as hyperbolic geometry. A neural embedding approach based on the Skipgram architecture is used to find hyperbolic embeddings for these networks, which exhibit a hierarchical structure and can benefit from hyperbolic geometry's continuous analogue of tree-like graphs. Recent research suggests that complex networks can be better described using non-Euclidean geometries like hyperbolic space. In hyperbolic space, isometric embeddings exhibit power-law degree distributions with high-degree hub vertices. Hub-and-spoke graphs can be embedded in hyperbolic space, preserving specific geometric properties that are impossible to reproduce in Euclidean space. In hyperbolic space, complex networks can be embedded with power-law degree distributions and high-degree hub vertices. The Skipgram architecture is a shallow neural network used for word embedding, trained on word pairs generated by a sliding window over a word sequence. In this paper, the concept of neural embeddings in hyperbolic space is introduced. Backpropagation in hyperbolic space is formulated, showing improved performance in vertex classification tasks across multiple networks. This approach builds on the Skipgram architecture used for word embedding in complex networks. The paper introduces neural embeddings in hyperbolic space, showing improved performance in vertex classification tasks across networks. Hyperbolic geometry allows for an infinite number of parallel lines passing through a single point, illustrated in FIG1. Hyperbolic space is one of three types of isotropic space defined by curvature. Hyperbolic space, one of three types of isotropic space defined by curvature, is larger than Euclidean space and well-suited for modeling complex networks with hierarchical structures. Complex network features like power-law degree distributions and strong clustering naturally emerge in hyperbolic space. The hyperbolic area of a circle or volume of a sphere grows exponentially with its radius, allowing for effective data representations in low-dimensional hyperbolic spaces. Complex networks often have vertices with high degrees, approximating hubs. The distance between spokes in hyperbolic space tends to the circumference as the number of vertices increases. In hyperbolic space, the distance between spokes tends to the circumference as the number of spokes increases. The Poincar\u00e9 disk model is used to represent hyperbolic space as a unit disk, allowing for different representations that conserve some geometric properties but distort others. The Poincar\u00e9 disk model represents hyperbolic space as a unit disk, where distances grow exponentially towards the edge. Straight lines in hyperbolic space intersect the boundary of the disk orthogonally, appearing as diameters or arcs of a circle. Geodesics in hyperbolic space appear curved in the Poincar\u00e9 disk due to shorter distances near the center. In the Poincar\u00e9 disk model, distances grow exponentially towards the edge, with vertices near the center having more 'near' neighbors. The distance metric is a function of the radius, and using polar coordinates simplifies the mathematical description. Points in the disk are represented as x = (r e , \u03b8), with r e \u2208 [0, 1) and \u03b8 \u2208 [0, 2\u03c0). The hyperbolic distance from the origin, r h, is given by DISPLAYFORM0, and the circumference of a circle of hyperbolic radius R is C = 2\u03c0 sinh R. In hyperbolic space, the cosine similarity weighted by the hyperbolic distance from the origin is used to quantify similarity between points in the embedding. This function helps retain key properties of hyperbolic space for learning embeddings that perform well on downstream tasks. In hyperbolic space, embeddings are learned by replacing Euclidean vector spaces with Poincar\u00e9 disks, improving on Euclidean geometry for graph embedding. In hyperbolic space, embeddings are learned by optimizing an objective function to predict context vertices using Poincar\u00e9 disks. The softmax function is used for conditional predictive distribution, and negative sampling is employed for efficiency. Optimization is done in polar native hyperbolic coordinates. In polar native hyperbolic coordinates, optimization is conducted without the need to constrain the optimizer for points inside the disk. Update equations in polar coordinates are simple modifications of Euclidean updates, avoiding the evaluation of the metric tensor for each data point. The negative log likelihood using negative sampling involves vector representations of input and context vertices, a set of samples drawn from the noise distribution, and the sigmoid function. The negative log-likelihood in polar hyperbolic coordinates involves updating vectors using negative samples drawn from a noise distribution. The derivatives are calculated with respect to the components of vectors in W, and the update equations map the vectors back to Euclidean coordinates on the Poincar\u00e9 disk. The asymptotic runtimes of the update equations in hyperbolic space are the same as Euclidean Skipgram. Hyperbolic embeddings show superior features compared to Euclidean embeddings. Experiments on public benchmark networks compare the performance of both embeddings on a vertex classification task. TensorFlow implementation and datasets are provided for replication. Visual comparisons highlight the usefulness of hyperbolic embeddings. Hyperbolic embeddings are compared with Euclidean plots using a 2D Poincar\u00e9 model. The hyperbolic embedding of a complete 4-ary tree shows the root vertex close to the origin with leaves clustered near their parents, representing the tree's branching factor. In contrast, the Euclidean embedding fails to capture the tree structure effectively. The hyperbolic embedding of the 34-vertex karate network in FIG6 clearly separates two factions, with a community of junior instructors (vertices 5, 6, 7, 11, 17) connected only through the instructor (vertex 1). The results of experiments with HyBed and 2D Deepwalk embeddings are shown in FIG8, with vertex colours indicating different values of the vertex labels. The legend in FIG8 applies to line graphs showing macro F1 scores against labeled data percentage for logistic regression classifier training with embeddings as features. HyBed embeddings cluster similar classes for linear separability, unlike Euclidean embeddings."
}