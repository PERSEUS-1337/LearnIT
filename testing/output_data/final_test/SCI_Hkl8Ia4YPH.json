{
    "title": "Hkl8Ia4YPH",
    "content": "In this paper, the authors propose a hierarchical generation approach for summary-to-article generation. They first generate a sketch based on the summary and then complete the article by enriching the sketch. To address discrepancies between training and inference sketches, they introduce an end-to-end joint training framework using multi-agent reinforcement learning. Their evaluation method involves using text summarization corpora in reverse and comparing the generated article with the original input summary. The proposed hierarchical generation approach can produce coherent and relevant articles based on input summaries, showing significant improvements over traditional seq2seq models. Unlike text generation tasks like machine translation and text summarization, long text generation is less explored. Existing studies either generate long text unconditionally or based on a single sentence prompt, lacking fine-grained content control. In this paper, the focus is on long text generation with fine-grained content control, specifically summary-to-article generation. This task involves generating a coherent and relevant long article based on a short summary of 3 to 5 sentences. The challenge lies in modeling complex dependencies between the summary and the article, making standard seq2seq models prone to degeneration. Inspired by previous work on text generation, a hierarchical summary-to-article generation approach is proposed to address the challenge of degeneration in standard seq2seq models. This approach decomposes the task into two subtasks: summary-to-sketch generation and sketch-to-article generation. By using a hierarchical generation approach, the length of the source text is not extended too much, improving coherence and relevance in the output article. To address the issue of degeneration in standard seq2seq models, a hierarchical generation model is proposed. It involves generating a sketch based on the summary before completing the article, bridging the gap between training and inference. This model utilizes a gated model fusion mechanism to establish communication between the summary-to-article and sketch-to-article generation models, enhancing coherence and relevance in the generated text. The study introduces a novel hierarchical generation approach for text summarization using multi-agent reinforcement learning. It proposes a gated model fusion mechanism and a denoising seq2seq pretraining objective to improve coherence and relevance in generated articles. Experiments show better results compared to traditional seq2seq models on CNN/DM and BIGPATENT datasets. The study introduces a novel hierarchical generation approach for text summarization using denoising seq2seq pretraining objective. A new evaluation method for summary-to-article generation is proposed, showing better correlation with human evaluation than traditional metrics like perplexity. The task aims to generate long articles based on a short summary, with finer-grained control compared to previous long text generation approaches. The task involves generating long articles from short summaries with improved coherence and finer control, addressing the challenge of information gaps between input and output. The proposed hierarchical summary-to-article generation model offers practical applications like news article generation from highlights or patent claim creation from descriptions. The hierarchical summary-to-article generation model proposes a two-step approach: summary-to-sketch generation and sketch-to-article generation. This method aims to address the issue of degeneration in text generation models by using a sketch as an intermediate step between the summary and the final article. This approach enhances coherence and relevance in the generated article. The proposed hierarchical summary-to-article generation model includes two seq2seq components: the summary-to-sketch generation model (G1) and the sketch-to-article generation model (G2). A skip-connection (G3) is added to ensure clean and good sketch generation for article generation. This allows the model to adaptively learn from both the summary and the hierarchical outputs of G2. The proposed model includes a hierarchical generation approach with a fusion mechanism to adaptively learn from both the summary and sketch information. The fusion mechanism involves concatenating decoder hidden states to learn gates and combining gated hidden layers to generate output tokens. The core of the approach focuses on constructing appropriate sketches for training. The proposed approach involves a heuristic method to extract important sentences from the article for constructing sketches for training. The relevance score of each sentence is computed based on its cosine embedding similarity with sentences in the summary. This training strategy includes MARL for joint training and denoising seq2seq pre-training to improve model robustness. The proposed approach involves extracting important sentences from the article to create sketches for training. The sentences are selected based on relevance scores calculated using cosine embedding similarity with sentences in the summary. This method ensures consistency in sequence length between input and output components of the model. Training the two generation models separately with Maximum Likelihood Estimation (MLE) presents a challenge due to discrepancies between training and inference sketches. The proposed approach involves training a sketch-to-article generation model using extracted sketches from the summary. To address discrepancies between training and inference sketches, an end-to-end joint training framework with multi-agent reinforcement learning and denoising pretraining is proposed. This approach aims to improve the model's adaptation to noisy sketches generated during inference. In an end-to-end joint training framework with reinforcement learning, two agents cooperate to generate a sketch from a summary and then produce an article based on that sketch. Agent G1 generates the sketch, while Agent G2 maximizes the likelihood of the article given the sketch. Training for Agent G1 involves using REINFORCE with a reward based on Agent G2's performance. The learning objective for Agent G1 includes entropy regularization to encourage diversity in decoding. The training objective for Agent G1 includes entropy regularization to encourage exploration and develop helpful communication policies for Agent G2. The summary-to-article generation model G3 is pretrained with MLE and fixed during reinforcement learning. Pretraining is done for the summary-to-sketch generation model G1 and the summary-to-article generation model G3 with MLE to provide a good initialization for reinforcement learning. The sketch-to-article generation model G2 employs denoising seq2seq pretraining to improve robustness against input noise. To enhance model robustness against input noise, sketches are corrupted with word-level and sentence-level perturbations during training. This helps prevent direct sentence copying in sketch-to-article generation. During inference, a summary is used to generate a sketch, which is then used to generate the final article. Top-p sampling is employed for sketch generation. In text generation, top-p sampling is used to avoid repetition and encourage diversity. Traditional metrics like BLEU and ROUGE are not effective for evaluating open-domain text generation systems. Evaluating the relevance between generated articles and summaries is crucial for fine-grained content control. In text generation, evaluating the relevance between generated articles and summaries is crucial for fine-grained content control. A novel evaluation metric called ROUGE-rec is introduced to assess how well the original summary can be reconstructed from the generated article. ROUGE-rec measures the similarity between the reconstructed summary and the original summary using the ROUGE-L score. This metric helps determine if the generated article effectively focuses on the main points of the summary. In text generation, evaluating the relevance between generated articles and summaries is crucial. A new metric, ROUGE-rec, assesses how well the original summary can be reconstructed from the article. Experiments are conducted on summary-to-article generation using datasets like CNN / Daily Mail and BIGPATENT. Automated metrics like PPL(gpt-2) are employed to evaluate model performance. The model has 340M parameters to measure long-range dependency and assess the fluency and coherence of generated articles. A new metric, ROUGE-rec, evaluates how well the generated article expands the input summary. The model architecture includes a seq2seq convolutional model with specific encoder and decoder block configurations. The vocabulary sizes for CNN/Daily Mail and BIGPATENT datasets are 142,971 and 195,401 respectively. The model has 340M parameters and uses a seq2seq convolutional model to generate articles. The vocabulary sizes for CNN/Daily Mail and BIGPATENT datasets are 142,971 and 195,401 respectively. The model is trained on 4 GPUs with specific hyperparameters and pre-training steps. The average length of generated articles is 712.3 and 3278.3 in the two datasets. The hierarchical model outperforms conventional seq2seq models in generating more fluent and coherent articles with better perplexity and ROUGE-rec scores. The gated model fusion mechanism in the hierarchical model enhances performance by allowing adaptive focus on input summary information. The hierarchical model improves article generation by generating a helpful sketch that reduces perplexity compared to the seq2seq baseline. This allows the model to learn content more easily and outperform traditional models. The study conducted human evaluation to compare the generated articles with the seq2seq baseline. Results showed that the approach consistently outperformed the baseline in pairing accuracy and human preference. The fusion mechanism's effectiveness was also confirmed. Additionally, the automated evaluation metrics correlated well with human evaluation results. Automated evaluation metrics like ROUGE-rec and PPL(GPT-2) correlate well with human evaluation, unlike conventional metrics such as BLEU and ROUGE. Human annotators prefer articles with higher ROUGE-rec, indicating better content control. The proposed ROUGE-rec may be crucial in evaluating long text generation models. The study conducted an ablation study on the CNN/DM dataset to investigate the effects of training strategies and sketch length on article generation models. The proposed training strategies reduced the gap between perplexity of articles based on extracted sketches and generated sketches, improving fluency and coherence. This confirms the strategies' effectiveness in bridging the gap between training and inference stages and enhancing model robustness. The study explored the impact of sketch length on training, finding that both shorter and longer sketches led to sub-optimal performance. The geometric mean of summary length and article length was suggested as a good default choice to maintain model robustness. The approach was inspired by decomposing text generation into multiple steps, with qualitative comparisons provided in the appendix. The study discusses different approaches to text generation, highlighting the limitations of existing neural models in generating long texts. The researchers propose constructing sketches of intermediate length to provide more information for generating final output and reduce the difficulty of long text generation. Existing studies either unconditionally sample from pretrained language models like GPT-2 or generate long text based on a single sentence prompt. The researchers propose a novel hierarchical summary-to-article generation approach that first drafts a sketch outlining the article based on the summary. They use multi-agent reinforcement learning for training and show improved performance over conventional seq2seq models for generating coherent and relevant articles from summaries. The researchers aim to improve summary-to-article generation models by investigating the inclusion of additional information in generated articles. They plan to address ethical concerns related to generating fabricated content. Previous studies have focused on pretraining conditional language models to generate text based on specific topics or domains, but they lack fine-grained control over the content generated. Decomposing text generation into steps has been explored in statistical and neural text generation models. Strategies involve transferring sentence compression, text summarization, and keyword extraction models to guide long text generation. Previous approaches generate noisy intermediary training data and construct intermediary output of similar length to the final output. Previous approaches in text generation have focused on decomposing the process into steps, such as generating a short \"plan\" before the final output. However, these methods do not effectively address the challenge of generating long texts with significant length differences between input and output. The hierarchical models proposed by various researchers are limited to generating short sentences or stories within a certain word limit. In contrast, a recent approach by Fan et al. (2019) suggests generating an action plan first, followed by an anonymized story and entity filling. This decomposition method extends the sequence length in the initial step, distinguishing it from other approaches. The curr_chunk discusses the use of denoising pretraining for text generation models to improve data sparsity issues. Different corruption methods are employed to train the sketch-to-article generation model, focusing on directly outputting the target sequence. The curr_chunk explores training multiple agents to communicate and cooperate for a common goal, similar to previous works in multi-agent reinforcement learning. Model fusion techniques have been studied, such as combining language models with seq2seq models to improve performance. The curr_chunk discusses training a seq2seq model with a fixed language model and learning a gate to filter information. It also introduces the fusion of two seq2seq models with different inputs to combine sources of information. The text includes samples generated by both baseline convolutional seq2seq architecture and the proposed model. Additionally, it mentions a study on female-named hurricanes causing more deaths and participants perceiving male storms as deadlier. Female-named hurricanes cause significantly more deaths according to a study presented at the University of California. Researchers analyzed the relationship between hurricane names and fatalities, based on census data and the number of identified hurricanes. They also considered factors like temperature and age in their analysis. Female-named hurricanes cause significantly more deaths, as researchers analyzed death rates from U.S. hurricanes over six decades. Participants perceived male storms as deadlier than female storms. Critics note earlier deadlier hurricanes were only named female. A study in the Proceedings of the National Academy of Sciences suggests that the planet's dead bodies may be over a year old and less likely to be found in the area. The study attributes the mass of blood from a rock star's body to high levels of mass injury. The study, to be published in the journal Nature, reveals high rates of an unknown disease, surprising researchers. Dr. Paul <unk>, a professor at the University of Texas, noted specific differences in age among patients. The findings have little impact on understanding the disease's prevalence globally. The study, to be published in the journal Nature, reveals high rates of an unknown disease, surprising researchers. Dr. Paul <unk>, a professor at the University of Texas, noted specific differences in age among patients. The results were not surprising to them. The study, to be published in the journal Nature, reveals high rates of an unknown disease, surprising researchers. Dr. Paul <unk>, a professor at the University of Texas, noted specific differences in age among patients. The results were not surprising to them. Figure 5: Sample generated by baseline model on the CNN/Daily Mail dataset. The famous rapper Iggy Azalea is dominating this year's American music scene. The famous rapper Iggy Azalea is dominating this year's American Music Awards nominees with six nominations, including artist of the year and best rapper. The competition is tough with 10 contenders in the category. Iggy Azalea and Beyonce are among the well-known artists vying for the title. The event is highly anticipated and meaningful for the rapper. The rapper Iggy Azalea is highly anticipated to win at the American Music Awards. She is competing against well-known artists like Beyonce. Winning the award would be a significant achievement for her. Iggy Azalea expressed her pride in her performance at the bar and her journey to success. She mentioned feeling scared during a previous incident but emphasized her happiness with her dance routine. She also highlighted her hard work and dedication to her craft. The text chunk discusses a series of resistances labeled as 14, 10, 11, and 12. The text chunk describes how resistances 11, 12, 14, and 16 are interconnected, leading to the creation of a switch for control unit A. The text chunk discusses different perturbations at both sentence and word levels, such as shuffling sentences, dropping sentences, and replacing words. These perturbations are explained for their potential effects on generated text. The text explains the role of different perturbations in improving model robustness during summary-to-sketch generation. It includes truncating news articles and patent claims to specific token limits, tokenizing training data, and using human annotators for evaluation. Randomly sampling 100 summaries from the test set, articles are generated using different models and distributed to human annotators. The annotators perform a triple pairing task to match articles with prompts and a human preference task to choose between articles generated by different models. Each model receives 300/200 results in the triple pairing task and the preference test. In the study, each model undergoes a preference test 2 times, with 300/200 results in the triple pairing task and human preference task. Patent claims are shortened to 200 and 400 words for easier evaluation."
}