{
    "title": "S1nQvfgA-",
    "content": "The proposed algorithm for training generative adversarial networks allows for learning latent codes for identities and observations simultaneously. It enables the generation of diverse images of the same subject by fixing the identity code and traversing the manifold of subjects while maintaining contingent aspects by fixing the observation code. The algorithm utilizes a pairwise training scheme with a Siamese discriminator to ensure photorealistic and distinct image generation. Our algorithm uses Siamese discriminators for pairwise training to generate convincing, identity-matched photographs. It enables a multistage generative process for diverse product catalog images, leveraging disentangled representations. The dataset contains labeled identities with limited observations, allowing for varied depictions and new product imaginings. Generative adversarial networks (GANs) learn mappings from latent codes to natural data points through adversarial training. While GANs can generate high-fidelity images, they do not explicitly disentangle latent factors. Semantically Decomposed GANs (SD-GANs) proposed in this paper encourage a specified portion of the latent space to correspond to known sources of variation. SD-GANs decompose the latent code into portions corresponding to identity and other observation aspects. The training scheme involves pairs of images with a common identity code. The generator must produce diverse, photorealistic images with the same identity when fixed. The discriminator is modified to determine if a pair of samples match. SD-GANs can generate contrasting images of the same subject by learning that certain properties can vary across observations but not identity. They are trained on datasets of face photographs and product images to produce stylistically-contrasting, identity-matched image pairs. SD-GANs can generate diverse images of the same subject, matching identities accurately. They leverage neural networks to learn generative models and can imagine new identities, unlike conditional GANs. Training involves a minimax game between the generative model G and discriminative model D. SD-GANs use discriminative and generative models in a minimax game to generate diverse images accurately matching identities. Training involves maximizing log likelihood by updating discriminator and generator weights through stochastic gradient ascent and descent. Minibatches are sampled from real and generated image distributions, with labels assigned accordingly. The generator learns to create new identities through neural networks. Energy-based GANs (EBGANs) update generator weights using stochastic gradient descent. The discriminator acts as an energy function, consisting of an autoencoder. Boundary Equilibrium GANs (BEGANs) optimize the Wasserstein distance between autoencoder loss distributions, introducing a stabilizing method with a new hyperparameter \u03b3. The BEGAN model introduces a new hyperparameter \u03b3 to maintain a desired ratio between reconstruction errors. It adapts DCGAN and BEGAN algorithms to the SD-GAN training scheme, aiming to decompose data variation into two parts: one due to identity and the other due to other factors. The goal is to decompose the latent space Z into ZI and ZO for convenient operations. The SD-GAN method decomposes the latent space Z into subspaces ZI and ZO, allowing for the disentanglement of variations in data. This is achieved through a pairwise training scheme where real data samples and generated images are paired based on identity, enabling the model to learn a latent space decomposition. The SD-GAN method decomposes the latent space Z into subspaces ZI and ZO, enabling the model to learn a latent space decomposition through a pairwise training scheme. The discriminator in SD-GANs can reject pairs based on photorealism or subject identity, utilizing novel architectures to adapt DCGAN and BEGAN for this purpose. The Siamese setup BID3 BID5 is chosen for the symmetrical image problem, sharing weights between encoders. DCGAN adaptation involves stacking feature maps D e (x 1 ) and D e (x 2 ) with an additional strided convolution for information aggregation. In BEGAN, the discriminator is an autoencoder with a complex architecture involving concatenation of image representations and a bottleneck matching latent codes. SD-BEGAN uses a shared intermediate layer to separate matched and unmatched pairs. The method is validated using two datasets: MS-Celeb-1M for celebrity face images and a dataset of shoe images from Amazon. The celebrity face dataset offers a rich test bed, while the shoe dataset is useful for verifying the approach. In the MS-Celeb-1M dataset, 12,500 celebrities were randomly selected with 8 images each resized to 64x64 pixels. The dataset was split into training, validation, and test subsets. Duplicate images were removed by hashing, but label noise was not addressed. Pixel values were scaled to [-1, 1] with no additional preprocessing. In the shoes dataset, product images primarily differ in orientation and distance, making it a suitable domain for SD-GAN training. The Amazon dataset contains around 3,000 unique products in the \"Shoe\" category with an average of 6.2 photos per product. SD-DCGANs were trained on the dataset for 500,000 iterations using batches of 16 identity-matched pairs. The Adam optimizer with specific hyperparameters was used for optimization. Latent vectors were sampled from a uniform distribution, and the algorithm can be applied with k-wise partitioning of latent codes. Our algorithm can be applied with k-wise training, exploring the effects of using k > 2. We also experiment with an SD-DCGAN sampling k = 4 instances from the latent space. Training an SD-BEGAN on both datasets increases complexity and training time. Results from the SD-DCGAN k = 4 model are compelling, but the SD-BEGAN k = 4 variant resulted in early mode collapse and was excluded from evaluation. The evaluation compares SD-GAN to AC-GAN, highlighting differences in identity coding and image generation. AC-GAN's discriminator predicts identity, limiting new identity generation. Evaluation methods for generative models vary due to poor correlation in sample quality measures. The evaluation of SD-GANs and baselines includes identity coherence assessment using a face verification model and human judgments. FaceNet, a deep neural network, achieves high accuracy in face verification tasks. It produces embeddings for normalized images to depict the same person. FaceNet is trained to learn embeddings that minimize the distance between matched faces and maximize it for mismatched pairs. The similarity between two faces x1 and x2 is measured using the embedding space. A threshold \u03c4v is used to label images as a match. FaceNet's performance is compared on MS-Celeb-1M test set pairs against samples from SD-GAN models and AC-DCGAN baseline. Images are preprocessed and resized for consistency. In the evaluation, 10,000 pairs from MS-Celeb-1M are prepared, half identity-matched and half unmatched. Generative models generate 5,000 pairs each with z. An intra-identity sample diversity metric is proposed to ensure diverse identity-matched images. The multi-scale structural similarity metric is used to measure image similarity. Results are reported in terms of AUC, accuracy, and FAR of FaceNet on real and generated data, along with diversity statistics. FaceNet verifies pairs based on learned embeddings. The proposed diversity statistics show that FaceNet achieves 87% accuracy on real data and 86% on data from the SD-BEGAN model. The SD-GANs may produce less semantically diverse images compared to AC-DCGAN. The memory footprint of G and D for all methods is also reported, with the number of parameters growing linearly with the number of identities in the training data. The complexity of SD-GAN is constant in the number of identities. Human validation experiments using Mechanical Turk show that identity-matched SD-GAN samples are verified by FaceNet. Annotators determine if pairs from MS-Celeb-1M depict the \"same person\" or \"different people\" through majority vote. The quality of Mechanical Turk ensembles was assessed by manually judging 200 pairs from MS-Celeb-1M. Human annotators on Mechanical Turk answered \"same person\" less frequently than FaceNet. Annotators achieve higher accuracy on pairs from AC-DCGAN than pairs from SD-BEGAN, with AC-DCGAN pairs showing higher \"same person\" responses compared to real data. Samples from SD-DCGAN and SD-BEGAN are shown in FIG1, highlighting active research areas in style transfer and novel view synthesis. Style transfer and novel view synthesis are active research areas. Early attempts to disentangle style and content manifolds used factored tensor representations, applying their results to face image synthesis. Recent work focuses on learning hierarchical feature representations using deep convolutional neural networks to separate identity and pose manifolds for faces. GANs have been used to generate high-quality images, with Conditional GANs extending GANs to generate class-conditional data. Conditional GANs, introduced by Mirza & Osindero (2014), have been extended to generate class-conditional data. Odena et al. (2017) proposed auxiliary classifier GANs, combining cGANs with a semi-supervised discriminator. These approaches address a variety of image-to-image translation and style transfer tasks by ingesting text and full-resolution images as conditioning information. Additionally, BID4 devised an information-theoretic extension to GANs to maximize mutual information between latent variables and generated data, while BID0 proposed conditional GANs for synthesizing artificially-aged faces. SD-GANs can model contingent factors implicitly without supervision, unlike other methods that require explicit conditioning. Mathieu et al. combine GANs with a traditional reconstruction loss to disentangle identity, using an encoder-decoder generator with a variational bound on the encoder embedding. Their experiments focus on small grayscale face images, while our work offers a simpler approach for synthesizing higher-resolution, color photographs. Our approach can be seen as a generative view of Siamese networks used for learning similarity metrics in discriminative tasks like face or signature verification. In our work, we use a Siamese architecture to enable the discriminator to differentiate between matched and unmatched pairs. Our evaluation shows that SD-GANs can disentangle factors of variation related to identity. By varying the observation vector, SD-GANs can change clothing color, facial features, lighting, and more while maintaining the apparent identity. SD-GANs can disentangle factors of variation related to identity and observation in images. The SD-DCGAN model produces convincing results on a shoe dataset, with the ability to manipulate the identity code to generate distinct shoe images. The algorithm presented in the paper shows promise for future work in disentangling latent factors in images. The approach aims to disentangle latent factors related to commonalities and apply it to identity-conditioned speech synthesis. It involves estimating latent vectors for unseen images using SD-GANs and showcasing the disentangled representations for identity estimation. The process involves minimizing the distance between the generated image and the unseen image. Linear interpolation across subspaces is demonstrated using SD-BEGAN, with consistent identity and observation codes. An AC-GAN baseline is described using an embedding matrix for real identities as latent codes. In an experiment, the DR-GAN method was modified to incorporate an encoding network to transform images into identity representations. Pairwise discrimination was also combined with this encoder-decoder approach. Results were shown in FIG9, indicating promising qualitative outcomes. The use of pairwise discrimination with pairs of real images is not a novel concept in GANs. Tran et al. (2017) proposed Disentangled Representation learning-GAN (DR-GAN) for face frontalization, using pairwise discrimination with real and fake images. The DR-GAN generator accepts an input image, pose code, and noise vector, while the discriminator classifies the pose and determines if the image is real or fake. DR-GAN can disentangle pose and illumination from the latent space. We modified DR-GAN to only disentangle identity, using a DCGAN discriminator architecture. The generator quickly learned to produce a single output image for each input, so we excluded this experiment from evaluation. The evaluation focused on AC-DCGAN generation with random identity vectors that sum to one. The AC-GAN generator received identity input z I \u2208 [0, 1] 10000, but querying with a random vector for new identities produced little variety compared to the normal one-hot strategy. The architectural details for SD-DCGAN and SD-BEGAN models were listed, specifying k as the number of images generated per identity and d I as the number of identities. In experiments, the dimensionality of the latent space Z I is important. The bottleneck layer of the SD-BEGAN discriminator autoencoder has an output dimensionality of 150 with k = 2 and d I = 50. Generators are parameterized by k for clarity, but implementations can collapse k into the batch size. Stacked-channels versions of discriminators change the number of input image channels to 3k. Identity and observation vectors are randomly drawn for qualitative comparison. Figure 18 shows samples generated from SD-DCGAN trained with Wasserstein GAN loss BID1 using RMS-prop optimization. FaceNet had an AUC of .770 and 68.5% accuracy on data from this model, excluded from Table 1 for brevity."
}