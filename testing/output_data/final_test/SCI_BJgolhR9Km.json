{
    "title": "BJgolhR9Km",
    "content": "In adversarial attacks on machine-learning classifiers, small perturbations are added to correctly classified input, creating virtually indistinguishable adversarial examples that are misclassified. Attackers can craft these examples in standard neural networks for deep learning to cause misclassification. RBFI units are introduced as a new type of network units that are resistant to adversarial attacks. On permutation-invariant MNIST, networks using RBFI units perform similarly to sigmoid units and slightly below ReLU units in the absence of attacks. When subjected to adversarial attacks, RBFI networks maintain accuracies above 75%, while ReLU and Sigmoid networks drop below 1%. Additionally, RBFI networks trained on regular input match or exceed the accuracy of networks trained with adversarial examples using Sigmoid and ReLU units. RBFI units are difficult to train with standard gradient descent due to their non-linear structure. However, RBFI networks can be efficiently trained using pseudogradients, crafted functions that aid learning instead of true derivatives. Machine learning with deep neural networks has been successful in various applications, but adversarial examples pose a challenge. These examples involve small modifications to input that can lead to misclassification, and they can be transferred between neural networks without precise knowledge of the target network's weights. This makes it difficult to guarantee the behavior of deep neural networks. Training neural networks with adversarial examples can lead to label leaking, where networks associate specific adversarial examples with original inputs. This method does not increase resistance to general adversarial attacks. However, using more general optimization techniques can result in networks with significantly higher resistance to such attacks. Neural networks can be made resistant to adversarial attacks by using different techniques, leading to increased complexity and computational requirements. Alternatively, using inherently impervious neural network types can prevent adversarial attacks, even when trained on standard input. The presence of adversarial examples is connected to the local linearity of neural networks, where small input perturbations can cause significant output changes in deep neural networks. Deep neural networks may be locally linear, allowing for adversarial attacks. RBFI units, a variant of radial basis functions, are highly non-linear and resistant to attacks. They can scale input components individually and cover more input space effectively. RBFI units use the infinity norm to measure distance from the center of the Gaussian, making them highly non-linear and resistant to attacks. Networks with RBFI units cannot be effectively trained using gradient descent, but can be trained efficiently using pseudogradients. RBFI units utilize pseudoderivatives for specific functions like the exponential and maximum operator to aid in training. Pseudoderivatives widen the detectable gradient region around Gaussians and propagate gradient back to all inputs of the maximum operator. This unconventional approach is necessary as traditional methods like AdaDelta and gradient descent with momentum do not follow pure gradient descent. RBFI units utilize pseudoderivatives for specific functions like the exponential and maximum operator to aid in training. They operate at the granularity of the individual unit and can be easily trained with standard random gradient descent methods. RBFI networks match the classification accuracy of networks with sigmoid units and are close to the performance of networks with ReLU units on permutation invariant MNIST. RBFI networks, utilizing pseudoderivatives for specific functions, match the accuracy of sigmoid networks and are close to ReLU networks on permutation invariant MNIST. They are more resistant to adversarial attacks, outperforming sigmoid and ReLU networks even when trained with adversarial examples. RBFI networks can be successfully trained with pseudogradients, offering a viable alternative to current adversarial training regimes for achieving robustness to attacks. Adversarial examples were first noticed in previous studies, where a connection was established between linearity and attacks. The paper discusses the perturbation of linear forms using the fast gradient sign method (FGSM) to create adversarial examples. It explores the transferability of adversarial examples across networks and questions the possibility of constructing non-linear structures for robustness. Training on adversarial examples via FGSM does not provide strong resistance to attacks due to label leaking phenomenon. In BID21, small random perturbations before applying FGSM enhance network robustness. The network trained using I-FGSM and ensemble method won the NIPS 2017 competition on defenses against adversarial attacks. Carlini and Wagner argue that training with simple heuristics fails to provide true resistance to attacks and advocate for more general optimization processes. FGSM and I-FGSM rely on local gradients for training. In BID21, small random perturbations before applying FGSM enhance network robustness. Using projected gradient descent (PGD) to generate adversarial attacks improves network resistance without affecting general optimization techniques. Defensive distillation protects networks from FGSM and I-FGSM attacks but not general adversarial attacks. Training neural networks on PGD-generated adversarial examples increases resistance but requires more computational resources. In BID7, adversarial attacks are linked to the linearity of models. To combat this, units with small output variations for small input changes in infinity norm are used. Linear forms are replaced with infinity-norm distances between points for better resistance. RBFI units are defined using infinity norm and consist of two weight vectors u and w. The output is calculated using the Hadamard product, with w representing the point from which the distance to x is measured. The scaling factors in u allow for flexibility in emphasizing certain inputs while disregarding others. The output of an RBFI unit is close to 1 when certain conditions are met. RBFI units are used in neural networks, with And and Or gates, to resist adversarial attacks. Sensitivity to attacks is computed using the infinity norm. The Or RBFI unit is introduced as U OR (u, w) = 1 \u2212 U(u, w). The network consists of layers with And units, Or units, and mixed layers. The output of RBFI units is close to 1 when x is close to w in coordinates with large scaling factors. The sensitivity to adversarial attacks in neural networks is calculated using the infinity norm. For ReLU and Sigmoid units, sensitivity increases linearly with input size, while for RBFI units, sensitivity remains constant regardless of input size. The sensitivity of RBFI units remains constant regardless of input size, and formulas can be extended to bounds for whole networks. Weight regularization can be used to achieve robustness against attacks by adding c\u015d to the loss function during training. This regularization helps train more robust RBFI networks but does not have the same effect on ReLU networks. The non-linearities in neural networks containing RBFI units make them difficult to train using standard gradient descent. The shape of Gaussian functions causes the problem, as the derivatives may not be large enough to move the weights towards useful places in the input space during training. To address this issue, pseudoderivatives are used in the chain-rule computation of the loss gradient, resulting in a pseudogradient. In neural networks with RBFI units, pseudoderivatives are used in the chain-rule computation of the loss gradient to address the issue of non-linearities. This helps transmit feedback to inputs approaching the maximum value, improving training speed and stability. However, there is a risk of converging to solutions where the pseudogradient is null, leading to suboptimal results. Training with true gradients after pseudogradients failed to improve accuracy on the testing set for RBFI networks. Adversarial examples are produced by perturbing correctly classified inputs. The gradient of the cost function is computed for each input in the testing set to generate adversarial examples. Adversarial examples are generated using techniques like Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) to create perturbations in the input data for improved attack adaptability. The study involves using projected gradient descent (PGD) to find mis-classified inputs around a given input x within a certain perturbation size. Additionally, the robustness of the networks is tested by introducing noise to the inputs. The study implemented FGSM, I-FGSM, and PGD attacks for RBFI using standard gradients and pseudogradients. Pseudogradient-based results were denoted as RBFI [psd]. The evaluation of attack resistance should include general optimization methods like PGD attacks. RBFI networks were implemented in PyTorch framework to extend with new functions. To implement RBFI, PyTorch was extended with two new functions: LargeAttractorExp and SharedFeedbackMax. These functions are used in RBFI units with AutoGrad mechanism for backward gradient propagation. The MNIST dataset was used with 60,000 training examples and 10,000 testing examples. Each digit image was flattened to a 784-dimensional feature vector and fed to a fully-connected neural network. Various fully-connected network structures were compared for accuracy, including ReLU networks. The study compared the accuracy of different fully-connected network structures, including ReLU, Sigmoid, and RBFI networks. Square-error loss performed well for Sigmoid and RBFI networks. The networks had layers with 512, 512, 512, and 10 units. RBFI networks with various geometries were tested using AdaDelta optimizer. Attacks such as FGSM, I-FGSM, and noise attacks were applied to the test set. PGD attacks were only applied to one run, and performance was computed for the first 5,000 examples. 20 searches were performed for each input in the test set. For each input x in the test set, 20 searches are performed with 100 steps of gradient descent using AdaDelta. Results on accuracy and resistance to adversarial examples for networks trained on MNIST are summarized in Table 1. RBFI networks show a performance drop of (1.66 \u00b1 0.21)% compared to ReLU networks. RBFI networks exhibit superior performance compared to ReLU networks and are comparable to sigmoid networks. They outperform in the presence of perturbations like adversarial attacks or noise. Gradient masking makes regular gradient attacks ineffective against RBFI networks, with pseudogradients being the most effective attack method. Including adversarial examples in the training set enhances neural network resistance to attacks. Neural networks trained with a mix of normal and adversarial examples show resistance to adversarial attacks compared to RBFI networks trained on standard examples only. Results for Sigmoid networks were consistently inferior to those for ReLU networks. Different training methods were compared, including FGSM, I-FGSM, and PGD for ReLU networks. The study compared different neural network training methods, including FGSM, I-FGSM, and PGD for ReLU networks. Adversarial examples were generated for training with a focus on PGD attacks. RBFI networks trained without adversarial examples showed the best performance overall, while ReLU(PGD) networks performed well specifically against PGD attacks due to gradient masking. After comparing the performance of RBFI networks trained with pseudogradients and regular gradients, it was found that pseudogradients yielded higher accuracy levels. Specifically, after 30 epochs of training, RBFI networks with pseudogradients achieved (96.79 \u00b1 0.17)% accuracy compared to (86.35 \u00b1 0.75)% accuracy with regular gradients. This trend was consistent across different network sizes, with pseudogradients consistently outperforming regular gradients in training efficiency. Regularization techniques were explored to enhance robustness to adversarial attacks in ReLU and RBFI networks. For ReLU networks, adding weight regularizations had a limited impact on robustness. However, for RBFI networks, regularization improved resistance to adversarial examples by influencing the choice of upper bounds for network components. In experiments, RBFI networks showed improved resistance to adversarial attacks with weight regularization. Increasing the upper bound to 10 without regularization decreased accuracy, but adding regularization recovered lost accuracy, achieving high accuracy and resistance to attacks."
}