{
    "title": "SJlbGJrtDB",
    "content": "Dynamic Sparse Training is a novel network pruning algorithm that optimizes network parameters and structure simultaneously. It allows for fine-grained layer-wise adjustments of pruning thresholds through backpropagation. The algorithm can train very sparse neural network models with minimal performance loss compared to dense models, achieving prior art performance on various network architectures. It also highlights the limitations of traditional pruning algorithms and offers insights for designing more compact network architectures. The algorithm guides the design of compact network architectures by addressing the memory and computation intensity issues in deep neural networks. Network pruning is recognized as an effective approach to improve inference efficiency in resource-limited scenarios. Sparse training methods have been proposed to conduct network pruning during the training process, but they face three main problems. During the training process, existing pruning methods face issues such as predefined pruning schedules with additional hyperparameters and failure to properly recover pruned weights. Most methods use a fixed pruning schedule for all network architectures, regardless of complexity, and conduct epoch-wise pruning. Existing pruning methods face challenges in properly recovering pruned weights. Hard pruning methods set weights to 0 directly, leading to the loss of historical parameter importance. This makes it difficult to determine when and how pruned weights should be recovered. Current methods randomly initialize or set recovered weights to the same value, lacking a precise recovery strategy. Existing pruning methods struggle with properly recovering pruned weights. Modern neural networks have varying degrees of parameter redundancy among layers, making it suboptimal to prune the same percentage of parameters at each layer. Using a single global pruning threshold or layer-wise greedy algorithms for dynamic layer-wise pruning rates is challenging. Pruning algorithms based on a single global threshold lack consistency and robustness, while layer-by-layer greedy pruning methods may overlook the importance of early layer neurons. The proposed end-to-end sparse training algorithm addresses the issue of unimportant neurons in early layers affecting later layers, leading to error propagation. It introduces dynamic fine-grained pruning and recovery with automatic adjustment of layerwise pruning rates. The algorithm achieves state-of-the-art performance and offers step-wise pruning and recovery during training epochs. Our method introduces dynamic fine-grained pruning and recovery with trainable neuron-wise or filter-wise thresholds, offering step-wise pruning and recovery during training epochs. Our method introduces dynamic fine-grained pruning and recovery with trainable neuron-wise or filter-wise thresholds, offering step-wise pruning and recovery during training epochs. The learning rate is a crucial hyperparameter that decays during training. Our algorithm automatically adjusts layer-wise pruning rates to achieve optimal sparse network structures with consistent patterns. Traditional pruning methods include using second-order derivatives or parameter magnitudes as criteria. Sparse Neural Network Training: Recent works aim to find sparse networks directly during training without the need for pruning and fine-tuning. Mocanu et al. (2018) proposed Sparse Evolutionary Training (SET) inspired by neural cell growth, allowing pruned neurons to randomly revive. However, manual sparsity level setting and random network connection recovery may lead to unexpected effects. DEEP-R by Bellec et al. (2017) uses Bayesian sampling for pruning and regrowth configuration decisions. Dynamic Sparse Reparameterization and Dynamic Network Surgery are two methods that aim to find sparse structures in neural networks. However, they both have limitations such as coarse-grained adjustment of pruning thresholds and manually determined fixed thresholds during the sparse learning process, making it difficult to adapt. Dynamic Sparse Reparameterization and Dynamic Network Surgery aim to find sparse structures in neural networks. However, they face limitations such as coarse-grained adjustment of pruning thresholds and manually determined fixed thresholds during sparse learning, making adaptation difficult. Dettmers & Zettlemoyer (2019) proposed sparse momentum using exponentially smoothed gradients for pruning and regrowth, with a fixed percentage of parameters pruned at each step. The pruning ratio and momentum scaling rate require searching a high parameter space. The pruning process in neural networks involves finding binary parameter masks for each parameter matrix, preserving sparse structure information. By applying a binary mask to each parameter, a set of binary parameter masks is dynamically found for the network. The pruning process in neural networks involves finding binary parameter masks for each parameter matrix to preserve sparse structure information. A trainable pruning threshold vector is defined for each parameter matrix, and a unit step function is used to determine the masks based on parameter magnitudes and thresholds. The masks are applied to the parameters to create sparse parameters while preserving historical information about parameter importance. The trainable masked fully connected, convolutional, and recurrent layers are introduced using a threshold vector and dynamic parameter mask. Sparse parameters are used in batched matrix multiplication for fully connected and recurrent layers, while sparse kernels are used for convolutional layers. The elements in the threshold vector are made trainable via back-propagation. The derivative of the binary step function S(x) is needed to train the threshold vector elements via back-propagation. Previous works used a clip function called straight through estimator (STE) for derivative estimation. Xu & Cheung (2019) discussed using a long-tailed higher-order estimator H(x) for this purpose. The method utilizes a long-tailed higher-order estimator H(x) with a wide active range to prevent gradient vanishing during training. The estimator provides a tighter approximation than STE near zero, allowing for training of vector threshold elements via backpropagation. Trainable masked layers enable the network parameter to receive performance and structure gradients for better model performance and sparse structure. The structure gradient facilitates updating pruned weights through backpropagation. Further details on feed-forward and backpropagation in trainable masked layers are provided in Appendix A.3. The proposed method involves fine-grained step-wise pruning and recovery automatically by updating pruned weights, unpruned weights, and threshold vector elements through back-propagation. To achieve higher sparsity in parameter masks, a sparse regularization term penalizing low threshold values is added to the training loss. The regularization term for a deep neural network with trainable masked layers is calculated using exp(\u2212t i ) for each layer's threshold value. The proposed method involves training a sparse neural network directly with back-propagation algorithm using a sparse regularization term to increase model sparsity. The regularization function penalizes low thresholds without encouraging them to become extremely large, leading to higher sparsity in the network. The proposed method involves training a sparse neural network directly with a sparse regularization term to increase model sparsity. It dynamically finds a balance between model sparsity and performance by adjusting thresholds. The method is evaluated on various datasets and network architectures to quantify pruning performance. The proposed method involves training a sparse neural network directly with a sparse regularization term to increase model sparsity. The model remaining percentage is defined as k m \u00d7 100%. For all trainable masked layers, the thresholds are initialized to zero. Experimental results on MNIST and CIFAR-10 show that the method can prune up to 98% of parameters with minimal loss of performance on various network architectures. Dynamic Sparse Training (DST) outperforms state-of-the-art algorithms like DSR and Sparse momentum on CIFAR-10 by varying the scaling coefficient \u03b1 for sparse regularization. The model remaining ratio decreases with increasing \u03b1, allowing for sparse models with comparable or higher accuracy than dense models. The choice of \u03b1 in Dynamic Sparse Training (DST) ranges from 10^-9 to 10^-4, impacting model performance. Figures show smooth changes in layer remaining ratios during training, unlike fixed step sizes in other pruning methods. The layer remaining ratios fluctuate dynamically within the first 100 training steps in Dynamic Sparse Training (DST), indicating step-wise fine-grained pruning and recovery. Different layers in multilayer neural networks have varying importance, with the last layer (layer 3) requiring careful pruning. The input layer (layer 1) with the largest parameters can safely prune background pixels in images from the MNIST dataset. The remaining ratios of different layers in a Lenet-300-100 model were analyzed during dynamic sparse training. Layer 1 and layer 2 showed decreasing ratios, while layer 3 maintained a ratio of 1. The test accuracy of the sparse model remained similar to the dense model, indicating a proper balance between remaining ratio and model performance. The study showed that continuous fine-grained pruning can balance model remaining ratio and performance. Similar training tendencies were observed in various network architectures, with detailed results in Appendix A.4. Learning rate decay is commonly used to improve performance, with a decrease in test accuracy followed by stabilization. For example, VGG-16 on CIFAR-10 had a learning rate decay from 0.1 to 0.01 at 80 epochs. The remaining ratio of FC 2 is unexpectedly low for the first 80 epochs but increases after the learning rate decay from 0.1 to 0.01 at 80 epochs. This decay allows for fine-grained parameter updates, leading to quicker convergence to good local minima. After the learning rate decay at 80 epochs, the remaining ratio of the output layer abruptly increases, leading to a significant improvement in test accuracy from 85% to over 92%. This indicates that the network connections in the output layer become more important, while the ratios of the preceding convolutional layers remain unchanged, showing the necessity of these parameters in finding critical features. The method can dynamically adjust the pruning schedule based on changes in hyperparameters during training, including different initial learning rates. The VGG-16 model was trained with different initial learning rates to study its effect on network parameters. The test accuracy increased to over 90% with a smaller initial learning rate, allowing layers to extract useful features before the learning rate decay. The test accuracy only slightly increased after the learning rate decay at 80 epochs. The method adapts the pruning schedule based on different initial hyperparameters choices. The model performance under dynamic schedule shows that sparse accuracy is consistently higher than dense accuracy during training with initial learning rates of 0.01 and 0.1. Network architecture search is seen as the future of deep neural network design, with challenges in determining layer redundancy. The sparse structure revealed from network pruning may guide network architecture design, but current pruning strategies have limitations. A consistent sparse pattern observed during dynamic sparse training offers valuable insights into layer redundancy for compact network design. The consistent sparse pattern observed during dynamic sparse training provides insights into layer redundancy for compact network design. The sparse patterns of VGG-16 on CIFAR-10 show that certain layers have high redundancy and can be pruned to create more compact models. This phenomenon is consistent across different network architectures. Our method analyzes layer-wise redundancy in new architectures. LSTM models are trained with Adam optimization for 20 epochs, batch size 100, and learning rate 0.001. Sparse regularization term has scaling factor \u03b1 of 0.001. CIFAR-10 models use SGD with momentum 0.9, batch size 64, and learning rate 0.1 decayed at 80 and 120 epochs. ImageNet ResNet-50 models use SGD with momentum 0.9, batch size 1024, and learning rate 0.1 decayed at 30, 60, 80 epochs. Trainable thresholds for masked layers are initialized to zero. The trainable thresholds are initialized to zero. High scaling coefficient \u03b1 can dominate the sparse regularization term, leading to all-zero masks in certain layers. To prevent this, the pruning threshold is reset to zero if over 99% of mask elements are zero in a layer. Different threshold options (vector, scalar, matrix) are tested on various architectures, with the matrix threshold showing similar model remaining ratio as the vector threshold. The scalar threshold is less robust. The matrix threshold almost doubles the parameter storage overhead during training compared to the vector threshold. The vector threshold adds less than 1% additional network parameter and brings lighter computation in feed forward and backpropagation phases. The vector threshold is chosen for trainable masked layers due to the balance between overhead and benefits. In trainable masked layers, a sparse mask M is obtained for each layer, and the sparse parameter W M is used in matrix-vector multiplication or convolution. The back-propagation process involves gradients for P, M, Q, and the vector threshold t. In trainable masked layers, a sparse mask M is obtained for each layer, and the sparse parameter W M is used in matrix-vector multiplication or convolution. The back-propagation process involves gradients for P, M, Q, and the vector threshold t. The gradient received by the parameter W is dW = dP M + dP W H(Q) sgn(W). The elements in W are distributed within [\u22121, 1], while elements in the vector threshold are distributed within [0, 1]. The remaining ratios during dynamic sparse training for different architectures are presented. WideResNet models have one fully connected layer at the end as the output layer. The WideResNet models have one fully connected layer at the end as the output layer. Figures 8, 9, and 10 show results for different WideResNet architectures. Similar sparse patterns are observed across various architectures. Figures 11 and 12 display results for additional WideResNet models."
}