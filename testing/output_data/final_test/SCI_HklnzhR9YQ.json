{
    "title": "HklnzhR9YQ",
    "content": "We develop new approximation and statistical learning theories of convolutional neural networks (CNNs) via the ResNet-type structure, showing that a ResNet-type CNN is a universal approximator with expression ability comparable to fully-connected neural networks (FNNs) with a block-sparse structure. The general theory allows for optimal learning on CNNs in approximation and estimation of various function classes. The Barron and H\\\"older function classes can be estimated using the clipped empirical risk minimization (ERM) estimator, achieving the same rate as Fully Connected Neural Networks (FNNs) even with constant channel size, filter size, and width of Convolutional Neural Networks (CNNs). The proof is minimax optimal for the H\\\"older class, utilizing evaluations of CNN covering numbers and parameter rescaling techniques. Despite CNN's practical popularity in deep learning research, theoretical justification for its power remains limited. Theoretical justification for the power of Convolutional Neural Networks (CNNs) is still scarce from the viewpoint of statistical learning theory. Existing work on Fully Connected Neural Networks (FNNs) dates back to the 80's, providing theoretical explanations for their approximation ability and generalization power. Recently, statistical learning theory for CNNs has been studied, relating their approximation ability to that of FNNs. The ability of CNNs to approximate functions has been compared to that of FNNs, with recent studies showing that CNNs can approximate functions realized by FNNs. However, achieving the minimax optimal rate in important function classes with CNNs remains a challenge. In this paper, the learning ability of ResNet-type ReLU CNNs with identity mappings and constant-width residual blocks is analyzed. Recent methods like identity mappings, initialization schemes, and normalization techniques have made architectures with large depth feasible. There is a growing demand for theories that can accommodate constant-size architectures like these. Our strategy aims to replicate the learning ability of FNNs by constructing tailored ResNet-type CNNs that leverage block-sparseness to decrease model complexity and promote better bounds. This approach has not been extensively explored, especially in relation to the interplay between FNNs and CNNs. Our approach involves constructing ResNet-type CNNs that leverage block-sparseness to replicate the learning ability of FNNs. This allows for a decrease in model complexity and better bounds. The method is general and not limited to specific function classes, demonstrating wide applicability. Our work introduces a novel approach using ResNet-type CNNs with block-sparseness to achieve optimal estimation error for \u03b2-H\u00f6lder class. This method allows for minimax optimal error even with constant filter size, channel size, and width. The contributions include developing approximation theory for CNNs with constant-width residual blocks and proving the realizability of any M-way block-sparse FNN in such a CNN. Our work introduces a novel approach using ResNet-type CNNs with block-sparseness to achieve optimal estimation error for \u03b2-H\u00f6lder class. The contributions include developing approximation theory for CNNs with constant-width residual blocks and proving the realizability of any M-way block-sparse FNN in such a CNN. The upper bound of the estimation error is derived in terms of the approximation error of FNNs and the model complexity of CNNs. The general theory is applied to the Barron class and H\u00f6lder class, resulting in identical approximation and estimation error rates compared to FNNs, even with constant channel and filter size. In Table 1, the differences in CNN architectures between our work and previous studies by Zhou (2018) and Petersen & Voigtlaender (2018) are discussed. Zhou (2018) focused on a specific function class, while Petersen & Voigtlaender (2018) utilized group invariance structure for CNN construction. Zhou (2018) used a single-channel CNN with linearly increasing width, which is uncommon for regression or classification tasks. Petersen & Voigtlaender (2018) made theoretical analysis easier by leveraging the group invariance structure. The structure of CNNs enables the incorporation of mathematical tools like group theory and Fourier analysis for investigating equivariance properties. However, the assumption on the group structure excludes important convolution operations like padding layers. Applying their construction method to H\u00f6lder functions results in a CNN with a large filter size equal to the input dimension. Their construction is not aware of the internal sparse structure of approximating FNNs. Our approach employs padding-and ResNet-type CNNs with multiple channels, fixed-size filters, and constant widths. It is applicable to block sparse FNNs, including Barron and H\u00f6lder cases, achieving the same approximation rate as FNNs. The optimal CNNs can achieve this while the number of channels remains independent of the sample size, and is minimax optimal for the H\u00f6lder class. Theoretical analysis for ResNet has been explored recently, contributing to statistical learning theory. Our theory is the first to provide the approximation ability of the CNN class that can accommodate ResNet-type models. We import approximation theories for FNNs, including the Barron class and H\u00f6lder class. The estimation error of the ERM estimator is O(N \u2212 2\u03b2 2\u03b2+D) according to Schmidt-Hieber (2017). In a regression task, the ERM estimator's estimation error is O(N \u2212 2\u03b2 2\u03b2+D), proven to be minimax optimal. A random variable Y is defined as f\u2022 (X) + \u03be, with joint distribution P of (X, Y). The goal is to estimate the true function f\u2022 from dataset D = ((x1, y1), ...(xN, yN)). Performance evaluation is based on squared error using the L2-norm and sup norm of f. In this section, CNNs are defined using real-valued sequences with finitely many non-zero elements. The one-sided padding and stride-one convolution are defined as an order-4 tensor. In this section, CNNs are defined using real-valued sequences with finitely many non-zero elements. The one-sided padding and stride-one convolution are defined as an order-4 tensor. The building blocks of CNNs, including convolutional layers and fully-connected layers, are also defined with input channel size, output channel size, filter size, weight tensor, bias vector, and activation function. The fully-connected layer FC is defined as a vectorization operator that flattens a matrix into a vector in a ResNet-type CNN. The CNN consists of one convolution block, M residual blocks, and one fully-connected layer. The schematic view of the CNN is shown in FIG2. The CNN is defined with weight tensors, biases, and the number of residual blocks and depth of each block. The stack of convolutional layers can include a fully-connected layer. In this paper, a CNN is defined as a stack of convolutional layers with or without a fully-connected layer. Different types of CNNs are distinguished based on the activation function used. Norm constraints are imposed separately on the convolutional and fully-connected parts of the architecture. No sparse constraints are imposed on the hypothesis class of ReLU CNNs. In this paper, the authors discuss their design choice for a block-sparse FNN, which consists of M possibly dense FNNs concatenated in parallel. They also mention one-sided padding and compare their approach with the original ResNet proposed by He et al. (2016). The authors discuss the design of a block-sparse FNN, consisting of M dense FNNs in parallel. They introduce the architecture of a block-sparse FNN and define a M-way block-sparse FNN. The paper presents main results related to function realizability by FNNs within a restricted domain. The main results of this paper include the realisability of M-way block-sparse FNNs by ResNet-type CNNs with added parameters. Theorems show that approximating a function with a block-sparse FNN allows approximation with a CNN as well. Another theorem bounds the estimation error of the clipped ERM estimator. The term FORMULA27 represents the approximation error of FNN, while M1 and M2 are determined by the architectural parameters of CNN. The trade-off between approximation error and model complexity can be balanced by appropriately choosing M. The Barron class is an example of a function class that can be approximated by block-sparse FNNs. The Barron function can be approximated by a linear combination of ridge functions using a 2-layered ReLU FNN. This approximation can also be achieved using CNNs with M residual blocks. The choice of setting B (bs) and B (fin) affects the estimation error. The Barron function can be approximated by a linear combination of ridge functions using a 2-layered ReLU FNN or CNNs with M residual blocks. The choice of setting parameters in the block-sparse and fully-connected parts affects the estimation error. Rescaling operations enable choosing parameters for the residual blocks and fully-connected parts to approximate any \u03b2-H\u00f6lder function. Yarotsky and Schmidt-Hieber showed that FNNs with O(S) non-zero parameters can approximate any D variate \u03b2-H\u00f6lder function with the order of \u00d5(S - \u03b2D). Yarotsky and Schmidt-Hieber demonstrated that FNNs with O(S) non-zero parameters can approximate any D variate \u03b2-H\u00f6lder function with the order of \u00d5(S - \u03b2D). Schmidt-Hieber also showed a similar result using a different construction method, specifying width, depth, and non-zero parameter counts of the approximating FNN. By transforming the constructed FNNs into block-sparse ones, the approximation and estimation errors can be derived. The study establishes new approximation and statistical learning theories for Convolutional Neural Networks (CNNs) using ResNet-type architecture and block-sparse structure of Fully Connected Neural Networks (FNNs). It shows that a clipped ERM estimator can achieve minimax optimal rate up to logarithmic factors, even when width, channel size, and filter size are constant with respect to sample size. The study demonstrates that block-sparse Fully Connected Neural Networks (FNNs) can be realized using Convolutional Neural Networks (CNNs) with additional parameters. The theory is general and applicable to various function classes. The approximation and error rates for different classes were derived, showing that CNNs have similar estimation errors as FNNs. Key techniques involved evaluating Lipschitz constants of CNNs and weight parameter rescaling of FNNs. An open question remains regarding the role of weight rescaling. The study explores the role of weight rescaling in improving estimation error rates in neural networks. By leveraging the homogeneous property of the ReLU activation function, the relative scale between block-sparse and fully-connected parts can be adjusted. This rescaling technique, applicable to various function classes, enhances the relationship between approximation and estimation capabilities of FNNs and CNNs. Additionally, it is suggested that by analyzing the internal structure of FNNs and utilizing techniques like repetition, CNNs may require fewer parameters and achieve better estimation error rates. The study discusses improving estimation error rates in neural networks by exploring weight rescaling techniques. It is suggested that analyzing the internal structure of FNNs and utilizing techniques like repetition can help CNNs require fewer parameters and achieve better estimation error rates. The study explores improving estimation error rates in neural networks by utilizing weight rescaling techniques. It involves doubling the channel size in the FNN and converting them into convolution layers, using the first channel for input signal and the second for accumulating block outputs. The convolution operation with size-1 filter is equivalent to a dimension-wise affine transformation. The approximation error of Theorem 2 is related to the estimation error. The study focuses on improving estimation error rates in neural networks by utilizing weight rescaling techniques. The approximation error of Theorem 2 is linked to the estimation error using the covering number of the hypothesis class F (CNN). Lemmas 4 and 5 provide bounds on the logarithm of the covering number, leading to Theorem 2. Corollary 1 is proven by setting M = O(N \u03b1) and analyzing the order of the right-hand side with respect to N. The convolutional layer operates by taking the inner product with the input signal. The inner product with the first K elements of the input signal acts as a \"left-translation\" by K \u2212 1. Define w to take the inner product with the K left-most elements in the first channel and shift the input signal by K \u2212 1 with the second channel. Set b := (0, ..., 0 L0 \u2212 1 times, t). The lemma shows how any linear CNN can be converted to a ReLU CNN with approximately 4 times larger parameters. This lemma is also found in Petersen & Voigtlaender (2017) (Lemma 2.3). The text discusses the conversion of a linear CNN to a ReLU CNN, showing how the claim holds for different values of l. It also mentions the concatenation of two CNNs with the same depths and filter sizes in parallel. The proposition states that C(0) and C(0) may not necessarily be 1, defining w and b in a similar manner. The text discusses constructing a CNN with M residual blocks, starting with a single convolutional layer in the m = 0 block. Weight parameters are specified, and the channel size is doubled in this part. The identity mapping is emphasized, and the construction process involves utilizing Lemma 1 and Lemma 2 to realize hinge functions with ReLU CNNs. In constructing a CNN with M residual blocks, the channel size is doubled in the m = 0 part. Filters with zeros are added to neglect input signals from the second channel. For the l-th layer (l = 2, ..., L m), size-1 filters are prepared using the Kronecker product of matrices. The depth of Conv m is at most 4D, and the final fully-connected layer is set as w := B, with specified parameters for w, b, and x. The activation function can be ReLU or identity function id. The activation function in CNNs can be ReLU or identity function id. The number of non-zero summands in the summation is at most W 0, each bounded by W \u221e x \u221e. The architecture of CNNs is denoted by DISPLAYFORM2, with norm constraints on the convolution part. The architecture of CNNs, denoted by DISPLAYFORM2, includes norm constraints on the convolution part. The text discusses bounding terms and applying formulas to analyze the convolution layers and residual blocks. The proof involves using Propositions and Lemmas to establish bounds within a metric space. The proof involves using Lemma 12 of Schmidt-Hieber (2017 to bound the estimation error of the clipped ERM estimatorf in the context of CNN architecture with norm constraints. The subset F 0 is created by selecting combinations of bins for parameters, leading to a cardinality of at most DISPLAYFORM2 D.2. The proof involves converting the problem setting to fit the assumptions of Lemma 10 of Schmidt-Hieber (2017). It shows that the ERM estimator of the regression problem can be obtained using dataset D. The estimation error of the clipped ERM estimator is no worse than that of the ERM estimator. Additionally, the existence of a block-sparse FNN with constant-width blocks that approximates a given \u03b2-H\u00f6lder function optimally is proven. Our study constructs a block-sparse FNN to approximate a given \u03b2-H\u00f6lder function optimally, similar to SchmidtHieber (2017). The CNN design does not include ReLU activations at certain points for simplicity in proofs, but can be easily modified to include them. Future research can explore extending our results to ResNet-type CNNs with pooling or Batch Normalization layers."
}