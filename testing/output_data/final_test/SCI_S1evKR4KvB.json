{
    "title": "S1evKR4KvB",
    "content": "Extreme Classification Methods, particularly for Information Retrieval (IR) problems, have gained importance due to scalable algorithms. Group Testing is a model class addressing memory and speed challenges in extreme multi-label learning. Multi-label Group Testing (MLGT) methods create label groups to predict and recover original labels. MACH (Merged Average Classifiers via Hashing) is a novel approach that projects label vectors to a manageable matrix, reducing memory requirements to O(logK) for K classes. MACH is a simple yet effective algorithm for practical use. In this paper, the authors address the gap in theoretical understanding of the trade-offs with MACH, a simple yet effective algorithm for extreme multi-label classification. They leverage count-min sketch theory to quantify memory-identifiability tradeoffs and propose a novel quadratic approximation using the Inclusion-Exclusion Principle for multi-label classification. Their estimator shows lower reconstruction error compared to typical CMS estimators across various scenarios. Extreme Classification has been a focal point in Data Mining and Information Retrieval research in recent years. Extreme Classification involves tackling the scale challenge of multiclass and multilabel classification with a large number of classes. Various paradigms like 1-vs-all methods, tree-based methods, and embedding models have been used to address this challenge. Group Testing is a recent approach proposed to alleviate the scale challenge of Multilabel Classification. In Extreme Classification, Group Testing is a recent approach proposed to alleviate the scale challenge of Multilabel Classification. A method called Merged Average Classifiers via Hashing (MACH) was introduced to address the issue of hard assignment of clusters in predicting true labels. MACH (Medini et al., 2019) addresses the hard-prediction problem in Group Testing by identifying the best labels based on prediction probabilities. It learns to predict a count-min sketch (CMS) matrix of the original probability vector for multiclass classification. However, the theory does not extend naturally to multilabel learning. Variance and error bounds for multiclass classification depend on the number of hash tables and their sizes. The classification in MACH relies on the number of hash tables and their sizes. This work contributes by analyzing reconstruction estimators for multiclass learning, proving variance is related to hash table parameters, establishing lower bounds on hyperparameters, and proposing a novel estimator for multilabel learning using the Inclusion-Exclusion principle. Count-Min Sketch (CMS) was proposed for frequency counting in large streaming settings with infinite elements. It uses O(log K) signatures for each class with 2-universal hash functions. The new estimator for multilabel learning shows significantly lower Mean-Squared Error (MSE) in reconstructing probability vectors. The Count-Min Sketch (CMS) algorithm assigns 'signatures' to each class using 2-universal hash functions. It maintains a counting-matrix to estimate frequencies of elements in a stream of classes. During inference, the algorithm reduces estimation offset by taking the minimum of all estimates as the approximate frequency. Connecting CMS and Extreme Classification: In extreme classification, the goal is to compress the probabilities of K classes into log K measurements. Encoder and Decoder based models like Compressive Sensing can help recover high probability classes from a smaller measurement vector, allowing for the training of a smaller classifier. The MACH paradigm for extreme classification utilizes universal hashing to reduce memory and computations by merging classes into meta-classes. This approach involves independently hashing input and label vectors multiple times before training small models in parallel. The process then uses off-the-shelf classifiers to predict the meta-classes. The MACH paradigm uses universal hashing to merge classes into meta-classes for extreme classification. It involves hashing input and label vectors multiple times to train small models in parallel. The process aggregates outputs from small meta classifiers to predict the best class. The input is assumed to be a large sparse vector, which can be feature hashed to reduce model size. The theoretical analysis of MACH focuses on retrieving relevant labels from meta-class predictions. Subsequent sections formalize the algorithm and provide bounds on mean, variance, error, and hyper-parameters. MACH uses universal hashing to merge classes into meta-classes for extreme classification. It bypasses training a huge last layer by modeling probability differently. The process involves hashing input and label vectors multiple times to train small models in parallel, aggregating outputs to predict the best class. The algorithm provides bounds on mean, variance, error, and hyper-parameters. MACH uses universal hashing to merge classes into meta-classes for extreme classification. It models probability differently to avoid training a large last layer. The process involves hashing input and label vectors multiple times to train small models in parallel, aggregating outputs to predict the best class. The algorithm provides bounds on mean, variance, error, and hyper-parameters. During prediction, an unbiased estimator is used to recover the K vector from the P j b matrix. The analysis diverges between Multiclass and Multilabel classification problems, with an unbiased estimator of p i obtained given the R classifier models. The text discusses the use of universal hashing in merging classes for extreme classification. It explains the expected value of indicators, linearity of expectation, and variance calculations. The analysis focuses on the variance of estimation based on the original probability values. The text discusses the use of universal hashing in merging classes for extreme classification, focusing on variance of estimation and computational complexity. It suggests tuning parameters R and B for optimal performance and presents Theorem 3 for identifying classes with high probability. Theorem 3 guarantees distinguishability of class pairs with high probability, but does not consider ease of classification. Subsequent theorems quantify requirements on R, B based on recovery error tolerance and prediction ease. Based on Theorem 3, we can estimate the bucket size B and number of models needed for multi-label classification. The inclusion-exclusion principle is used to recover original probability vectors from MACH measurements in this scenario. The analysis perspective differs between multi-class and multi-label classification, with eqn. 1 not applicable in the latter case. Based on Theorem 3, the inclusion-exclusion principle is used to recover original probability vectors from MACH measurements for multi-label classification. The assumption of sparsity in probability vectors is crucial for deriving an estimator. The analysis assumes an average number of active labels per input, leading to a simplified mean calculation in a typical multilabel dataset scenario. Based on Theorem 3, the inclusion-exclusion principle is used to recover original probability vectors from MACH measurements for multi-label classification. The analysis assumes sparsity in probability vectors and an average number of active labels per input, simplifying the mean calculation. By limiting the expression to first-order summation, a better estimator for true probability is obtained. However, proposing an unbiased estimator using this result is challenging due to Jensen's inequality. Simulation experiments in the next section support the proposed estimator for multilabel classification. The simulation experiments in the section validate our proposed estimator for multilabel classification, showing significantly lower mean-squared-error compared to a plain mean estimator. The process involves setting a base probability, initializing a vector, generating label vectors, converting them into binary labels using hash functions, and repeating the process for different functions. The simulation experiments validate the proposed estimator for multilabel classification, showing lower mean-squared-error compared to a plain mean estimator. The process involves setting a base probability, initializing a vector, generating label vectors, converting them into binary labels using hash functions, and calculating the mean of dimensional labels. The comparison of the proposed quadratic estimator against the plain mean estimator is shown by varying values of K, B, V, and base prob. Increasing K leads to higher MSE due to noise from small non-zero probabilities for many classes, but top classes are still retrieved with high certainty. The MSE decreases as B increases, resulting in less noisy predictions. A higher V leads to an increase in MSE due to more 'true' class collisions. The MSE decreases with base prob, indicating more confident 'true' classes among K. The proposed estimator shows commendably low recovery of original probabilities. The proposed square-root estimator consistently shows significantly lower MSE compared to the vanilla mean estimator for various configurations of K, B, and V. The MSE decreases as B increases, resulting in less noisy predictions, while a higher V leads to an increase in MSE due to more 'true' class collisions. The estimator also recovers the original probabilities with commendably low MSE. The Count-Min-Sketch estimator has lower MSE than the mean estimator. A new estimator is proposed to reconstruct original probability vectors with significantly lower MSE. Compressive Sensing and Count-Sketch are not suitable due to their measurement characteristics. The classifier predicts a compressed distribution of classes using regression models to minimize the norm between true and predicted vectors.softmax-loss is not suitable for this type of classification."
}