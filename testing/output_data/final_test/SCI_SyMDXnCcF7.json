{
    "title": "SyMDXnCcF7",
    "content": "Our mean field theory for batch normalization in fully-connected feedforward neural networks reveals that gradient signals grow exponentially in depth due to batch normalization, making vanilla batch-normalized networks without skip connections untrainable at large depths. Tuning the network close to the linear regime can reduce gradient explosion and improve the trainability of deep batch-normalized networks without residual connections. Our theory explores the learning dynamics of deep batch-normalized networks without residual connections, showing that gradients stabilize quickly during optimization. This analysis leverages Laplace, Fourier, and Gegenbauer transforms to derive new identities. The success of deep neural networks is often attributed to architectural innovations like convolutions, residual connections, and batch normalization. The lack of theoretical clarity in neural network progress has led to confusion regarding the impact of novel components on generalization. Recent research has focused on studying neural networks at initialization to understand the distribution of network parameters and activations. This approach aims to improve the deliberate design of neural networks. Understanding the distributions of network parameters and activations at initialization is crucial for designing well-conditioned priors for neural networks. Recent advancements have closed the gap in applying these techniques to practical network architectures, including those with skip connections, convolutional networks, and gated recurrent networks. Yang (2019) has extended this formalism to a wider range of architectures, but further study of architectural innovations is needed before analyzing state-of-the-art models in this framework. In this paper, a theory of fully-connected networks with batch normalization is developed, focusing on randomly distributed weights and biases. The main result is the adaptation of the theory for random networks to be applied to batches of data, specifically addressing the non-local dependency of batch normalization on the entire batch. The theory is extended to include batch normalization explicitly and validated through Monte-Carlo simulations, enabling the prediction of valid hyperparameter configurations. Additionally, several previously unknown findings are identified during the investigation. The investigation into batch normalization in deep networks reveals previously unknown properties that induce symmetry in embeddings, leading to exponential gradient explosions in fully-connected networks. This limits the trainable depth of batch normalized networks, which can be partially alleviated by using more linear activation functions at initialization. Despite the gradient explosion, networks with batch normalization naturally equilibrate the gradient scale during training. Batch normalization helps equilibrate gradients in deep networks, allowing for adequate training. It enables exact Bayesian inference in wide neural networks and has become essential in deep learning. Other modifications like layer normalization and weight normalization have also been proposed. Several modifications have been proposed in addition to batch normalization, such as layer normalization and weight normalization. The effectiveness of these different schemes has been difficult to compare. Batch normalization was originally introduced to prevent \"internal covariate shift\" and has since been approached from a theoretical angle in various papers. Studies have shown that batch normalization in residual networks allows for deep gradient signal propagation and helps by smoothing the loss landscape. Some research suggests that batch normalization works in this setting because it induces beneficial gradient dynamics, enabling a larger learning rate. Batch normalization allows for larger learning rates and can induce acceleration in training. Previous studies have explored the benefits of batch normalization in deep networks, with some making simplifying assumptions. However, our analysis considers networks with multiple batchnorm layers and general activation functions, showing that deep stacking of batchnorm can lead to gradient explosion. In this work, the authors provide a theoretical analysis of gradient statistics in fully-connected networks with batch normalization. They extend mean field theory to compute network statistics over a batch of data, considering networks with multiple batchnorm layers and general activation functions. The pre-activations of the network are defined by the recurrence relation, with weights and biases chosen to be i.i.d. In the mean field approximation, pre-activations are replaced by Gaussian random variables with matching moments. In the infinite width limit, this approximation becomes exact. The mean of each pre-activation is zero due to i.i.d. weights with zero mean. In the mean field approximation, pre-activations are Gaussian random variables with zero mean and covariance between neurons is zero. The covariance matrices can be computed efficiently using a recurrence relation, involving O(B^2) two-dimensional integrals. This method defines a computationally efficient approach for evaluating the covariance matrices. Eq. (2) provides a computationally efficient method for computing neural network statistics after random initialization, focusing on dimensionality reduction. It defines a dynamical system over covariance matrices, simplifying the study of random feed-forward networks. Investigating Eq. (2) near its fixed points reveals insights, with a common fixed point structure of \u03a3 * = q * [(1 \u2212 c * )I + c * 11T]. The structure of fixed points in neural networks with batch symmetry breaking is discussed, with BSB1 fixed points having permutation symmetry. BSBk fixed points with fewer symmetries may also be preferred in batch normalization. In deep networks, all inputs yield pre-activations of identical norm and angle at these fixed points, making them untrainable. Local Convergence to BSB1 Fixed Point is analyzed by considering the Taylor series in the deviation from the fixed point. The evolution of the matrix operator in batch normalization must be evaluated as a whole, similar to the study of convolutional networks. The dynamics induced by the eigenvalues of J control the evolution of the covariance matrix in batch normalization. If all eigenvalues of J are less than 1, the fixed point will be stable and approach zero exponentially. The number of layers for convergence will be determined by -1/log(\u03bb1). If any eigenvalue is greater than 1, the fixed point will be unstable, requiring identification of a different stable fixed point. Computing the eigenvalues of J allows for immediate understanding of the dynamics. The Diagonal-Offdiagonal Semidirect (DOS) operator simplifies the eigendecomposition with two eigenspaces for changes in the off-diagonal and diagonal elements. It is a form of ultrasymmetric operators, important for batchnorm analysis. Gradient dynamics theory is developed using backpropagation to propagate gradients efficiently through the network. The DOS operator simplifies eigendecomposition for changes in diagonal and off-diagonal elements, crucial for batchnorm analysis. Gradient dynamics theory uses backpropagation to efficiently propagate gradients through the network, leveraging statistics to understand error signals. The mean field formalism is extended to include batch normalization in neural networks, with modified equations and parameters to prevent division by zero. Pre-activations are invariant to certain parameters, and batch normalization introduces hyperparameters that can be set as constants. To avoid degenerate results, a minimum batch size is assumed. In the context of batch normalization in neural networks, theoretical tools such as the Laplace method, Fourier method, spherical integration, and the Gegenbauer method are presented to analyze high dimensional integrals. The pre-activations in the network are jointly Gaussian as the width grows, with a projection operator introduced to handle the nonlinearity. The minimum batch size is assumed to avoid degenerate results. The Laplace, Fourier, spherical integration, and Gegenbauer methods are used in analyzing high dimensional integrals in batch normalization for neural networks. The Laplace method simplifies expressions with positive homogeneous \u03c6, while spherical integration is useful for numerical evaluation. The Gegenbauer method expresses objects in terms of Gegenbauer coefficients of \u03c6. The matrix \u03a3 defines the projection \u03a3 G = G\u03a3G, with analytic recurrence relations for random neural networks with batch normalization. The fixed point structure of Eq. (8) reveals unique solutions in terms of Gegenbauer basis, providing concise closed forms for activation functions. The activation functions with degree \u03b1 positive homogeneous properties lead to concise closed forms. For ReLU, Theorem 3.5 (BSB1 fixed point for ReLU) is obtained. The eigenvalues of d\u03a3 \u03a3=\u03a3 * can be determined by considering batch normalization. The function n(h) normalizes a centered h by its standard deviation. The Jacobian can be rewritten using the chain rule. It is advantageous to study DISPLAYFORM1 as the nonzero eigenvalues are identical. The nonzero eigenvalues of the object DISPLAYFORM1 are identical to the Jacobian's nonzero eigenvalues. The BSB1 \u03a3 * permutation symmetry simplifies the analysis of the complex object. Ultrasymmetric operators have specific symmetries that simplify their eigendecomposition. Theorem 3.6 states that ultrasymmetric matrix operators have an orthogonal eigendecomposition on symmetric matrices. The eigendecomposition of DISPLAYFORM0 under trace inner product includes an eigenspace \u03a3 with eigenvalue 0 and a 1-dimensional eigenspace RG with eigenvalue \u03bb DISPLAYFORM1 M. The eigenspaces are orthogonal, making DISPLAYFORM2 self-adjoint. In the context with T = \u0134, the eigenspaces represent components of deviation \u2206\u03a3 from the fixed point, capturing average norm, fluctuation of norms, and covariances in the batch. The RG-component decreases to 0 after 1 step due to explicit normalization of batchnorm. The eigenvalues of a deep ReLU-batchnorm network converge to a BSB1 fixed point as the batch size increases. The stability of the fixed point is analyzed using specific coefficients and methods like spherical integration and the Gegenbauer method. The BSB1 fixed point is not locally attracting under certain conditions, providing insight into its stability. The eigenvalues of a deep ReLU-batchnorm network converge to a BSB1 fixed point as the batch size increases. The coefficients show that \u03bb \u2191 M is typically much smaller than \u03bb \u2191 L, and there are exceptions like \u03c6 = sin. The larger a 1 is, the smaller \u03bb \u2191 L is, leading to convergence to a BSB1 fixed point. Exploding gradients at initialization are a severe problem in networks with batch normalization. Fully-connected networks with batch normalization exhibit exploding gradients for any choice of nonlinearity, converging to a BSB1 fixed point. The backpropagation equation is modified due to the activation functions no longer acting point-wise on pre-activations. The resulting covariance matrix is computed using a recurrence relation, with a defined linear operator for any vector-indexed linear operator. The behavior of gradients near the fixed point of a linear operator in fully-connected networks with batch normalization is studied. The dynamics are determined by eigenvalues, and the composition involves three operations. The eigenvalues can be computed using a Gegenbauer expansion, which requires a new identity. The Gegenbauer expansion is used to compute eigenvalues in fully-connected networks with batch normalization. A new identity involving Gegenbauer polynomials integrated over a sphere is required. Gradients explode at a certain rate, contrasting with non-normalized networks. Batch normalization forces layer Jacobian singular values away from 1, disproving a previous conjecture. Appendix G.1 discusses numerical evaluation of eigenvalues. In ReLU-batchnorm networks, the gradient norm explodes exponentially at a rate of DISPLAYFORM5, decreasing to approximately 1.467 as B approaches infinity. Weight gradients also explode at the same rate as the hidden preactivations, with the weight gradient norms at layer l being \u03a0 l, \u00b5 * G = \u00b5 * tr \u03a0 l. The effect of \u03bb on gradient explosion is investigated, although it is typically treated as a small constant and not tuned as a hyperparameter. The analysis investigates the effect of \u03bb on gradient explosion in ReLU-batchnorm networks. Larger values of \u03bb can help reduce gradient explosion issues. The correlation between preactivations of samples in a batch and different batches is also studied. The dynamics can be generalized to simultaneous propagation of k batches, with diagonal and off-diagonal blocks representing within-batch and cross-batch covariance. Empirical observations show a clear jump in variance at the transition. Representative covariance matrices for the two phases are plotted. Figure A.1 displays the impact of batch normalization on the input-output map of a linear network. The network acts on two minibatches of size 64, with one perturbed datapoint in each minibatch. Batch normalization causes nonlinear changes in the coefficients of the affine transformation, resulting in an ellipse transformation of a circle in input space. Each pane shows activations at a given layer for all datapoints in the minibatch. The scatterplot of activations in a given layer for all datapoints in the minibatch is projected onto the top two PCA directions. Batch normalization leads to a chaotic input-output map with increasing depth, causing exploding gradients in very deep networks. The correlation between two minibatches decreases with depth due to batch norm nonlinearity. The network forms a 2D circle in input space, with each point perturbed separately in each minibatch. The network performs an affine transformation on its inputs, causing the input space to remain an ellipse. Due to batch normalization, the affine transformation changes nonlinearly as datapoints in the minibatch vary. This chaotic map leads to exploding gradients in deep networks when batch normalization is applied. The correlation between two minibatches changes with depth in a linear network due to batch normalization. Scatterplots of activations at each layer show how the affine transformation on inputs changes nonlinearly as datapoints vary in the minibatch. Batch normalization causes chaotic input-output maps due to the nonlinearity, leading to exploding gradients in deep networks. The correlation between minibatches decreases with depth, despite being near one at the input layer. Gradient norms equilibrate after initialization, reducing the impact of gradient explosion. Batch normalization leads to chaotic input-output maps with increasing depth in neural networks. A linear network with batch norm shows nonlinear changes in coefficients for affine transformations as datapoints in the minibatch are altered. Scatterplots of activations at each layer for all datapoints in the minibatch are shown, revealing the impact of batch norm on the network's behavior. PCA directions are computed using the concatenation of two minibatches, which grow increasingly dissimilar with depth due to batch norm nonlinearity. Despite a correlation near one at the input layer, the two minibatches rapidly decorrelate with depth in the network. The global fixed point of this dynamics is cross-batch BSB1, with diagonal BSB1 blocks and off-diagonal entries equal to a constant c. After mean centering preactivations, the covariance matrix in deep batchnorm networks becomes a multiple of identity, leading to exponential loss of correlation information between input batches as signal propagates through the network. This loss occurs rapidly with depth, regardless of the nonlinearity used. This contrasts with vanilla networks, where information loss can be controlled by adjusting initialization variances. The absence of coordinatewise nonlinearity maximally suppresses information loss and gradient explosion in neural networks with batch normalization. The trainability of these networks is controlled by gradient explosion, quantified by the depth scale over which gradients explode. Strong agreement is found between this depth scale and the maximum trainable depth. In experiments on rectified linear networks, the relationship between trainability and initialization was investigated for different batch sizes. Results showed that networks deeper than 50 layers were untrainable regardless of batch size. Additionally, gradients in networks with batch normalization quickly reached dynamical equilibrium, as shown in Figure 4. During the first 10 steps of training, networks of varying depths show a nearly identical trainable region, even at intermediate depths where significant gradient explosion occurs. The experiment records weights, gradients, and pre-activations, revealing constant weight norms before learning. This behavior is counterintuitive but consistent across different depths. After initial training steps, networks of different depths exhibit similar trainable regions despite gradient explosion. The weights remain constant, while gradients grow exponentially. Batchnorm scaling leads to stable gradients and exponential weight growth. Despite initial ill-conditioned gradients, networks quickly reach a stable equilibrium, benefiting shallower networks more than deeper ones. The gradient vanishing can cause lower layers to remain constant during training, leading to issues for networks deeper than 50 layers. Batch normalization results in exploding gradients for nonlinearity converging to a BSB1 fixed point. Experiments in FIG8 show ways to address gradient explosion by tuning activation function nonlinearity. In the \u03b3 \u2192 0 limit, the function is linear, and in the \u03b2 \u2192 \u221e limit, the function is also linear. The maximum trainable depth increases significantly with decreasing \u03b3 and increasing \u03b2. Experiments with tanh and rectified linear networks show a critical point where gradients do not explode, enabling the training of very deep networks with batch normalization. The study presents a theory for neural networks with batch normalization at initialization, revealing counterintuitive aspects and methods to reduce gradient explosion for training significantly deeper networks. Batch normalization enables training of deeper networks by creating a chaotic input-output map with increasing depth. A linear network with batch norm acts on two minibatches, transforming a circle in input space into an ellipse. The coefficients of this transformation change nonlinearly due to batch norm, allowing for more advanced network architectures in the future. The chaotic input-output map created by batch normalization leads to exploding gradients in deep networks. The correlation between minibatches decreases with depth, allowing for more advanced network architectures. After initialization, batchnorm causes gradient explosion in deep networks. The relative gradient norms equilibrate for weight and scale parameters. Training and validation accuracy are analyzed with different learning rates and initialization epochs. The study follows prior literature on backprop computations. In backprop computations, the assumption of multiplying by an iid copy instead of W T is rigorously justified for various architectures like multilayer perceptron, residual networks, and convolutional networks. This assumption does not extend to batchnorm due to singularity in its Jacobian at 0, but experiments suggest a possible proof can be found. The Jacobian singularity at 0 in batchnorm poses a challenge, but experiments indicate a potential extension of Yang & Schoenholz (2017)'s work. Definitions and notations for batchnorm and coordinatewise applications are provided, along with the use of projection matrices and functions like the Gamma and Beta functions. In the context of batchnorm, various dynamics induced by batchnorm in a fully-connected network with random weights are studied, including forward and backward propagation equations. Different ways of analyzing these dynamics are explored in the following sections. The text discusses different methods for analyzing the dynamics induced by batchnorm in a fully-connected network with random weights. The Laplace Method simplifies quantities involving integrals, while the Fourier Method uses Fourier expansion for polynomially bounded nonlinearity. The text discusses methods for analyzing dynamics induced by batchnorm in a fully-connected network with random weights, including spherical integration and the Gegenbauer method for expressing nonlinearity in a basis. The Gegenbauer method is used to express nonlinearity in a basis for analyzing dynamics induced by batchnorm in a fully-connected network with random weights. It reveals that the eigenvalues of the forward and backward dynamics are ratios of quadratic forms of Gegenbauer coefficients, highlighting the necessity of gradient explosion. Numerical computations benefit from 1-dimensional integrals for coefficient calculation, but slow-decaying coefficients may require many integrals for accuracy. The convergence rate of forward dynamics and gradient explosion of backward dynamics involve linear operators on matrices. In Appendix E.5, ultrasymmetric operators with symmetries are studied, leading to structural results crucial for deriving dynamics' asymptotics. Further analysis of forward and backward dynamics is done in Appendices F to I, focusing on Eq. FORMULA2 convergence to a BSB1 fixed point with B \u2265 4. Appendices J and K explore dynamics outside this regime and BSB2 fixed point dynamics, respectively. The backward dynamics studied involves gradients with respect to hidden preactivations. The curr_chunk discusses the computation of weight gradients and the global convergence of Eq. (43) to BSB1 fixed point for specific cases. It also delves into the local convergence properties of BSB1 fixed points using various methods such as spherical integration, the Laplace method, and the Gegenbauer method. Additionally, it explores the Gegenbauer expansion of the BSB1 local convergence rate and discusses gradient dynamics. The curr_chunk discusses gradient dynamics and convergence rates using spherical integration, the Gegenbauer method, and the Laplace method. It covers the computation of weight gradients, cross batch covariances, and correlation between gradients of two batches. Key techniques and tools are introduced for these analyses. The Laplace method is useful for deriving closed form expressions for positive homogeneous \u03c6. Schwinger parametrization is introduced to deal with normalization. Lemma E.1 states the key lemma in the Laplace method, proving the well-defined and continuous nature of \u2118(\u03a3) for full rank \u03a3. The proof involves exchanging the order of integration using Fubini-Tonelli's theorem. The Laplace method involves proving the continuity of \u2118(\u03a3) for full rank \u03a3 by exchanging the order of integration using Fubini-Tonelli's theorem. The function s \u2192 E[ f (y) : y \u223c N (0, \u03a3(I + 2s\u03a3) \u22121 )] is continuous, and \u2118(\u03a3) is defined whenever rank \u03a3 > 2k, with continuity established by dominated convergence. The Laplace method proves the continuity of \u2118(\u03a3) for full rank \u03a3 by exchanging the order of integration. The function s \u2192 E[ f (y) : y \u223c N (0, \u03a3(I + 2s\u03a3) \u22121 )] is continuous, and \u2118(\u03a3) is defined whenever rank \u03a3 > 2k, with continuity established by dominated convergence. The integral exists if rank \u03a3/2 > k, and Eq. (25) holds for all rank \u03a3 > 2k.\u03c6 is degree-\u03b1 positive homogeneous, and Lemma E.2 can be applied with k = \u03b1. Matrix simplification is done by simplifying the expression G(I + 2s\u03a3G) \u22121 \u03a3G, leveraging the fact that G is a projection matrix. Definition E.3 introduces matrix e as an orthonormal basis of an orthogonal matrix. The Laplace method establishes the continuity of \u2118(\u03a3) for full rank \u03a3 by rearranging integration. Positive homogeneous functions are crucial, with ReLU being a common example. These functions can be represented as a combination of powers of ReLU and its reflection. The text discusses the \u03b1-ReLU function and its integral transforms, specifically focusing on V and W transforms of \u03c1 \u03b1. It also introduces Propositions and Definitions related to these transforms. The text discusses the properties of the \u03b1-ReLU function, including its increasing and convex nature for \u03b1 > -1/2. It also mentions the fixed point structure of J\u03b1(c) = c for \u03b1 \u2208 [1/2, 1), with both unstable and stable solutions. Additionally, it highlights the relations between \u03b1-ReLUs, such as the ability to differentiate between \u03b1 values. The text discusses the properties of the \u03b1-ReLU function, including differentiation between \u03b1 values and expressing functions as linear combinations of powers of \u03b1-ReLUs. The text discusses positive-homogeneity and expressing functions as linear combinations of powers of \u03b1-ReLUs for degree \u03b1 positive-homogeneous functions on R \\ {0}. It also shows how this generalizes to PSD matrices of arbitrary dimension. The text discusses positive-homogeneity and expressing functions as linear combinations of powers of \u03b1-ReLUs for degree \u03b1 positive-homogeneous functions on R \\ {0}. It also generalizes to PSD matrices of arbitrary dimension. The function \u03c6 restricted to R \\ {0} can be written as x \u2192 a\u03c1 \u03b1 (x) \u2212 b\u03c1 \u03b1 (\u2212x) for some a and b. Let \u03a3 \u2208 S B. The partial derivatives of V \u03c6 are recorded. The text discusses positive-homogeneity and expressing functions as linear combinations of powers of \u03b1-ReLUs for degree \u03b1 positive-homogeneous functions on R \\ {0}. It also generalizes to PSD matrices of arbitrary dimension. The Laplace method crucially used the fact that we can pull out the norm factor Gh out of \u03c6, so that we can apply Schwinger parametrization. For general \u03c6 this is not possible, but we can apply some wishful thinking and proceed as follows. Finally, in Eq. (31), we need to extend the definition of Gaussian to complex covariance matrices, via complex Gaussian integration. The text extends the definition of Gaussian to complex covariance matrices through complex Gaussian integration. It defines DISPLAYFORM14 for general complex \u03a3, assuming \u03a3 is nonsingular and the integral exists. The derivation is not mathematically correct due to other points, but the end result can be justified rigorously. DISPLAYFORM16 is uniformly bounded, and DISPLAYFORM17, DISPLAYFORM18, and DISPLAYFORM19 exist and are finite for each r > 0. If G is the mean-centering projection matrix and \u03a3 G = G\u03a3G, then by the same reasoning as in Thm E.5, DISPLAYFORM21 holds. The text discusses the conditions under which certain assumptions are satisfied for coordinatewise nonlinearities like ReLU, identity, and tanh. It warns against swapping the order of integration due to divergence issues. The proof for the first equation is shown using dominated convergence and a nonnegative bump function. The text demonstrates the use of dominated convergence to prove the equation DISPLAYFORM29, utilizing a dominating integrable function and the Fubini-Tonelli theorem to swap the order of integration. The text discusses spherical integration using angular coordinates in R. It defines the spherical angular coordinates and the integration element in this coordinate system. It also provides helpful integrals for simplifying expressions involving spherical integration. The text discusses spherical integration using angular coordinates in R, defining the spherical angular coordinates and integration element. It provides helpful integrals for simplifying expressions involving spherical integration, including antisymmetry of cos and change of coordinates. The text discusses simplifying high-dimensional spherical integrals using angular coordinates and symmetry. Lemmas are provided to simplify integrals without trigonometry, using change of coordinates. The proof of Lemma E.35 is shown, along with Lemmas E.36 and E.37. The Gegenbauer polynomials are orthogonal polynomials with respect to a weight function. They are normalized and satisfy specific identities. They are related to zonal harmonics, which are spherical harmonics depending only on the height along a fixed axis. This is useful for the Gegenbauer expansion and local convergence rate analysis. The Gegenbauer polynomials are orthogonal with specific identities. A new identity is presented showing a quadratic form of \u03c6, dependent on its derivative, is diagonalized in the Gegenbauer basis. This is crucial for proving gradient explosion under the BSB1 fixed point assumption. Lemmas E.48 and E.49 are used to prove Theorem E.47, showing specific sequences and conditions for u1, u2 in SB-2. The text discusses the concept of BSB1 fixed points and matrices with specific structures. It introduces the definition of BSB1 matrices and their spectrum, along with relevant lemmas and equations. The text discusses the concept of ultrasymmetry in matrices, specifically focusing on the eigenvalues and eigenspaces of a B \u00d7 B matrix. Lemma E.51 states that a matrix of the form \u00b5I + \u03bd11 T has eigenvalues \u00b5 and B\u03bd + \u00b5. Lemma E.52 shows that matrices G and BSB1 B (a, b) can be simultaneously diagonalized. The text also mentions the isotropic nature of BSB1 covariance matrices and introduces the concept of ultrasymmetry in tensor objects. In the context of ultrasymmetry in matrices, the text discusses the eigendecomposition of an ultrasymmetric T and the projection for the space of symmetric matrices \u03a3 of dimension B. It introduces the concept of normalized representations and defines eigenspaces for T. The text also mentions the orthogonal decomposition w.r.t Frobenius inner product and defines matrices GLG as the sum of outer products. The text discusses the eigendecomposition of ultrasymmetric operators and the linear span of matrix orbits under simultaneous permutations. It also presents the calculation of diagonal and off-diagonal entries of a matrix. The text explores the eigendecomposition of ultrasymmetric operators and the linear span of matrix orbits under simultaneous permutations. It also discusses the calculation of diagonal and off-diagonal entries of a matrix, along with the properties of eigenmatrices and eigenvectors under transformations. The text discusses the eigendecomposition of ultrasymmetric operators and the properties of eigenspaces. It highlights that eigenspaces may not be orthogonal under trace inner product, but become self-adjoint after projection by G. Theorem E.62 presents the eigendecomposition of an ultrasymmetric linear operator T, with eigenspaces RG and L having specific eigenvalues. The proof is similar to that of Thm E.61, and the eigenspaces are shown to be orthogonal. The ultrasymmetric operator T is self-adjoint when G \u22972 \u2022 T H G. A simpler form of ultrasymmetric operators is defined as diagonal-off-diagonal semidirect (DOS). Theorems E.62 and E.61 still apply to DOS operators, with Lemma E.65 providing specific computations for eigenvectors. Lemma E.67 states that for any L B (B - 2, 1)-shaped matrix L, the desired results can be obtained using permutation symmetry. Lemma E.68 shows that the linear map (a, b) \u2192 (ua, wb + 2va) has eigenvalues u and w with corresponding eigenvectors (u - w, 2v) and (0, 1). Lemma E.69 further explains the eigenvectors of the ultrasymmetric operator T. Definition E.71 defines the set M B as matrices in S B with Diag\u03a3 = 0. Theorem E.72 states that for DOS B (u, v, w) with w = u, the eigendecomposition of T H B includes L B (w-u, v) as an eigenspace with eigenvalue u. This eigenspace is spanned by L B (a, b) where a = u(w-u) and b = uv. The eigendecomposition also includes RG with eigenvalue DISPLAYFORM12. The eigendecomposition of the matrices studied involves eigenvalues and eigenspaces, with a focus on dynamics of PSD matrices. The convergence, limit points, and speed of convergence are analyzed, particularly when the function \u03c6 is the identity function. The key lemma shows exponential convergence, with a constant related to the largest and smallest eigenvalues. The eigenvalues of \u03a3 are computed, showing that they fall in decreasing order. The eigenvalues of \u03a3 are proven to be equal to the eigenvalues of \u03a3, leading to a log-convex function T(\u03bb). The log-convex function T(\u03bb) is strictly convex in \u03bb and maximized by its extremal points due to strict convexity. The unit sum condition is derived from the normalization of the iteration map. The extremal points of A are shown to be \u03c9 k, leading to T(\u03bb) \u2264 1 with equality at \u03bb = \u03c9 k. The gap between \u03bb 1 l and \u03bb A l approaches 0 as l \u2192 \u221e, with two cases considered for the behavior of \u03bb A l. The log-convex function T(\u03bb) is strictly convex and maximized by extremal points. The gap between \u03bb 1 l and \u03bb A l approaches 0 as l \u2192 \u221e. Asymptotically, the gap decreases exponentially, proving convergence.\u03bb A l converging to 0 leads to a contradiction, showing the exponential convergence of the gap. The convergence behavior of Eq. (43) is characterized by exponential convergence, where the gap between eigenvalues decreases exponentially as l \u2192 \u221e. The dynamics are described by a diagonal matrix D with no zero entries and an orthonormal basis matrix \u00ea. The basin of attraction for general \u03c6 remains uncharacterized, requiring manual identification of fixed points. Batchnorm B \u03c6 is permutation-equivariant, suggesting a look into BSB1 fixed points \u03a3*. The main result of this section provides an expression for the fixed point entries, allowing for numerical computation. A symmetry argument leads to Lemma F.4, involving a symmetric function \u03a6 and the calculation of E[\u03a6(X)\u03a6(X)\u1d40]. The function \u03a6 is permutation-equivariant, with E[\u03a6(X)\u03a6(X)\u1d40] = \u00b5I + \u03bd11\u1d40. For any BSB1 \u03a3, V B \u03c6(\u03a3) is the unique fixed point of Eq. (43). The expression for V B \u03c6(\u03a3) depends on \u03b8 1 and \u03b8 2. Laplace's method can be applied for positive-homogeneous \u03c6 to obtain a nicer form of the BSB1 fixed point. The BSB1 fixed point is described in a closed form for a positive homogeneous function \u03c6 using the J function. The diagonal and off-diagonal entries of the fixed point matrix are determined by K \u03b1,B J \u03c6 (1) and c \u03b1 respectively. The existence and uniqueness of the fixed point are proven, and its relationship with the matrix G is established. If \u03a3 * is the BSB1 fixed point of Eq. (43) from Thm F.8, then G\u03a3 * G = \u00b5 * I B\u22121 where \u00b5. Corollary F.10 states that for any BSB1 \u03a3 \u2208 S B , V B relu (\u03a3) is BSB1 with diagonal entries 1/2 and off-diagonal entries. Corollary F.11 shows that for any BSB1 \u03a3 \u2208 S B , V B id (\u03a3) is BSB1 with diagonal entries 1 and off-diagonal entries. The Laplace method cannot be easily adapted for the Fourier method due to the lack of a simple relation between V \u03c6 (c\u03a3) and V \u03c6 (\u03a3). The BSB1 fixed point can be described using Gegenbauer coefficients of \u03c6, independent of the values of \u03a3. In this section, linearization of dynamics is considered by examining linear operators on the space of PSD linear operators S B. The Jacobian of V B \u03c6 at its BSB1 fixed point is discussed, along with helper results to aid in understanding batchnorm. Now we prepare helper results to understand batchnorm. Define n(x) = \u221a Bx/x, where B is the sample standard deviation. Batchnorm B \u03c6 can be broken down into three steps: meancentering, division by standard deviation, and coordinate-wise application of nonlinearity. The unique fixed point \u03a3* is related to the eigendecomposition of U. The eigenvalues of BA can be recovered from those of U, as shown in Thm E.61. The eigenvalues of BA can be recovered from those of U. By symmetry, Bv cannot be zero, contradicting the fact that v is an eigenvector with a nonzero eigenvalue. The eigenspaces with different eigenvalues are linearly independent. Lemma F.18 states that for any invertible \u03a3, a measurable function f, and \u039b, the eigenvectors of BA with nonzero eigenvalues are all linearly independent. For a measurable function f : R B \u2192 R A and invertible \u03a3 \u2208 S B, the eigenvalues of G \u22972 \u2022 U can be computed using Thm E.61. By Lemma E.35, the eigenvectors of BA with nonzero eigenvalues are linearly independent. The eigenvalues of G \u22972 \u2022 U can be computed using Thm E.61, and the eigenvectors of BA with nonzero eigenvalues are linearly independent. The local convergence rate is computed via Gegenbauer expansion, and the eigenvalues associated with L(B \u2212 2, 1) are determined using Lemma E.60. The eigenvalue associated with L(B \u2212 2, 1) can be evaluated using Lemma E.60 and is a ratio of quadratic forms. The eigenvalue is typically \u2265 1 when \u03c6( \u221a B \u2212 1x) explodes as x \u2192 1 or x \u2192 \u22121, indicating the explosiveness of \u03c6 affects the eigenvalue. The explosiveness of \u03c6 as x approaches 1 or -1 affects the convergence to a BSB1 fixed point. For large B and l, the eigenvalue is positive. The more \"linear\" \u03c6 is, the larger a21 is compared to the rest of {al}l, making the eigenvalue < 1. Higher degree Gegenbauer polynomials explode more violently as x \u2192 \u00b11, supporting the claim. The text discusses evaluating values of \u00c312(12), \u00c312(13), \u00c312(34) by exploiting linear dependences. Symmetries of \u00c3 lead to equations simplifying the values, allowing for Gegenbauer expansions. The proof involves rearranging sums and combining results to obtain a final expression. The eigenvalue of U with respect to the eigenspace M is typically much smaller than the eigenvalue for L. By differentiating Eq. (26) at G \u22972 {\u03a3 * } = G\u03a3 * G, a smooth path \u03a3 t \u2208 S G B is considered. Trivial simplifications lead to a Lemma regarding the time derivatives \u03a3 and \u03a3. Lemma F.25 states that for a positive-homogeneous function \u03c6 of degree \u03b1, a smooth path is considered with a straightforward computation. Lemma F.27 and F.28 also involve straightforward computations. Theorem F.29 and F.30 provide eigendecompositions for functions with finite Gaussian moments and positive-homogeneous functions of degree \u03b1, respectively. The proof involves computations using Proposition E.22 and Theorem E.73. The eigendecomposition of H B \u2192 H B is shown using Thm E.72 and Thm F.30. Theorem F.33 discusses eigenvalues of positive-homogeneous functions. Proposition F.34 is obtained with routine computation as B \u2192 \u221e. Proposition F.34 states that for \u03b1 \u2265 1, the maximum eigenvalue of U is always achieved by eigenspace L for large enough B. The definition of the V operator is extended to act on matrices \u03a0. The adjoint of a linear operator is defined as the space of linear functionals. The backward dynamics are described by an equation involving \u03a3. The backward dynamics are studied through the equation involving \u03a3, focusing on exponential convergence to a BSB1 fixed point. The linear system approximation is analyzed, with emphasis on the eigendecomposition of the adjoint of the dynamics. Basic calculations are made to derive key relationships. The text discusses the application of chain rule to derive key relationships involving diagonal matrices and fixed points. Integration techniques are used to simplify calculations, leading to the evaluation of eigenvalues. The focus is on results obtained from various methods, with a simplification observed when a specific condition is met. The text discusses expressing F(\u03a3 * ) aa in terms of Gegenbauer basis by lifting the spherical integral over S B\u22122 to S B. However, this technique cannot be extended to F(\u03a3 * ) ab, a = b. The text also mentions leveraging Thm E.47 to handle F(\u03a3 * ){G} ab with a weight factor. In Thm E.47, by setting u1 = u2 = a, we obtain DISPLAYFORM9. Similarly, setting u1 = a, u2 = b, gives DISPLAYFORM10. The eigenvalue of DISPLAYFORM11 is minimized when \u03c6 is linear. Assuming \u03c6 is degree \u03b1 positive-homogeneous, we can rewrite Eq. (52) as DISPLAYFORM0, where \u03a3* is the BSB1 fixed point of Eq. (43). Each term in the sum is ultrasymmetric and has the same eigenspaces RG, M, L. The eigenvalues for each term can be computed in order using Lemma E.2. Lemma E.2 is applied to relate the quantity to V \u03c6. Thm E.73 provides the eigendecomposition for a specific matrix. Further simplifications are made using Proposition E.22. The computation involves defining a measurable function and considering a specific identity. The desired result is a positive-homogeneous degree \u03b1 + 1 function. The eigendecomposition of DISPLAYFORM10 DISPLAYFORM11 yields eigenspaces RG and M with eigenvalues DISPLAYFORM12 and DISPLAYFORM13 respectively. The value of \u03bb B G is justified by computing C using Lemma E.2. For a smooth path \u03a0 t in H B with \u03a0 0 = I, \u03c6(y) = \u03b1 \u22121 Diag(\u03c6 (y))y. The eigendecomposition of C shows eigenspaces RG and M with eigenvalues DISPLAYFORM19 and DISPLAYFORM20. This implies that DISPLAYFORM21 has eigenspaces RG, M, L with eigenvalues DISPLAYFORM22. In this section, the extended dynamics on multiple batches are studied, focusing on convergence behavior and fixed points. A matrix \u03a3 is considered CBSB1 if it has a specific block structure. The fixed point \u03a3* is analyzed, with a proof provided for k = 2. The fixed point \u03a3* is studied for k = 2, with a proof provided. The dynamics of diagonal blocks are discussed, showing independence between certain blocks. Gegenbauer expansion is used to compute a positive homogeneous function, leading to the desired result. Thm H.3 provides the desired result for positive homogeneous functions. The Laplace method is not necessary for computing c* as per Corollary H.4. The computation is useful for understanding eigenvalue computation. For A, B, C \u2208 N, let f : DISPLAYFORM0. \u2118(\u03a3) on {\u03a3 \u2208 S A+B : rank \u03a3 > 2(a + b)} is well-defined, continuous, and satisfies DISPLAYFORM1. The proof involves the Fubini-Tonelli theorem for full rank \u03a3. The equation simplifies to (y, z) \u223c N (0, (\u03a3 \u22121 + 2D 2 ) \u22121 ). The case of general \u03a3 with rank \u03a3 > 2(a + b) is addressed using continuity arguments. Theorem H.6 expands on Corollary H.4. Theorem H.6 expands on Corollary H.4 by providing additional details. It involves computing the cross-batch block and obtaining the eigendecomposition of F. The eigen-properties of BDOS operators are discussed before specializing to the case of F. The corollary indicates that stacking batchnorm in a deep neural network leads to chaotic behavior, with cross batch correlation decreasing exponentially with depth. The optimal function to mitigate this loss of information is linear. For batch sizes less than 4, batchnorm may not be well-defined, with specific outcomes for batch sizes of 1 and 2. The gradient behavior of batchnorm networks transitions abruptly from vanishing for B = 2 to explosion for B \u2265 4. Empirical observations suggest that B = 3 exhibits similar explosion as B = 4, with symmetry breaking occurring in nonlinearities other than BSB1. Symmetry breaking is necessary for convergence to a fixed point, with the dominant block appearing in different parts of the matrix \u03a3. In real networks, symmetry is broken by network weight randomness, leading to large changes in output due to small input fluctuations. The gradient is large, especially at BSB2 fixed points, where dominant blocks undergo similar dynamics as BSB1 fixed points. Appendix K delves into understanding BSB2 fixed points, with a focus on a specific form with a 1 \u00d7 1 dominant block. Finite width effects can cause different favored fixed points between large and small width limits for certain nonlinearities. The text discusses the dynamics of eigenspaces and eigenvalues in BSB2 matrices, focusing on the difficulty of obtaining fixed points analytically. The Gegenbauer method used for BSB1 matrices does not directly apply to BSB2 matrices due to differences in eigenvalues. As a result, there are no rigorous results on the BSB2 case currently, but it is expected that the main block of the BSB2 fixed point will undergo similar dynamics to that of a BSB1 fixed point. The main block of the BSB2 fixed point is expected to undergo dynamics similar to a BSB1 fixed point, leading to gradient explosion. Pathological gradient vanishing may occur in certain situations, resembling the gradient vanishing for B = 2 batchnorm. Corollary K.3 can be extended to the B = 1 case, with further details provided by Thm K.4."
}