{
    "title": "r11Q2SlRW",
    "content": "Our real-time method, the auto-conditioned Recurrent Neural Network (acRNN), can synthesize highly complex human motions, including dances and martial arts, by explicitly addressing autoregressive noise accumulation during training. Unlike existing methods, our approach can generate over 18,000 continuous frames (300 seconds) of new complex human motion with different styles. The synthesis of realistic human motion has gained interest with applications beyond animation and video games. Challenges include generating new variations of motions while preserving a certain style, aided by large human-motion capture databases and database-driven frameworks. Our paper introduces a robust framework using neural networks to synthesize highly complex human motion variations of arbitrary styles like dancing and martial arts without relying on a database. Our paper presents a novel deep auto-conditioned RNN network architecture to synthesize human motion variations like dancing and martial arts. Existing methods using RNNs for this task produce realistic output but suffer from unrealistic motion after a few seconds due to error accumulation. The acRNN structure addresses the issue of error accumulation in RNN training by linking the network's predicted output into its future input streams. This allows the network to output long sequences without failure, such as generating motion for hundreds of seconds. The method is lightweight and can be used with any RNN based learning scheme, presenting a new approach to RNN training. The new RNN training method can generate indefinitely long sequences of realistic human motions with different styles. Various approaches have been developed over the years for generating realistic human motion, including simulation-based techniques that consider physical constraints on the human skeleton. Various approaches have been developed for generating realistic human motion, including simulation-based techniques that consider physical constraints on the human skeleton. Some recent works have attempted to use less rigid objectives, employing adversarial and reinforcement learning. However, these motions often look uncanny and not human-like. Motion graphs can generate locomotion along arbitrary paths but are limited to producing memorized sequences. Frameworks based on linear and kernel methods are also being explored. Various methods have been used to generate realistic human motion, including linear and kernel frameworks, Gaussian Process Latent Variable Models, and reinforcement learning in reduced spaces. Different techniques such as conditional restricted boltzmann machines and motion-fields have been employed for interactive control of character movement and real-time style transfers. Additionally, binary latent variables and physical parameter estimation from motion capture data have been explored. However, database methods have limitations in creating new variations of motion. Deep Learning Approaches like recurrent networks are used for human motion prediction and generation. Encoder-recurrent-decoder (ERD) is proposed to learn skeleton embedding and sequential information for realistic body positions. RNNs are also employed to learn spatio-temporal graphs of interaction in human skeletons. The curr_chunk discusses the limitations of previous approaches in learning dance sequences and motion generation, highlighting the problem of converging to a mean position over time. The proposed training method aims to address this issue by ensuring that motion remains recognizable and continuous. Holden et al. demonstrate the use of an autoencoder trained on the CMU dataset to learn a manifold of human motion, extending the work with a deep convolutional auto-encoder and a disambiguation network. The curr_chunk discusses using a deep convolutional auto-encoder and a disambiguation network to generate realistic walking motion and perform style transfer. It can also be used for tasks like punching and kicking. The method does not use an RNN structure and can only generate fixed length sequences. Holden et al. use a Phase-Functioned Neural Network to produce motion along a user-defined path, considering the scene's geometry. In the experiments that follow, a special type of RNN called an \"LSTM\" is used for motion prediction. The network trained with this method is referred to as \"acLSTM\". The major drawback of using LSTM/RNN methods for motion prediction is the problem of error accumulation. During training, an RNN is conditioned on ground truth input sequences, leading to error accumulation issues during testing due to differences between input and ground truth. This problem is exacerbated by the backpropagation algorithm optimizing parameters based on ground truth input, causing output divergence or freezing. During testing, the network faces challenges due to novel situations not encountered during training, leading to poor performance. Previous methods struggle to produce realistic output quickly. Autoencoder frameworks can partially address this issue, but do not generalize well to long sequences. The acRNN explicitly deals with poor network output by using it during training, incorporating the network's own outputs periodically. During training, the network uses its own outputs periodically to improve performance. The method of BID0 also utilizes network output during training but does so stochastically without fixed condition lengths. Changing the condition/ground-truth length while keeping the proportion of ground-truth input fixed affects both the accuracy and variation of the output. During training, the network uses its own outputs periodically to improve performance. Changing the condition/ground-truth length affects accuracy and output variation. Auto-conditioning trains the network to produce longer sequences without additional input, forcing it to produce multiple frames simultaneously. The dataset used is the CMU motion-capture dataset containing 57 skeleton joint positions in 3D-space. The dataset used is the CMU motion-capture dataset with 57 skeleton joint positions in 3D-space. Joint positions are represented as relative distances to the root joint for better motion capture. All distances are stored in meters, using a skeleton with a height of 1.54 meters in a neutral pose for experiments. This representation ensures small Euclidean distances for frames with periodic motion. The acLSTM model is trained with three fully connected layers and a memory size of 1024. Ground-truth inputs of the time series are connected to instances of the network's output for subsequent input streams. The model is trained with a sequence length of 100 for 500000 iterations using the ADAM backpropagation algorithm on an NVIDIA 1080 GPU. Euclidean loss is used as the objective function. We use a 1080 GPU for training on different datasets, implementing the python caffe framework BID15. Sampling sequences at various frame rates and rotating them randomly to increase training size. The loss function is based on Euclidean distance, with an initial learning rate of 0.0001. Motion results are evaluated on networks trained on subsets from the CMU motion capture database. The study evaluates motion synthesis using subsets from the CMU motion capture database, including martial arts, Indian dance, Indian/salsa hybrid, and walking. Results show acLSTM performs well in long-term synthesis, especially for Indian dance. Comparison with other methods like LSTM-3LR and BID25 shows varying performance across different time frames. The stochasticity of human motion makes longer-term prediction challenging. TAB3 in the appendix displays the error difference between versions of the Indian dance network trained with different condition lengths. The graph in Figure 2 illustrates the average change between subsequent frames for acLSTM and the basic scheme, showing that acLSTM continues producing motion in the long-term while the basic scheme results in stagnated motion. Similarly, Figure 9 in the appendix shows the average change for the Indian dance network trained with different condition lengths, indicating that while methods like BID4 and BID25 do not freeze completely like the basic scheme, their motion becomes unrealistic at a certain point. The qualitative results show that the implementation of ERD and seq2seq can only generate motion for a short period before becoming unrealistic. Scheduled sampling BID0 performs better but also freezes eventually. Hybrid motions can be created by mixing training sets. The motion in the framework never permanently fails but short-term freezing may occur. Short-term freezing may occur in motion sequences generated by acLSTM, especially when there are jumping motions or aperiodic movements. However, the motion never completely diverges or freezes, and when stagnation occurs, it recovers relatively quickly. The acLSTM architecture can produce extended sequences of complex human motion without freezing, even after 20,000 frames of synthesis. This is demonstrated in Figure 5, showing a qualitative comparison with a basic LSTM-3LR trained on the Indian dance dataset. The vanilla network freezes at around 1000 ms, while our network continues to produce varied motion for the same time frame. The acLSTM architecture shows state-of-the-art results in generating complex human motion for extended periods, surpassing previous works limited to short sequences. Challenges include occasional choppy motion, self-collision, and unrealistic foot sliding. Further advancements in GAN methods like BID19 could enhance realism, but training difficulties such as mode collapse persist. Integration with physically based simulation may improve realism post-synthesis. Further analysis is needed to study the effects of using various condition lengths during training for the acLSTM architecture. Figure 9 suggests a potential trade-off between motion change over time and short-term motion prediction error. Further experiments with different condition lengths on various motion styles are necessary to draw meaningful conclusions."
}