{
    "title": "BJlr0j0ctX",
    "content": "Adversarial training is a strong defense against attacks, but generating examples for every mini-batch is costly. The study explores how this training improves classifier robustness and suggests using simple regularization methods like label smoothing and logit squeezing. Using simple regularization methods and Gaussian noise injection, researchers have achieved strong adversarial robustness in Deep Neural Networks without the need for adversarial examples. Adversarial examples have raised security concerns in computer vision systems, leading to the development of various defenses such as feature squeezing, denoising, and encoding to make models more robust to attacks. Adversarial training remains one of the most effective strategies for pre-processing images to remove adversarial perturbations. Various approaches focus on hardening neural classifiers, including specialized non-linearities, modified training processes, and gradient obfuscation. Adversarial training minimizes a loss function that measures model performance on clean and adversarial data, posing it as a game between two players. Adversarial training involves computing adversarial examples on each iteration, but the main drawback is the high computational cost. Training with strong adversarial examples is necessary to resist attacks, but it can be 10-100 times more time-consuming than standard training methods. Current state-of-the-art adversarial training methods use expensive iterative adversaries like Projected Gradient Descent (PGD) or Basic Iterative Method (BIM). This high cost makes it challenging to scale adversarial training. In this study, it is shown that strong robustness can be achieved using fast optimization without adversarial examples by applying standard regularization methods like label smoothing and logit squeezing. These inexpensive tricks, combined with random Gaussian noise, can match or surpass the performance of adversarial training on some datasets. For instance, a CIFAR-10 classifier achieved over 73% accuracy against black-box iterative attacks with just label smoothing and augmentation with random Gaussian noise. Regularized networks without adversarial training are more robust against non-iterative attacks and achieve higher accuracy on non-adversarial examples. The study compares the effectiveness of label smoothing, logit squeezing, and adversarial training in achieving robustness against attacks. Random Gaussian data augmentation is also explored for its importance in improving defense mechanisms. Random Gaussian data augmentation is explored for its importance in improving defense mechanisms for adversarial robustness. Adversarial training injects adversarial examples into the training data to decrease the logit corresponding to the correct class, aiming to preserve correct labeling under attack. Surprisingly, adversarial training decreases the logit, rather than creating a large \"logit gap\" as initially thought. Adversarial training decreases the logit gap by injecting adversarial examples into the training data. It aims to preserve correct labeling under attack by adding small perturbations to images, affecting the logits produced by a neural network. This approach helps in achieving adversarial robustness and understanding the impact of adversarial training on classifiers. The perturbation \u03b4 is determined by comparing the perturbed logit of the true class with other classes. Equation 2 is modified using this perturbation to measure robustness in predicting the smallest perturbation needed to switch the class of an image. The formula for L makes linearity assumptions. The formula for L accurately predicts the robustness of classifiers on CIFAR-10 and MNIST datasets. Strategies for hardening a classifier include increasing the logit gap, squashing adversarial gradients, and maximizing gradient coherence. Our experimental investigation reveals that adversarial training decreases the logit gap and gradient gap simultaneously, rather than increasing the numerator in the equation for achieving robustness. This strategy is observed in distributions of logit gaps for naturally and adversarially trained models on MNIST. The cross entropy loss limits adversarial training from increasing logit gaps, resulting in a decrease in classifier accuracy in the presence of adversarial examples. Adversarial training minimizes cross entropy loss by reducing logit gaps and adversarial gradients, increasing robustness. Logits shrink due to training with strong or weak adversaries, but not enough to fully explain the decrease in gap size. The text discusses the relationship between adversarial training, logit gaps, and gradient coherence. It explores the idea of directly decreasing the logit gap or logits themselves to crush adversarial gradients. Two approaches are mentioned: replicating the decrease in logit gap through label smoothing, and directly crushing all logit values through \"logit squeezing\" with a regularization term. Label smoothing, also known as \"logit squeezing,\" involves adding a regularization term to penalize large logits during training. This technique converts one-hot label vectors into one-warm vectors to encourage low-confidence classifications and reduce large logit gaps. It is commonly used to prevent overfitting and boost adversarial robustness in classification tasks. The smoothing parameter \u03b1 \u2208 [0, 1] determines the level of ambiguity in the decision-making process. Logit squeezing, a regularization technique, crushes logit values to mimic adversarial training. It adds a penalty term to the training objective to reduce large logits. While simple regularizers alone may harm adversarial robustness, combining them with other strategies can be highly effective. Adding Gaussian noise to images during training can improve the adversarial robustness of classifiers. The effects of Gaussian noise augmentation are more complex than expected, with label smoothing and logit squeezing becoming highly effective when combined with Gaussian noise. Training with Gaussian noise alone widens the logit gap and slightly decreases the gradient gap, as shown in robustness plots. The combination of Gaussian noise and label smoothing has a powerful synergistic effect, causing a dramatic drop in the gradient gap and a surge in robustness. Regularization methods can align adversarial gradients during training but may not be effective on images off the training data manifold at test time. Training the classifier on images with random perturbations enforces desired properties for input images off the manifold. Smaller noise levels lead to stronger synergistic effects and better robustness to attacks, while larger noise levels enable regularization properties to generalize further off the manifold. Results on MNIST and CIFAR-10 show the impact of different levels of Gaussian noise. Label smoothing reduces the variance of logits, leading to a decreased gradient gap. This results in smaller gradient magnitudes and more aligned gradients. Larger smoothing parameters further decrease the gradient magnitude and alignment. Logit squeezing with Gaussian augmentation also decreases gradient magnitudes. The distribution of cosines between gradients widens with logit squeezing and Gaussian noise. This behavior is similar to adversarial training. Logit squeezing increases the numerator of Equation 4, potentially giving an advantage. Regularizers do not provide more robustness than adversarial training for MNIST, but offer promising results at a lower cost. Increasing the number of training iterations affects the robustness of the MNIST classifier. Increasing the number of training iterations improves the robustness of models for logit squeezing and label smoothing. Larger smoothing and squeezing parameters, along with Gaussian augmentation, enhance model robustness against PGD and FGSM attacks on MNIST classifiers. Wide-Resnet CIFAR-10 classifiers were trained with aggressive parameter values for improved performance. The study compares results on CIFAR10 dataset using data-augmentation techniques and weight-decay. Results show logit squeezing outperforms label smoothing in white-box attacks, while label smoothing is slightly better in black-box attacks. Aggressive logit squeezing with parameters \u03b2 = 10 and \u03c3 = 20 results in a more robust model than adversarially trained models. Aggressive logit squeezing with parameters \u03b2 = 10 and \u03c3 = 20 results in a more robust model than adversarially trained models. Attacks on CIFAR-10 models show defenses working by obfuscating and masking gradients. BID0 suggests identifying these models through \"sanity checks\" like attacking them with PGD and FGSM attacks. The unbounded adversary can degrade model accuracy to 0.00% using PGD attacks, creating a false sense of security by convoluting the loss landscape. Increasing adversary strength through more steps and random restarts can reveal this false security. The stronger PGD attacks on a sample model with hyper-parameters k = 160k, \u03b2 = 10, and \u03c3 = 30 resulted in a slight drop in accuracy to 49.86% with 9 random restarts. Increasing the number of PGD steps to 1000 further decreased accuracy to 40.94%. Despite being less robust than adversarially trained models in some aspects, the hardened model showed high robustness in the black box setting and against non-iterative attacks, along with fast training time and high test accuracy on clean data. To ensure our model's robustness, we visualize the loss landscape of our sample model and observe that it remains unaffected by increasing the number of PGD attack steps. We also test our regularizers on the CIFAR-100 dataset, maintaining high performance with aggressive logit squeezing. The white-box performance of logit-squeezed models and PGD adversarially trained models for CIFAR-100 classifiers is compared in Table 5. Aggressive logit squeezing can make models as robust as adversarially trained ones at a lower cost. The logit-squeezed model trained for 80k iterations shows high accuracy for clean examples and is more robust against white-box attacks than the adversarially trained model. Increasing training to 160k iterations further improves robustness and accuracy. The study compares the robustness and accuracy of models with k = 160k to adversarially trained models. Adversarial training, label smoothing, and logit squeezing are analyzed for their impact on robustness. Simple regularization methods show promising results on MNIST, CIFAR-10, and CIFAR-100 datasets, outperforming adversarial training in both robustness and accuracy. The goal is to make robust training more accessible to practitioners. Adversarial training on CIFAR-10 involves shrinking adversarial gradients and logit gaps, with a more dramatic effect on gradients. Logit squeezing worsens robustness without Gaussian augmentation but improves when combined. The addition of Gaussian augmentation squashes gradients, increases logit gap, and aligns gradients slightly, giving an edge to logit squeezing over label smoothing. The results of experiments on MNIST show that Gaussian random augmentation alone increases robustness on black-box attacks but not on white-box attacks. Logit squeezing and label smoothing without augmentation are vulnerable to white-box attacks. Increasing noise magnitude improves robustness but degrades performance on clean examples. Table 6 shows the accuracy of different models trained on MNIST with attacks under white-box and black-box threat models. Logit squeezing outperforms label smoothing in the white-box setting, but training with a large squeezing coefficient often results in low accuracy on test data. The breakdown of training for label smoothing rarely occurs even with large smoothing parameters. White-box attacks on CIFAR-10 models show low accuracy on test data. Logit squeezing, label smoothing, and adversarial training have similarities in affecting linear approximation but differ in other metrics. Summing activations in the logit layer and penultimate layer helps analyze feature representation learned by the neural network. Adversarial training and techniques like label smoothing and logit squeezing deactivate a significant number of neurons in the penultimate layer, reducing the effective dimensionality of the deep feature representation layer. This helps prevent adversaries from exploiting certain features. Black-box attacks on CIFAR-10 models demonstrate the impact of these techniques on model robustness. The study evaluates PGD and FGSM attacks on MadryLab's adversarially trained model. The models are tested for robustness by performing unbounded PGD attacks, showing a decline in accuracy as a sanity check. This indicates that the models are not completely breaking the PGD attack and are not obfuscating the gradients. The study evaluates the robustness of a sample hardened CIFAR-10 model against PGD attacks. Increasing random restarts does not significantly degrade robustness, plateauing at 9 restarts. Increasing PGD steps can strengthen the adversary. Increasing the number of PGD steps can strengthen the adversary, but it does not greatly affect the robustness of the sample logit-squeezed model. The model remains resistant against attacks with certain parameters, as shown in Table 11 and Figure 9. The classification loss landscape surrounding input images is analyzed by moving in the adversarial direction and one random direction. The logit-squeezed model is shown to not mask gradients, with changes in loss primarily along the adversarial direction. Gaussian noise and label smoothing are combined in different scenarios for analysis. The combination of Gaussian noise and label smoothing deactivates roughly 400 neurons, shrinking the effective dimensionality of the deep feature representation layer. This effect is similar to adversarial training, as shown in a plot comparing different training methods on MNIST data."
}