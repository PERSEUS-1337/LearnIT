{
    "title": "HygP3TVFvS",
    "content": "Gaussian processes are commonly used in nature and engineering. Neural networks in the infinite-width limit have priors that correspond to Gaussian processes. This correspondence is extended to finite-width neural networks, resulting in non-Gaussian processes as priors. The methodology allows for tracking preactivation distributions by integrating out random variables from lower to higher layers, similar to renormalization-group flow. Bayesian inference with weakly non-Gaussian priors can be performed using a perturbative approach. Gaussian processes model various phenomena in the physical world, such as Brownian motion and the theory of elementary particles. In the context of machine learning, neural networks can give rise to Gaussian processes in the infinite-width limit, allowing for exact Bayesian inference. Gaussian processes serve as starting points to understand complex systems, despite being idealizations. In theoretical physics, Gaussian processes are used as starting points to understand complex systems, with a focus on non-Gaussianity. Perturbative treatment of finite-width corrections to neural networks explores non-Gaussian priors through integrating out preactivation of neurons layer by layer. This approach yields recursion relations resembling renormalization-group flow. In this paper, the authors explore the use of Gaussian-process Bayesian inference in neural networks with finite-width corrections. They derive recursive formulae to control the flow of preactivation distributions between layers and extend the method to non-Gaussian priors. The study focuses on real finite-width neural networks with asymptotically large hidden layers and constant input/output dimensions. The approach resembles renormalization-group flow in theoretical physics. In the context of neural networks with finite-width corrections, the study focuses on multilayer perceptrons with Gaussian priors for biases and weights. Wick's contractions are used to obtain higher moments, and the formalism heavily relies on them. The infinite-width limit is considered, where the number of hidden layers approaches infinity. The prior distribution of outputs in neural networks with finite-width corrections is governed by a Gaussian process with a kernel, and Wick's contractions are used to obtain higher moments. A recursive formula allows for the evaluation of the kernel for any pair of inputs. Exact Bayesian inference can be performed to predict mean outputs for test data, contrasting with stochastic gradient descent optimization. In neural networks with finite-width corrections, Bayesian inference marginalizes over all model parameters to predict outputs for test inputs. Corrections to Gaussian-process priors at finite widths lead to nontrivial preactivation correlation functions that dictate the distribution of preactivations. Keeping the number of outputs constant is crucial for consistent Bayesian inference. Our aim is to trace the flow of preactivation distributions in neural networks with finite-width corrections, leading to nontrivial correlation functions. Specifically, we show that preactivation correlation functions exhibit symmetry and recursive formulas link core kernel components across network layers. These components are used to calculate Bayesian inference corrections at finite widths. Our Schwinger operator approach provides the leading 1/n correction for Bayesian inference at finite widths, distinct from previous approaches. We study finite-width effects on Bayesian inference and find that the renormalization-group picture naturally emerges. Activation correlation functions are introduced as auxiliary objects in recursive steps to track leading finite-width corrections. The paper discusses integrating out preactivations and biases/weights to obtain relations between activation correlations. Wick's contractions are used for simplification, leading to a combinatorial hack for correlation functions. The approach provides corrections for Bayesian inference at finite widths, with a focus on activation correlation functions for tracking corrections. The paper introduces a combinatorial hack using Wick's contractions to evaluate activation correlations, providing corrections for Bayesian inference at finite widths. The solution involves intricate algebraic manipulations detailed in Appendix B. The paper introduces a combinatorial hack using Wick's contractions to evaluate activation correlations, providing corrections for Bayesian inference at finite widths. The right-hand side of Equation (10) is finite as n \u2192 \u221e, with the leading contribution being the Gaussian process kernel. Higher-point connected preactivation correlation functions are suppressed by O(1/n^2). Recursive equations are derived and the preactivation distribution in the first layer sets the initial condition for the flow through the layers. The paper discusses recursive equations for evaluating activation correlations using Wick's contractions, providing corrections for Bayesian inference at finite widths. The resulting distribution of outputs can be encoded by a probability distribution, which can be evaluated numerically or analytically for ReLU activations. An alternative analytic method is developed for any polynomial activations. An alternative analytic method is proposed for polynomial activations, allowing for the evaluation of activation correlations using Wick's contractions. This method simplifies the recursion relations and yields core kernel and vertex results, replicating those obtained through planar diagrams. In a special setup, quadratic activation is analyzed, simplifying recursion relations for a single input case. For ReLU activation, a layer-independent core kernel is obtained. A factor (1/n) is identified, guiding network architectural design. The theory is tested on a black-white image dataset without preprocessing, showcasing results for depth L = 3. In a study using a black-white image dataset without preprocessing, the depth is set to L = 3, with specific bias and weight variance values. The prior distribution of outputs over instances of Gaussian weights is compared with theoretical predictions, showing deviations as networks narrow. The theory is further supported by additional experiments in Appendix C.3. Theoretical predictions are validated through experiments in Appendix C.3, confirming the agreement between theory and experimental data for prior distributions of outputs. Recursive equations for Gaussian-process kernel and finite-width corrections are derived, leading to weakly non-Gaussian prior distributions. A formalism for inferring outputs for test inputs is developed, akin to Bayes' method, with perturbative extensions from Williams & Rasmussen (2006). Theoretical predictions are validated through experiments, confirming agreement between theory and experimental data. A formalism for inferring outputs for test inputs is developed using Bayes' method with perturbative extensions. Gaussian-process contributions are segregated out through textbook manipulation, leading to the mean posterior output prediction. The perturbative formalism developed in this paper captures the flow of preactivation distributions from lower to higher layers, showing a resemblance to renormalization-group flow equations in physics. Further large-scale experiments are needed to explore the regularization effect of finite widths and non-Gaussian priors on neural functions. Investigating fixed points away from Gaussian asymptopia in high-energy and statistical physics is appealing. Exploring the dream of neural networks washing away microscopic irrelevancies and extracting relevant features is exciting. Perturbative Bayesian inference scheme is universally applicable for weakly non-Gaussian prior distributions, especially in cases of neural networks at finite widths. Revisiting the comparison between SGD optimization and Bayesian inference at finite widths, particularly for convolutional neural networks, is recommended. There is a growing interest in studying SGD dynamics within large-width networks. In the context of investigating fixed points in high-energy physics, there is a growing interest in studying SGD dynamics within large-width neural networks. The formalism explores corrections to neural tangent kernels and aims to capture a phase transition from lazy-learning to feature-learning regimes using Wick's contractions and cumulants. The text discusses Gaussian-distributed variables and moments, including Isserlis-Wick's theorem for even moments. It also touches on connected correlation functions and rearranging variables. The examples provided aim to aid in understanding the concepts presented. The definition of Gaussian-distributed variables and moments is further illustrated with examples, emphasizing the hierarchical structure and asymptotic limits. The text also discusses weakly-coupled field theories and the symmetric nature of certain parameters. The text discusses the symmetric nature of certain parameters in connected correlation functions and Wick's contractions, allowing for the evaluation of E[z \u03b11 \u00b7 \u00b7 \u00b7 z \u03b1m] through mindless repetitions. The hierarchical structure (S12) is assumed, leading to the consequence (CLUSTER) using the inverse kernel K \u22121 \u03b11,\u03b12 as a metric to lower indices. In this Appendix, a full inductive proof is provided for one of the main claims in the paper, streamlined in the main text. It assumes the hierarchical structure (S12) and the symmetry \u03b1 1 \u2194 \u03b1 2 of S \u03b11,\u03b12, leading to the evaluation of E[z \u03b11 \u00b7 \u00b7 \u00b7 z \u03b1m] through mindless repetitions. The operators in Equations (OS') and (OV') become the operators in Equations (OS) and (OV) in the main text. The ( + 1)-th layer is assumed to test mastery of Wick's contractions and tricks. Disentangling cases with different neuron indices, terms at order O 1 n contribute, using inductive hierarchical assumption. Special cases yield expressions from the main text. The derivation does not assume anything about activation functions. The text discusses the distributions of preactivations in multilayer perceptrons with quadratic activation functions. It utilizes master recursion relations and Wickology to study finite-width corrections on Bayesian inference. The recursive equations simplify for a single input, and monomial activations like deep linear networks and quadratic activations are also explored. The text explores distributions of preactivations in multilayer perceptrons with quadratic activation functions, using master recursion relations and Wickology. It simplifies recursive equations for a single input and investigates monomial activations like deep linear networks and quadratic activations. The four-point vertex solution is discussed, guiding the narrowing of hidden layers through nonlinear activations like ReLU. Experiments with black-white images from the MNIST dataset are conducted with different depths and widths. The text explores distributions of preactivations in multilayer perceptrons with quadratic activation functions, using master recursion relations and Wickology. Results show agreement between theoretical predictions and experimental data for different hidden layer widths and depths. Nonlinear activations quickly amplify non-Gaussianity in the distributions. Nonlinear activations amplify non-Gaussianity in distributions. The identity simplifies expressions and can prove Equation (GP\u2206). The L = 2 multilayer perceptron with quadratic activation is used for illustration. Regularization effects of finite widths are shown in Figure S2, with peak performance at small N R. Test accuracy for N E = 10000 MNIST is displayed in Figure S2. Peak performance is achieved at finite widths in a multilayer perceptron with quadratic activation. Test accuracy for MNIST data improves with regularization effects at small training data sizes."
}