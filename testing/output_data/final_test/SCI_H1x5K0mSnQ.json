{
    "title": "H1x5K0mSnQ",
    "content": "Deep learning has seen significant advancements but struggles with few-shot learning due to limited labeled data. A novel algorithm using discrete geometry models samples as simplices to measure class scatter. During testing, a new simplex is formed to quantify similarity between test samples and classes. Feature maps from convolutional neural networks are used to construct simplices. Using local regions of feature maps from convolutional neural networks, experiments on Omniglot and miniImageNet confirm the effectiveness of simplices in few-shot learning. Deep learning, particularly the use of deep Convolutional Neural Networks (CNNs) like AlexNet, has greatly improved visual concept comprehension in various fields. However, training these networks requires a large number of handcrafted class labels for supervision. For example, the ImageNet object recognition benchmark has over one thousand samples per class. Deep learning algorithms struggle with recognizing new categories with sparse data, leading to research on one-shot or few-shot learning. Bayesian framework models few-shot learning by learning empirical knowledge of available categories and updating the prior with unseen class examples. Bayesian theory offers a solution for learning with limited data. The recent work on few-shot learning focuses on deep neural networks, such as Siamese neural networks and triplet loss, to improve recognition performance by learning intrinsic manifold structures of training data. Recent advancements in few-shot learning include Prototypical Networks, Memory-augmented architectures, Matching Networks, and meta learning to improve recognition performance by learning intrinsic manifold structures of training data. These methods aim to enhance the network's generalization and robustness for untrained objects. Meta learning techniques, such as memory networks and matching networks, are used for rapid learning with limited samples. A LSTM-based meta learner mimics optimization algorithms to train few-shot learners. Other approaches include fast adaptation of neural networks and utilizing pre-trained models for few-shot learning. In the last feature layers, a transformation is learned for predicting new classes from activations. Deep learning algorithms improve feature extraction quality, but softmax classifier may deteriorate performance if new class distribution is not accurately modeled. K nearest neighbors method lacks global structural information. A geometric method is proposed for few-shot learning to accurately characterize each class. The proposed geometric method aims to accurately characterize each class in few-shot learning by using convex polytopes to measure class scatter and compute distances from test samples. The construction of polytopes is based on convolutional feature maps to highlight structural details. However, there is no exact formula for calculating the volume of general convex polytopes. The algorithm simplifies volume calculation by using simplexes instead of general convex polytopes. It represents each class as a polytope with feature vectors as vertices, allowing for effective few-shot recognition. The test sample's feature vectors are expected to be close to its own class's polytope. The algorithm simplifies volume calculation by using simplexes instead of general convex polytopes. A simplex is the conceptual extension of a triangle in high-dimensional spaces, with the condition of n = d implying the need for exact d + 1 points to constitute a simplex in the d-dimensional space. A simplex in d-dimensional space is called a d-simplex. Simplexes like lines, triangles, and tetrahedrons have lengths, areas, and volumes respectively. The content of a simplex can be expressed using the Cayley-Menger determinant. The content of a simplex has two expressions that coincide with each other, showing the relationship between the Gram determinant and the Cayley-Menger determinant. The analysis is based on formula (2) using feature sets derived from deep neural networks. The content of a simplex is a plausible measurement for class scatter, with the content increasing when a test sample is merged into the feature set. The dissimilarity measurement of a test sample to a class can be expressed as DISPLAYFORM0. The dissimilarity measurement of a test sample to a class can be written as DISPLAYFORM0, where the Cayley-Menger matrix pertaining to simplex Y is used. The test sample point t is mapped to this space and a new simplex is obtained in a higher-dimensional space, showing intra-class similarities and inter-class diversities. The dissimilarity measurement of a test sample to a class can be represented by a Cayley-Menger matrix. The simplex formed by the class can be expressed mathematically. By substituting formulas, a relationship between the test point and the class simplex is derived. A visual example is provided to illustrate the algorithm. It is noted that the class simplex cannot form a d-simplex due to the number of data points. However, the few-shot learning model can still proceed without modification by introducing an isometric embedding of the simplex. Points on geometric entities can be expressed with coordinates in an ambient space. In an ambient space, geometric entities (like polytopes) can be represented with coordinates. The intrinsic dimension is usually much smaller than the ambient dimension. Isometric embedding ensures that the contents of a simplex and its embedding satisfy the same identity. This correlation allows for the construction of low-dimensional simplices in cases where the number of vertices is insufficient. Visual examples show how a segment line can be embedded in a 1-dimensional space. In an ambient space, geometric entities like polytopes can be represented with coordinates. Isometric embedding ensures that a simplex's geometric properties remain unchanged when mapped into a higher dimensional space. This allows for the construction of low-dimensional simplices in cases where the number of vertices is insufficient. For few-shot classification tasks, reduced simplices are common due to high-dimensional feature representations and few available examples per class. Images classification tasks often use deep convolutional networks as the embedding function. The VGG16 network, commonly used for image classification tasks, produces a 4096-dimensional vector. In few-shot learning, reduced simplices of dimension four are utilized for C(Y) computation and dimension five for C(Y \u222a t). This geometric analysis is akin to manifold learning theories like the Isomap algorithm. Matching networks extend metric learning through attention models, and analyzing the metric characteristics can provide insights into few-shot learning. Formula (6) details are crucial for Theorem 1. The geometric dissimilarity measurement from point t to Y is detailed in Formula (6) as described in Theorem 1. The proof involves Schur's determinant identity and the expansion of matrices P and Q. Despite not being positive definite, they still hold algebraic meaning in the context of metric matrices. The first factor w1 is an incremental ratio with respect to the class, while the second factor w2 is equivalent to a scale-normalized distance transformation. A new approach is presented to construct a simplex using spatial feature maps of CNNs, which have been effective in visual relational reasoning. In classification tasks, feature maps are flattened to a 1-D vector, losing spatial structure information. This flattening is necessary for conventional tasks but not ideal for few-shot tasks. To improve classification accuracy for few-shot tasks, utilizing multiple local regions for information collection is essential. By applying attention windows of various scales to feature maps in the last convolution layer, more feature vectors can be generated for simplex construction with few samples. This strategy involves flattening operation within each local region to preserve local information, enabling the generation of feature vectors. Tagging cells with their coordinates helps preserve adjacency information in the feature maps. By tagging cells with coordinates, the model retains spatial relationships on a manifold, improving performance. In few-shot tasks like N-way k-shot classification, limited examples require preventing overfitting by avoiding training from scratch. The model was tested on miniImageNet and Omniglot datasets. Our model was tested on miniImageNet and Omniglot datasets to prevent overfitting in few-shot tasks. The algorithms compared include deep learning methods like matching networks, Meta-Learner LSTM, and prototypical networks, as well as conventional methods like Mahalanobis distance and Minimum Incremental Coding Length (MICL). A simple four-block CNN architecture was used for learning data representations. The network proposed by BID15 consists of four 3x3 convolutional layers with 64 filters, followed by batch normalization, ReLU activation, and max-pooling. It includes a fully connected layer and a softmax layer for classification. The output is optimized using cross-entropy loss with the Adam optimizer. The few-shot accuracy on miniImageNet is shown in Table 2, with comparisons to traditional algorithms like KNN and Mahalanobis Distance. The study evaluates various few-shot learning algorithms on miniImageNet, a subset of ImageNet with 100 categories. The best performing algorithm, Prototypical networks, achieved 49.42% accuracy in 1-shot 5-way tasks and 68.20% accuracy in 5-shot tasks. The study evaluates few-shot learning algorithms on miniImageNet, with the best performing algorithm achieving high accuracy in 1-shot and 5-shot tasks. The embedding network is trained on a 64-class training set and validated on a 16-class set, with a 20-class test set used for few-shot experiments. Different local region sizes are tested for optimal performance, with 2x2 and 3x3 regions performing best on 1-shot and 5-shot tasks respectively. The model's performance is compared with other models on the test set using the same splits proposed by BID15. Our model, trained on a pre-trained 4-block CNN, outperforms Baseline KNN, matching networks, and Meta-learner LSTM in one-shot learning on the Omniglot dataset. However, prototypical networks perform better than our simplex algorithm. The dataset contains characters from 50 alphabets with 20 examples each. The training setting involves splitting the dataset into training and validation/testing sets, using a 4-layer CNN with an output feature map of 64@1. The output feature map is changed to 64@1 \u00d7 1 for testing on miniImageNet. N-way k-shot performance comparison with state-of-the-art models is shown in TAB1. Additional experiments on MICL and model robustness are conducted. Adjusting the distortion parameter in MICL improves model performance. A free parameter is set in the simplex model using eigenvalues of Q. Our model outperforms MICL in terms of robustness, with stable performance across a wide range of distortion values. The simplex metric can be integrated with prototypical networks and meta-learning methods for improved accuracy, especially in few-shot cases. The meta learning methods focus on learning models between tasks rather than individual data points, making them suitable for improving performance based on the simplex metric in few-shot cases. MICL and a new algorithm achieve better 5-shot recognition performance than other approaches by exploiting features of new classes without retraining or fine-tuning CNNs. Both methods leverage the geometric structure of class data, suggesting a potential for geometry-based solutions in few-shot learning. In this paper, a novel method based on high dimensional convex geometry was designed to address few-shot learning problems. The approach transforms the learning problem into studying volumes of simplices and investigates the relation between test samples and classes using polytopes' volumes. Extensive simulations validated the accuracy and robustness of the geometry-based method compared to existing literature."
}