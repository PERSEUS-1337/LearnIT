{
    "title": "rJeB36NKvB",
    "content": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters. Filters in CNNs may know what they are looking at but not where they are positioned in the image. Deep CNNs may implicitly encode absolute position information. Experiments confirm the presence of absolute position information in neural networks and provide insights into how and where this information is represented. State-of-the-art results in computer vision tasks such as object classification, detection, face recognition, semantic segmentation, and saliency detection have been achieved using CNNs. However, CNNs lack interpretability, leading to the utilization of capsule or recurrent networks to model spatial relationships within learned feature layers. It is unclear if CNNs capture absolute spatial information, crucial for position-dependent tasks like semantic segmentation and salient object detection. Regions deemed most salient tend to be near the center of an image. In this paper, the authors investigate the role of absolute position information in CNNs. They conduct randomization tests to show that CNNs learn to encode position information from zero-padding, a commonly used operation in convolution. This study sheds light on the importance of position information in learned features of CNNs. The text discusses the visualization of learned features in CNNs, highlighting the challenges of modeling relationships as the number of layers increases. Various methods, such as deconvolutional networks and pixel-level gradients, are proposed to understand the patterns learned by feature maps. CNNs have emerged as a way to handle the large number of weights in a fully connected network. A study showed that cropping can shift features rightward, impacting output regions deemed salient. While existing approaches focus on visualizations, our work aims to understand spatial relationships encoded by CNN models. CNNs rely on spatial positioning information in images to a greater extent than expected. This hypothesis is tested and evidence is provided that CNNs learn both what and where objects are in an image. They extract fine-level details such as edges and implicitly encode spatial position along with features. CNNs extract fine-level details like edges and implicitly encode spatial position along with features. The goal is to predict position information from different CNN archetypes in an end-to-end manner. Given an input image, the aim is to predict a gradient-like position information mask. The Position Encoding Network (PosENet) predicts absolute position information using gradient-like masks generated for supervision. It consists of a feedforward convolutional encoder network (f enc) and a position encoding module (f pem) that extracts features at different levels of abstraction. ResNet and VGG based architectures are used for the encoder networks. The encoder module in the Position Encoding Network consists of feature extractor blocks with frozen weights. The position encoding module focuses on extracting position information while the encoder network maintains its weights. Multi-scale features are concatenated and processed with convolution operations. The trainable weights in the transformation function validate the learning of position information implicitly. The position encoding module in the Position Encoding Network validates the learning of position information implicitly by generating gradient-like masks and Gaussian distribution maps. This helps to determine if the model can learn absolute position on one or two axes, in addition to repeated patterns. The design of gradient ground-truth in the position encoding module allows for the modeling of position information independently of image content. Synthetic images are used to validate the hypothesis, and the encoder network is frozen during training. The position encoding module generates the position map of interest for input images. During training, the network applies a supervisory signal on the predicted position map by upsampling it to match the ground-truth position map. A pixel-wise mean squared error loss is used to measure the difference between the predicted and ground-truth position maps. The overall objective function of the network involves vectorized predicted positions and ground-truth maps. Synthetic images are also utilized in the experiments, following a common setting in saliency detection. The position information is relatively content independent, allowing for the use of any images in the experiments. Evaluation Metrics: The position encoding performance is measured using Spearman Correlation (SPC) and Mean Absolute Error (MAE). SPC calculates the correlation between ground-truth and predicted position map, while MAE measures the pixel-wise difference. The network is initialized with pretrained ImageNet classification task and trained using stochastic gradient descent for 15 epochs. During training and inference, images are resized to 224\u00d7224. Feature maps are aligned to 28 \u00d7 28. Experimental results are reported for VGG, ResNet, and PosENet models. Position information is learned directly from input images. Ground-truth patterns include horizontal and vertical gradients, 2D Gaussian distribution, and horizontal and vertical stripes. Pretrained models like VGG and ResNet are used to validate position information encoding. PosENet can extract position information consistent with the ground-truth position map only when coupled with a deep encoder network. The experiments focus on validating how much position information a CNN model encodes. Only one convolutional layer with a kernel size of 3 \u00d7 3 is used in PosENet for this experiment. The correlation with input is considered a type of randomization test. The model performance on test sets shows it is not overfitting to noise but extracting true position information. However, it struggles with repeated patterns due to model complexity. Different patterns are analyzed across architectures, showing correlation between predicted and ground-truth position maps. The study validates that position information is implicitly encoded in neural network architectures without explicit supervision. ResNet models outperform VGG16 models, possibly due to different convolutional kernels or prior knowledge of semantic content. Further investigation focuses on natural images. In this section, the study focuses on natural images and conducts ablation studies on the role of the position encoding network. The experiments examine varying kernel sizes and stack lengths of convolutional layers to extract position information. Results show that position information is learned from object classification tasks, and by using a stack of convolutional layers, hidden position information can be extracted more accurately. In Table 2 (a), experimental results for PosENet show that increasing the number of layers improves the readout of positional information. Stacking multiple convolutional filters allows for a larger effective receptive field, enhancing position information extraction. Positional information may require more than first-order inference, as indicated by experiments with pretrained CNN models. In an experiment with PosENet using VGG16, it was found that top layers encode more position information than bottom layers. This could be due to deeper layers extracting more feature maps. The padding near the border helps deliver positional information for learning. The padding near the border in convolutional layers provides position information for learning. Zero-padding is commonly used to maintain spatial dimensions in convolutional layers. Removing padding in VGG16 significantly reduces performance on natural images compared to the default setting. The impact of zero-padding on position information in PosENet is explored. Adding padding improves performance, especially with padding=1. Without padding, position information is encoded from a pretrained CNN model. The experiment validates the importance of zero-padding in extracting position information. The impact of semantics on position information is visualized through content loss heat maps. PosENet shows larger content loss around corners, while VGG and ResNet correlate more with semantic content. ResNet's deeper understanding leads to stronger interference in generating smooth gradients. Saliency detection validates these findings in position-dependent tasks like semantic segmentation and salient object detection. In position-dependent tasks like semantic segmentation and salient object detection (SOD), the impact of zero-padding on the VGG network is validated. Results show that zero-padding is crucial for delivering position information, as models without padding perform significantly worse. This is further confirmed in semantic segmentation tasks, where models with zero padding outperform those without. In position-dependent tasks like semantic segmentation and salient object detection (SOD), the importance of zero-padding in the VGG network is highlighted. Pretrained models on these tasks outperform those on classification, showing the significance of position information. In this paper, experiments reveal that convolutional neural networks implicitly encode absolute position information. Larger receptive fields and non-linear readout further enhance positional information, even without semantic cues. Results suggest joint encoding of semantic features and absolute position, with zero padding and borders serving as anchors for spatial abstraction in CNNs. The results show a new fundamental property of CNNs that requires further exploration."
}