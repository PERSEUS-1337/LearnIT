{
    "title": "Skey4eBYPS",
    "content": "The Convolutional Conditional Neural Process (ConvCNP) is a new model that incorporates translation equivariance in data modeling. It embeds data into an infinite-dimensional function space and utilizes convolutional deep-sets for translation-equivariant embeddings. ConvCNPs outperform existing Neural Processes (NPs) in various settings and enable zero-shot generalization to out-of-domain tasks. Neural Processes (NPs) define a conditional distribution over output variables given input variables and observed data points. They use an encoder to embed context sets into a representation space, allowing for meta-learning and multi-task/transfer learning. Conditional NPs (CNPs) are a deterministic variant that can be trained simply with maximum likelihood learning. Neural Processes (NPs) can be trained simply with maximum likelihood learning, mimicking test time usage for strong performance. NPs are applied in time series, spatial data, and images with missing values, commonly used for benchmarking. Ideal solutions in these domains should be translation equivariant, relating to stationarity. Current NP models must learn this structure from the dataset, which is inefficient in terms of samples and parameters. The goal of this paper is to build translation equivariance into Neural Processes (NPs). Unlike Convolutional Neural Networks (CNNs), NPs operate on partially observed context sets and embed sets into a finite-dimensional vector space, making equivariance with respect to input translations challenging. To address this, the CONVCNP is introduced as a new approach. In this work, the CONVCNP is introduced as a new member of the NP family that accounts for translation equivariance. Key contributions include a representation theorem for translation-equivariant functions on sets, extending the NP family to include translation equivariance, and demonstrating excellent performance on various benchmarks. The CONVCNP is a new member of the NP family that incorporates translation equivariance. It introduces a representation theorem for translation-equivariant functions on sets and shows strong performance on benchmarks. X = R d, Y \u2286 R d, Z M = (X \u00d7 Y) M, Z \u2264M = M m=1 Z m, Z = \u221e m=1 Z M, p(y|x, Z) = p(y|\u03a6(x, Z), \u03b8), where \u03a6 = \u03c1 \u2022 E, E : Z \u2192 R e, \u03c1 : R e \u2192 C b (X , Y), E(Z) \u2208 R e, C b (X , Y) is the space of continuous, bounded functions X \u2192 Y. NPs use latent variables to specify indirectly. In this work, the focus is on CNP models that do not employ latent variables to specify predictive distributions. The goal is to achieve translation equivariance with respect to translations on X. To address the issue of equivariance with input translations in X, the encoder is enriched to map into a function space H containing functions on X. The focus is on achieving translation equivariance in CNP models without latent variables. The encoder maps to a function space H for functions on X, allowing for translation-equivariant functions on sets with specific functional embeddings. The CONVCNP model utilizes a continuous and translation-equivariant map \u03c1 : H \u2192 C b (X , Y) for encoding. Theoretical results in Section 3.1 establish equivalence between function properties and the encoder definition. Practical implementation details are discussed in Section 4 based on the theoretical foundation. The CONVCNP model establishes the theoretical foundation by defining multiplicity and stating key theorems related to permutation invariance and translation. The model requires specific inputs like CNN, context, density, image, and target mask for prediction. The forward pass is illustrated for both on-the-grid and off-the-grid cases. The CONVCNP model defines multiplicity and key theorems on permutation invariance and translation. It requires inputs like CNN, context, density, image, and target mask for prediction. The forward pass is shown for on-the-grid and off-the-grid cases. The proof of Theorem 1 is discussed, highlighting practical implications for CONVCNPs. Key points include setting \u03c8 to a positive-definite kernel, defining \u03c6 as powers of y up to order K, and requiring \u03c1 to be a powerful function approximator. Theoretical results inform the implementation of CONVCNPs, extending previous work by embedding sets into an infinite-dimensional space. The RKHS formalism allows for translation equivariance and handling sets of varying sizes. The model requires \u03c6(y) to expand up to order K, with results holding for sets of any finite size M. CONVCNPs model the conditional distribution with key considerations on the design of \u03c1, \u03c6, and \u03c8 for \u03a6. For data with a single output per input location, \u03c6 is a power series of order one. The first output \u03c61 provides information on observed data points, distinguishing between no observation and a datapoint with y = 0. The functional representation h at x with y = 0 is denoted as a \"density channel\". Normalized convolution is used to improve performance with large variation in input location density. The EQ kernel with a learnable length scale parameter is chosen for \u03c8. Theorem 1 suggests that \u03c1 should be continuous and translation-equivariant. Theorem 1 suggests that \u03c1 should be a continuous, translation-equivariant map between function spaces. In deep learning, any translation-equivariant model can be represented as a CNN. To approximate \u03c1 with a CNN, the input is discretized, the CNN is applied, and the output is transformed back to a continuous function X \u2192 Y using evenly-spaced basis functions. The CONVCNP method approximates \u03c1 with a CNN for translation-equivariant mapping between function spaces. This is particularly useful for on-the-grid data, such as images, where the discretization can be chosen as pixel locations. The resulting continuous functions are used to generate predictive mean and variance, which can then be used to evaluate log-likelihood. The CONVCNP method uses a CNN to approximate \u03c1 for on-the-grid data like images. Context points are selected using a context mask, and convolutions are applied to form density and normalized channels. Non-negative, positive-definite kernels are suggested, but using a fully trainable kernel with positive values also works well. The on-the-grid version of \u03c1(\u00b7) is implemented with a CNN and a shared MLP. The algorithm involves multiplying by \u03c8 and summing where (\u00b5, \u03c3) are the image mean and standard deviation. Training includes maximum-likelihood training with the data set D = {Zn}Nn=1 and parameters \u03b8. Maximum-likelihood training involves splitting Zn into context (Zn,c) and target (Zn,t) sets, standard practice in neural auto-regressive models. Stochastic gradient descent methods can be used for optimization. Evaluation of CONVCNPs focuses on translation-equivariant models' performance and generalization to off-the-grid settings with irregularly sampled time series data. The study compares neural process models using off-the-grid and on-the-grid data sets, demonstrating improvements over existing models. Different architectures are proposed for the CNN component, named CONVCNP and CONVCNPXL. The models are evaluated on synthetic regression problems and show substantial improvements. The study compares neural process models using different data sets and architectures, including CONVCNP and CONVCNPXL. Models are evaluated on synthetic regression problems, showing improvements over existing models. The log-likelihood means and standard errors of the models are reported in Table 1. CONVCNP outperforms other models in all cases, even when extrapolation is not required. It can generalize well to data observed outside the training range, unlike ATTNCNP. The PLAsTiCC dataset simulates transients observed by the LSST telescope, containing 3,500,734 \"light curves\" measured in six astronomical filters. The data set consists of flux measurements in six astronomical filters, treated as a six-dimensional time series. A Kaggle competition involved classifying variable sources using these light curves. The winning entry used Gaussian Processes (GPs) to model the light curves and generate features for a classifier. A comparison was made with a multi-input-multi-output CONVCNP model, which outperformed the GP models. CONVCNP accepts six channels as inputs and returns 12 outputs. Experimental results showed higher log-likelihood with CONVCNP compared to GPs. The CONVCNP model outperformed Gaussian Processes in classifying variable sources using flux measurements in six astronomical filters. It can be trained on simulation data and deployed with real-world training data. CONVCNP was trained on synthetic data from the Lotka-Volterra model and can condition on real-world data for prediction tasks. The CONVCNP model, trained on synthetic data from the Lotka-Volterra model, accurately interpolates using real-world data from the Hudson's Bay lynx-hare dataset. The ATTNCNP struggled with variable length time series data, impacting its performance. CONVCNP was also evaluated for image completion tasks, outperforming ATTNCNP in this aspect. In experiments, CONVCNP outperforms ATTNCNP on various benchmarks such as MNIST, SVHN, and CelebA datasets. CONVCNP shows better performance with a large receptive field size and comparable performance with a small receptive field size. CONVCNP outperforms ATTNCNP on various benchmarks like MNIST, SVHN, and CelebA datasets, showing better performance with a large receptive field size and comparable performance with a small receptive field size. Generalization to multiple, non-centered objects is tested, with CONVCNP significantly outperforming ATTNCNP in a zero shot multi-MNIST setting. CONVCNP is able to extrapolate to out-of-distribution test sets, as shown in Figure 4a. CONVCNP outperforms ATTNCNP on various benchmarks like MNIST, SVHN, and CelebA datasets, showing better performance with a large receptive field size and comparable performance with a small receptive field size. Interestingly, CONVCNP is able to extrapolate to out-of-distribution test sets, while ATTNCNP predicts a centered \"mean\" digit independently of the context. However, CONVCNPXL does not perform as well on this task, with performance decreasing as the receptive field becomes very large. The hypothesis is that this decrease in performance is related to the model's behavior at the edges of the image. CNNs with larger receptive fields are able to model non-stationary behavior by considering the distance from any pixel to the image boundary. Further experimental evidence regarding the effects of receptive field on the ZSMM task is provided in Appendix D.6. The CONVCNP model shows superior performance compared to ATTNCNP on various datasets like MNIST, SVHN, and CelebA. It exhibits better generalization and computational efficiency, with lower memory and time complexity. Even with 95% of pixels removed, CONVCNP can still produce reasonable reconstructions. In contrast, ATTNCNP requires significantly more memory and struggles with larger image sizes. CONVCNP outperformed ATTNCNP on datasets like MNIST, SVHN, and CelebA due to its superior generalization and computational efficiency. ATTNCNP required a batch size of 6 and 19139 MB of memory, while CONVCNP showed better performance even with 95% of pixels removed. Future work includes exploring the relationship between restricted attention and CONVCNPs, as well as potential extensions to the NP family and representing functions on sets. Our work suggests that embedding sets into a function space can alleviate issues related to cardinality. Future analysis aims to formalize this in a broader context. Recent research on 3D point-cloud modeling has considered translation equivariance, resembling CONVDEEPSETS. One key difference is the correlation and consistency under marginalization in the predictive distribution of CON-VCNP. The predictive distribution in autoregressive models like PixelCNN++ lacks correlations and appears noisy. Consistency under marginalization is a challenge for neural autoregressive models, but consistent variants have been developed. Exchangeable neural process models and latent variables can introduce dependencies between variables in neural processes. The predictive distribution in autoregressive models lacks correlations and appears noisy. Consistency under marginalization is a challenge for neural autoregressive models, but consistent variants have been developed. Exchangeable neural process models introduce dependencies between variables. CONVCNPXL is shown for datasets, providing proof for Theorem 1 with a defined topology for fixed-sized sets. Our proof strategy involves defining a suitable topology for fixed-sized sets and demonstrating the homeomorphic properties of our proposed embedding into function space. We extend the embeddings of fixed-sized sets to varying-sized sets by \"pasting\" them together while maintaining homeomorphic properties. The resulting embedding can be composed with a continuous mapping to our desired target space, establishing a continuous mapping between two metric spaces. Finally, we combine these results to prove Theorem 1. The Moore-Aronszajn Theorem guarantees a unique Hilbert space of real-valued functions on X with a reproducing kernel \u03c8. An interpolating RKHS interpolates any finite number of points, induced by strictly positive-definite kernels like the exponentiated quadratic kernel. The quotient space A n / S n consists of equivalence classes of permutations in a Banach space A. The quotient space A n / S n consists of equivalence classes of permutations in a Banach space A. The canonical map A n \u2192 A n / S n is continuous under the metric topology induced by d. Topologically closed sets in A n closed under permutations remain closed in A n / S n under the metric topology. The quotient topology on A n / S n induced by the canonical map is metrizable with the metric d. The canonical map is surjective, leading to a unique topology on A n / S n where the canonical map is a quotient map. In this section, A is specialized to A = X \u00d7 Y, with elements denoted as (x, y) in A and ((x 1 , y 1 ) , . . . , (x M , y M )) in A M. Elements in Z M = A M are permuted according to \u03c0 \u2208 S M. Lemma 3 states that permutation-invariant functions on Z M correspond to functions on the quotient space induced by the equivalence class of permutations, Z M / S M. This result is crucial for proving the main result. Before proving Lemma 3, we show that embedding sets of a fixed size into an RKHS is continuous and injective. The Hilbert space constructed from the RKHS H with \u03c8 as a reproducing kernel and inner product f, g is injective, invertible, and continuous. Proof involves showing E M is injective, using inner product properties, and interpolation in H. Permutation of X is shown, leading to a conclusion about the number of terms in the sum. Lemma 2 states that the 'sum-of-power mapping' is injective and a permutation of the elements. It also shows that the mapping is a homeomorphism, proving continuity. To define Z 2 with multiplicity one, one can require a non-zero spacing between input locations to avoid closure issues. This construction can be extended to higher numbers of observations and multiplicities. The result that E \u22121 M must be continuous is based on the intuition that for unbounded Z M, if a function converges in H, the points (x i) M i=1 must be bounded. This is formalized in the proof of Lemma 2, showing that H M is closed and compact. The density channel \u03c6 1 ( \u00b7 ) = 1 ensures well-behaved behavior of (x i) M i=1. The image of E M | [Z J] contains the limit f, showing that H M is closed. E -1 is continuous, and a continuous bijection from a compact space to a metric space is a homeomorphism. Let f 1 denote the first element of f. By the reproducing property of \u03c8, f 1 in H converges uniformly pointwise. A lemma states that an encoding for sets with no more than M elements can be constructed into a function space, where the encoding is injective and every restriction to a fixed set size is a homeomorphism. The curr_chunk discusses the injectivity of a function E and the construction of an interpolating, continuous positive-definite kernel \u03c8 in a Hilbert space. It demonstrates that E is injective by showing that the sets (Hm) are pairwise disjoint. The space of continuous bounded functions from X to Y is also discussed, with the assumption that \u03c8 is a stationary kernel. The text discusses the construction of an interpolating, continuous positive-definite kernel \u03c8 in a Hilbert space. It proves that a function \u03a6 is continuous, permutation invariant, and translation equivariant if it has a specific representation form. The text discusses the construction of a continuous positive-definite kernel \u03c8 in a Hilbert space. It proves that a function \u03a6 is permutation invariant and translation equivariant. The proof follows a strategy used by Zaheer et al. (2017) and Wagstaff et al. (2019). The function \u03c1 : H \u2264M \u2192 C b (X , Y) can be extended to H K+1 using a generalisation of the Tietze Extension Theorem by Dugundji et al. (1951). Variants of Dugundji's Theorem also preserve translation equivariance. In our experiments, we compare our models to conditional neural process models, including a vanilla CNP (1d only) and an ATTNCNP. The baseline CNP has a 3-layer MLP encoder with 128 hidden units in each layer and RELU non-linearities. Context points are embedded and averaged before being concatenated with target inputs and passed to the decoder. The ATTNCNP model used in the experiments follows a similar architecture to the baseline CNP. Context points are encoded to latent representations and then processed through self-attention layers before predicting values at target points. The model utilizes a single hidden layer MLP for embedding context and target points. The ATTNCNP model utilizes a single hidden layer MLP to embed context and target points. Cross-attention is used to estimate the target representation, and a Gaussian pdf with diagonal covariance is used for the conditional predictive posterior. The decoder consists of a 4 hidden layer MLP for images and the same decoder as the CNP for 1d experiments. A minimum standard deviation is enforced to avoid infinite log-likelihoods. In experiments, weights are optimized using Adam with weight decay, and learning rates are specified. Two models, CONVCNP and CONVCNPXL, are considered with different architectures. Input kernel is EQ with a learnable length scale parameter, and output kernel is also EQ. To avoid numerical issues, \u03b5 = 10^-8 is added when dividing by density channel. Length scales for EQ kernels are initialized based on spacing between discretization points. The receptive field size in CNNs is influenced by the kernel width and spacing between discretization points. Depthwise-separable convolutions can reduce parameters and allow for increased discretization points without affecting the receptive field. CONVCNP and CONVCNPXL have different architectures, with CONVCNP using a 4-layer convolutional design for 1D experiments. The CONVCNP model uses a 4-layer convolutional architecture with a kernel size of 5 and 16, 32, 16, 2 channels. The final channels are processed by an EQ-based layer with mean and standard deviation channels. The CONVCNPXL model is inspired by UNet and employs a 12-layer architecture with skip connections. Channels are doubled for the first 6 layers and halved for the final 6 layers, using concatenation for skip connections. The CONVCNP model uses a 4-layer convolutional architecture with a kernel size of 5 and 16, 32, 16, 2 channels. The CONVCNPXL model is inspired by UNet and employs a 12-layer architecture with skip connections. Channels are doubled for the first 6 layers and halved for the final 6 layers, using concatenation for skip connections. The kernels used for the Gaussian Processes in this experiment are defined, including the EQ kernel with a length scale parameter of 1. The predictive posterior of the models is shown when data is presented in the same range as training, outside the training data range, and for model predictions. During training, context and target points are randomly selected from a uniform distribution. Models are trained for 200 epochs with a learning rate of 3e\u22124, except for CONVCNPXL on sawtooth data which uses a learning rate of 1e\u22123. The models are evaluated with 64 points per unit. The CONVCNPXL model uses a learning rate of 1e\u22123 on sawtooth data. Random sawtooth samples are generated with amplitude, frequency, and time parameters. The CNP and ATTNCNP models use a learning rate of 10\u22123. CONVCNP was trained for 200 epochs with batch size 4. Context and target set sizes are sampled over [3, 100]. Context points are randomly selected from a uniform distribution between 1 and the total points available in the series. The CONVCNPXL model uses a learning rate of 1e\u22123 on sawtooth data, while CONVCNP was trained for 200 epochs with a batch size of 4. Context and target set sizes are sampled over [3, 100], with context points randomly selected from a uniform distribution. The number of points available in the series ranges between 10-30 per bandwidth, with statistics computed over 1000 evaluations. CONVCNPXL was used for testing, comparing it to GP models and normalizing the data according to specific values. Outliers in GP results with log-likelihood values less than -10 were removed, and the same datapoints were removed from CONVCNP results. The learning rate was set to 10^-3, and E(Z) was discretized by evaluating 256 points per unit. The Lotka-Volterra model describes the generation of simulated training data for an experiment in Section 5.3. It involves the number of predators (X) and prey (Y) in a simulation, with events like predator births and deaths, and prey births and deaths. Parameter values and initial values govern the simulation behavior. Specific parameter values are chosen, leading to reasonable time series. The Lotka-Volterra model generates simulated training data for an experiment involving predators (X) and prey (Y) with specific parameter values. Time series are simulated using Gillespie's algorithm, with oscillating populations sensitive to parameter choices. The simulations can yield a maximum population of approximately 300, while the lynx-hare data set has a maximum population of about 80. The simulation population is scaled by a factor of 2/7 based on the lynx-hare data set with a maximum population of about 80. Time series longer than 100 units, with over 10000 events, or where one population is zero are removed. Context points for training batches are randomly selected between 3 and 80, with 150 \u2212 n target points. The Hudson's Bay data set time values range from 1845 to 1935, but model values range from 0 to 90 for consistency. Evaluation involves removing 18 points as a target set and using the remaining 72 points as a context set to assess interpolation and uncertainty. The CONVCNP model was used for training with a learning rate of 10^-3 and 100 points per unit for E(Z) discretization. An attempt to train an ATTNCNP was made, but it struggled with predicting outside its training interval, as shown in Figure 9. The ATTNCNP could learn the first part of the time series but struggled with data outside the first 20 time units. The ATTNCNP model struggled with predicting outside the first 20 time units, performing better on synthetic data than real data. Training details include sampling context and target points, optimizing weights with Adam, and rescaling pixel values. The proposed convolutional CNP, CON-VCNP, utilizes gridded inputs for images or time series data. It simplifies the specification of context and target points using masks, making it easier to implement in deep learning libraries. The architecture includes depthwise separable convolutional layers for efficiency. The CON-VCNP model introduces a density channel to indicate points in the image, with 4 channels per pixel (3 RGB and 1 density). A convolution is applied to the density channel and a normalized convolution to the signal, ensuring output depends on signal scale. The output channel size is 128 dimensional, with kernel size dependent on image shape. Element-wise positivity is enforced on trainable filters. The CON-VCNP model introduces a density channel with 4 channels per pixel. A CNN with residual blocks is applied to the signal and density. The output channels are 128 dimensional, with a shared pointwise MLP applied in the second stage. The CON-VCNP model introduces a density channel with 4 channels per pixel, using a CNN with residual blocks. The MLP outputs means and standard deviations for a Gaussian predictive distribution. The model is evaluated on the zero-shot multi-MNIST setting to test translation equivariance and generalization capabilities. Training set includes 60000 MNIST training digits on a black background, while the test set consists of 10000 pairs of digits randomly sampled from the MNIST test set. The CONVCNP model uses a density channel with 4 channels per pixel and a CNN with residual blocks. It is evaluated on the zero-shot multi-MNIST setting and shows better generalization compared to ATTNCNP. The test set includes 10000 pairs of digits sampled from the MNIST test set placed on a black background. The CONVCNP model was evaluated by performing an ablation study on the different components of the first layer. Results showed that appending a density channel helps, enforcing the positivity constraint is important with normalized convolution, and using a less expressive EQ filter does not significantly decrease performance. The first filter learned by CONVCNPXL, CONVCNP, and CONVCNP EQ for all datasets was also analyzed. The plotted filters for the first channel (red) vary in size. Using a less expressive EQ filter does not impact performance significantly. CONVCNPXL with a large receptive field performs worse on the ZSMM task compared to CONVCNP. The models suggest that CONVCNPXL learns non-stationary behavior, possibly due to image boundary treatment. The circular padding in ZSMM helps prevent the model from learning non-stationarities as the receptive field size increases. Circular padding makes it harder to distinguish padded values from actual values. The effect of padding on other datasets has not been tested, and circular padding could lead to other issues. The log-likelihood of ZSMM is shown in Figure 15, comparing zero padding to circular padding."
}