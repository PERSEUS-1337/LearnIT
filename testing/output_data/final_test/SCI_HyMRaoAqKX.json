{
    "title": "HyMRaoAqKX",
    "content": "In this paper, the \"implicit autoencoder\" (IAE) is described as a generative autoencoder with both generative and recognition paths parametrized by implicit distributions. Two generative adversarial networks are used to define reconstruction and regularization cost functions, with learning rules based on maximum-likelihood learning. Implicit distributions allow for more expressive posterior and conditional likelihood distributions, enabling the latent code to capture abstract information while the remaining details are captured by the implicit conditional likelihood distribution. Implicit autoencoders can disentangle global and local information and perform deterministic or stochastic reconstructions of images. Deep generative models like the generative adversarial network (GAN) have been successful in learning implicit distributions through a two-player min-max game. GANs can disentangle discrete factors of variation from continuous factors in an unsupervised manner, enabling clustering and semi-supervised learning. In probabilistic machine learning, GANs and VAEs are used to learn expressive distributions through neural networks. VAEs have limitations in learning factorized distributions, leading to the proposal of \"implicit autoencoder\" (IAE) for more expressive posterior and conditional likelihood distributions. Implicit distributions, like those used in adversarial autoencoders and other works, enable learning more expressive posterior and conditional likelihood distributions. This allows for a global vs. local decomposition of information, with the latent code capturing high-level information while the noise vector captures low-level details. The implicit autoencoder (IAE) is proposed in this section, connecting with previous works like PixelCNN autoencoders and PixelVAE. The encoder defines an implicit variational posterior distribution, while the decoder defines an implicit conditional likelihood distribution. The latent code captures high-level information, and the noise vector captures low-level details. The latent code and noise vector are key components in the implicit autoencoder (IAE). The variational distribution q(z|x) matches the joint model distribution p(x, z) to the joint data distribution q(x, z). Various entropies and mutual information are defined under the joint data distribution q(x, z) and its marginals. The implicit autoencoder (IAE) utilizes the aggregated posterior distribution q(z) to define joint and aggregated reconstruction distributions. Different forms of the evidence lower bound (ELBO) are used to describe the IAE and its connections with VAEs and AAEs. The IAE enables learning implicit distributions for posterior and conditional likelihood, unlike VAEs and AAEs. The Implicit Autoencoder (IAE) learns implicit distributions for both the posterior and conditional likelihood using a GAN. Unlike VAEs and AAEs, the IAE uses a powerful implicit decoder for stochastic reconstructions, matching the expressive decoder distribution to the inverse encoder distribution. Other variants of VAEs can also learn expressive decoder distributions using autoregressive neural networks. Autoregressive neural networks are used in autoencoders to encourage matching the decoder distribution to the inverse encoder distribution. In autoencoders, the cost function also aims to minimize conditional entropy or maximize mutual information, enforcing the latent code to capture global and local information. In contrast, Implicit Autoencoders (IAEs) do not penalize the encoder for losing local information as long as the decoder can invert the encoder distribution. The reconstruction cost function of IAEs can be minimized by rewriting it as a distribution matching cost function between the joint distributions. Implicit Autoencoders (IAEs) rewrite the reconstruction cost function as a distribution matching cost function between joint data and reconstruction distributions. This is minimized with the reconstruction GAN. IAEs also have a regularization cost function that matches aggregated posterior distribution with a fixed prior distribution, similar to AAEs. The training process involves passing data through the encoder and decoder to train the discriminator of the reconstruction GAN to distinguish positive and negative examples. The adversarial reconstruction and adversarial regularization processes involve updating encoder and decoder weights by confusing the discriminator with positive and negative examples, respectively. The discriminator in the regularization GAN identifies positive examples from negative examples to update only the encoder weights. The adversarial regularization process involves updating only the encoder weights by confusing the discriminator with positive and negative examples. Optimizing this process encourages the model distribution to capture the data distribution. The gradient obtained from adversarial training only approximately follows the gradient of the variational bound on the data log-likelihood. The objective of the GAN can be modified to optimize any f-divergence, including the KL divergence. An information theoretic interpretation of the ELBO of IAEs using the Bits-Back coding argument is described in the appendix. In IAEs, the dimension and distribution of the latent vector and noise vector control the information decomposition between the latent code and implicit decoder. The global vs. local information decomposition is discussed in Appendix C. Optimization in IAEs can focus on reconstruction or both reconstruction and regularization costs. In IAEs, the reconstruction cost can be optimized by removing noise vectors, making the encoder and decoder deterministic. This leads to exact and deterministic image reconstruction, with the latent code learned unconstrainedly. Unlike standard autoencoders, IAEs use adversarial reconstruction to encourage accurate reconstructions and handle uncertainty differently. The IAE encourages sharp reconstructions by capturing uncertainty with a noise vector. It defines a valid generative model where the latent code captures data distribution information. Sampling involves passing a sample from the prior through a deterministic decoder. The optimization focuses on matching joint distributions while p(x|z) is a stochastic implicit distribution. The model defines a generative process where the latent code captures data distribution information. Sampling involves passing a sample from the prior through a decoder to obtain a reconstruction. The optimization focuses on matching joint distributions, with the decoder being either deterministic or stochastic. The global latent code in the IAE captures high-level image information, similar to PixelCNN autoencoder BID23. The decoder uses an autoregressive neural network to learn distributions, while the latent code is unconstrained. To address the issue of the prior not being easily sampled from, a parametric prior p(z) can be fitted to q(z) after training or a fixed prior can be imposed during training as a regularization term. By adding an adversarial regularization cost function to match q(z) to p(z), the model ensures r(x) = p data (x) = p(x). Sampling from this model involves sampling from the prior z \u223c p(z) and then from the conditional implicit distribution to obtain x \u223c r(x). The latent code captures high-level information, while the decoder captures local details. This information decomposition is empirically demonstrated on different datasets. The main drawback of existing methods like PixelVAE, variational autoencoders, and PixelGAN is their use of non-parallelizable autoregressive decoders, making them computationally expensive. Implicit decoders, like in IAE, capture local statistics in the code representation, unlike autoregressive decoders. IAE connects with ALI and BiGAN models, using separate networks for joint data and model distributions. In the IAE model, the encoder and decoder are stacked and trained jointly, with the input to the conditional likelihood being samples of the variational posterior distribution. This training dynamic is similar to autoencoders, leading to better reconstructions. Various ALI variants have been proposed for improved performance, such as HALI using a Markovian generator for better reconstructions and ALICE augmenting the cost for joint optimization. The IAE model uses a joint distribution matching cost function to learn global vs. local information decomposition. By adjusting the dimensions of global and local codes, control over information decomposition is achieved. Removing the local code results in a deterministic autoencoder with perfect reconstructions. Varying the global and local code sizes affects the IAE's performance on the MNIST dataset. The IAE model utilizes global and local codes to capture different aspects of information. Adjusting the dimensions of these codes allows for control over information decomposition. Varying the sizes of the global and local codes impacts the model's performance on different datasets, as shown in Figure 3 with the SVHN dataset. The IAE model uses global and local codes to capture information, with different code dimensions affecting performance. For example, a 150D global code without a local code achieves almost perfect reconstructions, while a 75D global code with a 1000D local code only captures middle digit information. The implicit decoder can generate synthetic digits based on the middle digit style. On the CelebA dataset, a 150D global code without a local code also achieves almost perfect reconstructions, but a 50D global code with a 1000D local code only retains general face shape information. The IAE model uses global and local codes to capture information, with different code dimensions affecting performance. In IAEs, a categorical global code along with a Gaussian local code can disentangle factors of variation for clustering and semi-supervised learning. To perform clustering with IAEs, the architecture is modified by using a softmax function in the encoder's last layer for a continuous relaxation of the categorical global code. The regularization GAN is trained on the continuous output probabilities of the softmax simplex to impose the categorical distribution on the aggregated posterior distribution. The adversarial regularization in the IAE model enforces constraints on the encoder output for confident cluster assignments and even distribution of points. The global code captures discrete factors like class labels, while the Gaussian local code handles the image structure. The GAN trained on mixture data shows that the GAN struggles to associate categorical noise vectors with different components, relying heavily on Gaussian noise. The IAE model uses discrete latent variables to improve generative models, with a 7D categorical global code and a 10D Gaussian noise vector. It clusters data in an unsupervised manner and conditions on inferred cluster labels to generate mixture components. This approach contrasts with GANs, which struggle to incorporate categorical noise vectors effectively. The IAE model utilizes discrete latent variables for generative modeling, with a 30D categorical global code and a 10D Gaussian local code. It demonstrates clustering performance on the MNIST dataset, capturing digit identities with the global code and writing styles with the local code. The network achieves a 5% error rate in unsupervised digit classification by matching clusters to digit types. The IAE model uses discrete latent variables for generative modeling, achieving a 5% error rate in unsupervised digit classification by matching clusters to digit types. It can also be used for semi-supervised classification, achieving low error rates on MNIST and SVHN datasets compared to other models like AAE and Improved-GAN. The \"Flipped Implicit Autoencoder\" (FIAE) is closely related to IAEs, utilizing latent code from a prior distribution. The FIAE model parametrizes implicit distributions for conditional likelihood and variational posterior. It defines joint and aggregated latent reconstruction distributions. FIAEs optimize the reverse KL divergence by splitting it into a reconstruction and regularization term. The FIAE model includes a regularization term and a reconstruction term, utilizing a GAN to train the encoder and decoder. It differs from ALI and BiGAN models in that the recognition network only sees synthetic samples from the simulated data. Training the recognition network on simulated data in FIAEs is similar to the \"sleep\" phase of the wake-sleep algorithm. One drawback is that early in training, the simulated data may not resemble real data, leading to potential issues with the recognition path learning to invert the generative path in a non-representative data space. However, in experiments with FIAEs, this was not a major problem. Connections with InfoGAN are discussed in relation to FIAEs. InfoGANs use an explicit reconstruction cost function on the code space for learning the variational posterior. Training FIAEs and InfoGANs on a toy dataset with a 2D Gaussian prior shows differences in the posterior distribution. InfoGANs enforce a factorized Gaussian variational posterior distribution through the Euclidean cost, restricting the model's conditional likelihoods. The model has learned a conditional likelihood whose true posterior is axis-aligned to match the factorized Gaussian variational posterior. Optimizing the reverse KL divergence leads to mode-covering behavior in the variational posterior, unlike the mode-picking behavior in FIAE, which can learn a more expressive q(z|x). This mode-averaging behavior can be observed in the Gaussian posteriors of different data-points in InfoGAN. The FIAE objective is an upper-bound on KL(p(x) p data (x)), KL(p(z) q(z)), and KL(p(z|x) q(z|x)), matching variational and true posteriors. In contrast, InfoGAN lacks this upper-bound, leading to potential mismatches in posteriors. The FIAE can invert its conditional likelihood function and perform reconstructions of images by removing noise vectors. It can also be used for clustering, as demonstrated on the MNIST dataset with discrete categorical latent codes and continuous Gaussian noise vectors. The implicit autoencoder uses Gaussian noise vectors to capture style and perform inference on digit identity. It achieves a 2% error rate in classifying digits unsupervisedly by matching categorical codes to digit types. The implicit decoder efficiently captures uncertainty in digit identity through different noise vector draws. The implicit autoencoder uses implicit distributions to learn expressive variational posterior and conditional likelihood distributions. It can disentangle high-level and abstract information from low-level statistics using a low-dimensional Gaussian distribution for the global code. By using a categorical latent code, it can learn discrete factors of variation and perform clustering and semi-supervised learning. The ELBO of IAEs can be interpreted information-theoretically using Bits-Back coding argument. The code is designed under the model distribution p(x). The sender transmits z first, requiring H(z) bits ideally, but pays a penalty of KL(q(z) p(z)) extra bits due to mismatch. The receiver resolves uncertainty of q(x|z) to reconstruct x, needing H(x|z) bits ideally but paying E z\u223cq(z) KL(q(x|z) p(x|z)) extra bits on average. The autoencoder fails to achieve perfect stochastic reconstruction. The sender can encode other information using the stochasticity of q(z|x). The sender can use the stochasticity of q(z|x) to encode additional information, gaining H(z|x) \"bits back\" that need to be subtracted to determine the true cost of transmitting x. The VAE minimizes the total number of bits required for transmission, while the IAE only minimizes the extra bits needed for transmitting x. The Bits-Back argument applies to continuous random variables as well, where transmitting the real-valued random variable z incurs a cost of H(z) = h(z) - log \u2206z bits. The sender can use the stochasticity of q(z|x) to encode additional information, gaining H(z|x) \"bits back\" that need to be subtracted to determine the true cost of transmitting x. The VAE minimizes the total number of bits required for transmission, while the IAE only minimizes the extra bits needed for transmitting x. In IAEs, the global code captures the global information of data, while the local noise vector captures the remaining local information. The global vs. local decomposition of information is described from an information theoretic perspective. The sender transmits z first, then transmits the residual bits for reconstructing x. If capacities of p(z) and p(x|z) are limited, extra bits are sent due to distribution mismatch, resulting in regularization and reconstruction errors. Different decompositions of information lead to different numbers of extra bits. The sender must decompose information compatible with source codes based on p(z) and p(x|z), using a low-dimensional Gaussian or categorical distribution. Regularization cost encourages encoding simple concepts in z consistent with p(z) to minimize extra bits. The choice of information in z affects the reconstruction cost, as it must remove stochasticity of q(x|z) to lower burden on p(x|z) for matching. The reconstruction cost encourages learning concepts that reduce uncertainty from the data distribution by balancing regularization and reconstruction costs. The latent code learns global concepts like digit identities in datasets or objects in images. Two methods are used to condition the reconstruction GAN on the global code. The method involves transforming the global code to a tensor and adding it to the discriminator as an adaptive bias. This allows the global code to learn location-invariant information such as class labels. The method involves using a two-layer neural network as the regularization discriminator in clustering and semi-supervised learning experiments. The architecture of the encoder, decoder, and reconstruction discriminator for each dataset is specified."
}