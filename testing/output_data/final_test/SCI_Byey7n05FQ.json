{
    "title": "Byey7n05FQ",
    "content": "We propose a \"plan online and learn offline\" framework for an agent with an internal model to act and learn in the world. Our work focuses on the relationship between local model-based control, global value function learning, and exploration. We investigate how local trajectory optimization can handle approximation errors in the value function and accelerate learning. Additionally, we explore how approximate value functions can improve policies beyond local solutions. Finally, we show how trajectory optimization can aid in temporally coordinated exploration to enhance the learning process. In a setting where an agent with limited resources is dropped into a complex world, it must simultaneously act and learn to become proficient in various tasks. The agent may have some prior knowledge in the form of a dynamics model, but the large and diverse state space of the world, combined with limited computational capabilities, requires the agent to learn while acting. This necessitates the agent to become competent through experience rather than relying on omniscient knowledge. The agent, with limited resources, needs to act and learn to become competent in a complex world. It uses trajectory optimization as a local search procedure to explore and learn about the environment. Despite limited computational resources, the trajectory optimizer helps the agent gain experience and become moderately competent in new tasks and skillful in repeated situations. The proposed plan online and learn offline (POLO) framework aims to address performance degradation when acting greedily using an approximate value function. It integrates local trajectory optimization, global value function learning, and exploration to accelerate and stabilize value function learning. Exploration is crucial for propagating global information and escaping local solutions in trajectory optimization. POLO framework utilizes trajectory optimization for efficient exploration in tasks like maze navigation, humanoid manipulation, and object re-positioning. The agent forms reward hypotheses and executes coordinated actions to find global solutions, contrasting with traditional exploration strategies. The framework addresses performance degradation by integrating local trajectory optimization, global value function learning, and exploration to accelerate and stabilize learning. In this work, the POLO framework combines local trajectory optimization, global value function approximation, and exploration strategy for robotic agents to continually learn and acquire skills in the physical world. The framework utilizes a nominal dynamics model to accelerate learning of novel task instances. The POLO framework combines local trajectory optimization, global value function approximation, and an uncertainty and reward aware exploration strategy for robotic agents to learn skills in the physical world. It models the world as an infinite horizon discounted Markov Decision Process (MDP) with continuous state and action spaces, a reward function, dynamics model, and discount factor. The policy's performance is evaluated over a start state distribution. The agent in a complex world uses an internal model to explore and understand the state space. It can revisit states, reason about actions, and learn efficiently. The optimal value function describes long-term rewards under the optimal policy. For small MDPs, dynamic programming algorithms like value iteration can be used. For small MDPs, value iteration can find the optimal value function. In more complex cases, approximate techniques like fitted value iteration with a neural network approximator are used to compute the optimal value function. The success and convergence of the procedure in Eq. (2) depend on the function approximator's capacity and structure (\u03b8) and the sampling distribution (\u03bd). Lemma 1 states that the performance of the policy degrades with a dependence on the effective problem horizon determined by \u03b3, as the policy pays a price at every timestep. In practice, errors may be inevitable due to function approximation, especially in temporally extended tasks where \u03b3 \u2248 1. The arg max operation in \u03c0 could exploit approximation errors, affecting policy quality. Fitted value iteration methods rely on sampling distribution for global information propagation, crucial in sparse reward settings. Generating good sampling distributions automatically for new tasks is challenging, akin to exploration. Trajectory optimization and model predictive control (MPC) in robotics involve computing locally optimal action sequences based on dynamics models. The optimization problem includes running and terminal reward functions, with local policies obtained as solutions. The first policy is used for MPC. The optimization problem involves using a local time-indexed policy for MPC. This approach has been successful in various control systems like power grids and robotics. MPC looks forward only a few steps, making it a local method unless coupled with a global value function. The synergy between MPC and global value function is a key motivation for POLO. The synergy between Model Predictive Control (MPC) and global value function is a primary motivation for POLO. MPC policy with horizon H > 1 is less affected by approximation errors compared to greedy action selection. Additionally, MPC enables faster convergence of the value function approximation. In the tabular setting, |B DISPLAYFORM2 H allows for propagation of global information for H steps, accelerating convergence. MPC based on local dynamic programming methods provides an efficient way to realize B H for continuous control problems, accelerating and stabilizing value function learning. Using MPC allows the agent to explore in the space of trajectories, critical for the convergence of RL algorithms. Using Model Predictive Control (MPC), the agent can explore trajectories and consider potential reward regions in the state space. This approach, known as Plan Online and Learn Offline (POLO), involves selecting actions based on MPC, updating value functions, and executing temporally coordinated actions to cover the state space efficiently. The text discusses the benefits of exploring state space efficiently using Model Predictive Control (MPC) and the Plan Online and Learn Offline (POLO) approach. It highlights the use of Bayesian methods to track posterior over value functions and demonstrates the benefits of uncertainty estimation for exploration. The text discusses using Model Predictive Control (MPC) and the POLO approach for efficient state space exploration. It utilizes Bayesian methods to track posterior over value functions and emphasizes uncertainty estimation for exploration. POLO employs a global value function approximation scheme, local trajectory optimization, and optimistic exploration. The POLO scheme utilizes an internal model and MPC to pick optimal actions, handling exploration through value function uncertainties. Experiences are stored in a replay buffer, with value functions updated every Z steps using trajectory optimization. The POLO scheme utilizes trajectory optimization to generate targets for value function approximation, accelerating convergence and improving stability. Empirical evaluation aims to answer key questions regarding exploration strategies, planning horizon reduction, and value function learning speed. POLO can handle complex high-dimensional agents like 3D humanoid and anthropomorphic hand BID23. Video demonstrations are available. The POLO scheme uses trajectory optimization for value function approximation, focusing on exploration in tasks with misaligned rewards. It considers a point mass agent in various 2D worlds, showcasing the interaction between trajectory optimization and exploration. POLO, with its ensemble of value functions, utilizes MPC for coordinated actions, contrasting with random exploration. POLO, with its ensemble value function, outperforms alternatives in region coverage in point mass worlds. It utilizes MPC for coordinated actions, allowing navigation through corridors effectively. In testing, POLO's learned terminal value function outperforms pure MPC in average reward over trials in different tasks. The agent's height increase shows task-relevant value function. Value learning may reduce planning horizon for MPC in high-dimensional tasks like humanoid getup and in-hand manipulation. Comparisons between MPC and full POLO algorithm performance are made in FIG2. After testing, it was found that POLO's learned terminal value function outperforms pure MPC in average reward over trials in different tasks. The agent's height increase demonstrates task-relevant value function. Value learning can reduce the planning horizon for MPC in high-dimensional tasks like humanoid getup and in-hand manipulation. Comparisons between MPC and the full POLO algorithm performance are shown in FIG2. POLO consistently outperforms MPC, allowing the agent to escape local solutions and stand up consistently. The humanoid is able to increase its height from the floor, indicating that the value function captures task-relevant details. Greedy optimization with this value function does not yield good results, suggesting it is only approximate. In-hand manipulation tasks have spatial complexity with rapidly changing targets, leading to high variance in gradient estimates for function approximation methods. Trajectory optimization is well-suited for efficiently computing near-optimal actions in such scenarios. The trajectory optimizer optimizes a fixed instance of the task, while the value function learns from multiple target changes to give high values to states with affordances for upcoming tasks. Increasing the horizon (N) for computing value function targets accelerates learning with fewer interactions. The use of N-step returns helps balance bias-variance trade-off in stable value function learning and actor-critic methods. Longer planning horizons make trajectory optimization more tolerant to errors in the value function, as shown in experiments. This suggests the potential for using Model Predictive Control (MPC) to enhance transfer learning between tasks or robot platforms. Transfer learning between tasks or robot platforms has been explored using planning and search with approximate value functions in discrete game domains. Prior data can guide the search process in continuous Model-based Cross-Entropy Method without explicitly learning a value function. Some approaches consider learning value functions based on states visited by the agent, but do not explicitly use planning. The contribution of value functions to Model Predictive Control was found to be weak for simple tasks. Cost shaping can also be seen as specifying an approximate value. Planning and exploration in reinforcement learning involve various approaches such as cost shaping and local trajectory optimization. These methods aim to improve over Model Predictive Control by generating datasets for training global policies through imitation learning. Unlike MPC, these approaches may require retraining for task or environment changes. Exploration is a crucial aspect in RL, and through exploration schemes, tasks where MPC fails can be successfully solved. Exploration in reinforcement learning is a critical aspect, with strategies like -greedy and Gaussian exploration being used to solve dense reward problems. However, in high-dimensional settings with sparse or delayed rewards, these strategies can become intractable. Parameter-space exploration methods generate correlated behaviors based on explored parameters at the start, but do not consider exploration as an intentional act. Deep exploration strategies sample a value function from the posterior to explore efficiently. Our work investigates exploration strategies in reinforcement learning, focusing on deep exploration methods that sample value functions from the posterior for action selection. Unlike approaches based on intrinsic motivation or information gain, our work incorporates planning to explore efficiently and reach high predicted reward regions. This approach is similar to the E3 framework but differs in that it utilizes Model Predictive Control (MPC) to quickly solve altered MDPs for local instance-specific solutions. Our work explores how training times can be reduced with an accurate internal model in reinforcement learning. For example, POLO only requires 12 CPU core hours and 96 seconds of agent experience for certain tasks, compared to model-free methods. This suggests that less experience may be needed for the control aspect of the problem. Our work serves as a strong model-based baseline for model-free RL to compete with and is useful for researchers studying simulation to reality transfer. Our work demonstrates the effectiveness of learning value functions for real-time action selection using Model Predictive Control (MPC) in high-dimensional continuous control tasks. This approach contrasts with using internal models for variance reduction in model-free Reinforcement Learning (RL) and focuses on improving the quality of internal models for near-optimal performance. In this work, POLO combines trajectory optimization and value function learning for action selection in high-dimensional continuous control tasks. Planning for exploration is studied to control complex agents like 3D humanoid and five-fingered hand. Future work includes studying approximation errors in the internal model and improving it over time using real-world interaction data. The model used for humanoid experiments has 27 degrees of freedom. The model used for humanoid experiments has 27 degrees of freedom and utilizes direct torque actuation for control with a small timestep of 0.008 seconds. For value function approximation in POLO, an ensemble of 6 neural networks is used, each with 2 layers and 16 hidden parameters. Training involves 64 gradient steps on minibatches of size 32 using ADAM every 16 timesteps. The agent experiences a horizon of 600 timesteps with 20 episodes, totaling 12000 timesteps or 96 seconds. In the experiments, the agent's lifetime consists of 20 episodes totaling 12000 timesteps or 96 seconds. A control cost is shared for each scenario, with task-specific rewards in the getup and walking scenarios. In the getup scenario, the agent aims to reach a target height of 1.1 meters, while in the walking scenario, it is penalized for deviation from target height, speed, and distance from the world's x-axis. The agent is encouraged to walk in a straight line with a target speed of 1.0 meters/second. In the box environment, a 0.9 meter wide cube needs to be pushed to a specific point with low friction causing the box to slide out of reach. POLO learns to limit the initial push to achieve the goal."
}