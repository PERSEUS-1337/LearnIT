{
    "title": "ByebT3EYvr",
    "content": "Counterfactual Regret Minimization (CFR) is a successful algorithm for finding Nash equilibria in imperfect information games, but its scalability is limited due to full game-tree traversals. Deep CFR applies deep learning to CFR, allowing abstraction and generalization without expert knowledge. Single Deep CFR (SD-CFR) is a variant with lower approximation error, avoiding the training of an average strategy network. SD-CFR is a theoretical and empirical improvement over Deep CFR in poker, converging towards a Nash equilibrium for imperfect information games. It outperforms Deep CFR in exploitability and one-on-one play. Researchers have augmented CFR with neural network function approximation to address scalability issues in poker games like HUNL. This approach led to the development of DeepStack, which, along with Libratus, was able to defeat professional players in HUNL. In the game of HUNL, professional poker players have been defeated by Deep CFR, a variant of tabular CFR that uses neural networks for strategy approximation. Single Deep CFR (SD-CFR) simplifies this approach by obtaining its final strategy after just one neural approximation, reducing the complexity. SD-CFR improves convergence in poker games by training an additional network for weighted average strategy approximation. Extensive-form games are introduced, defining histories, actions, and players. This work focuses on zero-sum games with imperfect information represented by partitioning H into information sets. Each player chooses actions according to a behavioural strategy \u03c3 i. A strategy profile \u03c3 is a tuple of players' strategies. In zero-sum games with imperfect information, players choose actions based on behavioral strategies. A strategy profile \u03c3 is a tuple of players' strategies. A Nash equilibrium is reached when no player can increase their expected utility by deviating from their strategy while the other player plays optimally. The exploitability of a strategy profile is measured by how much its optimal counter-strategy can beat it. Counterfactual Regret Minimization (CFR) is an iterative algorithm used in zero-sum games with imperfect information. It can run simultaneous or alternating updates to produce new iteration-strategies for players. The algorithm converges to a Nash equilibrium by minimizing regret, which quantifies how much a player could have won if they had chosen a different action. The iteration-strategy for a player is derived based on the overall regret on each iteration. The Counterfactual Regret Minimization (CFR) algorithm aims to converge to a Nash equilibrium by minimizing regret in zero-sum games with imperfect information. Various improvements have been proposed over the years, including alternative regret update methods, automated abstraction design schemes, and sampling variants of CFR. Real-time solving or re-solving techniques have also been employed in successful algorithms. Discounted CFR (DCFR) and linear CFR (LCFR) are modifications that slightly alter the equations for regret and iteration-strategies. Monte-Carlo CFR (MC-CFR) proposes tabular methods that visit only a subset of information sets on each iteration, with variants like External Sampling (ES) and Average Strategy Sampling being useful in games with many player-actions. LCFR and CFR + are fast variants compatible with CFR. Deep CFR is a sample-based method that approximates linear CFR with alternating player updates. It fits a value network for one player to approximate advantage in two-player zero-sum games. It divides reach-probabilities to address neural network learning challenges across orders of magnitude. Deep CFR modifies the iteration-strategy by heuristically choosing the action with the highest advantage. It obtains training data via batched external sampling and stores regret values in a memory buffer updated through reservoir sampling. Training losses are weighted based on iteration-number, and a neural network is fitted to approximate the linear average strategy at the end of the training procedure. Deep CFR uses a neural network to approximate the linear average strategy, collecting training data in a separate reservoir buffer. It weights the training loss based on iteration-number and controls the expected frequency of datapoints from different iterations in its buffers. Storing all iteration-strategies allows for computing the average strategy during play in both tabular and approximate CFR variants. Deep CFR eliminates the need to store multiple large tables during training and play by using neural networks to compress strategies. It avoids sampling and approximation errors, reducing computational work. SD-CFR selects a value network at the start of the game to provide action samples based on the average strategy. The method trajectory-sampling assigns sampling weights to each information set and uses the policy \u03c3 i for the entire game trajectory. This ensures that iteration-strategies are weighted proportionally to their reach-probabilities in any state along the trajectory. The query cost remains constant with the number of iterations. The computation in SD-CFR involves feedforward passes through each network in B Mi, caching reach-probabilities along the trajectory, and mimicking T i correctly. SD-CFR may not be equivalent to linear CFR like Deep CFR, as shown in an experiment. Deep CFR's performance degrades if reservoir sampling is done on B M after exceeding buffer capacity. The neural network for Deep CFR in large poker games has under 100,000 parameters and requires under 400KB of disk space. Storing 25,000 networks on disk would only need 10GB of space. Keeping all value networks in memory is not a problem in practice. SD-CFR is a better approximation of linear CFR as long as all value networks are stored. In theory, SD-CFR is superior to linear CFR if all value networks are stored. Empirical evaluation compares SD-CFR to Deep CFR, using the same value networks to ensure convergence to the same Nash equilibrium. Environment observations in Leduc Poker include pot size and card representation. In Leduc Poker, players start with an ante and a private card, followed by two betting rounds. The winner is determined by hand strength, with pairs winning over high cards. Hyperparameters favor Deep CFR, but SD-CFR minimizes exploitability better. Storing all value networks is feasible, as concluded in the study. The study analyzed the effect of reservoir sampling on B M, showing plateauing and oscillation up to |B M | = 1000. Results of matches between SD-CFR and Deep CFR in 5-Flop Hold'em Poker were presented, highlighting disagreements in average strategies. The neural architecture and shared value networks during training were discussed. The y-axis plots SD-CFR's average winnings. The study compared SD-CFR and Deep CFR in 5-Flop Hold'em Poker, showing SD-CFR's superiority even after reaching maximum training capacity. The average strategies of both algorithms were analyzed at different depths of the game tree. The study compared SD-CFR and Deep CFR in 5-Flop Hold'em Poker, showing SD-CFR's superiority even after reaching maximum training capacity. We ran 200,000 trajectory rollouts for each player, evaluating on comfortable trajectories. Deep CFR's approximation is good on early levels but has larger errors in information sets after multiple decision points. Regression CFR (R-CFR) applies regression trees to estimate regret values but recent work failed to combine it with sampling. Advantage Regret Minimization (ARM) is similar to R-CFR but only applied to single-player environments. DeepStack defeated professional poker players in one-on-one gameplay of Heads-Up No-Limit Hold'em Poker using real-time solving and counterfactual value approximation with deep networks. However, it relies on tabular CFR methods without card abstraction, limiting its application to domains with more private information states. NFSP was the first algorithm to apply deep reinforcement learning from single trajectory samples to large environments. NFSP was able to learn a competitive strategy in Limit Texas Hold'em Poker over just 14 GPU/days. Recent literature introduces novel actor-critic algorithms with similar convergence properties as NFSP and SD-CFR. Deep CFR was only evaluated in games with three player actions, suggesting alternative sampling methods for games with more actions. Continuous approximations can avoid action translation in action-abstracted games. After training in an action-abstracted game, continuous approximations of large discrete action-spaces could be achieved by having value networks predict parameters to a continuous function. The iteration-strategy could be derived by normalizing the advantage clipped below 0. SD-CFR could potentially be applied to less structured domains than poker with modifications to its neural architecture and sampling procedure. Evaluating whether SD-CFR is preferred over other approaches such as PPO in deep reinforcement learning methods could be a first step in this research direction. Single Deep CFR (SD-CFR) is a new variant of CFR that uses function approximation and partial tree traversals to generalize over the game's state space. It extracts the average strategy directly from a buffer of value networks from past iterations. SD-CFR is more attractive in theory and performs better in practice than Deep CFR. Data is collected over 1,500 external sampling traversals on each iteration, and new value networks are trained to convergence. Average-strategy networks are also trained to convergence from a random initialization. Leduc Hold'em Poker is a two-player game with 3 fully-connected layers of 64 units each. Players add 1 chip as an ante and are dealt a private card from a deck of 6 cards. The game has pre-flop and flop rounds where players can choose actions like fold, call, or raise. The first player to act can also check. In Leduc Hold'em, players can choose actions like fold, call, or raise. The number of raises per round is capped at 2, with each raise adding chips to the pot. On the transition from pre-flop to flop, a card is revealed publicly. The winner is determined by matching private cards to the flop card or by the highest card according to A B C. In experiments investigating function approximation harm in Leduc Hold'em variants with more than 3 ranks and multiple bets, Deep CFR and SD-CFR showed closer performance. The exploitability curves of early iterations in a variant with 12 ranks and 6 bets per round were plotted, showing smaller performance differences. The equilibrium in this game is less sensitive to small action probability differences, making it less susceptible to approximation errors despite having more states. In experiments with a variant of Leduc Hold'em with 12 ranks and 6 bets per round, Deep CFR and SD-CFR were compared. The game, with 12 distinguishable cards, is less prone to approximation errors. The networks were trained with specific parameters and architecture, with slight differences in the card-branch units."
}