{
    "title": "BJl3s1h9aV",
    "content": "LSTM-based language models learn long-range dependencies compositionally by building them from shorter constituents during training. Backpropagation through time for a language model involves carrying associations through intervening constituents, known as conduits, to affect representations. Training examples may use predictable conduits structured in familiar ways to facilitate learning. The text discusses the importance of conduits in carrying gradients for long-range dependencies in LSTM-based language models. It explains how the backpropagated message is multiplied by gradients in the conduit, and how the magnitude and distribution of recurrence gradients affect learning. The text explores how the predictability of conduits impacts the learning of long-range patterns in language models. Training models on synthetic data reveals that memorizing local patterns can expedite learning but hinders the acquisition of long-range rules. Understanding the hierarchical structure of language model representations is crucial for comprehending LSTM training and neural learning dynamics. The hierarchical structure is essential for high performance in neural language models. LSTMs learn effectively from natural language data, encoding information about part of speech, morphology, and verb agreement. However, the compositional nature of these representations does not necessarily reflect the learning process, which differs from human learning methods. The LSTM's ability to learn hierarchically and encode context-sensitive languages is influenced by inductive priors in the training process. While LSTMs can learn associations from recent items in a sequence, they can also encode grammatical inflection from the first word. The hierarchical nature of the representation may be a result of the data rather than induced by training biases. The LSTM's ability to learn hierarchically and encode context-sensitive languages is influenced by inductive priors in the training process. Investigating further into LSTM learning dynamics, it is debated whether learning begins with a memorization phase followed by a compression phase to make the model more general. The transition from memorized to compressed rules may explain easy-first learning, with dependency range affecting the order of learning in an LSTM. The learning dynamics of an LSTM show that lower layers converge faster than higher layers, implying that local connections are learned before distant connections. Simple local rules are learned first, but may not be used compositionally in constructing longer rules. The learning dynamics of an LSTM show that lower layers converge faster than higher layers, implying that local connections are learned before distant connections. Simple rules learned early on might inhibit the learning of more complex rules through gradient starvation. Experiments use a one layer LSTM with inputs from an embedding layer and outputs processed by a softmax layer. Hidden dimensions are 200, learning rate is set at 1, and gradients are clipped at 0.25. Momentum and weight decay are not used as they slow rule learning. In our running example, contextual decomposition (CD) is used to analyze the individual influences of words and phrases in a sequence on the output of a recurrent model. CD breaks down the output vector from an LSTM layer into relevant (contributed only by the input word or phrase of interest) and irrelevant parts, making it easier to understand the impact of specific elements in the sequence. The dynamics of LSTMs are approximately linear in natural settings, as shown by BID10 using canonical correlation analysis. The CD approach linearizes the output of a LSTM layer to separate relevant components from the rest of the sequence. BID11 focused on analyzing phrases and words, while we aim to determine if a dependency between two words has been learned. The decomposed logits can be used as inputs for a softmax. The decomposed logits from LSTM can be converted into probability distributions using softmax. This allows for analyzing the effect of specific elements on later elements while controlling for the rest of the sequence. Synthetic data is used to test LSTM's ability to learn long-distance dependencies, considering the irregularity of natural language and the frequency of rules. The synthetic data sets test LSTM's ability to learn long-distance dependencies by generating data with fixed occurrences of the relationship, using a vocabulary \u03a3 and inserting instances of the long-distance rule \u03b1\u03a3 k \u03c9. The analysis shows that CD yielded an approximation error, but there are limitations in isolating the role of long-range dependencies in natural language data. The predictability of language data differs from synthetic data as LSTM models take advantage of linguistic structure. Conduit length affects the model's ability to learn rules, requiring more examples for longer conduits. Probability assigned to symbols is based on contributions of open symbols, excluding interactions from other tokens. The probability of the close symbol is influenced by the conduit length and the open symbol, as shown in FIG2. Modifying the synthetic data to increase conduit predictability impacts the model's ability to learn rules. In an experiment with conduits from a vocabulary of 100, each conduit appears 10 times in the training corpus. In the predictable-conduit setting, conduits are distributed throughout the corpus, inhibiting long-term learning of the rule but promoting early learning. This suggests that long-range dependencies are learned from constituent structures. The model's ability to learn longer dependencies is delayed during training due to the need for effective learning of constituents first. Longer rule spans require more examples for an LSTM model to learn effectively. A predictable conduit between rule symbols aids in early rule learning but hinders the model's confidence in learning long-range connections."
}