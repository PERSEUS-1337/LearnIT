{
    "title": "H1guaREYPr",
    "content": "This work proposes a multi-modal learning framework to generate human faces from voice data without human-labeled annotations. Inference networks are trained to match speaker identity across modalities, and cooperate with the generation network using conditional voice information. Studies in neuroscience and machine learning have shown the effectiveness of utilizing audio-visual cues for person recognition. The study explores the relationship between faces and voices in human speech production, aiming to match identities and generate face images from speech signals using a two-step approach. The inference networks for each modality are trained to extract features and compute useful information for cross-modal self-supervision. The study uses inference networks to extract features and compute cross-modal identity matching probability between speech and face. To generate face images from speech, latent factors are incorporated into the neural network using conditional generative adversarial networks (cGANs). This approach aims to capture diverse characteristics in natural face images. Conditional generative adversarial networks (cGANs) incorporate latent factors into the neural network to produce face images dependent on both speech condition and stochastic variables. The use of cGANs aims to disentangle facial attributes relevant to voice from irrelevant ones. However, challenges arise in providing raw signals like speech to the network, requiring an encoder module to generate pseudo conditional labels for meaningful information extraction. The text discusses the use of conditional generative adversarial networks (cGANs) to generate faces from voice inputs. A new loss function called relativistic identity cGANs (relidGANs) is proposed to improve the distinctiveness of generated faces. The approach involves training an encoder to extract meaningful information from speech inputs and utilizing pseudo-embedded conditional vectors for training the generator and discriminator adversarially. The text discusses the use of cGANs to generate faces from voice inputs, introducing a new relidGANs loss function for identity preservation. It explores self-supervised learning in audio-visual data without labels, focusing on cross-modal identity matching tasks. In recent studies, there has been a focus on matching speaker identity between faces and voices in the machine learning community. Different approaches have been proposed, such as using CNN-based biometric matching networks and modeling personal identity nodes to allow for more flexible inference. DIMNet, a new model, was also introduced for this purpose. In recent studies, there has been a focus on matching speaker identity between faces and voices in the machine learning community. Different approaches have been proposed, such as using CNN-based biometric matching networks and modeling personal identity nodes to allow for more flexible inference. Wen et al. (2019a) proposed DIMNet, where two independent encoders are trained to map speech and face into a shared embedding space, classifying each independently using supervised learning signals. Duarte et al. (2019) and others have worked on generating face images from speech, exploring cross-modal generation possibilities. The speech encoder was trained to estimate the input parameter of the face decoder directly, incorporating stochasticity in the latent space for generating different face images. This method differs from previous approaches by addressing the generation process's stochasticity and the challenge of modeling the image. The speech encoder is fixed and trained to produce larger image sizes. The model is trained on the AVSpeech dataset with diverse dynamics. The work aims to train the whole inference and generation stages using self-supervised learning, a first attempt. Two encoders are trained for each modality to identify speaker identity. The speech encoder, utilizing SincNet and 1d-CNN layers, outperforms traditional MFCC features in speaker verification. A 2d-CNN based face encoder with residual connections is also used. Both encoders are trained to identify speaker identity in a cross-modal matching task. The speech encoder, using SincNet and 1d-CNN layers, surpasses MFCC features in speaker verification. A 2d-CNN face encoder with residual connections is employed. The cross-modal identity matching task involves selecting faces from speech segments and vice versa, computed by inner product of embedding vectors and softmax function. The method allows flexibility in inference and training settings, enabling different K values in test and training phases. The method involves training two encoders using self-supervised learning signal from crossentropy error. The latent space of the face is divided into a deterministic variable from the speech encoder and a random variable sampled from a Gaussian distribution. This is modeled using cGANs to generate face images conditioned on speech by concatenating Gaussian noise and speech condition as input for the generator function. The AdaIN technique is used for conditioning in the generator network, while the projection discriminator method is adopted for conditioning in the discriminator. This allows for effective provision of condition information to the discriminator. The inner-product of two vectors, c and \u03c6(f), is used to provide condition information to the discriminator. The conditioning signals are replaced with an inner-product operation, and Eq. 2 is rewritten using trained speech and face encoders. Relativistic GANs loss is adopted for stable image generation performance. Refer to Jolicoeur-Martineau (2019) for more details. The proposed modification to the objective function for stable image generation performance involves combining the condition term with relGANs loss. The discriminator is adjusted to penalize mismatched face and voice more than positively paired ones, addressing the issue of catastrophic forgetting. This approach utilizes non-saturating loss and is detailed in Jolicoeur-Martineau (2019). The proposed objective function (relidGANs loss) for discriminator LD and generator LG is defined based on the AVSpeech dataset, which contains diverse face images extracted from YouTube videos. The dataset is challenging for training a generation model due to the variety of facial expressions and video quality, making it a fully self-supervised training process. The dataset used is fully self-supervised, with each audio-visual clip representing an individual identity. Positive pairs are sampled within a single clip, while negative samples are randomly selected from other clips. The datasets used are VoxCeleb and VGGFace, which provide diverse audio-visual and facial images for training models. The dataset used for training models includes audio and facial images from VoxCeleb and VGGFace, with 1,251 speakers. Face images are from both datasets, while speech audio is from VoxCeleb. Training involved using 6-second audio samples, with shorter or longer samples adjusted. Stochastic gradient descent optimizer was used during training, and Adam optimizer during generation. The discriminator network was regularized using R. The inference networks were trained on the AVSpeech dataset with no additional information about speaker identity. The models were trained in two settings (V-F and F-V) in a 10-way setting. Evaluation was conducted 5 times with different negative samples each time. Results in Table 1 show the network's ability in cross-modal identity matching task. Our network can perform crossmodal identity matching with reasonable accuracy in a self-supervised manner. Comparison with SVHFNet and DIMNet models trained on VoxCeleb and VGGFace datasets showed our model performs worse in a 2-way setting. SVHFNet used a pretrained speaker identification network and DIMNet trained the network in a supervised manner with identity labels. Our model was trained from scratch without labels and outperformed DIMNet in a more challenging scenario. Qualitative and quantitative analyses were conducted to examine the relationship between conditions and generated face samples. Random samples from the (z, c) plane showed different characteristics controlled by z, such as head orientation and background color. The study explored how different latent variables control various aspects of generated face images, such as head orientation, background color, gender, age, and ethnicity. The experiment involved interpolating speech conditions to generate face images, showing a correlation between the generated samples and speech conditions. The study investigated the relationship between speech conditions and generated face images by sampling random variables and speech condition vectors from different speakers. Face images were generated and encoded to extract face embeddings, then the cosine distance between speech condition vectors and face embeddings was calculated. The Pearson correlation between these distances was computed to determine if there is a positive correlation between the embedding spaces of speech and face modalities. The study analyzed the correlation between speech conditions and generated face images by examining the cosine distance between speech condition vectors and face embeddings. Positive correlation was found, indicating that the face images are not randomly sampled. Additionally, the study tested the impact of controlling the gender of the speakers on the closeness of the speech condition vectors and generated face images. The study tested the impact of controlling speaker gender on the closeness of speech condition vectors and generated face images. Setting the genders the same resulted in smaller cosine distances, indicating a correlation between speech conditions and face images. Two inference experiments were conducted to determine cross-modal matching probabilities between face images and speech segments. The study found that the generator can produce plausible images based on speech conditions, with a 76.65% chance of choosing generated samples over ground truth images. The proposed loss function improves identity reflection in generated images, with a decrease to 47.2% without it. This does not imply superiority over paired face images, as the experiment is limited by the inference network's performance. The study showed that the proposed loss function enhances identity reflection in generated images, with a 79.68% chance of selecting samples from the generator trained with the new loss. Another experiment in F-V setting measured the accuracy of the inference network in selecting the correct audio segment based on a generated image. The study found that the network selects one audio segment with a 95.14% chance, and the accuracy of the inference network in a 2-way F-V setting is 88.47%. A face retrieval experiment was conducted using generated images as queries, with a dataset of 5,000 images from 100 speakers. The study conducted a face retrieval experiment using 5,000 images from 100 speakers. The trained face encoder measured feature distance between generated images and the dataset, achieving higher performance than Speech2Face. The proposed relidGANs loss function was crucial for generating face images reflecting speaker identity in a self-supervised manner. The study proposed a framework for training cGANs in a self-supervised manner, focusing on encoding speech as a conditional embedding. They introduced relidGANs loss to enhance identity distinction in generated face images. Future work includes addressing data bias issues in datasets and expanding methods to multi-modal datasets. The study proposed a framework for training cGANs in a self-supervised manner, focusing on encoding speech as a conditional embedding. They used stochastic gradient descent with specific parameters for optimization. Training configurations and optimization methods were detailed for the cGANs. The study implemented a cGAN framework for self-supervised training, emphasizing speech encoding. Specific optimization parameters were used, including fixed learning rates for the generator and discriminator. The model was trained for 500,000 iterations with a batch size of 24. The truncation trick from Brock et al. (2019) was applied with different thresholds for QLA 1, QLA 2, and QTA 3. An inference network with a speech encoder based on PASE architecture was utilized. The speech encoder utilized a PReLU activation and average pooling for time invariance. The face encoder employed 2d-CNN based ResBlocks, similar to the discriminator network. The generator structure was adapted from a previous model, incorporating z and c concatenation and AdaIN. The discriminator included additional layers and a sigmoid activation function. Details of the architectures are depicted in Figures 6, 7, and 8. The generator in Figures 7 and 8 produces images, with additional examples shown in Figure 9 using interpolated z vectors with fixed c. The latent vector z controls non-identity related parts in the generated face images."
}