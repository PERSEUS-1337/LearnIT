{
    "title": "HklKEUUY_E",
    "content": "Normalising Flows (NFs) are a type of generative models that transform simple densities into data densities. They are compared to other generative modeling techniques like Variational Autoencoders (VAEs) and denoising autoencoders. A new model called Variational Denoising Autoencoder (VDAE) is introduced, showing a connection between NFs and denoising autoencoders. Experiments on MNIST handwritten digits reveal insights into the modeling assumptions of these models. Unsupervised learning utilizes unlabeled data to create representations for downstream tasks or generate new data in costly domains. Generative models, like NFs, provide a statistical model of data with applications in various fields. High-dimensional data poses challenges in optimizing the model's density function. Generative modelling has seen advancements with likelihood-based models and Generative Adversarial Networks. Autoencoders are popular for learning complex latent representations through a parameterized mapping. Many variants of autoencoders have been proposed to improve learning. Autoencoders aim to create a compact latent representation of data by regularizing the model. Denoising Autoencoders add noise to input data to encourage robust representations. Variational Autoencoders assume a probabilistic latent variable model for generating data. In variational autoencoders, the task is to learn parameters to maximize the log marginal likelihood by constructing a variational lower bound using a variational approximation to the unknown posterior. The evidence lower bound (ELBO) can be jointly optimized with stochastic optimization. The ELBO encourages good encoding of data in the latent space and reconstruction from samples of this encoding. The ELBO in variational autoencoders encourages good encoding of data in the latent space and reconstruction from samples of this encoding. Normalizing Flows use an invertible transformation to obtain the sought distribution, allowing for exact maximum likelihood learning. The contribution of this work is two-fold. It sheds new light on the relationship between DAEs, VAEs, and NFs, discussing the pros and cons of these model classes. Additionally, it introduces Variational Denoising Autoencoders (VDAEs) as extensions of these models, generalizing NFs and DAEs to discrete data and learned noise. The Variational Denoising Autoencoders (VDAEs) generalize Normalizing Flows (NFs) and Denoising Autoencoders (DAEs) to discrete data and learned noise distributions. VDAEs can use non-invertible transformations when injected noise is minimal. Experimental results on the MNIST dataset show the theoretical advantages of VDAEs. The model includes hierarchical latent variables z with a tractable prior distribution and a neural-network conditional distribution. The encoder q\u03b8(z|x) is used for approximate inference in training the model. The encoder q\u03b8(z|x) samples latents z as a tractable noise distribution. The entropy of q\u03b8(z|x) is evaluated for the KL-term in the variational lower bound on data log marginal likelihood. This approach resembles a denoising autoencoder, reconstructing original data x from corrupted data x. The proposed model aims to reconstruct original data x from corrupted data x using a conditional model p(x|z) with additional terms to regularize latent representations z. It allows for learning the noise distribution q( ) as a fixed hyperparameter, and is a generalization of normalizing flows, enabling non-zero noise levels \u03c32. Our model for reconstructing data from corrupted data does not require tying the parameters of the encoder and decoder, allowing for more flexibility in the decoder's functional form. By adding a significant amount of noise, a decoupled decoder can achieve a higher variational lower bound compared to a tied-parameter decoder. This approach is more general than normalizing flows and is applicable to highly-structured data like text without the need for explicit dequantization. The VAE proposed in Section 2 is more general than NFs but requires an invertible one-to-one encoder with tractable Jacobian-determinant. To relax these restrictions, the model is generalized to allow non-invertible encoders as well, by performing a Taylor expansion of the latent variables around zero. This approach aims to address limitations in modeling choices and data dimensionality for various applications. The VAE proposed in Section 2 is a more general model than NFs, allowing for non-invertible encoders and non-one-to-one transformations. This broader class of VAE, referred to as L-VDAE, incorporates a linearised approach for evaluating the entropy of the encoder. When using transformations without a tractable Jacobian, the log-determinant of the covariance matrix is computed using eigenvalue decomposition. The log-determinant of the covariance matrix in the VAE model is computed using eigenvalue decomposition, which incurs additional computational cost. The Jacobian is evaluated using reverse-mode automatic differentiation with respect to each element of z. The computational complexity of this process is comparable to the cost of invertible convolutions in later layers of the model. The computational cost of scaling up L-VDAE to larger latent spaces requires stochastic approximations of the log-determinant. Efficient approximations can be implemented through Jacobian-vector and vector-Jacobian products, enabling linear computational complexity in n. Sampling from the Gaussian variational posterior is necessary for training and inference, achieved using the reparameterisation trick with a Jacobian-vector product for efficient sampling. VDAE combines ideas from VAE and NFs literature, focusing on efficient sampling using Jacobian-vector products. It is related to methods combining variational inference and NFs, with a sampling procedure inspired by DAE. VDAE is a probabilistic model formulation similar to Denoising VAEs, offering an alternative approach to DAEs as VAEs. The VDAE model offers an alternative probabilistic formulation of DAEs as VAEs, focusing on efficient sampling using Jacobian-vector products. It differs from previous methods by reconstructing the original data and injecting more noise, making it applicable to highly-structured data. The linearised form of VDAE can be seen as an extension of the vanilla VAE. The linearised form of VDAE extends the vanilla VAE by replacing the diagonal Gaussian posterior with a Jacobian-based full covariance posterior. This method increases flexibility in transformations without adding parameters to the networks, similar to other methods like autoregressive or mixture distributions. It can be compared to Invertible Residual Networks and FFJORD, as it does not restrict the form of the resulting transformation's Jacobian. i-ResNets and FFJORD both focus on invertibility without requiring an analytical inverse like VDAE. i-ResNets use ResNet architecture and spectral normalization to ensure invertibility, while FFJORD is inspired by re-interpretation methods. The Continuous-time Normalising Flows (CNFs) parameterize an ODE as a neural network to transform data from data-space to latent z-space. The use of stochastic trace estimation in CNFs alleviates the need for tractable Jacobian log-determinants, but requires an ODE solver for evaluation and backpropagation through log p \u03b8 (z (t)). The VDAE model is related to autoregressive generative models, both being likelihood-based generative models. Autoregressive models factorize likelihood of data as simple per-dimension conditional distributions. The factorized structure of these models requires sequential sampling and a good choice of dimension ordering. Manipulating data representations in autoregressive models is more challenging than in VAEs or NFs. The VDAE model, a likelihood-based generative model, is compared to VAEs and NFs in terms of data manipulation ease. Empirical studies on image generation using VDAE on the MNIST dataset were conducted, with comparisons to VAE and NICE. Various architectures were used for the VDAE encoder and decoder, including additive couplings and ResNet blocks. Gaussian and Bernoulli distributions were utilized for the decoder, with details provided in the appendix. The VDAE model dequantises pixel values, adds noise, and decouples the encoder and decoder networks to achieve higher ELBOs. Decoupling the weights improves ELBO compared to a coupled network, especially for architectures with a sigmoid activation in the decoder's last layer. In the following experiments, the ELBO rapidly improves with decreasing noise levels in the decoder. The more general ResNet architecture is considered for the VDAE decoder. Results show that L-VDAE consistently outperforms the VAE baseline when the latent dimensionality is smaller than the input space. In experiments, L-VDAE outperforms VAE with smaller latent space dimensions, showing a more powerful variational posterior. Increasing latent space dimensionality improves variational lower bound, except when n = m. Proposed extensions do not match NFs' likelihoods due to architectural differences. L-VDAE with m < n uses ResNet blocks, while m = n employs NICE flow in the encoder. NICE flow in VDAE may not be sufficient for a denoising VAE. When using a NICE flow in the decoder, VDAE outperforms L-VDAE in likelihood, suggesting VDAE can enhance linearised models with a more powerful flow. Without additional regularization, models struggle to produce high-quality samples for continuous MNIST dataset. VDAE applied to binarised MNIST dataset shows promise for structured data analysis. Results in TAB1 show that L-VDAE consistently achieves higher ELBO than the VAE baseline on dynamic MNIST, with improvements as latent dimensionality increases. However, models using NICE in the decoder, such as L-VDAE and VDAE, have significantly worse likelihood despite higher dimensionality. VDAE also outperforms L-VDAE with a NICE encoder. Despite these differences, all models produce plausible handwritten digit samples. Variational Denoising Autoencoders (VDAEs) bridge VAEs, NFs, and DAEs, extending NFs to discrete data with non-invertible encoders using lower-dimensional latent representations. Preliminary experiments on MNIST demonstrate the success of this model. Experiments on MNIST handwritten digits show that the model performs well on discrete data but struggles when latent and input dimensionalities are the same. Future work should focus on improving the variational posterior by using invertible convolutions and affine coupling blocks. Additionally, conditioning the transformation on the data could be beneficial. The VDAE model is trained using a hyper-network BID16, with variational lower bound and additive coupling blocks. Projection layers reduce dimensionality using linear maps, initialized with random orthogonal vectors. Models are trained for 1000 epochs with ADAM optimizer and batch size of 1000 samples. Learning rate is warmed up for stability. The model architecture closely followed the described hyper-parameters and consisted of 4 additive coupling blocks with fully-connected hidden layers. Dimension partitioning alternated between even and odd dimensions after each block. L2 regularization with weight \u03bb = 0.01 was used when the model was implemented standalone. The L-VDAE and VAE models used a fully-connected ResNet architecture with consecutive residual blocks in the encoder, with the last block projecting to the dimensionality of the latents. Each residual block had 2 hidden layers with ReLU activations and a linear layer. The hidden size and block multiplicity varied within specific ranges. The VDAE and L-VDAE models employed a ResNet architecture with specific hyper-parameter values. Logistic and factorised normal priors were used, affecting sample quality at different noise levels. The VDAE and L-VDAE models focus on reconstruction error and organizing the latent space. Noise variance \u03c3 2 in VDAEs can act as a regularizer and be adjusted for sample quality."
}