{
    "title": "SyxknjC9KQ",
    "content": "Artificial neural networks are typically built with linear combination and non-linear activation functions, but learning the parameters can be challenging. The choice of activation function greatly impacts network performance. This paper proposes using morphological operations like dilation and erosion as basic operations in neurons, creating Morph-Net. These networks can approximate smooth functions with fewer parameters than traditional neural networks, and show favorable performance compared to similar structures. The study compares the performance of their network with similar structured networks using datasets like MNIST, Fashion-MNIST, CIFAR10, and CIFAR100. Artificial neural networks use artificial neurons to compute linear combinations of inputs, followed by non-linear activation functions to model output non-linearity. These networks have been successful in solving real-world problems like image classification, semantic segmentation, and image generation. Efficient training of these models can be challenging and requires special techniques like batch normalization and dropout. In this paper, new building blocks for neural networks are proposed using morphological operations as the elementary operation of neurons. These operations eliminate the need for additional activation functions and require fewer neurons for better performance. The network built with dilation-erosion neurons can approximate any continuous function with enough neurons. The paper introduces a neural network using dilation-erosion operations as the basic operation of neurons, eliminating the need for separate activation functions. This approach allows for learning complex decision boundaries with a small number of parameters. The paper discusses prior work on morphological neural networks, introduces the proposed network, and demonstrates its capabilities theoretically and empirically on benchmark datasets. In a recent work, BID14 introduced morphological neurons in a general setting, building on the single layer architecture proposed by BID18 for binary classification tasks. BID25 extended this to a two-layer architecture capable of learning multiple axis parallel hyperplanes. To address non-axis parallel decision boundaries, BID0 proposed learning a rotational matrix to rotate input before classification. In a separate work, BID20 proposed using L1 and L\u221e norm to smooth decision boundaries instead of dilation and erosion operations. BID19 introduced dendritic structure for morphological neurons, creating hyperbox decision boundaries. BID21 extended this structure to multiclass classification. BID26 introduced morphological perceptrons with competitive neurons using winner-take-all strategy. BID23 later proposed a new structure. Pessoa & Maragos (2000) combined classical perceptron with morphological perceptron, creating a network capable of complex classification tasks. They proposed a methodology to address the non-differentiability of max/min operations. de A. Arajo (2012) utilized a similar network architecture for regression tasks. de A. Arajo (2012) used a network architecture similar to morphological perceptrons for stock market forecasting, replacing the argmax operator with a linear function for regression. This allows for gradient descent training. BID31 proposed using a softmax function for morphological neurons with dendritic structure, enabling gradient descent training while retaining hyperbox boundaries. The network's basic components include dilation and erosion operations. The proposed network utilizes dilation and erosion neurons to compute functions on input data. The gradient of these operations is crucial for training artificial neural networks using back-propagation. The computational graph model of dilation operation and its gradient flow is shown, with the goal of optimizing the structuring element for improved performance. The proposed 'DenMo-Net' is a feed forward network with dilation and erosion neurons, followed by classical artificial neurons. The network aims to optimize the structuring element for improved performance by computing gradients using the chain rule. The dilation-erosion layer contains n dilation neurons and m erosion neurons, followed by c neurons in the linear combination layer. The DenMo-Net is a feed forward network with dilation and erosion neurons, followed by classical artificial neurons. It contains n dilation neurons and m erosion neurons in the dilation-erosion layer, with c neurons in the linear combination layer. The network can approximate any continuous function with the linear combination of dilation and erosion, and the approximation error decreases with an increase in the number of neurons. The DenMo-Net is a feed forward network with dilation and erosion neurons, followed by classical artificial neurons. It contains n dilation neurons and m erosion neurons in the dilation-erosion layer, with c neurons in the linear combination layer. The network can approximate any continuous function with the linear combination of dilation and erosion, and the approximation error decreases with an increase in the number of neurons in the dilation-erosion layer. A k-order hinge function consists of (k + 1) hyperplanes continuously joined together, while a d-order hinging hyperplanes (d-HH) is defined as the sum of multi-order hinge functions. This allows any continuous piece-wise linear function of d variables to be written as an d-HH, i.e. the sum of multi-order hinge functions. The DenMo-Net with n dilation and m erosion neurons, followed by a linear combination layer, can approximate any continuous smooth function by computing g(x) as a sum of multi-order hinge functions. The Stone-Weierstrass approximation theorem states that a continuous function can be approximated by a piecewise linear function. The Universal approximation theorem shows that a single dilation-erosion layer followed by a linear combination layer can approximate any continuous smooth function with enough nodes in the dilation-erosion layer. The DenMo-Net with enough dilation and erosion neurons can approximate any continuous function by computing g(x) as a sum of multi-order hinge functions. Increasing the number of neurons in the dilation-erosion layer reduces the approximation error. Each morphological neuron allows only one input to pass through due to the max/min operation. The dilation-erosion layer in DenMo-Net allows each neuron to choose one component of the input vector, leading to the computation of hyperplanes based on the selected components. Increasing the number of neurons exponentially increases the possible number of hyperplanes formed in the d-dimensional space. The DenMo-Net layer exponentially increases the number of hyperplanes for complex decision boundaries with a small number of neurons. Empirical validation shows advantages over other networks like NN-tanh, NN-ReLU, and Maxout network. Experiments were conducted on toy and benchmark datasets to visualize decision boundaries. In experiments on benchmark datasets like MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, categorical cross entropy loss was used with softmax function in the last layer. Networks were optimized using Adam optimizer with mini batches of size 32. Decision boundaries were visualized on data from two concentric circles. Classical neural networks struggled to classify data with only two hidden neurons, learning one hyperplane per neuron. The network with ReLU activation function learns boundaries with one hidden neuron. Maxout network outperforms with 87.17% training accuracy by introducing extra parameters for non-linearity. DenMo-Net in dilation-erosion layer learns 6 lines for decision boundary compared to 4 lines in maxout layer. The classification accuracy and parameter comparison are shown in table 1. The DenMo-Net network, with 10 neurons in the output layer, shows significant accuracy improvement on the MNIST dataset. Images are converted to column vectors for input. The network structure includes input, dilation-erosion, and linear combination layers. Test accuracy after 150 epochs with varying nodes in the dilation-erosion layer is shown in TAB1 and FIG2. Increasing the number of nodes in the dilation-erosion layer improves non-linearity and test accuracy. Test average accuracy of 98.43% is achieved after training 3 times with DenMo-Net using 200 dilation and 200 erosion neurons for up to 400 epochs. Experimentation with different neuron types shows that using both erosion only and both dilation and erosion neurons results in better accuracy. The Fashion-MNIST dataset aims to replace the MNIST dataset, providing 28x28 images of 10 classes with 60,000 training and 10,000 testing samples. The authors argue that MNIST is too easy and does not represent modern computer vision tasks. The CIFAR-10 dataset consists of 50,000 training and 10,000 test color images of size 32x32, with 10 classes. The images are converted to column vectors before being fed to the DenMo-Net for classification tasks. The experiments involve training the network separately 3 times up to 300 epochs, with reported test accuracy being the average of the 3 runs. The method used in this experiment yields better results compared to other networks with the same number of neurons in the hidden layer. DenMo-Net achieves the best accuracy in all cases, outperforming Maxout network even with more parameters. The network can learn more hyperplanes with a similar number of parameters to normal artificial neural networks. Using only erosion neurons slows down learning, while using both dilation and erosion neurons improves performance. The DenMo-Net network achieves the best accuracy by leveraging both dilation and erosion neurons. Results on the CIFAR-100 dataset show improved performance compared to using only erosion neurons. The network achieves better accuracy using erosion only compared to using both dilation and erosion neurons. Learning the structuring element and weights through gradient descent may result in slow learning due to only a single element being updated at a time. Stacking multiple layers does not necessarily lead to improved results. The network achieves better accuracy using erosion only compared to using both dilation and erosion neurons. Stacking multiple layers can lead to two types of networks, Type-I and Type-II, with different behaviors. Type-I networks show a combination of opening and closing operations, leading to slow training due to gradient propagation issues. Type-II networks tend to overfit, requiring further exploration. In this paper, a new class of networks using normal and morphological neurons is proposed. The network consists of three layers: input layer, dilation-erosion layer, and linear combination layer. It can approximate smooth functions without non-linear activation functions and learn hyperplanes efficiently. The dilation-erosion layer's max/min operator may contribute to improved results. Further exploration of deeper network versions is suggested. The paper proposes a new network using normal and morphological neurons with three layers: input, dilation-erosion, and linear combination. The max/min operator in the dilation-erosion layer is highlighted for potential improvement. Extending the work to include convolution layers for better performance on image data is recommended. The proof of Lemma 1 is provided in the appendix, showcasing equations and structuring elements for dilation and erosion neurons. Equations 17 to 24 introduce the concept of hinge hyperplanes in a dimensional input space, with terms represented by \u03b1 values. The equations show the relationship between terms with \u03b1 values of 1 and -1, and the sum of multi-order hinge functions. The expressions demonstrate the structure and composition of the terms in the context of the dimensional input space. Equations 17 to 24 introduce hinge hyperplanes in a dimensional input space with terms represented by \u03b1 values. The sum of multi-order hinge functions is discussed, showing the structure and composition of the terms. Propositions and theorems demonstrate the properties of PWL functions and their approximation capabilities. The DenMo-Net with n dilation and m erosion neurons followed by a linear combination layer can approximate any continuous function by increasing the neurons in the dilation-erosion layer, reducing the error bound as the number of nodes increases."
}