{
    "title": "HJzgZ3JCW",
    "content": "Convolutional Neural Networks (CNNs) are computationally intensive, especially on mobile devices due to the high number of multiplies needed for convolutions. Winograd's minimal filtering algorithm and network pruning can reduce operation count, but combining them is challenging. To address this, two modifications are proposed for Winograd-based CNNs: moving the ReLU operation into the Winograd domain to increase sparsity in transformed activations, and pruning weights in the Winograd domain to exploit static weight sparsity. These modifications result in a significant reduction in the number of multiplications for models on CIFAR-10, CIFAR-100, and ImageNet datasets, with minimal loss of accuracy. Deep Convolutional Neural Networks (CNNs) have shown significant improvement in machine learning applications but are compute-limited due to the high number of multiplies needed for convolutions. BID16 proposed a model with fewer multiplies for digit classification, while later models like AlexNet increased the number of multiplies significantly. Despite the powerful representational ability of large CNNs, their computational workload hinders deployment on mobile devices. The proposed modifications for Winograd-based CNNs, such as moving ReLU to the Winograd domain and weight pruning, result in a significant reduction in multiplications with minimal loss of accuracy. The ability of large scale CNNs is hindered by their computational workload, making deployment on mobile devices challenging. Two research directions have been explored to address this issue: using Winograd's minimal filtering algorithm to reduce the number of multiplies needed for convolutions, and pruning the model to exploit dynamic sparsity of activations. However, these two directions are not compatible, as the Winograd transformation eliminates the gain from exploiting sparsity. Modifications to the original Winograd-based convolution algorithm are introduced in this paper to address this problem. The paper introduces modifications to the Winograd-based convolution algorithm to combine the gains of Winograd's algorithm and exploiting sparsity. The approach involves moving the ReLU operation after the Winograd transform and pruning weights after transformation to reduce operation count. Additionally, previous research has explored using the linear algebra property of convolution to reduce the number of multiplies, achieving significant savings. The approach in BID14 achieves a 47% saving in multiplies by exploiting the element-level linear algebra property of convolution with Winograd's minimal filtering algorithm. Model compression techniques, such as pruning network parameters and exploiting weight sparsity, can reduce the number of multiplies required in CNNs. Dynamic activation sparsity can be achieved through ReLU non-linearity, reducing multiplication workload in model compression. BID5 and BID11 demonstrated significant reductions in multiplies by exploiting sparsity in weights and activations. Novel architectures and optimizations for deep learning accelerators also leverage activation sparsity for efficiency. The Winograd-ReLU CNN architecture preserves sparsity in weights and activations, reducing computational workload. It utilizes a novel mechanism to efficiently skip zeros in input activations, enhancing efficiency in deep learning accelerators. The Winograd-ReLU CNN architecture efficiently skips zeros in input activations, reducing computational workload. It utilizes matrices to transform input activation patches and kernel weights, resulting in output feature maps. The algorithm reduces the number of multiplies and utilizes a \"vanilla\" pruned network for ReLU non-linear operations. Activation patch S is obtained from equation FORMULA1, illustrated in FIG0 (a) for p = 4. While g and d may be sparse, the element-wise multiply remains dense due to transformations filling spatial-domain zeros. The Winograd-domain pruned network, introduced by Liu et al. (2017) and BID17, involves ReLU-ed spatial-domain input d and pruned Winograd-domain weight GgG T. The output activation patch S is obtained from equation (3), with the algorithm shown in FIG0 (b) for p = 4. Despite sparse Winograd-domain weights, activations remain dense due to B(\u00b7)B T transforms. Sparsity in spatial activations does not reduce the number of multiplies. The Winograd-ReLU Network introduces a new CNN architecture where ReLU is applied in the Winograd domain to reduce the number of multiplies. The spatial-domain kernel is eliminated, and training involves three phases: dense training, pruning, and retraining. This new architecture is not equivalent to vanilla CNN or conventional Winograd CNN. In the Winograd-ReLU Network, training involves dense training, pruning, and retraining. The transformed kernel is trained directly in the transform domain, eliminating the need for a spatial kernel. Pruning is done by setting weights below a threshold to zero, and retraining involves using a sparsity mask to maintain zero weights. The methodology involves training the Winograd-ReLU Network with dense training, pruning, and retraining. The gradient of the network's loss is calculated using the chain rule. Experiments were conducted on different CNNs with 3x3 kernels on datasets like CIFAR-10, CIFAR-100, and ImageNet 2012 using VGG-nagadomi, ConvPool-CNN-C model, and a variation of ResNet-18 architectures. The study involved training various CNN models on different datasets like CIFAR-10 using VGG-nagadomi. The models were pruned and re-trained, with the Winograd-ReLU CNN achieving the highest validation set accuracy of 93.43%. The first convolution layer was found to be most sensitive to pruning, with a density set at 80%. The study involved pruning convolution layers with density ranging from 80% to 20%. Test accuracy vs density for three models on VGG-nagadomi was compared, showing that the Winograd-ReLU model outperformed baseline models. Pruning reduced workload significantly, with the Winograd-ReLU model showing the highest improvement. The study compared test accuracy vs density for three models on VGG-nagadomi, showing that the Winograd-ReLU model outperformed baseline models. Pruning convolution layers from 80% to 20% significantly reduced workload, with the Winograd-ReLU model achieving the highest improvement in accuracy. The study compared test accuracy vs density for three models on VGG-nagadomi, showing that the Winograd-ReLU model outperformed baseline models. Pruning convolution layers significantly reduced workload, with the Winograd-ReLU model achieving the highest accuracy at lower density levels. The input activation density and workload comparisons for pruned convolution layers in the models are shown in TAB3. The Winograd-ReLU model had the highest accuracy at a given density and reduced the overall network workload significantly compared to the baseline models. The 2012 dataset was used for its performance and structure that suits the Winograd-ReLU approach. It differs from the original ResNet-18 by replacing certain convolution layers with max-pooling layers. Three models were trained from scratch, and Figure 4 shows the validation accuracy for these models on the ResNet-18 variation. The study evaluated the density of three models based on a variation of ResNet-18. Results showed that the models could be pruned to 60%, 50%, and 30%/35% density without significant loss of accuracy. Top-1 accuracies ranged from 66.45% to 66.67%, and top-5 accuracies ranged from 87.30% to 87.42%. Pruning reduced the workload of convolution layers by 5.1\u00d7 to 13.2\u00d7, showing significant improvements. Pruning the Winograd-ReLU model significantly reduces the overall network workload by 2.3\u00d7 and 2.6\u00d7 compared to baselines. The experiment results compare models based on weight and activation dimensions, as well as the dynamic density of activations. In a convolutional neural network, the decision boundaries of the classifier are determined by weights, and insufficient non-zero weights or activations can lead to accuracy loss. Experimental results show that Winograd-ReLU CNN achieves similar accuracy to baseline CNNs without pruning and is more robust to aggressive pruning. The architecture has advantages in weight and activation dimensions, allowing for classification on a higher dimension with complex decision boundaries. The Winograd-ReLU CNN model achieves similar accuracy to baseline CNNs without pruning and is more robust to aggressive pruning. It classifies on a higher dimension with complex decision boundaries, showing stronger representational ability in high dimensional image feature space. Activation densities vary little among layers in this model, with most close to 50%. The nature of image convolution ensures spatially smooth activations, benefiting classification within a patch. The proposed Winograd-ReLU model's kernels are visualized. The Winograd-ReLU model's kernels are visualized, showing distinct (2, 2) elements that are likely preserved during aggressive pruning due to their unique transformation properties. The (2, 2) elements are the only ones with a non-zero mean in the Winograd-ReLU model. Sparse weights and activations are combined with the Winograd transform for computational savings. We prune weights in the transform domain and move the ReLU operation after the transform to make activations sparse. This approach reduces computation by 10.4\u00d7, 6.8\u00d7, and 10.8\u00d7 on CIFAR-10, CIFAR-100, and ImageNet datasets. Using larger patch sizes (e.g., p = 6) and exploring different Winograd transformation matrices (B, G, and A) can lead to greater computation savings. Implementing different pruning rates for each network layer can help maintain accuracy and reduce overall workload. Combining the Winograd-ReLU network with other simplification techniques like weight and activation quantization (BID4, BID18, BID20) can further decrease energy consumption during computation."
}