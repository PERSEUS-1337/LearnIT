{
    "title": "HygQBn0cYm",
    "content": "In this work, a policy is trained by penalizing the mismatch between distributions of states during training and execution. A model of environment dynamics is used for multiple time steps, with a policy network minimizing a cost over the trajectory. The cost includes a policy cost and an uncertainty cost, measured by the uncertainty of the dynamics model. The approach is evaluated using a large-scale observational dataset of driving behavior. In recent years, model-free reinforcement learning methods using deep neural network controllers have shown effectiveness in various tasks. However, these methods often require many interactions with the environment to learn, limiting their application in realistic environments. Building a simulator can help mitigate this issue. Model-based reinforcement learning approaches aim to learn a model of the environment dynamics to plan actions or train a policy. This method reduces the number of interactions needed to obtain an effective policy. However, in some cases, a single poor action in a real environment can have unacceptable consequences. Algorithms need to learn policies from observational data due to the consequences of poor actions in a real environment. Autonomous driving, for example, can utilize trajectories of human drivers collected from traffic cameras. Learning from observational data is challenging as it may only cover a small region of the defined space. One option is imitation learning, but it suffers from a mismatch. In the context of learning policies from observational data, one option is imitation learning, which suffers from a mismatch between training and execution states. Another option is to learn a dynamics model from observational data, but this may lead to arbitrary predictions outside the trained domain. The policy network may exploit errors in the dynamics model, leading to wrongly optimistic states. The problem can be self-correcting in interactive settings but persists if the dataset is fixed. The proposed solution is to train a policy while penalizing the mismatch. In this work, a policy is trained while penalizing the mismatch between induced trajectories and training data. A learned dynamics model is used for multiple time steps, with a policy network minimizing a cost over the trajectory. The cost includes a policy cost and an uncertainty cost, measured using dropout. This approach is applied to learning policies for autonomous driving in dense traffic using real-world driving trajectories. Model-based control with additional uncertainty is shown to be effective. Model-based control with an uncertainty regularizer outperforms unregularized control, enabling the learning of driving policies using only observational data. Leveraging an action-conditional stochastic forward model, the approach involves learning a dynamics model from observational data and training a policy network to minimize policy and uncertainty costs. In this work, recent approaches for stochastic prediction using Variational Autoencoders are considered. The stochastic model takes input sequences of observed states, actions, and latent variables to predict the next state. Latent variables are sampled from a distribution output by a posterior network during training. The model is trained to minimize the KL divergence between the posterior distribution and a fixed prior. In this work, a stochastic video prediction model is trained using a mixture of two Gaussians for the posterior distribution over latent variables. The model aims to accurately respond to input actions and not encode factors of variation in the outputs due to actions. This approach is different from recent models that do not use their model for planning or training a policy network. The text chunk discusses the use of z-dropout in training a parameterized policy network after training a forward model. The policy network is optimized using a differentiable objective function with a policy cost and another term. The text chunk introduces a policy cost C and an uncertainty cost U for training a policy using a stochastic forward model. The uncertainty cost is based on the uncertainty of the dynamics model calculated using dropout, reflecting the likelihood of predicted states under the distribution the training data is drawn from. The text discusses using dropout as a regularization technique in neural networks to estimate uncertainty. By calculating the covariance of outputs over multiple dropout masks, a differentiable scalar estimate of uncertainty can be obtained. This uncertainty estimate is based on the distribution the training data is drawn from. The text discusses using dropout as a regularization technique in neural networks to estimate uncertainty. A scalar measure of uncertainty is defined based on the prediction model with different dropout masks. Minimizing this quantity encourages the policy network to produce actions that the forward model is confident about. The uncertainty measure is calculated based on the output sequence of states from the dataset, aiming to output actions leading to a similar trajectory as observed. The text discusses training policy networks using uncertainty regularization techniques such as dropout. Two approaches, MPUR and MPER, optimize objectives over multiple time steps using a learned dynamics model. This allows for penalizing actions leading to large divergences from the training manifold in the future. The MPUR approach is akin to training a Bayesian neural network with latent variables. Training a Bayesian neural network (BNN) with latent variables using variational inference involves approximating the distribution over model predictions for future time steps. The variational distribution q is parameterized by \u03c6 for latent variables and \u03b8* for forward model parameters, which are approximated using dropout. Minimizing the loss function in Equation 1 for the stochastic forward model with dropout is equivalent to minimizing the Kullback-Leibler divergence. The approximate posterior distribution over outputs can be obtained by sampling latent variables and dropout masks. The covariance of the outputs can be decomposed into aleatoric and epistemic uncertainty terms. The first term represents epistemic uncertainty, while the second term considers the variance from sampling different latent variables. The uncertainty penalty penalizes the trace of the first matrix, approximating the expectation over latent variables with a single sample from the prior. The covariance matrix for aleatoric uncertainty varies with inputs, allowing for handling heteroscedastic environments. This approach was applied to learn driving policies using a large-scale dataset of driving videos from traffic cameras. The dataset of driving videos from traffic cameras captures complex driver behavior with high uncertainty. 5596 car trajectories were tracked and split into training, validation, and testing sets. Preprocessing yielded a state and action representation for each car suitable for learning a predictive model. The images represent the ego car's position, velocity, lane markings, and neighboring cars. Actions consist of acceleration, braking, and steering angle changes. Cost functions include proximity to other cars and overlap with lane markings. The dataset was adapted for evaluating learned policies in an environment similar to OpenAI Gym BID5. A single car is controlled by the policy being evaluated while other cars follow trajectories from the dataset. This approach avoids hand-designing policies for neighboring cars but may make the problem more challenging as neighboring cars do not react to the controlled car. The text discusses the use of learned forward models for planning in the context of controlling a car in an environment with other cars. The approach involves backpropagating through a learned forward model to train policies, with a focus on penalizing uncertainty in the model. This method differs from previous works that used dropout for model uncertainty estimates in model-based reinforcement learning. The text discusses applying uncertainty estimates to high-dimensional states of a forward model to train a policy network through gradient descent. This method aims to penalize uncertainty in the model during planning for controlling a car in an environment with other cars. The text discusses using variational autoencoders combined with dropout for high-dimensional video prediction to improve uncertainty estimates in training paramaterized policies. This approach aims to address the problem of covariate shift in imitation learning by efficiently utilizing expert feedback. The text discusses addressing covariate shift in imitation learning without expert feedback. It compares the MPER approach to previous works using neural networks for trajectory imitation in high-dimensional settings. In this work, the focus is on learning driving policies in dense traffic using high-dimensional state representations. The model includes a deterministic and stochastic forward model with convolutional layers for processing images and fully-connected layers for processing vectors and actions. Experimental results are reported, with details provided in the Appendix. The model includes deterministic and stochastic forward models with convolutional layers for processing images and fully-connected layers for processing vectors and actions. Predictions were generated using both models, with the deterministic model producing increasingly blurry predictions and the stochastic model producing sharp predictions far into the future by sampling different sequences of latent variables. Additional video results and model details can be found at the provided URL. The testing set shows collision-free human trajectories. Different policy networks were trained using single-step imitation learning, stochastic value gradients, value gradients, MPUR, and MPER with deterministic or stochastic models. The policies aim to match expert trajectories from the training set. Performance details can be seen in Figure 6. The policy is trained to match expert trajectories from the training set. Performance is measured in success rate and distance travelled, with a cost term penalizing uncertainty being essential for good performance. Using the modified posterior distribution (z-dropout) improves performance with the stochastic forward model. Training policies with longer rollouts through the environment dynamics model also significantly improves performance. The evaluation criteria include reaching the end of the road segment without collisions or driving off the road, and the distance travelled before the episode ends. The approach is compared against baselines that can also learn from observational data. The policy trained with unregularized VG exploits errors in the forward model to produce actions yielding low predicted cost but high uncertainty. Including uncertainty cost improves performance in the environment, with effective behaviors such as braking, accelerating, and turning to avoid other cars. Trajectories from different methods show that MPUR and MPER methods primarily stay within lanes, while single-step imitation learner produces divergent trajectories. Removing uncertainty penalty makes MPUR equivalent to VG in deterministic setting, highlighting the significant performance difference. The comparison between the VG and MPUR methods shows that while VG has lower predicted policy cost, it has high uncertainty cost. On the other hand, MPUR has higher policy cost estimates but lower uncertainty cost, performing better in the environment. The stochastic model with z-dropout parameterization shows a significant improvement over the standard Gaussian posterior model. The URL provides further comparisons of action-conditional predictions between the two models. The standard model is less responsive to input actions than the model trained with z-dropout, leading to differences in performance. Z-dropout discourages encoding output information in latent variables, resulting in better performance by predicting outputs from actions. Performance of MPUR and MPER improves with longer rollout lengths, aligning policy-induced states and training distribution over longer horizons. Stochastic model with z-dropout outperforms the standard model. See Appendix E for more experiments and discussion. In this work, a general approach for learning policies from observational data is proposed. It involves a learned stochastic dynamics model to optimize policy cost over multiple time steps, an uncertainty term penalizing trajectory divergence, and a modified posterior distribution to keep the model responsive to input actions. Applied to real-world traffic data, the approach effectively learns policies for navigating dense traffic, outperforming other observational data-based approaches. However, there remains a performance gap compared to human performance. The text describes a general approach for learning policies from observational data, focusing on optimizing policy cost over multiple time steps in dense traffic conditions. The dataset and environment used are detailed, with a gap still existing between learned policies and human performance. The approach is deemed applicable to various settings with expensive interactions but abundant observational data. The text describes the process of extracting car trajectories from a video, splitting them into training, validation, and testing sets, and preprocessing the data to create state representations for learning a predictive model. The state representation includes an image of the car's neighborhood and a vector of its position and velocity. The text describes rendering images of cars with lane markings and neighboring car locations encoded in different channels. Each image has dimensions 3 \u00d7 117 \u00d7 24 and is denoted by i t. Vectors u t = (p t, \u2206p t) represent position and velocity, with safety distance increasing with speed. Action vectors consist of acceleration and angle changes. The cost function in the context of rendering images of cars includes proximity and lane costs, computed using masks in pixel space. The proximity cost measures the distance to neighboring cars, while the lane cost measures overlap with the lane. These operations are differentiable for gradient backpropagation. Building an environment for evaluating policies for autonomous driving involves adapting a dataset of state-action pairs for training a prediction model. The goal is to evaluate planning methods in a realistic environment where cars follow human-like behavior. In evaluating policies for autonomous driving, creating a realistic environment is challenging. One approach is to let other cars follow their trajectories in the dataset while controlling one car with the policy being evaluated. The controlled car's trajectory is updated based on policy actions, while other cars' trajectories remain fixed. Collisions end the episode, with other cars behaving human-like but not reacting to the controlled car. This approach creates a challenging driving task, similar to training a Bayesian neural network with latent variables using variational inference. The distribution over model predictions for s t+1 is given by a Bayesian neural network (BNN) BID30 with latent variables using variational inference. The posterior distribution over model weights and latent variables is approximated with a variational distribution q parameterized by \u03b7. This distribution represents a mixture of two Gaussians with small variances over each weight matrix in the forward model, with the mean of one Gaussian fixed at zero. Samples can be drawn by applying different dropout masks. The variational distribution q is parameterized by \u03b7 = {\u03b8*, \u03c6} to approximate the posterior over model weights and latent variables. The evidence lower bound is optimized by minimizing the Kullback-Leibler divergence between the approximate and true posterior. The second KL term in the prior p0(\u03b8) can be ignored, leading to scaled L2 regularization on model parameters. By approximating the integral with a single sample, the distribution over model predictions is obtained. The architecture of the forward model includes four neural networks: a state encoder, an action encoder, a decoder, and a posterior network. The state encoder processes 20 previous states, each consisting of an image and a vector encoding the car's position and velocity. The images and vectors are fed through convolutional and fully connected networks, respectively. The model is trained as a variational autoencoder with dropout. The architecture of the forward model includes a state encoder, action encoder, decoder, and posterior network. The posterior network generates a distribution over latent variables from input states and ground truth, producing a sample z t. This sample is expanded to size n H and combined with action representation. The result is passed through a deconvolutional network for prediction. The forward model architecture includes a deconvolutional network with 256-128-64 feature maps for predicting the next image and a 2-layer fully-connected network for predicting the next state vector. A cost predictor is trained to output proximity and lane costs. The prediction model is trained in deterministic mode for 200,000 updates and then in stochastic mode for another 200,000 updates. Our model was trained using Adam BID20 with learning rate 0.0001 and minibatches of size 64, unrolled for 20 time steps, and with dropout (p dropout = 0.1) at every layer. Cars are initialized at the beginning of the road segment with their initial speed from the dataset and controlled by the policy network. Performance is reported for cars in testing trajectories only. All policy networks have the same architecture: a 3-layer ConvNet followed by 3 fully-connected layers outputting the parameters of a 2D Gaussian distribution. The policy networks are trained with Adam using a learning rate of 0.0001. The MPER and MPUR policies are trained through backpropagation with the reparameterization trick. The single-step imitation learner minimizes the negative log-likelihood of ground truth actions. MPUR policies use a weighting of \u03bb = 0.5 for uncertainty cost. Gradients of predicted costs are detached to prevent excessive slowing down of the policy. Policy training is repeated with 3 random seeds for each method. The policy training for VG, SVG, and MPUR includes minimizing the policy cost, which prioritizes avoiding other cars and staying within lanes. MPUR also minimizes model uncertainty cost by normalizing uncertainty estimates based on rollout length. If uncertainty estimate is lower than mean uncertainty estimate, loss is zero. The lack of responsiveness of the stochastic forward model is pronounced when given a sequence of inferred latent variables instead of sampled ones. This difference may be due to the high dependence of latent variables in the first case and their independence in the second case. If action information is encoded in the latent variables, the effects on the output may cancel each other out. The performance of MPUR policies drops significantly when using inferred latent variables compared to sampled ones. The forward model is less sensitive to actions when latent variables are sampled from the prior instead of the posterior. The z-dropout parameterization helps reduce this issue. Additionally, results from running Proximal Policy Optimization (PPO) BID41, a model-free algorithm, are also reported for completeness. The study compares the performance of MPUR policies using inferred latent variables versus sampled ones. Results show that PPO achieves slightly higher final performance than MPUR. Regret, a measure of poor decisions during learning, is also considered. PPO outperforms MPUR in final performance but incurs higher regret due to poor early-stage policies. MPUR learns from observational data and starts with a good policy before interacting with the environment."
}